multiagent reinforcement learning multi robot systems survey yang gu department computer science university essex park colchester essex sq united kingdom multiagent reinforcement learning multirobot systems challenging issue robotics artificial intelligence 
increasing interests theoretical researches practical applications currently lot efforts providing solutions challenge 
difficulties scaling multiagent reinforcement learning multi robot systems 
main objective provide survey completely multiagent reinforcement learning multi robot systems 
reviewing important advances field challenging problems promising research directions analyzed 
concluding perspectives authors 
multi robot systems mrss fulfil tasks difficult accomplished individual robot especially presence uncertainties incomplete information distributed control asynchronous computation performance mrss redundancy operation contributes task solutions reliable faster cheaper way 
practical potential applications unmanned aerial vehicles uavs spacecraft autonomous underwater vehicles ground mobile robots robot applications hazardous unknown environments benefit mrss 
mrss received considerable attention decade 
emails essex ac uk challenging issues mrss 
challenges involve realization basic behaviours trajectory tracking formation keeping control collision avoidance allocating tasks communication coordinating actions team reasoning practical multi robot system firstly basic behaviours lower functions feasible available 
time upper modules task allocation planning designed carefully 
designing mrss impossible predict potential situation robots may encounter specify robot behaviours optimally advance 
robots mrss learn adapt operating environment counterparts 
control learning important challenging problems mrss 
mainly focus learning problems multirobot systems assume basic behaviours participating robot available 
currently great deal research multiagent reinforcement learning rl mrss 
multiagent reinforcement learning allows participating robots learn mapping states actions rewards payoffs obtained interacting environment 
mrss benefit rl aspects 
robots mrss expected ordinate behaviours achieve goals 
robots obtain operative accelerate learning speed learning 
rl algorithms learning attracted great deal attention 
explicit presentation emergent idea operative behaviours individual learning algorithm 
improving learning efficiency learning shown tan 
study indicates operative robots learned faster individually 
tan demonstrated sharing perception learning experience accelerate learning processes robot group 
learning applied mrss forage robots soccer playing robots prey pursuing robots robots moving target observation robots research applications focused tackling large learning spaces mrss 
example modular learning approaches advocate large learning space separated small learning spaces ease exploration 
normally mediator needed approaches select optimal policies generated different modules 
theoretically environment mrss stationary 
basic assumption traditional learning working violated 
rewards payoffs learning robots receive depend actions action robots 
individual learning methods unable model dynamics simultaneous learners shared environment 
decade increasing interest extending individual rl multiagent systems particularly mrss 
theoretic viewpoint attractive research field expand range rl realm simple single agent realm complex agents learning simultaneously 
advances gent systems mrss 
objective review existing works analyze challenging issues viewpoint multiagent rl mrss 
hope find interesting directions ongoing research projects 
ii 
preliminaries markov decision process markov decision processes mdps mathematical foundation rl single agent environment 
formally definition follows definition markov decision process markov decision process tuple finite discrete set environment states finite discrete set actions available agent discount factor transition function giving state action probability distribution states reward function agent giving expected immediate reward received agent actions state :10.1.1.32.7692
definition policies policy denoted description behaviours agent 
stationary policy probability distribution actions taken state 
deterministic policy probability action state 
mdp deterministic stationary optimal policy 
mdp agent acts way maximize long run value expect gain 
discounted objective factor controls effect rewards decisions moment 
denoting expected discounted reward agent starting state action step policy define set simultaneous linear equations state function function deterministic stationary policy optimal starting state defined set equations max 
greedy policy defined function definition greedy policy policy said greedy assigns probability action argmax state reinforcement learning objective rl learn act dynamic environment experience maximizing payoff functions minimizing cost functions equivalently 
rl state dynamics reinforcement function partially unknown 
learning occurs iteratively performed trial error methods reinforcement signals experience interactions agent environment 
learning learning value learning version rl learns utility values values state action pairs 
form model free rl provides simple way agents learn act optimally controlled markovian domains 
viewed method asynchronous dynamic programming 
essence learning temporal difference learning method 
objective learning estimate values optimal policy 
learning agent uses experience improve estimate blending new information prior experience 
may optimal policy values unique 
learning agent experience consists sequence distinct episodes 
available experience agent mdp environment described sequence experience tuples st rt 
table shows scheme qlearning 
individual learning discrete cases proved converge optimal values probability state action pairs visited infinite times learning rate declines 
theorem provides set conditions qt converges theorem bounded rewards rt learning rates ti ti qt probability denotes index th time action tried state greedy policy optimal policy qt agent may explore sufficient amount guarantee convergent performance greedy policy adopted choose actions learning process 
overcome conflict glie greedy limit infinite exploration policy proposed 
show convergence glie policy concept crucial definition convergence behaviour agent converges behaviour action distribution stationary limit 
definition littman pointed glie policy need converge behaviour ties greedy actions broken arbitrarily 
agent learning converge behaviour unique table learning algorithm observes current state st chooses action performs 
observes new state receives immediate reward rt 
adjusts qt values learning factor rule qt rt vt qt st qt vt qt optimal policy function converges 
convergence results come 
theorem single agent environment qlearning agent converge probability 
furthermore agent glie policy convergence behaviour probability optimal policy unique 
may find theorem difficult apply know unique optimal policy 
qlearning situation oriented usually values optimal policy unique 
matrix games matrix games elementary type players particularly player games 
matrix games players select actions available action space receive rewards depend player actions 
definition matrix games matrix game tuple rn number players ai ri finite action set payoff function respectively player matrix games bimatrix games formulate frameworks multiagent reinforcement learning 
particularly give definition follows definition bimatrix games bimatrix game defined pair payoff matrices 
matrix mi dimension entry mi gives reward ith player joint action pair 
matrix games joint actions correspond particular entries payoff matrices 
reinforcement learning purpose agents play matrix game repeatedly 
applying repeated matrix game theory multiagent reinforcement learning payoff structure explicitly 
requirement restriction applying matrix games domains mrss payoff structure difficult define advance 
stochastic games currently multiagent learning focused theoretic framework stochastic games sgs markov games mgs 
sgs extend state mg multi state cases modelling state transitions mdp 
state sg viewed mg sg player viewed mdp 
follows review important concepts easing understanding related works survey 
definition best response policy said best response player policies optimal policies 
definition nash equilibrium nash equilibrium collection strategies players player strategy players strategies 
nash equilibrium player better changing strategies unilaterally players don change nash strategies 
nash equilibrium existed game 
definition mixed strategy nash equilibrium mixed strategy nash equilibrium bimatrix game pair strategies players player strategy best response player strategy mathematically probability distributions corresponding action spaces agent respectively 
game theory multiagent reinforcement learning game theory gt explicitly designed reasoning multiple players 
players gt assumed act rationally 
take best policies play play tricks 
great deal research multiagent learning borrowed theoretic frameworks notions sgs 
sgs studied field multiagent reinforcement learning appear natural powerful extension mdps multi agent domains 
framework sgs nash equilibria important solution concept problem simultaneously finding optimal policies presence learning agents 
nash equilibrium agent player playing optimally respect nash equilibrium policy 
agents playing policy nash equilibrium rationally agent learn better policy 
iii 
theoretic frameworks multiagent reinforcement learning sg frameworks framework sgs mgs widely adopted researchers model multiagent systems finite states actions :10.1.1.138.2589
particularly framework sgs exploited extending learning multiagent systems 
sg agents select actions simultaneously 
reward agent receives depends joint actions agents current state state transitions markov property 
reinforcement learning framework sgs formal definition :10.1.1.138.2589
definition framework sgs learning framework sgs described tuple rn finite state space corresponding finite sets actions available agent 
state transition function state action agent 
probability distribution state space ri represents reward function agent 
discount factor 
learning framework sgs learning agents attempt maximize expected sum discounted rewards 
correspondingly set agent defined stationary policies single agent system multiagent systems joint actions determine state rewards agent 
selecting actions agents transitioned state receive rewards 
fictitious play framework known sg framework fictitious play technique finding equilibria 
learning paradigm fictitious play applied form theoretical framework 
provides quite simple learning model 
framework fictitious play algorithm maintains information average estimated sum discounted rewards 
agents fictitious play method deterministically chooses actions agent done best past 
computing estimated sum discounted rewards simple temporal difference backup may 
compared framework sgs main merit fictitious play capable finding equilibria zero sum games classes general sum games 
obvious disadvantage framework fictitious play merely adopts deterministic policies play stochastic strategies 
hard apply zero sum games find equilibrium policy play policy 
addition learning stability serious problem 
fictitious play framework inherent discontinuity small change data lead abrupt behaviour 
overcome unstable problem variants fictitious play developed see literature 
bayesian framework multiagent reinforcement learning algorithms developed sg framework minimax nash require converge desirable equilibria 
sufficient exploration strategy space needed convergence established 
solutions multiagent reinforcement learning problems usually equilibrium 
obtain optimal policy agents find identify equilibria policy current state 
bayesian framework exploration multiagent reinforcement learning systems proposed 
bayesian framework modelbased reinforcement learning model 
framework learning agent priors reason action influence behaviours agents 
prior density possible dynamics reward distribution known learning agent advance 
basic assumption bayesian framework learning agent able observe actions taken agents resulting game state rewards received agents 
course assumption problem coordination restrict applications settings opponent agents generally broadcast information 
establish belief learning agent bayesian framework priors probability distribution state space possible strategy space 
belief updated learning observing results actions action choices agents 
order predict accurately actions agents learning agent record maintain appropriate observable history 
assumed learning agent keep track sufficient history predictions 
aforementioned assumptions extra assumptions belief 
priors models factored independent local models rewards transitions 
second needs assumed belief opponent strategies factored represented convenient form 
policy iteration framework value iteration frameworks policy iteration framework provide direct way find optimal strategy policy space 
policy iteration framework bowling veloso proposed wolf phc algorithm agents assumed playing stationary policies 
works thinking lines 
reported works algorithms framework agents system considered learn simultaneously 
compared aforementioned frameworks researches policy iteration reinforcement learning multiagent systems need done 
fortunately research results policy iteration algorithms instance may refer literature 
possible way extend existing policy iteration algorithms single agent systems field multiagent systems 
iv 
multiagent reinforcement learning algorithms difference single agent multiagent system exists environments 
multiagent systems adapting agents environment longer stationary violating markov property traditional single agent behavior learning relies 
individual robot learning traditional qlearning successfully applied paradigms 
researchers apply learning straightforward fashion agent multiagent system 
aforementioned fact environment longer stationary multiagent system usually neglected 
decade researchers efforts rl methodology particularly learning framework alternative approach learning mrss 
pointed early basic assumption traditional learning working violated case mrss 
minimax learning algorithm sg framework littman proposed minimax learning algorithm zero sum games learning player maximizes payoffs worst situation 
players interests game opposite 
essentially minimax learning value function reinforcement learning algorithm 
minimax learning player try maximize expected value face worst possible action choice opponent 
player cautious learning 
calculate probability distribution optimal policy player littman simply linear programming 
illustrating version minimax learning algorithm shown table ii 
minimax learning algorithm firstly just included empirical results simple zero sum sg game version soccer 
complete convergence proof provided works summarized theorem theorem player zero sum multiagent sg environment agent minimax learning algorithm converge optimal probability 
furthermore agent glie policy converge behaviour probability limit equilibrium unique 
minimax learning algorithm may provide safe policy performed regardless table ii minimax learning algorithm initialize 
choose action learn exploring probability return action uniformly random 
return action probability receiving reward moving state action opponent action update linear programming find arg max min min decaying rate learning parameter 
existence opponent 
policy minimax learning algorithm guarantee receives largest value possible absence knowledge opponent policy 
minimax learning algorithm manifest advantages domain zero sum multiagent sg environment explicit drawback algorithm slow learn episode state linear programming needed 
linear programming significantly increases computation cost system reaches convergence 
nash learning algorithm hu wellman extended game framework littman games developed nash learning gorithm multiagent reinforcement learning 
extend learning multiagent learning domain joint actions participating agents merely individual actions needed take account 
considering important difference single agent multiagent reinforcement learning nash learning algorithm needs maintain values learner players 
idea find nash equilibria state order obtain nash equilibrium policies value updating 
apply nash learning algorithm define nash value 
nash value defined expected sum discounted rewards agents follow specified nash equilibrium strategies period 
definitions directly come un nash learning algorithm 
definition nash function nash ith agent defined sum current reward plus rewards agents follow joint nash equilibrium strategy ri vi joint nash equilibrium strategy ri period reward ith agent state joint action vi total discounted reward infinite periods starting state agents follow equilibrium strategies probability distribution state transitions joint action 
definition nash joint strategy joint strategy constitutes nash equilibrium stage games mn kmk kmk ak product strategies agents previous preliminaries nash learning algorithm summarized table iii 
hu wellman quadratic programming find nash equilibrium nash learning algorithm general sum games 
hu wellman shown nash learning algorithm multi player environment converges nash equilibrium policies probability conditions additional assumptions payoff structures 
formally main results summarized theorem theorem multiagent sg environment agent nash learning algorithm converge optimal function probability long functions encountered coordination equilibria update rule 
furthermore agent glie policy converge behaviour probability limit equilibrium unique 
guarantee convergence nash learning algorithm needs know nash equilibrium unique value 
littman argued applicability theorem pointed hard apply strict conditions difficult verify advance 
tackle difficulty littman proposed called friend foe qlearning ffq algorithm introduced subsection 
friend foe learning ffq algorithm motivated conditions theorem convergence nash learning littman developed friend foe learning ffq algorithm rl general sum sgs 
main idea agent system identified friend foe 
equilibria classified coordination adversarial 
compared nash learning ffq learning provide stronger convergence guarantee 
littman results prove convergence ffq learning algorithm theorem foe learns values nash equilibrium policy adversarial equilibrium friend learns values nash equilibrium policy game coordination equilibrium 
true regardless opponent behaviour 
theorem foe learns function corresponding policy achieve learned values regardless policy selected opponent 
convergence property improved nash learning algorithm complete treatment general initialize table iii nash learning algorithm give initial state 
take ith agent learning agent 
ai ai 
repeat choose action observe rt rt st update 
defined sum stochastic games friend foe concepts lacking 
comparison nash learning algorithm ffq learning require learning estimates functions opponents 
require strong condition application agent know equilibria game equilibrium known coordinating adversarial advance 
ffq learning provide way find nash equilibrium identify nash equilibrium coordination adversarial 
nash learning apply system coordination adversarial equilibrium exists 
rq learning algorithm morales developed called rq learning algorithm dealing large search space problem 
algorithm state set need defined advance 
state defined set order relations goal front team robot left opponent robot ball action described set pre conditions generalized action possibly set post conditions 
action defined properly condition satisfied action applicable particular instance state applicable instances state 
rq learning algorithm reduce size search space process table iv 
rq learning algorithm useful dealing large search space problem may difficult define state action set properly particularly case incomplete knowledge concerned furthermore state space guarantee defined actions adequate find optimal sequence primitive actions suboptimal policies produced 
table iv rq learning algorithm initialize state set action set arbitrarily 
repeat fictitious play algorithm initialize state rels evaluating set relations state repeat step episode choose persistently exciting policy ii choose action randomly iii apply action observe iv update rels max terminal 
nash equilibrium learning difficulty finding nash equilibria fictitious play may provide method deal multiagent reinforcement learning sg framework 
fictitious play algorithm beliefs players policies represented empirical distribution past play 
players need maintain values related joint actions weighted belief distribution players actions 
table shows fictitious play algorithm zero sum sgs model 
stationary policies players fictitious play algorithm variants individual learning 
non stationary policies players fictitious play approaches empirically competitive games players model adversarial opponents called opponent modelling collaborative games players learn values joint actions player called joint action learner jal 
fictitious play approaches algorithms converge nash equilibrium games iterated dominance solvable players playing fictitious play 
fictitious play learning eliminate necessity finding equilibria learning agents model learning convergence depend heuristic rules 
multiagent sarsa learning algorithm minimax nash learning algorithms policy rl replace max operator individual learning algorithm best response nash equilibrium policy 
rl policy learning algorithm tries converge optimal values optimal policy regardless policy currently executed 
policy learning algorithms greedy table fictitious play algorithm initialize qi ai ai 
repeat state ai arg max ai ai qi ai 
update qi ai ai qi qi 
policies usually balance exploration exploitation learning space 
sarsa algorithm policy rl algorithm tries converge optimal values policy currently executed 
considering disadvantages minimax nash learning algorithms sarsa multi agent algorithm called extended optimal response learning developed 
fact opponents may take stationary policies taken account nash equilibrium policies 
opponents take stationary policies need finding nash equilibria learning 
learning updating simplified eliminating necessity finding nash equilibria opponents take stationary policies 
addition heuristic rules employed switch algorithm learning fictitious learning 
algorithm depicted table vi 
basic idea algorithm agent learn policy optimal response opponent policy tries reach nash equilibrium opponent adaptable 
nash leaning algorithm max ai ai qi ai st gorithm difficulty exist multiple equilibria 
obvious shortcoming algorithm agent assumed capable observing opponent action rewards 
cases serious restriction agents may learn strategies simultaneously agent obtain actions opponent advance 
opponent may take stochastic strategy deterministic policies 
observing rewards obtained opponent difficult rewards available policies put action practically 
empirical results algorithm lacks theoretic foundation 
complete proof convergence expected 
theory results provided may helpful obtain convergence properties algorithm 
policy hill climbing phc algorithm phc algorithm updates values way fictitious play algorithm maintains mixed policy stochastic policy performing hill climbing space mixed policies 
bowling veloso proposed wolf phc algorithm adopting idea win learn fast table vi algorithm multiagent version sarsa algorithm initialize repeat state choose action suitable exploration 
observe rewards action taken opponent state update value functions qi qi qi update estimate opponent policy vector follows update policy maximize os defined os tuning parameter max wolf variable learning rate 
full algorithm agent shown table vii 
wolf principle result agent learning quickly doing poorly cautiously performing 
change way learning rates helpful convergence overfitting agents changing policies 
point wolf phc algorithm attractive 
examples mgs zero sum general sum sgs complete proof convergence properties provided far 
rigorously speaking wolf phc algorithm multiagent version phc algorithm learning factors agents non markovian environment taken account 
rational reasonable agents playing stationary strategies 
addition convergence may slow wolf principle applied 
table vii wolf phc algorithm agent take learning rates 
initialize ai repeat state ai choose action state mixed strategy suitable exploration 
observe reward new state update value function ai max ai update estimate average policy ai step closer optimal policy sa sa arg max sa sa sa min ai ai ai terminal state 
algorithms sen studied multiagent coordination learning classifier systems 
action policies mapping perceptions actions multiple agents learn coordination strategies relying shared information 
experimental results provided indicated classifier systems effective widely learning scheme multiagent coordination 
multiagent systems learning agent may learn faster establish new rules utility unseen situations experiences knowledge agents available 
considering fact possible benefits gained extracting proper rules agents knowledge weighted strategy sharing wss method proposed coordination learning rl 
method agent measures agent team assigns weight knowledge learns accordingly 
table cooperative agent changed randomly 
tackling problem coordination multiagent systems boutilier proposed method solving sequential multiagent decision problems allowing agents reason explicitly specific coordination mechanisms 
method extension value iteration state space system augmented state adopted coordination mechanism needs defined 
method allows agents reason short long term prospects coordination decisions engage avoid coordination problems expected value 
bayesian approach coordination multiagent rl proposed 
method requires restrictive assumption learning agent ability observe actions states rewards agents rational coordination multiagent systems 
advantage method need find equilibria obtaining best response policy minimax nash learning algorithms 
scaling reinforcement learning multi robot systems multi robot learning challenge learning act non markovian environment contains robots 
robots mrss interact adapt environment learn adapt counterparts stationary policies 
tasks arising mrss continuous state action spaces 
result difficulties directly applying aforementioned results multiagent rl finite states actions mrss 
state action abstraction approaches claim extracting features large learning space effective 
approaches include condition behaviour extraction teammate internal modelling relationship state estimation state vector quantisation 
approaches viewed variants individual learning algorithms modelled robots parts environment stationary policy holders 
research scaling reinforcement learning robocup soccer reported stone sutton :10.1.1.1.8859
robocup soccer viewed special class test bed developing ai techniques multiagent systems 
challenging issues mrss appear robocup soccer large state action space uncertainties approach episodic smdp sarsa linear tile coding function approximation variable designed learn higher level decisions subtask soccer :10.1.1.1.8859
general theory rl function approximation understood linear sarsa best understood current methods scaling reinforcement learning robocup soccer :10.1.1.1.8859
claimed advantages policy methods learning unstable linear kinds function approximation 
answer open question sarsa fails converge 
study cooperation problems learning behaviours rl subtask robocup soccer keep away investigated combining sarsa linear tile coding function approximation 
single agent rl techniques including sarsa eligibility traces tile coding function approximation directly applied multiagent domain 
pointed previously straightforward applications single agent rl techniques multiagent systems sound theoretic foundation 
hu kanerva coding technique produce decision making module possession football robocup soccer 
application kanerva coding generalisation method form feature vector raw sensory reading rl uses feature vector learn optimal policy 
results provided demonstrated learning approach outperformed number benchmark policies including hand coded lacked theoretic analysis series single agent rl techniques domain multiagent systems 
formulation rl enables learning concurrent multi robot domain 
methodology adopted study behaviours conditions minimize learning space 
credit assignment problem dealt shaped reinforcement form heterogeneous reinforcement functions progress estimators 
morales proposed approach rl robotics relational representation 
relational representation method applied large search spaces domain knowledge incorporated 
main idea approach represent states sets properties characterize particular state may common states 
states actions represented terms order relations proposed framework policies learned generalized representation 
order deal state space growing exponentially number team members studied robot awareness cooperative mobile robot learning proposed method requires cooperative mechanism various levels awareness communication 
results illustrated applications cooperative multi robot observation multiple moving targets shows better performance purely collective learned behaviour 
variety methods reviewed demonstrate learning multi robot domain 
study behaviours thought underlying control representation handling scaling learning policies models learning agents 
proposed pessimistic algorithm distributed lazy qlearning cooperative mobile robots 
pessimistic algorithm compute lower bound utility executing action situation robot team 
learning lazy learning author neglected important fact applicability learning multi agent systems environment stationary 
park studied modular learning multi agent cooperation robot soccer modular learning assign proper action agent multiagent systems 
approach architecture modular learning consists learning modules mediator module 
function mediator select proper action learning agent value obtained learning module 
variety rl techniques developed multiagent learning systems techniques scale mrss 
hand theory multiagent rl systems finite discrete domains underway wellestablished 
hand essentially difficult solve mrss general case continuous large state space action space 
vi 
fuzzy logic systems multiagent reinforcement learning fuzzy logic controllers generalize learning continuous state spaces 
combination learning proposed fuzzy learning single robot applications 
modular fuzzy cooperative algorithm multiagent systems advantage modular architecture internal model agent fuzzy logic multiagent systems 
algorithm internal model estimate agent action evaluate agents actions 
overcome problem huge dimension state space fuzzy logic map input fuzzy sets representing state space learning module output fuzzy sets denoting action space 
fuzzy rule base learning module built learning providing convergence proof 
developed minimax fuzzy learning cooperative multi agent systems 
method learning agent need observe actions agents take uses minimax learning update fuzzy values fuzzy state fuzzy goal representation 
noted minimax learning sense fuzzy operators max min totally different minimax learning littman 
similarly proof guarantee optimal convergence minimax fuzzy learning 
fuzzy game theoretic approach multiagent coordination considering utility values usually approximate differences utility values somewhat vague 
fuzzy game theoretic approach may useful utility values 
establishing framework fuzzy game series notions including fuzzy dominant relations fuzzy nash equilibrium fuzzy policies defined fuzzy logic theory game theory 
shown fuzzy strategy outperform mixed strategy traditional game theory tackling cases multiple equilibria challenging issue game theory 
study merely focused stage policy fuzzy game rl 
combination fuzzy game theoretic approach popular rl techniques quite possible multi agent robot systems 
convergence proof appears difficult particularly multiagent reinforcement learning fuzzy generalizations 
convergence proof single agent fuzzy reinforcement learning frl provided 
find example reflect theoretic study 
obvious fact triangular membership functions experiment gaussian membership functions basis theoretic 
find proving techniques outcomes difficult extend domains multiagent reinforcement learning fuzzy logic general 
furthermore watkins pointed learning may converge correctly representations look table representation function 
vii 
main challenges mrss challenges multiagent learning systems continuous state action spaces uncertainties nonstationary environment 
aforementioned algorithms section iv require enumeration states policies value functions major limitation scaling established multiagent reinforcement learning outcomes sgs studied multiagent rl simple agent background players execute perfect actions observe complete states partially observed full knowledge players actions states rewards 
true mrss 
unfeasible robots completely obtain players information especially competitive games opponents actively broadcast information share players 
addition adversarial opponents may act rationally 
accordingly difficult find nash equilibria nash equilibrium approaches model dynamics approaches 
account state art multiagent learning system particular difficulty scaling established partially recognized multiagent rl algorithms minimax leaning nash learning mrss large continuous state action spaces 
hand theoretic works multiagent systems merely focus domains small finite state action sets 
hand lacking sound theoretic grounds guide scaling multiagent rl algorithms mrss 
result learning performance convergence efficiency stability guaranteed approximation generalization techniques applied 
important fact multiagent rl algorithms minimax leaning nash learning value function iteration method 
applying technique continuous system value function approximated discretization general approximators neural networks polynomial functions fuzzy logic 
researchers pointed combination dp methods function approximators may produce unstable divergent results applied simple problems see 
viii 
research directions coordination games teams cooperation due aforementioned difficulties possible opportunity rl mrss learn robot coordination 
robots team may learn share learned experience accelerate learning processes limited physical communication observation abilities 
robots common interests identical payoffs 
games characteristics referred ordination games 
littman proved convergence learning coordination games 
hard apply practice conditions difficult verify advance games may contain adversarial equilibria ordination equilibria simultaneously idea adopted 
special case team game players values 
littman showed conditions easily satisfied team games mdp conditions 
phenomenon multiple equilibria coordination games questions practical values remains challenge issue 
ongoing research select multiple nash equilibria 
furthermore exact values hard maintain physical robot sensory executive uncertainties 
research needs performed gain clear understanding learning convergence coordination games 
state action abstraction mrss incomplete information large learning space uncertainty major obstacles learning mrss 
learning behaviour robotics effectively reduce search space size dimension handle uncertainties locally 
action space transformed continuous space control inputs limited discrete sets 
convergence proof algorithms state action abstraction mrss challenging problem 
advances state action abstraction mrss completely satisfactory solutions cope continuous state action spaces occurring domains mrss example see 
generalization approximation state action spaces system small finite discrete lookup table method generally feasible 
mrss state action spaces huge continuous lookup table method inappropriate 
solve problem state action abstraction function approximation generalization appears feasible solution 
learning partially observable nonstationary environment area multiagent systems multiagent domain independent coordination mechanisms perceptual coordination mechanism observing coordination mechanism 
advantage approach multiple agents require explicit communication learn coordinated behaviors 
cope huge state space function approximation generalization techniques 
unfortunately proof convergence function approximation generalization techniques provided 
currently generic theoretic framework proving optimal convergence function approximation implementation popular rl algorithms qlearning established 
interestingly increasing effort direction single agent multi agent systems 
single agent temporal difference learning linear function approximation studied convergence analyzed asymptotic properties 
mild conditions sure convergence temporal difference learning linear function approximation upper error bound determined 
continuous reinforcement learning difficulties extending rl discrete finite domains continuous learning systems great deal efforts directly developing continuous rl techniques complex continuous systems 
theoretic framework viscosity solutions conducted study rl continuous state space time control problems 
continuous case value function satisfy hamilton jacobi bellman hjb equation nonlinear second order equation 
known solving hjb equation hard task 
solve hjb equation powerful framework viscosity solutions showed unique value function solution hjb equation sense viscosity solutions 
continuous learning method incremental topology preserving map partition input space tion bias initialize learning process 
resulting continuous action average discrete actions winning unit weighted values 
interesting author showed experimental results robotics indicating continuous learning method works better standard discrete action version learning terms asymptotic performance learning speed 
continuous qlearning method focus single agent systems 
version continuous learning method multiagent systems expected accordingly 
ongoing research solving continuous cases continuous rl feedback control systems 
continuous rl algorithm developed applied control problem involving refinement proportional integral pi controller 
interestingly tu claimed results continuous rl algorithm outperforms discrete rl algorithms 
continuous learning studied rl methodology control real systems 
linear quadratic regularization lqr techniques learning combined linear nonlinear continuous control systems 
ix 
related survey field rl kaelbling computer science perspective 
central issues rl including trading exploration exploitation learning delayed reinforcement learning making generalization discussed investigation 
stone veloso survey systems machine learning perspective series general multiagent scenarios 
investigation focus exclusively robotic systems robotic soccer test bed robotic multiagent systems discussed 
robot soccer perspective kim gave survey mul systems cooperative robotics 
cooperative robotics related issues group architecture resource conflict geometric problems involved 
theoretical research advances mentioned 
technical report sgs multiple learning players multiagent rl systems 
survey scaling multiagent rl mrss 
shoham powers critical survey ai multiagent rl 
argued learning sgs flawed lot research field frameworks sgs 
fundamental flaw thought problem problems addressed 
questioned focus equilibria 
commented results concerning convergence nash learning quite awkward 
particularly multiple optimal equilibria exist agents need oracle coordinate choices order converge nash equilibrium begs question learning coordination 
negative comments current rl sgs agreed results unproblematic cases zero sum sgs team pure coordination games common payoff functions 
concluding remarks growing interests scaling multiagent rl mrss 
rl option learning multiagent systems continuous state action spaces hamper applicability mrss 
fuzzy logic methodology candidate dealing approximation generalization issues rl multiagent systems 
scaling approach remains open 
particularly lack theoretical grounds proving convergence predicting performance fuzzy logic multiagent rl fuzzy multiagent learning 
cooperative robots systems research outcomes special cases available difficulties multiple equilibrium selecting payoff structure directly applying practical robotic soccer system 
gave survey multiagent rl mrss 
main objective review important advances field completely 
challenging problems promising research directions provided discussed 
provide complete exhaustive survey multiagent rl mrss believe help clearly understand existing works challenging issues ongoing research field 
acknowledgment research funded engineering physical sciences research council epsrc gr 
cao fukunaga kahng cooperative mobile robotics antecedents directions autonomous robots vol 
pp 

matari reinforcement learning multi robot domain autonomous robots vol 
pp 

matari learning history behavior mobile robots non stationary conditions autonomous robots vol 
pp 

balch arkin behavior formation control multirobot teams ieee transactions robotics automation vol 
pp 
december 
asada hosoda operative behaviour acquisition mobile robots dynamically changing real worlds vision reinforcement learning development artificial intelligence vol 
pp 

wiering schmidhuber reinforcement learning soccer teams incomplete world models autonomous robots vol 
pp 

moreira lima cooperative learning planning multiple robots 
online 
available citeseer nj nec com html robot awareness cooperative mobile robot learning autonomous robots vol 
pp 

fernandez parker learning large operative multi robot domains international journal robotics automation vol 
pp 

liu wu multi agent robotic systems 
crc press 
matari learning behavior multi robot systems policies models agents journal cognitive systems research vol 
pp 

bowling veloso simultaneous adversarial multirobot learning proceedings eighteenth international joint conference artificial intelligence august 
xi design analysis internet tele coordinated multi robot systems autonomous robots vol 
pp 

nardi distributed coordination heterogeneous multi robot systems autonomous robots vol 
pp 

mataric sukhatme multirobot task allocation uncertain environments autonomous robots vol 
pp 

distributed lazy learning cooperative mobile robots international journal advanced robotic systems vol 
pp 

fujii arai endo multilayered reinforcement learning complicated collision avoidance problems proceeding ieee international conference robotics automation pp 

wiering schmidhuber learning team strategies soccer case studies machine learning vol 
pp 

sen multiagent systems milestones new horizons online 
available citeseer ist psu edu html wei dillenbourg multi multiagent learning ch 
collaborative learning cognitive 
online 
available tum de docs weiss dillenbourg pdf 
kim multi agent systems survey robot soccer perspective international journal intelligent automation soft computing vol 
pp 

hu rl kanerva generalisation reinforcement learning possession football proceedings ieee rsj international conference intelligent robots systems hawaii 

park 
kim 
kim modular learning multi agent cooperation robot soccer robotics autonomous systems vol 
pp 

riedmiller reinforcement learning approach robotic soccer robocup pp 

maes manderick reinforcement learning large state spaces simulated robotic soccer testbed 
online 
available citeseer ist psu edu 
html smart kaelbling effective reinforcement learning mobile robots proceedings ieee international conference robotics automation 
online 
available www ai mit edu people papers icra pdf babu reinforcement learning soccer problem 
online 
available www cis 
ksu edu babu final html htm reinforcement learning multiagent systems 
online 
available www cs mcgill ca rl mas pdf watkins dayan learning machine learning vol 
pp 

harmon harmon reinforcement learning tutorial 
online 
available citeseer ist psu edu harmon reinforcement html sutton barto reinforcement learning 
cambridge massachusetts mit press 
tan multi agent reinforcement learning independent vs cooperative agents proceedings tenth international conference machine learning amherst ma pp 

littman markov games framework multiagent learning proceedings eleventh international conference machine learning san francisco california pp 

littman ri generalized model convergence applications proceedings thirteenth international conference machine learning bari italy july pp 

ri littman generalized markov decision processes dynamic programming algorithms department computer science brown university technical report cs november 
online 
available www cs duke edu docs html claus boutilier dynamics reinforcement learning multiagent systems proceedings fifteenth national conference artificial intelligence madison wi pp 

hu wellman multiagent reinforcement learning stochastic games 
online 
available citeseer ist psu edu hu multiagent html sun qi rationality assumptions optimality learning design applications intelligent agents third pacific rim international workshop multi agents prima ser 
lecture notes computer science zhang 
soo eds vol 

springer pp 

ri littman unified analysis value function reinforcement learning algorithms neural computing vol 
pp 

stone veloso multiagent systems survey machine learning perspective autonomous robots vol :10.1.1.1.8859
pp 

hu wellman learning agents dynamic multiagent system journal cognitive systems research vol 
pp 

littman value function reinforcement learning markov games journal cognitive systems research vol 
pp 

littman stone leading best response strategies repeated games th annual international joint conference artificial intelligence workshop economic agents models mechanism 
littman friend foe learning general sum games proceedings th international conference machine learning morgan kaufman pp 

dahl lagging anchor algorithm reinforcement learning player zero sum games imperfect information machine learning vol 
pp 

cooperative learning ieee transactions systems man cybernetics part cybernetics vol 
pp 
february 
multiagent reinforcement learning stochastic games multiple learning players department computer science univeristy toronto technical report march 
online 
available www cs toronto edu ps hayashi multiagent reinforcement learning algorithm extended optimal response proceedings international joint conference autonomous agents multiagent systems bologna italy july pp 

boutilier multiagent reinforcement learning theoretical framework algorithm second international joint conference autonomous agents multiagent systems aamas melbourne australia july pp 

hu wellman nash learning general sum stochastic games journal machine learning research vol 
pp 

singh jaakkola littman convergence results single step policy reinforcement learning algorithms machine learning vol 
pp 

basar dynamic noncooperative game theory 
london academic press 
banerjee peng adaptive policy gradient multiagent learning proceedings second international joint conference autonomous agents multiagent systems 
acm press pp 

fudenberg tirole game theory 
london mit press 
boutilier sequential optimality coordination multi agent systems proceedings sixteenth international joint conference artificial intelligence pp 

lagoudakis parr value function approximation zero sum markov games 
online 
available www cs duke edu papers pdf uai pdf li refining basis functions square approximation zero sum markov games 
online 
available www org research li basis pdf shoham powers multi agent reinforcement learning critical survey 
online 
available www stanford edu pdf hu wellman multiagent reinforcement learning theoretical framework algorithm proceedings fifteenth international conference machine learning san francisco california pp :10.1.1.138.2589

bowling multiagent learning presence agents limitations ph dissertation school computer science carnegie mellon university pittsburgh pa may cmu cs 
bowling veloso multiagent learning variable learning rate artificial intelligence vol 
pp 

online 
available citeseer ist psu edu bowling multiagent html banerjee peng convergent gradient ascent general sum games proceedings th european conference machine learning august pp 

sutton mcallester singh mansour policy gradient methods reinforcement learning function approximation advances neural information systems 
mit press pp 

morales scaling reinforcement learning relational representation workshop adaptability multi agent systems robocup australian open sydney australia january 
fudenberg levine theory learning games 
cambridge massachusetts mit press 
sen multiagent coordination learning classifier systems proceedings ijcai workshop adaption learning multi agent systems wei sen eds vol 

springer verlag pp 

online 
available citeseer ist psu edu sen multiagent html cooperation coordination fuzzy reinforcement learning agents partially observable markov decision processes proceedings th ieee international conference systems fuzz ieee 
advantages cooperation reinforcement learning agents difficult stochastic problems proceedings th ieee international conference systems fuzz ieee 
generalized markov decision processes dynamic programming reinforcement learning algorithms intelligent inference systems sunnyvale ca technical report iis october 
online 
available www com projects multi agent tech rep iis pdf modular fuzzy cooperative algorithm multi agent systems advances information systems second international conference turkey october proceedings ser 
lecture notes computer science ed vol 

springer pp 

minimax fuzzy learning cooperative multi agent systems advances information systems second international conference turkey october proceedings ser 
lecture notes computer science ed vol 

springer pp 


wu 
soo fuzzy game theoretic approach multi agent coordination multiagent platforms pacific rim international workshop multi agents prima singapore november selected papers ser 
lecture notes computer science ishida ed vol 

springer pp 

convergent frl algorithm application power management wireless transmitters ieee transactions fuzzy systems vol 
pp 
august 
study reinforcement learning continuous case means viscosity solutions machine learning vol 
pp 

multiagent reinforcement learning function approximation ieee transactions systems man cybernetics part application reviews vol 
pp 
november 
convergence temporal difference learning linear function approximation machine learning vol 
pp 

del mill learning machine learning vol 

tu continuous reinforcement learning feedback control systems master thesis computer science department colorado state university fort collins colorado 
online 
available www engr colostate 
edu papers tu ps gz hagen continuous state space learning control nonlinear systems ph dissertation faculty science ias university amsterdam kruislaan sj amsterdam 
kaelbling littman moore reinforcement learning survey journal artificial intelligence research vol 
pp 

