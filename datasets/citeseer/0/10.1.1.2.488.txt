pointing command gestures augmented reality thomas moeslund st erik granum computer vision media technology laboratory aalborg university niels vej dk aalborg east denmark tbm mst aau dk wearable computing augmented reality applications call obtrusive intuitive human computer interfaces keyboards mice 
way realize interfaces gestures pointing order replace mouse 
obtrusive way gesture recognition computer vision methods 
computer vision gesture interface part augmented reality system 
recognize pointing gesture click gesture static command gestures 
optimized lookup table color segmentation fast gesture recognition method enable real time performance hz standard pc 

decade idea having computers located places desktops widely accepted terms pervasive computing ubiquitous computing part vocabulary researchers 
aspect development computer worn human commonly referred wearable computing 
things notion wearable computing covers idea worn computer enhance human visual sensors eyes augmenting artificially generated information visual input 
currently done having user wear hmd head mounted display showing real world overlay graphics hmd 
principle known ar augmented reality number great applications 
example surgeon operating patient primary bio parameters patient overlaid see hmd images recorded yesterday may overlaid proper voltage current values overlaid wires currently looking 
application example requires knowledge viewpoint hmd order align overlaid data correctly 
requirement ar applications currently common approach computer vision input hmc head mounted cameras mounted hmd 
hmc hmd pho 
user doing pointing gesture wearing hmd hmc 
requirement kind interface ar system generating graphics add remove overlaid graphics 
different approaches tried far interface constructed 
evident hand gestures part interface hand provide deictic information easy precise manner modern wimp interfaces proven necessity 
hmc gesture interface ar applications 
wimp windows icons menu pointer 
widely graphical user interface desktop computers hand pda 

related recognition gestures consists steps capturing motion configuration pose fingers hands arms depending level detail required hand classify captured data belonging predefined gesture classes 
number different devices applied order capture data magnetic devices accelerometers bend sensors general capturing performed system see optical system 
due non intrusive nature focus gesture research including 
optical gesture recognition systems may divided categories model appearance 
model system geometric model hand created 
model matched preprocessed image data order determine state tracked hand 
model elaborate model dof degrees freedom digiteyes system cardboard model contour model hand seen straight 
continuously fitting model hand video frames process tracking complete state hand just position 
process consequently called state tracking 
model contains sufficient number internal dof recognition static gestures reduced inspection state 
appearance approaches recognition pixel representation learned training images 
explicit model hand exists internal dof specifically modeled 
means differentiating gestures straight forward model approach 
gesture recognition typically involve sort statistical classifier set features represent hand 
pixel representation hand quite comprehensive representation principal component analysis applied see 
approaches require relative high computational complexity undesirable ar interface close real time performance low computational complexity required due systems driven wearable computer graphics hmd dof localization head respect world 
extent overcome introducing signal noise enhancements markers fingers hands infrared lighting 
additional information regarding previous gesture recognition systems computer vision general ar particular see 
context gesture recognition system part ar project goal develop multi user ar system round table meetings architects see additional information 
ar system interface build pho placeholder objects pointing devices gestures 
pho physical objects located table tracked position orientation table plane dof pointers tracked dof 
moving pho different parameters system may controlled position scale virtual object 
pho pointer gestures recognized color computer vision 

focus aim develop gesture recognition system contains sufficient number different gestures order useful interface low computational complexity allows system run concurrent ar components loosing sense real time interaction interface 
achieved introducing enhancements markers infrared lighting 
particular show computational simple reliable gesture recognition system designed test concrete ar interface 
structured follows gestures recognized defined section 
section segmentation images described section explains gesture recognition 
section system performance section 

defining gestures ar interface designers believe gesture interface minimum requires pointing click gesture 
primary argument number pc applications interaction done gestures 
order avoid lots pop menus kinds interactions want interface include command gestures order provide shortcuts 
furthermore easy user remember perform gestures 
thinking terms pointing gesture natural way performing index finger 
requirements command gestures combined pointing gesture idea defining additional gestures associating number fingers 
idea definition gesture set illustrated easy remember set gestures provides rich vocabulary create interesting useful interfaces 

gestures applied 
technical point view set gestures recognizable gestures may distinguished plane 
ask users perform gestures plane recognition problem reduced easier problem solve 
hard limitation system gestures related plane different users quickly adapted constraint 
furthermore recognition method section sensitive small rotations rotational axes hand usually seen recognition systems 
words soft constraint 
recognition method relies pre segmented image hand described section 

segmentation task low level segmentation detect recognize mentioned pho pointers hands images captured hmc shows example input image 
camera objects may move respect projection objects may vary considerably size form 
furthermore form hand changes depending gesture 
order achieve invariance changing size form objects detected color pixel approach segment blobs similar color image 
compared gray tone methods color advantage providing extra dimensions objects similar gray tones different color space 
problem color feature color appearance objects depends illumination objects exposed 
illumination changes may divided intensity changes color changes 
intensity changes 
hmc input image showing pho pointer gesture 
skin color segmented image 
may due light source due changing illumination geometry distance light source 
illumination color changes due different spectral composition light sources daylight fluorescent light light 
approach achieve invariance changing intensities transform rgb colors color space separates intensity color information 
color spaces hsv normalized rgb 
problem changing illumination color complex intensity changes solved simple transformation 
assumed mainly intensity changes small changes color illumination 
normalized rgb called achieve invariance intensity calculated dividing rgb elements norm mapping space plane 
shows rg distributions different colors camera input image 
distributions modeled unimodal gaussian mean values covariance matrices estimated done initialization step 
mean values covariance matrices calculate confidence ellipses confidence ellipses shown 
segmenting image tested rg chromaticity pixel confidence ellipses mahalanobis distance 
recursive region growing find connected areas image features center mass bounding boxes calculated 
segmentation result ellipse skin color shown seen chromaticity background skin chromaticity 
chromaticity plane showing components colors detected 
colors ellipses labeled respective object color 
distributions ellipse measured chromaticity distributions hand object colors 
hand objects detected skin color 
defined skin blob certain minimum maximum number pixels 
pixels hand detected skin may due camera noise 
removed opening morphology filter 
final segmentation result image seen ignore concentric circles 

implementation issues implementing method issues considered 
cameras limited dynamic intensity range 
low rgb camera responses usually high level noise calculation unreliable 
pixels minimum intensity imin imin 
bit rgb output bit channel imin 
high rgb camera responses elements pixel color information distorted 
checked channel bit 
pixels element ignored 
cameras non linear intensity response high rgb outputs maximum may set lower value 
achieving invariance intensity changes transformation rgb rg assumes linear relationship intensity camera response 
furthermore noted gamma correction set automatic white balance disabled 
explained method includes pixel multiplications divisions relational operators greater summations calculate 
furthermore mahalanobis distances need calculated number different colors detected example 
order reduce required processing power pre calculate rgb lookup table lut initialization step 
rg chromaticity ellipses cones rgb space truncated camera dynamic range minimum intensity shown colors 
triangle rg plane spanned axis dash line chromaticity plane confidence ellipses shown 

rgb lut example 
solid cube encloses lut equal camera dynamic range 
cones representing object color 
dashed part cones clipped ignored pixels 
rgb camera bit color depth channel requires bytes mb 
calculation pixel reduced simple lookup 
required processing reduced scanning image objects steps pixels pixel 
region growing pixel tested 

gesture recognition having segmented hand pixels image task find gesture performed 
done described section 
approach divided steps corresponding different algorithms detects number fingers handles point click gestures 
algorithms developed computational complexity mind 

count number fingers seen hand fingers approximated circle number rectangles number equal number fingers 
observation follows simple approach counting number fingers polar transformation center hand count number fingers rectangles radius 
gestures performed hand pointing upwards interval investigated 
polar transformation upper half shown polar transformation upper half shown 
seen different radii give clear indication number fingers 
order speed algorithm sample segmented image concentric circles doing polar transformation 
step size consecutive circles set accordingly 

polar transformations upper half hand image respectively 
implementation polar transformation 
see text details 
center hand may distance transform setups turned arm placed hand entered field view 
allows center mass faster calculate compared distance transform 
smallest radius non skin pixel denoted rmin greatest radius skin pixel denoted rmax 
fist gesture corresponds situation rmin rmax ratio width height bounding box close 
fist gesture number fingers counted radii searching connected skin pixels 
number connected skin pixels upper lower threshold order accepted originating finger 
thresholds depend distance camera size hand 
gestures recognized hmc distance camera constant 
size users hands vary requires initialization thresholds critical initialization avoided 
final classification carried finding number fingers consecutive radii concentric circles 
algorithm contain information regarding relative distances fingers 
reason twofold 
firstly system general secondly experienced different users tend different preferences depending individual kinematics limits hands fingers 
algorithm robust different gestures performed 
example finger gesture performed ring finger middle finger index finger 
fact gesture performed number different ways see table 
table 
number different ways gesture performed 
gesture configurations step filter recognized gestures temporal filter basically states order recognized gesture accepted recognized number consecutive frames 

recognize point click gestures algorithm finds finger interpret pointing gesture 
tip pointing finger defined actual position user pointing 
point follows consecutive radii classify pointing gesture center finger radii values fitted straight line 
search line reach final point finger tip 
order point pointing gesture recognized opposed gestures 
pointing gesture recognized image run segmentation gesture recognition algorithms second hmc pointing gesture recognized second camera finger tip method camera position finger tip triangulation 
similar computer mouse need click interaction associated current position pointing gestures 
example click gesture indicate virtual object currently pointed selected 
natural define click gesture movement thumb 
focused computational complexity excluded number advanced methods condensation tracking 
experimented number ways measuring movement thumb difference images 
unfortunately turned sensitive motion hand difficult adjust furthermore sensitive left handed person system 
turned system contained sufficient information recognize click gesture 
definition click gesture detailed 
concretely defined movement static states 
third states thumb right index finger 
state thumb significant angle index finger 
movements states situations recognized changes width bounding box width bounding box grows shrinks 
sensitive approach general true 
apply recognition pointing gesture constant number consecutive frames stable 
furthermore height bounding box needs stable transitions states ensures change width originate scale hand 
note approach system independent left right handed users 

system performance gesture recognition implemented part computer vision system ar multi user application 
hmc single ccd color micro camera heads cv pal jai connected rgb picasso frame low level segmentation section robustly segment different colors background skin color lands calibrated 
jai dk imaging products colors pho pointers big changes illumination color 
shows screenshot segmentation result 

hmc input image hand gesture showing segmentation results 
system users including people technical background difficult give quantitative results gesture pointing recognition 
qualitatively stated users adapted quickly showing gestures plane perpendicular camera optical axis learned users gesture interface useful ar application recognition rate considered sufficient ar interface 
ghz pc entire computer vision system pho pointer gesture tracking runs hz hmc pal resolution camera input images contain objects hands 

gesture recognition part ar interface 
system performs hz able recognize pointing gesture clicking gesture command command gestures defined number fingers 
qualitative user tests showed recognition rate gestures robust ar interface 
tests validated quantitative tests 
research part funded arthur project european commissions ist program ist 
support gratefully acknowledged 
augmented round table architecture urban planning arthur 
eu ist rtd project 
www www fit fraunhofer de arthur 
birk moeslund madsen 
realtime recognition hand alphabet gestures principal component analysis 
th scandinavian conference image analysis finland 

extraction hand shape posture image sequences sign language recognition 
international workshop analysis modeling faces gestures nice france october 
low 
face detection survey 
computer vision image understanding sept 
isard blake 
condensation conditional density propagation visual tracking 
international journal computer vision pages 
kohler 
survey video gesture recognition stereo mono systems 
technical report research report nr 
fachbereich informatik university dortmund 

survey hand posture gesture recognition techniques technology 
technical report cs department oc computer science brown university providence rhode island 
lin wu huang 
capturing human hand motion image sequences 
workshop motion video computing orlando florida december 
maccormick isard 
partitioned sampling articulated objects interface quality hand tracking 
th european conf 
computer vision volume pages dublin ireland 
laaksonen 
behavior skin color varying illumination seen different cameras different color spaces 
hunt editor spie machine vision industrial inspection ix volume san jose california usa jan 
moeslund 
brief overview hand gestures wearable human computer interfaces 
technical report computer vision media technology lab aalborg university dk 
pavlovic sharma huang 
visual interpretation hand gestures human computer interaction review 
transactions pattern analysis machine intelligence 
thomas 
system demonstrating new techniques mobile augmented reality modelling 
third australasian conference user interfaces pages melbourne victoria australia 
rehg kanade 
digiteyes vision hand tracking human computer interaction 
workshop motion non rigid articulated bodies pages 
starner gandy 
gesture pendant self illuminating wearable infrared computer vision system home automation control medical monitoring 
international symposium wearable computing atlanta ga october 

finger tracking interaction augmented environments 
international symposium augmented reality new york new york october 
watson 
survey gesture recognition techniques 
technical report tcd cs department computer science trinity college dublin 
wu huang 
vision gesture recognition review 
editor international workshop number lnai 
springer 
