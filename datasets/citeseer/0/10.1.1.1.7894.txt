appear handbook brain science neural networks michael arbib ed 
exploration active learning sebastian thrun universitat bonn institut fur informatik iii 
bonn germany phone fax mail thrun carbon cs bonn edu research machine learning decades produced variety techniques automatically improve performance computer programs experience 
approaches machine learning roughly divided categories passive active making characteristic assumptions learner environment 
passive learning passive learning paradigm learner learns purely observing environment 
environment assumed generate stream training data unknown probability distribution 
passive learning techniques differ type results seek produce way generalize observations 
common learning tasks clustering classification prediction data 
passive learning techniques subdivided order free order sensitive approaches 
order free approaches rest assumption temporal order training data arrives matter task learned 
assumed training examples generated independently stationary probability distribution 
majority machine learning approaches falls category 
example unsupervised learning usually aims characterize underlying probability distribution cluster data 
supervised learning hand concerned approximating unknown target function conditional probability set observed input output examples 
passive learning studied order sensitive learning scenarios settings temporal order training data carries information relevant learning task 
case example consecutive training examples conditionally dependent sebastian thrun exploration active learning learning dependencies crucial success learner 
time series prediction speech recognition examples order sensitive learning domains 
active learning active learning paradigm differs passive learning paradigm learner pure observer learner ability interact environment 
specifically learner execute actions impact generation training data 
freedom execute actions imposes important challenge specific active learning actions shall learner generate learning 
learner efficiently explore environment 
active learning distinguish order free order sensitive cases 
order free active learning rests assumption observed environment depends executed action 
best studied approach kind learning queries angluin atlas baum lang query learning available actions queries values unknown target function 
environment provides immediate responses answers queries 
order sensitive approaches hand observations may depend actions 
example approaches learning optimal control airplane control game playing fall category 
describe long term dependencies actions observations frequently proven helpful assume environment possesses internal state information 
actions influence state environment state determines learner observes 
exploration refers process selecting actions active learning 
exploration techniques reviewed applicable active learning general primarily focus action selection issues order sensitive scenarios 
approaches listed originally applied order sensitive frameworks 
notice simplifying restrictive assumption state environment fully observable 
action selection strategies principles learner pick right action learning 
glance appropriate random action selection mechanisms generate actions whitehead random action selection frequently primarily reasons simple usually ensures possible finite sequence actions executed eventually 
shown theoretical analyses empirical findings sophisticated query exploration strategies drastically reduce number training examples required successful learning 
responses different actions typically carry different amounts information 
random sampling take full advantage sebastian thrun exploration active learning opportunity select informative query action 
intuitively speaking order learn efficiently execute actions informative learning task hand 
expects learn outcome action better greedy principle optimization knowledge gain employed approaches action selection active learning 
query selection research query learning led variety approaches active selection queries 
example atlas cohn approaches learning queries described neural network model learner uncertainty 
learning queries favored predictable outcome 
uncertainty estimated difference models constructed observations atlas analysis parameters estimator cohn approaches proven superior random sampling empirical comparisons 
kindermann propose method integrates external cost function active learning framework 
specifically approach favors queries minimize decision costs allows focus learning performance relevant areas 
approach computationally expensive relies explicit monte carlo integration 
exploration order sensitive scenarios exploring unknown parts environment requires sequences actions executed 
example lunar robot agent aims explore back side moon get getting require exploration 
techniques employ models expected knowledge gain direct explorative actions unknown parts environment called directed exploration techniques see thrun overview 
existing approaches share philosophy selecting actions maximizing knowledge gain differ particular way knowledge gain estimated 
estimate quantity implicitly specific data structure utilize explicit models represented separate data structures 
addition variety heuristic estimators estimating expected knowledge gain 
estimators typically related quantities frequency density recency empirical prediction errors 
example kaelbling suggests approach exploration actions favored repeatedly disadvantageous 
consequence actions selected exhibit performance unexplored 
similar approach line thought proposed koenig simmons approach bears close resemblance heuristic search techniques graphs korf differs assume availability model environment 
koenig simmons derive worst case bounds complexity exploration deterministic shortest path problems 
sutton called exploration bonus assigned actions 
bonus measures environment state elapsed time available action sebastian thrun exploration active learning executed 
consequence actions executed long time favored exploration 
sutton employs dynamic programming technique propagate exploration utility state space environment model environment easy obtain environments studied 
approach exploration proposed dayan sejnowski unpublished 
exploration achieved bayesian prior expresses uncertainty function action executed 
thrun approaches compared empirically combined approach frequency recency account 
approaches specific memory learning moore schaal atkeson memory learning memorizes training data explicitly 
approaches density previous data points asses utility actions exploration 
schaal takes account knowledge goal learning focus exploration 
focussing exploration active learning techniques estimate expected knowledge gain learner applicable action select actions maximizing knowledge gain 
order methodology efficiently assumptions heuristic estimating gain knowledge yield approximately correct action preferences gaining knowledge se helpful learning task 
assumptions necessarily fulfilled practice 
heuristics exploration somewhat ad hoc effectiveness varies environments learning tasks 
depending goal learner aims achieve parts environment known order perform optimally 
typically case example context reinforcement learning barto appear sutton watkins dayan reinforcement learning learning task generate control learn action policies maximize reward function 
exploring regions state space irrelevant task learning control waste time memory resources 
common strategy focus exploration explore exploit simultaneously knowledge gain task specific utility actions account 
boltzmann distributions semi uniform distributions provide ways combine random exploration exploitation 
distributions explore flipping coins likelihood individual actions determined task specific exploitation utility boltzmann distributions likelihood picking action exponentially weighted utility semi uniform distributions action largest utility distinct high probability executed 
notice aforementioned approaches originally proposed combination task specific exploitation 
thrun empirically demonstrated combination exploration exploitation yield faster learning component isolation 
fundamental dilemma choosing right ratio exploration task specific exploitation called exploration exploitation dilemma 
exploration sebastian thrun exploration active learning exploitation traded dynamically exploration fades time 
complexity results addition empirical studies theoretical results emphasize importance exploration active learning 
result whitehead shows random walk exploration require exponential learning time various cases shown directed exploration techniques reduce complexity active learning exponential training time random exploration polynomial training time thrun koenig simmons similar results exist query learning framework angluin baum lang results apply certain deterministic environments practice carry stochastic environments 
example section briefly describes artificial neural network approach exploration real valued domains 
approach shares ideas current literature expected knowledge gain estimated learning actions selected greedily maximize knowledge gain 
approaches operates real valued domains uses artificial neural networks estimate gain knowledge 
modeling competence assume learning task approximate unknown target function denotes input space denotes output space approximation denoted assume instance time learner execute actions denoted actions influence state environment impact training examples function approximator 
notice experiments reported product state space environment 
competence map function 
assesses accuracy trained follows 
observed input output example hi theta model error jj gamma jj 
competence map models error function training example produces training example illustrated fig 

competence map direct exploration selecting actions maximize specifically learner explores picking actions competence minimal internal models inaccurate 
actions assumed maximize gain knowledge 
noted competence estimates described may approximately correct dynamics estimators usually hard model 
addition due model limitations fails model environment sufficient detail unmodeled effects constant source model error perpetually provoke exploration 
case example function approximators artificial neural networks employed highly stochastic environments sebastian thrun exploration active learning prediction pred 
prediction input target environment error competence map model training model competence map 
empirical results illustrate exploration competence map consider environment depicted fig 

input learner position dimensional world 
task navigate starting position box goal position cross avoiding collisions walls obstacle 
actions denoted deltax deltay small displacements executed added current position 
agent reaches goal alternatively collides wall obstacle reset starting position 
addition coordinates learner able perceive potential function depicted fig 

potential measures distance goal obstacles steepest descent yields collision free path goal location arbitrary starting positions 
state transition function potential function initially unknown 
goal learning learn control strategy selection actions carry agent goal 
done learning state transition function potential function 
reasonable model functions identified pure hill climbing result admissible paths goal reached collision 
experiments multi layer network trained backpropagation algorithm model motion dynamics potential function values 
input network current position action deltax deltay 
trained predict position corresponding potential function value 
actual network consisted separate components predicting position hidden units predicting potential function value 
component consisted units radial bases activation functions hidden layer units activation function second layer 
competence modeled artificial neural network received input values model trained predict squared model prediction error ff pred gamma obs pred gamma obs pred gamma obs 
ff appropriate normalization constant ensures competence values lie 
actual implementation sebastian thrun exploration active learning potential function 
darkness indicates combined distance goal obstacle walls 
competence network hidden layers logistic units 
learning exploration exploitation combined selective attention mechanism described thrun traded exploration exploitation dynamically expected costs benefits 
experimental study approaches exploration compared random exploration pure exploitation best known path directed exploration competence 
pure exploitation get stuck fail explore exhaustively rare cases actions generated randomly 
learning steps technique learned linear motion dynamics 
produced different models potential function 
random exploration fig 
performed poorly 
resulting model accurate allow agent navigate goal 
actions generated pure exploitation reasonable path start goal 
approach yielded performance terms navigation 
model inaccurate world poorly explored easily seen fig 

best results terms control model accuracy directed exploration competence map 
competence map exploration resulted smaller number collisions learning yielding accurate model 
findings demonstrate advantage directed exploration techniques random exploration 
details may thrun sebastian thrun exploration active learning results 
random exploration 
exploitation possible 
exploration competence map 
active learning learner ability execute actions learning 
learner certain extent control stream training data 
key challenge active learning select actions optimize rate learning 
reviews discusses heuristic approaches selection actions active learning primarily literature neural networks reinforcement learning 
order illustrate ideas practice concrete exploration mechanisms artificial neural networks outlined 
approach exploration achieved estimating learner competence 
basic philosophy approaches selection actions estimate expected gain knowledge function actions executed 
learner picks actions maximize expected knowledge gain 
area passive order free learning studied extensively field machine learning statistics artificial neural networks considerably little effort spent exploration active learning issues 
attributed fact passive learning simpler learning tasks provide opportunity select actions 
natural learners animals humans learn actively capability act influence environment 
truly embedded environments act observe 
artificial agents need learn autonomously follow learning principles 
angluin queries concept learning machine learning 
atlas cohn ladner el marks park training connectionist networks queries selective sampling sebastian thrun exploration active learning advances neural information processing systems touretzky ed pp 
san mateo ca morgan kaufmann 
barto singh computational economics reinforcement learning connectionist models proceedings summer school touretzky elman sejnowski hinton eds pp 
san mateo ca morgan kaufmann 
barto bradtke singh appear learning act real time dynamic programming artificial intelligence 
baum lang constructing hidden units examples queries advances neural information processing systems lippmann moody touretzky eds pp 
san mateo morgan kaufmann 
cohn queries exploration optimal experiment design advances neural information processing systems cowan tesauro alspector eds san mateo ca morgan kaufmann 
kaelbling learning embedded systems 
mit press cambridge ma 
koenig simmons complexity analysis real time reinforcement learning proceeding eleventh national conference artificial intelligence aaai pp 
menlo park ca aaai aaai press mit press 
korf real time heuristic search new results proceedings sixth national conference artificial intelligence aaai pp 
los angeles ca computer science department university california aaai press mit press 
moore efficient memory learning robot control phd thesis trinity hall university cambridge england 
kindermann bayesian query construction neural network models advances neural information processing systems san mateo ca morgan kaufmann appear 
schaal atkeson assessing quality learned local models advances neural information processing systems san mateo ca morgan kaufmann 
sutton integrated architectures learning planning reacting approximating dynamic programming proceedings seventh international conference machine learning june pp 
san mateo ca morgan kaufmann 
sebastian thrun exploration active learning thrun efficient exploration reinforcement learning technical report cmu cs carnegie mellon university pittsburgh pa 
thrun role exploration learning control handbook intelligent control neural fuzzy adaptive approaches white david donald eds 
van nostrand reinhold florence kentucky 
watkins dayan learning machine learning 
whitehead complexity cooperation learning proceedings eighth international workshop machine learning birnbaum collins eds pp 
san mateo ca morgan kaufmann 
