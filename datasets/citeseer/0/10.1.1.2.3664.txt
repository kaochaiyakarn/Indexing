ensemble selection libraries models rich caruana caruana cs cornell edu alexandru niculescu cs cornell edu geo crew gc cs cornell edu alex ak cs cornell edu department computer science cornell university ithaca ny usa method constructing ensembles libraries thousands models 
model libraries generated di erent learning algorithms parameter settings 
forward stepwise selection add ensemble models maximize performance 
ensemble selection allows ensembles optimized performance metric accuracy cross entropy mean precision roc area 
experiments test problems metrics demonstrate bene ensemble selection 

ensemble collection models predictions combined weighted averaging voting 
dietterich states necessary su cient condition ensemble classi ers accurate individual members classi ers accurate diverse 
methods proposed generate accurate diverse sets models 
bagging breiman trains models type decision trees bootstrap samples training set 
opitz bags features training points 
boosting schapire generates potentially diverse set models bagging weighting training set force new models attend points dicult classify correctly 
sullivan 
boost features training points 
errorcorrecting output codes ecoc dietterich bakiri creates models decorrelated errors training models multi class problems di erent dichotomies 
munro created di appearing proceedings st international conference machine learning ban canada 
copyright authors 
verse neural nets competition nodes 
generate diverse sets models di erent algorithms 
support vector machines svms arti cial neural nets anns memory learning knn decision trees dt bagged decision trees bag dt boosted decision trees bst dt boosted stumps bst stmp 
algorithm train models di erent parameter settings 
example train svms varying margin parameter kernel kernel parameters varying gamma rbf kernels 
train models problem 
models excellent performance equal better best models reported literature 
models mediocre poor performance 
combine bad models ensemble forward stepwise selection library models nd subset models averaged yield excellent performance 
basic ensemble selection procedure simple 
start empty ensemble 

add ensemble model library maximizes ensemble performance error metric validation set 

repeat step xed number iterations models 

return ensemble nested set ensembles maximum performance validation set 
models added ensemble averaging predictions models ensemble 
adding model ensemble fast allowing ensembles excellent performance minutes libraries models 
selection procedure allows optimize ensemble easily computed performance metric 
evaluate performance ensemble selection performance metrics 
believe rst time learning method evaluated wide variety performance metrics 
performance metric compare ensemble selection model library performs best metric 
generate di erent models libraries usually contain models excellent performance performance metric 
just selecting best single model library yields remarkably performance 
ensemble selection nds ensembles outperform best single models 
suggests di erent learning methods parameter settings generates libraries containing diverse set performing models 
parameters vary algorithm generate models described appendix 
note determine parameters yield best performance training models 
models added library matter bad 
model predictions train hillclimbing sets cached 
simpli es working library model selection faster models executed selection 

improving ensemble selection simple forward model selection procedure fast ective ts hillclimbing set reducing ensemble performance 
additions selection procedure reduce tting 
discussed sub sections 
methods may useful applications forward stepwise selection prone tting feature selection kohavi john :10.1.1.30.525

selection replacement model selection replacement performance improves best models added ensemble peaks quickly declines 
performance drops best models library selection add models hurt ensemble 
shows behavior root mean squared error 
unfortunately error metrics yield graphs hillclimbing done small data sets making dif cult reliably pick stopping point 
loss performance signi cant peak missed 
shows selecting models replacement greatly reduces problem 
selection replacement allows models added ensemble multiple times 
peak performance reached unused models hurt ensemble performance selection adds models added hurt performance 
performance curve past peak allows selection ne tune ensembles weighting models models added ensemble multiple times receive weight 
number models ensemble selection replacement selection replacement test selection replacement test selection replacement 
selection replacement 
selection replacement curve test set needed determine adding models ensemble 
hillclimbing set hillclimbing 
means ensemble selection need test sets base level models select model parameters 
ensemble selection uses validation set parameter model selection 

sorted ensemble initialization forward selection ts early selection ensembles small 
way prevent initialize ensembles models 
starting empty ensemble sort models library performance put best models ensemble 
chosen looking performance hillclimbing set 
typically adds best models ensemble greedy stepwise selection begins 
best models performs form strong initial ensemble dicult greedy selection nd models added ensemble 

bagged ensemble selection number models library increases chances nding combinations models hillclimbing set increases 
bagging minimize problem 
reduce number models selection choose drawing random sample models library selecting sample 
particular combination models ts probability models random bag models fraction models bag 
bag ensemble selection times insure best models opportunities selected 
nal ensemble average ensembles 
bags ensembles complex ensemble just weighted average models average set ensembles simple weighted average base level models 
bagging discussed section 
data sets experiment problems adult cover type letter letter uci repository blake merz medis pneumonia data set slac data collaborators stanford linear accelerator hyper spect indian pines hyperspectral data 
adult medis slac binary problems 
cover type letter hyper spect class problems respectively 
converted binary metrics study de ned binary problems learning methods svms boosting easier apply binary problems 
cover type converted binary treating largest class class 
letter converted ways 
letter treats confusable letter class remaining letters class yielding unbalanced binary problem 
letter 
uses letters class letters class yielding dicult balanced problem 
hyper spect converted binary treating large confusable class soybean class 
data sets selected large allow moderate size train validation sets data left large nal test sets 
experiments training sets points 
training sample split train set points hillclimbing validation set points 
nal test sets problems contain points discerning small differences performance reliable 

performance metrics performance metrics accuracy acc root mean squared error rms mean cross entropy mxe lift lft precision recall break point bep precision recall score fsc average precision apr roc area roc measure probability calibration cal 
tenth metric sar acc roc rms 
sar robust metric correct metric unknown 
attractive feature ensemble selection optimize metrics sar 
compare performance metrics different metrics appropriate di erent settings learning methods perform metric perform metrics 

empirical results section compare ensemble selection best models trained learning algorithms ensemble methods 

normalized scores performance metrics acc rms range lft mxe range depends data 
metrics lower values indicate better performance 
higher values better 
baseline rates metrics roc independent data acc baseline rates depend data 
baseline acc acc probably poor performance bayes optimal acc achieving acc excellent performance 
allow averaging problems metrics convert performances normalized scale 
scale represents baseline performance metric problem roc baseline acc problem represents best performance seen model nal test set 
bottom table shows normalized scores learning algorithm parameters selected problem metric validation sets ensemble selection 
entries table averages problems 
scores near indicate model performs close best performance observed problems 
negative entries mean best models perform baseline 
bold entries bottom table show learning algorithms yield best performance metric 
column mean normalized score metrics 
ignoring ensemble methods top table best algorithms bagged trees svms anns mean normalized scores respectively 
boosted trees perform probability metrics rms mxe set top scale bayes optimal performance problem metric knew 
table 
normalized scores best single models type bottom tbl ensemble selection bayesian model averaging stacking regression averaging models picking best model type top tbl 
model acc fsc lft roc apr bep rms mxe cal sar mean ens 
sel 
best avg stack lr bag svm ann bst dt knn dt bst stmp cal excellent threshold metrics acc fsc lft ordering metrics roc apr bep 
mean performance measured just metrics boosted trees outperform bagged trees svms anns 
top table shows normalized scores ensemble selection bayesian model averaging best individual models type best simple average models avg stacking logistic regression stack lr 
stacking wolpert logistic regression performs poorly regression ts dramatically highly correlated input models points validation set 
unweighted average models avg better weighting models regression unweighted averaging validation set 
averaging performs worse just picking best single models models poor performance hurt ensembles 
expected picking best single model library best performs better learning algorithm bottom table pick best model learning method metric test problem 
bayesian model averaging domingos mean score signi cantly better bagged trees best single model 
bayesian model averaging model library weighted likelihood data validation set model model predictions treated probabilities wm targ red targ red wm weight model targ target case red predicted probability case class 
bayesian model averaging higher mean performance best outperforms best metrics best outperforms 
main reason best lower mean performance bayesian averaging best performs poorly cal metric 
mean normalized score ensemble selection clear winner 
outperforms ensemble methods best individual models metrics signi cant 
believe ensemble selection consistently outperforms ensemble methods reasons ensemble selection able optimize ensemble di erently performance metric tting serious problem models combine 
methods section combat tting contribute ensemble selection excellent performance 
consistently strong performance ensemble selection suggests di erent learning methods parameter settings ective way generating diverse collection models 
consistent performance suggests forward stepwise selection ective way selecting high performance ensembles models variety performance metrics tting carefully controlled 
cal high variance metric 
selecting best model models small validation set high variance metric models look validation set perform poorly nal test set selected 
best performs worse bagged trees svms anns cal table 
percent reduction loss ensemble selection best models learning algorithm 
problem acc fsc lft roc apr bep rms mxe cal sar mean adult cover type letter letter medis hyper spect slac mean 
percent reduction loss error section compare ensemble selection best single models best table percent reduction error 
advantage percent reduction error compared normalized scores normalized scores change better models shift top scale 
percent reduction error model depends performances models model de nes top performance scale 
calculate percent reduction error rst convert metric loss represents perfect performance represents worst performance 
perfect prediction yields acc bep fsc roc apr sar rmse mxe cal loss 
example help 
acc improves losses respectively percent reduction error 
potential disadvantage percent reduction loss gives emphasis reductions low loss reducing loss reduction reducing loss absolute change reduction 
domains bias appropriate 

normalized scores bias 
table shows percent reduction loss ensemble selection test problems metrics compared best models selected problem metric 
normalized scores nal performances estimated large nal test sets training average trials problem 
positive entries table mean error reduced ensemble selection performed better 
ensemble selection wins times signi cant 
average ensemble selection reduces loss best models problem metric 
concrete best model accuracy loss reduc fraction models bagged samples apr roc cal sar mxe lft bep rms fsc acc 
bene bagged selection metrics 
right side graph represents bagging 
tion loss corresponds reducing loss increasing accuracy 
similarly best model rms reduction loss corresponds reducing rms 
test sets contain cases 
increasing accuracy means cases predicted correctly ensembles 
improvement dramatic consistently increasing accuracy reducing rms compared best models impressive best models performance 
don want improvement ensemble selection 
comparison bagging boosting trees yields reduction loss compared raw trees times bene ensemble selection 
improving performance mediocre high variance models decision trees easier improving performance best models 

bagging minimize tting shows percent reduction loss ensemble selection bagging fraction models bags varies metrics averaged problems 
bags contain models equivalent bagging 
average bagged ensemble selection reduces loss additional 
results tables value selected looking results shows suboptimal 
third total bene see ensemble selection 
uses nal test sets suggests range yield improvement 
currently experimenting cross validation pick near optimal value problem metric 

case study classifying sub atomic particles case study optimizing ensemble correct metric impact application machine learning particle physics problem 
stanford linear accelerator center slac high energy particle beams collided generate particles 
major challenge experiments correctly classify particle tracks 
performance measured slac score percent events accepted prediction probability misclassi cation 
application speci performance metric estimates statistical power model 
increasing equivalent having data potentially saves hundreds thousands dollars accelerator time 
bagged trees best performance increasing improvement neural nets decision trees 
behaves cross accuracy calibration surprising bagged trees models best calibration performance best 
ensemble optimized ensemble selection performance increases best bagged trees 
increase ective sample size represents large potential savings accelerator time 

discussion 
validation hillclimbing sets ensemble selection uses validation set train ensembles 
hillclimbing validation set give ensemble selection unfair advantage models 
validation set needed select parameters algorithm parameter selection pick best algorithm model selection 
ensemble selection uses validation set parameter selection model selection ensemble creation 
algorithms validation data put back train set model retrained parameters selected 
done ensemble selection 
strategy reusing validation sets including cross validation ensemble selection 
currently running experiments fold cross validation increase size hillclimbing set include training data just held samples 

models selected ensembles table shows total weight type model metric adult cover type 
average metrics right column shows knn bst dt receive weight ensemble cover type 
weights strikingly di erent adult bst stmp ann dt receive weight 
substantial di erences model types preferred di erent metrics 
example cover type fsc gives high weight boosted trees low weight knn weights reversed rms lft 
anns get modest weight mxe rms anns optimize low weight acc lft 
suggests ensemble selection able exploit different strengths biases di erent learning algorithms optimizing ensemble metric problem 

computational cost building libraries expensive 
models independent easy parallelize model creation distribute training machines 
takes hours train models cluster linux machines 
model training automated 
parameter tuning examining performance validation sets 
usually model critical necessary wait models trained library 
provides time avor ensemble selection ensembles trained models available ensemble needed 
easy add models 
libraries built performance metric known libraries depend metric optimize ensemble 
model libraries reusable 
table 
aggregate weight di erent types models ensembles 
adult acc fsc lft roc apr bep rms mxe sar avg wt ann knn svm dt bag dt bst dt bst stmp cov type acc fsc lft roc apr bep rms mxe sar avg wt ann knn svm dt bag dt bst dt bst stmp forward stepwise ensemble selection ecient 
adding model ensemble requires averaging model predictions ensemble predictions size hillclimbing set 
models choose done times selection step 
selection run steps cost ensemble selection assuming metric computed 
metric expensive roc requires sorting logd recomputing metric dominates 
java implementation selecting ensemble library models hillclimbing set points takes minute medium power workstation 
selection bagged times takes minutes build nal ensemble 

optimizing performance metric anns usually trained minimize cross entropy squared error 
trees svms usually maximize accuracy 
boosting designed maximize accuracy 
metrics precision recall roc hard optimize 
model averaging fast ensemble selection try adding model library ensemble step 
performance ensembles evaluated quickly metric ensemble optimized metric greedy brute force search 
ensemble usually baselevel models combinations yield performance metric 
know optimize base level models metrics ensemble optimized 

binary classi cation ensemble selection straightforward binary classi cation regression 
base level models predictions multiple classes modi cation ensemble selection procedure necessary multiclass problems 
base level models predictions dichotomy time svms ensemble selection easiest base level models combined return predicted probability class 
experimented multi class ensemble selection 

ensemble selection uses forward stepwise selection libraries thousands models build ensembles optimized performance metric 
variety learning algorithms parameter settings appears ective generating libraries diverse high quality models 
ensemble selection important feature optimize ensemble performance easily computed performance metric 
experiments test problems performance metrics show ensemble selection consistently nds ensembles outperform models including models trained bagging boosting bayesian model averaging 

appendix building model libraries knn values ranging 
knn euclidean distance euclidean distance weighted gain ratio 
distance weighted knn locally weighted averaging 
kernel widths locally weighted averaging vary times minimum distance points train set 
ann train nets gradient descent backprop vary number hidden units momentum 
don validation sets weight decay early stopping 
nets di erent epochs nets dt vary splitting criterion pruning options smoothing laplacian bayesian smoothing 
dt models buntine ind package bayes id cart cart mml 
generate trees type pruning bs bayesian smoothing mml laplacian smoothing 
see provost domingos descriptions 
bag dt bag trees type 
tree trained bootstrap sample added library nal bagged ensemble averages trees 
bst dt boost tree type 
boosting add boosted dts library steps boosting 
bst stmp stumps single level decision trees di erent splitting criteria boosted steps 
svms kernels svmlight joachims polynomial degree radial width gg vary regularization parameter factors output range svms 
svm predictions compatible models platt method convert svm outputs probabilities tting sigmoid platt 
acknowledgments charles chiu helped initial design ensemble selection 
tony help hyper spect data collaborators slac help slac data performance metric 
blake merz 

uci repository machine learning databases 
breiman 

bagging predictors 
machine learning 
dietterich 

ensemble methods machine learning 
international workshop multiple classi er systems 
dietterich bakiri 

solving multiclass learning problems error correcting output codes 
journal arti cial intelligence research 
domingos 

bayesian averaging classi ers tting problem 
proc 
th international conf 
machine learning pp 

morgan kaufmann san francisco ca 
johnson 

support vector machine classi ers applied data 
proc 
eighth jpl airborne geoscience workshop 
joachims 

making large scale svm learning practical 
advances kernel methods 
kohavi john 

wrappers feature subset selection 
arti cial intelligence 
meek thiesson heckerman 

staged mixture modeling boosting technical report msr tr 
munro 

competition networks improves committee performance 
advances neural information processing systems 
opitz 

feature selection ensembles 
aaai iaai pp 

platt 

probabilistic outputs support vector machines comparison regularized likelihood methods 
advances large margin classi ers pp 

provost domingos 

tree induction probability rankings 
machine learning 
schapire 

boosting approach machine learning overview 
msri workshop nonlinear estimation classi cation 
sullivan langford caruana blum 

featureboost meta learning algorithm improves model robustness 
proceedings seventeenth international conference machine learning 
wolpert 

stacked 
neural networks 
