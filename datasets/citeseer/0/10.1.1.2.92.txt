apprenticeship learning inverse reinforcement learning pieter cs stanford edu andrew ng ang cs stanford edu computer science department stanford university stanford ca usa consider learning markov decision process explicitly reward function observe expert demonstrating task want learn perform 
setting useful applications task driving may difficult write explicit reward function specifying exactly different desiderata traded 
think expert trying maximize reward function expressible linear combination known features give algorithm learning task demonstrated expert 
algorithm inverse reinforcement learning try recover unknown reward function 
show algorithm terminates small number iterations may recover expert reward function policy output algorithm attain performance close expert performance measured respect expert unknown reward function 

sequential decision making problem posed markov decision process mdp formalism number standard algorithms exist finding optimal near optimal policy 
mdp setting typically assume reward function 
reward function mdps state transition probabilities value function optimal policy exactly determined 
mdp formalism useful problems easier specify reward function directly specify value function optimal policy 
believe reward function frequently difficult specify manually 
consider example task highway driving 
driving typically trade dif appearing proceedings st international conference machine learning banff canada 
copyright authors 
ferent desiderata maintaining safe distance keeping away curb staying far pedestrians maintaining reasonable speed slight preference driving middle lane changing lanes 
specify reward function driving task assign set weights stating exactly trade different factors 
despite able drive authors believe confidently specify specific reward function task driving practice means reward function manually tweaked cf 
reward shaping ng desired behavior obtained 
conversations engineers industry experience applying reinforcement learning algorithms robots believe problems difficulty manually specifying reward function represents significant barrier broader applicability reinforcement learning optimal control algorithms 
teaching young adult drive telling reward function easier natural demonstrate driving learn demonstration 
task learning expert called apprenticeship learning learning watching imitation learning learning demonstration 
number approaches proposed apprenticeship learning various applications 
methods try directly mimic demonstrator applying supervised learning algorithm learn direct mapping states actions 
literature wide survey examples include sammut 
kuniyoshi 
demiris hayes amit mataric pomerleau 
notable exception atkeson schaal 
considered note true reward function may easy state english 
instance true reward function trying maximize driving personal happiness practical problem model happiness explicitly function problems states reinforcement learning algorithm applied 
problem having robot arm follow demonstrated trajectory reward function quadratically penalizes deviation desired trajectory 
note method applicable problems task mimic expert trajectory 
highway driving blindly expert trajectory pattern traffic encountered different time 
entire field reinforcement learning founded presupposition reward function policy value function succinct robust transferable definition task natural consider approach apprenticeship learning reward function learned 
problem deriving reward function observed behavior referred inverse reinforcement learning ng russell 
assume expert trying necessarily succeeding optimize unknown reward function expressed linear combination known features guarantee algorithms correctly recover expert true reward function show algorithm find policy performs expert performance measured respect expert unknown reward function 

preliminaries finite state markov decision process mdp tuple finite set states set actions psa set state transition probabilities psa state transition distribution action state discount factor initial state distribution start state drawn reward function assume bounded absolute value 
mdp denote mdp reward function tuple form 
assume vector features states true reward function related idea seen biomechanics cognitive science researchers pointed simple reward functions usually ones constructed hand suffice explain complicated behavior policies 
examples include minimum jerk principle explain limb movement primates hogan minimum torque change model explain trajectories human arm movement uno related examples economics literatures 
see discussion ng russell 
case state action rewards offers additional difficulties features form algorithms apply straightforwardly 
order ensure rewards bounded assume 
driving domain vector features indicating different desiderata driving trade just collided car re driving middle lane 
unknown vector specifies relative weighting desiderata 
policy mapping states probability distributions actions 
value policy es st st st expectation taken respect random state sequence 
drawn starting state picking actions 
define expected discounted accumulated feature value vector succinctly feature expectations st 
notation value policy may written es 
reward expressible linear combination features feature expectations policy completely determine expected sum discounted rewards acting policy 
denote set stationary policies mdp 
policies construct new policy mixing 
specifically imagine operates flipping coin bias probability picks acts probability acts 
linearity expectation clearly 
note randomization step selecting occurs start trajectory step taken mdp 
generally set policies 
want find new policy feature expectations vector convex combination policies mixing policies 
probability picking assume access demonstrations expert specifically assume ability observe trajectories state sequences generated expert starting actions may helpful think optimal policy reward function require hold 
algorithm require estimate expert feature expectations 
specifi cally set trajectories mi generated expert denote empirical estimate 
sequel assume access reinforcement learning rl algorithm solve mdp augmented reward function 
simplicity exposition assume rl algorithm returns optimal policy 
generalization approximate rl algorithms offers special difficulties see full 
ng 
algorithm problem mdp feature mapping expert feature expectations find policy performance close expert unknown reward function 
accomplish find policy 
st st inequality follows fact second 
problem reduced finding policy induces feature expectations close 
apprenticeship learning algorithm finding policy follows 
randomly pick policy compute approximate monte carlo set 
compute maxw value attains maximum 

terminate 

rl algorithm compute optimal policy mdp rewards 

compute estimate 

set go back step 
termination algorithm returns 

examine algorithm detail 
iteration policies 
optimization step viewed inverse reinforcement learning step try guess practice truncate trajectories finite number steps 
log horizon time introduces error approximation 

iterations max margin algorithm 
reward function optimized expert 
maximization step equivalently written maxt 
eq 
see algorithm trying find reward function es es reward expert better margin policies previously 
step similar ng russell algorithms norm constraint posed linear program lp quadratic program 
readers familiar support vector machines svms recognize optimization equivalent finding maximum margin hyperplane separating sets points 
vapnik equivalence obtained associating label expert feature expectations label feature expectations 
vector want unit vector orthogonal maximum margin separating hyperplane 
svm solver find 
svm problem quadratic programming problem qp generic qp solver 
show example iterations algorithm look geometrically 
shown example different iterations algorithm 
suppose algorithm terminates 
algorithm terminates discussed section 
directly eq 

means policy set returned algorithm performance expert performance minus 
stage ask agent designer manually test examine policies algorithm previously assumed specifying true rewards theoretical results assumption implement algorithm eq 

pick acceptable performance 
slight extension method ensures agent designer examine different policies see footnote 
wish ask human help select policy alternatively find point closest convex closure 
solving qp min 
separated points margin know solution 
mixing policies mixture weights discussed previously obtain policy feature expectations 
previous discussion eq 
policy attains performance near expert unknown reward function 
note called step algorithm inverse rl step algorithm necessarily recover underlying reward function correctly 
performance guarantees algorithm depend approximately matching feature expectations recovering true underlying reward function 

simpler algorithm algorithm described requires access qp svm solver 
possible change algorithm qp solver needed 
call previous qp algorithm method new algorithm projection method 
briefly projection method replaces step algorithm set computes orthogonal projection line set set iteration set full justification method deferred full ng sections give convergence results empirically compare max margin dimensional space point convex combination set points written convex combination subset points original points theorem :10.1.1.2.92
applying arg min obtain set policies equally close expert feature expectations performance guarantees 
denotes convex hull 

iterations projection algorithm 
algorithm 
example showing iterations projection method shown 
theoretical results results previous section predicated assumption algorithm terminates 
algorithm terminate takes exponentially large number iterations terminate useful 
shows case 
theorem 
mdp features 
apprenticeship learning algorithm max margin projection versions terminate iterations 
log previous result section assumed exactly known calculated 
practice estimated monte carlo samples eq 

ask sample complexity algorithm trajectories observe expert guarantee approach performance 
theorem 
mdp features 
suppose apprenticeship learning algorithm max margin projection version run estimate obtained monte carlo samples 
order ensure probability algorithm terminates number iterations eq 
outputs policy true reward st st suffices log proofs theorems appendix case true reward function lie exactly span basis functions algorithm enjoys graceful degradation performance 
specifically residual error term algorithm performance worse expert 

experiments 
gridworld set experiments multiple sparse rewards 
reward known algorithm sample trajectories expert optimal policy 
agent actions try move compass directions chance action fails results random move 
grid divided non overlapping regions cells call regions small number resulting positive rewards 
value 
feature indicating state rewards may written 
weights generated randomly give sparse rewards leads fairly interesting rich optimal policies 
basic version algorithm run dimensional features 
tried version algorithm knows exactly non zero rewards values dimension reduced contain features corresponding non zero rewards 
compare max margin projection versions algorithm known exactly 
plot margin distance expert policy vs number iterations features 
expert policy optimal policy respect mdp 
algorithms exhibited fairly similar rates convergence projection version doing slightly better 
second set experiments illustrates performance algorithm vary number sampled expert trajectories estimate 
performance measure value best policy set output algorithm 
ran algorithm features features truly correspond non zero rewards 
report performance details expected horizon order 
true reward function generated follows 
probability reward zero probability weight sampled uniformly 
renormalized 
instances fewer non zero entries non interesting discarded 
initial state distribution uniform states 
note text apprenticeship learning algorithm assumes ability call reinforcement learning subroutine case exact mdp solver value iteration 
experiments interested distance expert feature distribution max margin projection number iterations 
comparison convergence speeds max margin projection versions algorithm grid 
euclidean distance expert feature expectations plotted function number iterations 
rescaled feature expectations plot shows averages runs 
performance performance expert irl non zero weight features irl features parameterized policy stochastic parameterized policy majority vote mimic expert log number sample trajectories 
plot performance vs number sampled trajectories expert 
shown color available 
averages instances plotted 
note base logarithm scale axis 
simple algorithms 
mimic expert algorithm picks action expert taken finds state previously observed expert picks action randomly 
parameterized policy stochastic uses stochastic policy probability action constant set empirical frequency observed expert 
parameterized policy majority vote algorithm takes deterministically frequently observed action 
results shown 
algorithm sampled expert trajectories far fewer methods needed attain performance approaching expert 
note log scale axis 
mainly question times expert demonstrate task learn perform task 
particular rely expert demonstrations learn state transition probabilities 
parameterized policies reach expert performance policy class rich 
restricted policy class better 
screenshot driving simulator 
learning compact representation reward function algorithm significantly outperforms methods 
observe algorithm told advance features non zero weight true reward function able learn fewer expert trajectories 

car driving simulation second experiment implemented simulation applied apprenticeship learning try learn different driving styles screenshot simulator shown 
driving highway mph faster cars 
mdp different actions cause car steer smoothly lanes cause drive parallel road left right side 
speed fixed want avoid hitting cars necessary drive road 
simulation runs hz experiments follow expert features estimated single trajectory samples corresponding minutes driving time 
features indicating lane car currently including left right total features distance closest car current lane 
note distance nearest car implies collision 
running apprenticeship learning algorithm step reinforcement learning required implemented solving discretized version problem 
experiments algorithm run iterations policy selected inspection discussion section 
wanted demonstrate variety different driving styles corresponding highly unsafe driving see algorithm mimic style instance 
considered styles 
nice highest priority avoid collisions mimic expert algorithm initially 
precisely distance single closest car current lane discretized nearest car length total features 
cars prefer right lane middle lane left lane driving road 

nasty hit cars possible 

right lane nice drive right lane go road avoid hitting cars right lane 

right lane nasty drive road right get back road hit cars right lane 

middle lane drive middle lane ignoring cars crashing cars middle lane 
style demonstrated algorithm authors driving simulator minutes apprenticeship learning try find policy mimics demonstrated style 
videos demonstrations resulting learned policies available www cs stanford edu irl instance algorithm qualitatively able mimic demonstrated driving style 
true reward specified experiments report results algorithm table shows driving styles feature expectations expert estimated minute demonstration feature expectations learned controller interesting features 
shown weights generate policy shown 
theory guarantee set weights note values generally intuitive sense 
instance driving style see negative rewards collisions driving larger positive rewards driving right lane lanes 

discussion assumed access demonstrations expert trying maximize reward function expressible linear combination known features algorithm apprenticeship learning 
method inverse reinforcement learning terminates small number iterations guarantees policy performance comparable better expert expert unknown reward function 
algorithm assumed reward function expressible linear function known features 
set features sufficiently rich assumption fairly 
extreme case separate feature state action pair fully general reward functions learned 
remains important problem develop methods learning reward functions may non linear functions features incorporate automatic fea table 
feature expectations teacher selected learned policy estimated monte carlo 
weights corresponding reward function generate policy shown 
note compactness interesting features total features shown 
collision left right ture construction feature selection ideas algorithms 
possible derive alternative apprenticeship learning algorithm dual lp solve bellman equations 
manne specifically lp variables state action visitation rates possible place constraints learned policy stationary distribution directly 
algorithms approximating dual opposed primal lp large mdps exact solutions feasible small mdps consider interesting direction 

supported department interior darpa contract number 
ng 

apprenticeship learning inverse reinforcement learning 
full 
www cs stanford edu irl 
amit mataric 

learning movement sequences demonstration 
proc 

atkeson schaal 

robot learning demonstration 
proc 
icml 
demiris hayes 

robot controller learning imitation 
hogan 

organizing principle class voluntary movements 
neuroscience 
kuniyoshi inaba inoue 

learning watching extracting reusable task knowledge visual observation human performance 
ra 
manne 

linear programming sequential decisions 
management science 
ng harada russell 

policy invariance reward transformations theory application reward shaping 
proc 
icml 
ng russell 

algorithms inverse reinforcement learning 
proc 
icml 
pomerleau 

alvinn autonomous land vehicle neural network 
nips 
rockafellar 

convex analysis 
princeton university press 
sammut hurst michie 

learning fly 
proc 
icml 
uno kawato suzuki 

formation control optimal trajectory human arm movement 
minimum torque change model 
biological cybernetics 
vapnik 

statistical learning theory 
john wiley sons 
proofs theorems ng give longer easier read proofs theorems 
due space constraints proofs follow quite dense 
preparation proofs definitions 
set policies define convex hull set feature expectations attained policies 
vector feature expectations set policies 
mixture weights pn pn 
point mixing policies obtain new policy feature expectations exactly 
mixture policies defined section 
define 
convex hull set feature expectations policies iterations 
algorithm 
due space constraints give full proof case reading proofs may helpful conveying geometric intuition 
lemma establishes improvement single iteration algorithm 
lemma 
mdp features set policies optimal policy mdp augmented reward arg max mentioned previously noisy estimate 
may valid feature expectation vector policy necessarily extended version proofs consider small ball radius centered intersects prove convergence ball 
proof similar technicalities significantly longer ng 

progress iteration step 
projection line point convex combination proof 
simplicity notation origin coordinate system coincide order definition rewriting rewriting terms arg max points considered lie norms bounded 
origin coincides proves eq 

easily verified definition cauchy schwarz max implies proof theorem 
note point lemma gives way construct point distance smaller factor eq 

long current iterate max margin algorithm sets arg min 
projection algorithm sets keeps track single point convex combination previously obtained points distance reduced iteration factor maximum distance log log log 
completes proof 
proof theorem 
recall denote th component 
applying hoeffding inequality sample estimate gives exp 
eq 
union bound gives 
exp rewritten exp 
substituting gives exp dimension feature vectors feature expectations number sample trajectories estimate 
take log probability dimensional spaces get log 
theorem guarantees sufficient number iterations assuming eq 
sufficient iterations algorithm return policy satisfies 
keeping mind implies prove order definitions cauchy schwarz eq 


