improving data locality loop transformations kathryn mckinley university massachusetts amherst steve carr michigan technological university chau wen tseng university maryland college park past decade processor speed significantly faster memory speed 
small fast cache memories designed overcome discrepancy effective programs exhibit data locality 
article compiler optimizations improve data locality simple accurate cost model 
model computes temporal spatial reuse cache lines find desirable loop organizations 
cost model drives application compound transformations consisting loop permutation loop fusion loop distribution loop reversal 
demonstrate program transformations useful optimizing programs 
validate optimization strategy implemented algorithms ran experiments large collection scientific programs kernels 
experiments illustrate kernels model algorithm select achieve best loop structure nest 
complete applications executed original transformed versions simulated cache hit rates 
collected statistics inherent characteristics programs ability improve data locality 
knowledge studies breadth depth 
performance improvements difficult achieve benchmark programs typically high hit rates small data caches optimizations significantly improved programs 
categories subject descriptors programming languages processors compilers optimization general terms languages performance additional key words phrases cache compiler optimization data locality loop distribution loop fusion loop permutation loop reversal loop transformations microprocessors simulation steve carr supported nsf ccr hewlett packard 
chau wen tseng supported part nsf cise postdoctoral fellowship experimental science 
authors initiated research rice university 
authors addresses mckinley computer science department university massachusetts amherst ma email mckinley cs umass edu carr department computer science michigan technological university houghton mi email carr cs mtu edu 
tseng department computer science university maryland college park md email tseng cs umd edu 
permission digital hard copy part material fee granted provided copies distributed profit commercial advantage acm copyright server notice title publication date appear notice copying permission association computing machinery 
acm 
copy republish post servers redistribute lists requires prior specific permission fee 
acm acm transactions programming languages systems vol 
july pages 
improving data locality 
processor speed increasing faster rate memory speed computer architects turned increasingly memory hierarchies levels cache memory 
caches take advantage data locality programs 
data locality property memory location adjacent locations reused short period time 
caches impact programming programmers substantially enhance performance style ensures memory handled cache 
scientific programmers expend considerable effort improving locality structuring loops innermost loop iterates elements column stored consecutively fortran 
task time consuming tedious error prone 
achieving data locality responsibility compiler 
placing burden compiler programmers get uniprocessor performance originally wrote program vector parallel machine 
addition programs portable programmers able achieve performance making machine dependent source level transformations 
optimization framework experiments experiences believe compiler optimizations improve data locality proceed order improve order memory accesses exploit levels memory hierarchy loop permutation fusion distribution skewing reversal 
process machine independent requires knowledge cache line size 
fully utilize cache tiling combination strip mining loop permutation irigoin 
knowledge data size cache size cache line size essential coleman mckinley lam 
higher degrees tiling applied exploit multilevel caches tlb 
promote register reuse unroll jam known register tiling scalar replacement callahan carr kennedy 
number type registers available required determine degree unroll jam number array replace scalars 
article concentrate step 
algorithms complementary fact improve effectiveness optimizations performed steps carr 
steps interactions steps scope article 
overview compiler strategy effective simple model estimating cost executing loop nest terms number cache line 
article extends previous kennedy mckinley slightly accurate memory model 
model derive loop structure results fewest accesses main memory 
achieve loop acm transactions programming languages systems vol 
july 
kathryn mckinley structure compound loop transformation algorithm consists loop permutation fusion distribution reversal 
algorithm implemented source source fortran translator 
extensive empirical results kernels benchmark programs validate effectiveness optimization strategy 
reveal programmers programming styles locality 
measure inherent data locality characteristics scientific programs ability improve data locality 
cache rate program nonnegligible show usually opportunities improve data locality 
optimization algorithm takes advantage opportunities consequently improves performance 
expected loop permutation plays key role 
addition loop fusion distribution produce significant improvements 
algorithms opportunity loop reversal improve locality 

background section characterize data reuse data locality cost model 
data dependence assume reader familiar concept data dependence kuck 
hybrid distance direction vector precise information derivable 
represents data dependence array corresponding left right outermost loop innermost loop enclosing 
data dependences loop independent accesses memory location occur loop iteration loop carried accesses occur different loop iterations 
sources data reuse sources data reuse temporal reuse multiple accesses memory location spatial reuse accesses nearby memory locations share cache line block memory level memory hierarchy 
access common type spatial locality 
temporal spatial reuse may result self reuse single array group reuse multiple wolf lam 
loss generality assume fortran column major storage 
processor speeds memory factors ranging current uniprocessors single cache inner loop iteration degrade performance 
measure locality number cache lines loop nest accesses 
minimize accesses memory minimizing number times cache line fetched memory 
simplify analysis concentrate reuse occurs small numbers inner loop iterations 
memory model assumes conflict capacity cache misses iteration innermost loop 
algorithms determine total number cache lines accessed candidate loop placed innermost loop position 
result reveals relative amounts reuse loops nest lam support assumption 
acm transactions programming languages systems vol 
july 
improving data locality groups loop loop fig 

example 
disjoint nests drives permutation fusion distribution reversal improve data locality minimizing number cache lines accessed 
groups cost model applies algorithm calculate group reuse 
group exhibit group temporal reuse access cache line different iterations inner loop 
formulation general previous kennedy mckinley slightly restrictive uniformly generated gannon 
goal algorithm avoid overcounting cache lines accessed multiple generally access set cache lines 

ref ref belong group respect loop ref ref loop independent dependence small constant entries zero ref ref refer array differ subscript dimension equal cache line size terms array elements 
subscripts identical 
condition accounts group temporal reuse condition detects forms group spatial reuse 
note group algorithm puts group meets conditions group 
specify implementation previous dependence testing constant distances greater 
addition cache line size elements require cache lines 
consider example nest 
fail tests regardless loop places distinct groups 
loop satisfy condition satisfy condition 
loop acm transactions programming languages systems vol 
july 
kathryn mckinley input ln ref representatives group cls lbl cache line size data items coeff il coefficient index variable il subscript stride il output coeff il loopcost number cache lines accessed innermost loop algorithm loopcost fj trip coeff il invariant coeff fj il stride il cls cls coeff il stride il coeff fj il unit fig 

loopcost algorithm 
group satisfy conditions 
loops carry dependence belong group loops 
loop cost terms cache lines account group reuse calculate reuse carried loop functions loopcost 
determine cost cache lines group select arbitrary array deepest nesting group 
loop trip iterations nest considered candidate innermost position 
cls cache line size data items stride step size multiplied coefficient loop index variable 
calculates locality number cache lines uses loop invariant trip cls stride consecutive trip nonconsecutive 
loopcost calculates total number cache lines accessed innermost loop 
simply sums groups multiplies result trip counts remaining loops 
loopcost appear 
method evaluates imperfectly nested loops see section example complicated subscript expressions nests symbolic bounds mckinley 
give example computing loopcost matrix multiply 
algorithm respect loops puts group acm transactions programming languages systems vol 
july 
jki ordering loopcost cls refs total fig 

loop cost matrix multiply 
improving data locality group 
respect loop detects self spatial reuse carried assigns cost cls cache lines 
loop invariant reuse cost 
loopcost loop machine cls andn outer iterations loops 
loopcost respect loops similar 

compound loop transformations section show cost model guides loop permutation fusion distribution reversal 
subsection describes tests cost model determine individual transformations profitable 
components section presents compound algorithm discovering applying legal compound loop nest transformations aim minimize number cache lines accessed 
transformations implemented experimental compiler 
loop permutation determine loop permutation accesses fewest cache lines rely observation 
loop promotes reuse loop considered innermost loop promote reuse outer loop position 
simply rank loops loopcost ordering loops outermost innermost ln li loopcost li 
call permutation nest cost memory order 
bounds symbolic compare dominating terms 
define algorithm permute achieve memory order possible perfect nests 
determine order legal permute corresponding entries distance direction vector 
result lexicographically positive permutation legal transform nest 
section perform imperfect interchanges distribution 
evaluation method drive imperfect loop interchange wolfe implement 
acm transactions programming languages systems vol 
july 
kathryn mckinley input original loop ordering dv set original legal direction vectors ln output permutation best estimated locality permutation legally close possible algorithm dv lis legal permutation return lj direction vectors pk legal pk break endif endfor endwhile fig 

permute algorithm 
definition original distance direction vector legal lexicographically positive allen kennedy banerjee 
legal permutation exists positions loop reuse innermost algorithm guaranteed find 
desired inner loop obtained desirable inner loop positioned innermost possible 
data reuse occurs innermost loop positioning correctly yield best data locality 
formally stated memory ordering loops reuse algorithm builds legal permutation testing see loop legal outermost position 
legal added removed legal loop tested 
loop positioned process repeated starting empty 
permute works positioning outer loops partial direction vectors lexicographically positive means entries zero positive entry precedes negative entries partial direction vectors pk 
consider placing loop position single dependence 
direction vector entry position positive zero legal position 
entry negative pk positive legal position 
entry position negative pk zero positioning create negative illegal direction vector 
notice permutation pk change positive vector zero enable negative entry placed position 
property enables permute greedily outermost loop innermost 
theorem acm transactions programming languages systems vol 
july 
holds permute algorithm 
improving data locality theorem 
exists legal permutation innermost loop permute find permutation innermost 
original set legal direction vectors legal clearly innermost 
proof contradiction theorem proceeds follows 
step guaranteed find loop results partial positive direction vectors original legal allen kennedy banerjee 
addition loop may legally positioned prior 
permute places loops carrying reuse innermost possible 
desired inner loop obtained places desirable inner loop innermost position possible 
characteristic important data reuse occurs innermost loop positioning correctly key achieving best data locality 
complexity 
memory order legal loops test suite permute simply sorts loops loopcost tests legality 
algorithm permute selects legal permutation close memory order possible testing legality loop permutations worst case 
steps involve testing data dependences evaluating locality loop nest turns expensive part algorithm 
algorithm computes best permutation evaluation step invocation loopcost loop nest 
complexity algorithm permute number loopcost invocations number loops nest 
example matrix multiplication 
saw algorithm ref group matrix multiply puts group separate groups loops 
algorithm uses loopcost select jki memory order exhibit spatial locality exhibits loop invariant temporal locality resulting fewest cache line accesses 
validate cost model gathered results possible permutations ranking left right highest cost jki kji jik ijk kij 
consistent model choosing inner loop results best execution time 
changing inner loop dramatic effect performance 
impact greater versus matrices larger portion working set stays cache 
execution times vary significant factors sparc rs 
entire ranking accurately predicts relative performance 
performed type comparison kernels small program result memory order resulted best performance 
loop reversal loop reversal reverses order iterations loop nest execute legal dependences remain carried outer loops 
reversal change pattern reuse enabler may enable permutation achieve acm transactions programming languages systems vol 
july 
kathryn mckinley execution times seconds vs loop organization jki kji jik ijk kij jki kji jik ijk kij jki kji jik ijk kij sun sparc intel ibm rs jki kji jik ijk kij jki kji jik ijk kij jki kji jik ijk kij sun sparc intel ibm rs fig 

performance matrix multiply 
better locality 
extend permute perform reversal follows 
memory order legal permute places outer loops position building lexicographically positive dependence vectors 
permute legally position loop desired position permute tests reversal legal enables loop put position 
reversal improve locality experiments discuss 
loop fusion loop fusion takes multiple loop nests combines bodies loop nest 
legal data dependences reversed warren 
example effect consider code fragment written fortran performs adi integration 
scalarizing fortran fortran results code exhibits poor temporal poor spatial reuse 
problem fault programmer inherent computation expressed fortran 
fusing loops results temporal acm transactions programming languages systems vol 
july 
improving data locality sample fortran loops adi integration translation fortran loop fusion interchange loopcost cls total total total fig 

loop fusion 
locality array addition compiler able apply loop interchange significantly improving spatial locality arrays 
transformation illustrated 
profitability loop fusion 
loop fusion may improve reuse directly moving accesses cache line loop iteration 
algorithm discovers reuse nests treating statements loop body 
loop headers compatible loops number iterations 
nests compatible level loops level compatible headers perfectly nested level determine profitability fusing compatible nests cost model follows compute loopcost statements nest fused 
compute loopcost independently candidate add results 
compare total 
acm transactions programming languages systems vol 
july 
kathryn mckinley fuse input lk nests fusion candidates algorithm build hj hi hk compatible nests depth hi depth hi build dag dependence edges weights hi hm hm locality edge legal fuse fuse update endfor endfor endfor fig 

fusion algorithm 
fused loopcost lower fusion result additional locality 
example fusing loops lowers loopcost candidate loops fusion need nested common loop 
note memory order fused loops may differ individual nests 
loop fusion enable loop permutation 
loop fusion may indirectly improve reuse imperfect loop nests providing perfect nest enables loop permutation better data locality 
instance fusing loops enables permutation loop nest improving spatial temporal locality 
cost model detect transformation desirable loopcost loop lower loops memory order achieved loop structure 
test fusion inner nests legal creates perfect nest memory order achieved 
loop fusion algorithm 
fusion serves purposes improve temporal locality fuse inner loops creating nest permutable 
previous research shown optimizing temporal locality adjacent set compatible loop nests np hard kennedy mckinley 
problem harder headers necessarily compatible 
apply greedy strategy depth compatibility 
build dag candidate loops 
edges dependences loops weight edge difference fused versions 
partition nests sets compatible nests deepest levels possible 
yield locality fuse nests deepest compatibility temporal locality 
nests fused legal dependences violated loops dag 
update graph fuse level compatible sets considered 
algorithm appears 
acm transactions programming languages systems vol 
july 
execution time seconds vs organization sun sparc intel ibm rs fig 

performance 
improving data locality hand coded distributed allow loop nests reordered due fusion may need calculate loopcost pair loop nests 
complexity fusion algorithm number invocations loopcost number candidate nests fusion 
fused adjacent loop nests complexity algorithm drop 
example 
original hand coded version program solving pdes adi integration arrays consists single statement loops memory order 
applied loop distribution hand loops containing multiple statements placing statement separate loop nest 
loops fully distributed version resembles output fortran 
fully distributed version created optimized versions program 
applied permute transform individual loop nests memory order 
second optimized version applied fuse obtain temporal locality 
measure performance original program hand transformed fully distributed program distributed fused version fused 
fusion improvement hand coded distributed versions 
statement separate loop variables shared loops 
permuting loops memory order increases locality nest slightly degrades locality nests degradation performance distributed version compared original 
benefits fusion additive multiplicative loop permutation impact significant 
impact increase programs written fortran array syntax 
loop distribution loop distribution separates independent statements single loop multiple loops identical headers 
maintain meaning original loop statements recurrence cycle dependence graph include input dependences placed loop 
groups statements loop called partitions 
system loop distri fused acm transactions programming languages systems vol 
july 
kathryn mckinley distribute input lm loop nest containing sk statements algorithm restrict dependence graph carried level deeper loop independent divide finest partitions pm sr st recurrence sr st pi 
compute pi achievable distribution permutation perform distribution permutation return endfor fig 

distribution algorithm 
bution indirectly improve reuse enabling loop permutation nest permutable statements different partitions may prefer different memory orders achievable distribution 
algorithm distribute appears 
divides statements finest granularity partitions tests enables loop permutation 
performs distribution innermost loop enables permutation 
nest depth starts loop level works outermost loop stopping successful 
invoke algorithm distribute memory order achieved nest inner nests fused see section 
distribute tests distribution enable memory order achieved partitions 
dependence structure required test loop permutation created restricting test dependences statements partition interest 
perform distribution combines permutation improve actual loopcost 
loopcost calculated individual partition complexity algorithm distribute number individual partitions created loop distribution 
see section example 
compound transformation algorithm driving force application compound loop transformations minimize actual loopcost achieving memory order statements nest possible 
algorithm compound uses permutation fusion distribution reversal needed place loop provides reuse innermost position statement 
algorithm compound considers adjacent loop nests 
optimizes nest independently applies fusion resulting nests distribution effective temporal locality partitions accessed arrays numerous fit cache register pressure concern 
address issues 
compound transformation algorithm section follows distribution permutation fusion regain lost temporal locality 
acm transactions programming languages systems vol 
july 
improving data locality compound input nk adjacent loop nests algorithm tok compute ni permute ni places inner loop memory order continue ni perfect nest contains adjacent loops mj mj places inner loop memory order continue distribute ni fuse fuse fig 

compound loop transformation algorithm 
legal data locality improved 
optimize nest algorithm begins computing memory order determining loop containing reuse placed innermost 
algorithm goes loop 
tries enable permutation memory order fusing inner loops form perfect nest 
fusion enable memory order algorithm tries distribution 
distribution succeeds enabling memory order new nests may formed 
distribution algorithm divides statements finest partitions nests candidates fusion recover temporal locality 
complexity 
complexity algorithm compound nm number invocations loopcost number loops nest number adjacent loop nests 
invocations loopcost needed calculate memory order loop nest process may need repeated times applying loop fusion 
fortunately fusion distribution need invoked original loop nest permuted memory order 
practice loop fusion algorithm seldomly applied need consider adjacent loop nests 
loop distribution may increase thenumber adjacent loop nests creating additional loop nests 
worst case increase number statements program 
increase number loop nests negligible practice single application distribution created new nests 
compilation time 
accurately estimating increase compilation time caused applying algorithm compound difficult 
implementation depends efficiency infrastructure cooper 
second implementation top especially efficient 
caveats tests showed increase compilation time just parsing dependence analysis compound applied 
time required algorithm compound time required apply dependence analysis 
feel cost prohibitive highly optimizing compilers 
acm transactions programming languages systems vol 
july 
kathryn mckinley example cholesky factorization 
consider optimizing cholesky factorization kernel 
notice nested different levels 
temporal locality places group 
loopcost uses deeply nested compute cost cache lines 
loopcost selects kji best loop organization ranks nests lowest cost highest kji jki kij jik ijk 
compound tries achieve loop organization 
kji achieved permutation fusion help compound calls distribute 
loop depth distribute starts testing distribution depth loop 
go separate partitions recurrence level deeper 
memory order kji 
distribution loop places ij nest may legally interchanged memory order shown 
note system handles permutation triangular rectangular nests 
gather performance results cholesky generated possible loop permutations legal 
permutation applied minimal amount loop distribution necessary 
wolfe enumerates loop organizations wolfe 
compared matrix multiply variations observed predicted behavior 
variations due triangular loop structure compound attains loop structure best performance 

experimental results validate optimization strategy implemented algorithms executed original transformed program versions test suite simulated cache hit rates 
measured execution times architectures ibm rs model hp pa risc model 
measure ability improve locality determined memory model best locality achievable loop transformations ideal case assuming correctness ignored 
collected statistics data locality original transformed ideal programs 
statistics cache configuration ibm rs 
methodology implemented cost model transformations algorithms described memory compiler programming environment carr carr kennedy cooper kennedy 
source source translator analyzes fortran programs transforms improve cache performance 
increase precision dependence analysis perform auxiliary induction variable substitution constant propagation forward expression propagation dead code elimination pfc allen kennedy 
determines scalar expansion enable distribution 
scalar expansion integrated note execution time experiments hp pa risc able perform dependence analysis codes pfc lost platform pfc runs ibm written pl acm transactions programming languages systems vol 
july 
kij form sqrt improving data locality kji form loop distribution triangular interchange sqrt refs loopcost total total total execution times seconds vs loop organization kji jki kij jik ijk kji jki kij jik ijk kji jki kij jik ijk sun sparc intel ibm rs fig 

cholesky factorization 
acm transactions programming languages systems vol 
july 
kathryn mckinley current version transformer applied hand directed compiler 
resulting code dependence graph gather statistics perform data locality optimizations algorithm compound 
test suite programs perfect benchmarks spec benchmarks nas kernels miscellaneous programs 
ranged size lines 
execution times ibm rs ranged seconds couple hours 
transformation results table report results transforming loop nests program 
program table lists number loop nests depth considered transformation 
mem order inner loop columns reflect percentage loop nests inner loops respectively originally memory order permuted memory order fail achieve memory order 
numbers sum 
percentage loop nests program memory order transformation sum original permuted entries 
similarly inner loop sum original permuted entries percentage nests desirable innermost loop positioned correctly 
table lists number times fusion distribution applied compound algorithm 
fusion distribution applied programs 
loop fusion column number candidate nests fusion number nests fused 
candidate nests fusion adjacent nests pair nests compatible 
fusion improved group temporal locality programs find opportunities enable interchange 
adjacent loop nests candidates fusion fused nests improve reuse 
fusion applicable programs completely fused nests depth 
wave arc compound fused nests respectively 
loop dist column number loop nests distributed achieve better loop permutation number nests resulted 
compound algorithm applied distribution enabled permutation attain memory order nest innermost loop resultant nests 
compound applied distribution programs 
nests distribution enabled loop permutation position inner loop entire nest correctly creating additional nests 
bdna ocean applu cor nests resulted 
loopcost ratio table estimates potential reduction loopcost final transformed program program entire program 
remember ideal program achieves memory order nest acm transactions programming languages systems vol 
july 
table memory order statistics improving data locality mem order inner loop loop loop loopcost fusion dist ratio prog percentages perfect benchmarks adm arc bdna flo mdg mg ocean qcd spc track trfd spec benchmarks doduc fpppp mdp msp ora nas benchmarks applu buk cgm mgrid miscellaneous programs simpl wave total regard dependence constraints limitations implementation 
ignoring correctness sense best data locality achieve 
final ideal versions average ratio original loopcost transformed loopcost listed 
ratio includes loops compound transform reveals potential locality improvement 
acm transactions programming languages systems vol 
july 
kathryn mckinley may obtain memory order due reasons loop permutation illegal due dependences loop distribution followed permutation illegal due dependences loop bounds complex rectangular triangular 
nests compiler achieve memory order permutation distribution followed permutation applied dependence constraints 
rest loop bounds complex 
sophisticated dependence tests may enable algorithms transform nests 
coding styles imprecise dependence analysis factor limiting potential improvements application suite 
example dependence analysis program cgm expose potential data locality algorithm imprecision due index arrays 
program mg written linearized arrays 
coding style introduces symbolics subscript expressions dependence analysis imprecise 
inability analyze index arrays linearized arrays prevents optimizations deficiency specific system 
coding styles may inhibit optimization system 
example matrix written modular style singly nested loops enclosing function calls routines contain singly nested loops 
improve programs written style requires interprocedural optimization cooper hall optimizations currently implemented translator 
loop nests original programs memory order loop carrying reuse innermost position 
result indicates scientific programmers pay attention data locality opportunities improvement 
compiler able permute additional loop nests memory order resulting total nests memory order total inner loops memory order position 
improved data locality nests programs 
successful transformation illustrate ability transform data locality program figures 
figures characterize programs percentage nests inner loops originally memory order transform memory order 
half original programs fewer nests memory order 
transformed versions fewer nests memory order 
half nests memory order 
results dramatic 
majority programs inner loops positioned correctly best locality memory model 
transformation algorithms determine achieve memory order majority nests programs 
unfortunately ability successfully transform programs may result run time improvements reasons data sets benchmark programs tend small fit cache transformed loop nests may cpu acm transactions programming languages systems vol 
july 
number programs number programs original final percentage loop nests memory order improving data locality fig 

achieving memory order inner loop 
original final percent inner loops memory order fig 

achieving memory order loop nests 
bound memory bound optimized portions program may significantly contribute execution time 
performance results performance test suite running ibm rs model kb cache way set associative replacement policy byte cache lines 
performance test suite hp kb direct mapped cache byte cache lines 
figures detailed results kernels dnasa btrix emit vpenta 
results reported normalized execution time base time indicated 
arithmetic mean includes programs shown bar graph 
machines standard fortran compiler option compile original program version produced automatic source source transformer 
applications successfully compiled executed rs 
applications flo wave compile run hp 
applications acm transactions programming languages systems vol 
july 
kathryn mckinley normalized execution time normalized execution time original transformed fl emit apsp lpd wave mean fig 

performance results ibm rs model 
original transformed sp apsp mean fig 

performance results hp 
listed performance improvement degradation occurred 
show number applications significant performance improvements arc dnasa btrix emit vpenta simple 
results indicate data locality optimizations particularly effective vector programs programs structured emphasize vector operations cache line reuse 
predicted improvements materialize programs 
explore results simulated cache behavior determine cache hit rates test suite 
simulated cache cache kb way set associative byte cache lines cache cache kb way set associative byte cache lines 
cache chosen reveal potential optimizations small cache 
program cache determined change hit rates just optimized procedures entire program 
table ii presents rates 
small variations cache hit rates program transformations caused changes cache interference code generation 
places compiler affected cache hit rates greater emphasis 
carr wu simulate hp style cache results similar rs 
acm transactions programming languages systems vol 
july 
improving data locality final columns chose better fused versions program 
illustrated table ii reason programs improve rs due high hit ratios original programs caused small data set sizes 
cache reduced kb optimized portions significant improvements 
instance program hit rates dnasa show significant improvements optimization smaller cache barely changed larger cache 
optimizations obtained improvements program hit rates adm arc dnasa hydro simple 
improvements optimized loop nests dramatic 
improvements carry entire program unoptimized nests may dominate execution time 
measured hit ratios applying loop fusion 
kb cache fusion improved program hit rates hydro respectively 
surprised improve performance fusion subroutine entire program 
initialization routine performance usually measured 
unfortunately fusion lowered hit rates track dnasa wave degradation may due added cache conflict capacity misses loop fusion 
recognize avoid situations requires cache capacity interference analysis similar performed evaluating loop tiling coleman mckinley lam 
fusion algorithm attempts optimize reuse innermost loop level may merge array interfere overflow cache 
intend correct deficiency 
results favorable compared wolf results direct comparisons difficult combines tiling cache optimizations reports improvements relative programs scalar replacement wolf 
wolf applied permutation skewing reversal tiling perfect benchmarks dnasa decstation kb direct map cache 
results show performance degradations change adm showed small improvement execution time 
transformations degrade performance perfect programs performance arc significantly improved 
results routines dnasa similar wolf showing improvements btrix vpenta 
wolf improved mxm decstation slightly degraded performance 
wolf slowed cholesky decstation slight amount 
improve degrade kernel 
direct comparisons possible wolf cache hit rates execution times measured different architectures 
data access properties interpret results measured data access properties test suite 
report data access properties inner loops original orig ideal memory order final versions programs tables iii iv 
locality group classifies percentage displaying acm transactions programming languages systems vol 
july 
kathryn mckinley table ii 
simulated cache hit rates optimized procedures program cache cache cache cache program orig final orig final orig final orig final perfect benchmarks adm arc bdna dyfesm flo mdg mg ocean qcd spec track trfd spec benchmarks dnasa doduc fpppp hydro matrix su cor swm tomcatv nas benchmarks applu mgrid miscellaneous programs simple wave cache kb cache way byte cache line rs cache kb cache way byte cache line cold misses included 
form self reuse invariant unit stride 
contains percentage constructed partly completely group spatial reuse 
amount group reuse indicated measuring average number refs group size greater implies group temporal reuse occasionally group spatial reuse 
amount group reuse type self reuse average avg 
loopcost ratio column estimates potential improvement average avg acm transactions programming languages systems vol 
july 
table iii 
data access properties improving data locality locality groups loopcost groups refs group ratios program gp avg avg wt perfect benchmarks adm orig final ideal arc orig final ideal bdna orig final ideal dyfesm orig final ideal flo orig final ideal mdg orig final ideal mg orig final ideal ocean orig final ideal qcd orig final ideal spec orig final ideal track orig final ideal trfd orig final ideal spec benchmarks dnasa orig final ideal doduc orig final ideal fpppp orig final ideal matrix orig final ideal tomcatv orig final ideal acm transactions programming languages systems vol 
july 
kathryn mckinley table iv 
data access properties locality groups loopcost groups refs group ratios program avg avg wt nas benchmarks orig final ideal applu orig final ideal orig final ideal buk orig final ideal cgm orig final ideal orig final ideal orig final ideal mgrid orig final ideal miscellaneous programs orig final ideal orig final ideal simple orig final ideal wave orig final ideal orig final ideal nests weighted average wt uses nesting depth 
row contains totals programs 
table iii reveals applications improved arc dnasa simple significant gain self spatial reuse unit inner loop original program 
spatial locality key getting cache performance 
programmers effort ensure unit acm transactions programming languages systems vol 
july 
improving data locality stride access applications shown optimization strategy unnecessary 
having compiler compute machine dependent loop ordering variety coding styles run efficiently additional programmer effort 
programs row table iv indicates average fewer exhibited group temporal reuse inner loop displayed group spatial reuse 
programs exhibit self spatial reuse 
programs adm trfd dnasa ideal program exhibits significantly invariant reuse original final 
invariant reuse typically occurs loops reductions time step loops involved recurrences permuted 
analysis usually determines spatial reuse benefit temporal reuse carried different loops 
cases tiling may able exploit invariant reuse carried outer loops continue benefit spatial reuse carried inner loops 
analysis individual programs examine arc simple applications improved applu application degradation performance 
note specific coding styles system effectively ported rs hp pa risc 
arc fluid flow solver perfect benchmarks 
main computational routines exhibit poor cache performance due stride accesses 
main computational loop imperfect loop nest inner loops nesting depth nesting depth 
algorithm able achieve factor improvement main loop nest attaining unit stride accesses memory loops nesting depth 
improvement accounted factor application 
additional improvement illustrated attained similarly improving time critical routines 
optimization strategy need programmer select correct loop order performance 
simple dimensional hydrodynamics code 
contains loops written vectorizable form recurrence carried outer loop innermost loop 
loops exhibited poor cache performance 
compound reorders loops data locality spatial temporal vectorization achieve improvements shown 
case improvements cache performance far outweigh potential loss low level parallelism recurrence carried innermost loop 
regain lost parallelism unroll jam applied outermost loop callahan carr kennedy 
important note programmer allowed write code form type machine attain machine independent performance compiler optimization 
benchmark kernel dnasa performs gaussian elimination rows resulting spatial locality 
structure may author viewed gaussian elimination conceptually translated poor performance 
distribution permutation achieved unit stride accesses innermost loop 
programmer allowed write code acm transactions programming languages systems vol 
july 
kathryn mckinley form understands compiler handles machine dependent performance details 
applu suffers tiny degradation performance rs 
leading dimensions main data arrays small 
model predicts better performance unit stride access arrays small array dimensions give original reductions inner loop better performance rs 
locality innermost loops problem 

related abu discussed applying compiler transformations data dependence loop interchange fusion distribution tiling improve paging 
article extend validate research integrate optimizations target parallelism memory hierarchy kennedy mckinley 
extend original cost model capture types reuse 
transformation perform loop permutation integrate permutation fusion distribution reversal comprehensive approach extensive experimental results 
approach advantages previous research 
measure effectiveness approach optimization studies inherent data locality characteristics programs ability exploit 
applicable wider range programs require perfect nests nests perfect conditionals ferrante gannon li pingali wolf lam 
quicker expected worse case 
previous research focused evaluating data locality loop permutation ferrante gannon 
evaluate permutation may consider 
loop permutations typically small order find loop permutation yields best data locality 
specifies algorithm generating smaller search space 
comparison approach evaluates reuse carried loop directly determines best loop permutation 
evaluation expensive step expect algorithm faster practice 
algorithm combine loop fusion distribution loop permutation 
wolf lam unimodular transformations combination permutation skewing reversal tiling estimates temporal spatial reuse improve data locality 
prune search space ignoring loops carry reuse loops permuted due legality constraints may legal loop organizations remaining locality evaluated 
memory model potentially precise directly calculates reuse outer loops may precise ignores loop bounds known constants 
wolf lam evaluation performed perfect benchmarks routines dnasa spec benchmarks subset test suite wolf lam wolf 
difficult directly compare experiments cache optimization results include tiling scalar replacement executed different processor 
improve programs routines 
addition cache optimizations degrade programs routines acm transactions programming languages systems vol 
july 
improving data locality case 
degrade program slight applu nas benchmarks 
wolf lam experiments skewing needed reversal seldom applied wolf 
chose include skewing implemented system kennedy model drive 
integrate reversal help improve locality 
li pingali linear transformations linear mapping loop nest loop nest optimize data locality parallelism 
propose exhaustive search search space infinite transform loop nest certain program 
give details heuristic order loops locality 
offer comparison effectiveness complexity 
applying exhaustive search approach practical including loop fusion distribution create combine loop nests 
fusion improving reuse np hard kennedy mckinley 
driving heuristics cache model algorithms efficient usually find best loop organization data locality permutation fusion distribution 
compared previous gannon wolf lam cache model loses precision loopcost algorithms simplifying assumptions outer loops 
algorithms consider order outer loops loop invariance spans multiple inner loops 
practice inaccuracy affect ability derive best loop organization algorithms find compare invariance forms reuse precisely innermost loops 
position best inner loop may better outer loop organization 
imprecision exactly enables achieve single evaluation step lower algorithmic complexity 
open question precise cache model yield performance improvements practice real applications 

tiling permuting loops memory order maximizes estimated short term cache line reuse iterations inner loops 
compiler apply loop tiling combination strip mining loop interchange capture long term invariant reuse outer loops coleman mckinley irigoin lam wolf lam wolfe 
tiling applied judiciously affects scalar optimizations increases loop overhead may decrease spatial reuse tile boundaries 
cost model provides key insight guide tiling primary criterion tiling create loop invariant respect target loop 
access significantly fewer cache lines consecutive nonconsecutive making tiling worthwhile despite potential loss spatial reuse tile boundaries 
machines long cache lines may advantageous tile outer loops carry unit stride transposing matrix 
intend study cumulative effects optimizations article tiling unroll jam scalar replacement 
acm transactions programming languages systems vol 
july 
kathryn mckinley 
article presents comprehensive approach improving data locality combine loop permutation fusion distribution reversal integrated algorithm 
accept imprecision cost model algorithms simple inexpensive practice making ideal compiler 
importantly simplifying assumptions model appear hinder compiler ability exploit data locality scientific applications 
empirical results article validate accuracy cost model algorithms selecting best loop structure data locality 
addition show approach wide applicability existing fortran programs regardless original target architecture particularly vector fortran programs 
believe significant step achieving performance machine independent programming 
wish ken kennedy providing impetus guidance research 
obliged peter craig digital inspiring addition loop reversal 
grateful research group rice university software infrastructure depends 
particular appreciate assistance nathaniel mcintosh simulations 
acknowledge center research parallel computation rice university supplying computing resources experiments simulations 
wish wu ran experiments hp 
abu 
improving performance virtual memory computers 
ph thesis dept computer science univ illinois urbana champaign 
allen kennedy 
automatic loop interchange 
proceedings sig plan symposium compiler construction 
acm newyork 
allen kennedy 
automatic translation fortran programs vector form 
program lang syst oct 
banerjee 
theory loop permutations 
languages compilers parallel computing gelernter nicolau padua eds 
mit press cambridge mass 
callahan carr kennedy 
improving register allocation subscripted variables 
proceedings sigplan conference programming language design implementation 
acm newyork 
callahan cocke kennedy 
estimating interlock improving balance pipelined machines 
parall 
distrib 
comput 
aug 
carr 
memory hierarchy management 
ph thesis dept computer science rice univ houston tex carr kennedy 
improving ratio memory operations floating point operations loops 
acm trans 
program 
lang 
syst 
nov 
carr kennedy 
scalar replacement presence conditional control flow 
softw 
prac 
exper 
jan 
carr wu 
analysis loop permutation hp pa risc 
tech 
rep tr michigan technological univ houghton mich feb coleman mckinley 
tile size selection cache organization data layout 
proceedings sigplan conference programming language design implementation 
acm newyork 
acm transactions programming languages systems vol 
july 
improving data locality cooper hall hood kennedy mckinley mellor crummey torczon warren 
parallel programming environment 
proc 
ieee feb 
cooper hall kennedy 
methodology procedure cloning 
comput 
lang 
feb 
ferrante sarkar thrash 
estimating enhancing cache effectiveness 
languages compilers parallel computing th international workshop banerjee gelernter nicolau padua eds 
springer verlag berlin 
gannon jalby gallivan 
strategies cache local memory management global program transformation 
parall 
distrib 
comput 
oct 
kennedy tseng 

practical dependence testing 
proceedings sigplan conference programming language design implementation 
acm new york 
hall kennedy mckinley 
interprocedural transformations parallel code generation 
proceedings supercomputing 
ieee new york 
irigoin 
supernode partitioning 
proceedings th annual acm symposium principles programming languages 
acm newyork 
kennedy mckinley 
optimizing parallelism data locality 
proceedings acm international conference supercomputing 
acm newyork 
kennedy mckinley 
maximizing loop parallelism improving data locality loop fusion distribution 
languages compilers parallel computing banerjee gelernter nicolau padua eds 
springer verlag berlin 
kennedy mckinley tseng 

analysis transformation interactive parallel programming tool 
concurrency pract 
exper 
oct 
kuck kuhn padua wolfe 
dependence graphs compiler optimizations 
conference record th annual acm symposium principles programming languages 
acm newyork 
lam rothberg wolf 
cache performance optimizations blocked algorithms 
proceedings th international conference architectural support programming languages operating systems 
acm newyork 
li pingali 
access normalization loop restructuring numa compilers 
proceedings th international conference architectural support programming languages operating systems 
acm newyork 
mckinley 
automatic interactive parallelization 
ph thesis dept computer science rice univ houston tex warren 
hierachical basis reordering transformations 
conference record th annual acm symposium principles programming languages 
acm newyork 
wolf 
improving locality parallelism nested loops 
ph thesis dept computer science stanford univ stanford calif wolf 
data locality optimizing algorithm 
proceedings sigplan conference programming language design implementation 
acm new york 
wolfe 
advanced loop interchanging 
proceedings international conference parallel processing 
crc press boca raton fla wolfe 
iteration space tiling memory hierarchies 
proceedings rd siam conference parallel processing 
siam philadelphia pa wolfe 
tiny loop restructuring research tool 
proceedings international conference parallel processing 
crc press boca raton fla received august revised january accepted march acm transactions programming languages systems vol 
july 
