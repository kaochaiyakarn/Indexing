probabilistic kernel matrix learning mixture model kernels zhang dit yan yeung james kwok department computer science hong kong university science technology clear water bay kowloon hong kong cs ust hk addresses kernel matrix learning problem kernel methods 
model kernel matrix random positive definite matrix wishart distribution parameter matrix wishart distribution represented linear combination mutually independent matrices wishart distributions 
defines probabilistic mixture model kernels represented hierarchical model involving levels relating target kernel matrix parameter kernel matrix hyperparameter kernel matrices mixture components 
cases linear combination wishart matrices longer wishart propose approximation method employing new wishart matrix approximate parameter matrix preserving second moments linear combination wishart matrices 
wishart matrix estimated kernel matrix learning solved expectation maximization em learning algorithm infer missing data kernel matrix unknown parameter matrix distribution 
study kernel matrix learning problem context classification problems kernel nearest neighbor classifier 
classification experiments benchmark data sets show promising results 
furthermore method opened possible direction addressing kernel model selection kernel parameter estimation problems 
keywords probabilistic kernel matrix learning mixture model kernels wishart distribution em algo rithm kernel classifier corresponding author zhang fax email cs ust hk contents kernel methods 
kernel learning 
research agenda 
mathematical background kronecker product 
wishart distribution 
learning generative model kernel matrix probabilistic generative model 
em algorithm kernel matrix learning 
learning mixture model kernels mixture model kernels 
approximating linear combination wishart matrices 
kernel matrix learning mixture model 
kernel matrix learning classification problems defining target kernel matrix 
defining hyperparameter matrices 
predicting labels test patterns 
experiments experiment mixtures different types kernels 
experiment mixtures type kernels 
concluding remarks kernel methods years kernel methods increasingly popular machine learning due conceptual simplicity strong theoretical foundations 
kernel learning models algorithms support vector machines svm kernel principal component analysis pca kernel fisher discriminant analysis fda nonlinearly mapping data input space higher dimensional feature space defined mercer kernel relatively simple algorithms traditional linear algorithms applied :10.1.1.15.9362
nonlinear mapping simple linear operations feature space correspond powerful complex nonlinear operations input space 
mapping relates kernel function mapping xi xj xi xj inner product xi xj words explicitly carrying nonlinear mapping possibly infinite dimensional called kernel trick allows inner products computed input vectors 
xi denote finite set input vectors define positive semi definite kernel matrix gram matrix kij kij xi xj 
kernel learning kernel plays central role kernel methods poor kernel choice significantly degrade performance 
existing kernel methods require appropriate kernel manually selected ad vance 
better approach requires parametric form kernel function specified leaving kernel parameters adjustable course learning process 
examples commonly parametric kernels include polynomial gaussian kernel polynomial order gaus sian width corresponding kernel parameters 
typically adaptation kernel parameters performed optimizing certain quality functional generalization error bound 
adapting kernel parameters development allows adapting form kernel 
transductive setting needs consider kernel evaluations set training testing data kernel learning problem simplified learning just kernel matrix 
introducing notion alignment cristianini proposed kernel matrix learning method optimizes coefficients spectral decomposition full kernel matrix training test data 
direct connection alignment generalization error 
lanckriet derived generalization bound choosing kernel formulated kernel matrix learning problem convex optimization problem prone local minima :10.1.1.13.6524
advances interior point methods solving convex programming problems semi definite programming sdp problem computationally expensive large data sets 
sdp bousquet herrmann proposed simple efficient gradient descent algorithm orders magnitude faster typical sdp solver 
crammer proposed kernel matrix learning method boosting accurate kernel constructed simple base kernels solving generalized eigenvector problem 
tsuda considered kernel matrix completion problem missing data problem developed parametric approach em algorithm information geometry positive definite matrices :10.1.1.37.8662
kandola extended alignment optimization method transductive setting inductive setting 
proposed probabilistic approach kernel matrix learning problem devising probabilistic generative model kernel matrix random positive definite matrix wishart distribution 
kernel matrix learning problem reduced missing data problem maximum posteriori map framework 
kernel matrix training data devised expectation maximization em algorithm inferring kernel matrix test data kernel matrix relating training data test data parameter matrix wishart distribution 
alternative approach tanner wong algorithm markov chain monte carlo mcmc method proposed 
strong bayesian flavors 
learning kernel matrix approach formulate problem learning kernel function directly 
theory reproducing kernels ong introduced renamed defining space kernels reproducing kernel hilbert space 
kernel learning problem reduced optimization problem 
method inductive setting 
research agenda existing kernel matrix learning methods constrain target kernel weighted combination fixed base kernels :10.1.1.13.6524
learning problem simplified estimation weighting coefficients 
motivated propose extension probabilistic approach incorporating kernel mixture model section 
kernel learning methods probabilistic formulation 
specifically consider parameter matrix wishart distribution linear combination mutually independent matrices wishart distributions 
cases linear combination wishart matrices longer wishart 
intractable obtain analytical solution kernel mixture model 
inspired previous works new wishart matrix approximate linear combination wishart matrices 
new wishart matrix preserves second moments linear combination approximation optimal information theoretic sense 
applying method classification problem section worth noting kernel learning forms integral part entire classifier training process 
contrary traditional kernel classification methods svm kernel fda treat model selection kernel selection problem classifier training problem separately successive processes :10.1.1.15.9362
typically start selecting appropriate kernel empirically learning target kernel adaptively 
classifier built selected learned kernel 
aspects computational cost classification accuracy wants deal processes jointly single paradigm 
attempts address issue 
experiments study different settings kernel mixture 
setting different components mixture correspond positive definite matrices induced kernel functions different forms 
second setting components correspond kernel functions form different parameters 
see setting regarded kernel model selection second setting regarded kernel parameter estimation 
rest organized follows 
relevant mathematical background introduced section 
probabilistic kernel matrix learning approach proposed briefly reviewed section 
section presents extension development kernel mixture model section applies context classification problems 
experimental results section section contains concluding remarks 
mathematical background self contained briefly review relevant terminologies results matrix algebra matrix variate distributions 
detailed discussions readers referred chapter chapter 
denote matrix vector boldface uppercase letter boldface lowercase letter respectively 
matrix 
denote transpose trace tr determinant inverse exists addition write positive definite positive semi definite 
kronecker product definition aij matrix bij matrix 
kronecker product denoted ps qt matrix defined 
qb qb 
ap ap 
definition yij matrix vec pq vector defined vec 
yp 
yp 

formed stacking columns form column vector 
yij symmetric matrix dimensional column vector defined 

formed elements including diagonal entries taken columnwise 
definition permutation matrix known commutation matrix hst order st st defined hst eij ij eij denotes matrix unit element th entry zero 
important properties kronecker product summarized 
proposition 
ax 
nonsingular tr tr tr 
positive semi definite proposition wishart distribution tr hst tr 
definition random symmetric positive definite matrix said distributed wishart distribution denoted wm parameters density function exp tr degree freedom positive definite parameter matrix rm normalization term denoting gamma function 
definition random symmetric positive definite matrix said distributed inverted wishart distribution denoted parameters density function exp tr 
proposition wm cov vec 
denotes expectation cov covariance matrix identity matrix permutation matrix 
theorem wm nonsingular matrix wm 
theorem 
wk independent wishart matrices wk wm rk wk wm rk 
theorem wm 
iw 
learning generative model kernel matrix section briefly review probabilistic approach kernel matrix learning proposed 
kernel matrix learning methods focus transductive setting :10.1.1.13.6524
probabilistic generative model denote input parts training test sets xi input vectors xi input vectors respectively 
classification problem outputs corresponding xi outputs corresponding xi unknown 
kernel matrix defined partitioned matrix relating training data test data kernel matrices defined training test sets respectively 
problem interest infer 
probabilistic formulation adopted 
particular assumed random wishart matrix distributed wn user defined degree freedom 
prior distribution parameter matrix assumed inverted wishart distribution 
equivalently theorem tells distributed wn 
em algorithm kernel matrix learning prior distribution parameter matrix available kernel matrix learning formulated missing data problem solved finding map estimate 
devised em algorithm inferring missing parts kernel matrix unknown parameter matrix 
partition similar way obtain schur complements respectively 
th estimate parameter matrix step em algorithm seeks obtain th estimate missing data 
hand step seeks obtain th estimate parameter matrix th estimate missing data em algorithm iterates alternating step step convergence 
learning mixture model kernels mixture model kernels kernel learning literature target kernel constrained weighted combination fixed base kernels 
common choice base kernels mi 

set independent vectors 
target kernel matrix constrained form imi learning problem subsequently simplified estimation usually assumed orthogonal implying respectively eigenvectors eigenvalues generally mi different kernel matrices resulting say different kernels functions 
section wn kmk nonnegative constants mk mutually independent random wishart matrices mk mk wn 
defines probabilistic mixture model kernels 
notice mixture model differs usual ones gaussian mixture model random matrices random vectors 
setting matrix theorem easy see kmk wn 
distribution wishart theorem 
unfortunately assumption restrictive 
part observation kernel mixture model contains missing data difficult infer model way gaussian mixture models 
approach approximate distribution distribution random wishart matrix section wn em algorithm section performed section 
depicts hierarchical model relationships kernel sub matrices parameters mk hyperparameters 


mixture model kernels 
known 
approximating linear combination wishart matrices introduced section distribution single wishart matrix approximate 
problem obtain parameters 
proposition second moments cov vec hn cov vec hn 
equating moments cov vec cov vec obtain hn 
note degree freedom unknown parameters correspond different equations 
equation directly equate respective traces sides 
propositions get tr tr substituting obtain tr tr 
ktr tr tr ktr 
substituting back obtain estimate 
previously tan gupta proposed methods approximation problem comparing expectations generalized variances 
method equates traces generalized variances similar 
method hand equates determinants generalized variances 
applied get hn hn subsequently 
requires computing determinant matrix intractable large 
unfortunately usually case kernel matrix learning 
recommend equation kernel matrix learning problem 
recall definition condition hold wishart distribution wn meaningful 
condition proved 
theorem guarantees condition holds methods theorem assume 
nonzero 
defined proof appendix 
traditional mixture models shall assume sequel 
extreme case 
easy see degenerate respectively 
mixture model reduces case 
words mixture model extension probabilistic kernel matrix learning model 
kernel matrix learning mixture model obtaining parameters approximating linear combination wishart matrices directly apply em algorithm section 
summarizes complete learning algorithm input give mixture approximation estimate obtain wn 
em learning iterate alternating step step substituted output obtain calculate 
proposed algorithm learning kernel mixture 
kernel matrix learning classification problems general definitions kernel matrix hyperparameter matrices depend concerned problem prior knowledge available 
kernel matrix learning framework limited classification problem focus classification problem illustrate kernel matrix learning performed 
typically kernel classification methods svm kernel fda select learn appropriate kernel data empirically train classifier kernel :10.1.1.15.9362
problems kernel selection classifier training treated entirely separate processes 
integrate problems tightly kernel learning forms integral part entire classifier training process 
defining target kernel matrix denote input parts training test sets xi respectively 
similarly output parts yi xi yi classification problem outputs yi unknown estimated 
denote entire training test sets xi yi xi yi respectively 
furthermore ki ko denote input output kernel matrices defined combined input set combined output set respectively 
kernel matrix regarded affinity matrix point pairs ki measures similarity input vectors ko measures similarity output vectors 
similarity kernel matrices ki ko turn measured alignment frobenius norm matrices 
ki ko ki ki ko ko motivates directly employ output kernel matrix ko define section 
specific define defined defined defined 
specified training data obtained kernel matrix learning algorithm 
captures class label information training data captures class label information test data characterizes similarity class labels training test data 
expect effective perform classification employing information 
defining hyperparameter matrices major decision select hyperparameter matrices 
set different choices input kernel matrix ki 
illustrates hierarchical model relates input kernel matrices hyperparameter kernel matrices output kernel matrix target kernel matrix indirectly hidden kernel matrix called parameter kernel matrix 
model clearly different kernel alignment illustrated model involves hidden kernel 
hierarchical model advantages 
see kernel matrix learning algorithm summarized hyperparameter kernel matrices play role regularization term overfitting avoided 
second argued crammer alignment may measure classification tasks classifier may achieve zero error rate alignment far 
point illustrated experiments discussed section 
certain extent method overcome problem 
target kernel output kernel parameter kernel hidden kernel hierarchical model 
hyperparameter kernels input kernels target kernel kernels kernel alignment 
comparison proposed hierarchical model kernel alignment 
shaded parts known unshared parts 
predicting labels test patterns discussions propose classification method tightly integrates kernel selection classifier training 
basic process classification method illustrated 
easy see method fact kernel version nearest neighbor classifier called kernel nearest neighbor classifier knnc 
kernel methods knnc works feature space explicit feature vectors 
kernel classification methods svm kernel fda knnc require solving quadratic programming problem eigen decomposition problem :10.1.1.15.9362
consequence method simpler significantly efficient 
input give xi yi xi kernel learning kernel matrix learning algorithm estimate missing entries classification assign yi yj arg maxj ij 
kernel nearest neighbor classifier possibility kernel version nearest mean classifier called kernel nearest mean classifier classification 
apparently seen nearest mean classifier kernel induced feature space 
denote size class denoted ci ni feature space class mean ci mi ni xj ci xj 
allocates data point test set ci mi mj mi imi mi xj xl ci xj xl ni xj ci xj kernel trick 
problem am interested implementation feature space corresponding output space 
simply done replacing xi respectively 
experiments experiments define employing ideal kernel ij output part training set 
furthermore set yi yj ij yi yj 
small positive number term avoiding possible singularity 
set em algorithm run iterations 
notice known fixed required compute inverse 
remaining steps em algorithm require basic matrix operations computational requirement considerably low 
test set classification accuracy report alignment estimated kernel sub matrix ideal kernel matrix test set results reported sequel averages random splits data data training testing 
standard deviations reported inside brackets tables 
experiment mixtures different types kernels experiment mixtures components hyperparameter matrices corresponding gaussian kernel polynomial kernel linear kernel exp xi xj respectively 
set set 
experiments performed benchmark data sets wisconsin breast cancer ionosphere soybean wine sonar iris uci machine learning repository experiments performed non mixture setting hyperparameter matrices 
compare hyperparameter kernel matrix corresponding target kernel obtained kernel matrix learning 
kernel matrix learning em learning algorithm equations 
comparison kernel alignment classification accuracy 
table shows results data sets 
rows marked gaussian polynomial linear show corresponding results hyperparameter kernel choices 
see classification accuracy obtained target kernel learned outperforms corresponding hyperparameter kernel directly kernel learning 
case kernel alignment criterion comparison 
example wisconsin breast cancer ionosphere data sets alignment values target kernel polynomial hyperparameter kernel significantly lower corresponding values polynomial kernel directly uci data sets downloaded www ics uci edu mlearn mlrepository html 
highest alignment values classification accuracies kernel learning methods shown boldface 
kernel learning 
sonar soybean data sets target kernel gives lower alignment values choices hyperparameter kernel 
supports argument crammer alignment may satisfactory measure classification tasks 
conjecture higher alignment sufficient necessary obtaining higher classification accuracy distance classification methods 
words higher alignment expected give higher classification accuracy higher classification accuracy necessarily require higher alignment 
terms classification accuracy gaussian kernel best hyperparameter kernel choice wisconsin breast cancer ionosphere sonar soybean data sets polynomial kernel best iris data set linear kernel best wine data set 
results show exist single kernel choice best data sets 
apply kernel mixture model data sets 
table shows estimated values data sets 
clearly inequality holds data sets 
kernel alignment values classification accuracies kernel mixture model shown rows table marked mixture 
mixture model kernel learning simply kernel matrix terms classification accuracy target kernel obtained kernel learning outperforms corresponding hyperparameter kernel kernel learning 
comparing different cases kernel learning performance mixture model intermediate non mixture cases 
suggests kernel mixture model tends achieve performance tradeoff individual non mixture models 
experiment mixtures type kernels second experiment kernel mixtures components 
corresponding rameter matrices 
kernel function gaussian kernel different kernel parameter settings 
table kernel alignment values test set accuracies obtained experiment 
highest alignment values accuracies shown boldface 
data set kernel target kernel hyperparameter kernel alignment accuracy alignment accuracy breast cancer gaussian polynomial linear mixture ionosphere gaussian polynomial linear mixture soybean gaussian polynomial linear mixture wine gaussian polynomial linear mixture sonar gaussian polynomial linear mixture iris gaussian polynomial linear mixture table estimated values kernel mixture model experiment 
breast cancer ionosphere soybean wine sonar iris section equal set 
experiments performed largest data sets breast cancer ionosphere sonar 
seen table test set accuracies obtained learned kernel mixture close best setting alignment values obtained learned kernel mixtures respectively wisconsin breast cancer ionosphere sonar data sets 
significantly higher non mixture cases 
estimated values data sets respectively 
recall set experiment values data sets respectively 
happens related equal 
interesting phenomenon 
may explained revisiting learning procedure 
approximates mixture consisting single kernel matrix 
replace mk wn mk wn 
theorems follows kmk mk wn 
interpret phenomenon kmk approaching wishart distribution 
furthermore hyperparameter matrix wishart distribution approaches gaussian kernel matrix appropriate width 
think kernel mixture model provides effective way choosing appropriate parameter value gaussian kernel 
particular estimate interval appropriate value width belongs select values interval apply model values 
contrary mentioned phenomenon occur experiment estimated values data sets see table 
implies new hyperparameter matrix differing gaussian polynomial linear kernels constructed 
opens possible direction addressing kernel selection problem 
table kernel alignment values test set accuracies obtained second experiment 
data set parameter target kernel hyperparameter kernel alignment accuracy alignment accuracy breast cancer mixture ionosphere mixture sonar mixture concluding remarks extended previous probabilistic kernel matrix learning representing parameter matrix wishart distribution linear combination mutually independent wishart matrices 
probabilistic mixture model kernels seen hierarchical model levels relating target kernel matrix parameter kernel matrix hyperparameter matrices mixture model 
particular context classification problems define hyperparameter kernel matrices target kernel matrix input space output space respectively kernel nearest neighbor mean classifier prediction 
efficacy methods demonstrated experiments benchmark data sets 
recall mixing coefficients model specified advance 
exper iments number mixture components 
general take nonnegative values 
better mixing coefficients preferably adjusted adaptively 
traditional finite mixture models gaussian mixture model component corresponds distribution random vector missing data labels observations component mixture model corresponds distribution random matrix missing data part observation 
casting adaptation current em algorithm non trivial 
obviously solving problem essential design general method kernel model selection kernel parameter estimation 
interesting topic research 
classification method represents attempt integrate kernel selection classifier construction 
way treating kernel selection classifier construction single process separate processes potentially fruitful research direction kernel procedures 
appendix proof theorem appendix prove condition defined 
quote result 
lemma min denote ordered singular values ab min min ab min ab 
ab 
furthermore positive definite matrices denote ordered eigenvalues ab ab ab ab 
proof theorem 
prove case defined 
lemma tr ab ab ab ab tr tr 
tr ab tr tr 
rewrite tr ktr tr ktr tr tr ktr clear second term nonnegative tr tr tr numerator term shows tr ktr tr ktr defined consider proposition know 
result symmetric positive semi definite result proposition amari :10.1.1.37.8662
information geometry em em algorithms neural networks 
neural networks 

theory kernels 
transactions american mathematical society 

generalized discriminant analysis kernel approach 
neural compu tation 
bousquet herrmann 
complexity learning kernel matrix 
advances neural information processing systems cambridge ma 
mit press 
chapelle vapnik bousquet mukherjee 
choosing multiple parameters support vector machines 
machine learning 
cortes vapnik 
support vector networks 
machine learning 
crammer keshet singer 
kernel design boosting 
advances neural information processing systems cambridge ma 
mit press 
cristianini kandola elisseeff shawe taylor 
kernel target alignment 
dietterich becker ghahramani editors advances neural information processing systems cambridge ma 
mit press 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
duda hart stork 
pattern classification 
john wiley sons new york second edition 
gupta 
matrix variate distributions 
chapman hall crc 
horn johnson 
topics matrix analysis 
cambridge university press cambridge uk 
kandola shawe taylor 
refining kernels regression uneven classification problems 
bishop frey editors proceedings ninth international workshop artificial intelligence statistics key west fl 
kandola shawe taylor cristianini 
optimizing kernel alignment combinations kernels 
neurocolt technical report nc tr 

multivariate generalization statistics mean square successive difference 
communications statistics theory methods 
kwok 
evidence framework applied support vector machines 
ieee transactions neural networks 
lanckriet cristianini el ghaoui bartlett jordan :10.1.1.13.6524
learning kernel ma trix semi definite programming 
proceedings th international conference machine learning pages 
mclachlan krishnan 
em algorithm extensions 
john wiley sons new york 
mclachlan peel 
finite mixture models 
john wiley sons new york 
ong smola 
machine learning 
th international conference machine learning 
ong smola williamson 

advances neural information processing systems 
sch lkopf smola klaus robert ller 
nonlinear component analysis kernel eigenvalue problem 
neural computation 
sch lkopf smola 
learning kernels 
mit press 
tan gupta 
approximating linear combination central wishart matrices positive coefficients 
communications statistics theory methods 
tanner wong 
calculation posterior distributions data augmentation discussion 
journal american statistical association 
tsuda asai 
em algorithm kernel matrix completion auxiliary data 
journal machine learning research 
vapnik 
statistical learning theory 
john wiley sons new york 
zhang yeung kwok 
homotopy kernels generalized eigenproblem bayesian inference tanner wong algorithm 
technical report cs department computer science hong kong university science technology 
zhang yeung kwok 
probabilistic kernel matrix learning 
technical report cs department computer science hong kong university science technology 
available ftp ftp cs ust hk pub techreport tr ps gz 

