parallel multigrid summation body problem jes department computer science engineering university notre dame notre dame usa thierry department informatics university bergen norway parallel multigrid summation method body problem 
method works vacuum periodic boundary conditions 
hierarchical decomposition computational kernels multiple grids 
low accuracy calculations appropriate molecular dynamics sequential implementation faster fast multipole particle mesh ewald pme 
parallel implementation scalable pme comparable fast multipole 
method combined multiple time stepping integrators produce powerful simulation protocol simulation biological molecules materials 
parallel implementation mpi tested variety clusters shared memory computers 
available open source sourceforge net 
auxiliary tool allows automatic selection optimal parameters molecular systems accuracies required available cse nd edu 
key words parallel body solvers multigrid summation fast electrostatic solvers particle mesh ewald method fast parallel evaluation body problem 
core problem sum slowly decaying pair wise interactions particles 
motion planets galaxies folding proteins determination electronic structures materials examples applications need solve body problem 
preprint submitted elsevier science february body problem solved cutoff distance origin 
cases cutoff affects results interest significantly 
example gravitational problems cutoffs may exclude important gravitational effects large bodies 
true molecular systems particularly biological relevance dna proteins simulations cutoff lead artifacts simulations peptides proteins fail membrane simulations 
consider isolated periodically replicated systems 
isolated systems straightforward computation pairwise interactions leads algorithm 
hierarchical decomposition space leads barnes hut log algorithm 
clever interpolation harmonics leads fast multipole method fmm 
periodically replicated systems decomposition real fourier terms leads ewald algorithm 
solution fourier part grid fft leads log particle mesh ewald pme algorithm 
extensions fast multipole exist periodic systems 
extension periodic boundary conditions parallelization multigrid summation technique mg 
method implemented tested software framework available open source project tests water systems proteins ranging atoms show method produces correct dynamics thermodynamics isolated periodic systems 
tests systems atoms show potential massive simulations enabled multigrid summation technique 
mg highly parallelizable 
implementation uses mpi efficient tested shared distributed memory computers 
concretely ibm turbo mg exhibits parallel efficiency processors pme highly optimized parallel library fftw parallel efficiency little 
similarly beowulf cluster myrinet mg parallel efficiency processors pme 
mg easier recommender system called allows user prepare system simulation 
chooses optimal parameters governing rules run time fine tuning rest organized follows sections give mathematical description body problem section describes mg method section discusses related section gives parallel imple sourceforge net fftw org cse nd edu mentation mg section reports computational experiments mg fast electrostatic solvers section contains discussion results 
body problem isolated systems body problem isolated molecular system particles consists computing electrostatic energy electrostatic rn rj ri position partial charge th atom ri qi andthe dielectric coefficient 
set pairwise interactions excluded th atom 
gradient potential energy force 
compute molecular dynamics newton equations motion ma 
core computation spent evaluating long range electrostatic forces time steps 
body problem replicated systems modeling molecular systems common consider system infinitely replicated space periodic boundary conditions pbc 
advantage pbc system size reduced compared systems boundary conditions 
periodicity may introduce spurious results requires careful study 
liquid simulations periodicity effects minimal 
charged polar systems high dielectric medium show minimal periodicity effects 
careful studies low dielectric biomolecular systems needed 
case replicated system periodic boundary conditions problem solved electrostatic rn rj ri ml sum periodic cells index self interactions excluded length periodic box cell dimensions primed sum excludes interactions pairs exclusion list 
conditionally convergent sum physically meaningful interpretation 
ewald splits sum rapidly convergent sums cf 
eq 

real reciprocal space parts fourier domain 
ewald chooses softening function rij rj ri erf rij real part short ranged rij rij rij exp rij ds 
erf rij rij solved directly reciprocal space part solved fourier series exp qj exp rj parameter controls computation done real space part 
optimal value ewald summation obtained varying cutoff square root periodic cell length 
case complexity 
approaches solving ewald summation solution poisson equations pbc large finite array copies simulation cell immersed dielectric medium 
particle mesh pm method evaluates potential particles interpolating charges regular grid 
fft obtain solution discretized poisson equation 
interactions nearby particles poorly represented 
particle mesh method splits contributions short range long range solves short range part directly 
particle mesh ewald pme method chooses splitting parameter short ranged part interpolates fourier series mesh allowing log fft smooth part exp fj qj fj charges grid grid positions 
multilevel summation methods multilevel methods recursively separate length scales problem approximate slower longer range scales coarser representation achieve fast computation 
kinds multilevel methods cell methods fast multipole algorithm oct tree decomposition space ii grid methods brandt fast summation method multiple grid hierarchy 
steps methods follows separation length scales short range smooth 
short range calculations computed directly 
cell tree methods cells considered smooth slowly varying separated mathematical criterion 
multiple grid method splits computational kernel short range smooth parts cutoff rc switching functions example smooth short ranged local smooth smooth local smooth vanishes rc 
coarsening 
involves approximating smooth part coarser grid 
cell methods barnes hut log algorithm approximates source multipole algorithms approximate source destination 
coarsening normally involves interpolation multipole algorithms truncated taylor interpolation exploits harmonicity potential permit inexpensive basis spherical harmonic potentials 
coarsening multi grid algorithms involves interpolation basis functions grids 
approach easy produce forces energies continuous function positions cf 
difficult multipole methods cf 

continuous forces derivatives potential energy necessary stability molecular dynamics md integrators 
hierarchical decomposition involves recursive separation length scales coarsening problem scale 
follows steps described detailed mg method 
henceforth assumed vacuum boundary conditions 
changes needed making algorithm pbc noted 
separation length scales separation length scales done switching function brings computational kernel smoothly zero cutoff distance rc 
switching functions varying degrees smoothness computational cost cf 
examples 
coarsening kernel smooth approximated source smooth smooth rh rh points grid grid point separation piecewise polynomials local support grid cells 
coefficients basis functions smooth rh approximated destination smooth rh smooth rh rh resulting double sum grid cells necessary interpolation smooth smooth rh rh 
charges grid point defined qh qi ri 
definitions smooth part electrostatic energy written simply qh rh rh 
sum particle pairs reduced sum grid point pairs 
hierarchical decomposition smoothed kernel particle level smooth approximated fine 
superscript indicates grid level 
redo level grid smooth splitting eq 
smooth smooth smooth smooth local smooth local smooth smooth zero rc 
process applied recursively general grid level kernels defined smooth ri rj gsk rj ri rj ri sk 
rj ri 
gsk rj ri softening function polynomial parameterized sk gsk sk sk 
softening distance level sk rc cf fig smooth rj ri piecewise polynomial continuity piecewise cubic 
local part smoothed kernels defined follows local ri rj local ri rj rj ri smooth rj ri rj ri rc smooth rj ri gsk rj ri rj ri sk 
note local ri rj pre computed represented single table corresponding pre factor imposing constant coarsening ratio 
linear algebra view informative cast eq 
vector matrix vector product gq vector particle partial charges electrostatic potential kernel defined gij rj ri included interactions 
original kernel smoothed kernel smoothed kernel fig 

plot original kernel smoothed kernels smooth ri rj softening distances ands gs continuous 
gs symmetric matrix values green function laplacian zeros due interaction pairs excluded 
isnot bounded small multigrid formulation consists approximation matrix sum sparse matrices 
mg approximates smooth smooth smooth sparse matrix number nonzeros proportional matrix smooth slowly varying elements smooth ij smooth rj ri approximate sparse matrix smooth level grid finest grid 
particles considered level grid notation 
mg ik uses sparse interpolation matrix sparse adjoint interpolation matrix operators smooth 
note choose smooth rm gk smooth rh rh 
charges grid level particle charges 
similarly represents electrostatic energy values grid level andv values particles 
lattice particle positions ri 
recursive mg scheme levels smooth rk local fi qi jv 
ri ri denotes lattice points ri local support similarly eq 
cast mq ij rj ri ml included interactions lattice cells summation approximated finite number terms corresponding neighborhood periodic cell 
case algorithm handles pbc doing coordinates wrapped dimension periodically extended 
interpolation grid points support 
exception gl local coarsest grid wrap 
multiple copies periodic cell included summations 
support includes closest copy boundary grid point 
running grid points boundary grid points neighbor copies considered 
consequence pbc grids cover area level vacuum area covered increases increasing level coarsest grid covers grids points finer grid points outside area support doing coarser grid 
reformulating mg scheme described algorithm defining cycle 
cycle reflects order grids algorithm telescopes coarsest grid works way back finest grid describing pseudo code main handles aggregation charges particles coarsest grid interpolation kernel values coarsest grid particles 
lastly computes force contributions total energy 
multiscale recursively performs aggregation charges interpolation potential values local correction depicted 
due uniformity grids interpolation coefficients pre calculated represented dimensional set coefficients 
holds local correction softening distance proportional corresponding mesh size 
assuming uniform grids constant coarsening ratio order interpolation mesh size finest grid average particle density bounded constant particle level level ak cp cg ci 
denotes local correction particle level ak local corrections grid level potential evaluation coarsest grid 
aki represent interpolation aggregation 
cp cg ci constants weigh different 
total aka cp cg total bounded 
general deduce ak ak dominated terms order np essentially interpolation particle positions finest grid 
total 
accuracy governed size interpolation error smooth part 
interpolation error depends smoothness kernel size interpolation order assuming continuous kernel order accurate interpolation relative force error smooth total relative force error part 
point charges force values fig 

multilevel scheme multi grid algorithm 
aggregate coarser grids compute potential induced coarsest grid interpolate potential values coarser grids local corrections 
detailed error estimation mg 
pp 
mg performance vacuum systems compared direct method dimensional system charges 
mg compared fast multi pole method implemented parallel program water systems 
expected experiments show mg converges direct method error drops zero monotonously 
increases monotonously large encompass pairs remains constant 
furthermore indicated mg produces stable simulations molecular dynamics lower accuracy multi pole methods 
implementation mg order interpolation schemes chosen generic interpolation routines operate particles grid grid particles grid grid 
internally algorithm keeps multi grid structure contains hierarchy grids 
interpolation particles grid grid data structures reused pme 
mg competitive periodic boundary conditions times faster pme systems atoms cf 

enabled material science simulations millions atoms intractable cf 
pp 
ff 
related multigrid summation techniques earliest uses multilevel matrix multiplication context fast solution integral equations 
np method order interpolation applied kernel ln early vectorized version mg summation kernel 
method extended oscillatory kernels cost log 
low accuracy adaptive multigrid summation developed 
low accuracy computations method competes favorably fast multipole implementation board collaborators high accuracy 
speaking parallelism authors note structure adaptive multigrid algorithm 
highly parallel levels parallelism exploited 
include calculation force field different points scheme tandem parallel evaluation summation point level parallel reduction evaluation sum different levels atomic level sequence grids parallel additive property sum parallelism 
implementation mg method context md 
smooth interpolation show mg better fast multipole md better smoothness properties numerical integrator equations motion stable 
achieve considerably faster implementation mg 
version mg summation extends pbc parallelized 
iterative multigrid method uses pm approach transforms problem mesh solution elliptic pde uses iterative multigrid solver achieving solver 
scheme algorithm 
method expected expensive scalable parallel due iterative solver fewer levels parallelism available method 
slower pme single processor method faster pme single processor 
similar method groot implement fast grid context dissipative particle dynamics 
ewald multipole methods important comparison fmm particle particle particle mesh ewald 
empirically test reasonably efficient implementations methods 
method similar spirit pme 
faster fmm easier implement efficiently relies commonly available software fft subroutines 
ewald method easily implemented parallel architectures method clear choice large systems 
similarities pme smooth pme influence methods parameters accuracy described 
conclude flexible approach capable achieving largest accuracy force interpolation variant method 
similar reached 
point cases expensive higher order interpolation perform extra fft smooth pme preferable 
particular holds parallel implementations fft scalable 
discuss variant pm methods fast fourier poisson method york yang 
avoids errors interpolation sampling gaussian sources directly grid point clever mathematical identities 
higher accuracy pme costly 
advantages multipole methods particularly context parallel processing reviewed 
reviews fast summation techniques 
ewald summation pme substantial errors close range interactions hydrogen bond pairs part interaction computed reciprocal space 
may limit mts implementations 
cell methods continued improved multipole extended pbc hierarchical cell method developed conserves momentum symmetric cartesian taylor approximations tree particle mesh algorithm experimentally shown faster elegant adaptive implemented vacuum pbc shown easier implement multipole 
parallel body solvers influential parallel body solvers parallel cell code 
board collaborators produced robust paral lel software multipole method 
book early area 
exploits hierarchical decomposition space cell methods provide high compression ratios larger raw data enables faster storage retrieval simulation data 
force decomposition basis implementation parallel mg described :10.1.1.35.6971
adopted ibm project computer 
advantage simply load balanced performs irregular geometries 
improves scaling due replicated data techniques simpler full spatial decomposition 
comparisons static dynamic decomposition techniques appear 
adaptive applications molecular dynamics astrophysics simulations combinations static dynamic load balance proven useful example combination static decomposition grid dynamic balance particles parallel code scales thousands processors 
similar ideas appear 
parallel iterative multigrid pde solvers 
solvers different mg summation share multiple grid structure similar parallelization issues 
papers deal clever partitionings adaptive grids 
comparison techniques try compensate deficiency parallelism coarser grids multigrid solvers 
evaluate multiple coarse grid technique advocated additive mg allows computation grids simultaneously 
analysis suggests standard algorithms substantially efficient method highly impractical number processors 
fully dynamic load balancing advocated researchers example context quantum chemistry context body solver heterogeneous grid configurations 
parallel multigrid summation scalable parallelization multigrid summation domain decomposition spatial data distribution node works local domain propagates results tree fashion way 
local domain consists assigned domain overlap ghost points ensure correct interpolation 
overlap depends interpolation order softening distance computing local corrections 
order achieve high scalability needs minimize overlap assign idle nodes example computing direct part 
combination atom force decomposition 
step mg algorithm grid distributed nodes 
local contributions propagated level mg algorithm proceeds 
involves synchronization nodes going grid 
direct sum direct sum consists pair wise interactions softening distance intra molecular correction adds effects bond interactions 
easily distributed nodes pairs statically topology system 
pairwise interactions determined statically number location interactions softening distance vary function time 
cell algorithm called geometric hashing split sum interactions small parts solve problem time 
part consists interactions cell neighboring cells softening distance 
simple distribution modulo function gives load balancing 
interpolation parallelization interpolation atom decomposition 
requires global sum grid values proceed step mg algorithm interpolation force contributions need global summation force energy contributions updated lazy manner time step needed 
smooth part sum computation smooth part mg consists interpolation steps grids steps local corrections potential charge grids step direct part coarsest grid step 
dimensions grids softening distance exactly predict single mg step 
enables distribute evenly nodes communication scheduler 
approach requires global reduction mg step avoid idle nodes distribution 
total nodes sequential run case distributed implementation requiring ghost grid points 
coarse grids amount increases significantly grid size comparable number ghost points 
testing useful platform implement test fast electrostatic methods mg plain ewald summation pme different computer systems architectures 
implementations parallelized reasonably optimized commonalities algorithms exploited 
example interpolation routines common mg pme 
algorithms computing direct sums cell lists common algorithms 
ported aix irix hp ux solaris linux windows vendor specific compilers possible gnu 
test cases consist types problems 
proteins surrounded water 
second water molecules 
coulomb crystals consisting ions outer field holding 
time accuracy measured relative force error relative total energy error 
ewald method assumed standard comparison experiments done pbc direct method comparison experiments done vacuum 
figures show parallel speedup scalability mg implemented applied coulomb crystal systems defined computationally dominant electrostatic part electric field linear complexity 
simulations performed ibm turbo 
note sequential speedup order compared direct method lower accuracy speedup order observed 
compares run time mg smooth pme 
experiments tip water model atoms 
mg method tested periodic boundary conditions vacuum pme tested periodic boundary conditions 
tests performed pentium processors running linux 
continuous switching function tests 
compare parallel efficiency mg pme implemented linux clusters 
tests conducted linux cluster speedup processors fig 

parallel speedup mg electrostatic solver applied coulomb crystal systems relative error order performed ibm turbo 
processors time step processors fig 

parallel scalability mg electrostatic solver applied coulomb crystal systems relative error order performed ibm turbo 
containing intel xeon ghz myrinet test system protein atoms 
pme uses parallel fft library fftw 
mg uses grid levels levels change efficiency iss cse nd edu time md step pme pbc mg pbc ewald pbc mg vacuum direct vacuum number atoms fig 

time md step body solvers implemented relative error order efficiency pme mg mg processors fig 

efficiency comparison mg vs pme 
mg run different softening distances dimensions finest grid 
pme uses parallel fft library fftw performed linux cluster xeon ghz myrinet interconnection network 
increases run time accuracy 
see parallel efficiency processors 
fast ethernet mg pme scaled efficiently processors respectively 
discussion major advantages mg produces stable dynamics lower accuracy methods converges correct solution softening distance goes infinity 
furthermore mg excellent candidate multiple time stepping grid level associated time step 
shows better parallel efficiency mg pme suffers fft part may able nodes efficiently number nodes prime 
furthermore doubling points finest grid dimension improves scalability mg local correction increases factor total number points finest grid 
illustrates parallel efficiency data scaling mg different system sizes coulomb crystals 
upper part indicates excellent scaling integration step propagation positions velocities redundantly performed nodes 
lower part shows data scaling 
close linear slight increase due cache memory effects fact accuracy increases slightly system size distance grid points kept systems 
linear momentum angular momentum preserved mg 
space perturbs value potential energy function invariant rigid body rotation translation 
momenta fluctuate constant value suffer freezing 
limitation mg parameters determine simpler methods ewald 
developed tool called assists potential users determination optimal parameters pme mg system size accuracy 
areas improvement method include development higher accuracy version careful analysis interpolation errors better interpolation functions cf 

method combined multiple time stepping algorithms precise accuracy control conditions necessary produce robust simulation protocol cf 
optimization pme coupled 
important deduce formulas computing mg 
scaling nodes benefit dynamic balancing direct summation particle space 
iii miller spector molecular dynamics simulations nucleic acid systems cornell force field particle mesh ewald acs symposium series molecular modeling structure determination nucleic acids am 
chem 
soc 
barnes hut hierarchical log force calculation algorithm nature 
greengard rokhlin fast algorithm particle simulations comput 
phys 

york pedersen particle mesh ewald 
log method ewald sums large systems chem 
phys 

smooth particle mesh ewald method chem 
phys 

brandt binder eds multiscale computational methods chemistry physics vol 
nato science series series iii computer systems sciences ios press amsterdam netherlands 
hardy multiple grid methods classical molecular dynamics comp 
chem 

hampton ko ma object oriented framework prototyping novel algorithms molecular dynamics accepted acm trans 
math 
softw 

coulomb bi crystals species identical charge mass ratios prl 
leeuw smith simulation electrostatic systems periodic boundary conditions 
lattice sums dielectric constants proc 
soc 
lond 

ewald die berechnung und ann 
phys 

optimisation ewald sum large systems mol 
sim 

case schlick optimized particle mesh ewald step integration molecular dynamics simulations chem 
phys 

computer simulation particles mcgraw hill new york 
greengard rokhlin fast algorithm particle simulation comput 
phys 

brandt multilevel matrix multiplication fast solution integral equations comput 
phys 

bishop schulten difficulties multiple fast multipole algorithm molecular dynamics comp 
chem 

rankin board portable distributed implementation parallel multipole tree algorithm ieee symposium high performance distributed computing duke university technical report 
framework design parallelization force computation molecular dynamics ph thesis university bergen bergen norway 
brandt multilevel methods fast solution body hybrid systems international series numerical mathematics vol 
birkh user verlag basel pp 

brandt multilevel computations integral transforms particle interactions oscillatory kernels comput 
phys 
commun 

zaslavsky schlick adaptive multigrid technique evaluating long range forces biomolecular simulations applied mathematics computation 
board jr jr schulten accelerated molecular dynamics simulation parallel fast multipole algorithm chem 
phys 
lett 

multigrid methods classical molecular dynamics simulations chem 
phys 

groot electrostatic interactions dissipative particle chem 
phys 

pollock comments fmm ewald method large periodic systems computer physics communications 
holm mesh ewald sums 
theoretical numerical comparison various particle mesh routines chem 
phys 

molecular dynamics simulations longrange electrostatic effects ann 
rev biophys 

struct 

york yang fast fourier poisson method calculating ewald sums chem 
phys 

jr ewald summation techniques perspective survey computer physics communications 
greengard science 
berman grid multipole calculations siam sci 
comput 

pedersen chem 
phys 

lambert board multipole algorithm efficient calculation forces potentials macroscopic periodic assemblies particles comput 
phys 

hierarchical force calculation algorithm journal computational physics 
bode tree particle mesh adaptive efficient parallel code simulation astrophysical journal supplement series 
duan adaptive computing potential energy classical molecular systems journal computation chemistry 
duan ewald summation multipole method chem 
phys 

warren salmon astrophysical body simulations hierarchical tree data structures supercomputing ieee computer society press pp 

warren salmon portable parallel particle program comput 
phys 
commun 

board efficient particle mesh ewald approach fixed induced interactions chem 
phys 

board schulten fast multipole algorithm ieee comp 
sci 
eng 

fox johnson otto salmon walker solving problems concurrent processors vol 
prentice hall englewood cliffs nj 
yang sarin ramakrishnan compression particle data hierarchical approximate methods acm trans 
math 
softw 

hendrickson new parallel method molecular dynamics simulation macromolecular systems comp :10.1.1.35.6971
chem 

singh gupta hennessy load balancing data locality adaptive hierarchical body methods barnes hut fast multipole radiosity journal parallel distributed computing 
kumar sameh scalable parallel formulations method body simulations parallel computation 
seidel coats load balancing algorithm parallel electromagnetic particle cell code computer physics communication 
hendrickson vaughan gardner parallel transient dynamics simulations algorithms contact detection smoothed particle hydrodynamics paral 
distrib 
comp 

brown jones multigrid distributed memory machines siam sci 
comput 

mitchell full domain partition approach distributing adaptive grids applied numerical mathematics 
parallel multigrid adaptive pde solver hashing space filled curves parallel computation 
matheson tarjan parallelism multigrid methods international journal parallel programming 
frederickson mcbryan normalized convergence rates method siam sci 
stat 
comput 

gannon structure parallelism highly concurrent pde solver paral 
distrib 
comp 

gan linear scaling computation fock matrix 
vi 
data parallel computation exchange correlation matrix chem 
phys 

harvey das biswas designing efficient partitioning algorithm grid environments application body problems computational science applications 
hasse structure energy spherical coulomb crystals phys 
rev 
sarin sameh improving error bounds multipole siam sci 
comput 

humphreys berne multiple time step molecular dynamics algorithm macromolecules phys 
chem 

algorithm pseudo code recursive multi grid scheme cycle 
code shown processor 
number particles ith processors grid dimensions nx ny nz size spatial hashing 
main charges particles ni finest charge grid global sum grid values step call multiscale maxlevel level interpolate forces finest grid particles number ni step eq 
correct kernel particles mi step eq 
compute total energy due grid points number eq 
multiscale maxlevel level maxlevel compute kernel coarsest grid maxlevel points global sum grid maxlevel values step eq 
charge grid coarser charge grid global sum grid values step call multiscale maxlevel interpolate coarser kernel grid kernel grid step correct kernel grid global sum grid values step eq 

