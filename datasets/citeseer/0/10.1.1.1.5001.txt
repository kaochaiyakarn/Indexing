principal components analysis graph relationships spectral clustering marco francois yen pierre dupont information systems research unit universit catholique de louvain place des louvain la neuve belgium yen isys ucl ac department computing science engineering universit catholique de louvain place louvain la neuve belgium info ucl ac 
presents novel procedure computing distances nodes weighted undirected graph called euclidean commute time distance subspace projection nodes graph preserves variance possible terms principal components analysis graph 
markov chain model random walk graph 
model assigns transition probabilities links nodes random walker jump node node 
quantity called average commute time computes average time taken random walker reaching node starting node coming back node square root quantity distance measure nodes nice property decreasing number paths connecting nodes increases length path decreases 
computed pseudoinverse laplacian matrix graph kernel 
define principal components analysis pca graph subspace projection preserves variance possible terms 
graph pca interesting links spectral graph theory particular spectral clustering 

introduces general procedure allowing compute dissimilarities nodes weighted undirected graph represent nodes graph euclidean space reduced dimensionality 
computing dissimilarities pairs nodes allows determine item relevant similar item allows instance cluster 
application technique collaborative filtering promising results related 
procedure compute dissimilarities markov chain model 
define random walk model graph assigning transition probability edge 
random walker jump node node node represents state markov model 
markov chain model compute quantity called average passage time see instance average number steps needed random walker reaching state time starting state symmetrized quantity called average commute time see instance provides distance measure pair states nodes 
fact quantity distance graph proved independently klein gobel 
show distance nodes takes remarkable form referred euclidean commute time distance 
easily computed pseudoinverse laplacian matrix graph shown valid kernel 
quantities nice property decreasing number paths connecting nodes increases length path decreases communication facilitated 
short nodes considered similar short paths connecting 
contrary shortest path called geodesic dijkstra distance necessarily decrease connections nodes added capture fact strongly connected nodes smaller distance weakly connected nodes 
fact recognized field mathematical chemistry attempts commute time distance shortest path distance 
knowledge interesting alternatives known shortest path geodesic distance graph quantities exploited context pattern recognition machine learning 
indirectly framework spectral clustering shown section 
provides new interpretation spectral clustering show spectral clustering interpreted terms 
show project nodes space graph euclidean subspace preserves linear subspace projections 
subspace optimal sense keeps variance projected data possible terms 
equivalent principal component analysis terms call technique principal components analysis graph 
summary main contributions suggests average passage time nodes graph useful pattern recognition tool shows average passage time computed terms pseudoinverse laplacian matrix graph definition average passage time shows kernel svm classification introduces pca graph principal component analysis computed matrix provides elegant interpretation spectral clustering spectral embedding terms random walks graph 
section introduces random walk model markov chain model 
section develops dissimilarity measures iterative formulae compute 
section gives details computation average passage time average commute time laplacian matrix graph 
derives number interesting properties laplacian pseudoinverse 
section introduces eigen vector decomposition pseudoinverse laplacian matrix maximizes variance projected data 
shows pseudoinverse valid kernel 
section summarizes related develops interesting relationships spectral clustering 
section 

markov chain model random walk graph 
laplacian matrix weighted graph consider weighted undirected graph symmetric weights wij couple nodes linked edge say nodes total 
weight wij edge connecting node node set meaningful value convention important relation node node larger value wij consequently easier communication edge 
notice require weights positive wij symmetric wij wji 
elements aij adjacency matrix graph defined standard way wij node connected node aij symmetric 
introduce laplacian matrix graph defined usual manner diag ai 
degree matrix dii ii ai 
aij 
furthermore volume graph defined vg vol dii aij 
suppose graph single connected component node reached node graph 
case rank number nodes 
column vector denotes matrix transpose column vector le etl hold doubly centered 
null space onedimensional space spanned easily show symmetric positive semidefinite see instance 

random walk model graph markov chain describing sequence nodes visited random walker called random walk weighted graph see instance 
associate state markov chain node define random variable representing state markov model time step random walker state time say define random walk single step transition probabilities aij dii pij 
words state node associate probability jumping adjacent node proportional weight wij edge connecting transition probabilities depend current state past ones order markov chain 
graph connected markov chain irreducible state reached state 
denote probability state time xi define transition matrix entries pij evolution markov chain characterized pt provides state probability distribution 
xn time initial probability distribution known see details 

average passage time average commute time section review basic quantities computed definition markov chain probability transition matrix average time average commute time 
average passage time defined average number steps random walker starting state take enter state time 
precisely define minimum time hitting state tik min realization stochastic process 
average passage time expectation quantity starting state tik ei tik 
verify recurrence relations see instance pij equations order iteratively compute average times 
meaning formulae quite obvious order go state state go adjacent state proceed 
introduce closely related quantity average commute time defined average number steps random walker starting state take entering state time go back 
notice symmetric definition 
shown authors average commute time distance measure states 
section show distance graph takes remarkable form 
average passage time average commute time provide dissimilarity measures pairs nodes 

computation basic quantities means methods computing quantities matrix iterative procedures 
matrices large computation pseudoinverse case may iterative techniques equation sparseness probability transition matrix 
section show average passage time average commute time computed equation pseudoinverse laplacian matrix graph plays fundamental role number interesting properties 
developments section inspired klein showed electrical equivalence see paragraph section effective resistance equivalent average commute time computed laplacian matrix 
extend results showing formula computing average commute time directly derived providing formulae average passage time 
denote lij element laplacian matrix words lij ij moore penrose pseudoinverse see denoted elements ij ij appendix prove useful properties show computation average passage time terms obtained equation ij ik kj kk djj obtain equation vg ii jj ij formula obtained electrical equivalent commute times effective resistance 
define ei ith column ei 
equation rewritten form vg ei ej ei ej node represented unit basis vector ei node space 
easily observe distance euclidean space nodes graph positive semidefinite 
mentioned called euclidean commute time distance 

principal components analysis graph section show eigenvector decomposition nodes vectors ei mapped new euclidean space preserves subspace keeping variance possible terms 
provide proof lengthy technical 

transformation euclidean space preserving show node vectors ei mapped euclidean space preserves 
positive semidefinite matrix transformed diagonal matrix orthonormal matrix eigenvectors 
un un column vectors uk orthonormal eigenvectors uj ij see instance 
diagonal matrix contains eigenvalues decreasing order importance 
vg ei ej ei ej vg xi xj xi xj vg xi xj xi xj vg xi xj xi xj vg transformations xi ei xi dimensional euclidean space transformed node vectors exactly separated 
appendix show kernel corresponds matrix inner products easily show centered center gravity 

approximate projection subspace transformed space introduced previous section dimensionality graph order applications 
define approximation transformation preserves information possible 
called spectral eigenvector decomposition defined see textbook linear algebra instance 
eigenvalues previous section column vectors uk orthonormal eigenvectors uj ij 
suppose compute eigenvector expansion ut 
um diag 

compute corresponding distance nodes vg ei ej ei ej 
ut reasoning previous section recompute distance follows vg time column vectors containing zeroes position 

transformation defined xi ei xi subspace dimensional space commute time distances approximately preserved 
bound approximation provided vg easily observe uk coordinate eigenvector uk uk uk corresponding eigenvalue xi coordinate vector xi xi xi xi holds 
kuk coordinate vector 
words coordinate node vectors xi corresponding axis transformed space 

coordinate node vectors simply projection original node vectors en eigenvector weighted 
generally coordinate node vectors transformed space simply projection original node vectors 
en uk weighted idea discard axes corresponding smallest eigenvalues 
relations principal components analysis show decomposition similar principal components analysis sense projection maximal variance possible candidate projections 
denotes data matrix containing coordinates nodes transformed space row see appendix easily deduce 






fig 
example principal components analysis 
original graph weight edge inversely proportional length shown left projection principal components shown right 
known principal components analysis data matrix yields kth principal component eigenvector vk tx variance centered 
tu 
diagonal matrix deduce expressed principal components coordinate system eigenvectors tx basis vectors transformed space 
coordinate vector corresponds projection node kth principal component 
variance terms nodes cloud principal component conclude projection viewed principal components analysis euclidean space nodes exactly separated 
decomposition defines projection node vectors maximal variance terms possible candidate projections 
exemple pca provided 
notice shown performing multidimensional scaling gives exactly results principal components analysis 
furthermore rank set eigenvectors inverse eigenvalues eigenvalues eigenvalues ln see appendix need explicitly compute pseudoinverse order compute projection 
need compute smallest eigenvectors lowest eigenvalues largest 
related vast literature spectral graph theory see monograph results laplacian spectrum hyper graphs summarized 
spectral techniques applied wide variety contexts including high performance computing image segmentation web page ranking information retrieval rna motif classification data clustering dimensionality reduction :10.1.1.31.1768:10.1.1.160.2324:10.1.1.120.3875:10.1.1.19.8100
particular spectral clustering refers collection techniques cluster data points eigenvectors matrix derived affinity matrix wij exp xi xj xi xj denotes dissimilarity points xi xj free parameter see review :10.1.1.43.7945
common choice xi xj xi xj automatic procedure determining minimize cluster distortion proposed 
spectral clustering graph theoretic approach clustering affinity matrix precisely corresponds weighted adjacency matrix defined section 
context image segmentation shi malik introduced normalized cut ncut criterion define optimal bipartitioning graph denotes bipartition set vertices graph :10.1.1.160.2324:10.1.1.160.2324
cut total weight edges connecting disjoint sets cut wij 
ncut criterion aims finding bipartition minimizing ncut va va cut va vol subgraph volume 
criterion seeks balance goal clustering finding tight clusters segmentation finding separated clusters 
similar notion conductance cut cut min va va 
finding optimal ncut np complete graph regular grid approximation computing eigenvalues normalized laplacian ld ad :10.1.1.160.2324
particular eigenvector associated second smallest eigenvalue known algebraic connectivity fiedler value bipartition graph 
note standard spectral graph partitioning uses laplacian matrix normalized version 
corresponds minimization average cut criterion cut graph order 
version exactly similar method showed previous section computing largest eigenvalues eigenvectors equivalent computing smallest non trivial eigenvalues eigenvectors meila shi links spectral segmentation markov random walks 
random walk model identical defined section different properties stressed 
shown particular ncut aa aa pab denotes probability random walk transiting state set state set step current state random walk started stationary distribution markov chain 
application random walk model partially labeled classification proposed 
learning framework known transductive learning unlabeled examples provide information structure domain class labels examples known 
spectral clustering data point associated graph vertex state associated markov chain 
model assumes uniform initial distribution includes distribution class label state estimated em margin 
classification point performed maximize posterior probability pr ik denotes power transition matrix classification example depends ik probability markov process started state ended steps 
value parameter controlling smoothness random walk representation 
laplacian spectrum transductive learning described 
random walk model section defines transition matrix markov process adjacency affinity matrix mention alternative definitions markov transition matrix proposed context supervised unsupervised classification 
laplacian eigenmaps called spectral embedding dimensionality reduction procedure proposed belkin niyogi 
authors solve related eigenproblem lu du 
smallest eigenvalue left small eigenvalues embedding 
embedding computed spectral clustering algorithm shi malik :10.1.1.160.2324
noted equivalent result obtained renormalizing adjacency matrix ad computing eigenvectors eigenvalues 
clear reduction technique closely related definition principal components analysis graph 
smola kondor connections spectral graph theory graph kernels 
particular define graph regularization aims emphasize role smallest non trivial contrary discard largest ones 
interesting links definition pca graph pca discard smallest nontrivial eigenvectors stressed correspond largest 
stress links techniques web page ranking algorithms pagerank hits randomized hits 
intriguing correspondence random walk graph electrical networks theory popularized doyle snell nice book see 
average commute time equivalent terms electrical networks 
shown vg ij re ij effective resistance node node words average commute time effective resistance basically measure quantity see details 

introduced general procedure computing dissimilarities nodes graph 
particular markov chain model random walk graph 
precisely compute quantity called euclidean commute time distance provides distance measure pair nodes 
introduced subspace projection method preserving variance terms possible defines principal components analysis graph 
exploiting various problems including inexact graph matching collaborative filtering supervised classification clustering 
working definition discriminant analysis graph application graph 

barnett 
matrices methods applications 
oxford university press 

belkin niyogi 
laplacian eigenmaps spectral techniques embedding clustering 
dietterich becker ghahramani editors advances neural information processing systems volume pages 
mit press 

belkin niyogi 
laplacian eigenmaps dimensionality reduction data representation 
neural computation 

bollobas 
modern graph theory 
springer 


markov chains gibbs fields monte carlo simulation queues 
springer verlag 

buckley harary 
distance graphs 
addison wesley publishing 

campbell meyer 
generalized inverses linear transformations 
pitman publishing 

chandra raghavan ruzzo smolensky tiwari 
electrical resistance graph captures commute cover times 
annual acm symposium theory computing pages 

chung 
spectral graph theory 
american mathematical society 

dumais furnas landauer harshman 
indexing latent semantic analysis 
journal american society information science 

doyle snell 
random walks electric networks 
mathematical association america 

fiedler 
property eigenvectors nonnegative symmetric matrices applications graph theory 
czech 
math 


gan zorn tang kim schlick 
rag rna graphs database concepts analysis features 
appear bioinformatics 

gobel 
random walks graphs 
stochastic processes applications 

joachims 
transductive learning spectral graph partitioning 
proceedings th international conference machine learning washington dc 

kamvar klein manning 
spectral learning 
proceedings international joint conference artificial intelligence april 

kemeny snell 
finite markov chains 
springer verlag 

klein 
resistance distance 
journal mathematical chemistry 

kleinberg 
authoritative sources hyperlinked environment 
journal acm 

lay 
linear algebra applications th ed 
addison wesley 

meila shi 
random walks view spectral segmentation 
proceedings 

mohar 
laplacian spectrum graphs 
chartrand editors graph theory combinatorics applications volume pages 
wiley 

mohar 
laplace eigenvalues graphs survey 
discrete mathematics 

ng jordan weiss 
spectral clustering analysis algorithm 
dietterich becker ghahramani editors advances neural information processing systems volume pages vancouver canada 
mit press 

noble daniels 
applied linear algebra th ed 
prentice hall 

norris 
markov chains 
cambridge university press 

page brin motwani winograd 
pagerank citation ranking bringing order web 
technical report computer system laboratory stanford university 

pothen simon 
liou 
partitioning sparse matrices eigenvectors graphs 
siam journal matrix analysis 

kannan vempala 
clusterings bad spectral 
proceedings st annual symposium foundations computer science 

rao mitra 
generalized inverse matrices applications 
john wiley sons 


computing dissimilarities nodes graph application collaborative filtering 
technical report universite catholique de louvain www isys ucl ac staff francois articles 

scholkopf smola 
learning kernels 
mit press 

shi malik 
normalised cuts image segmentation 
ieee transactions pattern matching machine intelligence august 

smola kondor 
kernels regularization graphs 
warmuth sch lkopf editors proceedings conference learning theory kernels workshop 

szummer jaakkola 
partially labeled classification markov random walks 
dietterich becker ghahramani editors advances neural information processing systems volume vancouver canada 
mit press 

weiss 
segmentation eigenvectors unifying view 
international conference computer vision 

zha ding gu simon 
spectral relaxation means clustering 
dietterich becker ghahramani editors advances neural information processing systems volume pages vancouver canada 
mit press 

zheng ng jordan 
stable eigenvector algorithms link analysis 
proceedings th international conference research development information retrieval pages 
acknowledgments prof van prof blondel partement ing math universit catholique de louvain insightful discussions 
appendix useful properties laplacian matrix symmetric 
symmetric matrix see easily obtain symmetric 
ep matrix 
ep matrix matrix commutes pseudoinverse aa real symmetric automatically ep matrix see 
particular worth mentioning properties 
ui eigenvalues eigenvectors ui corresponding eigenvalues eigenvectors hand uj eigenvalues eigenvectors eigenvalues eigenvectors 
particular rank null space 
previous property implies doubly centered sum columns rows zero just see chapter discussion topic 

properties ep matrices described 
positive semidefinite 
previous property eigenvalues sign positive semidefinite positive semidefinite 
appendix kernel section show kernel see instance 
matrix containing inner products transformed vectors xi xj xj ej ej ij denotes data matrix containing coordinates nodes row elements ij 
