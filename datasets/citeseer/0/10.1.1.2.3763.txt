global optimizations parallelism locality scalable parallel machines jennifer anderson monica lam computer systems laboratory stanford university ca data locality critical achieving high performance large scale parallel machines 
non local data accesses result communication greatly impact performance 
mapping decomposition computation data processors scalable parallel machine key issue compiling programs architectures 
describes compiler algorithm automatically finds computation data decompositions optimize parallelism locality 
algorithm designed distributed shared address space machines 
scope algorithm dense matrix computations array accesses affine functions loop indices 
algorithm handle programs general nestings parallel sequential loops 
mathematical framework enables systematically derive decompositions 
algorithm exploit parallelism fully parallelizable loops loops require explicit synchronization 
algorithm trade extra degrees parallelism eliminate communication 
communication needed algorithm try introduce expensive forms communication parts program frequently executed 
minimizing communication increasing locality data important optimization achieving high performance large scale parallel machines 
long messagepassing overhead multicomputer architectures intel touchstone minimizing communication essential 
locality important scalable machines support shared address space hardware 
example local cache accesses stanford dash shared memory multiprocessor orders magnitude faster remote accesses 
improving locality greatly enhance performance machines 
mapping computation processors parallel machine termed computation decomposition program 
similarly placement data processors local memories research supported part darpa contract nsf young investigator award fellowship digital equipment western research lab 
proceedings sigplan conference programming language design implementation pldi albuquerque new mexico june called data decomposition 
describes compiler algorithm automatically finds computation data decompositions optimize parallelism locality program 
algorithm designed distributed shared address 
machines distributed address space compiler follow phase pass maps decomposition explicit communication code 
necessary manage memory directly machines shared address space techniques manage data distributed memory machines improve cache performance 
choices data computation decomposition interrelated important examine opportunities parallelism reuse data determine decompositions 
example available parallelism computation lies operating different elements array simultaneously allocating elements processor renders parallelism unusable 
data decomposition dictated available parallelism loop nest affects decision parallelize loop nest distribute computation minimize communication 
may parallelism create larger granularity tasks communication cost overwhelms benefit parallelization 
popular approach complex optimization problem solicit programmer help determining data decompositions 
projects approach include superb id vienna fortran fortran 
current proposal high performance fortran extension fortran relies user specified data decompositions 
significant benefit programmer eliminating tedious job managing distributed memory explicitly programmer faced difficult programming problem 
tight coupling mapping data computation means programmer effect analyze parallelization program specifying data decompositions 
best decomposition may change basedon architecture machine programmer fully master machine details 
furthermore data decompositions may need modified program run efficiently different architecture 
goal research automatically derive data computation decompositions domain dense matrix code loop bounds array subscripts affine functions loop indices symbolic constants 
algorithm finds decompositions loops number iterations larger number processors 
emphasis finding order shape decompositions 
address issues load balancing choosing block size block cyclic decomposition determining number physical processors lay dimension fitting computation data exact number physical processors 
issues impact performance parallel machines effect secondary address 
developed mathematical framework expressing calculating decompositions 
framework general handle broad class array access patterns including array sections calculate replication read data 
possible program systematic solution successfully reduce complex problem manageable 
model basedon property equivalent decompositions data computation allocated single processor 
aspect decomposition determined show assignment specific processors easily calculated 
cost communication determined data movement pattern 
communication pattern nearest neighbor shifts data amount data transferred significantly reduced blocking 
form communication inexpensive compared communication patterns require general movement entire data structure transpose 
differentiate communication occurs parallel loop explicit synchronization loops due mismatches decompositions 
call communication loop nest pipelined communication 
communication due mismatches decompositions require moving entire data structure called data reorganization communication 
single data decomposition array reorganization communication program consider decomposition static may minor nearest neighbor communication parallel loop nests 
section briefly presents background optimizing parallelism locality loop nest 
introduce issues involved automatically calculating decompositions formulate problem mathematically 
section describes components decomposition gives overview approach 
illustrate basic ideas decomposition model discuss simplified subproblem section 
algorithm finds data computation decompositions data reorganization pipelined communication 
reapply concepts find decompositions pipelined communication section 
section uses algorithms section building blocks develop algorithm takes account pipelined data reorganization communication 
section presents additional techniques handling replication minimizing number idle processors amount replication 
implemented algorithms described suif compiler stanford 
section describes experimental results compiler 
section discusses related conclude section summary contributions 
problem overview section briefly discusses optimizations parallelism locality single loop nest introduces issues involved finding decompositions way simple example 
presenting mathematical formulation decompositions formally state problem 
background techniques maximizing parallelism locality single loop nest literature 
number researchers looked specific problem mapping single loop nest parallel machines 
refer loop level techniques local analysis 
global analysis responsible optimizing parallelism locality multiple loop nests 
compiler normalizes loops performs loop distribution executing decomposition algorithms 
compiler runs loop fusion pass decomposition regroup compatible loop nests 
compiler uses algorithm developed wolf lam apply unimodular transforms find coarsest granularity parallelism loop nest 
pass leaves loop nests canonical form consisting nest fully permutable loop nests 
nests large possible starting outermost loops 
loop nest fully permutable arbitrary permutation loops nest legal 
fully permutable loop nest depth transformed get gamma degrees parallelism 
compiler positions loops fully permutable nest parallel loops outermost 
example code gamma gamma gamma local analysis produce code shown top 
keyword forall indicates iterations loop executed parallel 
loop nest depth outermost parallelizable loop loop outermost fully permutable loop nest size greater 
loops gamma sequential degenerate fully permutable nests size dependences iterations loops 
local phase responsible finding transformations minimize communication loops respect outer sequential loops 
parallelizable loops allocated neighboring loops iteration space neighbors mapped processor space 
simple example consider code shown top 
array elements shaded light grey dark grey respectively identify position arrays 
loop iterations shaded similarly 
assumes arrays stored row major order 
naive approach considers loop nest individually distribute outermost loop loop nest get coarsest granularity parallelism 
processor accesses rows arrays second loop nest parallel loop distributed processor accesses columns array rows array communication occur processor accesses different section array loop nests 
solution communication parallelize loop loop nest 
processor access columns rows columns loop nests 
loop nests analyzed determine relative positions arrays communication necessary 
complete communication free decomposition shown 
rest shows mathematical representation decompositions discussed sections 
forall forall gamma forall gamma gamma processors span span span processors processors span simple decomposition example 
squares represent array elements circles represent iterations 
lines connect array elements iterations allocated processor 
problem formulation section presents mathematical model decomposition problem 
represent data computation decompositions affine transformations 
discussion loops normalized unit step size arrays subscripts adjusted start 
loop nest depth loop bounds affine functions loop indices iteration polytope dimensional space 
iteration loop nest corresponds integer point polytope identified index vector 
array dimension defines array space dimensional rectangle 
element array accessed integer vector am 
similarly dimensional processor array defines processor space dimensional rectangle 
write affine array index function linear transformation constant vector 
definition index dimensional array data decomposition array dimensional processor array function ffi theta linear transformation matrix ffi constant vector 
definition iteration loop nest depth computation decomposition loop nest dimensional processor array function fl theta linear transformation matrix fl constant vector 
problem stated formally follows 
want find computation decomposition loop nest data decomposition array loop nest parallelism maximized communication minimized 
formal decompositions simple example previous section shown 
basic concepts problem finding data decompositions ffi computation decompositions fl broken distinct components affine model 
partition determines computation data allocated processor 
mathematically data computation partitions described nullspace matrices definitions 
orientation represented matrices describes mapping axes array elements loop iterations processors 
lastly displacement gives offset starting position data computation corresponds constant vectors ffi fl 
illustrate partition orientation displacement developing communication free decomposition sample program 
partition 
data dependence loop nest serializes loop 
communication necessary elements column array row array assigned processor 
elements column array processor iterations loop nest assigned processor execute sequentially dependences loop nest 
turn columns array allocated processor 
partitions example shown 
informally data computation partitions specify array elements iterations respectively assigned processor processor 
formally subspace array space accessed array referenced loop nest denoted range array index matrix range array dimension rank ae data decomposition matrix def 

array elements allocated processor gamma gamma ker conversely array elements gamma ker may assigned different processors considered distributed 
computation decomposition matrix def 

iterations executed processor gamma gamma ker iterations gamma ker said distributed may run different processors 
mathematical representation partitions example shown 
data partitions indicate array elements direction column assigned processor 
similarly data partition means elements direction assigned processor 
computation partitions indicate iterations loop loop nest iterations loop second loop nest executed processor 
orientation 
partition determines array elements iterations local single processor 
orientation displacement specify processor data computation allocated 
particular orientation gives data computation dimensions processor dimensions 
loop nest columns array accessed reverse order columns loop nest columns array accessed order rows array solution satisfies requirements allocate columns forward order columns rows reverse order 
iterations loop loop nest reversed 
orientation illustrated 
formally matrix def 
defines data orientation matrix def 
computation orientation 
matrices example shown 
note exist different communication free orientations partition 
example just easily chosen allocate columns reverse order columns rows forward order 
alternative orientation result dx gamma dy dz gamma 
displacement 
displacement specifies offsets array elements iterations respect processors 
loop nest accesses loop columns array offset rows array loop nest accesses arrays offset 
assigning columns array processors columns processors rows processors satisfies requirement 
iterations loop nest processors 
complete decompositions displacements illustrated 
formally displacements ffi fl constant vectors definitions respectively 
orientation matrix derived partition plus displacement forms complete decomposition 
shows data computation displacements final decompositions example 
case orientations possible displacements lead communication free decompositions 
summarize basis approach 
different equivalent decompositions partition 
reduce complexity finding decomposition functions array loop nest finding partition guaranteed lead desired decomposition 
simple calculation find appropriate orientations displacements completely specify decompositions 
static decompositions section algorithm find data computation decompositions pipelined communication data reorganization communication 
simplified problem illustrates basic ideas decomposition model 
algorithm finds single static decomposition array loop nest considers parallelism available forall loops 
relationship data computation communication occur data local processor data 
relationship data computation expressed theorem 
theorem computation decomposition loop nest data dx fxj array index function array loop nest iterations elements array local processor elements dx fxj ffi fl communication displacement level inexpensive amount data transferred canbe significantly blocking 
priority finding best partitions tions focus version eqn 
omits displacements 
letting fxj fxj array index function matrices fxj array loop nest decomposition free reorganization communication data decomposition matrix dx computation decomposition eqn 
true 
trivial solution guarantees communication execute sequentially setting computation decomposition matrices data decomposition matrices 
ker span entire iteration space ker span entire array space maximizing parallelism means finding data computation decompositions partition ker nullspace loop nests small possible 
partition constraints consider base case outer sequential loop containing number perfectly nested loops 
assume local analyzed transformed perfectly nested loops individually canonical form 
section discuss general case multiple nesting levels 
collection loops nests arrays represented bipartite interference graph vc vd 
loop nests form set vertices vc arrays form set vertices vd undirected edge array loop nest array referenced loop nest 
edge contains array index functions accesses array loop nest 
connected component interference graph corresponds set arrays loop nests inter related decompositions 
algorithms section operate single connected component time 
find static decomposition constraints placed data computation partitions 
single loop nest 
constraints single loop nest characterize loops assigned processor 
constraints initialize computation partition loop nest 
algorithm considers forall loops initial computation partition loop nest depth span set dimensional elementary basis vectors representing sequential loops loop nest 
loop nesting level sequential ek included initial computation partition 
multiple arrays 
role constraints due multiple arrays ensure exists single data decomposition matrix array 
constraints initialize data partitions 
constraints necessary distinct paths interference graph array array example consider code fragment forall forall forall forall kth elementary vector written ek kth position zero positions 
array index functions loop nest fx fy array index functions second loop nest fx fx fy respectively 
decomposition free reorganization communication decomposition matrices dx dy arrays respectively eqn 
holds loop nests respectively 
iterations loop nest loop nest dy fy dy fy array index functions invertible equations produce equations dy dy fy gamma dy fy gamma dx fx fy gamma gamma fx fy gamma fx fy gamma gamma fx fy gamma ker dx array access functions array equal equation yield dx ker dx additional constraints placed partitions 
example eqn 
dx ih gamma ij 
dx ih gamma gamma ij 
simplifying equation gives constraint partition array ker dx spanf gamma similar analysis yields constraint partition array ker dy spanf gamma partition means elements diagonal allocated processor 
general array index functions invertible introduce auxiliary variables pseudo inverse function 
techniques similar literature 
analysis run pairs arrays involved cycle interference graph including degenerate case multiple access functions array loop nest 
array involved multiple cycles multiple constraints constraints summed 
general computing data constraints array multiple loop nests possible loop nests access different subsections array 
case loop nest contributes constraints section array 
data computation relation 
constraint ensures relationship data computation eqn 
holds 
iterations loop nest mapped processor data array access mapped processor 
gamma section ker eqn 
fxj ker dx formally ker dx spanf fxj ker similarly iterations loop nest mapped processor data array access mapped processor 
gamma ker ker ker ker fxj ker fxj iterations array location mapped processor 
range fxj 
general ker spanf ker fxj fxj ker dx sequential loops loop nest cause elements array referenced loop nest allocated local processor 
local array elements cause iterations loop nests access elements executed sequentially 
calculating partitions find partitions maximize parallelism pipelined data reorganization communication find minimum partitions satisfy constraints 
constraint single loop initialize computation partitions constraint multiple arrays initialize data partitions 
iterative algorithm satisfy constraint data computation relationship 
overview algorithm shown 
algorithm update arrays loop nest ig interference graph ig vc vd dp set set vector space foreach referenced ker dx ker dx spanf fxj ker foreach algorithm algorithm update loops array ig interference graph ig vc vd cp set set vector space foreach loops ker ker spanf ker fxj fxj ker dx foreach algorithm algorithm calc relation ig interference graph ig vc vd cp set set vector space dp set set vector space changes changed vd update loops ig cp set changed vc update arrays ig dp set algorithm algorithm partition ig interference graph ig vc vd computation data partitions cp set set vector space dp set set vector space satisfy constraints foreach vc ker single loop constraint multiple loop constraint ig dp set calc relation ig cp set dp set algorithm algorithm calculating partitions 
iterative algorithm calculates effects loop nests arrays eqn 
arrays loop nests eqn 

continues stable partition 
informally partition algorithm trades extra degrees parallelism eliminate communication 
going back simple example array index functions arrays fx fz index functions array second loop nests fy gamma fy respectively 
ker initialized ker initialized spanf data partitions ker dx initialized 
routine update arrays called loop nest 
eqn 
applied arrays resulting ker dy spanf ker dz spanf routine update loops called arrays array eqn 
applied loop nest resulting ker spanf update arrays called loop nest ker dx spanf lemma partition algorithm finds maximum parallelism minimum partitions satisfy constraints guaranteed terminate 
proof partition algorithm satisfies constraints data computation partitions initialized constraints 
algorithm finds minimum partition satisfies data computation relation constraint algorithm increases partitions order ensure constraint satisfied 
prove termination fact spaces ker data partitions ker computation partitions increase size monotonically algorithm progresses 
worst case partitions span entire space algorithm terminate 
data partition array computation partition loop nest step determine number virtual processor dimensions 
number virtual processor dimensions max arrays dim sx gamma dim ker dx sx loops range fxj total array space accessed typically entire array 
equation yield value parallelism partition algorithm exploited 
example 
calculating orientations partition number virtual processor dimensions algorithm finds orientations 
partitions determine kernels decomposition matrices 
orientations interference graph relative choose arbitrary decomposition matrix derive rest decomposition matrices component 
algorithm starts choosing theta data decomposition matrix dx array dimension nullspace dx data partition ker dx eqn 
computation decomposition matrix loop nest array simplicity presentation assume array index functions invertible 
data decomposition matrix dy array accessed loop nest calculated dy gamma yj gamma yj remaining decompositions connected component calculated similar fashion 
array index function accesses subsection array ae ay auxiliary variables temporarily unspecified dimensions data decomposition matrix 
note calculating orientations non integer entries decomposition matrices result 
orientations relative matrices multiplied common multiple eliminate fractions 
lemma orientation algorithm finds decomposition matrices arrays loop nests exactly nullspace partition algorithm proof outline proof space considerations 
prove lemma induction 
base case array chose arbitrary decomposition matrix specified kernel 
partition constraints show decomposition matrix calculated correct nullspace holds 
theorem partition orientation algorithms find decomposition matrices arrays loop nests maximize parallelism communication loops reorganization communication loops 
proof theorem follows directly lemmas 
calculating displacements expect communication displacement level relatively inexpensive nearest neighbor communication consider sacrificing parallelism avoid communication due displacements 
algorithm minimizes communication caused conflicting displacements possible 
displacements calculated partitions orientations determined 
compiler uses simple greedy strategy takes account branch predictions offset sizes find displacements minimize communication frequently executed paths 
eqn 
says full data decomposition dx ffi array referenced loop nest array index function fxj computation displacement fl dx ffi data displacement ffi array accessed loop nest calculated ffi fl gamma dy blocked decompositions section discuss problem finding data computation decompositions pipelined communication data reorganization communication 
previous section considered parallelism available forall loops 
may case possible legally transform iteration space outermost forall loops 
example consider point difference operation gamma gamma gamma gamma parallelism available wavefront diagonal original loop nest 
tiling known blocking unroll jam interchange wellknown transformation allows parallelism locality exploited loop nest 
original iteration space shown demonstrates loop executed parallel doacross parallelism 
iterations shaded block assigned different processors 
computation proceeds original iteration space 
iteration spaces showing parallel execution tiled loops 
arrows represent data dependences 
wavefront dynamically explicit synchronization enforce blocks 
dimensions iteration space blocked idle processors blocks diagonal execute parallel 
gain advantages tiling idle processors assigning entire rows columns different processors 
cases processor assigned strip iteration space processors start executing parallel 
example tiled code corresponds follows ii gamma gamma ii min gamma ii gamma gamma gamma loop nest tiled original loop split dimensions outer ii loop inner loop 
allocating shaded strip different processor spreads iterations ii loop processors iterations loop reside processor 
tiling reduce communication loop nests forall parallelism available nests 
consider example adi alternating direction implicit integration forall gamma forall gamma loop nest sequential loop accesses columns second loop nest sequential loop accesses rows communication data partition ker dx spanf computation partition loops ker spanf partition specifies entire array allocated processor loops run sequentially 
considering parallelism available forall loops provides options run loops sequentially incur data reorganization communication loops 
compiler tiles loops extract wavefront parallelism reorganization communication reduced inexpensive pipelined communication 
tiled version adi code shown 
ii ii min ii gamma gamma ii ii min ii gamma gamma loop nests outer ii distributed processors inner loops executed processor 
processor assigned block columns array 
loop nest dependences blocks pipelined communication loop nest 
second loop nest data dependences block communication necessary 
blocked decomposition model decomposition model easily extended incorporate concept tiling 
general tiling creates sets loops inner loops iterate block outer loops iterate blocks 
inner loops allocated processor outer loops distributed processors 
way achieve locality block parallelism blocks 
mathematically represented computation allocated processor vector space ker focusing loops inner block iterations allocated processor form vector space lc vector space lc called localized vector space lc represent tile iterations cache locality 
model localized vector space lc contains dimensions iteration space local processor completely local blocked 
ker lc dimension iteration space lc gamma ker blocked 
iterations finite block allocated processor entire dimension 
blocks distributed processors 
similarly define vector space ld characterize array dimensions block allocated processor 
relationship data partition ker space ld ker ld adi example blocked computation partitions ker lc spanf similarly blocked data partition ker dx spanf algorithm finds computation data partitions ker ker spaces correspond dimensions entirely mapped processor 
blocking desired algorithm finds lc ld iterations lc gamma ker data ld gamma ker distributed blocked 
calculating blocked decompositions algorithm find data computation partitions may pipelined communication 
algorithm tries apply partition algorithm specified section considering parallelism available outermost forall loops 
try find solution parallelizable loop parallelism reorganization pipelined communication 
solution compiler tries exploit doacross parallelism 
recall local phase compiler transforms loop nest largest possible fully permutable loop nests outermost 
fully permutable nest forall loops positioned outermost 
loop nest fully permutable fully tiled 
dependence vectors fully permutable loop nest distance vectors pipelined communication inexpensive data elements block boundaries need move 
cost communication loop weighed cost reorganization communication loops 
algorithm partition blocks ig interference graph ig vc vd computation data partitions cp set set vector space dp set set vector space computation data localized spaces cl set set vector space dl set set vector space try find solution communication partition ig cp set dp set parallelism record localized spaces foreach ker cp set lc ker foreach ker dx dp set ker dx find blocked iterations data foreach vc ker single blocked loop constraint multiple loop constraint ig dp set calc relation ig cp set dp set algorithm algorithm calculating partitions blocks 
overview algorithm shown 
algorithm determines solution reorganization pipelined communication degree parallelism recalculates ker ker partition algorithm reapplied change single loop constraint initialize computation partitions 
dimensions tiled considered initial computation partitions 
initial computation partition loop nest depth span set dimensional elementary basis vectors 
loop nesting level sequential tiled ek included initial computation partition 
multiple array constraints initialize data partition ker iterative partition algorithm run find data computation partitions 
final ker ker represent computation data allocated processor 
need find lc ld determine iterations array elements completely local processor blocked 
note vector spaces calculated 
partition algorithm called find solution pipelined communication resulting ker exactly vector nest 
similarly resulting ker exactly vector space ld array 
partitions orientation displacement calculated discussed sections 
iteration data spaces blocked orientations displacements entire blocks 
adi example loop nests fully permutable completely tiled 
compiler discovers forall parallelism exploited communication tries exploit doacross parallelism loops 
initial computation partitions ker initial data partition ker dx 
running iterative partition algorithm change partitions 
lc spanf spanf spaces completely tiled 
note algorithm yields solution allows entire iteration data space tiled constrain partitions unnecessarily 
solution idle processors shown 
optimizations compiler uses reduce idle processors described section 
exploit doacross parallelism possible different processors write array location loop nest 
single static data decomposition array loop nest 
cases amount data communicated small respect amount computation 
code generator shared address space machine need know exactly processor current value data 
code generator distributed address uses data flow analysis individual array accesses find efficient communication data moves loop nest 
dynamic decompositions section solve problem finding data computation decompositions maximize parallelism data reorganization pipeline communication allowed 
data occur decomposition array loop nest differs decomposition array loop nest 
find data decomposition array loop nest computation decomposition loop nest 
communication graph model decompositions change dynamically communication graph 
nodes graph correspond loop nests program 
edges graph represent places program data reorganization occur 
edges graph calculated information similar fortran compiler 
fortran reaching decompositions defined set decomposition statements may reach array uses decomposition 
case loop nests may define 
decomposition array loop nest reaches loop nest possible values array loop nests 
problem calculated manner similar standard reaching definitions data flow problem 
edges communication graph chains formed reaching decompositions directed 
simplicity presentation discussion assumes array read written loop nests access array 
associated edge probability decomposition loop nest reach loop nest 
loop node function number instructions loop estimate number times loop executes 
implementation currently uses profile information calculate probabilities edge weights loop execution counts loop node weights 
problem formulation formally state dynamic decomposition problem 
communication graph want find data decomposition array loop node corresponding computation decomposition loops 
computation decomposition determines degree parallelism loop nest 
loop node computation decomposition loop node weight estimate benefit execution time result parallelism loop nest 
note tiling parallelism benefit loop takes account cost pipeline communication loop 
data reorganization occur decomposition array differs decomposition array loop 
data decompositions probabilities edges estimate communication time 
value graph sum parallelism benefits loop nodes minus total communication cost 
goal label arrays loops value graph maximized 
example consider program fragment 
forall forall expr forall forall forall forall shows communication graph assuming expression true time arrays size theta 
edges labeled estimated communication time assuming decompositions match data reorganized loop nest 
value edge nodes sum communication estimates arrays loop nest decomposition communication graph 
components resulting dynamic decomposition 
final decompositions 
probability reaching loop nest decomposition 
figures illustrate final decompositions example discussed section 
theorem dynamic decomposition problem np hard 
proof prove dynamic decomposition problem np hard transforming known np hard problem colored multiway cut subproblem problem 
colored multiway cut problem graph weighted edges partial coloring vertices subset function extended total function total weight edges different colored endpoints minimized 
consider subproblem dynamic decomposition problem program accesses single array parallelized loop node value greater sum weights edges value 
reduce instance colored multiway cut instance subproblem polynomial time 
give brief overview reduction 
node original problem loop nest depth subproblem input program edge weights branch probabilities 
array dimensional dimension represents color write input program node color ith loop parallel loop nest representing node gamma loops corresponding loop nest parallel 
finding dynamic decompositions edges communication correspond cutset edges colored multiway cut problem 
edges removed array decomposition loop nests connected component communication graph 
dynamic decomposition algorithm dynamic decomposition problem np hard compiler algorithm uses heuristics find dynamic decompositions 
dynamic algorithm uses greedy approach eliminates largest amounts communication 
algorithm joins loop nodes greatest edge costs component eliminating possibility data reorganization loop nodes 
consider loop nests degree parallelism joining components 
purely sequential loops treated component 
overview algorithm shown 
routine single level describes algorithm base case single nesting level 
rest algorithm deals multiple level case described section 
nesting level algorithm operates communication graph 
algorithm initializes components node communication graph component calculates edge weights 
edge weights worst case approximation actual communication cost 
worst case occurs decompositions match data reorganized loop nest 
edges examined decreasing order weights 
edge algorithm tries join current component current component single component 
interference graph created loop nodes arrays referenced loops new joined component 
partition algorithm section called interference graph find new partitions 
forming new component algorithm eliminates data reorganization cost edge 
union operation may cause loop nodes execute sequentially may generate pipeline communication loop nodes result tiling 
algorithm finds value graph new partitions calculated 
value graph greater join new component saved 
algorithm records new partitions loops arrays new component 
communication edge new component discarded 
consider communication graph 
example assume loop node weights large 
dependences code distance vectors assume tiling practical 
edge nodes examined 
partition algorithm determines degree parallelism data reorganization communication loops nodes joined 
algorithm examines nodes edge 
case partition algorithm find parallelism nodes 
algorithm tries add node component 
time partition algorithm finds way eliminate reorganization communication run loops sequentially algorithm decides add node 
nodes form node component 
illustrates resulting components shows final decompositions component 
putting far considered base case outer sequential loop containing number perfectly nested loops 
dynamic decomposition routine shown bottom gives overview decomposition algorithm general case 
nesting level examined bottom order 
effect pushing communication outermost loops possible 
components re initialized level loop nest considered context sequential outer nest current level 
partitions level initialize partitions level 
algorithm single level cg communication graph cg computation data partitions cp set set vector space dp set set vector space computation data localized spaces cl set set vector space dl set set vector space joined comp comp comp component ig interference graph val integer initialize components foreach calculate val value cg foreach decreasing order weight comp find component comp find component joined comp union components comp comp ig create interference graph joined comp partition blocks ig cp set dp set cl set dl set value cg val val value cg install joined comp discard joined comp record data reorganization foreach algorithm algorithm dynamic decomposition cg communication graph computation data partitions cp set set vector space dp set set vector space computation data localized spaces cl set set vector space dl set set vector space foreach nesting level bottom order cg create comm graph cp set dp set cl set dl set single level cg cp set dp set cl set dl set foreach calculate orientations calculate displacements algorithm algorithm calculating dynamic decompositions 
loop indices outside current nesting level treated symbolic constants finding partition constraints 
manner constraints current level considered 
bipartite interference graph edge array node loop node accesses array 
array node bipartite interference graph single decomposition 
dynamic algorithm discovers array decomposition changes node array interference graph split subsequent levels 
edges adjusted loops reorganized decomposition point proper array node 
formed algorithm finds orientations displacements 
orientations arrays loops component relative example additional communication result transposing decompositions component 
observation reduce amount communication finding orientations components 
compiler chooses orientation component matches closely possible components connected edges 
compiler uses greedy strategy edge weights decide components orient 
orientations displacements component algorithms described section section respectively 
dynamic decomposition algorithm shown driver algorithm finding decompositions general case 
finds data computation decompositions maximize parallelism minimize data reorganization pipelined communication 
partitioning algorithms previous sections subroutines dynamic algorithm 
particular static dynamic algorithm able successfully join loop nodes single component 
general algorithm reports data decomposition array loop nest computation decomposition loop nest 
optimizations ways improve program decompositions previous section 
section briefly summarizes minimize number idle processors find minimize replication read data 
idle processors loop nest subsection array number virtual processor larger nesting depth loop nest 
result fraction processors busy execution loop nest 
avoid idle processors computation decomposition find processor dimensions parallelism loops 
equation number virtual processor dimensions modified limited minimum distributed iteration space min max arrays dim sx gamma dim min loops gamma dim ker sx loops range fxj array space accessed 
reduce number virtual processor dimensions projecting dimensional virtual processor space dimensional processor space 
choosing dimensions virtual processor space project vectors selected ker computation decomposition matrices means projections processor dimension idle execution loop nest 
replication replication read data common technique improve performance parallel machines 
algorithms find amount read data replication needed maintain degree parallelism inherent read write data introducing additional communication 
consider types replication constant replication dimension replication 
constant replication occurs multiple data decompositions array 
dimension replication means processors copy data 
increase space requirements accommodate constant replication linear function array size dimension replication cause space needed grow number processors 
focus dimension replication 
allow necessary replication run decomposition algorithm account read data program 
resulting computation partition find data partitions read arrays eqn 

find processor dimensions contain replicated data 
dimension replication modeled reduced processor space expanded full dimensional processor space 
processors expanded dimensions copies data corresponding processor reduced space 
replicated dimensional array data partition ker dx dimensionality reduced processor space nr nr dim sx gamma dim ker dx sx loops range fxj array space accessed 
degree replication array number processor dimensions data copied gamma nr data decomposition matrix array nr theta decomposition matrix dx maps array elements reduced processor space 
computation decomposition matrix loop nest accesses array maps iterations full processor space 
relate full processor space reduced processor space nr theta matrix eqn section modified express relationship computation data replication matrix maps reduced space nullspace ker corresponds dimensions replication 
algorithm uses data partition ker dx find ker section eqn 
find data computation decomposition matrices 
similarly eqn 
find decomposition matrices replication 
computation data decompositions initially derived consideration amount replication needed 
result amount replication called greater practical target machine 
techniques section limit degree replication projecting virtual processor smaller processor space 
experimental results implemented algorithms described suif compiler stanford 
experiments described section performed stanford dash shared memory multiprocessor 
code generator dash point implemented hand parallel spmd programs decompositions generated compiler 
programs compiled sgi compiler optimization level 
dash multiprocessor number physically distributed clusters 
cluster silicon graphics power station consisting mips processors 
directory protocol maintain cache coherence clusters 
takes processor cycle retrieve data cache cycles local memory cycles remote memory 
dash operating system allocates memory clusters page level page assigned specific cluster allocated cluster touches page 
compare decomposition algorithm finds decomposition sgi power fortran accelerator version parallelizing compiler 
compare results possible decompositions 
ran programs cluster dash multiprocessor mb main memory cluster 
looked heat conduction phase application simple dimensional lagrangian lawrence livermore national lab 
heat conduction routine conduct lines long loop nests 
routine set loops performs adi integration parallelism rows arrays columns arrays 
cases blocked distribution scheme 
shows speedups best sequential version different decompositions routine problem size theta double precision optimization static dynamic pipelining dynamic pipelining number processors speedup speedup sequential execution time conduct 
problem size theta double precision 
total amount data routine order mb 
amount memory needed cluster exceeds memory available cluster dash operating system allocates memory available cluster 
executing fewer clusters data application may allocated cluster 
curve labeled optimization shows results sgi power fortran compiler 
allowed dash operating system allocate pages cluster accessed data 
fortran arrays allocated column major resulted blocks columns allocated clusters 
available parallelism row processors perform remote reads writes 
second curve labeled static shows performance single data decomposition array 
case blocks rows contiguous shared address space 
represents best possible static decomposition forall parallelism exploited 
third curve labeled dynamic pipelining data dimension parallelism changes 
case program incurs cost reorganization communication data reallocated 
curve represents best possible decomposition forall parallelism 
fourth curve labeled dynamic pipelining shows results allocating blocks rows contiguously explicit synchronization processors parallelism column 
version processors synchronize blocks columns block size 
decomposition compiler finds considering pipeline reorganization communication 
related number researchers addressed problems related decomposition problem 
sarkar gao developed algorithm uses collective loop transformations perform array contraction 
loop interchange reversal transformations orient computation 
ju dietz search algorithm find data layout loop restructuring combinations reduce cache shared memory machines 
hwang hu describe method finding computation mapping systolic array stages share single array 
algorithm works calculating projection vector similar call partition computation mapping 
projects examined problem finding array alignments call data orientations displacements data parallel programs 
approaches focus element wise array operations try eliminate communication consecutive loops 
li chen prove problem finding optimal orientations np complete developed heuristic solution implement functional language crystal messagepassing machines 
contrast approaches model supports loop parallel general affine array index functions 
approaches optimize fixed degree parallelism decisions loops run parallel 
researchers developed data decomposition algorithms searching fixed set possible decompositions 
gupta banerjee developed algorithm automatically finding static data decomposition 
approach exhaustive search various possible decompositions system cost estimates 
carle developed interactive tool part fortran project finds data decompositions phases procedure 
data remapped dynamically phases 
approach uses static performance estimator select best decompositions fixed set choices 
comparison algorithm avoids expensive searches systematically calculating decompositions 
result mathematical model able derive decompositions take account pipeline communication loop nests data reorganization communication loop nests 
summary decomposition problem complex inter related issues addressed 
addresses full problem automatically calculating data computation decompositions programs systematic way 
algorithms mathematical model decompositions affine functions 
framework general broad class array 
affine model structure decompositions components partition orientation displacement 
equivalent decompositions partition solve partition evaluate possible decomposition designs simultaneously 
maximize parallelism algorithm exploits forall parallelism doacross parallelism tiling 
minimize communication algorithm tries find static decomposition exploits maximum degree parallelism available program reorganization pipeline communication 
algorithm trade extra degrees parallelism eliminate communication 
communication needed algorithm try reduce expensive reorganization communication inexpensive pipelined communication tiling 
necessary data reorganization communication inserted frequently executed parts program 
allen kennedy 
automatic translation fortran programs vector form 
acm transactions programming languages systems october 
amarasinghe lam 
communication optimization code generation distributed memory machines 
proceedings sigplan conferenceon programming language design implementation june 
irigoin 
scanning polyhedra loops 
proceedings third acm sigplan symposium principles practice parallel programming pages april 
fox kennedy kremer 
static performance estimator guide data partitioning decisions 
proceedings third acm sigplan symposium principles practice parallel programming pages april 
callahan 
global approach detection parallelism 
phd thesis rice university april 
published 
carle kennedy kremer mellor crummey 
automatic data layout distributed memory machines programming environment 
technical report rice university february 
chapman mehrotra zima 
programming vienna fortran 
scientific programming fall 
chatterjee gilbert schreiber 
teng 
automatic array alignment data parallel programs 
proceedings th annual acm symposium principles programming languages pages january 
dahlhaus johnson papadimitriou seymour yannakakis 
complexity multiway cuts 
proceedings th acm symposium theory computing pages may 
gao olsen sarkar thekkath 
collective loop fusion array contraction 
proceedings fifth workshop programming languages compilers parallel computing pages august 
gilbert schreiber 
optimal expression evaluation data parallel architectures 
journal parallel distributed computing september 
gupta banerjee 
demonstration automatic data partitioning techniques parallelizing compilers multicomputers 
transactions parallel distributed systems march 
high performance fortran forum 
high performance fortran language specification november 
version 
kennedy 
tseng 
compiling fortran mimd distributed memory machines 
communications acm august 
huang sadayappan 
communication free hyperplane positioning nested loops 
banerjee gelernter nicolau padua editors languages compilers parallel computing pages 
springer verlag berlin germany 

hwang hu 
systolic mapping multistage algorithms 
proceedings ieee international conference application specific array processors pages august 
intel santa clara ca 
ipsc ipsc user guide june 
irigoin 
supernode partitioning 
proceedings sigplan conference programming language design implementation pages january 
ju dietz 
reduction cache coherence overhead compiler data layout loop transformation 
banerjee gelernter nicolau padua editors languages compilers parallel computing pages 
springer verlag berlin germany 
kennedy mckinley 
optimizing parallelism data locality 
proceedings acm international conference supercomputing pages july 
knobe steele 
data optimization allocation arrays reduce communication simd machines 
journal parallel distributed computing 
koelbel mehrotra van 
supporting shared data structures distributed memory architectures 
proceedings second acm sigplan symposium principles practice parallel programming pages march 
kulkarni kumar basu paulraj 
loop partitioning distributed memory multiprocessors unimodular transformations 
proceedings acm international conferenceon supercomputing pages june 
kumar kulkarni basu 
deriving transformations mapping nested loops hierarchical parallel machines polynomial time 
proceedings acm international conference supercomputing pages july 
lam wolf 
compilation techniques achieve parallelism locality 
proceedings darpa software technology conference pages april 
lenoski gharachorloo laudon gupta hennessy horowitz lam 
stanford dash multiprocessor 
ieee computer march 
li chen 
generating explicit communication shared memory program 
supercomputing pages 
ieee may 
li chen 
index domain alignment minimizing cost cross referencing distributed arrays 
proceedings frontiers third symposium frontiers massively parallel computation pages 
ieee october 
maydan 
accurate analysis array 
phd thesis stanford university september 
published csl tr 
prins 
framework efficient execution array languages simd computers 
proceedings frontiers third symposium frontiers massively parallel computation pages 
ieee october 
rogers pingali 
compiling locality 
proceedings international conference parallel processing pages june 
sarkar gao 
optimization array accesses collective loop transformations 
proceedings acm international conferenceon supercomputing pages june 

tseng 
optimizing fortran compiler mimd distributed memory machines 
phd thesis rice university january 
published rice comp tr 

tseng 
parallelizing compiler distributed memory parallel computers 
phd thesis carnegie mellon university may 
published cmu cs 

automatic data mapping distributed memory parallel computers 
phd thesis carnegie mellon university may 
published cmu cs 
wolf 
improving locality parallelism nested loops 
phd thesis stanford university august 
published csl tr 
wolf lam 
data locality optimizing algorithm 
proceedings sigplan conference programming language design implementation pages june 
wolf lam 
loop transformation theory algorithm maximize parallelism 
transactions parallel distributed systems october 
wolfe 
optimizing supercompilers supercomputers 
mit press cambridge ma 
zima 
bast 
superb tool semi automatic mimd simd parallelization 
parallel computing january 

