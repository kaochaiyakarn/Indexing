discriminative methods multi labeled classification sunita sarawagi iit bombay mumbai india sunita iitb ac 
methods enhancing existing discriminative classifiers multi labeled predictions 
discriminative methods support vector machines perform uni labeled text classification tasks 
multi labeled classification harder task subject relatively attention 
multi labeled setting classes related part hierarchy 
new technique combining text features features indicating relationships classes discriminative algorithm 
enhancements margin svms building better models presence overlapping classes 
results experiments real world text benchmark datasets 
new methods beat accuracy existing methods statistically significant improvements 
text classification task assigning documents pre specified set classes 
real world applications including spam filtering mail routing organizing web content topical hierarchies news filtering rely automatic means classification 
text classification broadly categorized discriminative techniques typified support vector machines svms decision trees neural networks generative techniques na bayes nb expectation maximization em methods 
performance point view nb classifiers known fastest learning probabilistic generative model just pass training data 
accuracy relatively modest 
spectrum lie svms elegant foundations statistical learning theory 
training time quadratic number training examples known accurate 
simplest task text classification determine document belongs class interest 
applications require ability classify documents classes 
sufficient talk document belonging single class 
granularity coverage set classes document topic 
document describing politics involved sport cricket classified sports cricket society politics 
document belong class called multi labeled 
multi labeled classification harder problem just choosing classes 
algorithms existing discriminative classification techniques building blocks perform better multi labeled classification 
propose enhancements existing discriminative methods 
new algorithm exploits correlation related classes label sets documents combining text features information relationships classes constructing new kernel svms heterogeneous features 
methods improving margin svms better multi labeled classification 
experiments comparing various multi labeled classification methods 
review related conclude research directions 
multi labeled classification discriminative classifiers suppose vector space representation documents 
bag words model document vector di component term feature proportional importance term frequency tfidf commonly 
document vector normalized unit norm associated labels 
training data dj ci 
ci 
linear svm finds vector scalar constant ci wci dj minimized 
optimization corresponds fitting possible slab positive negative documents 
discriminative classifiers including svms essentially class classifiers 
standard methods dealing multi class problems create ensemble binary classifiers label 
method called vs 
label li positive class includes documents li labels negative side includes documents 
application set labels associated document dj wk dj bk 
basic svm method denoted svm serves baseline compare methods 
limitations basic svm method text classification svms faced issue classifiers ensemble rejecting instances 
vs constituents ensemble emit wc bc score multi labeled classification admit classes predicted set score wc bc 
practice find significant fraction documents get negative scores classifiers ensemble 
discriminative multi class classification techniques including svms historically developed assign instance exactly set classes assumed disjoint 
contrast multi labeled data nature consists highly correlated overlapping classes 
instance reuters dataset classes wheat grain crude fuel class parent class knowledge explicitly available classifier 
overlap classes hurts ability discriminative methods identify boundaries class 
devise techniques handle problem section 
correlation classes boon 
exploit strong mutual information subsets classes pull classes term information insufficient 
section new method directly exploit correlation classes improve multi label prediction 
combining text class membership features opportunity improving multi labeled classification provided occurrence relationships classes label sets documents 
propose new method exploiting relationships 
classification class ci indicator classification class cj way enhance purely text svm learner augment feature set extra features label dataset 
cyclic dependency features labels resolved iteratively 
training train normal text svm ensemble 
augment document set new columns corresponding scores wci bci class ci positive scores transformed negative scores transformed 
case scores output negative negative score transformed 
text features original document vector scaled new label dimensions scaled 
documents get new vector representation columns number term features 
supervised set labels 
train new svm ensemble 
call method svms heterogeneous feature kernels denoted svm hf 
complete pseudo code shown 
approach directly related previous cross training label mappings different taxonomies help building better classification models taxonomies 
represent document vector term space build vs rest svm classifier text tokens document apply getting vector scores see text concatenate vectors single training vector label carried relative term label weight determined maintaining add vector training set induce new vs rest svm classifier 
svms heterogeneous feature kernels testing application test documents classified 
document transformed scores appended new columns appropriate scaling 
document submitted obtain final predicted set labels 
scaling factor differential scaling term feature dimensions special reasons 
applies special kernel function documents training 
kernel function linear svms gives similarity document vectors kt di dj di dj di dj document vectors scaled unit norm simply cos angle document vectors standard ir similarity measure 
scaling term label dimensions sets new kernel function di dj kt di dj kl di dj kt usual dot product kernel terms kl kernel label dimensions 
tunable parameter chosen cross validation held validation set 
label dimensions interact independent text dimensions way set modified kernel 
just scaling document vector suitably sufficient kernel change code needed 
improving margin svms multi labeled classification tasks second opportunity improvement provided tuning margins svms account overlapping classes 
label set attached individual instances incomplete 
discriminative methods best classes disjoint 
experience reuters dataset multi labeled instances incomplete label sets 
multi labeled data best treated partially labeled 
set includes instances truly belong positive class 
propose mechanisms removing examples large negative set similar positive set 
method document level second class level 
removing band points hyperplane presence similar negative training instances side classifier svm ensemble hampers margin re orients separating hyperplanes slightly differently points absent 
remove points close resultant hyperplane train better hyperplane wider margin 
algorithm consists iterations 
iteration train basic svm ensemble 

svm trained remove negative training instances threshold distance band learnt hyperplane 
re train ensemble 
call method band removal method denoted 
selecting band careful remove instances crucial defining boundary class 
held validation dataset choose band size 
appropriate band size tries achieve fine balance large margin separation achieved removing highly related points generalization achieved removing points truly belonging negative class 
confusion matrix pruning way countering similar positive negative instances completely remove training instances confusing classes 
confusing classes detected confusion matrix quickly learnt held validation data moderately accurate fast classifier na bayes 
confusion matrix class problem nxn matrix ij th entry mij gives percentage documents class misclassified class mij threshold prune away confusing classes side constructing vs classifier 
method called confusion matrix pruning method denoted 
step method specified 
obtain confusion matrix original learning problem fast moderately accurate classifier 
select threshold 

construct vs svm ensemble 
class leave entire class set mij 
parameter small lot classes excluded set 
small classes may excluded resulting original ensemble 
chosen cross validation 
faster train relying confusion matrix fast nb classifier requires svm ensemble trained 
user domain knowledge relationships classes hierarchies classes easily incorporated 
experiments describe experiments text classification benchmark datasets report results comparison various multi labeled classification methods 
compare baseline svm method svm hf 
experiments performed processor ghz machine gb ram running debian linux 
rainbow feature text processing svmlight svm experiments 
datasets reuters reuters text categorization test collection standard text categorization benchmark 
mod apte split evaluate methods train test split classes 
separately www cs cmu edu mccallum bow svmlight joachims org random train test splits averaged random splits test statistical significance subset classes 
feature selection stemming stopword removal considered tokens occurred document selected top features mutual information 
patents patents dataset text classification benchmark 
alpha collection english language collection patent applications classified hierarchy classes subclasses groups 
take sub classes top level train test split 
report average random train test splits subhierarchy 
consider text patent classification feature selection reuters dataset 
evaluation measures evaluation measures discussed instance basis aggregate value average instances 
document dj true set labels predicted set labels 
accuracy measured hamming score symmetrically measures close accuracy dj 
standard ir measures precision recall defined multi labeled classification setting dj dj dj dj dj dj dj 
comparison figures shows comparison various methods reuters patents datasets 
shows comparison classes reuters results averaging random train test splits subset classes 
shows comparison subclasses patents average random train test splits class sub hierarchy 
datasets see svm hf best accuracy 
svm best precision best recall 
observe svm hf comparable measures 
class subset classes method accuracy precision recall accuracy precision recall svm svm hf 
reuters dataset directional test statistical significance svm svm hf methods class subset sub hierarchy 
accuracy scores svm hf better svm small significant difference level significance 
values respectively minimum required value df 
class sub hierarchy subclasses method accuracy precision recall accuracy precision recall svm svm hf interpreting 
patents dataset documents scaled unit norm inspecting components label dimensions derived svm hf gives interesting insights various kinds mappings labels 
signed components label dimensions represent amount positive negative influence dimension classifying documents 
example reuters dataset label dimension grain highly indicative class grain 
wheat high positive component grain money fx sugar relatively high negative components 
indicates document getting classified wheat positive indicator class grain document classified sugar money fx negative indicator class grain 
comparing number labels shows size true set labels predicted set fix 
instance instances svm method instances assigned 
singleton labels svm precise admits label methods admit extra labels 
corresponding svm svm hf 
percentage instances various sizes classes reuters 
test instances dataset greater 
see svm tends give lesser number predictions just compared methods high percentage instances column 
reason way vs resolved 
negative scores vs resolved choosing negative score treating positive 
forces prediction set size semantics negative unclear 
percentages documents assigned negative scores svm classes reuters svm hf assign negative scores documents respectively 
related limited done area multi labeled classification 
crammer propose vs family online topic ranking algorithms 
ranking wci model class wci learnt similar perceptrons update wci iteration depending imperfect ranking compared true set labels 
kernel method classification tested gene dataset elisseeff 
propose svm formulation giving ranking function set size predictor 
methods topic ranking methods trying improve ranking topics 
ignore ranking irrelevant labels try improve quality svm models automatically predicting labels 
ideas exploiting correlation related classes improving margin multi label classification unique 
example learning pebl semi supervise learning method similar 
uses idea removing selected negative instances 
disjunctive rule learned features strongly positive instances 
svms iteratively trained refine positive class selectively removing negative instances 
goal pebl learn small positive large unlabeled pool examples different multi labeled classification 
multi labeled classification attempted generative models discriminative methods known accurate 
mccallum gives generative model document probabilistically generated topics represented mixture model trained em 
class sets generate document exponential number heuristics required efficiently search subset class space 
aspect model generative model naturally employed multi labeled classification current exists 
documents probabilistically generated set topics words document generated members topic set 
model unsupervised clustering supervised classification 
methods discriminative multi labeled classification 
new method svm hf exploiting occurrence classes label sets documents iterative svms general kernel function heterogeneous features 
methods improving margin quality svms 
see svm hf performs better terms accuracy basic svm method small statistically significant difference 
note svm hf comparable results better svm 
best recall giving largest size predicted set help human labeler data creation process suggesting set closely related labels 
explore svms positive set containing class 
composition positive set related candidate classes unexplored 
secondly theoretically understand reasons accuracy improvement svm hf extra information terms linear combinations terms 
learner pay attention features information pure text features 
explore methods application domains 
acknowledgments author supported fellowship award technologies limited bangalore india 
grateful soumen chakrabarti helpful discussions insights comments 

joachims 
text categorization support vector machines learning relevant features 
proceedings ecml 

schapire singer 
boostexter boosting system text categorization 
machine learning 

vapnik 
statistical learning theory john wiley 

sarawagi chakrabarti 
cross training learning probabilistic mappings topics 
proceedings acm sigkdd 

sarawagi chakrabarti 
scaling multi class support vector machines inter class confusion 
proceedings acm sigkdd 

crammer singer 
family additive online algorithms category ranking 
journal machine learning research 

elisseeff weston 
kernel methods multi labelled classification categorical regression problems 
technical report technologies 

yu han 
pebl positive example learning web page classification svm 
proceedings acm sigkdd 

mccallum 
multi label text classification mixture model trained em 
aaai workshop text learning 

hofmann puzicha 
unsupervised learning dyadic data 
technical report tr berkeley 
