rong jin dept computer science engineering michigan state university east lansing mi cse msu edu automatic weighting scheme collaborative filtering collaborative filtering identifies information interest particular user information provided similar users 
memory approaches collaborative filtering pearson correlation coefficient approach identify similarity users comparing ratings set items 
approaches different items weighted equally predefined functions 
impact rating discrepancies different users taken consideration 
example item highly favored users smaller impact user similarity item different types users tend give different ratings 
simple weighting methods variance weighting try address problem empirical studies shown ineffective improving performance collaborative filtering 
optimization algorithm automatically compute weights different items ratings training users 
specifically new weighting scheme create clustered distribution user vectors item space bringing users similar interests closer separating users different interests distant 
empirical studies datasets shown new weighting scheme substantially improves performance pearson correlation coefficient method collaborative filtering 
categories subject descriptors information search retrieval information search retrieval information filtering keywords collaborative filtering memory approach leave method item weighting scheme 

collaborative filtering predicts utilities items particular user rating information set items users 
past years collaborative filtering algorithms developed :10.1.1.21.4665:10.1.1.136.4322:10.1.1.38.6498
generally categorized classes memory algorithms model algorithms :10.1.1.21.4665
obtain prediction particular user test user memory algorithms identify users training database similar user terms rating patterns combine ratings 
category includes pearson correlation approach vector joyce chai dept computer science engineering michigan state university east lansing mi cse msu edu luo si school computer science carnegie mellon university pittsburgh pa lsi cs cmu edu similarity approach extended generalized vector space model :10.1.1.21.4665
model approaches group different users training database small number classes rating patterns 
order predict rating test user particular item approaches categorize test user predefined user classes rating predicted class targeted item prediction 
algorithms category include bayesian network approaches aspect model :10.1.1.21.4665
compared memory approaches model approaches advantage profiles models need stored 
memory approaches usually simpler model approaches require little offline computation model approaches usually spend computation cycles creating model profiles 
furthermore model approaches tend assume small number user classes sufficient modeling rating patterns different users may lose diversity users 
model approaches tend perform worse memory approaches number training users small 
ratings small number users usually insufficient create reliable clusters users 
combine strength approaches hybrid methods personality diagnosis approach developed outperforms model memory approaches :10.1.1.38.6498
simplicity robustness memory approaches widely real world applications 
key memory approaches identify users training database similar test user 
similarity different users usually computed matching ratings users set items 
memory approaches items treated equal importance 
apparently undesirable discrepancies different items taken account 
items may highly favored users rated significantly differently different users 
intuitively items similar ratings smaller impact determining user similarity different ratings 
words items large variance ratings tend important items small variance ratings 
necessarily true large variance ratings item arise difficulty rating item users 
shown herlocker weighting items rating variance leads slightly worse results weighting 
addition variance weights inverse user frequency entropy mutual information studied previous literature :10.1.1.21.4665
results indicate weighting schemes items able improve performance collaborative filtering 
reasons opinion current weighting schemes usually hand crafted computed predefined functions 
unclear global objectives weighting schemes try achieve 
address problem new weighting scheme leave loo method 
built intuition rating behavior individual user similar rating behaviors users 
weighting scheme items bring users similar interests closer separate users different interests apart 
way describing idea look user distribution item space 
consider vector space spanned different items user point space projection axis representing rating corresponding item 
intuition leads clustered distribution user points item space user point surrounded closely user points distant 
words rating behavior user explained users totally different 
weighting scheme items shape original user distribution user distribution weights items clustered distribution 
formalize idea probabilistic optimization problem appropriate weights items maximizing likelihood user similar users 
previous weighting schemes item weights determined predefined functions approach automatically computes appropriate weights different items observed ratings provided training users 
assumption having clustered distribution user points item space similar clustering assumption embedded model approaches 
important distinction approach explicit assumption clustered distribution user rating behaviors 
model approaches separate users classes algorithm requires user exists user similar user 
result algorithm specify exact number clusters model approaches 
view difference method model approaches model approaches generative models goal explain observed ratings different training users new approach discriminative model goal explain training users similar different 
result model approaches search seeds clusters generate ratings different users 
algorithm tries find weights different items bring user close similar users distant dissimilar ones 
disadvantage generative model versus discriminative model generative model explain observed ratings items ones useless distinguishing user interests 
contrast discriminative model focuses important items assigning higher weights 
effect various studies classification problems shown general discriminative model performs better generative model 
rest arranged follows section provides brief description major approaches collaborative filtering 
section discusses related item weighting 
section describes details weighting scheme collaborative filtering 
results empirical studies section followed section 
background section briefly describe major approaches collaborative filtering 
introduce notations 
set items set training users test user 
ratings information training database 
triple indicates item rated user user denotes set items rated ry denotes rating item denotes average rating 
rating scale goes max 
memory approaches commonly memory algorithms pearson correlation coefficient pcc algorithm resnick vector space similarity vs algorithm breese kadie 
main idea algorithms calculate similarities set training users test user 
predicate rating item test user ratings training users item averaged weighted similarities test user 
approaches differ computation similarity 
specifically pcc method defines similarity users ry ry ry yt yt ry ry ry yt yt yt vs method defines similarity details :10.1.1.21.4665
ry ry yt ry ry yt model approaches popular model algorithms aspect model am personality diagnosis model pd 
aspect model probabilistic latent space model models individual preferences convex combination preference factors 
latent class variable associated observed pair user item 
aspect model assumes users items independent latent class variable 
probability observation tuple calculated follows stands likelihood user class stands likelihood assigning item rating class users 
ratings user normalized norm distribution zero mean variance 
gaussian distribution parameter multinomial distribution 
personality diagnosis approach treats user training database individual model 
predicate rating item test user approach computes likelihood test user model training user uses aggregate average ratings item training users estimator 
assuming observed rating test user yt item drawn independent normal true distribution mean true rating true true yt yt probability test user model user training database written yt ry likelihood active user rate item computed yt yt previous empirical studies shown pd method able outperform approaches collaborative filtering including pcc method vs method bayesian network approach 

related automatically assigning appropriate weighting items studied previous literatures 
knowledge major approaches inverse user frequency weighting variance weighting 
approach borrows tf idf weighting schemes information retrieval 
authors proposed inverse user frequency weighting different items :10.1.1.21.4665
analogy inverse document frequency idf information retrieval 
specifically weight item defined log stands number training users stands number training users rated item similar idf weighting weights favor items rated training users 
apparently may idea items rated fewer training users may necessarily useful distinguishing users different interests 
empirical study conducted yu weighting degraded performance pcc method 
second approach weights different items proportionally variance ratings different users 
intuition item large variance ratings valuable discerning user interest item small variance 
weight item computed vw wk represents averaged ratings item discussed section item large variance ratings may necessarily item discerning user interests large variance caused fact item difficult rate users 

automatic weighting scheme items main idea new automatic weighting scheme find appropriate weights items user brought closer users share similar interests separated apart users different interests 
viewing item space weighting scheme result clustered distribution different user points user point closely surrounded neighbors distant points dissimilar users 
sections describe probabilistic model measuring similarity different users incorporate weights different items 
probabilistic description user similarity formulate problem finding appropriate weights optimization problem describe solution 
conditional exponential model measuring user similarity yi denote likelihood user similar user likelihood viewed similarity users larger likelihood yi similar user different standard similarity measurement likelihood asymmetric yi yi words user may find similar training users user find similar user yi measures similarity user user view point measures similarity view point second introduce pseudo user account users outside set training users 
complete user set yn user similar remaining users training database pseudo user sum probability yi different users need express likelihood yi explicitly 
indicated equation personality diagnosis method uses generative gaussian model express similar likelihood 
problematic requires ratings item user explained rating behavior user putting emphasis items useful discerning user interests 
result user determined dissimilar user provide ratings items users share similar opinion 
order incorporate weights items probabilistic model conditional exponential model describe likelihood yi yi exp wk vi variable denotes normalized rating item user definition similar pearson correlation coefficient method vi ry xk ry ry yi yi item xk rated user yt normalization factor ensures sum conditional probability yi written exp wk vi introduce constant account probability mass likelihood user similar pseudo user equals expression indicates user shares similar ratings training users large number items likelihood small normalization factor large case 
hand likelihood large user find similar rating behavior training users 
note equation similar similarity measurement pearson correlation coefficient method measure user similarity dot product normalized ratings different users 
differ aspects equation introduces weights determine importance different items weights automatically estimated leave method 
pcc method weights items 
equation likelihood yi asymmetric yi yi similarity measurement pcc method symmetric 
learning weights items aforementioned idea need adjust weights training user similar rest training users distant dissimilar ones 
realize goal take leave approach 
specifically training user measure rating behavior training user explained rating behaviors rest training users 
measure quantity training user compute expression quantity interpreted average similarity training users user important note yi equation words average similarity measured perspective training users user yi 
important replacing yi equation expression simplified quantity maximized zero weights items 
equation measures rating behavior user explained rest training users 
goal just targeting single training user ensuring rating behavior training user explained rating behaviors apply leave approach training user 
measurement product expression equation training users 
equation gives measurement quantity indicates globally rating behavior single training user explained rest training users 
maximizing measurement find optimal weights different items 
formally optimization problem stated follows arg max notation stands weights put weights conditional side likelihood yi yi emphasize likelihood parameterized weights understand optimization problem equation able twist weights items consider extreme case item rated identically training users 
larger weight assigned item items likelihood yi order small value 
result summation tends large measurement equation maximized 
result optimal assign high weights items rated similarly users 
addition formulation equation improve robustness learning algorithm introducing constraints weights 
restrict weights positive wk negative weight assigned item users ratings item smaller similarity users different ratings item 
apparently contradicts intuition 
second prevent single weight large introduce upper bound weight wk constraint indicates weight allowed times larger average weights 
result item similarity measurement 
putting constraints optimization problem equation final optimization problem arg max subject wk finding optimal solution equation difficult due non concave objective function constraints 
derive optimization strategy equation uses idea auxiliary function 
main idea divide optimization procedure steps 
step simple auxiliary function lower bound original objective function optimization 
details optimization procedure described separate 
predict ratings test user simply add weights standard memory approach 
experiments pearson correlation coefficient method basis 
computed weights similarity pcc method computed wx ry ry ry yt yt ry ry ry yt yt yt 
experiment conducted set experiments examine effectiveness new weighting scheme 
particularly address issues constant influence prediction accuracy 
constant controls likelihood training user similar user outside training database 
larger training user similar training users 
experiments conducted examine impact constant final performance new weighting scheme 
weighting scheme compared existing weighting scheme items 
approach compared commonly weighting schemes inverse user frequency variance ratings detailed comparative analysis computed weights :10.1.1.21.4665
weighted memory approach compared approaches 
compare weighted memory approach incorporating weighting scheme standard memory approach including pearson correlation coefficient pcc method vector similarity vs method aspect model am personality diagnosis pd method 
experiment design datasets movie ratings experiments eachmovie eachmovie dataset extracted subset users ratings 
global statistics datasets experiments summarized table 
table characteristics eachmovie 
eachmovie number users number items avg 
rated items user number ratings compare algorithms thoroughly experimented different configurations 
altered training size users eachmovie users training 
rest users cases testing 
furthermore testing user varied number rated items provided test user 
varying number training users number items able test weighting scheme different configurations 
particularly experiment training users able evaluate robustness weighting scheme small number training users 
evaluation metric experiments mean absolute error mae average absolute deviation predicted ratings actual ratings items test user voted 
mae test www cs usyd edu au movie data zip research compaq com src eachmovie denotes number test ratings 
predict ratings test user computed weights incorporated pearson correlation coefficient method described section 
experiments impact constant experiment vary constant 
results different constant computing item weights dataset eachmovie table respectively 
see performs slightly better configurations 
constant controls likelihood training user similar training users results indicate best weights items obtained assign medium chance training user find dissimilar training users 
experiment comparison existing weighting schemes experiment compare weighting scheme commonly weighting schemes inverse user frequency weighting variance weighting vw 
weighting scheme weighting schemes compared incorporated pearson correlation coefficient method predict ratings test users 
results listed table results pearson correlation coefficient method weighting scheme 
observation inverse user frequency weighting variance weighting improve performance table mae different 
smaller value means better performance 
training users size items items items table mae different eachmovie 
smaller value means better performance 
training users size items items items baseline method weighting items 
consistent founding 
contrast new weighting scheme able boost prediction accuracy weight movie index distribution weights computed movies dataset configuration training users 
normalized similarity user weighting weighting distributions normalized similarities database training users 
solid line represents distribution computed weights dot line represents distribution computed weights 
distribution weights items versus variance ratings 
table mae different weighting scheme 
title refers pearson correlation coefficient weighting title vw refers variance weighting title refers inverse user frequency weighting items title new refers new weighting scheme 
smaller value means better performance 
training users size configurations 
weight items items items vw new vw new vw new table mae different weighting scheme 
title refers pearson correlation coefficient weighting title vw refers variance weighting title refers inverse user frequency weighting items title new refers new weighting scheme 
smaller value means better performance 
training users size weight items items items vw new vw new vw new better understand weighting scheme improves performance pearson correlation coefficient method examine distribution weights different movies 
plots computed weight distribution dataset training users 
indicated distribution weights different movies skewed items weighted larger items weighted larger items weighted equal 
skewed weight distribution indicates items important determining user similarity items computation user similarity 
similar phenomenon text categorization people small number words important determining category document 
second examine computed weights shape distribution user similarities 
displays distribution normalized similarities calculated weights 
distribution similar similarities weights table mae vector similarity vs method pearson correlation coefficient proposed weighting scheme pcc personality diagnosis pd method aspect model am method 
smaller value means better performance 
training users size methods items items items vs pd am pcc vs pd am pcc vs pd am pcc table mae eachmovie vector similarity vs method pearson correlation coefficient proposed weighting scheme pcc personality diagnosis pd method aspect model am method 
smaller value means better performance 
training users size methods items items items vs pd am pcc vs pd am pcc vs pd am pcc shown 
normalized similarity user user computed sim yi max sim yi yk distributions computed configuration training users 
observed see introducing weights items large similarity larger small similarity smaller 
words incorporation weights similar users similar dissimilar users dissimilar 
effect consistent intuition stated section weighting scheme items bring similar users closer separate dissimilar users away 
third examine correlation computed weights variance ratings 
distribution weights versus variance ratings plotted 
variance weighting scheme weight item proportional variance ratings item plot indicates items medium variances assigned large weights items large small variances assigned small weights 
consistent studies information retrieval words medium frequency usually informative 
experiments comparison approaches collaborative filtering experiment compares pearson correlation coefficient approach weighting scheme methods vector similarity vs method aspect model am approach personality diagnosis pd method 
table summarizes results methods 
clearly pearson correlation coefficient method weighting scheme referred pcc outperforms methods configurations 
experiment shows new weighting scheme effective improving prediction accuracy collaborative filtering 

novel algorithm automatically determine appropriate weights different items collaborative filtering 
previous weighting schemes weights computed predefined function algorithm automatically learns weights different items ratings training users 
main idea weighting scheme adjust weights items bring similar users closer separate dissimilar users apart 
idea optimization approach developed efficiently search weighting scheme 
empirical studies shown new weighting scheme incorporated improve performance pearson correlation coefficient method substantially different configurations 
comparison existing weight schemes different approaches indicates effectiveness approach collaborative filtering 

breese heckerman kadie empirical analysis predictive algorithms collaborative filtering proceeding fourteenth conference uncertainty artificial intelligence uai :10.1.1.21.4665
herlocker konstan riedl 
algorithm framework performing collaborative filtering 
proceedings nd annual international acm sigir conference research development information retrieval sigir 
nicholas 
collaborative filtering generalized vector space model 
proceedings rd annual international conference development information retrieval sigir 
resnick iacovou suchak bergstrom riedl grouplens open architecture collaborative filtering netnews 
proceedings acm conference computer supported cooperative pages 
hofmann puzicha latent class models collaborative filtering proceedings international joint conference artificial intelligence uai 
charles nicholas 
combining content collaboration text filtering 
proceedings ijcai workshop machine learning information filtering melville mooney nagarajan content boosted collaborative filtering improved recommendations 
proceedings eighteenth national conference artificial intelligence aaai 
swami framework collaborative filtering algorithm development evaluation 
proceedings rd annual international conference development information retrieval sigir 
www cs usyd edu au movie data zip research compaq com src eachmovie dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
pennock horvitz lawrence giles collaborative filtering personality diagnosis hybrid memory model approach proceedings sixteenth conference uncertainty artificial intelligence uai 
yu wen xu ester feature weighting instance selection collaborative filtering proceedings nd international workshop management information web web data text mining hofmann gaussian latent semantic models collaborative filtering proceedings th annual international acm sigir conference si jin flexible mixture model collaborative filtering proceedings twentieth international conference machine learning icml 
berger pietra pietra maximum entropy approach natural language processing 
computational linguistics 
information retrieval ng jordan 
discriminative vs generative classifiers comparison logistic regression naive bayes 
nips salton buckley term weighting approaches automatic text retrieval 
information processing management lewis yang rose li 
rcv new text categorization test collection 
journal machine learning research 
