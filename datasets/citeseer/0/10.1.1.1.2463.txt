lucas kanade years unifying framework part simon baker ralph gross iain matthews cmu ri tr lucas kanade algorithm proposed image alignment widely techniques computer vision 
applications range optical flow tracking layered motion mosaic construction medical image registration face coding 
numer ous algorithms proposed variety extensions original formulation 
overview image alignment describing algorithms consistent framework 
concentrate inverse compositional algorithm efficient gorithm proposed 
examine extensions lucas kanade algorithm inverse compositional algorithm significant loss efficiency 
part series papers cover extension image alignment allow linear appearance variation 
consider linear appearance varia tion error function euclidean norm 
describe different algorithms simultaneous project normalization inverse compositional algorithms empirically compare 
consider combination linear appearance variation robust error functions described part series 
derive robust versions si normalization algorithms 
algorithms inefficient part derive efficient approximations spatial coherence 
empirical evaluation robust algorithms 
keywords image alignment unifying framework lucas kanade algorithm inverse com positional algorithm linear appearance variation robust error functions 
image alignment consists moving possibly deforming template minimize differ ence template image 
lucas kanade algorithm image alignment widely techniques computer vision :10.1.1.49.2019
optical flow applications include tracking parametric layered motion estimation mosaic construction medical image registration face coding :10.1.1.18.5663:10.1.1.147.629
usual approach image alignment gradient descent 
variety numerical gorithms proposed gradient descent defacto standard 
propose unifying framework image alignment describing various algorithms extensions consistent manner 
framework concentrate inverse compositional algorithm efficient algorithm proposed :10.1.1.1.2463
examine exten sions lucas kanade algorithm applied inverse compositional algorithm significant loss efficiency extensions require additional computation 
possible provide empirical results illustrate various algorithms extensions 
part series cover image alignment linear appearance variation 
linear appearance variation considered number authors notably hager belhumeur illumination black jepson general appearance variation cootes taylor non rigid face modeling :10.1.1.18.5663:10.1.1.1.2463:10.1.1.147.629
part distinguish cases error function euclidean norm case error function general weighted norm similar error function robust error function 
consider euclidean case section 
derive simultaneous inverse com positional algorithm name implies performs simultaneous optimization warp appearance parameters 
derive efficient approximation simultaneous inverse compositional algorithm describe extremely efficient project algorithm proposed hager belhumeur :10.1.1.147.629:10.1.1.147.629
project algorithm projects appear ance variation just solves warp parameters 
second step solves appearance parameters 
study project algorithm case appearance vari ation contains gain term show step size computed incorrectly propose way correcting error 
describe normalization algorithm attempts input image appearance component template 
variant normalization algorithm frequently appearance variation consists gain bias 
section empirically comparing euclidean algorithms variants 
consider robust case section 
derive robust versions simultaneous normalization algorithms 
algorithms inefficient part derive efficient approximations spatial coherence outliers 
possible directly generalize project algorithm notion orthogonality robust error function 
empirical evaluation robust appearance variation algorithms 
background image alignment algorithms lucas kanade algorithm original image alignment algorithm lucas kanade algorithm :10.1.1.1.2463:10.1.1.49.2019
goal lucas kanade algorithm align template image column vector containing pixel coordinates 
set allowed warps pixel template goal lucas kanade algorithm input image vector parameters 
warp maps sub pixel location image denote parameterized takes goal lucas kanade algorithm minimize sum squared error images template image warped back coordinate frame template warping back compute requires interpolating image sub pixel loca minimization equation performed respect tions performed pixels sum template image minimizing expression equa tion non linear optimization task linear pixel values general non linear fact pixel values essentially un related pixel coordinates optimize expression equation lucas kanade algorithm assumes current estimate known iteratively solves increments parameters expression approximately minimized respect parameters updated steps iterated estimates parameters converge 
typically test convergence norm vector derivation lucas kanade algorithm threshold lucas kanade algorithm gauss newton gradient descent non linear optimization algorithm derived follows 
non linear expression equation linearized performing order taylor expansion give expression gradient image evaluated computed coordinate frame warped back coordinate frame current estimate warp 
follow notational convention partial derivatives respect column vector laid row vector 
convention advantage chain rule results matrix multiplication equation 
term jacobian warp 
equation squares problem closed solution derived follows 
partial derivative expression equation respect denote steepest descent images 
setting expression equation equal zero solving gives closed form solution equation gauss newton approximation hessian matrix error image 
lucas kanade algorithm summarized consists iteratively applying equations 
gradient jacobian depend evaluated general steepest descent images hessian recomputed iteration algorithm 
see 
assume number warp parameters number pixels computational cost iteration lucas kanade algorithm expensive step far step 
see table summary details 
total iterate lucas kanade algorithm warp compute compute error image equation warp gradient evaluate jacobian compute steepest descent images equation compute hessian matrix equation invert compute compute update parameters lucas kanade algorithm consists iteratively applying equations estimates parameters converge :10.1.1.1.2463:10.1.1.49.2019:10.1.1.49.2019:10.1.1.49.2019
typically test convergence norm vector user specified threshold gradient evaluated jacobian evaluated steps repeated iteration algorithm 
table computation cost iteration lucas kanade algorithm 
number warp parameters number pixels template cost iteration expensive step far step computation hessian takes time step step step step step step step step step total inverse compositional algorithm goal inverse compositional algorithm number authors pointed huge computational cost re evaluating hessian iteration lucas kanade algorithm :10.1.1.147.629
hessian constant precomputed re 
proposed inverse compositional algorithm way reformulating image alignment hessian constant precomputed 
goal inverse compositional algorithm lucas kanade algorithm see equation inverse compositional algorithm iteratively minimizes respect expression updates warp composition warps expression inverse lucas kanade algorithm iteratively applies equations 
inverse composi tional algorithm iteratively applies equations 
somewhat surprisingly algorithms shown equivalent order take approximately steps minimize expression equation 
see proof equivalence 
derivation inverse compositional algorithm performing order taylor expansion equation gives assuming identity warp solution squares problem steepest descent images replaced hessian matrix computed new steepest descent images pre compute iterate inverse compositional algorithm evaluate gradient template evaluate jacobian compute steepest descent images equation compute hessian matrix equation invert warp compute compute error image equation compute compute update warp inverse compositional algorithm :10.1.1.1.2463:10.1.1.1.2463:10.1.1.1.2463
computationally demanding steps performed pre computation step 
main algorithm simply consists image warping step image differencing step image dot products step multiplication inverse hessian step update warp step 
steps efficient take time jacobian evaluated steepest descent im ages hessian depends pre computed 
inverse compositional algorithm summarized 
see schematic diagram algorithm :10.1.1.1.2463
inverse compositional algorithm far computationally efficient lucas kanade algorithm 
see table summary 
time consuming steps steps performed pre computation time additional cost inverting composing steps typically require operations 
see 
potentially steps fairly involved computational overhead completely negligible 
cost inverse compositional algorithm iteration substantial saving 
linear appearance variation euclidean norm algorithms aim minimize expression equation 
performing minimization implicitly assumes template appears input image table computation cost inverse compositional algorithm 
time pre computation cost computing steepest descent images hessian steps cost iteration substantial saving lucas kanade iteration cost albeit warped pre step step step step total computation step step step step step total iteration various scenarios may want assume appears input image warped appropriately set known appearance variation images set unknown appearance parameters 
example want allow arbitrary change gain bias template input image set generally appearance images image 
appropriate values expression equation model possible gain bias 
model arbitrary linear illumination variation general appearance variation :10.1.1.147.629
expression equation appear appropriately warped input image equation minimize simultaneously respect warp appearance parameters remainder section derive different algorithms variants minimize expression equation evaluating algorithms section 
simultaneous inverse compositional algorithm goal algorithm algorithm performs gauss newton gradient descent simultaneously warp appearance parameters 
inverse compositional parameter update warp param eters 
appearance parameters updated usual additive algorithm 
composition meaning 
replacing equation inverse compositional algorithm minimize equation operates iteratively minimizing simultaneously respect derivation algorithm appearance parameters performing order taylor expansion assuming section identity warp gives neglecting second order terms expression simplifies simplify notation denote updating warp equation appearance parameters similarly denote dimensional column vector containing warp parameters concatenated denote modified error image equation simplifies minimum attained hessian appearance variation dimensional steepest descent images summary simultaneous inverse compositional algorithm appearance variation proceeds iteratively applying equations compute incremental appearance parameters extracted updates warp update warp appearance parameters unfortunately steepest descent images depend appearance parameters re computed iteration 
result algorithm summarized 
algorithm slower original lucas kanade algorithm computational cost steps depends total number parameters warp parameters see table summary computation cost 
just number pre compute iterate simultaneous inverse compositional algorithm evaluate gradients evaluate jacobian warp compute compute error image equation compute steepest descent images equation compute hessian matrix equation invert compute compute update simultaneous inverse compositional algorithm appearance variation operates iteratively applying equations compute incremental updates warp appearance parameters extracted update parameters step 
steepest descent images depend appearance parameters see equation steps performed iteration 
see table summary computational cost 
table computation cost simultaneous inverse compositional algorithm 
algorithm slower lucas kanade algorithm computational cost steps depends total number parameters just number warp parameters pre step step total computation step step step step iteration step step step total efficient approximation main reason simultaneous inverse compositional algorithm slow steep est descent images depend appearance parameters 
see equation 
possible ap proximation algorithm assume appearance parameters vary significantly 
steepest descent images computed initial estimates appearance parameters 
updated 
result steps moved pre computation 
time pre computation cost cost iteration huge performance increase 
result efficient inverse compositional approximation results algorithm appearance variation computational cost steps depends total number parameters just number warp parameters section empirically investigate extent efficiency approximation reduces robustness speed convergence simultaneous inverse compositional algorithm 
project inverse compositional algorithm goal algorithm optimization equation non linear respect warp parameters linear respect appearance parameters hager belhumeur proposed way decomposing similar optimization steps :10.1.1.147.629
step non linear opti mization respect warp parameters performed subspace appearance variation ignored 
second step closed form linear optimization respect appearance parameters 
refer type algorithm project algorithm step appearance variation projected derive equivalent project inverse compositional algorithm 
hager belhumeur algorithm general inverse compositional algorithm :10.1.1.147.629:10.1.1.147.629
point failing project algorithm estimation step size propose method correcting 
derivation algorithm treat images vectors pixels rewrite equation unweighted euclidean norm 
expression minimized ously respect denote linear subspace spanned collection vectors orthogonal complement equation rewritten denotes euclidean norm vector projected linear subspace second terms immediately simplifies 
norm second term considers component vector orthogonal complement dropped 
wish minimize second terms depend term exactly term component minimum value represent vector result simultaneous minimum sequentially minimizing second term respect treating optimal value minimize term respect assuming appearance variation vectors constant orthonormal easily gram schmidt minimization term closed form solution difference minimizing second term equation original goal lucas kanade algorithm see equation need linear subspace working subspace achieved weighted norm assuming vectors orthonormal minimizing minimizing second term equation minimizing expression equations exactly thing :10.1.1.1.2463:10.1.1.1.2463
see details 
inverse compositional algorithm weighted norm described part minimize second term equation 
weighted steepest descent images see equation computed unweighted steepest descent images ing component direction computed projected turn 
weighted hessian matrix inner product vectors projected linear subspace just projected linear subspace 
see details 
summary minimizing expression equation simultaneously respect performed minimizing second term equation respect inverse compositional algorithm quadratic form equation 
changes pre compute iterate post computation project inverse compositional algorithm evaluate gradient template evaluate jacobian compute steepest descent images equation compute hessian matrix equation invert warp compute compute error image equation compute compute update warp compute appearance parameters equation project inverse compositional algorithm similar original inverse compositional algorithm 
changes compute project steepest descent images step compute project hessian step compute appearance parameters step 
online computational cost identical original algorithm summarized table 
needed algorithm weighted steepest descent images equation weighted hessian equation 
inverse compositional algorithm converged optimal value computed equation optimal warp parameters 
project inverse compositional algorithm summarized 
computational cost project inverse compositional algorithm identical original inverse compositional algorithm 
see table 
extra cost step step 
computation steepest descent images step substantially involved step pre computation step 
computation appearance parameters minimal just time caveat step size modeling gain computation cost summarized table 
project algorithm performs demonstrated section 
scenario performs particularly poorly 
particular linear appearance table computational cost project inverse compositional algorithm identical original inverse compositional algorithm 
additional cost computing steepest descent images step extra cost computing appearance parameters step 
pre step step step step total computation step step step step step total iteration post step total computation variation model gain step size estimated incorrectly show 
mentioned common linear appearance variation model gain bias 
gain image equation represented 
suppose gain modeled set input image bias relative original template 
gain bias perfect match template appearance correction set warp parameters input image set warp parameters appearance correction appearance variation consists term disappears direction 
inner product update parameter update perfect match steepest descent images component zero 
similarly parameter project algorithm inputs converge warp parameters algorithm takes steps larger factor second case 
algorithm diverge takes big steps 
algorithm probably converge rate convergence slow 
various ways correct step size problem 
possibility dynamic step size adjustment algorithm levenberg marquardt 
algorithms check step algorithm results improvement error 
error improve algorithm diverging smaller step size tried 
possibility compute magnitude component direction normalized appropriately reduce step size computed project algorithm factor step seen gain approximately step size correction affect algorithm 
similarly gain step size corrected appropriately 
additional computational cost evaluating equations just negligible 
section empirically evaluate step size correction algorithm defined equations 
discussion suppose set images spanned image appearance model variety different templates 
templates project algorithm result theoretically 
particular model includes bias term image bad choice template 
case gradient template steepest descent images exactly zero degenerate case algorithm applicable 
degeneracy suggests images choice template 
question best choice template choice template affects project algorithm differently simultaneous algorithm outside scope left study 
normalization inverse compositional algorithm goal algorithm frequently way coping gain bias variation normalize template input image particular applications face recognition eigenfaces mean pixel intensities colors set zero vector set unit norm similarly steps applied gain bias 
gain bias easily computed derive similar normalization algorithm arbitrary linear variation 
derivation algorithm equation re arranged result remove effect equation regarded projecting unit vector proposal normalization algorithm perform analogous step appearance vector minor difference projecting entire component direction project component template possible way apply appropriate normalization directly step algorithm see 
turns simple algebra normalization achieved normalizing error image component error image direction zero component 
added benefit estimate appearance parameter process 
particular normalization step consists assume direction computed orthonormal 
inserting step inverse composi tional algorithm gives normalization inverse compositional algorithm summarized 
difference normalization algorithm inverse compositional algorithm addition step 
computation cost algorithms similar 
see ta ble summary 
step performed iteration analogous step project algorithm normalization algorithm substantially slower project inverse compositional algorithm step size adjustment modeling gain normalization inverse compositional algorithm prone error step size esti mate project algorithm see section 
correction applied 
normalization algorithm compute step size correction efficiently 
pre compute iterate normalization inverse compositional algorithm evaluate gradient template evaluate jacobian compute steepest descent images equation compute hessian matrix equation invert warp compute compute error image equation estimate compute compute compute update warp equation normalization inverse compositional algorithm exactly original inverse compositional algorithm 
difference addition step input image normalized error image 
computation cost algorithm similar summarized table 
normalization algorithm lot slower 
table computation cost normalization inverse compositional algorithm identical original inverse compositional algorithm 
change addition step 
step analogous step project algorithm 
difference step performed iteration normalization algorithm lot slower project algorithm 
pre step step step step total computation step step step step step step total iteration modeling gain step size corrected equation estimated section empirically evaluate step size correction defined equations 
experimental results conducted variety experiments compare performance linear appearance variation algorithms simultaneous inverse compositional project normalization variants 
experimental procedure similar 
particular started image 
manually selected pixel template center face 
added appearance variation exact choice depends experiment question described detail 
randomly generated affine warps manner 
procedure 
selected canonical points template 
bottom left corner bottom right corner center top pixel canonical points 
randomly perturbed points additive white gaussian noise certain variance fit affine warp parameters perturbed points define 
warped affine warp run various algorithms starting identity warp 
appropriate appearance parameters initialized parameters affine warp different units combined way 
error measure 
current estimate warp compute destinations canonical points compare correct locations 
compute rms error points distance current correct locations 
prefer error measure normalizing units parameters error comparable 
compute average rate convergence average frequency conver gence large number randomly generated inputs precise 
input consists different randomly generated affine warp 
simultaneous normalization algorithms plot rate convergence appearance parameters 
measure meaningful project algorithm algorithm converged 
error measure appearance parameters euclidean norm appearance parameter vector experiment comparison inverse compositional goal experiment show linear appearance variation algorithms cope appearance variation perform original inverse compositional algorithm appearance variation 
experiments investigate algorithms compare varying degrees appearance variation presence additive noise 
just consider simple case 
particular just appearance variation image specific image image different face approximately aligned face template 
described section making code available reader experiment choices results shown 
figures include plots convergence rate frequency convergence appearance variation 
figures include similar plots relatively large amount appearance variation appearance variation accounts approximately quarter combined input image 
include curves algorithms original inverse compositional algorithm ap variation modeling ic simultaneous inverse compositional algorithm sic efficient variant simultaneous algorithm sic ea project algorithm po normalization algorithm nic 
main things note appearance variation algorithms perform identically original inverse compositional algorithm performs far worse appearance variation algorithms perform similarly 
results demonstrate appearance variation algorithms cope fairly substantial linear appearance variation original inverse compositional algorithm 
experiment varying experiment investigate performance appearance variation algorithms sic sic ea po nic varies amount appearance variation 
re ran experiment rms point error rms point error ic sic sic ea po nic iteration convergence rate iteration ic sic sic ea po nic converged ic sic sic ea po nic point sigma convergence frequency converged ic sic sic ea po nic point sigma convergence rate convergence frequency comparison inverse compositional algorithm appearance variation modeling ic different linear appearance variation algorithms simultaneous inverse compositional algorithm sic efficient variant simultaneous algorithm sic ea project algorithm po normalization algorithm nic 
contain results demonstrate algorithms perform similarly appearance variation 
contain results demonstrate inverse compositional algorithm breaks appearance variation appearance variation algorithms able cope appearance variation 
single appearance image variety different values include results figures plot rate convergence frequency convergence 
results show increases algorithms perform differently 
simultaneous algorithm performs far best performance algorithms sic ec po nic far worse 
note contribution rms point error converged sic sic ea po nic iteration convergence rate sic sic ea po nic point sigma convergence frequency error norm sic sic ea po nic iteration rms point error converged sic sic ea po nic iteration convergence rate sic sic ea po nic point sigma convergence frequency error norm iteration sic sic ea po nic appearance convergence appearance convergence comparison appearance variation algorithms sic sic ea po nic varying amounts appearance variation 
results show simultaneous algorithm sic performs far better algorithms large amounts appearance variation 
plot rate convergence appearance parameter algorithms estimate iteration estimate estimated algorithm converged 
estimate point accurate algorithms 
converges quickly 
project algorithm appearance image input image twice contribution template case template impossible see input image 
simul algorithm performs far best appearance variation large 
note algorithms perform identically 
project normalization algo rithms similar 
subspace orthogonal appearance variation 
project algorithm projects steepest descent images subspace 
normal ization algorithm projects error image subspace 
expected perform identically 
fact efficient approximation simultaneous algorithm performs identically harder explain 
see section discussion 
simultaneous algorithm perform far better algorithms 
fairly easy understand efficient approximation simultaneous algorithm perform 
approximation steepest descent images explicit efficient approximation algorithm 
normalization project algorithms poor performance caused fact component error image caused appearance variation subspace orthogonal appearance variation non zero alignment correct 
implicitly assumed component zero true template aligned input image 
non zero component generates unexpected perturbation parameter updates large cause algorithm diverge 
figures plot rate convergence estimate appearance vari ation parameter results show estimates appearance variation algorithms estimate iteration sic sic ea nic converge quickly 
project algorithm appearance variation estimated algorithm converged estimated estimated accurately algorithms 
experiment varying number appearance images experiment investigate performance appearance variation algorithms varies number appearance images 
need decide ap images appearance parameters 
chose appearance images large image scenery randomly selecting number sub images appropriate size 
results avoid depen dence magnitude appearance variation see experiment chose appearance parameters vary number appearance images 
set spread appearance variation equally 
results appearance images appearance images shown 
main point note performance appearance variation algorithms identical 
small drop performance appearance images evidence algorithms performs significantly worse 
experiment robustness additive noise experiment investigate performance appearance variation algorithms varies presence noise 
repeated conditions experiment procedure added zero mean white gaussian noise input image ran algorithms 
results additive noise standard deviation grey levels grey levels included 
results show simultaneous algorithm slightly robust noise algorithms perform identically 
ran experiments adding noise template appearance images 
results show algorithms perform similarly algorithms significantly robust noise 
results omitted lack space regenerated code making available 
see section experiment modeling gain experiment investigate performance algorithms varies modeling gain 
model gain set run algorithms variety different values appearance variation algorithms sic sic ea po nic run project rms point error converged error norm sic sic ea po nic iteration rms point error sic sic ea po nic iteration convergence rate app 
images convergence rate app 
images sic sic ea po nic point sigma converged sic sic ea po nic point sigma convergence frequency app 
images convergence frequency app 
images sic sic ea po nic iteration error norm sic sic ea po nic iteration appearance convergence app 
images appearance convergence app 
images comparison appearance variation algorithms varying numbers appearance images 
performance appearance images similar performance appearance images 
performance slightly worse images 
little evidence algorithms performs worse 
rms point error converged error norm sic sic ea po nic iteration rms point error sic sic ea po nic iteration convergence rate noise sd convergence rate noise sd sic sic ea po nic point sigma converged sic sic ea po nic point sigma convergence frequency noise sd convergence frequency noise sd sic sic ea po nic iteration error norm sic sic ea po nic iteration appearance convergence noise sd appearance convergence noise sd comparison appearance variation algorithms varying amounts zero mean white gaussian noise added input image algorithms run 
experimental conditions identical experiment 
results noise standard deviation grey levels included noise standard deviation grey levels 
simultaneous algorithm slightly robust noise appearance variation algorithms 
algorithm step size correction described section po ss normalization algorithm step size correction described section nic ss 
results gain included figures results gain figures results gain figures 
gain project normalization algorithms converge far slowly simultaneous algorithm 
perturbation affine warp large algorithms converge slowly iterations trials converge 
convergence frequency affected 
convergence frequency project normalization algorithms far worse algorithms 
normalization algorithms converge time 
looking rate convergence gain project plot see converge higher error algorithms 
point project normalization algorithms oscillate correct solution 
verified watching algorithms converge 
algorithms take step roughly twice reach approximately quadratic part error function close correct answer remain equally far away correct answer oscillating backwards forwards 
implementation loose definition convergence rms point error pixels cases counted converging frequency convergence unaffected 
gain project normalization algorithms fail converge time 
take steps twice big immediately diverge 
previous experiments project algorithm normalization algorithm efficient approximation simultaneous algorithm perform identically 
step size corrections project normalization algorithms appear correctly 
modified algorithms perform similarly simultaneous algorithm 
note underlying cause poor performance unmodified project normalization algorithms large gain variation essentially cause poor performance algorithm large appearance variation experiment 
difference rms point error sic sic ea po po ss nic nic ss iteration converged sic sic ea po po ss nic nic ss point sigma convergence rate gain convergence freq gain rms point error sic sic ea po po ss nic nic ss iteration converged sic sic ea po po ss nic nic ss point sigma convergence rate gain convergence freq gain rms point error sic sic ea po po ss nic nic ss iteration converged sic sic ea po po ss nic nic ss point sigma convergence rate gain convergence freq gain comparison appearance variation algorithms modeling gain 
addition algorithms studied previous experiments consider project algorithm step size correction described section po ss normalization algorithm step size correction described section nic ss 
performance project normalization algorithms step size correction significantly worse simultaneous algorithm 
step size correction modification algorithm correct problem resulting better performance 
appearance variation models gain error computation parameter updates manifested step size error corrected simply corrections sec tions 
similar correction applied algorithms arbitrary large appearance variation open question left 
linear appearance variation robust error function generalization expression equation robust error function sum squares euclidean norm 
robust extensions original inverse composi tional algorithm subject part series 
goal minimize respect warp parameters symmetric robust error function vector scale parameters 
see discussion consider symmetric error functions :10.1.1.1.2463
described scale parameters estimated error image ease explanation treat scale parameters known constants drop scale parameters simply denote robust function see discussion choose section consider combination linear appearance variation robust error function 
particular investigate minimize simultaneously respect warp appearance parameters 
section review inverse compositional iteratively reweighted squares algorithm optimize equation 
proceed sections describe robust extensions simultaneous normalization algorithms section 
algorithms ficient 
derive efficient approximations spatial coherence 
possible generalize project algorithm notion orthogonality robust error function 
explain detail section 
section em evaluating robust appearance variation algorithms efficient approximations 
background ic iteratively reweighted squares goal algorithm inverse compositional iteratively reweighted squares algorithm minimizes expression equation iteratively approximately minimizing respect updating warp proof order equivalence iterating steps forwards additive lucas kanade minimization expression equation contained 
derivation algorithm performing order taylor expansion assumed equation gives identity warp 
expanding gives error image 
performing taylor expansion gives inverse compositional iteratively reweighted squares pre compute iterate evaluate gradient template evaluate jacobian compute steepest descent images warp compute compute error image compute hessian matrix equation compute compute equation update warp inverse compositional iteratively reweighted squares algorithm consists iteratively applying equation updating warp hessian depends warp parameters re computed iteration 
naive implementation algorithm slow original lucas kanade algorithm 
see table details 
table computational cost inverse compositional iteratively reweighted squares algorithm 
cost iteration asymptotically slow lucas kanade algorithm 
algorithm slow considered efficient approximations algorithm algorithm takes advantage spatial coherence outliers 
approximations move cost computing hessian pre computation 
pre step step step total computation step step step step step step total iteration minimum quadratic form attained hessian matrix 
algorithm summarized 
computational cost iteratively reweighted squares algorithm summarized table 
algorithm slow lucas kanade algorithm 
algorithm slow considered efficient approximations algorithm algorithm takes advantage spatial coherence outliers 
approximations move cost computing hessian pre computation 
empirically second algorithms performs far better 
spatial coherence approximation spatial coherence approximation template subdivided set sub templates blocks 
usually template rectangular blocks sub rectangles choices possible 
suppose blocks tion rewritten pixels block 
equa spatial coherence outliers assume constant block assume say mately practice assumption holds approxi estimated example setting mean value computed block 
equation rearranged internal part expression depend robust function constant iterations 
denote hessian simplifies hessian sub template precomputed 
equation hessian vary iteration iteration cost computing minimal substantially smaller inverse compositional iteratively reweighted squares algorithm 
see table 
typically number pixels template cost computing hessian step original spatial coherence approximation inverse compositional iteratively reweighted squares algorithm just consists equation estimate hessian step equation 
equation course requires hessians block total cost pre computation simultaneous ic iteratively reweighted squares goal algorithm precomputed robust simultaneous inverse compositional algorithm operates iteratively minimizing simultaneously respect derivation algorithm equivalent equation section appearance parameters updating warp defined equation 
section leads 
pre compute robust simultaneous inverse compositional algorithm iterate evaluate gradients evaluate jacobian warp compute compute error image equation compute steepest descent images equation compute hessian matrix equation invert compute compute update robust simultaneous inverse compositional algorithm identical euclidean version 
main difference steps term added summation computational cost asymptotically 
see table details 
defined equation 
robust simultaneous inverse compositional algorithm summarized computational cost table 
algorithm asymptotically just slow euclidean version 
efficiency approximation proposed section applied robust gorithm update steepest descent images approximation effective improving efficiency 
iteration iteration 
computation steepest descent images step moved pre computation hessian depends current error image 
see equation 
order move step pre computation need spatial coherence approximation 
see section details 
combined approximations lead reasonably efficient algorithm takes time iteration 
section empirically evaluate algorithms efficient 
table computation cost robust simultaneous inverse compositional algorithm asymptotically exactly computational cost euclidean version 
see table comparison 
pre step step total computation step step step step iteration step step step total normalization ic iteratively reweighted squares goal algorithm key step euclidean normalization algorithm section normalize input image component direction template turns normalization applied error image 
see equation details 
unfortunately longer possible equation perform normalization equation relies fact appearance vectors orthonormal 
robust error function appearance vectors longer orthonormal 
derivation algorithm goal normalization step equation component error image direction zero whilst computing time 
need formulate problem robust error function 
suppose current estimate error image wish compute updates appearance parameters minimize squares minimum expression appearance hessian robust equivalent normalization equation compute equations 
appearance parameters updated error image updated equation 
hessian steepest descent parameter updates needs include robust error function robust normalization algorithm summarized computational cost ta ble 
computation cost robust normalization inverse compositional algorithm far euclidean version table 
computation hessian step moved iteration computation 
error image normalization step far computationally demanding computation appearance hes sian equation 
possible apply spatial coherence approximation see section computation hessian step computation ap hessian step 
computation computationally demanding steps moved pre computation 
hessians computed spatial coherence approximation result reasonably efficient algorithm takes pre compute robust normalization inverse compositional algorithm iterate evaluate gradient template evaluate jacobian compute steepest descent images equation warp compute compute error image equation compute equations update compute hessian matrix compute compute update warp equation invert robust normalization inverse compositional algorithm similar euclidean version 
changes robust method normalize error image steps incorporation weighting function steps 
table computation cost robust normalization inverse compositional algorithm far euclidean version table 
computation hessian step moved computation 
error image normalization step far computationally demanding primarily computation appearance hessian equation 
pre step step step total computation step step step step iteration step step step total time iteration 
section empirically evaluate algorithms robust normalization algorithm efficient approximation 
project ic iteratively reweighted squares euclidean project algorithm uses linear algebra turn simultaneous optimization sequential optimization key step derivation transition equation equation orthogonality 
unfortunately move robust error function equation equivalent step possible 
error image components orthogonal subspaces 
orthogonality defined general decomposed sum robust project algorithm sequentially solves approximation ignore lack orthogonality just continue euclidean project steepest descent images 
approach taken algorithm keep hessian constant yield efficient algorithm :10.1.1.147.629
section empirically compare algorithm robust simultaneous normalization algorithms 
experimental results evaluate robust appearance variation algorithms robust simultaneous inverse compositional algorithm rsic efficient approximation robust simultaneous inverse compositional algorithm spatial coherence approximation hessian rsic ea sc robust normalization inverse compositional algorithm rnic efficient spatial coherence approximation normalization inverse compositional algorithm rnic sc robust project algorithm similar hager belhumeur algorithm ignores lack orthogonality appearance images keeping steepest descent images constant approximates hessian algorithm rpo :10.1.1.147.629
described evaluating robust fitting algorithms difficult obvious noise model 
assume main cause noise outliers occlusion generate input image manner 
evaluations governed parameter note possible derive robust variants project algorithm projecting steepest descent images subspace orthogonal appearance variation weighted norm weighting function re appearance images iteration respect weighted norm 
algorithms sequentially solve appearance parameters computed iteration estimate variants slower robust normalization algorithm perform identically reason project normalization algorithms perform identically section 
algorithms true project algorithms slower robust normalization algorithm perform better describe 
percentage occlusion 
parameter randomly generate rectangle entirely template region occludes template appropriate percentage 
allow small relative error occlusion region allow discrete nature pixels 
synthetically occlude randomly generated rectangle replacing part image image appropriate size 
variety occluding images 
lack space just occluder extracted image natural scenery 
thing evaluating robust fitting algorithms hard choosing robust error function 
robust error function function classifies pixels outliers magnitude error larger scale parameter inliers weighted equally outliers zero weight 
estimate scale parameter assuming know number outliers 
estimate sorting error values setting correct number pixels classified outliers 
experiment varying percentage occlusion investigate variation performance algorithms varying percentage occlusion 
experiment set image scenery section set results occlusion shown figures occlusion figures occlusion figures 
lack space include plots rate convergence frequency convergence 
expected robust simultaneous inverse compositional algorithm performs best robust normalization algorithm performing slightly worse particularly higher levels occlusion 
efficient versions algorithms rsic ea sic rnic sc perform similarly slightly worse robust normalization algorithm 
robust project algorithm ignoring orthogonality algorithm rpo performs far worse rms point error rms point error rms point error rsic rsic ea sc rpo rnic rnic sc iteration converged rsic rsic ea sc rpo rnic rnic sc point sigma convergence rate occlusion convergence frequency occlusion rsic rsic ea sc rpo rnic rnic sc iteration converged rsic rsic ea sc rpo rnic rnic sc point sigma convergence rate occlusion convergence frequency occlusion rsic rsic ea sc rpo rnic rnic sc iteration converged rsic rsic ea sc rpo rnic rnic sc point sigma convergence rate occlusion convergence frequency occlusion comparison robust appearance variation algorithms robust simultaneous algorithm rsic efficient approximation rsic spatial coherence approximation hessian rsic ea sc robust normalization algorithm rnic efficient spatial coherence approximation rnic rnic sc robust project algorithm ignores lack orthogonality appearance images approximates hessian algorithm rpo 
rsic performs best rpo far worst highlighting importance right algorithm 
especially larger levels occlusion 
poor results illustrate important right algorithm 
minor modification algorithms result poor performance 
experiment varying investigate variation performance algorithms varying set image scenery 
set percentage occlusion 
results shown figures figures figures 
results similar experiment rsic performing best followed rnic 
experimental results section gap algorithms increases efficient variants algorithms rsic ea sc rnic sc perform similarly efficient algorithms perform significantly worse 
efficient variation normalization algorithm rnic sc performs slightly better 
experiment robust project algorithm rpo performs quite poorly cases 
experiment varying number appearance images investigate variation performance number appearance images 
fix percentage occlusion follow procedure experiment sec tion 
chose appearance images large image scenery randomly selecting number sub images appropriate size 
results avoid dependence magnitude appearance variation chose appearance parameters appearance images 
set vary number spread appearance variation equally 
results shown figures ures fig figures 
results similar experiment 
performance algorithms vary significantly number rms point error rms point error rms point error rsic rsic ea sc rpo rnic rnic sc iteration convergence rate rsic rsic ea sc rpo rnic rnic sc iteration convergence rate rsic rsic ea sc rpo rnic rnic sc iteration converged rsic rsic ea sc rpo rnic rnic sc point sigma convergence frequency converged rsic rsic ea sc rpo rnic rnic sc point sigma convergence frequency converged rsic rsic ea sc rpo rnic rnic sc point sigma convergence rate frequency convergence evaluation algorithms varying appearance parameter keeping percentage occlusion fixed 
experimental conditions remain 
rsic performs best rnic close 
gap algorithms increases efficient algorithms rsic ea sc rnic sc similarly quite poorly larger 
rpo performs fairly poorly cases 
rms point error rms point error rms point error rsic rsic ea sc rpo rnic rnic sc iteration converged rsic rsic ea sc rpo rnic rnic sc point sigma convergence rate convergence frequency rsic rsic ea sc rpo rnic rnic sc iteration converged rsic rsic ea sc rpo rnic rnic sc point sigma convergence rate convergence frequency rsic rsic ea sc rpo rnic rnic sc iteration converged rsic rsic ea sc rpo rnic rnic sc point sigma convergence rate convergence frequency evaluation algorithms varying number appearance images keeping total amount appearance variation fixed results similar experiment 
algorithms rsic rsic ea sc rnic rnic sc perform identically cases rpo performing significantly worse 
table summary algorithms described section image alignment linear appearance variation euclidean norm 
main algorithms simultaneous inverse compositional algorithm project inverse compositional algorithm normalization inverse compositional algorithm 
main algorithms variant 
algorithm computational cost performance gain ok simultaneous ic project ic normalization ic efficient approximation simultaneous ic project ic ss correction normalization ic ss correction medium medium medium medium medium appearance images 
magnitude appearance variation important 
algo rithms rsic rsic ea sc rnic rnic sc perform identically cases 
experiments rpo performs significantly worse algorithms 
summary section investigated problem image alignment linear appearance variation euclidean norm 
described main algorithms simultaneous inverse com positional algorithm project inverse compositional algorithm normalization inverse compositional algorithm 
described efficient approximation simultaneous algorithm step size corrections project normalization algorithms 
algo rithms summarized table 
algorithms simultaneous algorithm performs best unfortunately slow 
project normalization efficient approximation simultaneous algorithm efficient perform significantly worse main scenarios magnitude appearance variation large comparable magnitude tem plate algorithm model gain 
step size correction variants project algorithm normalization algorithm perform second cases break presence general large magnitude appearance variation 
table summary algorithms described section image alignment linear appearance variation robust error function 
main algorithms robust simultaneous inverse compositional algorithm robust normalization inverse compositional algorithm 
algorithms slow derived efficient approximation 
algorithm computational cost performance robust simultaneous ic rsic robust normalization ic rnic efficient rsic rsic ea sc efficient rnic rnic sc medium medium medium point note project normalization efficient approximation simultaneous algorithm perform identically experiments 
project normalization algorithms subspace orthogonal appearance variation 
project algorithm projects steepest descent images subspace 
normalization algorithm projects error image subspace 
surprising perform identically 
fact efficient approximation simultaneous algorithm performs identically harder explain 
finding explanation left 
section investigated problem image alignment linear appearance variation robust error function 
described main algorithms robust simultaneous verse compositional algorithm robust normalization inverse compositional algorithm 
described efficient approximations algorithms 
algorithms summarized table 
algorithms robust simultaneous algorithm performs best unfortunately slow 
robust normalization algorithm performs similarly slightly worse 
efficient approximations algorithms perform fairly ex cept magnitude appearance variation large comparable template 
empirically compared algorithms robust project algorithm similar hager belhumeur algorithm ignores lack orthogonality appearance images keeping steepest descent images constant approximates hessian algorithm :10.1.1.147.629
algorithm performs significantly worse algorithms table 
discussion parts series papers discussed best algorithm 
discussion centered topics nature noise efficient algorithm required 
true 
best algorithm depends noise computational requirements 
image noise approximately gaussian euclidean norm algorithms described section 
appropriate robust error function algorithms section 
computational speed issue best euclidean algorithm simultaneous algorithm 
clearly performs best experiments 
high efficiency required project algorithm fastest performs efficient algorithms efficient approximation simultaneous algorithm normalization algorithm 
care taken large appearance variation 
cases project algorithm perform near simultaneous algorithm 
system model gain step size correction variant project algorithm slow algorithm significantly increases robustness substantially 
terms robust algorithms computational speed issue best algorithm robust simultaneous inverse compositional algorithm 
clearly performs best experiments 
efficiency required efficient approximation robust normalization inverse compositional algorithm spatial coherence compute hessian probably best choice 
care taken large appearance variation 
magnitude appearance variation half magnitude template performance efficient robust normalization algorithm adversely affected 
part series papers covered forwards additive forwards compositional inverse additive inverse compositional algorithms 
covered newton gauss newton steepest descent levenberg marquardt diagonal hessian variants inverse composi tional algorithm 
part covered choice error function developed algorithms weighted norms robust error functions 
part covered addition linear appearance variation euclidean norm robust error function 
upcoming final part cover addition priors warp appearance parameters euclidean norm robust error function 
matlab code test images scripts matlab implementations algorithms described available world wide web www ri cmu edu projects project html 
include test images scripts generate experimental results 
acknowledgments research described conducted department defense contract 
baker gross matthews ishikawa 
lucas kanade years unifying framework part 
technical report cmu ri tr carnegie mellon university robotics institute 
baker matthews 
equivalence efficiency image alignment algorithms 
proceedings ieee conference computer vision pattern recognition volume pages 
baker matthews 
lucas kanade years unifying framework part quantity approximated warp update rule gradient descent approximation 
international journal computer vision accepted appear 
bergen anandan hanna hingorani 
hierarchical model motion estimation 
proceedings european conference computer vision pages 
black jepson :10.1.1.18.5663:10.1.1.1.2463
eigen tracking robust matching tracking articulated objects view representation 
international journal computer vision 
christensen johnson 
image consistent registration 
ieee transactions medical imaging 
cootes edwards taylor 
active appearance models 
ieee transactions pattern analysis machine intelligence june 
dellaert collins 
fast image tracking selective pixel integration 
proceedings iccv workshop frame rate vision pages 
huber 
numerical methods nonlinear robust regression problem 
journal statistical computational simulation 
gleicher 
projective registration difference decomposition 
proceedings ieee conference computer vision pattern recognition pages 
hager belhumeur :10.1.1.147.629
efficient region tracking parametric models geometry illumination 
ieee transactions pattern analysis machine intelligence 
huber 
robust statistics 
john wiley sons 
lucas kanade :10.1.1.49.2019
iterative image registration technique application stereo vision 
proceedings international joint conference artificial intelligence pages 
matthews baker 
active appearance models revisited 
technical report cmu ri tr carnegie mellon university robotics institute 

shum szeliski 
construction panoramic image mosaics global local alignment 
international journal computer vision 

