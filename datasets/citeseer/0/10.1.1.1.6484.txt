hierarchical text categorization neural networks miguel ruiz srinivasan school library information science university iowa main library iowa city ia 
presents design evaluation text categorization method hierarchical mixture experts model 
model uses divide conquer principle de ne smaller categorization problems prede ned hierarchical structure 
nal classi er hierarchical array neural networks 
method evaluated umls metathesaurus underlying hierarchical structure ohsumed test set medline records 
comparisons optimized version traditional rocchio algorithm adapted text categorization neural network classi ers provided 
results show hierarchical structure improves text categorization performance respect equivalent model 
optimized rocchio algorithm achieves performance comparable hierarchical neural networks 
keywords automatic text categorization applied neural networks hierarchical classi ers 
system performs text categorization aims assign appropriate labels categories prede ned classi cation scheme incoming documents 
assignments varied purposes ltering retrieval 
rapid growth information automatic text categorization important goal 
task explored researchers information retrieval ir arti cial intelligence ai communities 
di erent approaches decision trees id rule learning neural networks linear classi ers nearest neighbor knn algorithms support vector machine svm naive bayes methods explored :10.1.1.54.6608:10.1.1.11.6124
interestingly researchers tried take advantage hierarchical structure available certain classi cation schemes medical subject headings mesh yahoo 
topic hierarchy 
hierarchical structure classi cation scheme re ects relations concepts domain covered classi cation 
hierarchy typically encodes set inclusion relation called relation category members 
example classi cation living things set animals includes set sh includes set kluwer academic publishers 
printed netherlands 
hme artcl tex 
directional hierarchical link connects narrower concept trout sh turn similar connection animal 
relation asymmetric dogs animals animals dogs transitive pines trees pines trees 
believe hierarchical classi cation properties categorize entity trout closely related properties class sh comparison properties corresponding class 
suggest ignoring conceptual con gurations accompany classi cation schemes may limit potential text categorization methods 
primary research goal explore hypothesis text categorization procedure capable exploiting conceptual connections categories ective procedure designed exploit information 
speci cally explore categorization strategy designed exploit hierarchical structure underlying umls uni ed medical language system metathesaurus test ectiveness ohsumed test collection 
hierarchical classi er inspired hierarchical mixture experts model proposed jordan jacobs 
built collection neural networks classi er scale larger collections categories documents divides categorization problem set related sub problems 
experiments explore value hierarchical classi er comparison non hierarchical baseline classi er state art implementation rocchio classi er 
part research goal study feature selection selection training examples goal exploring hierarchical classi ers 
section theoretical background hierarchical mixture experts model section details implementation 
sections explain di erent methods feature selection training set selection sections describe experimental collection evaluation measures respectively 
section presents details experiments performed 
section presents analysis results section compares approach hierarchical text categorization 
section presents research plans 
hme artcl tex 
theoretical framework hierarchical mixture experts hme model supervised feedforward network may classi cation regression 
principle divide conquer large problem divided smaller easier solve problems solutions combined yield solution complex problem 
various methods subdividing large problems proposed 
simplest approach divide problem subproblems common elements called hard split data 
optimum solution smaller problems chosen winner takes basis 
classi cation regression trees cart principle 
stacked generalization uses hard split data weighted sum weights derived performance smaller problems partition space 
contrast hme divides large problem sub problems common elements soft split elements series overlapping clusters 
outputs simple problems combined stochastically obtain global solution 
model basic components gating networks expert networks 
gating networks located intermediate level nodes tree receive input vector input features representing document produce scalar output weights contribution child networks 
expert networks located leaf nodes receive input produce estimate output 
example hme binary branching shown 
hme may seen cascade networks works bottom fashion input rst experts generate output output experts combined second level gates generating new output 
outputs second level gates combined root gate produce appropriate result vector components number outputs 
nodes tree represent convex sum output child nodes computed output gate output corresponding child node 
original model proposed jordan jacobs networks tree linear perceptrons 
expert network produces output generalized linear function input ux weight matrix xed continuous non linear function 
vector assumed include xed component hme artcl tex value allow intercept term 
binary classi cation problems logistic function case expert outputs interpreted log odds success bernoulli probability model 
models multi way classi cation counting rate estimation survival estimation handled making choices 
gating networks generalized linear functions 
th output gating network softmax function intermediate variable number child nodes gating network intermediate variable de ned weight vector transpose operation 
positive sum interpreted providing soft partitioning input space 
output vector nonterminal node tree weighted sum output children nonterminal 
example output th nonterminal second layer level tree number child nodes connected gate output expert child gate jth output gate note depend input output nonlinear function input 
general hme model exible 
may choose function equation gate expert decision modules appropriate application 
gates experts depend input may choose bottom top processing whichever appropriate problem 
variation hme model binary classi cation function gates 
train model top 
choice direction number categories 
described detail collection categories dataset aim scalable categorization procedure handle large classi cation schemes 
umls classi cation presently concepts full ohsumed hme artcl tex expert network expert network expert network expert network gating network gating network gating network 
hierarchical mixture experts model dataset concepts 
bottom approach inecient expert module category 
computational requirements especially severe decision module expert node neural network 
contrast topdown approach binary classi cation gate restricts number expert networks activated document 
may observed studies bottom hme approach number categories small example handwriting recognition speech recognition 
study choose ecient direction postpone exploration bottom processing research 
model binary function gate trained yield value example document categorized descendent concepts 
example domain gate general concept heart disease trained yield document categorized speci categories coronary thrombosis 
testing categorization task starts root node gate decides general concept hme artcl tex gating network gating network network expert network expert network expert network expert gating network 
modi ed hierarchical mixture experts model document 
true second level nodes decisions process repeats reaches leaf nodes 
observe experts connected gates output value activated reducing response time classi cation presents modi ed hme classi er model 
key di erence depicted gating networks binary functions 
give statistical interpretation model de ne path gates root node gate parent expert assigns category input features represent document output vector categories assigned document 
probabilistic interpretation hierarchical model follows hybrid neural trees proposed buc method called trio learning build decision tree nodes neural networks 
trio learning uses decision tree algorithm partition examples positive negative classes recursively builds hierarchy 
similar cart method performs hard split set examples level 
observe model uses prede ned hierarchical structure 
di erence soft split data allows overlapping clusters 
hme artcl tex jx jx jz jz jx hierarchical structure hme model prede ned case hierarchy heart disease subset umls classi cation system 
hme hierarchy generally limited classes appear training set 
training set described able concepts heart disease subset 
alternatives training hme model 
jordan jacobs waterhouse method expectation maximization 
assume classi cation follows multinomial model implies object assigned multiple categories available classi cation 
classi cation task may viewed competition problem 
contrast interested multiway classi cation problem equivalent independent classi cations 
allow multi way classi cation backpropagation neural networks gates experts gradient descent method training 
gates trained recognize categories descendants document 
experts trained recognize presence absence particular categories 
backpropagation networks layers see 
tested con gurations results discussed detail 
general neural networks nodes input layer corresponding set features selected expert gate middle layer nodes output layer single node 
case sigma nodes responsible combining outputs gate expert obtain kth component output intermediate node hme 
observe general model allows experts output vector size implicit assumption category assignment independent 
appropriate set features training set manually categorized documents backpropagation network learns appropriate decisions 
observe experts gates set positive examples di erent 
set positive examples noted waterhouse proposed hme model dynamically generates hierarchical structure 
hme artcl tex 
example backpropagation network inputs experts subset positive examples ancestor gate 
consequence identical neural networks trained di erent subsets learn di erent probabilistic functions 
backpropagation neural network expert node learns input estimate desired output value category gate computes con dence value combined outputs children 

implementation hme want build classi er able ectively structured knowledge contained umls metathesaurus 
particular interested hierarchical relationships link general concepts speci ones 
umls metathesaurus contains concepts collected combining vocabularies health sciences 
study limit mesh medical subject headings vocabularies 
documents ohsumed test collection subset medline manually categorized mesh terms 
medline document assigned mesh concepts 
categorization task multi way classi cation problem 
model gates represent general concepts hierarchy 
interestingly manual assignment high level mesh category automatically determined assignment lower level observe mesh hierarchy directly 
decided umls hierarchy provides conceptual mapping extended areas health sciences 
hme artcl tex categories 
fact document assigned category unstable automatically assignment ancestors tree heart diseases myocardial coronary diseases 
fact manual assignment high level categories usually done medline document topic associated level generality abstraction 
model nonterminal node represented networks 
rst expert network node category second gating network representing general concept level classi cation scheme 
heart diseases node gate learns recognize general concept representing documents descendants expert network learns assign speci category heart diseases 
note point explicitly speci ed mean categories concepts word category 
purpose comparing results studies show results obtained mesh subtree heart diseases 
shows part hierarchy 
method especially top processing general applied set subset umls 

feature selection text categorization set possible input features consists di erent words appear collection documents 
usually large set small text collections hundreds thousands features 
reduction set features train neural networks necessary performance network cost classi cation sensitive size quality input features train network :10.1.1.11.6124:10.1.1.30.3522
rst step reducing size feature set elimination words stemming algorithms 
done set features typically large useful training neural network 
broad approaches feature selection literature wrapper approach lter approach 
wrapper approach attempts identify best feature subset particular algorithm 
example neural network wrapper approach selects initial subset measures performance network generates improved set features measures performance network 
process repeated reaches termination condition minimal value performance number iterations 
lter approach hme artcl tex heart diseases coronary variant arrhythmia myocardial myocardial shock coronary coronary thrombosis coronary myocardial coronary heart disease unstable 

part umls hierarchy heart diseases subtree 
commonly text categorization attempts assess merits feature set data 
ltering approach selects set features preprocessing step training data 
lter approach plan explore wrapper approach research 
select methods previous works correlation coecient mutual information odds ratio 
feature selection rst delete instances words medline records porter algorithm stem remaining words 
eliminate stems occur documents training collection 
feature selection done category zone explained remove stems occur positive example documents 
rank remaining stems feature selection measure select pre de ned number top ranked stems feature set 
hme artcl tex 
correlation coefficient correlation coecient feature selection measure proposed ng de ned number positive examples category feature occurs occur number negative examples category feature occurs occur 
measure derived measure sch correlation coecient interpreted side measurement 
measure reported measure text categorization yang petersen 
correlation coecient promotes features high frequency relevant examples rare non relevant documents 
features ranked method positive values correspond features indicate presence category negative values indicate absence category 
contrast ranks features higher strongly indicate presence absence category 
ambiguous features ranked lower 
compared correlation coecient feature selection neural networks architecture 
neural networks trained features selected correlation coecient outperformed trained categories 
con rms similar results reported ng 
note yang pedersen average value categories measure goodness term global sense local category level feature selection 
contrast mutual information correlation coe cient produce normalized values statistic 
normalization hold low populated cells contingency table 
scores correlation coecient low frequency terms unreliable 
reason removing rare features described 

mutual information mutual information measure text categorization researchers 
method mutual information concept developed information theory 
feature category de ned hme artcl tex log probability term occurring collection probability category occurring collection joint probability 
yang pedersen mutual information measure goodness term global feature selection approach combining category speci scores term ways avg max max fi number categories 
contrast feature selection evaluate goodness term respect individual categories 
words average values mutual information multiple categories 
variation may sucient produce di erent results obtain described 
yang pedersen point score produced mutual information strongly uenced marginal probabilities terms 
evident equivalent formula log wjc log terms equal conditional probability wjc rare terms higher scores common terms 
implies scores terms extremely di erent frequencies comparable 
frequency threshold described earlier compensates ect 

odds ratio odds ratio proposed originally van rijsbergen selecting terms relevance feedback 
odds ratio class problem goal prediction class values 
idea distribution features relevant documents di erent distribution features non relevant documents 
confusion term mutual information 
instance researchers refer measure yang pedersen information gain 
hme artcl tex 
graph odds ratio 
axis represents axis represents 
mladeni selecting terms text categorization 
odds ratio feature set positive examples pos negative examples neg category de ned follows log observe formula interpreted sum logarithm ratios distribution feature relevant documents log non relevant documents log 
document appears half relevant documents logarithm ratio relevant documents positive 
contrast feature penalized appears half non relevant documents 
words feature appears frequently relevant documents infrequently non relevant documents high score 
shows graph odds ratio 
function presents singularity points map case highest positive value 
logarithm de ned map case smallest negative value 
mladeni report odds ratio successful feature selection method hierarchical bayesian classi er compared mutual information cross entropy information gain weight evidence 
hme artcl tex study select features expert gating networks correlation coecient mutual information odds ratio methods 

training set selection supervised learning algorithm requires training set element correctly categorized 
expect availability large training set ohsumed bene cial training algorithm 
practice case 
problem occurs large collection categories assigned relatively small number documents 
creates situation category small number positive examples overwhelming number negative examples 
machine learning algorithm trained learn assignment function unbalanced training set algorithm learn best decision assign category 
overwhelming amount negative examples hides assignment function 
overcome problem appropriate set training examples selected 
call training subset category zone 
notion category zone similar local regions described wiener ng inspired query zone proposed singhal text routing :10.1.1.54.6608
query zoning observation large collection query set documents constitutes domain 
nonrelevant documents outside domain easy identify dicult di erentiate relevant non relevant documents query domain 
singhal de ne procedure tries approximate domain query domain train routing method 
suggest text categorization category domain 
easier train learning algorithm documents category domain potentially achieve better categorization performance 
explore di erent methods building category zone 
rst method creates category zone method similar singhal 
rst category zone call centroid created follows 
take positive examples category obtain centroid 

centroid query perform retrieval obtain top documents 
subset contain hme artcl tex positive examples negative examples closely related domain category 

obtain category zone adding positive examples set obtained previous step 
method creates category zones documents size increases categories positive examples outside retrieved set 
second method creating category zone uses knn approach category zone consists set nearest neighbors positive example category 
method produce variable sized category zones 
explored values 
main concern method obtain training set large train neural network tting 

experimental collection ohsumed collection created hersh collaborators 
collection records medline collection 
record collection elds see 
title 
training set mesh eld represents manual categorization decisions medline documents 
selected records titles abstracts mesh categories remaining abstracts :10.1.1.50.9950
rst years data dated records training year records testing 
corresponds split lewis 
categories heart disease subtree cardiovascular diseases tree structure umls categories positive examples training set 
limit experiments categories 
divide set categories sets high frequency categories hd includes categories examples training set 
set contains categories set high frequency categories lewis 
version umls 
hme artcl tex heart lung adult aids related complex di cardiac di case report bacterial di female heart diseases di ra heart failure congestive di heart valve diseases di human male myocardial diseases di support non gov 
miami vices ccu 
part ii 
cardiac manifestations aids 
cardiac manifestations aids probably occur frequently appreciated despite reports indicating deceased aids patients 
high index suspicion echocardiogram help revealing true incidence cardiac involvement aids 
valle bk 
example record ohsumed medium frequency categories hd set includes categories frequencies training set 
set contains categories equivalent second set categories lewis 
low frequency categories hd set includes categories frequencies training set 
set contains categories 
report results subsets complete set categories hd rst subsets allow analyze performance separately di erent levels positive evidence allow compare results published research collection 
heart diseases categories form level tree rst level corresponds root node fth level leaf label set hd order stay consistent labeling categories positive examples training set 
hme artcl tex myocardial diseases heart heart defects arrhythmia heart defects coronary myocardial myocardial heart valve experts experts experts experts experts experts experts experts experts experts 
tree categories heart diseases sub tree nodes 
number gates level starting root see 

evaluation measures order evaluate binary decision task rst de ne contingency matrix representing possible outcomes classi cation shown table measures ir ai communities de ned contingency table 
table ii shows formulas measures corresponding names community 
ir measures combine recall precision de ned break point bep measure 
bep proposed lewis de ned point recall equals precision 
van rijsbergen measure combines recall precision single score hme artcl tex table contingency table binary classi cation class positive class negative assigned positive true positives false positives assigned negative false negatives true negatives table ii 
eciency measures binary classi cation de ned information retrieval ir arti cial intelligence ai communities ir ai recall sensitivity precision predictive value fallout predictive value accuracy speci city error rate precision recall 
intermediate values di erent weights assigned recall precision 
common values assigned recall half important precision recall precision equally important recall twice important precision 
de ned occurs classi er assigns documents category related documents collection 
measures perfect appropriate problem 
example recall sensitivity show deceiving results system assigns category document show perfect recall 
accuracy hand works number positive negative examples balanced extreme conditions deceiving 
number negative examples overwhelming compared positive examples system assigns documents category obtain accuracy value close 
hme artcl tex pointed schapire bep shows problems :10.1.1.11.6124
usually value bep interpolated 
values recall precision far bep show values achievable system 
point recall equals precision informative necessarily desirable user perspective 
van rijsbergen measure best suited measure drawback dicult user de ne relative importance recall precision 
report values allows compare results researchers dataset 
general performance reported average value 
ways computing average macro average micro average 
macro average value computed category averaged get nal macro averaged micro average rst obtain global values true positive true negative false positive false negative decisions compute micro averaged value micro recall micro precision computed global values 
results reported macro averaged allows compare results researchers working ohsumed dataset 

experiments main questions address research hierarchical classi er built hme model improve performance compared classi er 
hierarchical method compare text categorization approaches 
research questions mind series experiments ohsumed collection 

baselines rst baseline represents classical rocchio classi er described section 
second baseline neural network classi er 
comparing performance hme classi er classi er allow answer rst research question 
comparing hme method rocchio classi er published results allow answer second research question 
implemented rocchio classi er hme classi er neural network classi er detailed 
hme artcl tex 
rocchio classifier rocchio algorithm developed mid improve queries relevance feedback 
proven successful feedback algorithms 
rocchio showed optimal query vector di erence vector centroid vectors relevant non relevant documents 
salton buckley included original query orig preserve focus query added coecients control contribution component 
mathematical formulation version new orig rel rel weighted document vector number relevant documents total number documents 
negative components nal vector new set zero 
techniques proposed improve ectiveness rocchio method better weighting schemes query zoning dynamic feedback optimization :10.1.1.50.9950
pointed schapire studies rocchio baseline constructed weak version classi er :10.1.1.11.6124:10.1.1.109.2516
show properly optimized rocchio algorithm achieve quite competitive performance 
noticed rocchio classi ers bene optimal feature selection step 
fair comparison neural networks rocchio classi ers set features selected correlation coecient category zones train neural network classi ers category 
observe important di erence respect previously published research rocchio classi ers studies vector computed set features 
feature selection measures select features indicative presence category classi er centroid vector de ned di erent subspace sub space selected features generated category zone 
build rocchio classi er presenting training examples category zone computing weights classi er rocchio formula 
rank full training collection similarity classi er vector 
threshold similarity value maximizes measure described evaluation measures section selected 
optimal rocchio classi er category weighted vector selected features similarity threshold 
hme artcl tex evaluation phase compute similarity optimal rocchio classi er vector test document vectors assign class documents threshold 
hierarchical mixture experts hme approach represented 
zone domain documents identi ed category explained 
feature selection applied category zone extract best set features 
tested feature selection methods experiments 
expert network backpropagation neural network trained corresponding category zone selected set features 
similarly gating network backpropagation network 
gate training subset combined category zones descendants classi cation hierarchy 
feature selection gate performed combined subset 
strategy combining zones descendent nodes gate reasonable consider fact gates represent hierarchical concepts particular categories described section 
input feature vectors documents weighted tf idf weights tf frequency term document idf inverse document frequency de ned idf log total number documents training collection number documents contain term training collection 
experts gates trained independently parameters learning rate error tolerance maximum number epochs 
parameter values xed experiments 
training network takes minutes expert depending number examples minutes gating network hp workstation 
workstations dynamic scheduling program speci cally designed task trained experts gating networks hours 
experts gates trained individually assemble umls hierarchical structure 
output network real value need transform output value binary decision 
step called thresholding 
selecting thresholds optimize values categories 
complete training set select optimal hme artcl tex thresholds 
working modular hierarchical structure choices perform thresholding 
approach binary decision gates optimize threshold experts examples reach leaf nodes 
observe computing optimal thresholds binary decisions gates experts multidimensional optimization problem 
decided optimize gates grouping levels nding value threshold level maximizes average value experts 
expert threshold optimized maximize value examples training set reach expert 
order constrain potentially explosive combination parameters decide thresholding gates experiments 
purpose conducted preliminary experiment search best combination thresholds level varying threshold xed values computing performance training set 
optimal thresholds set levels root respectively 
values obtained selecting best results threshold combinations level 
levels experiment run correlation coecient feature selection standard con guration input nodes hidden nodes 
best threshold xed runs 
test set processed trained networks assembled hierarchically established thresholds level gates expert network 

flat neural network classifier order assess advantage gained exploiting hierarchical structure classi cation scheme built neural network classi er 
decided build modular classi er implemented set individual expert networks 
similar rocchio model training phase results set classi er vectors 
model experts trained independently optimal feature set category zone individual category 
thresholding step performed optimizing value expert entire training set 
values report section neural network classi er 
observe gates level set optimal values values gates level 
gives possible combinations thresholds gates 
hme artcl tex model allows assess contribution adding hierarchical structure rst research question 

results stated report results heart diseases sub tree 
results set categories hd frequency subsets categories hd hd hd de ned section 

effect feature selection neural network architecture may observed network architecture feature selection methods studied combination 
fact basic level feature selection factors de ne con guration network 
complex combinations terms feature selection methods numbers nodes di erent layers expert gating networks approach problem stages 
follow top approach optimizes gates experts 
focused attention gating networks 
experts input features nodes hidden layers 
explored input features gating networks hidden layer twice number input nodes 
tried di erent feature selection methods mutual information odds ratio correlation coecient 
experiment done high frequency categories hd allow appropriate training neural networks variance di erent training runs smaller variance lower frequency categories 
interestingly di erences feature selection methods gating networks signi cant 
report results di erent combinations obtained correlation coef cient gates 
table iii shows increase performance features slight decrease networks larger number input nodes 
possible slight decrease caused limit number iterations network allowed run training 
usually larger network needs iterations training set converge optimal value 
observe feature selection methods show signi cant di erences gates expert networks 
somewhat surprising hme artcl tex yang pedersen reported mutual information perform compared methods 
noted averaging values multiple classes nd merit features global perspective mutual information local feature selection 
discard rare terms general ranked high mutual information 
explored feature selection mutual information running experiments discarding rare terms selecting top features 
average precision hd subset neural network hme model respectively 
signi cantly lower performance obtained discard low frequency terms shows conclusively mutual information feature selection measure address major weakness discard low frequency terms 
addressed selecting larger number input features 
go goal reducing number input features improve training processing time neural networks 
optimizing gates address number input nodes expert networks exploring individually independent hierarchical structure 
tested expert networks input features 
case twice number input nodes hidden layer feature selection methods 
best result obtained input features nodes hidden layer 
having determined optimal number inputs explore ect size hidden layer networks 
note far explored simple strategy having twice input nodes middle layer 
table iv shows variations performance di erent sizes middle layer 
case networks inputs single output node 
best performance obtained expert networks nodes hidden layer 
di erence nodes relatively small 
ran similar experiments gating networks varying size middle layer best size hidden layer table 
di erence runs small 
observe neural networks best performance number nodes hidden layer 
surprising smaller hidden layer tends produce better generalization category 
sections results neural networks inputs hidden nodes output experts inputs hidden nodes output gating network 
hme artcl tex table iii 
ect number input nodes gating networks feature selection methods expert networks feature selection method gating networks correlation coecient 
expert networks input features selected indicated feature selection method nodes middle layer output 
row shows performance classi er 
performance measured macro averaged hd set gating networks expert networks inputs corr 
coef 
odds ratio mutual inf 
flat table iv 
ect number hidden nodes expert networks 
expert networks input features selected correlation coecient feature selection method 
performance measured macro averaged hd set hidden nodes flat nn hme hme artcl tex table ect number hidden nodes gating networks :10.1.1.50.9950
expert networks input features hidden nodes 
gates inputs selected correlation coecient performance measured macro averaged hd set hidden nodes hme 
comparing category zone methods mentioned explore di erent types category zones centroid category zone knn category zone 
compare zoning strategies respect zone sizes classi er performance 
centroid category zone generates zones examples 
training set categories type category zone average size maximum zones 
category zone generate zones sizes proportional number positive examples category 
compact categories general small category zones 
explore di erent values 
small values problematic low frequency categories tend generate small training set neural network ts easily 
settled produces category zones large rare categories zones reasonable size frequent categories 
categories average size knn category zone examples maximum minimum 
category zones generated method 
mentioned category zone gate union individual category zones descendants hierarchy 
hme artcl tex measure impact zoning method trained gating expert networks documents corresponding zones 
categorization results test set shown table vi 
small di erences performance zones classi ers high frequencies hd set classi ers medium frequencies hd set 
di erences statistically signi cant 
low frequency categories hd show statistically signi cant di erence favor centroid zones hierarchical classi ers 
detailed analysis showed high value di erence due part contribution categories zero examples test set 
categories function hit function category zone training sets examples classi er learns reject documents consequence gets value 
contrast knn zones low frequency categories generate smaller zone neural networks trained tend assign documents 
classi ers trained centroid zones outperform knn classi ers categories classi ers trained knn zones outperform centroid classi ers categories remaining categories di erence 
set hd classi ers trained centroid zones outperform trained knn zones 
di erence statistically signi cant hme classi ers 
results zoning methods suggest category zones appropriate training hierarchical classi ers span categories varying frequencies 
may classi ers somewhat reduced con dence 

comparing hme flat nn optimized rocchio table vii shows performance neural network hme model optimized rocchio classi er trained zoning method features selected correlation coecient 
hme classi er consistently outperforms neural network classi er category sets 
di erence statistically signi cant hd hd hd 
important feature point hme model lower variance performance category sets 
result con rms theoretical claim categories examples test set document assigned class documents assigned class 
hme artcl tex table vi 
comparison categorization performance centroid knn category zones 
values macro averaged respective set categories test set documents 
centroid zone knn zone flat nn hme flat nn hme hd hd hd hd jordan jacobs soft splitting variance reduction method :10.1.1.50.9950
general desirable property classi er indicates performance stable categories 
comparing hme optimized rocchio classi er note rocchio signi cantly outperforms hme hd hd categories hme signi cantly outperforms rocchio hd categories 
signi cant di erence classi ers hd set 
suggested reviewer performance rocchio classi er may part due particular combination category zoning feature selection methods classi ers 
remind reader rst centroid category zone identi ed category 
zone documents feature selection 
approach lter approach described section speci cs centroid category zoning technique suggests approach wrapper rocchio classi er de nitely lter neural net models 
unfortunately expense wrapper approach neural net models prohibits exploration aspect 
limitations neural net models 
rocchio results emphasize schapire rocchio classi er properly trained performs methods :10.1.1.11.6124
result contrast performance rocchio classi ers observed researchers :10.1.1.109.2516
detailed analysis behavior hme respect neural network shows threshold hierarchical classi er equal threshold classi er categories 
expected result intermediate layers perform pre ltering bad candidate texts experts hme artcl tex table vii 
comparison nn hme optimized rocchio classi ers 
flat nn hme opt rocchio macro hd avg hd hd hd variance hd hd hd hd receive smaller number examples :10.1.1.50.9950
optimization process sets thresholds maximize values training set bad matches ltered algorithm able set lower threshold increases number true positives signi cantly increasing number false positives 
idea hierarchy ltering false positives 
table viii shows number documents pass gate test set 
number documents test collection 
root node lters documents pass 
observe gates allow big portion documents pass lower levels 
contain categories highest number training examples coronary diseases myocardial 
expected harm performance rest categories subnodes practice happen 
example categories coronary diseases subtree shown table slightly lower performance hme model classi er 

comparing results published works ohsumed collection researchers text categorization 
best knowledge studies entire set mesh categories 
main reason text categorization methods scale large dataset 
yang lewis lam ho published results subset categories hme artcl tex table viii 
number documents pass gate test set 
level doc 
threshold root heart diseases level level arrhythmia heart block pre excitation syndromes heart defects heart defects transposition great vessels heart failure congestive heart heart valve diseases myocardial diseases myocardial coronary diseases myocardial heart diseases sub tree hd 
standard set comparing results text categorization ohsumed collection 
reading works carefully uses di erent test set report results di erent number categories 
lewis set documents training documents year test set 
experiments follow exactly partition reduction training zoning techniques test set 
table ix shows neural network model performs level hd slightly worse hd exponentiated gradient algorithm lewis 
second ranked top ranked algorithm hd hd respectively 
note rows table show results hme artcl tex comparable 
hme model hand shows performance slightly lower top ranked widrow ho wh hd signi cantly higher best performance hd 
nn hme outperform rocchio classi er reported baseline lewis 
interestingly optimized rocchio classi er performs level wh hd signi cantly better classi ers hd 
yang conducts di erent experiment reducing collection documents positive examples categories hd 
limits training set documents test set documents 
explains reason reduction scalability llsf method needs compute singular values decomposition procedure performed eciently large matrix 
simpli cation creates partition ohsumed collection structurally di erent originally proposed lewis order explore di erences trained classi ers reduced test set thresholds 
results obtained neural networks hme optimized rocchio respectively 
observe hierarchical model yang reduction equivalent having perfect classi er root node tree gates levels 
ran second experiment retrain neural networks rocchio classi ers reduced training set threshold selection done set positive examples training set set documents training set gates levels 
results neural networks hme optimized rocchio classi ers reduced subset respectively scores signi cantly higher lewis partition 
scores ones reported yang llsf classi ers signi cantly yang baseline str classi er 
interestingly hme model outperform neural network model constrained experiment 
looked closely category due fact originally trained gates discard documents relevant descendants impacts nal performance classi er 
performance may improved train gates set positive examples believe original fact experimentation knn zoning uses smaller training set closely resembles positive examples hme better neural network classi er 
hme artcl tex table ix 
performance comparison nn hme rocchio classi ers classi ers yang lewis 
yang lewis method hd hd hd llsf str rocchio wh flat nn hme opt rocchio tion task realistic include positive negative examples test set :10.1.1.50.9950
lam ho report results experiments generalized instance set gis algorithm 
documents take rst documents training documents testing :10.1.1.50.9950
contrast yang reduction consistent partition proposed lewis retains documents collection just positive examples 
lam ho report results micro averaged bep set categories example training test sets 
tested previously trained hme model reduced test set obtained micro averaged bep value signi cantly lower performance gis algorithm rocchio generalization 
research train test hme model data set 
joachims published results ohsumed collection support vector machines 
uses rst documents year dividing sets documents training testing respectively 
reports impressive results text categorization task di erent ones previously discussed works 
joachims assumes category umls tree assigned general category hierarchy 
similar hme artcl tex de nition gates di erence uses high level disease categories :10.1.1.50.9950
simpli es categorization task considerably probably explains results obtained reported svm experiments 
focus general disease categories prevents comparison joachims results previously published results 
run experiments limited high level disease categories 
consider svm choice building architecture proposed plan explore research 
believe combination category zoning feature selection gives signi cant boost rocchio performance 
optimized versions rocchio classi er previous focused query zoning dynamic feedback optimization :10.1.1.11.6124
aware previous reducing set features centroid vector text categorization purposes observe feature selection method favors features indicative presence category discards features indicate absence category 
feature selection important impact similarity computation document centroid vector subspace formed selected features 

comparison related hierarchical categorization focus methods exploring hierarchical structures classi cation schemes 
koller sahami proposed hierarchical approach trains independent bayesian classi ers node hierarchy 
classi cation scheme starts root greedily selects best link second level classi er 
process repeated leaf reached child node candidate 
observe method selects single path highest probability assigns categories path document 
approach hierarchical structure lter activates single best classi cation path 
errors classi cation higher levels lower levels 
tested results reuters collection de ning higher nodes hierarchy categories subsume categories 
similar spirit approach di ers classi cation assignment model 
separate identi cation general concepts review article works feature selection context rocchio approach published hme artcl tex assignment general categories :10.1.1.50.9950
approach activates single path hierarchy contrast winner takes approach 
obvious di erence machine learning algorithm bayesian classi ers neural networks 
plan explore bayesian classi ers hme approach 
ng build hierarchical classi er perceptrons 
node hierarchical tree represented perceptron 
distinguish types nodes leaf nodes non leaf nodes 
apply reuters corpus categories re ect geographical topical hierarchy 
hierarchy rst level possible countries country di erent topics de ned economics politics 
leaf nodes speci categories second level economics communications industry 
hierarchical classi er receives document checks belongs rst level nodes root node connects di erent country nodes 
tested document activates rst level nodes descendant categories node tested recursively 
non leaf nodes process nds children candidate categorization stops branch recursion 
output classi er nal set leaf nodes reached recursion zero 
similar machine proposed koller sahami multiple outputs single output 
approach similar ng top approach 
di erence type classi er 
non linear classi ers node linear classi ers 
combination linear classi ers hierarchy creates non linear classi er studies shown covers limited number non linear problems 
approach di ers signi cantly way feature selection subset training selection done 
experiments correlation coecient feature selection results hierarchical classi er methods reuters collection 
report values best automatic feature selection method manually selected features considerable lower :10.1.1.109.2516
best results hierarchical classi er obtained manually selected features 
believe results may consequence manual feature selection 
furthermore approach category zones combined exploiting hierarchy robust allows due fact high level nodes create linear boundaries adjacent expert regions input space 
hme artcl tex get results similar best methods reported literature 
mccallum collaborators working text categorization speci cally targeting problem classifying web pages 
approach bayesian classi ers 
hierarchical classi cation structure improve accuracy bayesian classi ers statistical technique called shrinkage smoothes parameter estimates child node ancestors order obtain robust estimates 
bayesian classi cation schemes involve estimating parameters model training collection applying shrinkage method improve estimates prede ned hierarchy categories 
classi cation test set performed computing posterior probability class words observed test document selecting class highest probability 
experiments show shrinkage improves performance training data sparse reducing classi cation error 
approach totally di erent mccallum approach terms classi cation method assumptions categorization task 
observe approach assumes document belongs single category model re ects selecting probable classi cation 
mladeni explored hierarchical structures yahoo 
hierarchy classify web pages 
approach builds bayesian classi er 
node subject hierarchy classi er induced 
train non leaf classi ers set positive examples de ned positive examples node intermediate nodes valid categories plus positive examples descendants 
examples weighted position tree 
classi cation process works described bayesian classi ers set categories predicted probability assigned 
approach di ers mladeni terms classi er algorithms way hierarchy feature selection 
main ort creating weighting scheme combining probabilities obtained di erent nodes tree 
topic spotting wiener inspired try approach hme 
review process article published sequel applied hierarchical classi ers 
meta topic network level hierarchy reuters collection results competitive methods 
di ers theirs de nition gates gates behave binary lters meta topic network helps weight contribution experts 
hme artcl tex extends hierarchical model levels idea suggested developed 
evaluation ohsumed collection reuters collection allows test bene exploiting real hierarchical classi cation scheme 
think main contribution general method combining hierarchical structure classi cation feature selection category zones 
zones contain small optimal subsets documents yield features suitable training neural networks 
approach shows set features node get results comparable methods larger feature sets 
presents machine learning method text categorization takes advantage pre existing classi cation hierarchical structures 
response rst research question nd exploiting hierarchical structure hme model increases performance signi cantly 
response second research question nd hme approach equivalent performance optimized rocchio approach full set categories rocchio approach better medium high frequency categories hme better remaining low frequency ones 
comparison previous results rocchio classi er nd approach bene ts category zoning training set selection followed feature selection 
results con rm results published schapire carefully trained rocchio algorithm perform sophisticated methods :10.1.1.11.6124
method scale large test collections vocabularies divides problem smaller tasks solved shorter time 
top processing approach tested selected speci cally scalability mind 
category zoning valuable large collections counters overwhelming presence negative examples 
respect feature selection methods tested signi cant di erence performance correlation coecient odds ratio mutual information local feature selection 
particular results mutual information reported poor method global feature se feature sets features category :10.1.1.54.6608:10.1.1.54.6608
authors features collection 
hme artcl tex lection point importance addressing weaknesses feature selection methods low frequency terms 
close results obtained hme model comparable reported previously literature signi cantly better neural network classi er 
try di erent ways construct hierarchical structure support vector machines 
change training strategy variation boosting adapted hierarchical approach 
hope improve performance classi er adding richer features grams phrases reported help text categorization process 
plan explore di erent classi ers linear classi ers svm show improvements obtained neural networks 
want professors david eichmann ted herman gregg oden alberto university iowa initial comments feedback encouraged write article 
want dr fabrizio anonymous reviewers excellent comments suggestion helped signi cantly improve article 

apte damerau fj weiss sm 
automated learning decision rules text categorization 
acm transactions information systems pp 
july :10.1.1.50.9950

bridle probabilistic interpretation feedforward classi cation network outputs relationships statistical pattern recognition 
eds neuro computing algorithms architectures applications 
new york springer verlag 

breiman friedman jh olshen ra stone cj 
classi cation regression trees 
belmont ca wadsworth international group 

buckley salton optimization relevance feedback weights 
proceedings th annual international acm sigir conference research development information retrieval pp 
seattle wa july 

mf matwin sebastiani learner independent evaluation usefulness statistical phrases automated text categorization 
chin ed text databases document management theory practice 
idea group publishing pp 


cohen singer context sensitive learning methods text categorization 
proceedings th international acm sigir conference research development information retrieval pp 
zurich switzerland july 
hme artcl tex 
buc nadal jp 
trio learning tool building hybrid neural trees 
international journal neural systems pp 
december 

sebastiani simi experiments feature selection negative evidence automated text categorization 
proceedings ecdl th european conference research advanced technology digital libraries lisbon portugal pp 


hersh buckley leone tj ohsumed interactive retrieval evaluation new large test collection research 
bruce croft van rijsbergen editors proceedings th annual international acm sigir conference research development information retrieval pp 
dublin ireland july 

joachims estimating generalization performance svm eciently 
technical report ls report 
universit dortmund dortmund december 

joachims text categorization support vector machines learning relevant features 
technical report ls report university dortmund 

john kohavi eger irrelevant features subset selection problem 
machine learning proceedings eleventh international conference pp 
morgan kaufman publishers san francisco ca 

jordan mi jacobs ra 
hierarchical mixtures experts em algorithm 
technical report memo massachusetts institute technology 

koller sahami hierarchically classifying documents words 
icml proceedings fourteenth international conference machine learning pp 
san francisco ca 

lam ruiz srinivasan automatic text categorization application text retrieval 
ieee transactions knowledge data engineering pp 


lam ho cy 
generalized instance set automatic text categorization 
proceedings st international acm sigir conference research development information retrieval pp 


lewis ringuette comparison learning algorithms text categorization 
proceedings third annual symposium document analysis information retrieval sdair 

lewis 
evaluation phrasal clustered representations text categorization task 
proceedings th international acm sigir conference research development information retrieval pp 
june 

lewis dd schapire re callan jp papka training algorithms linear text classi ers 
proceedings th international acm sigir conference research development information retrieval pp 
zurich switzerland july 

liu motoda 
huan liu hiroshi motoda editors feature extraction construction selection data mining perspective chapter pp 

kluwer academic publishers boston ma 

mccallum nigam comparison event models naive bayes text classi cation 
learning text categorization papers hme artcl tex workshop 
aaai technical report ws pp 

aaai aaai press july 

mccallum rosenfeld mitchell ng ay 
improving text classi cation shrinkage hierarchy classes 
proceedings th international conference machine learning 
aaai morgan kaufmann san francisco ca july 

mccullagh nelder ja 
generalized linear models 
london chapman hall 

mladeni machine learning non homogeneous distributed text data 
phd dissertation university ljubljana faculty computer information science ljubljana slovenia 

moulinier ganascia jg applying existing machine learning algorithm text categorization 
eds connectionist statistical symbolic approaches learning natural language processing 
springer verlag heidelberg germany pp 


national library medicine 
uni ed medical language system umls knowledge sources 
th edition department health human services national institute health national library medicine january 

ng ht goh wb low kl 
feature selection perceptron learning usability case study text categorization 
nicholas belkin desai narasimhalu peter willett editors proceedings th annual international acm sigir conference research development information retrieval philadelphia pa pp 
july 


relevance feedback information retrieval 
gerald salton ed smart retrieval system experiments automatic document processing 
prentice hall englewood cli new jersey 

rumelhart de durbin golden chauvin backpropagation basic theory 
smolensky mozer rumelhart eds mathematical perspectives neural networks lawrence associates hillsdale nj pp 
:10.1.1.50.9950

sahami machine learning improve information access 
phd thesis stanford university computer science department 

schapire re singer singhal boosting rocchio applied text ltering 
proceedings st annual international acm sigir conference research development information retrieval melbourne australia pp 
august 

sch hull da pedersen jo 
comparison classi ers document representations routing problem 
proceedings th annual international acm sigir conference research development information retrieval seattle wa pp 
july 
:10.1.1.50.9950
singhal buckley mitra pivoted document length normalization 
proceedings th annual international acm sigir conference research development information retrieval zurich switzerland pp 
august 

singhal mitra buckley learning routing queries query zone 
proceedings th annual international acm sigir conference research development information retrieval philadelphia pa pp 
july 

van rijsbergen cj 
information retrieval 
butterworths london nd edition 
hme artcl tex 
waterhouse sr classi cation regression mixtures experts 
phd 
dissertation university cambridge cambridge england 

waterhouse sr robinson aj 
classi cation hierarchical mixtures experts 
proceedings ieee workshop neural networks signal processing iv pp 


weigend wiener ed pedersen jo 
exploiting hierarchy text categorization 
information retrieval pp 


wiener pedersen jo weigend 
neural network approach topic spotting 
proceedings sdair pp 
:10.1.1.50.9950

wolpert dh 
stacked generalization tech 
rep la ur santa fe institute santa fe nm 

yang feature subset selection genetic algorithm 
huan liu hiroshi motoda editors feature extraction construction selection data mining perspective chapter pp 
kluwer academic publishers boston ma 

yang evaluation statistical approaches medline indexing 
proceedings american medical association amia pp 


yang pedersen jo 
comparative study feature selection text categorization 
proceedings fourteenth international conference machine learning icml 
morgan kaufmann publishers san francisco ca july 

yang evaluation statistical approaches text categorization 
information retrieval pp 

hme artcl tex 
