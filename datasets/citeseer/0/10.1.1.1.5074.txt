inferring high level behavior low level sensors donald patterson lin liao dieter fox henry kautz university washington department computer science engineering seattle wa usa cs washington edu fox kautz 
method learning bayesian model traveler moving urban environment 
technique novel simultaneously learns unified model traveler current mode transportation route unsupervised manner 
model implemented particle filters learned expectation maximization 
training data drawn gps sensor stream collected authors period months 
demonstrate adding external knowledge bus routes bus stops accuracy improved 
central theme ubiquitous computing building rich predictive models human behavior low level sensor data 
strand concerns tracking predicting person movements outdoor settings gps 
location small part person state 
ideally want recognize predict high level intentions complex behaviors cause particular physical movements space 
higher order models enable creation new computing services autonomously respond person needs support accurate predictions behavior levels abstraction 
presents approach learning person uses different kinds transportation community 
gps data infer predict user transportation mode walking driving bus 
learned model predict mode transitions boarding bus location 
show higher level transportation model increase accuracy location prediction important order handle gps signal loss preparing delivery services 
key inferring high level behavior fusing user historic sensor data general commonsense knowledge real world constraints 
real world constraints include example buses take passengers bus stops cars left parking lots cars buses travel streets 
unified probabilistic framework accounts sensor error case gps loss signal triangulation error multi path propagation error commonsense rules 
broad applications ubiquitous computing systems motivating application call activity compass device helps guide cognitively impaired person safely community 
system notes user departs familiar routine example gets wrong bus provides proactive alerts calls assistance 
activity compass part larger project building cognitive assistants probabilistic models human behavior 
approach built successes particle filters variant bayes filters estimating state dynamic system 
particular show notion graph constrained particle filtering introduced integrate information street maps 
extensions technique include richer user transportation state models multiple kinds commonsense background knowledge 
introduce part model low level filter continuously corrects systematic sensor error particle filter uses switching state space model different transportation modes different velocity bands transportation mode street map guides particles high level transition model graph structure 
additionally show apply expectation maximization em learn typical motion patterns humans completely unsupervised manner 
transition probabilities learned real data significantly increase model predictive quality robustness loss gps signal 
organized follows 
section summarize derivation graph tracking starting general bayes filter show extended handle transportation mode tracking 
sect 
show learn parameters tracking model em 
concluding sect 
experimental results show learn effective predictive models transportation behavior 
tracking graph approach tracks person location mode transportation street maps ones route planning gps car tracking 
specifically model world graph set vertices set directed edges 
edges correspond straight sections roads foot paths vertices placed graph represent intersection accurately model curved road set short straight edges 
estimate location transportation mode person apply bayes filters probabilistic approach estimating state dynamic system noisy sensor data 
briefly describe bayes filters general case show project different quantities bayes filter structure represented graph discuss extensions state space model 
bayesian filtering graph bayes filters address problem estimating state dynamical system sensor measurements 
uncertainty handled representing quantities involved estimation process random variables 
key idea bayes filters recursively estimate posterior probability density state space conditioned data collected far 
data consists sequence observations posterior state time computed previous state update rule see details term probabilistic model object dynamics describes likelihood making observation location context location estimation state typically describes position velocity object space 
applying bayesian filtering graph state object triple denotes edge object resides indicates distance object start vertex edge indicates velocity edge 
motion model considers objects constrained motion graph may travel edge endpoint edge switch neighboring edge 
compute probability motion edge graph annotated transition probabilities describe probability object transits edge previous edge edge transition took place 
knowledge probability uniform distribution neighboring edges builds graph bayesian tracking hierarchically extending state model 
add higher level abstraction contains transportation information lower level sensor error variable 
resulting state consists variables shown fig 

presence bus near person binary variable presence parking lot modeled mode transportation denoted take different values denotes motion velocity location person time represented denotes expected sensor error current model compensates systematic gps offsets 
lowest level model raw gps sensor measurements represented gps tracking combined state space computationally demanding 
fortunately bayes filters independences different parts tracking problem 
independences typically displayed graphical model fig 

dynamic bayes net consists set variables time point arc variable indicates causal influence :10.1.1.131.2084
links equivalent causality fig 
represents causality time dashed arrows 
sense network large maximum value infinite assumption dependencies variables change time state space conforms order markov independence assumption necessary represent reason time slices time 
slices numbered variables labeled gps directly observable represent position velocity readings gps sensor possible value reading includes loss signal 
variables sensor error velocity gps parking lot location bus mode velocity offset correction gps gps reading fig 

slice dynamic bayes net model transportation domain showing dependencies observed hidden variables 
observed variables shaded 
intra temporal causal links solid inter temporal links dashed 
user location mode presence parking lot bus location hidden variables values inferred raw gps readings 
dependencies nodes fig 
quite complex 
gps reading time point influenced local sensor error user actual velocity location 
location time depends person previous location motion velocity 
note gps explicitly considered provide true user location urban interference map point errors gps error sensor failure cause true location hidden variable 
sensor offset correction node reason errors gps readings systematic time location 
node maintains probability distribution corrections gps signal caused multi path propagation error dynamic satellite geometry 
node updates belief state comparing gps readings street map gradually adjust local variations signal offset 
complex relationship governs mode transportation influences instantaneous velocity 
influence mode velocity complicated fact range possible instantaneous velocities mode overlap 
example movement km hr may walk slowly moving car bus 
simplify relationship mode velocity model continuous velocities gaussian mixture shown fig 

separate unsupervised expectation maximization em process determined parameters probability densities real velocity data 
model assumes velocities drawn randomly gaussians probability drawing particular gaussian depends mode 
example walking mode draws speed left cluster probability 
bus mode person chance slow frequency count transportation speed modeling foot bus car speeds bus car speeds bus car speeds car speeds instantaneous speed gps reading km hr fig 

gaussian mixture model dependency transportation mode velocities 
gaussians learned em previously collected velocity data 
frequencies raw velocity values indicated bins 
different transportation modes modeled sampling different probability gaussians 
est velocity clusters 
current approach probabilities gaussians different transportation modes set manually external knowledge 
learning weights mixture components depending transportation mode eventually location left research 
model motion mode time depends previous mode presence parking lot bus 
example person get bus node indicates presence bus 
values bus parking lot nodes depend location person indicated arrows model shown fig 

learning mode location transition probabilities important aspect approach discussed sect 

particle filter implementation particle filters provide sample implementation general bayes filters 
represent posterior distributions state space temporal sets weighted samples sample state called importance weights sum 
kalman filters particle filters apply recursive bayes filter update estimate posteriors state space kalman filters particle filters restricted unimodal posterior distribu non negative numerical factors tions basic particle filter updates posterior sampling procedure referred sequential importance sampling re sampling sisr see sampling draw samples distribution previous set generate new samples new samples represent density product density called proposal distribution step 
importance sampling assign sample likelihood observation sample importance weight re sampling multiply discard samples drawing samples replacement distribution defined importance weights shown procedure fact approximates bayes filter update sample representation 
application particle filters problem location mode estimation network shown fig 
straightforward 
particle represents instantiation random variables describing transportation mode location velocity parking lot bus variables extracted sample location determined globally particles estimating offset gps readings street map 
update steps particle filter implemented follows 
temporal sampling step corresponds advancing particle motion model transportation mode chosen previous transportation mode presence bus stops parking lots 
gives randomly pick velocity velocity model specific mode velocity advance position person graph 
sampled velocity implies transition edge edge drawn probability see information edge transitions 
sampling steps resulting states represent predicted location velocity transportation mode 
importance sampling step implemented weighting sample likelihood observing current signal gps sensor new location sample 
re sampling step particle filter algorithm changed 
consider multi hypothesis tracking viable alternative particle filter implementation 
multi hypothesis tracking overcomes restrictive assumption plain kalman filter estimating state multiple kalman filters 
implementation approach part research 
parameter learning advantages modeling world graph ability record behavioral data edge transitions 
discrete nature transitions facilitates unsupervised learning hierarchical model parameters 
intuitive prior expectation state transitions occur edges edge transitions occur uniformly edge neighbors mode transitions vary presence bus parking lot 
learning context means adjusting model parameters better fit training data typically better model individual user environment 
learning parameters specific individuals captures idiosyncratic motion patterns movements user commonly opposed logically possible set movements 
model includes transportation mode learning means changing prior expectations edges mode transitions occur 
bus stops parking locations conceptual locations mode transitions may occur 
model enables learning commonly subset locations highlight user frequently parks car example 
learned model supports better tracking prediction prior model foundation high level understanding user behavior built 
describe learn parameters graph model data collected person moving community 
motivating application activity compass forces learn transportation modes unsupervised manner 
deployed activity compass users required example keep diary weeks transportation modes order create supervised training set 
obvious difficulty learn motion model solely map stream non continuous noisy gps sensor data 
general approach solving learning problems known expectation maximization em algorithm 
application em observation learning model parameters easy knew person true location transportation mode point time 
unfortunately location transportation mode hidden variables observed directly inferred raw gps measurements 
em solves problem iterating expectation step step maximization step step 
nutshell step estimates expectations distributions hidden variables gps observations current estimate model parameters 
step model parameters updated expectations hidden variables obtained step 
updated model step obtain accurate estimates hidden variables 
em theory tells iteration estimation parameters improved eventually converge local optimum 
give detailed description apply em theory domain 
step denote parameters graph model want estimate denote estimation thereof th iteration em algorithm 
model parameters contain conditional probabilities needed describe dynamic system shown fig 

step estimates posterior distribution trajectories person observations parameters updated previous iteration 
states observations respectively 
possible find closed form solution posterior resort approximate approach 
observe particle filtering motion model parameter particle distribution time history particles approximation desired expectation computed graph particle filter described sect 

give implementation details step take closer look step 
step goal step maximize expectation distribution obtained step updating parameter estimations 
distribution represented history particles estimation parameters th em iteration computed summing trajectories number particles state history th particle follows independence condition observations independent model transition parameters state trajectory known 
simplicity assume particles equal weight resampled 
straightforward extend derivation case different weights 
approach fact direct extension monte carlo em algorithm 
difference allow particles evolve time 
shown number particles large monte carlo em estimation converges theoretical em estimation 
implementation details em learn parameters model described sect 
interested learning parts model describe typical motion patterns user 
parameters fixed adjusted specific user 
advantage approach requires training data learning parameters 
motion patterns specific user described location transitions graph mode transitions different locations 
learning process initialize probabilities reasonable values transition probability graph conditioned mode transportation just prior transitioning new edge 
conditional probability initialized uniform distribution outgoing edges exception bus routes strong bias forcing buses follow route bus routes obtained gis sources 
exception model preference specific path person 
mode transition probability 
probability depends previous mode location person described edge example person typical locations gets bus 
mode transitions initialized commonsense knowledge may switch bus car foot knowledge bus stops 
parking lots uniformly distributed map biases actual parking lots 
straightforward implementation step generate expectation state trajectories storing history particle see discussion 
re sampling phase history old samples needs copied new samples time step set samples histories 
step update model parameters simply counting particle histories 
example get count number times particle mode transits edge normalize counts edges approach easy implement suffers drawbacks 
efficient 
data log fairly long saving histories particles needs large amount space history replication slow 
second importantly number samples finite repetition re sampling gradually diminish number different histories eventually decrease accuracy particle approximation 
overcome problems observing interested learning discrete transitions edges modes probability transiting edge edge mode 
discreteness transitions allows apply known baum welch algorithm em algorithm hidden markov models hmm :10.1.1.131.2084
monte carlo version baum welch algorithm performs iteration forward backward time particle filtering unnecessary copy operations avoided tree data structures manage pointers describing history particles 
step 
forward backward filtering step algorithm counts number particles transiting different edges nodes 
obtain probabilities different transitions counts forward backward pass normalized multiplied corresponding time slices 
show works define number particles edge mode time pass particle filtering 
number particles edge mode time forward backward pass particle filtering 
probability transiting edge time mode time probability transiting mode edge short derivation gives update parameters similarly expected number transitions mode expected number transitions mode expected number transitions edge expected number transitions edge complete implementation depicted table 
number particles increases approximation converges theoretical em estimation 
fortunately approach efficient regard model parameters associated number edges modes graph number particles 
addition user specific parameters model requires specification parameters motion velocity gps sensor model 
motion velocity modeled mixture gaussians velocities drawn random 
usually need prior number transition 
discuss set prior value 
table 
em parameter learning algorithm model initialization initialize model parameters step 
generate uniformly distributed samples set time 
perform forward particle filtering sampling generate new samples existing samples current parameter estimation 
importance sampling reweight sample observation 
re sampling multiply discard samples importance weights 
count save set repeat 
generate uniformly distributed samples set 
perform backward particle filtering compute backward parameters step sampling generate new samples existing samples backward parameter estimation 
importance sampling reweight sample observation 
re sampling multiply discard samples importance weights 
count save set repeat 
compute normalize 

update 
loop repeat step step updated parameters model converges 
probabilities mixture components depend current motion mode learned data labeled correct mode motion 
standard model compute likelihood gps sensor measurement location person 
experiments test data set consists logs gps data collected authors 
data contains position velocity information collected second intervals periods time author moving outdoors 
data hand labeled modes transportation foot bus car 
labeling useful validating results unsupervised learning em learning process 
data set chose episodes representing total hours logs 
subset consists portions data set bounded gps signal loss intermediate loss signal seconds fig 

car left foot middle bus right training data experiments 
black dot common map point university washington campus 
contained change mode transportation point episode 
episodes divided chronologically groups formed sets fold cross validation learning 
fig 
shows cross validation groups training 
street map provided census bureau locations bus stops come king county gis office 
mode estimation prediction primary goals approach learning motion model predicts transportation routes conditioned mode transportation 
conducted experiment validate models ability correctly learn mode transportation instant 
comparison trained decision tree model supervised learning data 
provided decision tree features current velocity standard deviation velocity previous seconds 
data annotated hand labeled mode transportation task decision tree output transportation mode velocity information 
fold cross validation groups evaluate learning algorithm 
results summarized row table 
result indicates time decision tree approach able accurately estimate current mode transportation test data 
bayes filter approach learning model parameters uniform transition probabilities 
furthermore model consider locations bus stops bus routes provided parking locations algorithm 
contrast decision tree bayes filter algorithm integrates information time increasing accuracy 
benefit additionally considering bus stops bus routes obvious row shows mode accuracy 
em learn model parameters increases accuracy time test data training 
note value high fact change transportation mode detected instantaneously 
table 
mode estimation quality different algorithms 
model cross validation prediction accuracy decision tree speed variance prior graph model bus stops bus routes prior graph model bus stops bus routes learned graph model similar comparison done looking techniques ability predict just instantaneous modes transportation transitions transportation modes 
table shows technique accuracy predicting qualitative change transportation mode seconds actual transition example correctly predicting person got bus 
precision percentage time algorithm predicts transition actual transition occurred 
recall percentage real transitions correctly predicted 
table clearly indicates superior performance learned model 
learning user motion patterns significantly increases precision mode transitions model accurate predicting transitions occur 
table 
prediction accuracy mode transition changes 
model precision recall decision tree speed variance prior graph model bus stops bus routes prior graph model bus stops bus routes learned graph model example modes transportation predicted training cross validation set shown fig 

location prediction location prediction capabilities approach illustrated fig 

fig 
learned model predict location person 
done providing ground truth location transportation mode algorithm predicting path transition probabilities learned training data 
shows percentage trajectories predicted correctly different prediction horizons 
prediction length measured city blocks 
example cases location person fig 

map shows learned transportation behavior crossvalidation set containing nineteen episodes 
shown edges mode transitions learned model predicts high probabilities 
thick gray lines indicate learned bus routes thin black lines indicate learned walking routes indicate learned driving routes 
circles indicate parking spots triangles show subset bus stops model learned high probability transition bus 
call outs show detail 
shows frequently traveled road distinct parking spaces 
route parking spots indicate correctly learned car trips author home church 
shows frequently traveled foot route enters northeast frequently bus stops author 
main road running east west arterial road providing access highway author 
shows intersection northwest university washington campus 
learned bus stops 
author frequently takes bus north south location 
frequent car drop point author parking spot indication 
walking routes extend west shopping area east campus 
shows major university parking lot 
foot traffic walks west campus 
probability correctly predicting blocks probability correctly predicting blocks transportation modes predicting location transportation mode city blocks fig 

location prediction capabilities learned model 
bus car foot predicting location transportation mode city blocks fig 

location mode prediction capabilities learned model 
predicted correctly blocks person bus 
cases prediction correct blocks blocks predicted correctly cases 
note linear drop bus route prediction probability due fact data contained correctly predicted episodes block long bus trip 
obviously long term distance prediction accurate person walks 
due higher variability walking patterns fact people typically walk city blocks making long term prediction impossible 
fig 
learned model predict location transportation mode person 
done providing ground truth location algorithm predicting path sequence transportation mode switches transition probabilities learned training data 
graph shows cases model able correctly predict motion transportation mode person city blocks 
result extremely promising model trained tested subsets episodes 
helps lay foundation reasoning high level descriptions human behavior sensor data 
showed complex behaviors boarding bus particular bus traveling recognized gps data general commonsense knowledge requiring additional sensors installed environment 
demonstrated predictive user specific models learned unsupervised fashion 
key idea approach apply graph bayes filter track person location transportation mode street map annotated bus route information 
location transportation mode person estimated particle filter 
showed em algorithm frequency counts particle filter learn motion model user 
main advantage unsupervised learning algorithm fact applied raw gps sensor data 
combination general knowledge unsupervised learning enables broad range self customizing applications activity compass mentioned sect 

furthermore straightforward adopt approach life long learning user needs explicitly instruct device longer user carries device accurate user model 
current research extends described number directions including 
making positive negative information 
loss gps signal tracking causes probability mass spread governed transition model 
seen learning significantly reduces rate spread 
cases loss signal tighten estimation user location 
particular buildings certain outdoor regions gps dead zones 
signal lost entering area remains lost significant period time gps device active strengthen probability user left dead zone area 

learning daily weekly patterns 
current model absolute temporal information time day day week 
including variables model improve tracking prediction kinds common life patterns fact user travels place weekday mornings 

modeling trip destination purpose 
described segments movement terms transitions intersections modes transportation 
higher level abstraction movement segmented terms trips progress location set activities take place home location different class activities take place office 
single trip activity centers involve shifts modes transportation 
learning trip models expect able increase accuracy predictions 
significantly trip models provide way integrate sources high level knowledge user appointment calendar 

relational models predictions novel events 
significant limitation current approach useful predictions user location 
relational probabilistic models develops promising approach predictions novel states smoothing statistics semantically similar states :10.1.1.1.8783
example model predict user significant chance entering nearby restaurant noon history user particular restaurant 
acknowledgment partly supported nsf numbers iis iis afrl contract darpa mica program intel 

hightower borriello location systems ubiquitous computing 
computer ieee computer society press 

data fusion abs sensors gps enhanced localization car vehicles 
proc 
ieee international conference robotics automation 

cui ge autonomous vehicle positioning gps urban canyon environments 
proc 
ieee international conference robotics automation 

starner learning significant locations predicting user movement gps 
international symposium wearable computing seattle wa 
patterson etzioni fox kautz activity compass 
proceedings international workshop ubiquitous computing cognitive aids 

kautz borriello etzioni fox assisted cognition project 
proceedings international workshop ubiquitous computing cognitive aids sweden 
doucet de freitas gordon eds sequential monte carlo practice 
springer verlag new york 
liao fox hightower kautz schulz voronoi tracking location estimation sparse noisy sensor data 
proc 
ieee rsj international conference intelligent robots systems 

bar shalom li estimation applications tracking navigation 
john wiley 
dean kanazawa probabilistic temporal reasoning 
proc 
national conference artificial intelligence 

murphy dynamic bayesian networks representation inference learning 
phd thesis uc berkeley computer science division 
bar shalom li multitarget multisensor tracking principles techniques 
bar shalom 
del moral branching interacting particle systems approximations feynman kac formulae applications non linear filtering 
de 
number lecture notes mathematics 
springer verlag 
bilmes gentle tutorial em algorithm application parameter estimation gaussian mixture hidden markov models 
technical report icsi tr university berkeley 
rabiner tutorial hidden markov models selected applications speech recognition 
proceedings ieee ieee ieee log number 

levine casella implementations monte carlo em algorithm 
journal computational graphical statistics 
wei tanner monte carlo implementation em algorithm poor mans data augmentation algorithms 
journal american statistical association 
county gis graphical information system 
www gov gis mission htm 
thrun langford fox monte carlo hidden markov models learning nonparametric models partially observable stochastic processes 
proc 
international conference machine learning 

bureau census tiger line data 
www com data download census 
mitchell machine learning 
mcgraw hill 
anderson domingos weld relational markov models application adaptive web navigation 
proceedings eighth international conference knowledge discovery data mining acm press edmonton canada 

domingos weld dynamic probabilistic relational models 
proceedings eighteenth international joint conference artificial intelligence morgan kaufmann acapulco mexico 
