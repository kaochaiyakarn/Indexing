properties benefits calibrated classifiers ira cohen moises goldszmidt hewlett packard research laboratories page mill rd palo alto ca ira cohen moises goldszmidt hp com 
calibrated classifier provides reliable estimates true probability test sample member class interest 
crucial decision making tasks 
procedures calibration studied weather forecasting game theory machine learning showing empirically calibration classifiers helps decision making improves classification accuracy 
extend theoretical foundation empirical observations 
prove calibrated classifier provides bounds bayes error calibrating classifier guaranteed decrease classification accuracy procedure calibration provides threshold thresholds decision rule minimize classification error 
draw parallels differences methods receiver operating characteristic roc curves calibration procedures aimed finding threshold minimum error 
particular calibration leads improved performance multiple thresholds exist 
decision making task order evaluate different courses action useful obtain accurate likelihood estimates alternatives 
pattern classifiers provide automated mappings situations represented features outcomes represented class membership 
applicable decision making problems require reliable estimate true probability class membership test sample 
term calibrated refer classifiers reliable estimates class membership probabilities 
successful classifier terms classification accuracy necessarily calibrated naive bayes classifier procedures calibrating classifiers proposed different contexts weather prediction tasks game theory context pattern classification :10.1.1.13.7457
zadrozny elkan notice need calibrating classifiers decision making aids 
incentive study calibration came applying probabilistic classifiers problem characterizing forecasting response time large storage arrays passive observations 
forecasts scheduling purposes need accompany forecast accurate estimate probability forecast 
applied variant calibration procedure suggested noticed addition producing accurate estimates classification accuracy induced classifiers increased 
empirical results agree zadrozny elkan theoretical guarantee calibration degrade classification performance missing :10.1.1.13.7457:10.1.1.13.7457
investigation calibration produced results prove sections 
bound bayes error parameters result calculations needed calibration 
second guaranteed classification accuracy original classifier decrease consequence calibration 
classification accuracy increase 
third calibration process compute threshold thresholds decision rule classifier minimize classification error 
show single threshold derived calibration procedure result equivalent finding point minimum error roc curve 
calibration produces multiple thresholds decision rule error achieved lower single threshold derived roc methods 
addition producing accurate estimates posteriori probabilities calibration obviates need roc methods finding optimal thresholds 
rest organized follows 
section introduces formally notions calibration refinement brier score 
sections contain proofs main results 
section illustrates effects calibration classifiers induced real data observing effect sample size process calibration 
section discusses summarizes main results 
notation preliminary definitions classifier takes incoming vector features maps class label 
denote class variable values called classes 
assume binary classification problem takes values 
denote specific instantiation instantiation denoted sample 
assume samples true posteriori distribution class features 
optimal classification rule optimal function maps sample values cost function maximum aposteriori map rule argmax decision rule called bayes optimal decision eb min associated probability error 
error known bayes error bayes risk minimum probability error achievable set features 
unknown strategy classification induce estimate posteriori probability decision rule classification error ce minimized 
note plugging decision rule eq 
may optimal errors biases embedded estimate threshold implicit eq 
may minimize error eq 

return subject section show link calibration decision rule minimizes eq 

classification error provides way evaluate classifiers 
classifier output basis decision making need score takes account prediction accuracy classifier quality estimate 
score brier score 
brier score class called proper scores evaluating subjective probability assessment forecasters 
binary classification case brier score average squared difference forecaster probability true label bs xi ci number samples 
various intuitive justifications score decision theoretic considerations 
assume agent classifier forecaster pay price proportional confidence asserts decision 
brier score uses probability estimate providing appropriate penalty 
note eq 
agent predicts high probability ci penalty higher predicts low probability 
lower brier score lower penalty assessed agent 
note summation implies finite values features continuous features summation replaced integration 
maintain summation note analysis holds continuous features 
notion calibration derived directly brier score 
need preliminary definitions 
denote posteriori probability assessment forecaster 
assume takes finite number values interval 
denote rt set feature values classifier density yields forecast probability rt 
probability forecaster predicts probability random instance 
thought frequency forecaster predicted probability set samples 
probability density features expressed 
rt probability forecaster predicts probability brier score rewritten see derivation bs 
term measure calibration second term measure refinement forecaster denoted calibration indicates close probability assessment forecaster frequency occurs reality 
note calibration calibrated forecaster calibration equal 
notion calibration fits purposes probability assessments calibrated agent decision making indication confidence classification label provided 
refinement scores usefulness forecast 
illustration assume live place rains time 
forecaster announces rain confidence calibrated useful helping plan day 
ideally estimates close certainty 
concentrated refined classifier 
minimize brier score forecaster calibrated refined 
classifiers calibrated lower brier score refined 
describe relationship bias bayes error calibration refinement section 
brier score bias bayes error show calibrated weaker condition unbiased 
loosely speaking calibrated classifier average unbiased classifier 
show notion refinement second term eq 
bound bayes error 
particular twice refinement calibrated classifier upper bound bayes error case classifier unbiased refinement lower bound bayes error 
section illustrates practical implications various approximations calibrating practice 
bias calibration relationship calibrated requires 
bayes rule write rewritten rt rt denominator replaced eq 

numerator states probability joint event class variable takes value classifier states probability result summing precise events feature space rt 
assumption regarding nature samples holds 
state proposition unbiased classifier calibrated 
proof 
unbiased classifier limn number samples 
rt 
replacing eq yields condition calibration calibrated classifier unbiased 
see eq calibrated classifier forecast rt normalized average true posteriori probability region defined rt 
clearly construct cases classifier biased example suppose 
suppose classifier predicts non zero probability 
obviously classifier biased 
eq 
classifier calibrated 
bayes error refinement relationship start defining dependent error measure min 
essentially mirrors bayes error formula eq see upper bounds bayes error 
ready state result theorem calibrated classifier forecasts true posteriori probability corresponding bayes error rate holds 
proof 
recall calibrated classifier 
making appropriate substitution second term eq 
refinement written 
easy show min follows show eb rewrite expressions bayes error terms rt eb min 
rt eq 
substitute term eq 
obtain min 
rt reformulation show rt min min 
cases 
proceed proof case 
proof second case completely analogous 
case write min rt rt eq 
fact classifier calibrated replace right hand side equation get rt rt rt rt 
rt going rt cases depending 

get min 
follows eqs 
eq 
equal cases 
rt 
min 
sums eq 
bayes error adds smaller term 
follows proof see looseness upper bound bayes error occurs certain rt side respect close chance exist see eq 
close higher chances occurrence cases 
calibrated classifier mass close refined provides tighter bound bayes error 
classifier unbiased provide stronger result 
case know asymptotically rt follows fact classifier unbiased fact relation min holds 
recall samples placed rt value 
calibration classification error roc curves discussed section order minimize classification error eq 
need find appropriate decision rule 
turn translates finding probabilistic threshold classify sample belonging class 
section provide procedure finding terms calibration 
intuition follows 
real density optimal decision rule eq 
turn implies 
process calibrating may seen process bringing closer real density 
calibrating classifier mapping 
fact procedures proposed essentially implement mapping :10.1.1.13.7457:10.1.1.13.7457
certain conditions outline optimal threshold original classifier calibration mapping 
formalize intuitions need express classification error terms calibration mapping density 
suppose decision function say note plug decision rule 
density classification error function written dt dt takes value interval limited discrete set previous section 
integral eq 
weighted area calibration map predict class area proportional probability missed instances label 
second integral provides proportion error predict actual class label 
borrowing terms signal detection theory term proportional probability missed detection detection class second integral proportional probability false detection false alarm 
areas illustrated marked regions figures 
state theorem classifier posteriori probabilities density calibration map cross threshold achieves minimum probability error arg min 
proof 
derivative respect yields dp error 
setting derivative yields reason calibration map provides optimal threshold minimizing probability error quite simple calibration map thought new calibrated classifier single feature threshold new classifier optimal 
require calibrated classifier area fa area uncalibrated classifier area fa area fig 

illustration calibration map calibrated diagonal line non calibrated classifier 
calibration map cross single threshold feature achieves minimum error 
function create roc curves 
see recall roc curve plots probability detection pd ruth probability false alarm pf ruth created varying threshold likelihood ratio 
threshold varied start perfect detection maximum false alarm false alarms minimum detection 
stated integrals composing eq 
directly related pd pf put accurately pd dt pf dt varying threshold generate entire roc curve calibration map 
point clear methods find threshold minimum error roc curves produce exact result calibration procedure calibration map cross 
calibration procedure generalizes achieved roc method 
theorem extended case calibration map crosses requiring multiple thresholds original decision function minimizing error multiple thresholds decision function rewrite equation splitting integral number needed thresholds find minimizing probability error number thresholds occurs calibration map 
cases occur classifiers output posteriori probabilities ranked incorrectly 
example suppose class split clusters space classifier example linear separates clusters leaving clusters far decision boundary 
resultant calibration map classifiers cross places point minimum error 
inverting calibration map point provides thresholds decision rule 
illustrate dimensional example shown 
class marked circles class consists clusters divided class marked class 
learning logistic regression classifier data leads single linear boundary shown separating cluster leaves second far boundary 
data cluster higher probability belonging class data class 
shows calibration map logistic regression classifier 
map crosses values leading decision boundaries slope original different intersects 
boundaries clusters class separated resultant error significantly lower reducing original boundary new boundaries 
class class post calibration boundaries uncalibrated boundary class threshold threshold fig 

example calibration finding multiple thresholds decision rule 
decision boundaries calibration superimposed data 
calibration map original linear classifier 
results extended loss general loss function cost predicting class true class cost predicting class true class 
bayes decision rule minimizing risk loss function calls classifying sample classification error loss applying threshold estimated classifier may minimize risk generalized loss 
arguments theorem finding thresholds minimize generalized loss function classifier value calibration finite data finite data sets want estimate reliably 
procedure estimation provided binned interval calibration map estimated counting number samples fall bin :10.1.1.13.7457:10.1.1.13.7457
procedure originally suggested method calibrating naive bayes classifiers applicable classifier outputs probabilities distance measure converted probabilities tree augmented naive bayes logistic regression mixture models svms 
empirical success calibration various typically large sized data sets shown previous works section aim providing insight finite sample effects arise calibration 
estimating calibration map involves learning function scalar input scalar output 
insensitive number features classifier 
estimation sensitive sample size number bins estimation procedure 
evaluate effect sample size calibration procedure learning curves showing various performance metrics calibration 
calibration procedure prediction response time individual requests storage array 
data anonymized month long trace requests hewlett packard xp storage array collected storage systems group hewlett packard laboratories september october 
raw traces transformed features describe queue lengths locality sequentiality measured server issuing request storage array 
problem transformed binary classification problem determining response time faster equal msec considered fast response time slower msec considered slow 
data consists training data test data 
build competing models predict correct class request 
naive bayes classifier gaussian conditional distribution numerical feature multinomial distribution locality feature 
second mixture regression mor classifier 
mor model finds mixture regressors features response time provides distribution response time value features compute posteriori probability response time fast slow 
full training data naive bayes model achieves accuracy calibration calibration significant improvement 
mor model improves modest improvement expected model naturally calibrated compared naive bayes 
learning curves accuracy brier score shown 
generating learning curves fix number bins calibration procedure average results measured test set trials point curve 
see naive bayes classifier calibration improves accuracy brier score early curve training samples calibrated mor calibration procedure produce significant benefit performance fairly large training sets available 
observing changes brier score appears models achieve near convergence calibrated classifier early samples 
important note mor sample sizes smaller calibration procedure slightly degrades performance overfitting 
experiments illustrate models far calibrated benefit calibration data classification change decision boundary right direction large effect 
models close calibrated sensitive noise calibration map prone overfitting small data sets 
discuss possible ways overcome effects summary 
accuracy nb pre nb post mor pre mor post training samples training samples classification accuracy brier score brier score nb pre nb post mor pre mor post fig 

learning curves naive bayes mor prediction data 
summary characterize mathematical relation calibration bounds bayes error calibration find thresholds decision rule minimizing classifier error 
theoretical results coupled mounting empirical evidence literature illustrate importance value calibrating classifiers classification decision making 
result relating calibration decision rule minimizes classification error produces effective procedure finding optimal thresholds decision rule 
establishes direct relationship roc curves relationship informally alluded formalized 
learning algorithm finite sample effects considered learning curve experiments show simple calibration procedure performs large training sets cause overfitting small training sets 
reducing possibility overfitting done smoothing calibration map estimating smooth function sigmoid calibration map 
note number thresholds decision function depend smoothing function sigmoid threshold desirable 
observe calibration beneficial small sample sizes classifiers inherently calibrated naive bayes compared calibration classifiers naturally calibrated logistic regression 
includes providing bounds estimation error calibration map affects estimation optimal thresholds classification payoff terms decision making bounds help avoid overfitting especially small sample sizes 
extending method binary classification problems research direction similar methods extending roc curves binary classification 
exploring calibration semi supervised learning helping eliminate possibility performance degradation unlabeled data learning classifiers phenomenon occurs biased models output uncalibrated posteriori probabilities 
acknowledgments terence kelly help suggestions response time prediction 
kim keeton providing data tom fawcett comments roc curves george forman charles elkan providing feedback 

degroot fienberg comparison evaluation forecasters 
statistician 
levine easier way calibrate 
games economic behavior 
foster vohra asymptotic calibration 
biometrika 
zadrozny elkan obtaining calibrated probability estimates decision trees naive bayesian classifiers 
icml 

zadrozny elkan transforming classifier scores accurate multiclass probability estimates 
knowledge discovery data mining 

fawcett roc graphs notes practical considerations data mining representation 
technical report hpl hewlett packard labs palo alto ca 
lachiche flach improving accuracy cost class multi class probabilistic classifiers roc curves 
icml 

devroye lugosi probabilistic theory pattern recognition 
springer verlag new york 
brier verification forecasts expressed terms probability 
monthly weather review 
duda hart stork pattern classification 
john wiley sons new york 
friedman geiger goldszmidt bayesian network classifiers 
machine learning 
cozman cohen semi supervised learning mixture models 
icml 

