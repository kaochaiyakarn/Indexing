kernel conditional random fields representation clique selection john lafferty lafferty cs cmu edu zhu cs cmu edu yan liu cs cmu edu school computer science carnegie mellon university pittsburgh pa usa kernel conditional random fields introduced framework discriminative modeling graph structured data 
representer theorem conditional graphical models shows kernel conditional random fields arise risk minimization procedures defined mercer kernels labeled graphs 
procedure greedily selecting cliques dual representation proposed allows sparse representations 
incorporating kernels implicit feature spaces conditional graphical models framework enables semi supervised learning algorithms structured data graph kernels 
framework clique selection methods demonstrated synthetic data experiments applied problem protein secondary structure prediction 

classification problems involve annotation data items having multiple components component requiring classification label 
problems challenging interaction components rich complex 
text speech image processing example useful label individual words sounds image patches categories enable higher level processing labels depend highly complex manner 
biological sequence annotation desirable annotate amino acid protein label collection labels representing global geometric structure molecule 
labels principle depend physical characteristics molecule ambient chemical envi appearing proceedings st international conference machine learning banff canada 
copyright authors 
ronment 
case classification tasks naturally arise clearly violate assumption independent identically distributed instances majority classification procedures statistics machine learning 
central importance extend advances classification theory practice structured non independent data classification problems 
conditional random fields lafferty proposed approach modeling interactions labels problems tools graphical models 
conditional random field crf model assigns joint probability distribution labels conditional input distribution respects independence relations encoded graph 
general labels assumed independent observations conditionally independent labels assumed generative models hidden markov models 
crf framework obtain promising results number domains interaction labels including tagging parsing information extraction natural language processing collins sha pereira pinto modeling spatial dependencies image processing kumar hebert 
related taskar 
studied random fields known markov networks fit loss functions incorporate generalized notion margin observed kernel trick applies family models 
extension conditional random fields permits implicit features spaces mercer kernels framework regularization theory 
extension motivated significant body shown kernel methods extremely effective wide variety machine learning techniques example enable integration multiple sources information principled manner 
mercer kernels conditional graphical models motivated problem semi supervised learning 
domains collection annotated training data difficult costly requires efforts expert human annotators collection unlabeled data may relatively easy inexpensive 
emerging theme research semi supervised learning kernel methods particular graphical representations unlabeled data form theoretically attractive empirically promising set techniques combining labeled unlabeled data belkin niyogi chapelle smola kondor zhu 
section formalize learning problem version classical representer theorem kimeldorf wahba 
classical result kernel conditional random fields dual parameters depend potential assignments labels cliques graph observed labels 
motivates need algorithms derive sparse representations full representation parameters labeled clique graphs appearing training data 
section greedy algorithm selecting small number representative cliques 
clique selection algorithm parallels import vector selection algorithms kernel logistic regression zhu hastie feature selection methods previously proposed random fields conditional random fields explicit features mccallum 
section ideas methods demonstrated synthetic data sets effects underlying graph kernels clique selection sequential modeling clearly seen 
section report results experiments kernel crfs protein secondary structure prediction 
task mapping primary sequences amino acids string secondary structure assignments helix sheet coil 
widely believed secondary structure contribute valuable information discerning proteins fold dimensions 
compare kernel conditional random fields estimated clique selection support vector machine classifiers methods kernels derived position specific scoring matrices psi blast profiles input features 
addition give results graph kernels derived psi blast profiles transductive semi supervised framework estimating kernel crfs 
concludes brief discussion section 
representation proceeding formalism give intuition framework intended capture 
goal annotate structured data structure represented graph 
labels assigned nodes graph order minimize loss function error labels come small set example red blue green 
vertex graph associated feature vector xv image processing feature vector node include pixel intensity average pixel intensities smoothed neighboring regions wavelets 
protein secondary structure prediction node correspond amino acid protein feature vector node may include amino acid histogram protein fragments database closely match protein node 
section notation formal framework problems 

cliques labeled graphs denote collection finite graphs 
example set finite chains appropriate sequence modeling rectangular dimensional grids appropriate image processing tasks 
set vertices graph denoted size graph number vertices denoted 
clique subset vertices fully connected pair vertices joined edge denote set cliques graph 
number vertices clique denoted 
similarly denote collection cliques varying graphs 
words member consists graph distinguished clique graph 
kernels compare components different graphs 
example consider kernel 
consider labelings graph 
finite set labels infinite possible regression framework restrict finite simplicity 
set labelings graph denoted collection graphs 
similarly input feature space example set denotes set assignments feature vector vertex graph collection annotated graphs 
yc set cliques graph 
similarly define yc yc yc yc 

representer theorem prediction task conditional graphical models learn function labeling goal minimizing suitably defined loss function 
classifier hn chosen labeled sample labeled graph graph possibly changing example example 
limit complexity hypothesis assume determined completely function yc denote collection values varying cliques varying possible labelings clique 
assume loss function 
important example loss function consider negative log loss fc yc log exp fc fc yc shorthand yc 
negative log marginal loss considered minimizing node error 
negative log loss function corresponds conditional random field exp fc yc discuss representer theorem kernel machines kimeldorf wahba applies conditional graphical models 
simple extension re aware analogous formulation statistics machine learning literature 
mercer kernel yc yc yc yc 
intuitively assigns measure similarity labeled clique graph labeled clique possibly different graph 
denote hk associated reproducing kernel hilbert space associated norm yc 
consider regularized loss function form important note loss depends possible assignments labels clique just observed labeled data suppressing dependence graph notation kc 
argument standard representer theorem easily shown minimizer regularized loss function form expressed terms basis functions 
proposition representer theorem crfs 
mercer kernel yc associated rkhs norm strictly increasing 
minimizer exists form yc kc key property distinguishing result standard representer theorem dual parameters depend assignments labels 

special cases mercer kernel kernel defined terms matrix entries 
define kernel edges yc xv 
regularized risk minimization problem min min hk hk hk crf representer theorem implies solution form xv special case kernel follows xv probabilistic model simply kernel logistic regression 
special case get xv recover simple type semiparametric crf 

clique selection representer theorem shows minimizing function supported labeled cliques training examples may result extremely large number parameters 
pursue strategy incrementally selecting cliques order greedily reduce regularized risk 
resulting procedure parallel forward stepwise logistic regression related methods kernel logistic regression zhu hastie greedy selection procedure della pietra 
algorithm maintain active set yc yc labeled cliques labelings restricted appearing training data 
candidate clique represented basis function hk assigned parameter 
regularized risk log loss equation 
evaluate candidate strategy compute gain sup choose candidate having largest gain 
presents apparent difficulty optimal parameter computed closed form evaluated numerically 
sequence models involves forward backward calculations candidate cost prohibitive 
alternative adopt functional gradient descent approach evaluates small change current function 
candidate consider adding current model small weight 
dr functional derivative direction computed dr ef empirical expectation ef model expectation conditioned combined empirical distribution idea directions functional gradient dr large model mismatched labeled data direction added model correction 
results greedy clique selection algorithm summarized 
earlier notation sum cliques 
candidate functions include functions form initialize iterate 
candidate hk supported single labeled clique calculate functional derivative dr 

select candidate arg max dr having largest gradient direction 
set hh 

estimate parameters active minimizing 

greedy clique selection 
labeled cliques encode basis functions greedily added model form functional gradient descent 
specific instance particular clique yc labeling clique 
alternatively slightly greedy manner step selection procedure specific instance clique may selected functions clique labeling may added 
experiments reported sequences marginal probabilities expected counts state transitions required computed forward backward algorithm log domain arithmetic avoid underflow 
quasi newton method bfgs cubic polynomial line search estimate parameters step 
prediction carried forward backward algorithm compute marginals viterbi algorithm 

combining multiple kernels kernels enables semi supervised learning structured prediction problems 
emerging themes semi supervised learning graph kernels provide useful framework combining labeled unlabeled data 
undirected graph defined labeled unlabeled data instances generally assumption labels vary smoothly graph 
graph represented weight matrix construct kernel graph laplacian substituting eigenvalues non negative typically decreasing function 
high frequency components encourages smooth functions graph see smola kondor description unifying view graph kernels 
important note graph kernel semi supervised learning introduces additional graphical structure confused graph representing explicit dependencies labels crf 
example modeling sequences natural crf graph structure chain 
incorporating unlabeled data graph kernel additional graph generally cycles implicitly introduced 
graph kernel standard kernel may naturally combined linear combination see example lanckriet 

synthetic data experiments demonstrate properties advantages prepared synthetic datasets galaxy dataset investigate relation semi supervised sequential learning hmm gaussian mixture emission probabilities demonstrate properties clique selection advantages incorporating kernels 
galaxy 
galaxy dataset variant spirals see left 
note dense core points classes 
sequences generated state hidden markov model hmm state emits instances uniformly classes 
chance staying state 
idea sequence model example core better random chance labeled correctly context 
true non sequence model dataset bayes error rate iid assumption 
sample sequences length 
note choice semi supervised vs standard kernels sequence vs non sequence models orthogonal combinations tested 
construct semi supervised graph kernel creating unweighted nearest neighbor graph 
compute graph laplacian form kernel corresponds function eigenvalues 
standard kernel radial basis function rbf kernel bandwidth 
parameters tuned cross validation 
center shows results kernel logistic regression semi supervised kernel rbf kernel sequence structure ignored 
training set size ranges points random trials performed 
error intervals shown standard error 
labeled set size small graph kernel better rbf kernel 
kernels saturate bayes error rate 
apply kernels semiparametric model section see right 
note axis number training sequences sequence instances range center 
kernel crf capable getting bayes error floor non sequence model kernels sufficient labeled data 
graph kernel able learn structure faster rbf kernel 
evidently high error rate low label data sizes prevents rbf model effectively context 
hmm gaussian mixtures 
difficult dataset generated state hmm 
state mixture gaussians random mean covariance 
gaussians strongly overlap see left 
transition probabilities favor remaining state probability transition states equal probability generate sequences length 
rbf kernel 
graph kernel slightly worse rbf kernel dataset shown 
perform trials training set size trial perform clique selection select top vertices 
center right plots show semiparametric outperforms kernel logistic regression rbf kernel 
shows clique selection training size sequences averaged random trials 
regularized risk left training set likelihood plus decreases select vertices 
hand test set likelihood center accuracy right saturate worsen slightly showing signs overfitting 
curves change dramatically demonstrating effectiveness clique selection algorithm 
fact fewer vertex cliques sufficient problem 

protein secondary structure prediction protein secondary structure prediction task rs dataset current methods developed tested cuff barton 
non homologous dataset protein chains proteins share sequence identity length residues cuff barton 
dataset downloaded barton ebi ac uk 
adopt definition protein secondary structure sander hydrogen bonding patterns geometric constraints 
discussion cuff barton labels reduced state model follows map helix sheets states coil 
state art performance secondary structure prediction achieved window base methods position specific scoring matrices pssm input features psi blast profiles support vector machines svms underlying learning algorithm jones kim park 
raw predictions fed second layer svm filter physically unrealistic predictions sheet residue surrounded helix residues jones 
test error rate semi supervised rbf training set size test error rate semi supervised rbf training set size 
left galaxy data 
center kernel logistic regression comparing kernels rbf graph kernel unlabeled data 
right kernel conditional random fields take account sequential structure data 
test error rate training sequences test error rate training sequences 
left gaussian mixture data data points shown 
center kernel logistic regression rbf kernel 
right kernel crf kernel 
experiments apply linear transformation pssm matrix elements 
transform kim park achieved best results casp critical assessment structure predictions competition 
window size set cross validation 
number features position number amino acids plus gap 
clique selection 
rbf kernel bandwidth chosen cross validation 
left shows kernel crf risk reduction clique selection proceeds vertex clique candidates allowed note position independent edge parameters models prevent models degrading kernel logistic regression vertex edge cliques allowed 
kernel vertex cliques edge cliques 
total number clique candidates vertex vertex edge 
rapid reduction risk indicates sparse training kernel crfs successful 
flexibility allowed including edge cliques risk reduction faster 
flexible model higher test set log likelihood center improve test set accuracy right 
observations generally true trials 
residue accuracy 
evaluate prediction performance residue accuracy known 
experiment training set size sequences respectively 
size perform trials training sequences randomly sampled remaining proteins test set 
kernel crf select cliques vertex candidates vertex edge candidates 
compare svm light package joachims svm classifier 
methods rbf kernel 
see table 
svms comparable performance 
transition accuracy 
information obtained studying transition boundaries example transition coil sheet point view structural biology transition boundaries may provide important information proteins fold dimension 
hand positions secondary structure prediction systems fail 
transition boundary defined pair adjacent positions true labels differ 
classified correctly labels correct 
hard problem seen table able achieve considerable improvement svm 
semi supervised learning 
start unweighted nearest neighbor graph positions training regularized risk number selected vertices test log likelihood number selected vertices test accuracy number selected vertices 
clique selection gaussian mixture data 
left regularized risk center test set log likelihood right test set accuracy 
regularized risk number cliques vertex vertex edge test log likelihood number cliques vertex vertex edge test accuracy number cliques vertex vertex edge 
clique selection protein data 
left regularized risk center test set log likelihood right test set accuracy 
curves represent cases vertex cliques selected dashed vs vertex edge cliques selected solid 
test sequences metric euclidean distance feature space 
eigensystem normalized laplacian computed 
semi supervised graph kernel obtained function eigenvalues 
rest eigenvalues set zero 
graph kernel rbf kernel 
clique candidate associated kernel select best candidates iteration graph kernel rbf kernel 
run iterations trials 
report results transductive svms joachims rbf kernel 
results table see semi supervised graph kernel significantly better protein dataset achieves improvement 
diagnose cause look graph test labels 
find labels smooth graph average node neighbors label node 
detecting faulty graphs large amount labels constructing better graphs remain research 
approximate average running time trial including training testing minutes minutes svms hours 
majority time spent clique selection 

kernel conditional random fields introduced framework approaching graph structured classification problems 
representer theorem derived shows motivated regularization theory 
resulting techniques combine strengths hidden markov models general bayesian networks kernel machines standard discriminative linear classifiers including logistic regression svms 
formalism quite general apply naturally wide range problems 
experimental results synthetic data carefully controlled simple clearly indicate sequence modeling graph kernels semi supervised learning clique selection sparse representations framework 
success methods real problems depend choice suitable kernels capture structure data 
protein secondary structure prediction results suggestive 
secondary structure prediction problem extensively studied years task remains difficult prediction accuracies remaining low 
major bottleneck lies beta sheet prediction long range interactions regions protein chain necessarily consecutive primary sequence 
experimental results indicate semi supervised kernels protein set protein set method accuracy std accuracy std svm table 
residue accuracy different methods secondary structure prediction rbf kernel 
uses vertex cliques uses vertex edge cliques 
protein set protein set method accuracy std accuracy std svm table 
transition accuracy different methods 
protein set protein set method accuracy std accuracy std trans 
svm table 
residue accuracy semi supervised methods 
potential lead progress problem state art heuristic sliding window methods 
results suggest improvement due semi supervised learning hindered lack similarity measure construct graph 
construction effective graph challenge may best tackled biologists machine learning researchers working 
acknowledgments supported part nsf itr ccr iis iis 
altun tsochantaridis hofmann 

hidden markov support vector machines 
icml 
belkin niyogi 

semi supervised learning manifolds technical report tr 
university chicago 
chapelle weston 

cluster kernels semi supervised learning 
nips 
collins 

discriminative training methods hidden markov models theory experiments perceptron algorithms 
proceedings emnlp 
cuff barton 

evaluation improve ment multiple sequence methods protein secondary structure prediction 
proteins 
della pietra della pietra lafferty 

inducing features random fields 
ieee pami 
joachims 

text categorization support vector machines learning relevant features 
ecml 
joachims 

transductive inference text classification support vector machines 
icml 
jones 

protein secondary structure prediction position specific scoring matrices 
mol biol 
sander 

dictionary protein secondary structure pattern recognition geometrical features 
biopolymers 
kim park 

protein secondary structure prediction improved support vector machines approach 
protein eng 
kimeldorf wahba 

results spline functions 
math 
anal 
applic 
kumar hebert 

discriminative fields modeling spatial dependencies natural images 
nips 
lafferty mccallum pereira 

conditional random fields probabilistic models segmenting labeling sequence data 
icml 
lanckriet cristianini laurent el ghaoui jordan 

learning kernel matrix semi definite programming 
journal machine learning research 
mccallum 

efficiently inducing features conditional random fields 
uai 
pinto mccallum wei croft 

table extraction conditional random fields 
si gir 
sha pereira 

shallow parsing conditional random fields 
proceedings hlt naacl 
smola kondor 

kernels regularization graphs 
colt 
taskar guestrin koller 

max margin markov networks 
nips 
zhu hastie 

kernel logistic regression import vector machine 
nips 
zhu lafferty 

semisupervised learning gaussian fields harmonic functions 
icml 
