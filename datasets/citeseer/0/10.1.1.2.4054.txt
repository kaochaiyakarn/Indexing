proc 
th int riao conf montreal canada june pp 
sophisticated wrapping web information repositories boris uwe pierre yves chevalier rank xerox research centre grenoble laboratory de 
france mail chevalier grenoble xerox com tel access line information web exploding 
index retrieval engines start integrate huge variety heterogeneous repositories 
heterogeneity issue remains terms search formats formats result pages 
focus html search result presentations 
discuss experience design development maintenance wrappers context knowledge broker project 
outline different ways write wrappers illustrate lessons learned conclude describing semi automatic approach efficient wrapping web information repositories 
give illustrating examples hands readers 
keywords world wide web heterogeneous repositories wrapping information extraction rule parsing 
continuous growth documents available internet especially world wide web creates need new technologies mechanisms query different information stored heterogeneous repositories 
query facilities provide homogeneous access underlying heterogeneity allow homogeneous representation information 
existing indexing retrieval engines web support rudimentary ways querying huge amount information stored web repositories odl sek 
remains problem dealing diversity indexing schemes formats results appear web 
addition information retrieved different repositories merged unified 
apart communication retrieval engines problem consists sophisticated analyses result pages returned retrieval engines putting parsed answers unified format 
literature activities covered term wrapping 
time saving highly declarative level approaches wrapping cornerstone projects heterogeneous information retrieval qrs 
reality main problem encounter working web repositories unwrapping wrapping 
extraction relevant information raw result pages difficulties arise storing extracted information unified format wrapping cases minor problem 
remainder shall term wrapping unwrapping wrapping information 
presents experience high level wrapping heterogeneous information retrieved web repositories 
part knowledge broker project abp abp 
approach integration distributed information repositories contrasts federated database projects rely meta schema meta indexes keep system independent possible external sources 
provides greater flexibility integrating remote sources introduces additional problems respect maintenance wrappers 
report different approaches wrapping exploited starting naive hand coding advanced approach exploiting rule analysis 
assume protocol querying web repositories retrieving result pages 
organized follows 
section describes problem gives pointers related 
section classify aspects result pages provided html format 
distinguish format uniqueness completeness different levels 
section introduces reliability problem result page analysis 
wrapper design approached different ways illustrated section 
section discuss particular implementation issues 
section concludes 
problem description related key problems wrapper subsystem consist backend analysis wrapper coding wrapper maintenance 
backend analysis intrinsic requirement high quality solution problem clean wrapper design 
problem aspects vary detecting initial urls needed cgi structures retrieval engines backends question analyzing html pages returned engines 
main approaches wrapper coding analyzed 
naive hand coding approach 
second sgml parser spots html tags extracts raw data tags 
works html tags define transitions 
parser wrapper structure transparent usually longer 
experience length parser particular wrapper may lines code 
third approach uses lexical syntactical grammar rules automatically generate parser 
experience shows full parser defined rules 
key problems listed difficult issue wrapper maintenance 
information web continuously updated 
change numerous backend aspects may deeply impacts corresponding wrapper 
problem worse cases interactive user detect chang ing backend structure terms search form terms layout result page 
wrapper maintenance difficult due changes html page retrieval engine backend file locations html format result page importantly due changes search retrieval functionality 
public access repositories web stable quite library congress altavista webcrawler backends phase interrupted existence continuous improvements heavy modifications line catalogue database databases logic programming maintained www informatik uni trier de ley db index html 
worse control external sources global meta data catalog index available 
system interest wide range users need high level tool allows create maintain dozens hundreds wrappers simple transparent mode 
wrapping problem addressed research projects heterogeneous information retrieval 
tsimmis project cgh ghi qrs joint project stanford university database group ibm 
project focus integration structured unstructured data sources techniques rapid prototyping wrappers techniques implementing mediators 
system put lot emphasis conversion high level sql queries source specific queries translation returned results data meta model 
follows approach naturally biased database issues lack full flexibility needed accommodate diversity rapid evolution world wide web 
group pmg univ frankfurt germany exploits conceptual graphs matching problem functionally equivalent attributes despite different data schemata 
main focus lies building mediator functionality trading interworking open distributed environments 
ai techniques implement matching learning algorithms 
sims information mediator provides access heterogeneous data knowledge bases 
structured network cooperating information agents agents provides access subset resources available server information source agents 
focus sims representation knowledge management semantic issues extraction information se 
strengths system ability cope semantic changes information sources transparent manner 
actual connection external source specific respect hard wired 
sims lacks flexibility required extract raw information external sources cope evolution sources 
projects related wrapping problem include coin mit harvest univ colorado garlic ibm almaden hermes univ maryland information manifold infosleuth mcc kraft univ aberdeen univ wales univ liverpool bt 
classification aspects result pages html section look briefly classification aspects concerning result pages html format wrappers 
format uniqueness completeness homogeneity parameter pair provides obvious classification html pages returned different backends 
define completeness homogeneity search request returns set elements resulting html page 
classification rigorous structure unique format complete information example altavista returns name body element unstructured raw information 
semi rigorous structure unique format incomplete information example library congress provides list attributes returned item attributes common items 
semi relaxed structure unique format complete information semi relaxed structures result data sets put different legacy collections 
relaxed structure unique format incomplete information home pages web relaxed structures 
experience shows useful sources information web publish data stored internal structured format bibtex database formats 
data wrapped html format corresponding generator see 
wrapped format user gets queries search engine backend 
original structure data normally lost generated html keeps sort structure 
gateways html forms searching loose structures full fledged attribute rich protocol 
name just opac gateways written library congress munich germany 
publication structured data web trend increase near 
consider sources exhibit rigorous semi rigorous structures primary targets 
broker systems knowledge brokers try unify result pages different backends 
try extract attribute information increase precision hits local constraint solving techniques bcw 
systems solve inverse problem library congress unwrapping semi structured html structured data 
different aspects distinguished 
aspect deals fully inverse html generator 
html generators quite simple part simple 
second part concerns data 
part harder data typically input manually little emphasis accuracy coherence 
parsing step unwrapping example expect symbols sentences unpredictable position 
html quality level 
typical scheme application backend interaction aspect important previous 
classification ranges high level low level 
existing information repositories variation extreme classifications 
high level item result page surrounded couple html tags tagged items corresponds exactly attribute original bibliographic data title author body 
line catalogue ieee server www computer org cgi bin example returns sentence dt href www computer org catalog proc top htm conference proceedings object oriented database computing dd score size kbytes type html file low level string html tags corresponds output attributes 
case additional plain text separators separating different attributes original bibliographic data 
html generators provide author title attributes single author title string 
separation attributes context dependent 
analysis html structure result pages faced plain text analysis unsatisfactory task 
reliability problem result page analysis discuss different techniques web wrapper implementation introduce important aspect common wrapper reliability 
explain problem example rule parsing 
concerns fundamental difference standard parsers html parsing 
standard grammar parser invoked scan input files highly rigorous structure defined syntax free inconsistencies program program contains syntax error error message generated code produced 
clearly properties usually verified data stored web error messages states allowed web information parser 
information inconsistencies 
parsed accepted 
parsers quite smart 
aspect reliability problem analysis backend results essentially partial 
practice process web parser creation proceeds follows 
set priori defined queries generated goal obtain corresponding result pages generate parser works pages 
set may big small cover data backend repository may contain huge amount information 
wrapper testing parsing error happens new return page parser adapted properly handle particular error 
wrapper testing goes confident wrapper works 
problem validate reliable parser new queries covered testing queries submitted backend 
obtaining absolutely reliable parser scanning entire web repository unacceptable exists probability parsing fault due inconsistencies met previous result pages 
reason level parser reliability reached practice information backends continuously updated 
state having information correctly parsed unreachable 
probability parsing fault remains independent queries successfully processed parser reliability expressed probability gives extent parser reliable 
estimate parser reliability apply basic statistical methods 
testing phase provide parser set queries generated randomly 
random queries new version parser assume results pages obtained queries items total pages parsed accurately 
note rigorously page structured required reach level 
basically high reliability requires probability parser fail new result page low 
statistical evaluation apply central limit theorem mrt states random variable input error sqrt approximately distributed standard normal distribution 
calculate example items successfully parsed result pages confident 
practice value estimated level confidence fault probability demanded user 
sum main requirements wrapper design procedure wrapper permit analysis html structure explained look inside structures 
open possible backend modifications 
consequence order ease maintenance short transparent possible 
fast highly reliable 
project looked implemented main approaches backends result analysis manual coding parsing tag marks specification rules 
discuss approaches lessons learned far detail 
wrapper design start previous approaches wrapper design move semiautomatic design specification rules 
hr bass leonard generalized user interface applications programs ii commun 
acm june 
href pubs toc abstracts cacm html index terms review gray michael naumann david abstraction software development commun 
acm may 
href pubs toc abstracts cacm html index terms review avi sara users happy commun 
acm july 
href pubs toc abstracts cacm html index terms review van dyke computer literacy literally commun 
acm may 
href pubs toc abstracts cacm html index terms review get groove designing participation interactions april 
href pubs toc abstracts interactions html index terms hr 
html page returned acm publication server 
text bold font highlights cases requires specific attention rule approach due presence special characters quotes columns question marks remainder simplified example acm publication server www acm org pubs toc search html 
shows html fragment returned server 
represents list items item corresponding article published acm journals 
article information lines line denotes author list second line denotes title article 
third line shows journal title volume issue date pages line stores html link additional information article 
classification section html fragment rigorous structure low html quality 
manual coding approach naive approach prototype unwrapping wrapping data initial backends 
code written python represented simple step step analysis result pages 
main drawbacks code quite long assumptions concerning structure page parsed spread code 
maintenance wrapper got complicated impossible determine assumptions 
hand staff untrained parsing 
transition state automata depending state tag name void tag tag throws ioexception string tag getelement getname equals state skip sep state parse hits equals state parse hits state hit done state parse title equals state parse title state parse journal void tag tag throws ioexception 
called raw text encountered void byte text throws ioexception state parse title title append new string text state parse journal journal append new string text state parse vol volume append new string text subset acm tag marks wrapper parsing tag marks parsing tag marks goes step manual coding 
relies sgml parser implemented python java spots html tags page 
wrapper keeps track transitions tag collects raw information tags 
shows subset java wrapper acm publication server 
shows main methods keep track transitions extract information relative article 
extraction fields requires definition states plus additional postprocessing separate authors title build article 
approach mixes high level description parser definition transitions html tags low level processing extract information fields 
difficult follow flow automata assumptions structure hard wired code 
people trained parsing needed design wrappers expected simplicity reached 
maintenance remains difficult due excessive complexity rigidity code 
hand possible parsers tolerate inconsistencies exceptions html page 
errors simply ignored parser just moves forward tag 
specification rules advanced approach analysis backend results wrapper design high level text processing tools base grammar rules unix lex yacc 
lex generates programs simple lexical text analyzers yacc 
source file lex contains regular expressions search actions written language executed expressions 
yacc converts context free grammar set tables simple automaton executes lalr parsing algorithm 
yacc parser constructor proposed java language cup 
corresponding manual describes basic operation basic java constructor useful parsers java cup short 
yacc java cup system generating lalr parsers simple specifications 
offers features yacc 
unix yacc java cup written java uses specifications including embedded java code 
produces parsers implemented java 
sun released tool high level grammar specifications 
new tool called javacc javacc written java 
automatically generates parsers compiling high level grammar specification stored text file 
lex builds lexical analyzers regular expressions yacc reduces grammar specification table driven compiler produce code successfully parsed productions grammar 
grammars javacc accepts ll grammars opposed lalr grammars yacc 
power automatic parser generation allows users concentrate grammar worry correctness implementation 
tremendous time simple complex projects 
javacc powerful tool project parser generation 
avoid going details javacc syntactical agreements show grammar acm result pages bnf form see 
li list hr article article hr article authors title source source title volume issue date year pages href url authors name title string name string year number volume number issue number pages number number url string number html string grammar types elements 
bnf definition acm wrapper 
simplest elements constant sub strings source page parser tags 
may usual html tags hr plain text characters 

second group tokens number string name recognized lexical analyzer 
token sub string source page defined regular expression unix lex 

productions lower case divided subgroups terminal productions correspond output attributes authors title 
grammar productions right hand side 
nonterminal productions li list article source drive parsing process correspond output attribute 
lexical analysis problems 
key problem design appropriate lexical analyzer able recognize set characters 
apart auxiliary characters different kinds parenthesis occur frequently article titles see possible special characters languages german french greek may appear titles authors names fragments mathematical formulas accepted analyzer 
handle requirement analyzers negation methods regular expressions frequently direct methods 
direct regular expression indicate characters acceptable token 
contrary negation method indicate characters acceptable characters acceptable 
negation allows compact transparent definition grammar performance cost compared direct method 
sum main advantages disadvantages approach specification rules main advantages 
allows processing regular search result high level grammar 

time efficient generates java code compiled 

code short 
main disadvantages 
javacc html oriented special lex analyzer html pages missing implemented 

creating rules new parser backend requires knowledge lr parsing easy staff untrained parsers 

current version parser rigid compared approach 
allow inconsistencies just stops source systematic errors 
result able ignore badly formatted text continue smoothly 
hope possible overcome limitation versions parser 
implementation year half developed wrappers sources 
sources tag marks approach tag marks version rule version 
put lot emphasis code reuse quite natural context wrappers perform similar functions 
average source specific part tag marks wrapper lines code 
implementation time tag marks wrapper ranges hours days 
structured consistent html page easier wrap 
experience rule wrappers shows significant reduction development effort assuming masters concepts lr parsing 
lines productions nonterminal ones sufficient define main structure source page wrapper expects parse 
hope maintenance efforts mainly reduced updating main rules 
order increase amount code reuse approach grouping overlapping wrapper implementations 
main idea partition wrappers groups basis common features return pages exhibit 
due approach obtain called group wrappers 
group wrapper perform connection particular backend 
similar parser class contains data functions common group backends 
simplest group wrappers oriented typical cases seen result pages 
level page result search engine backend provides page contains items related original query databases logic programming server 

level multi page result search engine backend provides sequence pages portions full answer set 
broker wrapper follow hits links get full answer set altavista 

level pages item level result page wrapper follow full info link order navigate result page contains data related query elsevier publisher server www elsevier nl cgi bin inca search 
group wrapper class provides unified way scan html page returned backend 
step scan wrapper extracts useful data return page splits data pieces corresponding items called articles 
articles processed second step 
step backend group second step backend dependent 
words group wrapper backend group short description processing article prepared 
specification designed order achieve main goals recognize structure backend article associate data backend result output attributes data correspond 
specifications rules length description corresponding particular backend short lines code 
eventually second step result page scanning group wrapper loads main memory article specification corresponding backend uses extracting putting article data corresponding output attributes 
main benefits step approach lies simplicity ease maintaining large set wrappers 
controlled access structured web information heterogeneous information repositories challenge broker mediator trading facilitator system 
described approach sophisticated wrapper design followed knowledge broker project 
short problem description classified target environment html pages wrappers discussed html quality levels explained reliability problem result page analysis 
wrapper design described different approaches manual coding approach parsing specification rules 
addressed lessons learned course project 
acknowledgments wish glance careful reading manuscript 
wish jean marc andreoli laurent pareschi helpful discussions comments 
michael ley university trier germany help working databases logic programming server 
abp 
andreoli pareschi constraint agents information age 
universal computer science 
electronic version available www edu 
abp 
andreoli pareschi constraint knowledge broker model semantics implementation analysis 
symbolic computation press 
aho ullman theory parsing translation compiling volumes prentice hall 
combining knowledge heterogeneous information repositories 
universal computer science 
electronic version available www edu 
bcw 
chevalier adaptive refinement search patterns distributed information gathering 
proc 
int 
conf 
london december san diego society computer simulation pp 
cgh chawathe garcia molina hammer ireland papakonstantinou ullman widom tsimmis project integration heterogeneous information sources 
proc 
ipsj conf tokyo japan october ieee press 
cup cup java constructor useful parsers 
www cc gatech edu gvu people faculty hudson java cup status may 
ghi garcia molina hammer ireland papakonstantinou ullman widom integrating accessing heterogeneous information sources tsimmis 
proc 
aaai symposium information gathering pp 

javacc javacc parser generator java 
www com javacc status mar 
knoblock ambite agents information gathering 
appear software agents bradshaw ed aaai mit press menlo park ca 
looking lex yacc java don know jack 
jack old name javacc 
www javaworld com javaworld jw jw jack html status dec 
mrt mosteller rourke thomas probability statistical applications 
addison wesley 
odl obraczka danzig 
li internet resource discovery services 
ieee computer pp 

pmg ai trading open distributed environments 
proc 
rd int 
tc conf 
open distributed feb brisbane australia 
chapman hall 
burger new concepts qualitative trader cooperation 
proc 
int 
conf 
distributed platforms 
qrs quass rajaraman sagiv ullman widow querying semistructured heterogeneous information 
proc 
int 
conference deductive oo databases 
sek schwartz kahle neuman comparison internet resource discovery approaches 
computing systems pp 

