sensor fusion video surveillance gian luca niu varshney dept mathematics computer science dept electrical engineering computer science university udine syracuse university delle scienze udine syracuse ny italy usa dimi varshney syr edu multisensor data fusion system object tracking 
able track real time multiple targets outdoor environments 
system take advantage redundant information coming different sensors monitoring scene 
measurements positions targets obtained available sources fused obtain accurate estimate 
data fusion performed considering sensor reliability time instant 
confidence measure employed weight sensor data fusion process 
compared single camera systems adopted approach produced accurate continuous trajectories reducing calibration segmentation errors 
keywords surveillance systems multisensor data fusion object detection 
events september demonstrated need improving surveillance capabilities public areas airports metro railway stations parking lots tunnels bridges order prevent terrorist acts 
advanced multisensor surveillance systems represent possible answer prevent terrorist attacks enhancing monitoring control capabilities remote human operators large environments 
systems perform real time intrusion detection suspicious event detection complex environments 
new generation surveillance systems manage large amounts visual data optical infrared development sensor technology computer networks contributed increasing interest distributed systems real time information fusion 
system integrates optical infrared ir sensors support hours day real time surveillance outdoor environments proposed 
ir optical sensors level proposed architecture 
video signals physical sensor processed extract moving image regions called blobs features computed target tracking classification data fusion procedures 
integration allows improve accuracy object localization higher levels ground plane hypothesis object recognition 
level specialized processing nodes pn track detected blob image plane transform blob positions sensor coordinates system object positions coordinates monitored environment map 
level pn committed surveillance sub area monitored environment 
surveillance systems target tracking paramount importance 
user generally interested estimating position velocity objects scene trajectory 
estimates affected kinds noise process noise measurement noise 
due uncertainty model describes motion objects scene 
target trajectories fact completely predictable targets deliberately non cooperative maneuvering unpredictable manner military surveillance applications 
filtering techniques require definition model order predict target state reason actual measurements differ substantially predictions 
measurement noise primarily caused acquisition process coordinates transformation algorithms 
noise severely affect observation current state target affecting prediction phase 
process noise reduced adopting multimodel filtering techniques imm estimator measurement noise commonly tackled adopting accurate sensors 
advancements cameras processing technology multi sensor solution desirable 
greater system robustness performance fact achievable suite sensors data fusion techniques 
known practice radar applications data fusion considered video systems 
works addressed tracking humans vehicles multiple sensors 
main hurdle additional computational requirements removed great processing power today cpus 
intelligent sensors able perform great deal required computation available 
data fusion squeeze increased performance set unreliable cameras interesting results attained adopting standard ones 
multi sensor tracking system 
employs multiple video cameras monitoring area 
general architecture discussed section 
system explicitly takes account sensors accuracy fusion process reliability factor defined described section 
preliminary experimental results configurations involves homogeneous sensors color cameras second heterogeneous sensors employed optical infrared cameras monitoring outdoor area 
architecture processing adopted architecture follows guidelines video surveillance systems 
composed static sensors processing nodes pns area interest shown 
level nodes higher nodes sensor level ir optical map area area ir optical object trajectory fig 
distributed architecture system sensors monitoring area connected pn responsible tracking objects area 
pn charge image processing steps intelligent sensors available 
particular responsible running algorithms needed identify moving objects change detection video source image differencing filtering blob extraction 
known computer vision applications detailed description 
blob extraction low level processing step yields blobs moving objects scene 
point features dimensions area centroid coordinates extracted blob 
attributes fundamental processing phase object tracking 
blob extraction processing step occurs sensor level pinpoints moving regions image change detection algorithms 
motion detection blob ex higher level node level node sensors trajectories area visual trajectory trajectory fusion measurement fusion tracking tracking blob matching blob fusion blob matching ir vi blob extraction blob extraction optical sensor vi vi ir vi map trajectories trajectories area trajectories area ir trajectory ir sensor fig 
processing steps 
traction exploited layered background subtraction approach :10.1.1.33.8388
change detection performed algorithm automatic threshold computation euler numbers 
background updated kalman filter 
frame frame subtraction applied conjunction improve detection results :10.1.1.33.8388
morphological filters applied improve quality extracted blobs removing spurious pixels due noise enhancing regions connectivity :10.1.1.33.8388
scheme time images vi ir produced respectively optical infrared sensor 
sensor applies blob extraction procedure obtaining arrays vi ir containing blobs extracted current frame time respectively optical infrared sensor 
cameras color infrared monitoring location area presence fog low illumination 
color camera obviously performs poorly situation seen row pictures fig 

influences blob extraction procedure seen second row images 
silhouette walking person extracted correctly 
ir camera gives useful video signal third row images person clearly visible scene 
processed frames blob correctly extracted row 
target tracking video surveillance system usually multiple objects exist scene 
example parking lot daytime objects people vehicles move 
system needs maintain tracks objects simultaneously 
typical multi sensor multitarget tracking problem measurements correctly assigned associated target tracks target associated measurements different sensors fused ir ir obtain better estimation target state 
tracking procedure occurs locally image plane detected moving regions blobs matched objects previous frame previous time instant 
sensor image processing performed extract blobs indicated section 
system executes association algorithm match current detected blobs extracted previous frame 
number techniques available spanning template matching features matching sophisticated approaches :10.1.1.33.8388
approach system twofold exploiting predictions matching blob features hu moments base height ratio 
perform data fusion common frame needed sensors 
generally top view map monitored environment taken common coordinates system gps may employed globally pinpoint targets :10.1.1.33.8388
approach obviously straightforward implement known result projective geometry states correspondence image pixel planar surface planar homography 
pixel usually chosen represent blob transformed map coordinates projection blob centroid lower side bounding box 
measurement gating assignment performed 
local procedure considers objects field view corresponding sensor 
object known previous time instant measurements objects positions falling gating distance fig 
considered 
object trajectory position instant fig 
gating 
validation region measurements considered instant mahalanobis distance determine validation region 
step reduces probability erroneous associations due noise 
measurements coming sensor object falling gating region fused described section 
deal multi target data assignment problem especially presence persistent interference matching algorithms available literature nearest neighbor nn joint probabilistic data association jpda multiple hypothesis tracking mht assignment 
choice depends particular application detailed descriptions examples 
trajectory top view map object modeled linear kalman filter state vec tor vx vy constituted position velocity object map 
frame system processes frames second new measurement position received 
position estimates different sensors fused centralized fashion 
data fusion performed considering sensor reliability time instant 
confidence measure employed weight local estimates fusion process discussed section 
fusion process fusion performed kalman filter approach purpose obtaining better position estimates observed objects 
fusion schemes shown considered experiments measurement fusion track track fusion 
data fusion optimal estimate sensor data sensor data data fusion fused estimate estimate estimate feedback filter filter sensor data sensor data fig 
measurement track track fusion schemes 
scheme involves fusion positions target different sensors obtained right coordinate conversion function seen left 
performs fusion local estimates shown right 
measurement fusion algorithm theoretically optimal track fusion scheme computational requirements sub optimal nature 
dealing extremely noisy sensors video sensors performing poorly due low illumination conditions track track scheme generally preferred 
running kalman filter track obtain filtered estimate target position allows smoothing high variations due segmentation errors 
actual scheme employed track track feedback rationale computational constraints 
fact experiments high frequency measurements real time requirements allow take account feedback information 
process target involves steps collection measurements available local sensors grouping assignment measurements target known previous time instant updating target state feeding associated filtered estimates fusion algorithm 
fusion procedure maintains list targets 
note second step performed constraint single measurement sensor associated single target list maintained fusion procedure 
regulate fusion process automatically performance sensors confidence measure section weight local estimates 
appearance appearance ratio ar tracking accuracy improved data fusion exploiting redundancy multiple camera architecture 
fusing data collected different sensors requires determination measurements accuracy fused weighted manner 
making distinction measurements lead filter instability erroneous estimates especially presence malfunctioning sensors 
hardware failures unfavorable illumination conditions optical camera night time yield poor performance generate segmentation errors blobs partially extracted noise 
considering measurement equal weight fail accomplish data fusion objectives obtain result worse achievable single sensor 
idea obtain kalman filter fused estimate biased accurate measurements unaffected inaccurate ones 
filter responsiveness measurements adjusted measurement error covariance matrix eigenvalues particular matrix smaller corresponding measurement larger weight 
measure called ratio ar gives value degree confidence associated th blob extracted time sensor ar bs bs difference map obtained absolute difference current image normalization constant depending number color tones image 
ar real number ranging gives estimate level performance sensor extracted blob 
seen ar values reported bounding boxes blobs extracted infrared sensor considerably higher extracted optical 
ar values regulate measurement error covariance matrix weight position data fusion process 
function position measurement error developed gd ar gd gating distance 
function adjust measurement position error map positions calculated blobs high ar values trusted measurement error position close zero blobs poorly detected low ar value trusted measurement error equals gating distance 
results experiments real video sequences carried order test performance proposed approach 
images taken experiment reported 
color cameras employed follow movements persons walking 
daylight outdoor scene simple vision tracking problem purpose evaluate accuracy trajectories tracking 
trajectories calculated single sensors compared ground truth markers ground fusion approach 
row shows images taken sensor superior quality second camera proved effective detecting walking persons 
sensor monitoring area configuration optics wide angled second sensor detecting smaller blobs performed slightly better 
reflected ar values blobs second row generally greater fourth row 
experiment sensors performed reasonably 
shows trajectory persons indicated arrow sensor reports trajectory obtained second camera 
sensors reporting track similar ground truth black lines 
better result obtained data fusion 
advantages trajectory exploits estimates just sensor giving readings target field view column person left rows field view second sensor rows presence points view help disambiguate situations partial total occlusions second column maintaining correct continuous tracking targets 
notice ar value computed blob detected sensor recognized compound object generated occlusion associated objects previous time instant explicit weighting estimates fusion process ar account segmentation errors 
segmentation errors translate trajectory errors 
third column person center scene half concealed small tree second sensor giving proper detection gets low ar score blob 
data fusion reduces camera calibration errors due transformation image pixels fig 
images blobs sequence 
ar values indicated blob 
map points section 
sensor gives better segmentation results due wide angle setup optics camera calibration errors probable 
fused data weighted sensor due better video performance takes account second sensor suffers calibration errors 
fig 
trajectory color sensor 
performances sensors data fusion summarized table reported mean standard deviation distance pixels pixel cm measured ground truth positions map walking person indicated arrow fig 
trajectory second color sensor 
fig 
trajectory obtained data fusion 
fig 
images blobs fog sequence 
ar values indicated blob 

seen color sensors performing similarly fusing estimates allows reduction calibration error trajectory similar ground truth 
mean color sensor second color sensor data fusion table mean standard deviation pixels distance estimated ground truth positions sequence 
images taken second experiment shown 
video sequences taken night dense banks fog 
scene ir rays monitored color camera camera near infrared response 
seen ir sensor outperforms color camera ar values blob corresponding ir sensor consistently higher 
directly reflected correct segmentation silhouette person 
shows plot ar values scored sensors blob 
noted color camera able discriminate person moves away fog bank ar values indicated graph blob detected 
experiment shows ar values automatically dynamically select best sensors available 
threshold set sensor extracts blob ar value threshold estimate considered fusion process considered unreliable discarded 
case ar values color camera threshold contributed generate final trajectory fig 
ar values fog sequence entirely formed ir estimates 
trajectory computed system plotted dots trajectory plotted black denotes ground truth path covered walking person 
images represent trajectory computed optical ir data respectively 
fig 
trajectory person left color right ir camera 
seen comparing images results obtained color camera poor trajectory discontinuous affected segmentation errors 
right shows trajectory computed ir video signal 
expected trajectory continuous close ground truth 
target temporarily lost top right traverses dense fog bank 
table reports mean standard deviation distance pixels pixel cm measured ground truth positions th map walking person sensors 
notice ir camera mean ir sensor color sensor table mean standard deviation pixels distance estimated ground truth positions fog sequence 
clearly performing better 
extreme experiment shown demonstrate ar values dynamically evaluate performance sensors 
extremely important surveillance system outdoors weather illumination conditions vary continuously sensors respond differently variations 
exploiting ar values evaluate performance sensors allows choose performing better time instant 
ultimately leads obtaining accurate target detection trajectory estimation 
achieving better trajectory accuracy continuity paramount importance successive steps behavior understanding performed surveillance system 
particular trajectories objects scene analyzed detect suspicious events 
system fact trained discriminate patterns generated normal activities moving objects monitored space anomalous suspicious movements 
sensor reliability explicitly considered multi camera system video surveillance outdoor environments 
confidence measure defined automatically weight redundant measurements targets location coming different sensors data fusion process 
way localization errors due incorrect segmentation blobs reduced calibration errors due perspective transformations 
preliminary experimental results show effectiveness chosen confidence measure automatic sensor weighting greater accuracy achievable proposed data fusion approach comparison single camera systems 
particular fusion procedure produced trajectories continuous useful surveillance system 
acknowledgments partially supported italian ministry university scientific research framework project distributed systems multi sensor recognition augmented perception ambient security customization 
ramesh 
special issue video communications processing understanding third generation surveillance systems 
proceedings ieee 
collins lipton kanade 
special section video surveillance 
ieee transactions pattern analysis machine intelligence august 

multimedia video surveillance systems user requirements research solutions 
kluwer academic publishers 
varshney 
multisensor surveillance systems fusion perspective 
kluwer academic publisher 
iyengar kashyap 
information integration synchronization distributed sensor 
ieee transactions system man cybernetics sept oct 

data fusion large multiagent networks analysis network structure performance 
proceedings international conference multisensor fusion integration intelligent systems mfi pages october 
qi iyengar 
multiresolution data integration mobile agents distributed sensor networks 
ieee transactions systems man cybernetics part applications reviews 

real time detection multiple moving objects complex image sequences 
international journal imaging systems technology 

object detection tracking time varying badly illuminated outdoor environments 
optical engineering 
bar shalom li 
multitarget multisensor tracking principles techniques 
publishing 
hall 
multisensor data fusion 
proceedings ieee january 
bar shalom blair eds 
multitarget multisensor tracking applications advances volume iii 
artech house 

multilevel fusion approach object identification outdoor road scenes 
international journal pattern recognition artificial intelligence 
mori ohya yachida 
tracking multiple humans 
proceedings th icpr pages 
kato inokuchi 
human tracking distributed vision systems 
proceedings th icpr pages 
ishiguro trivedi 
real time target localization tracking ocular stereo 
ieee workshop omnidirectional vision pages 
takashi matsuyama 
real time multitarget tracking cooperative vision system 
proceedings ieee 
hall 
pitfalls data fusion avoid 
proceedings fusion july 
jain 
illumination independent change detection real world image sequences 
computer vision graphics image processing 
rosin ellis 
image difference threshold strategies shadow detection 
proceedings th british machine vision conference pages 
bmva press 

object recognition tracking remote video surveillance 
ieee transaction circuits systems video technology 
gian luca 
real time detection multiple moving objects complex image sequences 
international journal imaging systems technology 
collins lipton kanade :10.1.1.33.8388
system video surveillance monitoring 
proceedings ieee 

real time thresholding euler numbers 
pattern recognition letters june 
comaniciu ramesh meer 
kernel object tracking 
ieee trans 
pattern analysis machine intelligence 
tsai 
versatile camera calibration technique machine vision metrology shelf tv cameras lenses 
ieee journal robotics automation 
faugeras luong maybank 
camera theory experiments 
proceedings european conference computer vision eccv pages 
santini jain 
tracking objects multiple camera views 
proceedings january 

multiple target tracking radar applications 
artech house 

multi dimensional formulation data association problems arising multi target multi sensor tracking 
computational optimization applications 
bar shalom campo 
effects common process noise sensor fused track covariance 
ieee transactions aerospace electronic systems aes 
bar shalom 
tracking data association 
academic press 
gao harris 
remarks kalman filters multisensor fusion 
information fusion 
