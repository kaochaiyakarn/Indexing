bellman goes relational kristian kersting kersting informatik uni freiburg de university freiburg machine learning lab georges koehler freiburg germany martijn van cs utwente nl university freiburg machine learning lab georges koehler freiburg germany twente university department computer science box ae enschede netherlands luc de raedt informatik uni freiburg de university freiburg machine learning lab georges koehler freiburg germany motivated interest relational reinforcement learning introduce novel relational bellman update operator called rebel 
employs constraint logic programming language compactly represent markov decision processes relational domains 
rebel novel value iteration algorithm developed abstraction states actions plays major role 
framework provides new insights relational reinforcement learning 
convergence results experiments 

lot attention progress reinforcement learning rl markov decision processes mdps 
basic algorithms proposed behavior relatively understood today sutton barto 
led increased interest effects generalization new challenges 
concerns rl relational domains zeroski 
number relational rl algorithms developed essentially varying underlying function approximators driessens ramon rtner problem relational rl understood theory relational rl lacking 
appearing proceedings st international conference machine learning banff canada 
copyright authors 
traditional rl bellman backup operator central concepts 
particularly interesting approach dietterich flann showed value backups model rl upgraded region backups multiple states updated simultaneously backup operator reverses action operators 
inspired key contribution relational bellman backup operator called rebel 
rebel developed simple probabilistic strips relational formalism incorporates elements relational logical markov decision programming kersting de raedt van states represented relational queries 
rebel develop model relational rl algorithm demonstrate number experiments 
approach related boutilier 
employ situation calculus language 
certainly elegant principled due complexity language report complete implementation automated experiments 
contrast approach simpler fully automated 
deals fully automatically experimental example boutilier report 
outline section briefly reviews relational logic mdps 
discussing value iteration vi mdps section introduce language compactly specify mdps relational domains section 
section develop relational vi algorithm rebel 
empirically validated section 
concluding discuss related 
authors contributed equally 

preliminaries relational logic cf 
cheng de wolf alphabet set relation symbols arity set constants atom 
tm relation symbol followed bracketed tuple terms ti 
term variable constant conjunction set atoms 
set variables conjunction denoted vars 
substitution set assignments terms variables 
xn tn xi variables ti terms 
term atom conjunction called ground contains variables 
conjunctions implicitly assumed existentially quantified 
conjunction said subsumed conjunction denoted exists substitution general unifier mgu atoms denoted mgu 
horn clause consists positive atom conjunction read true true 
greatest lower bound glb conjunctions general conjunction subsumed subsumption glb defined clauses 
herbrand base hb set ground atoms constructed predicate symbols constants 
interpretation subset hb running example blocks world 
block moved top block denoted move 
valid relations block cl block clear 
model floor follow common approach 
set blocks top blocks 
markov decision processes cf 
sutton barto markov decision process mdp tuple set states set actions transition model reward model 
set actions applicable state denoted 
transition state caused action occurs probability reward received 
defines proper probability distribution states actions 
deterministic policy specifies action executed agent state 
value iteration mdp policy discount factor state value function represents value state policy expected rewards 
similar state action value function defined 
policy optimal optimal value functions denoted bellman optimality equa tion states max equation basically methods solving mdps derived 
example known exact solution technique value iteration vi obtained turning update rule vt max vt max qt 
equation vi algorithm stated follows starting value function states iteratively update value state get value functions vt 
vi guaranteed converge limit bellman optimality equation holds state 
traditional vi expressed equation assumes states values represented explicitly table 
impractical smallest state spaces 
furthermore relational domains number states grow large infinitely large infeasible 
methods specific states needed 
method developed sections 

markov decision programs traditional mdps essentially propositional state represented separate proposition 
markov decision programs propositional symbols replaced states definition state conjunction logical atoms logical query 
states represent sets states 
formally state interpretation set grounds facts 
consider state cl cl blocks world 
state cl 
represents states subsumed interpretations exists clear 
introduce basic ingredients markov decision programs actions rewards integrity constraints 
action defined follows 
definition action finite set action rules pi hi atom representing name arguments action state denoting preconditions hi th possible outcome holds pi 
assume vars vars hi vars 
semantics action definition current state subsumed action result hi probability pi 
preconditions outcomes possible 
illustration consider cl cl cl cl move move cl cl cl cl moves block probability 
probability action fails change state 
applied state action tells move result cl cl probability probability stay type action definition implements kind probabilistic strips operator 
model rewards specifies rewards generated entering states 
framework coincides initial state value function 
definition state value function finite list value rules form state state assigns maximal value matching value rules value 
rule matches consider 
true 
assigns true value rule assures state assigned value 
develop rebel employ action state value functions similar state value functions example section 
definition state action value function finite set rules form state action sake simplicity consider cost free actions 
framework adapted case action costs 
note meaning action differs context hierarchical rl 
state action pair assigns maximal value state action rules subsumed rewards specified queries existentially quantified goals 
simple expressive specify interesting problems studied relational rl community shortest path problems 
goal reach certain states 
goal state entered process ends 
rl episodic tasks encoded absorbing states 
encode artificial deter actions absorbing denotes states subsumed transition generate zero rewards 
example absorbing need way cope integrity constraints imposed domain 
instance move definition employed symmetry 
modeled set integrity constraints 
integrity constraint horn clause 
instance blocks world block free block top block false cl 
completion state fixpoint facts deducible 
example encode rules state completed furthermore completion includes false state satisfy constraints illegal state 
deal integrity constraints adapt notations action definitions generality 
action definitions constrained lead illegal states 
subsumption employ integrity constraints background theory buntine generalized subsumption framework buntine 
lines kersting de raedt van proven markov decision program induces possibly infinite mdp 

relational value iteration rebel develop value iteration algorithm markov decision programs reward model initial state value function compute state value functions vt 
main idea upgrade bellman traditional backup operator equation 
iterate regress preceding states vt compute qt regressed states 
compute vt maximizing qt 
discuss step turn 

regression vt current state value function say consider action move 
single bellman backup states lead condition action move computed 
reason preconditions 
example outcome move lead state cl cl inequality constraints omitted state 
compute weakest preconditions outcomes move definition states lead pi action rule hi constitute called weakest precondition wpi th outcome example lies weakest precondition wp move lie wp move compute wp move assume moved preconditions action rule partially caused outcome action 
illustration consider move caused state cl cl moved move cause moved states cl cl satisfying move move away 
constraints guarantee applying move preserves 
definition simplifies si completed state cl cl variables constants mutually different 
states logically define wp move far considered single effect 
general multiple initialize wp empty list subset subset hi mgu exists hi pairs hi mgu exists add add simplifications wp return wp procedure returns weakest pre pi condition action rule hi state set integrity constraints omitted legal completed states inserted wpi 
combined effects caused action move cf 
procedure 
consider example moving block block caused cf 
line 
assume effect caused 
empty cf 
line 
empty substitution cl cl inequality constraints omitted possible preimage cf 
line 
know move cause 
holds cf 
lines 
simplified instance legal state 
case action caused effects covered mgu exists line 
treated analogously 

computing state action values regressed states current state value function vt compute state action value function qt procedure 
treat outcome action single action compute state action value cf 
line 
combine values outcomes state action value cf 
lines 
sake brevity state constraints examples till section 
step consider outcome move 
weakest precondition wp move 
absorbing assign state action value action move move value dependent vt example 
assuming discount factor yields move initialize empty set 
pi action rule hi vt wp pi vt absorbing pairs glb exists add return procedure returns rules action reward model current value function vt discount factor 
note denotes action head keep substitution wpi 
omitted legal completed states inserted 
doing rules results move cl cl move cl cl move cl cl second outcome move step leads move cl cl move cl cl move cl cl step note rules describes situations state get value achieving th outcome action information combined state action values select rule say rule say check states time apply action 
words compute greatest lower bound glb logical clauses underlying value rules 
glb actions unify exists legal state inserted new rule cf 
line 
value new rule sum values combined rules 
yields move cl cl 
contrast give new rule 
blocks world example yields state action value function applied move absorbing initialize vt empty set rules 
sort decreasing order values empty remove top element rule exists subsumes add vt remove rules subsumed return vt procedure returns value functions vt rules computed vt actions 
absorbing move cl cl move cl cl move cl cl note sorted rules descending order sake readability 

computing state values set rules enables compute state value function vt 
contrast traditional case rules values state action pairs overlap rules compute state values fact vt maxa qt due eq 

general value preserving transformation applied 
simple separate andconquer rule learning approach rules learn examples learn coincide see procedure 
search rule having maximal value lines separate covered rules line recursively conquer remaining rules selecting rules rules remain line 
main difference select add vt rule left value body subsumes body cf 
line 
running example start rule subsumed rule having value add subsumes remove 
remaining highest valued rule iterate 
completing yields new value function constraints listed cl cl cl cl 
relational bellman backup operator summarize general scheme rebel compute weakest precondition action values xk xi 
fi xi xj 
xi fj 

blocks world experiment state value function cl goal iterations 
applies number blocks 
values rounded second digit 
fi block floor block 
states structurally different depicted ones get value outcome state vt weakest pre 
done assign state action outcome pair computed value combine glb 
maximize rules compute vt 
note outcomes action values th outcome combined combined values previous outcomes 
combinations action 
produces rules 
overcome adapt maximizing rules compress rules state different currently combined values compatible actions select higher 
safe higher valued rule subsumes lower valued 
selected case 
formally bellman backup requires infinite number iterations converge cf 
section 
practice value function changes small amount 

experiments section empirically validate rebel 
implemented rebel compressing prolog system yap version 
supplemented constraint handling rules library fr 
experiments assume discount factor goal reward states receive reward 
goal states absorbing 
experiments run ghz linux machine 
running times estimated yap statistics runtime 
focused standard 
values 



blocks world experiment ii parts value function iterations values rounded second digit 
applies number blocks 
omitted inequality constraints blocks mutually different 
fi block floor block 
state steps away goal get value 
examples known relational rl literature 
blocks world experiment consider cl goal probabilistic blocks world setting 
experiment shows simple problems rebel guaranteed converge structural level 
shows state value function iterations 
took rebel roughly minute iterate times 
highlights states step away goal get value 
value lower additional block top stack number blocks restricted value iteration 
proposition abstraction guarantee convergence infinite domains infinite number states required 
interesting infinite state spaces easily arise relational representations relational abstraction hoped solution 
relational value iteration converge infinite domains third experiment show 
blocks world experiment ii consider goal deterministic blocks world reported hard problem model free relational rl rrl approaches zeroski driessens ramon 
instance driessens ramon report average learned policies reach optimal performance blocks 
experimental set experiment deterministic move action rebel 
vt states bin 
tin rain 
tin rain 
tin rain 
tin rain 
tin bin rain 
tin bin rain 
tin bin rain 
tin bin rain 
tin 
table 
load unload experiment th column shows state value function th iteration 
value state value 
bold numbers highlight changed values 
computed minutes 
value function partially shown 
move action deterministic optimal blocks ground states 
optimal policy directly extracted computing maximizing rules state 
example results removing top elements stacks top compactly represent strategy needs define predicate 
experiments driessens ramon reported case 
policy rebel optimal matter blocks 
load unload experiment final experiment considers logistics domain boutilier 
solved semi automatically 
domain consists cities trucks boxes 
boxes loaded unloaded trucks trucks driven cities 
predicate denotes box truck bin denotes box city tin denotes truck city actions performed load unload specifying box loaded loaded truck drive specifying truck driven city actions domain probabilistic effects 
probability failing load unload action staying current state depends rains denoted rain 
action specification follows omit failing specifications sake brevity bin tin pr unload tin tin pr load bin tin tin drive tin probability pr rain rain 
correctly handle explicit negation rain provided false rain rain constraint 
goal domain get box stands paris bin get reward 
rebel ran seconds compute results summarized table 
contrast blocks world examples solution converges value level structural level 
take situation truck city different paris box 
take steps load drive unload reach goal state state value case rains 
state value function applies matter trucks boxes cities 

related past years increased significant interest rich relational representations modeling learning mdps 
model free relational rl studied different relational learners function approximation zeroski driessens ramon rtner 
applied learning pre specified state spaces kersting de raedt investigate pure learning van learns function learning underlying transition model 
fern 
extended previous upgrading learned policies small relational mdps approximated policy iteration 
guestrin 
reported class approximate value functions 
model approaches surprising lack research exact solution methods 
general point view rebel closely related decision theoretic regression dtr boutilier related regression planning way dtr dtr algorithms designed propositional representations 
exception authors know boutilier 

rebel relates model exact solution method 
key difference boutilier employ situation calculus representing 
situation calculus expressive consequence harder simplify logical descriptions value functions states obtained 
may explain best authors knowledge approach fully implemented experimented 
contrast simpler logical language simplification rebel computationally feasible 
shown experiments rebel successfully fully automatically implements relational value iteration 
dietterich flann concerned generalizing bellman backups relational representation 

key contribution rebel relational upgrade bellman update operator 
implement relational value iteration algorithm 
shown effective number simple significant examples 
turn led number novel insights relational mdps 
shown value methods relational mdps may converge infinite number states represented 
second highlighted cases background knowledge may enable learning optimal policies 
depending representation problem learn optimal policy 
background knowledge interesting feature cases necessity successful learning 
way explanation confirmed experimental insights early relational rl zeroski 
address combining rebel types value methods extending representation language efficient data structures complexity analysis employing learning algorithms compress value functions 
authors hope theoretical insights algorithm developed helpful advancing field relational rl contribute improved understanding problems involved 
authors anonymous reviewers helpful comments 
research supported european union ist programme contract 
fp april ii 
martijn van ot supported marie curie fellowship daisy ct 
bellman 

dynamic programming 
princeton new jersey princeton university press 
boutilier dean hanks 

decisiontheoretic planning structural assumptions computational leverage 
art 
intel 
res 
boutilier reiter price 

symbolic dynamic programming order mdp proc 
ijcai 
buntine 

generalized applications induction redundancy 
artificial intelligence 
dietterich flann 

learning reinforcement learning unified view 
machine learning 
driessens ramon 

relational instance regression relational reinforcement learning 
proc 
icml 
zeroski de raedt driessens 

relational reinforcement learning 
machine learning 
fern yoon givan 

approximate policy iteration policy language bias 
proc 
nips 
fr 

theory practice constraint handling rules 
journal logic programming 
rtner driessens ramon 

graph kernels gaussian processes relational reinforcement learning 
proc 
ilp 
guestrin koller 

generalizing plans new environments relational mdps 
proc 
ijcai 
kersting de raedt 

logical markov decision programs 
proc 
ijcai workshop learning statistical models relational data 


learning optimal dialogue management rules reinforcement learning inductive logic programming 
proc 
north american chapter association computational linguistics naacl 
pittsburgh 
cheng de wolf 

foundations inductive logic programming vol 
lecture notes artificial intelligence 
springer verlag 
sutton barto 

reinforcement learning 
cambridge mit press 
van 

reinforcement learning relational mdps 
machine learning conference belgium netherlands 
