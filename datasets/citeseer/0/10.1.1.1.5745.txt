integrating constraints metric learning semi supervised clustering mikhail bilenko cs utexas edu basu cs utexas edu raymond mooney mooney cs utexas edu department computer sciences university texas austin austin tx usa semi supervised clustering employs small amount labeled data aid unsupervised learning 
previous area utilized supervised data approaches constraint methods guide clustering algorithm better grouping data distance function learning methods adapt underlying similarity metric clustering algorithm 
provides new methods approaches presents new semi supervised clustering algorithm integrates techniques uniform principled framework 
experimental results demonstrate unified approach produces better clusters individual approaches previously proposed semisupervised clustering algorithms 

learning tasks unlabeled data plentiful labeled data limited expensive generate 
consequently semi supervised learning employs labeled unlabeled data topic significant interest 
specifically semi supervised clustering class labels pairwise constraints examples aid unsupervised clustering focus projects wagstaff basu klein xing bar hillel segal 
existing methods semi supervised clustering fall general approaches call constraint metric 
constraint approaches clustering algorithm modified user provided labels pairwise constraints guide algorithm appropriate data partitioning 
done modifying clustering objective function includes satisfaction constraints demiriz appearing proceedings st international conference machine learning banff canada 
copyright authors 
enforcing constraints clustering process wagstaff initializing constraining clustering labeled examples basu 
metric approaches existing clustering algorithm uses distance metric employed metric trained satisfy labels constraints supervised data 
distance measures metric semi supervised clustering including euclidean distance trained shortest path algorithm klein string edit distance learned expectation maximization em bilenko mooney kl divergence adapted gradient descent cohn mahalanobis distances trained convex optimization xing bar hillel 
previous metric semi supervised clustering algorithms exclude unlabeled data metric training step separate metric learning clustering process 
existing metric methods single distance metric clusters forcing similar shapes 
propose new semi supervised clustering algorithm derived means mpck means incorporates metric learning pairwise constraints principled manner 
mpck means performs distance metric training clustering iteration utilizing unlabeled data pairwise constraints 
algorithm able learn individual metrics cluster permits clusters different shapes 
mpck means allows violation constraints leads cohesive clustering earlier constraint methods forced satisfaction constraints leaving vulnerable noisy supervision 
metric constraint components unified method experimental results comparing combining approaches multiple datasets 
methods semi supervision individually improve clustering accuracy unified approach integrates strengths 
demonstrate semi supervised metric learning approach outperforms previously proposed methods learn metrics prior clustering learning multiple metrics lead better results 

problem formulation 
clustering means means clustering algorithm iterative relocation partitions dataset clusters locally minimizing total squared euclidean distance data points cluster centroids 
xi xi set data points xid th component xi represent cluster centroids li cluster assignment point xi li 

euclidean means algorithm creates partitioning xh objective function xi xi li locally minimized 
shown means algorithm essentially em algorithm mixture gaussians assumptions identity covariance gaussians uniform mixture component priors expectation particular type conditional distribution basu 
euclidean means formulation squared norm xi li xi li xi li point xi corresponding cluster centroid li distance measure direct consequence identity covariance assumption underlying gaussians 

semi supervised clustering constraints semi supervised clustering small amount labeled data available aid clustering process 
framework uses link link constraints pairs instances wagstaff associated cost violating constraint 
unsupervised learning applications clustering speaker identification conversation bar hillel clustering gps data lane finding wagstaff considering supervision form constraints realistic providing class labels 
class labels may unknown user specify pairs points belong different clusters 
constraint supervision general class labels set classified points implies equivalent set pairwise constraints vice versa 
means directly handle pairwise constraints formulate goal pairwise constrained clustering minimizing combined objective function defined sum total squared distances points cluster centroids cost incurred violating pairwise constraints 
set link pairs xi xj implies xi xj cluster set link pairs xi xj implies xi xj different clusters 
wij wij penalty costs violating constraints respectively 
goal pairwise constrained means minimize objective function point xi assigned partition xli centroid li xi li wij li lj xi xj wij li lj xi xj indicator function true false 
mathematical formulation motivated metric labeling problem generalized potts model kleinberg tardos 

semi supervised clustering metric learning pairwise constraints guide clustering algorithm better grouping adapt underlying distance metric 
pairwise constraints effectively represent user view similarity domain 
original data representation may specify space clusters sufficiently separated modifying distance metric warps space minimize distances cluster objects maximizing distances different cluster objects 
result clusters discovered learned metrics adhere closely notion similarity embodied supervision 
parameterize euclidean distance symmetric positive definite matrix follows xi xj xi li xi li parameterization previously xing 
bar hillel 

restricted diagonal matrix scales dimension different weight corresponds feature weighting new features created linear combinations original ones 
previous adaptive metrics clustering cohn xing bar hillel metric weights trained simultaneously minimize distance linked instances maximize distance linked instances 
fundamental limitation approaches assume single metric clusters preventing having different shapes 
allow separate weight matrix cluster denoted ah cluster equivalent generalized version means model described section cluster generated gaussian covariance matrix bilmes 
shown maximizing complete data log likelihood generalized means model equivalent minimizing objective function xi li li log det second term arises due normalizing constant li th gaussian covariance matrix li 
integrating constraints metric learning combining eqns leads objective function minimizes cluster dispersion learned metrics reducing constraint violations xi li li log det wij li lj xi xj wij li lj xi xj assume uniform constraint costs wij wij constraint violations treated equally 
penalty violating link constraint distant points higher nearby points 
intuitively captures fact linked points far apart current metric metric grossly inadequate needs severe modification 
clusters involved link violation corresponding penalty affect metrics clusters 
accomplished multiplying penalty second summation eqn func tion fm xi xj xi xj li xi xj lj analogously penalty violating link constraint points nearby current metric higher distant points 
reflect intuition penalty term violated link constraints assigned cluster li lj fc xi xj li xi xj li maximally separated pair points li li dataset li th metric 
form fc ensures penalty violating link constraint remains non negative second term greater 
combined objective function xi li log det li xj xi li lj xj xi li lj costs wij wij provide way specifying relative importance labeled versus unlabeled data allowing individual constraint weights 
section describes greedily optimized proposed metric pairwise constrained means mpck means algorithm 

mpck means algorithm set data points set link constraints set link constraints corresponding cost sets desired number clusters mpck means finds disjoint partitioning xh cluster having centroid local weight matrix ah locally minimized 
algorithm integrates constraints metric learning 
constraints utilized cluster initialization assigning points clusters distance metric adapted re estimating weight matrices ah iteration current cluster assignments constraint violations 
pseudocode algorithm fig 
algorithm mpck means input set data points xi set link constraints xi xj set link constraints xi xj number clusters sets constraint costs output disjoint partitioning xh objective function locally minimized 
method 
initialize clusters 
create neighborhoods np 
initialize weighted farthest traversal starting largest np initialize centroids np initialize remaining clusters random 
repeat convergence 
assign cluster assign data point xi cluster set arg min xi log det ah ah xi xj xi xj lj xi xj xi xj lj 
estimate means 
update metrics ah xh xi xh xi xi xi xj mh wij xi xj xi xj li lj xi xj ch wij 

initialization xi xj xi xj li lj 
mpck means algorithm initial centroids critical success greedy clustering algorithms means 
infer initial clusters constraints take transitive closure link constraints augment set entailed constraints assuming consistency constraints 
number connected components augmented set connected components create neighborhood sets np neighborhood consists points connected links 
pair neighborhoods np np link add link constraints pair points np np augment link set entailed constraints 
overload notation point refer augmented link link sets respectively 
preprocessing step get neighborhood sets np 
neighborhoods provide initial clusters mpck means algorithm 
initialize cluster centers centroids neighborhood sets 
initialize remaining clusters points obtained random perturbations global centroid select neighborhood sets weighted variant farthest algorithm heuristic initialization centroid clustering algorithms means 
weighted farthest traversal goal find points maximally separated terms weighted distance 
case points centroids neighborhoods weight centroid size corresponding neighborhood 
bias farthest select centroids relatively far apart represent large neighborhoods order obtain initial clusters 
weighted farthest traversal maintain set traversed points step pick point having farthest weighted distance traversed set standard notion distance set miny 
initialize cluster centers centroids neighborhoods chosen weighted farthest traversal 

step mpck means alternates cluster assignment step centroid estimation metric learning step see step fig 
step point assigned cluster minimizes sum distance cluster centroid local metric cost constraint violations incurred cluster assignment 
points randomly re ordered assignment sequence point assigned cluster subsequent points random ordering current cluster assignment calculate possible constraint violations 
note assignment step order dependent subsets relevant cluster may change assignment point 
experimented random ordering greedy strategy assigned instances closest cluster centroid involved minimal number constraints 
experiments showed order assignment result statistically significant differences clustering quality random ordering evaluation 
step point moves new cluster component contributed point decreases 
points new assignment decrease remain 

step step cluster centroid re estimated points corresponding xh 
result contribution cluster minimized 
pairwise constraints take part centroid reestimation step constraint violations depend cluster assignments change step 
term distance component minimized 
centroid re estimation step effectively remains means 
second part step performs metric learning matrices ah re estimated decrease objective function 
updated matrix local weights ah obtained partial derivative setting zero resulting ah ah xh xi xi wij xi xj xi xj li lj wij xi xj xi xj li lj mh ch subsets link constraints respectively contain points currently assigned th cluster 
ah obtained inverting summation covariance matrices eqn summation singular 
obtained singular conditioned adding identity ma trix multiplied small fraction trace tr saul roweis 
ah resulting inversion negative definite projecting set positive semi definite matrices described xing 
ensure parameterizes distance metric 
high dimensional large datasets estimating full matrix ah computationally expensive 
cases diagonal weight matrices equivalent feature weighting full matrix corresponds feature generation 
case diagonal th diagonal element dd corresponds weight th feature th cluster metric dd xh xid hd wij xid xjd li lj wij hd hd xid xjd li lj intuitively term sum xi xid hd scales weight feature proportionately feature contribution cluster dispersion analogously scaling performed computing unsupervised mahalanobis distance 
terms depend constraint violations stretch dimension attempting current violations 
metric weights adjusted iteration way contribution different attributes distance variance normalized constraint violations minimized 
multiple metrics ah algorithm single metric clusters 
metric updated similarly description summations eqns xh mh ch respectively 
objective function decreases cluster assignment centroid re estimation metric learning step till convergence implying mpck means algorithm converge local minima long ma obtained directly eqn 
ah conditioned described positive definite maximally separated points change iterations convergence longer guaranteed theoretically empirically problem experience 

experiments 
methodology datasets experiments conducted datasets uci repository iris wine ionosphere blake merz protein dataset xing 
bar hillel 
randomly sampled subsets digits letters handwritten character recognition datasets uci repository 
digits letters chose sets classes letters digits sampling data points original datasets randomly 
classes chosen represent difficult visual discrimination problems 
table summarizes properties datasets number instances number dimensions number classes table 
datasets experimental evaluation iris wine ionosphere protein letters digits pairwise measure evaluate clustering results underlying classes 
measure relies traditional information retrieval measures adapted evaluating clustering considering cluster pairs recision recall measure recision recall recision recall generated learning curves fold cross validation dataset determine effect utilizing pairwise constraints 
point learning curve represents particular number randomly selected pairwise constraints input algorithm 
unit constraint costs constraints original inferred datasets provide individual weights constraints 
clustering algorithm run dataset pairwise measure calculated test set 
results averaged runs folds 

results discussion compared constraint metric semi supervised clustering integrated framework purely unsupervised supervised approaches 
figs show learning curves datasets 
dataset compared clustering schemes mpck means clustering involves seeding metric learning unified framework described section single metric parameterized diagonal matrix clusters mk means means clustering metric learning component described section utilizing constraints initialization single metric parameterized diagonal matrix clusters means clustering utilizes constraints seeding initial clusters directs cluster assignments respect constraints doing metric learning outlined section means unsupervised clustering supervised means performs assignment points nearest cluster centroids inferred constraints described section 
algorithm provides baseline performance pure supervised learning constraints 
datasets unified approach mpck means outperforms individual seeding means metric learning mk means 
superiority semisupervised unsupervised clustering illustrates providing pairwise constraints beneficial clustering quality 
improvements semi supervised clustering supervised means indicate iterative refinement measure measure mpck means mk means means means supervised means number constraints 
iris mpck means mk means means means supervised means number constraints 
ionosphere centroids constraints unlabeled data outperforms purely supervised assignment neighborhoods inferred constraints ionosphere mpck means requires full weight matrix individual cluster metrics outperform supervised means results experiments shown fig 
wine protein letter ijl datasets difference methods utilize metric learning mpck means mk means means regular means pairwise constraints indicates absence constraints weighting features variance essentially unsupervised mahalanobis distance improves clustering accuracy 
wine dataset additional constraints provide improvement cluster quality dataset shows meaningful feature weights obtained scaling variance just unlabeled data 
metric learning curves display characteristic dip clustering accuracy decreases initial constraints provided certain point starts increase eventually rises initial point learning curve 
conjecture phenomenon due fact metric parameters learned constraints unreliable significant number constraints required metric learning mechanism estimate parameters accurately 
hand seeding clusters small number pairwise constraints immediate positive effect measure measure mpck means mk means means means supervised means number constraints 
wine mpck means mk means means means supervised means number constraints 
digits measure measure mpck means mk means means means supervised means number constraints 
protein mpck means mk means means means supervised means number constraints 
letters ijl final cluster quality providing pairwise constraints diminishing returns means learning curves rise slowly 
seeding metric learning utilized unified approach benefits individual strengths methods seen mpck means results 
set experiments evaluated utility individual metrics cluster usefulness learning full weight matrix feature generation opposed diagonal matrix feature weighting 
compared methods rca semi supervised clustering algorithm performs metric learning separately clustering process bar hillel shown outperform similar approach xing 

figs show learning curves datasets clustering schemes mpck means mpck means figs involves seeding metric learning single metric parameterized diagonal matrix clusters mpck means involves seeding metric learning multiple metrics parameterized diagonal matrices mpck means involves seeding metric learning single metric parameterized full matrix clusters mpck means involves seeding metric learning multiple metrics parameterized full matrices measure measure mpck means mpck means mpck means mpck means rca number constraints 
iris metric learning mpck means mpck means mpck means mpck means rca number constraints 
ionosphere metric learning rca clustering uses distance metric learning described bar hillel initialization inferred constraints described section 
seen results full matrix parameterization individual metrics cluster lead significant improvements clustering quality 
relative usefulness techniques varies datasets multiple metrics particularly beneficial protein digits datasets switching diagonal full weight matrix leads large improvements wine ionosphere letters 
results explained fact relative success techniques depends properties particular dataset full weight matrix helps attributes highly correlated multiple metrics lead improvements clusters dataset different shapes lie different subspaces original space 
combination techniques helpful requirements satisfied iris digits observed visualizing datasets 
datasets multiple metrics full weight matrix lead maximum performance isolation 
comparing performance different variants mpck means rca see early learning curves pairwise constraints available rca leads better metrics mpck means 
training data provided ability mpck means learn supervised unsupervised data individual metrics allows measure measure mpck means mpck means mpck means mpck means rca number constraints 
wine metric learning mpck means mpck means mpck means mpck means rca number constraints 
digits metric learning measure measure mpck means mpck means mpck means mpck means rca number constraints 
protein metric learning mpck means mpck means mpck means mpck means rca number constraints 
letters ijl metric learning mpck means produce better clustering 
results indicate integrated approach utilizing pairwise constraints clustering individual metrics outperforms seeding metric learning individually leads improvements cluster quality 
extending basic approach full parameterization matrix individual metrics cluster lead significant improvements basic method 

related previous constrained pairwise clustering wagstaff 
proposed cop kmeans algorithm heuristically motivated objective function 
formulation hand underlying generative model hidden markov random fields see basu detailed analysis 
bansal 
proposed framework pairwise constrained clustering model performs clustering constraints formulation uses constraints underlying distance metric points clustering 
schultz joachims introduced method learning distance metric parameters relative comparisons 
unsupervised clustering proposed variant means incorporated learning individual euclidean metric weights cluster approach general allows metric learning utilize pairwise constraints unlabeled data 
semi supervised clustering pairwise constraints cohn 
gradient descent weighted jensen shannon divergence context em clustering 
xing 
utilized combination gradient descent iterative projections learn mahalanobis metric means clustering 
bar hillel 
proposed redundant component analysis rca algorithm uses link constraints learn mahalanobis metric convex optimization 
metric learning techniques clustering train single metric supervised data perform clustering unsupervised data 
contrast method integrates distance metric learning clustering process utilizes supervised unsupervised data learn multiple metrics experimentally leads improved results 
unified objective function semi supervised clustering constraints proposed segal 
incorporate distance metric learning 

mpck means new approach semi supervised clustering unifies previous constraint metric methods 
variation standard means clustering algorithm uses pairwise constraints unlabeled data constraining clustering learning distance metrics 
contrast previously proposed semi supervised clustering algorithms mpck means allows clusters lie different subspaces different shapes 
individual components integrated approach experimentally compared metric learning constraints isolation combined algorithm 
results shown unifying advantages techniques integrated approach outperforms techniques individually 
shown individual metrics different clusters performing feature generation full weight matrix contrast feature weighting diagonal weight matrix lead improvements basic algorithm 
extending approach high dimensional datasets euclidean distance performs poorly primary avenue research 
interesting topics include selection informative pairwise constraints facilitate accurate metric learning obtaining initial centroids methodology handling noisy constraints cluster initialization sensitive constraint costs 

acknowledgments anonymous reviewers joel insightful comments 
research supported part nsf iis iis faculty fellowship ibm bansal blum chawla 

correlation clustering 
proceedings rd ieee symposium foundations computer science focs pp 

bar hillel hertz weinshall 

learning distance functions equivalence relations 
proceedings th international conference machine learning icml pp 

basu banerjee mooney 

semi supervised clustering seeding 
proceedings th international conference machine learning icml pp 

basu bilenko mooney 

probabilistic framework semi supervised clustering 
submission available www cs utexas edu ml publication 
bilenko mooney 

adaptive duplicate detection learnable string similarity measures 
proceedings ninth acm sigkdd international conference knowledge discovery data mining kdd pp 

bilmes 

gentle tutorial em algorithm application parameter estimation gaussian mixture hidden markov models tech 
report icsi tr 
icsi 
blake merz 

uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
cohn caruana mccallum 

semi supervised clustering user feedback tech 
report tr 
cornell university 
demiriz bennett embrechts 

semisupervised clustering genetic algorithms 
artificial neural networks engineering annie pp 



locally adaptive techniques pattern classification 
doctoral dissertation university california riverside 
klein kamvar manning 

constraints space level constraints making prior knowledge data clustering 
proceedings nineteenth international conference machine learning icml pp 

kleinberg tardos 

approximation algorithms classification problems pairwise relationships metric labeling markov random fields 
proceedings th ieee symposium foundations computer science focs pp 

saul roweis 

think globally fit locally unsupervised learning low dimensional manifolds 
journal machine learning research 
segal wang koller 

discovering molecular pathways protein interaction gene expression data 
bioinformatics 
schultz joachims 

learning distance metric relative comparisons 
advances neural information processing systems 
wagstaff cardie rogers 

constrained means clustering background knowledge 
proceedings th international conference machine learning icml pp 

xing ng jordan russell 

distance metric learning application clustering 
advances neural information processing systems pp 

