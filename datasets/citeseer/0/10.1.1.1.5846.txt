error estimation model selection von dipl inform 
tobias scheffer vom fachbereich informatik der universitat berlin zur des grades der naturwissenschaften dr 
nat 
dissertation prof dr stefan prof dr fritz wysotzki prof dr claus tag der wissenschaftlichen 
mai berlin mai acknowledgment wish helped studies contributed thesis 
particular wish blanz supported attempt obtain ernst von siemens fellowship hosted princeton 
paul compton hosting sydney helpful reviews earlier papers 
chris darken enjoyed working princeton 
enjoyed working russ greiner learned lot conducting scientific research 
ralf herbrich contributions chapter 
achim hoffmann agnostic view machine learning lead deeper insights learning theory 
achim contributed chapter 
hans peter jacobs installing laptop uploading games contributing implementation experiments 
thorsten joachims helped equations behave key contributions chapter 
morik hosting dortmund 
frank neumann help computer contributions implementation experiments 
appreciate ramesh lesson engineering 
alberto helped edit ijcai taught lesson english punctuation 
arun sharma striving scientific excellence source inspiration enjoyable discussions 
enjoyed working frank stephan complexity boosting 
claus interesting discussion expected error analysis carefully proof reading proofs 
advisor fritz wysotzki giving full autonomy phd project detailed comments thesis 
reported supported ernst von siemens fellowship partially supported wy wy german research council dfg travel german research association 
machine learning algorithms search space possible hypotheses estimate error hypotheses sample 
goal classification tasks find hypothesis low true generalization misclassification probability error rate sample empirical error rate measured minimized 
true error rate returned hypothesis unknown instance estimated cross validation general worst case bounds 
doctoral dissertation addresses compound questions error assessment intimately related selection hypothesis language learning algorithm problem 
part thesis new analysis generalization error hypothesis minimizes empirical error finite hypothesis language 
solution characterizes generalization error apparently best hypothesis terms distribution error rates hypotheses hypothesis language 
distribution error rates problem estimated efficiently sample 
effectively analysis predicts outcome learning algorithm learning algorithm having invoked 
immediately leads efficient algorithm selection hypothesis language model 
analysis predicts explains shape learning curves high accuracy contributes better understanding nature fitting 
study behavior model selection algorithm empirically particular comparison cross validation artificial problems large scale text categorization problem 
step study situations performing automatic model selection beneficial particular study occam algorithms cross validation 
model selection techniques tree pruning weight decay cross validation employed virtually practical learners generally believed enhance performance learning algorithms 
show belief equivalent assumption distribution problems learning algorithm exposed 
specify distributional assumptions quantify benefit occam algorithms cross validations situations 
distributional assumptions fail cross validation model selection increases generalization error returned hypothesis average 
distinct learners assessed respect particular problem learner assessed repeatedly distinct parameter settings effect arises similar overfitting occurs error minimization processes 
lowest observed error rate optimistic estimate corresponding generalization error 
quantify bias 
particular study bias imposed repeated invocations learner distinct parameter settings fold cross validation estimate error rate 
pursue information theoretic approach require assumption empirical error rates measured distinct cross validation folds independent estimates 
discuss implications results results empirical studies carried past propose experimental setting leads unbiased results 
address complexity issues model selection 
model selection learning learning algorithm restricted small model chosen model selection algorithm 
contrast boosting setting hypothesis allowed grow dynamically hypothesis fitted data 
giving new worst case time bounds adaboost algorithm show cases restriction small sets hypotheses causes high complexity learning contents machine learning 
need bias learning 
model selection 
applications machine learning 
principle contributions 
organization 
preliminaries terminology book 
models generalization 
gold framework learning 
pac vc models generalization 
relationship pac gold framework 
bayesian framework 
links pac vc bayesian learning 
free lunch 
model selection 
occam algorithms 
complexity penalization 
cross validation 
empirical methodology machine learning 
summary 
expected error analysis overview framework 
solution independent error values 
general solution 
estimating fhg ed jh dxy 
fitting 
robustness inaccurate estimates jh 
empirical studies 
artificial problem 
learning boolean decision trees 
scaling text categorization 
discussion 
contents summary 
assumptions justify model selection bounds performance model selection 
occam algorithms 
cross validation 
case study boolean functions 
discussion related results 
summary 
assessment learning algorithms chernoff bounds 
information theoretic approach 
parameter adjustment 
shot training test 
affected benchmark problems 
fold cross validation parameter adjustment 
affected benchmark problems 
fold cross validation fixed parameters 
affected benchmark problems 
unbiased assessment 
discussion 
summary 
complexity issues boosting 
definitions 
worst case bound adaboost perceptrons 
boosting decision stumps 
discussion related 
summary 
expected error analysis 
error rate intrinsic property hypothesis language 
model selection 
applicability learning algorithms 
appendix proof theorem 
efficient implementation theorem 
proof theorem 
proof theorem 
efficient implementation theorem 
proof theorem 
expected learning curve boolean functions 
contents boolean functions attributes 
expected learning curve boolean functions attributes 
notation 
bibliography chapter chapter provides informal supervised machine learning overview doctoral dissertation 
particular pac theory bayesian learning free lunch theorems principle idea model selection discussed intuitive level 
machine learning learning studied disciplines definitions try capture intuition consider learning 
attempts define learning simon 
simon defines learning modification behavior system leads improvement respect repeated performance task 
definition may appear overly general includes instance physical modification system 
specifically michalski 
define learning construction modification representations experience 
categories learning processes distinguished psychology statistics computer science 
learning settings differ task student accomplish information provided teacher 
concept formation setting student learner provided set positive additionally set negative examples concept 
identify target concept data 
classification differs slightly concept formation 
learner discriminate finitely classes 
regression studied statistics learner identify target function sample points just discriminate finitely classes 
language acquisition somewhat different meaning learning theory psychology gold mathematical model gold language learning considered problem finding grammar identifies language syntactically discriminates sentences complementary language acquisition semantic disregarded 
skill acquisition setting learner able perceive parameters system select control action performance criterion maximized 
order acquire control policy learner conduct control experiments setting referred reinforcement learning sutton barto observe perfect operator controlling system referred behavior cloning sammut 
thesis focus mathematical models try capture intuition learning 
models thesis committed classification detailed overview various fields machine learning mitchell 
assume unknown target function learner guess examples 
domain target function generally referred instances elements finite domain called class labels 
typically learner finite sequence input output pairs chapter 
sample 
various models differ success criteria gold model identification limit gold requires learner identifies target concept exactly certainty unbounded time valiant framework probably approximately correct learning valiant requires learner find high probability hypothesis incurs error batch examples preferably polynomial size error measured terms misclassification probability new instances 
notation misclassification probability error rate hypotheses implies existence unknown underlying distribution instances 
error rate chance hypothesis making false prediction instance drawn distribution 
setting nontrivial learner able perceive error incurred sample success criterion refers actual target function unknown learner 
way pac theory argues bound true generalization error rate hypotheses chance hypothesis error giving correct answer new instance gamma 
chance hypothesis classifying sequence instances correctly true error gamma provided examples independent identically distributed 
suppose learner searches finite hypothesis space returns arbitrary hypothesis consistent sample size worst case jhj hypotheses incur error hypothesis having chance gamma consistent sample 
chance jhj hypotheses incurs true error consistent sample jhj gamma employ worst possible learner return hypothesis highest true error consistent sample 
learner returns hypothesis consistent sample size claim probability gamma jhj gamma true error 
simple result gives intuition pac theory allows statistical claims true error hypothesis empirical error known 
result demonstrates knowledge generalization error learned hypothesis decreases size hypothesis language hypothesis learned 
consider hypothesis space single hypothesis empirical error unbiased estimate true error chance empirical error optimistic estimate just large chance pessimistic estimate 
unbiased means expectation empirical error rate hypothesis true error rate 
consider hypothesis space hypotheses certainly empirical error greater true error empirical error true error 
choose hypothesis low empirical error select optimistically estimated error 
distinct hypotheses space say true error hypothesis consistent sample 
result misinterpreted meaning generalization error returned hypothesis increases hypothesis language learned grows 
bayesian learning berger learner assumed extra information compared pac learner 
required perform target function set possible functions averages error rate bayesian learner possible target functions known prior probability target functions 
bayesian learning easier pac learning sense hurt learner performs poorly target functions learner knows prior probability target functions 
hypothesis minimizes expected error general loss called bayes hypothesis 
bayes hypothesis prediction instance weighted majority possible functions weights posterior probabilities js function having generated observed sample bayes rule posterior 
need bias learning works sjf sjf determined assumed known cases derived sjf 
situation optimal decision minimizes expected misclassification probability classification learning 
known summing possible tractable determine map maximum posteriori hypothesis maximizes posterior probability js having generated sample maximizing sjf constant learning problem 
unfortunately assuming prior known environment entirely realistic 
robust bayesian learners berger guaranteed perform class possible priors differ degree 
need bias learning term learning bias entails various mechanisms influence hypothesis learning algorithm going come 
mathematically learning bias written pl chance hypothesis returned learner data factor influences learning result hypothesis language referred language bias 
learning algorithms minimize empirical error rate decide hypotheses empirical error language return 
possible bias draw random uniform distribution hypotheses empirical error 
assume particular bias chapter 
learner follow completely different bias prefer hypothesis respect alphabetical ordering 
choosing error minimizing hypotheses learner choose particular hypothesis maximizes merit criterion complexity penalization algorithms leads particular learning bias 
obvious learning bias major impact generalization ability resulting hypothesis 
precisely relation learning bias generalization ability look 
asked way learning bias proven superior biases 
pac vc theory discussed carefully section study generalization ability intrinsic properties learning bias 
pac results interpreted suggesting order achieve generalization choose certain hypothesis languages avoiding 
careful analysis reveals interpretations undue 
contrast free lunch theorems detailed discussion see section clarify generalization ability particular learning bias property focused problem property bias 
pac theory requires learners produce hypothesis low true error minimal domain knowledge available class possible target functions known 
way quantifying generalization performance learners look sample size required guarantee low true error 
sample complexity learning studied intensely classes target functions hypothesis languages identified learned sample sizes polynomial size parameter function language class 
blumer 
proved known result generalization error hypothesis consistent sample size learned hypothesis language bounded confidence gamma ffi sample size log ffi assume hypotheses consistent sample learned hypothesis languages respectively jh jh blumer result shows prove better error bound tempting misinterpret result meaning chapter 
better language bias necessarily true particular true target function known lie different way thinking problem reveals average incur equal generalization error 
wolpert adopted different perspective question learners came insightful results free lunch theorems 
assume learners minimize sample error anyway sample error error instances sample interesting issue 
imagine classification problem instances sample 
hypothesis language powerful distinct hypotheses consistent sample hypotheses form version space mitchell behave differently remaining instances 
learner returns hypotheses consistent sample needs built preference choose hypotheses equally consistent sample preference referred learning bias 
learner return hypothesis classifies instances labels 
possible target functions behave differently label 
look sample error averaged possible target functions uniform distribution instances 
learner incurs sample error zero 
average incurs sample error learner incurs sample error averaged functions 
fact learners impose average sample error observation leads lunch theorem uniformly averaged possible target functions sample errors arbitrary learners equal 
theorem holds arbitrary learners basically says impossible construct learner better average problems 
important point averaged error uniformly target functions 
functions occur frequently known nonuniform prior possible construct learner performs particular distribution 
prior nonuniform unknown third free lunch theorem claims better average learner constructed 
see section formal presentation free lunch theorems argues far generalization error rate concerned thing intrinsically learning bias 
construct learner general accurate 
low generalization error due alignment bias learner prior probability target concepts occur domain 
justifies need learning bias adequate learning problem 
aspect learning bias complexity learning algorithms particular bias 
consider example 
target function term cnf conjunction disjunctions variables learner uses term cnf hypothesis language guaranteed find hypothesis consistent sample size time polynomial dnf disjunctions arbitrary conjunctions may consist boolean literals hypothesis language polynomial algorithm finds dnf consistent sample approximates target term cnf dnf proper superset term cnf 
dnf greedy algorithm exists term cnf selected hypothesis language hypothesis space enumerated check hypothesis consistent sample 
inductive logic programming ilp lavrac dzeroski muggleton target function set horn clauses usually class horn clauses subject restrictions 
learning problem calculating class label hypothesis assigns instance undecidable logic implication np complete subsumption 
consequently 
model selection learning problem cases extremely expensive 
turn certain sets clauses learned polynomial time usually subsumption proven efficiently kietz dzeroski scheffer equally large sets learned polynomially 
complexity oriented point view language biases intrinsically superior 
model selection consider situation order solve learning problem free choose hypothesis language set languages models decision trees variable depth neural network variable number hidden neurons 
think model collection structurally identical similar hypotheses 
statisticians think models parametric schemes hypotheses models instantiated parameters 
choose simple model tree depth say best hypothesis model incur high empirical high true error 
hand choose rich model neural network hidden units hypothesis poor due fitting effects 
problem referred bias variance trade breiman 
distinguish bias variance term generalization error 
bias part generalization error error rate best approximation target model 
increasing model size bias term decreases monotonically 
variance term quantifies error imposed improper labeling nodes caused limited data available 
variance term increases increase model size depends problem 
situation 
classes approaches distinguished hold testing methods parts data assess hypotheses learned increasingly complex models complexity penalization approaches minimize criterion empirical error consists empirical error plus complexity penalization term bayesian approaches exploit additional information terms prior target functions assumed known advance 
hold testing 
thing stratify hypothesis language increasingly complex models model consist networks hidden units cross validation stone toussaint bootstrapping efron obtain estimate true error hypothesis returned learner learner operates model starting smallest model learning algorithm returns hypothesis model 
hold set obtain estimate expected generalization error model lowest estimate selected learner invoked model sample 
order minimize variance estimate idea fold cross validation stone bootstrapping efron average error measurements generated re sampled data sets instances re sampled data set drawn original data set replacement case cross validation replacement case bootstrapping 
model incurred cross validation error selected learner run model complete training set 
method illustrates intimately error assessment model selection related 
shows trivial bound model selection techniques suppose stratify hypothesis language models jhj containing exactly distinct hypothesis 
sample decide model chose necessarily hypothesis just obtained model containing hypotheses 
applications fold cross validation works quite reliably noted class approaches yields reasonably accurate chapter 
estimate expected error yield estimate variance 
precisely empirical variance generally true variance error estimates similar data independent measurements dietterich 
main drawback approach high computational effort 
depending sample size learner invoked times model kohavi john 
domains learning time consuming number potential models large cross validation may incur unacceptably large computational effort 
case instance inductive logic programming muggleton neural segmentation satellite data number attributes extremely large milne 
complexity penalization 
assessing models means cross validation assigned complexity penalty term model 
hold testing algorithms sequentially consider hypotheses learned increasingly complex models complexity penalization methods minimize criterion consists empirical error complexity penalty term 
member class structural risk minimization srm vapnik vc framework vapnik chervonenkis 
support vector machine svm approximate implementation srm 
vanilla support vector machine inflates instance space introducing polynomials original attributes new attributes 
ideally result positive negative examples separable single hyper plane 
planes consistent sample svm chooses respect stratification defined terms width margin positive negative samples 
effectively svm returns plane inflated space maximizes margin examples distinct classes 
particular stratification leads maximally large margin proven beneficial practical learning problems 
intuitively svm works best classes clustered distinct centers 
svm trade model complexity empirical error empirical error pinned zero 
may result fitting cases consistent hypothesis best approximation fairly simple 
constraint weakened soft margin machines cortes vapnik introducing parameter trades higher vc dimension lower observed error adjusted cross validation heuristic 
similarly complexity penalization approaches regularization moody neural weight decay methods cun decision tree pruning algorithms quinlan mingers merit selected model depends strongly value chosen penalization regularization parameter 
effectively parameter forms meta level model selection problem trying different parameter settings incurs meta level fitting ng 
new penalization model selection algorithm proposed schuurmans context regression 
distribution unlabeled instances define metric hypotheses hypotheses target distribution 
knowing distance hypotheses triangle inequality decide distance target distribution increasing 
approach turns perform better cross validation complexity penalization methods problems steep variance profile schuurmans 
bayesian learners berger solve learning model selection problem time 
certain ideal conditions high computational effort derive bayes hypothesis posterior js guaranteed generalization error 
intuitively prior relates hypothesis complexity optimal coding scheme frequent hypotheses small description length likelihood sjf relates empirical behav 
applications machine learning ior hypothesis function bayes hypothesis yields optimal trade likelihood empirical behavior prior complexity 
posterior expensive model selection heuristics map maximum posteriori hypothesis maximizes chance having generated data mdl rissanen mdl hypothesis minimizes description length required data compressing hypothesis exceptions hypothesis data 
prior distribution assumed known advance strong assumption 
general belief bayesian learners fairly robust degree misalignment actual assumed 
free lunch theorems wolpert explain bayesian learners perform better randomly guessing actual assumed prior completely unaligned 
section discuss free lunch theorems claim construct learner performs better average possible problems 
theorems imply learner conducts model selection superior averaged possible target functions 
raises question situations conducting model selection beneficial 
applications machine learning engineering point view interesting aspect machine learning provides methods automatic adaptation system particular environment 
successful applications machine learning techniques numerous mentioned 
area applications gaining interest knowledge discovery databases siebes fayyad 
currently existing commercial databases contain large quantities potentially valuable knowledge regarding instance typical patterns customer behavior 
idea data mining automatically extract potentially interesting patterns 
frequently studied problems discovery association rules 
association rules agrawal simple implications database items form customer buys beer customer buy potato chips 
large size typical databases imposes particular difficulty data mining problems 
data mining algorithms usually required operate time linear size database preferably sub linear toivonen 
discovering patterns customer behavior detecting fraudulent credit card transactions belong popular data mining tasks 
large number applications fall general field pattern recognition 
entails computer vision jain optical character recognition cun recognition spoken words lee 
text categorization salton problem mapping texts semantic categories 
interesting applications automatic classification news stories research lewis lang classification web pages joachims 
automatic control large field application 
algorithms automatically acquire control skill fall classes reinforcement learning behavior cloning relative benefits approaches discussed scheffer 
reinforcement learning algorithms sutton barto acquire skill conducting control experiments receiving performance feedback 
successful application reinforcement learning td gammon tesauro program plays backgammon world championship level 
applications include instance automatic control cars highways pomerleau 
behavior cloning algorithms learn examples behavior applied chapter 
problems satellite control muller wysotzki flight simulators sammut 
knowledge acquisition quinlan process elaborating background knowledge domain domain expert implementing knowledge expert system 
knowledge acquisition generally considered bottleneck construction expert systems 
machine learning algorithms support process extracting knowledge examples behavior descriptions expert gaines compton kang scheffer 
applications machine learning classification algorithms applied problems medical diagnosis wysotzki richter kononenko regression algorithms applied share price prediction problems 
principle contributions doctoral dissertation discuss compound questions assessment hypotheses related selection hypothesis language learner 
sketch main results 

conduct new analysis expected true error hypotheses minimize observed error 
analysis characterizes generalization error apparently best hypothesis model terms prior distribution error rates model estimated efficiently 
results predict explain learning curves precisely pac style results contribute better understanding nature generalization 

immediate result analysis leads efficient algorithm model selection primary benefit efficient cross validation usually accurate 
conduct series empirical studies support claim 
demonstrate scalability algorithm text categorization problem 

study situations conducting model selection leads better generalization conducting model selection 
couple generally negative results characterize class learning scenarios located gap bayesian pac learning occam algorithms considered weak form model selection perform better pac learners 
develop framework quantifies expected error cross validation model selection 

abovementioned framework practical implications provides answer question sample split training hold sets predicts cross validation model selection respect stratification better worse simple error minimization 

instantiations parametric learning algorithm learners parameters learning rates regularization parameters compared respect performance collection data sets results necessarily optimistic results pessimistic estimates true performance 
best observed accuracy 
organization optimistic estimate 
quantify just optimistic estimate discuss consequences result empirical assessment learners 

model selection approach learner constrained fixed model 
contrast boosting approach hypothesis space allowed grow dynamically 
giving new worst case time bounds adaboost algorithm show cases restriction small sets hypotheses causes high complexity learning 
organization section gives overview structure chapters 
chapter discuss principles machine learning introduce necessary definitions methodology 
chapter new analysis error hypotheses minimize empirical error rate resulting model selection algorithm 
empirical results learning boolean functions text categorization 
chapter deals question model selection beneficial particular expected error cross validation model selection chapter analyzes accuracy apparently best hypotheses generated differently parameterized learners 
chapter addresses complexity model selection learning chapter contains concluding remarks 
appendix contains proofs derivations exceed page length 
appendix furthermore contains list frequently abbreviations 
chapter preliminaries thesis study problem classification learning labeled examples 
results refer expected generalization error hypotheses requires existence natural distribution instances 
terminology book instances 
set instances finite set class labels simplicity assumed set 
classification problem defined unknown distribution dxy jx dx labeled instances theta approximated closely possible 
jx yjx chance class label observed instance dx probability distribution density governs instances classical pac theory learning problems defined consisting function class target functions distribution dx instances 
convenient refer notation target function note definition subsumed notation dxy proposed kearns 
dx define dxy jx dx jx yjx iff 
hypotheses error 
hypothesis mapping instances class labels 
true generalization error rate hypothesis respect unknown distribution dxy difference predicted value class labels weighted dxy formally ed thetay zero loss function 
convenient talk target functions conjunction distributions dx instances write error dx 
switch notations target functions target distributions dxy whichever appropriate situation 
sample sequence labeled instances drawn independently identically distributed dxy sample size abbreviated book 
example drawn dxy put way sample drawn dxy empirical observed error difference predicted value class label observed sample sample instances 
hypothesis incurs empirical error zero sample said consistent sample 
hypothesis language model 
hypothesis language may infinite may infinite vc dimension 
stratification hypothesis language finite 
models generalization sequence models hh models properly include fact assume models monotonically growing 
chapter assume model finite subset learner 
learner takes input sample model returns hypothesis hl learner may deterministic case lh refers output sample stochastic 
case pl js distribution finite density infinite hypotheses 
learner may determine set fh min hypotheses empirical error 
hypothesis 
call learner determines draws hypothesis set random erm learner error minimizing learner corresponding hypotheses erm hypotheses 
definition erm learner sample model erm learner returns hypothesis hl minimum empirical error min multiple hypotheses minimum error learner picks random uniform distribution 
learning curve 
hh stratification models learner 
sample learning curve set points ed lh learning curve displays generalization error incurred learner model models 
sample fixed expected learning curve fixed sample size plotted 
target distribution prior distribution targets dxy fixed expected learning curve targets plotted 
learning curves shaped 
model selection algorithms try determine minimum learning curve 
target distribution dxy known learning curve estimated cross validation error plotted 
notations 
generally write probability distributions densities form fxg subscript indicates random variable 
distribution clear context 
fxg refers density discrete distributions thought chance drawing similarly write fxg expectation distribution clear context 
write binomial distribution denoting chance observing marked instances drawing instances replacement chance observing marked instance hypergeometric distribution written quantifies chance instances marked draw instances set instances theta marked 
models generalization section discuss distinct mathematical models generalization relations 
theory computation canonical model computability turing machine entails models computability lambda calculus completely entails intuition computability 
unfortunately canonical model learnability powerful completely capture intuition learning 
hierarchy learnability classes identification limit framework angluin smith jain orthogonal concepts learnability frameworks pac theory valiant bayesian framework berger statistical physics framework tishby 
briefly survey vanilla versions models relations 
survey come new result relationship identification limit pac theory 
chapter 
preliminaries gold framework learning construction identification limit model gold guided intuition language acquisition related study language acquisition linguistics wexler revealed restrictions grammatical structure natural languages necessary languages learnable 
learner said identify function language class limit iff guaranteed win game learner class possible target functions 
teacher starts producing text sequence instances may consist positive positive negative examples example occurs eventually 
new instance read learner may change mind new hypothesis conjecture target function 
learner wins game finitely mind changes hypothesis correct change reading new instances 
definition learning referred explanatory learning corresponding class learnable function classes abbreviated ex easy example class functions ex class functions denotes natural numbers zero finitely places 
learner reproduces observed nonzero function values guesses zero places identifies class limit 
finitely nonzero values example occurs eventually learner observed nonzero values finitely examples 
note learner know identifies target fact number examples bounded may arbitrarily nonzero values 
contrast class computable functions learned limit computable machine gold 
important variations gold learning paradigm learner identifies target function correct corresponding learnability class abbreviated bc finitely erroneous predictions finite point keeps making true predictions may change conjecture target function 
bc shown proper superset ex barzdin case smith 
finite identification shot learning learnability class imposes restriction learner finitely examples learner come correct hypothesis learner may change mind 
clearly setting restrictive ex learning learner aware data seen far suffices correct conjecture 
surprisingly ae ex lindner 
compounds questions studied identification limit framework learnability relative oracles lindner stephan 
set computable relatively oracle algorithm computes allowed ask questions form 
similarly class functions learnable relatively oracle algorithm may ask questions form identify 
massive corpus results exists identifiability functions relative queries teacher smith angluin learnability function classes teams learners jain sharma smith 
detailed overview identification limit framework reader referred jain 
pac vc models generalization pac framework generalization valiant overview see kearns vazirani strongly focused efficient learning learning fixed sized samples 
pac theory distinguishes hypothesis language class target functions learner pac generalizer guaranteed win game learner 
models generalization gets know class possible target functions information target 
learner parameters ffi 
point learner request required sample size note sample size depend parameters information available point 
teacher fixes target function arbitrary distribution dx instances 
teacher free choose target function may instance select function matches learning bias worst 
sample labeled instances size drawn dx learner chance learner returning hypothesis hl edx exceed ffi 
learner wins pac generalizer chance drawing sample size resulting hypothesis incurs error ffi dx pac theory worst case theory respects chosen sample size suffice distribution dx target function furthermore pac bounds required sample size hold learner bounds subject additional assumption learner manages select worst possible hypothesis 
setting particularly hard see fixed sample size depending ffi suffices dx pac learner said polynomial pac learner required sample size total running time polynomial ffi size parameter usually number attributes 
pac theory concerned function classes learned polynomially learned polynomial time polynomially sized sample 
fundamental learnability result obtained blumer 
chance error hypothesis hl consistent sample size exceeds error bounded edx je jhj gamma corollary conclude hypothesis hl consistent sample size edx je ffi holds sample size log jhj ffi edx je jhj gamma gamma log jhj ffi delta ffi elementary result provides easy follow scheme conducting learnability proofs pac framework function class learned polynomially hypothesis language log jhj required sample size polynomial typically number attributes algorithm constructs hypothesis consistent sample class target functions polynomial time 
function classes shown polynomially learnable conjunctive concepts valiant linear threshold units blumer dnf boolean disjunction literals conjunction valiant cnf valiant decision lists rivest polynomially learnable fixed hand disjunctions conjunctions polynomially learnable valiant conjunctions linear threshold units blumer conjunctive concepts structural domains consist nodes unary attributes haussler 
pattern languages variable mitchell pac learnable polynomially learnable 
unknown boolean formulae disjunctive normal form dnf polynomially learnable exists decision tree learner runs time polynomial size smallest possible decision tree 
virtually hypothesis space locally optimal hypothesis polynomial time greiner 
function classes polynomially learnable equals polynomial algorithm constructs consistent hypothesis 
cases classes polynomially learnable hypothesis language extended 
puzzling finding chapter 
preliminaries learning generally difficult larger hypothesis spaces 
example term cnf contains conjunctive normal forms conjunctions disjunctions arbitrarily literals variables pitt valiant 
log jk term polynomial required sample size polynomial 
order find consistent hypothesis exhaust hypothesis space requires exponential computational effort 
class dnf disjunctions conjunctions literals superset term cnf log jk polynomial sample size required learn dnf 
learning term cnf requires exhaust space greedy algorithm learns dnf polynomial time 
algorithm finds conjunction literals covers positive negative example 
procedure commits conjunction removes covered instances recurs positive instances covered 
examples cases extending eases learning task np complete find consistent neural network units task accomplished log arbitrarily units introduced learner chapter scheffer stephan 
classical pac theory subject major restrictions 
target function assumed member hypothesis language consistent hypothesis assumed exist second standard pac bounds proof techniques finite hypothesis languages 
finiteness particularly strong restriction finite languages pac learnable albeit necessarily polynomially learnable 
agnostic learning theory kearns haussler aims extending pac theory account cases hypothesis consistent sample available 
independently earlier vapnik chervonenkis developed theory connected pac theory fundamental result allows results similar elementary results agnostic learning theory 
vapnik tried find statistical explanation success rosenblatt experiments perceptron rosenblatt 
linear threshold units form infinite hypothesis space vapnik chervonenkis focused number distinct hypothesis find bound true error hypotheses 
vc dimension combinatorial property hypothesis languages accounts number distinct hypotheses defined follows set shattered set subset ae element intuitively shattered iff labeled possible jsj ways vc dimension largest number set jsj shattered intuitively vc dimension language iff realize possible boolean functions instances 
fundamental result links pac vc theory class functions pac learnable iff vc dimension finite blumer 
vc dimension bound difference empirical true error hypothesis chernoff bounds 
leads lower ehrenfeucht upper vapnik bounds sample size required empirical error hypothesis hypothesis space close corresponding true error 
vapnik proves largest difference true empirical error rate hypothesis model confidence gamma ffi log log ffi vc dimension 
models generalization relationship pac gold framework main difference gold valiant models generalization 
analyze explanatory ex pac learning distinctions occur ex learning requires exact match bc learner required emulate behavior perfectly point pac generalizer required find hypothesis behavior close behavior respect dx high probability 
argues gold framework stronger pac learning 
point time ex learner know target function identified 
learner contrast know identified learner required number learning steps bounded priori examples observed 
sample size required pac learner bounded class target functions information 
argues pac restrictive identification limit 
study question frameworks stronger referring function class studied pac identification limit framework pattern languages 
simple intuitive notion pattern languages formally introduced angluin studied extensively context formal language theory computational learning theory 
refer reader salomaa review pattern languages formal language theory 
quick definition pattern languages useful discussion 
sigma alphabet countable set constants language disjoint sigma countable set variables 
element sigma pattern 
pattern list distinct variables strings sigma pfx denotes string sigma obtained substituting occurrence language generated defined sigma pat denote set patterns pat denote set pattern languages 
angluin showed class pat identifiable limit positive data gold model 
pattern languages variants subject intense study identification limit framework review see shinohara arikawa 
reader note definition angluin class pat allow empty substitutions 
empty strings allowed substituted variables leads larger class extended pattern languages see shinohara 
class turns complex open finite alphabets size identified limit positive data pat identifiable limit positive data natural question gain negative data 
lange observed presence positive negative data class pat identifiable mind changes learner looking sufficient number positive negative examples comes correct pattern language restricted shot version identification limit referred finite identification 
theorem proves variable pattern languages learnable pac setting 
theorem mitchell scheffer sharma 
vc dimension variable pattern languages unbounded 
see mitchell subclass shown identifiable limit positive data 
mitchell 
show learnable alphabet size 
chapter 
preliminaries proof theorem mitchell 
theorem significant strengthening schapire negative result general pattern languages schapire blush contradict learnability result variable pattern languages kearns pitt 
closer look result kearns reveals learnability due assumption length substitution strings bounded 
precise explanation pattern languages ex learnable variable pattern languages pac learnable 
theorem mitchell scheffer sharma ffi 
variable pattern language arbitrary distribution sigma initial set positive sentences size min sg 
pattern consistent sample size min log ffi min err ffi 
theorem proof mitchell claims variables pattern languages learned required sample size bounded positive example observed 
reason positive sentence read length target pattern alphabet bounded renders space possible target patterns finite pac learnable 
far seen pac nonempty intersection 
raises question pac subset case 
fact pac subset bc consider class zo linear threshold functions dimensional real valued interval 
vc dimension zo 
zo bc identified limit 
theorem class threshold functions dimensional real valued interval bc identified limit zo bc 
proof 
target function drawn quasi uniformly interval 
show positive negative instances observed expected difference positive sample size hypothesis behaves correctly 
xm positive xm negative examples 
definition positive instances drawn quasi uniformly interval negatives 
max maxfx largest positive min negative instances 
negatives drawn open interval min gammax max holds 
quasi uniform distribution bounds min max possible values induce quasi uniform distribution interval max min 
max min guess learner 
averaged possible distributed restricted max min difference xmin xmax gamma xmax gamma xmin 
learner guaranteed produce correct hypothesis steps arbitrary 
results prove learnability concepts pac identification limit orthogonal illustrated 
bayesian framework bayesian learning bayes focused posterior distribution js function having generated observed data js construct bayes hypothesis 
models generalization bc fin pac ex relation pac identification limit 
follows argmin gamma js zero loss function fundamental theorem bayes bayes hypothesis minimizes expected generalization error 
unfortunately js hard get hold 
bayes rule reduced sjf reduced sjf map hypothesis determined constant hypotheses particular data set 
sjf cases determined leaves problems known advance strong assumption order determine bayes hypothesis sum space functions ongoing philosophical debate means 
think probabilities objective properties intrinsically stochastic systems neyman bayesians see probabilities means modeling behavior system subject predict precise behavior system due limited level system intrinsically stochastic 
motivates notation subjective probabilities 
bayesians take idea step think probabilities subjective beliefs chances events underlying systems required intrinsically stochastic 
detailed discussion meaning probability see berger jeffreys de finetti schafer 
far concerns results book probabilities interpreted intrinsically system subjectively lack information distinguish actual probabilities physical reality belief agent 
computing bayes hypothesis js extremely expensive heuristics proposed 
known heuristic choose map maximum posteriori hypothesis maximizes js 
minimum description length mdl rissanen known heuristic 
order communicate sample labeled instances transmit data set alternatively find hypothesis covers instances append instances properly classified hypothesis exceptions 
complex hypothesis examples covered exceptions appended 
mdl hypothesis minimizes description length hypothesis plus description length exceptions 
course determining description length hypothesis requires definition optimal code turn requires knowledge prior distribution 
certainly main drawbacks bayesian framework including mdl map hypotheses demand known 
third free lunch theorem wolpert function approximation opposed classification usually quadratic loss chosen function corresponding bayes hypothesis guaranteed minimize quadratic loss chapter 
preliminaries claims known learner assumes expected sample error averaged possible priors just error achieved random guessing 
interesting realistic assume knowledge scenario referred robust bayesian learning berger 
technically modeled assuming true element class possible distributions 
classes distributions studied intensely 
contamination classes defined distributions written gamma assumed prior contamination instance defined class distributions berger berliner moreno cano bose 
moment classes contain priors common set moments berger 
density bands classes priors lie bounds 
consequently priors necessarily integrate 
justification generalized priors guaranteed target function hartigan 
classes priors studied quantile classes cano mixture classes bose shape smoothness classes bose 
hypothesis robust bayesian learner js gamma js js actual js assumed posterior yields implications gain error incurred lack knowledge prior 
links pac vc bayesian learning relations vc framework bayesian learning characterized haussler 

haussler show vc dimension related learning curves bayesian learners correct inaccurate priors 
mcallester gives pac style error bounds consistent hypotheses presence known prior distribution 
interesting link pac bayesian learning free lunch theorems wolpert due worst case nature pac theory worst case respect target function pac error bounds achieved learner reflects data behaves arbitrarily poorly instances 
free lunch theorems prove impossible generalize function unobserved instances information prior 
baxter studied link hierarchical bayesian learning pac vc framework 
hierarchical bayesian learner prior distribution priors target functions available know actual prior knows certain priors 
studying sequence learning problems hierarchical bayesian learner attempt identify prior generates target functions turn generate data 
baxter gives pac style analysis bounds number learning problems observed prior estimated degree accuracy 
main result pac style error bounds th hypothesis gamma problems observed considerably lower known bounds stand learning problems 
chapter establish links studying partial knowledge prior improve sample complexity compared pac learner knowledge 
discuss learnability function classes pac learnable knowledge prior 

free lunch free lunch section discussed question learning bias superior learning biases informally 
pac results blumer 
similar vc style results vapnik show certain error rate hypothesis learned small hypothesis language error rate hypothesis origins large hypothesis language give rise idea case 
idea experiences intuitive support breiman bias variance decomposition breiman 
breiman split error rate cart algorithm bias term error rate tree depth class labels assigned optimally leaf nodes variance term error rate imposed improper labeling nodes caused insufficient data 
breiman shows bias term decreases hypothesis complexity tree depth increased 
shows know variance term error complexity grows worse bounds proven frequently misunderstood meaning variance term increases 
results lead general believe thing learning bias particular complexity hypothesis regularized model selected hypothesis accurate 
examples complementary behavior observed 
fisher schlimmer observed problems unpruned decision trees outperform decision trees regularized complexity 
schaffer shows controlled experiments support idea suitability particular learning bias property learning problem studied property learning bias 
schaffer argues conducting model selection just particular learning bias inherently useful 
wolpert studied question mathematically came result clarifies meaning learning bias 
free lunch theorems claim uniformly averaged problems learning biases equally provided learners minimize empirical error 
theorem wolpert arbitrary learners implement distributions js js returned hypotheses 
gammas hl gammas dp js gammas expected sample error hl incurred learners respectively true error instances occur sample 
dxy jx dx jx implements function jx yjx 
furthermore finite equations hold dx uniformly averaged 
uniformly averaged targets functions sg gammas jd xy gamma sg gammas jd xy 
words dx uniformly averaged targets learners incur equal sample error 

uniformly averaged targets sample gammas jd xy gamma gammas jd xy 
sample learners incur equal error averaged targets 

uniformly averaged distributions target functions fp sg gammas jm gamma fp sg gammas jm 
prior known learners assume distinct priors perform equally averaged possible priors 
free lunch theorems extended cover countable infinite domains wolpert chapter 
preliminaries 
uniformly averaged samples fp gammas js gamma fp gammas js 
prior known learners assume distinct priors perform equally averaged possible priors sample 
third free lunch theorems discuss expected sample error rate sample drawn 
contrast theorems discuss error rate particular sample 
note definition generalization error book see section resembles error rates theorems note free lunch theorems refer sample error generalization error 
claims hold particular problem claims discuss expected error rate distribution problems 
section discussed example demonstrates instances sample hypotheses consistent sample assign distinct combinations class labels remaining instances 
target functions assign distinct combinations class labels instances 
average error rate hypotheses possible target functions turns error rates average equal 
free lunch theorems explain impossible construct learner general accurate 
mean particular problem learners equally 
learner superior problem implies problem superior learners equally average possible problems 
way construct learner better average particular set problems implement additional assumptions target problems learner narrow range suited applications learner 
note contradict assumption machine learning useful 
means order obtain learning results necessary implement background knowledge learning algorithm available focused problems 
model selection discussed intuition model selection known model selection techniques informally section 
section treat techniques formally summarize results approaches 
generally task model selection select model learner minimizes expected loss model results thesis refer zero loss function generalization error discussion section restricted generalization error loss function 
model selection requires definition stratification models hh set hypotheses 
algorithms require sequence nested models ae ae ae cases sequence allowed infinite 
furthermore learner lh fixed maps samples hypotheses lh nondeterministic characterized distribution pl hypotheses sample 
model selection problem hh task model selection algorithm select model deterministic learners expected error lh fsg ed lh nondeterministic learners expected generalization error possible resulting hypotheses fs ed hl ed dpl js dxy minimized 

model selection hypothesis space stratification stratification space version occam razor competing occam learners give distinct guarantees hypotheses learner guarantees high probability incurs low error incurs slightly greater error incurs fairly high error contrast learner guarantees fairly low error hypothesis gives weak guarantee learners operate respect distinct give different definitions simple 
free lunch theorems claim equal errors averaged target concepts equally 
occam algorithms occam razor oldest elements folk lore machine learning 
dates back william ockham th century philosopher 
back william claimed items multiplied unnecessarily 
machine learning statement generously translated simplest explanations best generally referred occam razor principle 
justification pac theory gives statement blumer error bounds hypotheses consistent sample weaker size complexity hypothesis grows 
seen chapter easily proven ed je jhj gamma motivates occam algorithms require stratification nested ae ae select smallest index lh consistent sample 
example occam algorithm vanilla support vector machine 
support vector machine defines stratification models terms distance hyper plane positive negative examples 
models infinite hyper planes defined terms real valued vectors vc dimension considered measure number distinct hypotheses 
note stratification models depends sample obviously distance plane positive negative examples depends examples guarantees support vector machine comes approximate 
blush returning hypothesis restricted subset hypothesis space reasonable enable give better guarantees accuracy returned hypothesis 
wonder possible prove better bound largest difference empirical true error hypothesis accuracy returned hypothesis increases choose smallest sufficient model 
suppose occam learners different ae ae respectively illustrated 
version space entails hypotheses consistent sample 
learners distinct encoding schemes hypotheses opinions hypotheses simple belong model small index differ 
chapter 
preliminaries vapnik learner prove better bound lies model larger contain hypothesis consistent sample incurs unusually great true error worst case return hypothesis 
contrast learner guarantees fairly low error hypothesis gives weak guarantee larger contain unusually poor hypothesis learner return worst case 
learner prefers intrinsically better clear answer provided claim free lunch theorems 
uniformly averaged target functions hypotheses incur equally large generalization error consistent sample 
learners intrinsically superior 
learner better target functions lie left hand side hypothesis space case hypothesis model small index stratification consistent sample allow learner guarantee low error learner performs better target functions right hand side 
argues bayesian scenario certain target functions occam razor justified 
suppose models stratified prior probability target 
picks hypothesis consistent sample model index maximizes prior posterior probability selected hypothesis target function 
prior known construct optimal representation scheme optimal representation minimizes message length required transmit functions functions drawn respect 
optimal coding scheme functions occur frequently represented shorter texts occurring infrequently 
context occam algorithms certainly sense choosing shortest hypothesis consistent sample maximizes prior probability implicitly posterior probability picking correct target function 
occam algorithms special case model selection algorithms trade increases empirical error increases complexity select hypotheses consistent sample 
complexity penalization occam algorithms return hypotheses consistent sample special case complexity penalization algorithms 
complexity penalization algorithm uses stratification hh ae returns hypothesis minimizes criterion model smallest index hypothesis occurs 
penalty term delta delta penalizes size frequently models infinite number free parameters moody moody vc dimension vapnik model focused hypothesis occurs time stratification 
frequently penalty 
means penalization term complexity penalization algorithms try reconstruct learning curve ed lh complexity measure ideally close guess ed 
complexity penalization algorithms thought penalizing complexity vc dimension hypothesis 
note underlying suggestion hypotheses possess intrinsic complexity vc dimension misleading 
complexity hypothesis measured relatively encoding scheme 
varying encoding scheme assign hypothesis description length wish 
confusing speak 
model selection vc dimension hypothesis vc dimension defined sets elements see section 
vc dimension model included depends hypotheses wish group completely arbitrary decision 
example complexity penalization support vector machine svm 
original svm vapnik required empirical error zero see section svm extended cortes vapnik minimizes empirical error plus approximated worst case bound difference true empirical error hypothesis model 
minimum description length mdl rissanen considered complexity penalization model selection algorithm 
essentially correct mdl tries reconstruct learning curve ed lh models empirical error lh 
considering description length required encode hypothesis plus description length required encode difference examples empirical behavior instances occur sample 
mdl chooses model minimizes sum description lengths 
think mdl heuristic strategy bayesian learning mdl assumes prior known prior required determine optimal code selects hypothesis distinct easier determine bayes hypothesis see section 
discussion mdl principle applications machine learning see oliver baxter mehta oliver grunwald quinlan rivest 
complexity penalization model selection algorithm reflect idea generalization error intrinsic property hypothesis language try reconstruct learning curve empirical error complexity measure hypothesis space 
considerations discussed sections show approach certain limitations 
problems variance term generalization error increases steeply increasing model index problems case 
possible construct pair learning problems steeply increasing flat generalization error curve 
complexity penalization algorithm perform problems fail produce additional constant error decrease increasing sample size 
proven kearns 
observed empirically schuurmans 

inherent weakness complexity penalization contrast cross validation model selection algorithm perform reasonable model selection problems 
virtually practical learners employ sort complexity penalization technique instance decision tree learners mingers quinlan muller wysotzki neural network algorithms cun 
cross validation term cross validation slightly general hold testing entails wide range techniques estimation misclassification probabilities intimately related choosing model learning algorithm minimizes estimate 
cross validation techniques provide estimate error biased estimate variance cross validation results support approximate guarantees problem technique performs better high confidence 
suppose hypothesis true error ed 
draw sample independent identically distributed instances measure empirical error 
chapter 
preliminaries chosen minimizes strongly optimistically biased estimate ed fsg ed gamma lh case positive 
earliest days pattern recognition training set error proposed estimate true error rate smith estimate select model tend greater models impose stronger bias 
choice independent sample hold sample training distributed ed binomial distribution 
technique splitting available sample training set hold set originally known method holdout testing proposed 
greater empirical error approximately normally distributed 
empirical error essentially sum random variables distribution sum random variables converges normal distribution central limit theorem 
keeping mind easily determine chance difference true empirical error exceeds certain threshold 
threshold exceeded high confidence called confidence bound 
je gammae oe governed normal distribution oe ed gamma ed true standard deviation follows immediately central limit theorem 
course standard deviation known standard deviation requires true mean value ed unknown estimated 
oe gamma estimate 
note oe estimated examples independent identically distributed 
je gammae oe governed student distribution gamma degrees freedom 
table distribution easily determine chance threshold 
rule thumb confidence difference es gammae estimate subject pessimistic bias sample training expect learner better hold back part sample 
order reduce variance estimate tighten confidence interval minimize pessimistic bias caused hold set training methods proposed 
fold cross validation known double cross validation pi method mosteller tukey conducted follows 
runs learner conducted 
sample re sampled training sets hold sets disjoint disjoint process simulates drawing sample dxy fold hypothesis lh assessed hold data yields hold error rates 
equals method called leave method 
lh averaged hold error folds 
course chernoff bounds bound difference fsg ed jl dxy lh bounds loose practical purposes 
usually number folds large assume sum random numbers governed normal distribution 
usually assumes empirical error rate measured fold governed normal distribution assumption reasonable sample size theta sample size large binomial distribution converges normal distribution 
gamma je fsg ed jl dxy gamma lh qp gamma lh governed student distribution degree gamma student distribution table determine confidence cross validation error chance 
model selection certain amount 
requires estimate variance oe assumed error estimates independent 
unfortunately cross validation yield unbiased estimate variance 
hypotheses learned samples drawn sample 
examples samples identical 
estimated variance optimistically biased estimate 
bias quantified empirically dietterich 
bias worse hold examples drawn replacement happens bootstrapping efron cross validation restarted times kohavi john 
case error measurements example treated independent estimates 
conducting arbitrarily repetitions cross validation bootstrapping possible obtain empirical variance arbitrarily low 
trick prove performance difference learners really equally 
biased variance major problem cross validation model selection estimated error unbiased imposes danger wants study apparent difference really 
bootstrap experiments conducted re sampling number training sets size original data set size randomly drawing examples replacement 
average gamma distinct examples appear training set averaged accuracies remaining test sets provide optimistically biased estimate 
variance claimed lower cases variance cross validation efron observation may due stronger estimation variances compared cross validation 
cross validation model selection algorithms stratification hh select model minimizes lh 
advantage cross validation unbiased estimate target minimized loose bound 
generalization error learning algorithm uses hold testing decide model bounded follows kearns high probability gamma ffi resulting hypothesis hl incurs generalization error plus bound difference true empirical error chosen model plus additional penalty term accounts possibility model selection algorithm choosing sub optimal model 
bound difference true empirical error chosen iq log vapnik minimum models bound vc dimension model training set size hold sample size 
additional penalty term possibly choosing wrong model index log vc dimension greatest model cross validation model selection comes general relatively weak guarantee probability gamma ffi model selection algorithm log ffi flm better fold cross validation kearns 
contrast complexity penalization algorithms fail problems cross validation performs reasonably 
practical learners cross validation techniques select set attributes kohavi kohavi john adapt regularization parameters examples muller wysotzki 
hold testing excessively assess adapt architecture back propagation networks koza rice scheffer 
overview cross error estimation techniques see toussaint 
chapter 
preliminaries empirical methodology machine learning research machine learning attempts evaluate performance generalizers empirically 
clear want consider rosenblatt experiments perceptron rosenblatt experiments machine learning tradition artificial intelligence rosenblatt studying learning humans statistical learning emerged earlier fisher observed fitting effects regression back 
unfortunately artificial intelligence emerged computer science research discipline empirical studies statistics general play strong role 
assessing performance learner specify term performance precisely 
learner assessed respect particular problem dxy sample natural commonly performance criterion fsg ed lh jd xy expected generalization error expected samples learner stone 
noted expected generalization error rate requires existence fixed distribution instances learner exposed hypothesis exposed generated 
assuming existence distribution reasonable 
performance assessed respect particular learning task meaningful criterion difficult define 
order talk expected error expected problems refer prior distribution density dxy problems 
distribution problems learner exposed usually assumed 
remember free lunch theorems imply error minimizing learners precisely equally uniformly averaged target functions 
meaningful questions study targets dxy particular technique perform targets dxy particular technique perform better technique problems archived certain collections murphy aha michie studied imposes particular bias results 
usually performance learning techniques assessed hold testing fold cross validation see section 
learner incurs hold error lower hold error learner just due chance 
section discussed determine confidence hold error close true error 
similar considerations show apparent performance difference learners problem really just pure chance 
outcome learners problem 
define delta delta gamma 
ed gammae gamma delta oe delta governed student distribution 
procedure support claim really better paired test 

calculate gamma delta pp gammae gamma delta 
student distribution table determine smallest ffi gamma ffi 
gamma ffi claim value gamma ffi better particular problem 
chapter conduct set empirical studies draw target functions random uniform distribution set boolean functions 
procedure 
summary advantages benchmark data sets true error hypotheses determined exactly target functions known second results support claims boolean functions experiments benchmark data sets support claims performance learners problems drawn respect distribution problems benchmark problems drawn 
methodological problem occurs learners parameters 
parameter settings learner tried lowest measured error unbiased estimate true error learner parameter setting 
discuss issue chapter 
detailed overview methodological aspects cohen 
summary ffl book focused classification learning 
task minimize ed zero loss returned hypothesis learner empirical error hypotheses observed 
ffl pac vc theory support claims true error hl bounding chance terms sample size hypothesis empirical error true error 
ffl loss functions generalization error require existence stationary distribution instances learner exposed 
contrast identification limit framework learner successful guaranteed identify target function exactly eventually 
exactly means loss function required eventually means bound sample size sufficient learning 
principle difference valiant gold frameworks 
ffl bayesian framework exploits knowledge prior distribution target functions 
requires learner exposed stationary distribution target function assumed known 
ideal conditions determine bayes hypothesis minimizes expected loss 
ffl occam algorithms particular learning bias select hypotheses consistent sample 
consistent hypothesis description length particular arbitrarily chosen coding scheme preferred 
intuition hypothesis comes restricted subset hypothesis space able prove better bounds largest difference true empirical error hypothesis language 
improve expected error returned hypothesis average learners return hypothesis consistent sample equally 
ffl complexity penalization algorithms try reconstruct learning curve empirical error complexity models 
worst case bounds largest difference empirical generalization error hypothesis model construct assumed learning curve 
learning curves differ considerably complexity penalization algorithm perform reasonably model selection problem 
chapter 
preliminaries ffl cross validation model selection algorithms stratify hypothesis language models select model estimated expected error hypothesis returned learner model minimized 
error estimated recording average error hold data 
chapter expected error analysis suppose solve learning problem possible models available 
model contains just single hypothesis model contains hypotheses hypotheses incur unknown generalization errors 
draw sample hypothesis exhibits empirical error unbiased estimate true error 
unbiased means expected empirical error hypothesis just generalization error 
relation true empirical error known empirical error governed binomial distribution true error mean value 
suppose minimize empirical error hypothesis empirical error contains hypothesis minimizing empirical error trivial 
hypotheses incur equal empirical errors draw random 
unfortunately empirical error unbiased estimate true error 

chance empirical error hypothesis optimistic estimate true error chance pessimistic estimate 
optimistically assessed hypothesis greater chance selected pessimistically assessed 
return empirical error average optimistically biased 
empirical error empirical error empirical error unbiased empirical error lower known optimistic 
question strong bias chapter answer question 
see expected error hypothesis minimizes empirical error erm hypothesis model depends distribution error values model 
distribution error values hypotheses contains occurring error values true error rates unknown 
estimate distribution true error rates recording distribution empirical error rates distribution empirical error values case simple distinct values occur observed 
error values known suffices determine estimate expected generalization error interesting aspect analysis learning conducted order determine error hypothesis result learning 
giving general overview solution independence assumptions simple implemented easily scheffer joachims 
eliminate stronger independence assumptions come general solution slightly complicated 
discuss resulting formula evaluated efficiently 
chapter defined model selection problem consisting target dxy stratification hh learner sample size assume learner erm learner minimizes empirical error determining set hypotheses chapter 
expected error analysis minimize empirical error returns hypotheses breaking ties drawing random uniform distribution 
results chapter scheffer joachims 
overview framework give brief description important distributions refer theorems 
dxy dxy unknown distribution labeled instances focused model sample size examples drawn dxy 
fh je min fe set erm hypotheses 
apparently best hypotheses minimize empirical error rates confused hypotheses minimize true error rate 
course may hypotheses equally low error hypothesis set 
usually random variable refers hypothesis drawn uniformly hl refers hypothesis drawn uniformly returned learner 
fs ed jh dxy hl distribution error values hypothesis hl drawn uniformly set erm hypotheses sample size 
probability depends random variables sample drawn unknown dxy hl drawn uniformly 
model sample size fixed values 
fs ed jh dxy hl dxy read chance drawing sample subsequently hypothesis hl uniformly generalization error distribution error posterior 
fs ed jh dxy hl integrating fs ed jh dxy hl yields chance drawing sample subsequently hypothesis hl true error hl exceeds 
note difference chance probability interest pac theory fsg ed jh denotes chance drawing sample hypothesis consistent sample exceeds generalization error fs ed jh dxy hl refers chance returned hypothesis drawn uniformly exceeds error 
fs ed jh dxy hl expected error hypothesis hl drawn uniformly set hypotheses empirical error 
distribution error values hl expectation easily derived 
cross validation straightforward expensive way estimating expected value 
fhg ed jh dxy distribution true error values model 
assume model contains finitely hypotheses induces distribution errors 
ed jh dxy chance picking hypothesis random true error distribution error prior 
fs hg jh dxy distribution empirical error rates hypotheses current model 
fs hg jh dxy chance drawing sample hypothesis uniformly empirical error relation fhg ed jh dxy fs hg jh dxy relatively straightforward 
solution independent error values chance randomly drawn hypothesis incurring true error fhg ed jh dxy hypothesis true error incurs empirical error governed binomial distribution mean new example classified properly misclassified probability happening 
fs hg jh dxy ed dp fhg ed jh dxy 
solution independent error values section discuss derivation additional assumption true error rates hypotheses considered learner independent random variables 
solution generalized solution section 
solution discussed section easier general refer solutions results chapter 
crucial relation analysis posterior fs ed jh dxy hl prior fhg ed jh dxy 
main claim theorem fs ed jh dxy hl function fhg ed jh dxy 
high level intuition derivation repeatedly applying bayes formula reduce fs ed jh hl computable terms plus priors 
fhg ed jh dxy chance randomly drawn hypothesis having certain error fs hg jh dxy chance randomly drawn hypothesis having certain empirical error fs hg je dxy chance randomly drawn hypothesis empirical error erm hypothesis 
fs hg jh dxy derived fhg ed jh dxy hypothesis incurs empirical error binomially distributed mean value ed 
discuss fhg ed jh dxy estimated efficiently ground framework empirically 
order determine fs hg je dxy consideration achieves error strictly browsing model learner observes jh error values values distributed fs hg jh dxy 
assuming values corresponding true error values independent random variables easily calculate chance having error doing see process exhausting hypothesis space stochastic process error values instantiations random variable observed 
assumption empirical error rates hypotheses independent estimates corresponding true errors 
jh jh dxy ed ed jh jh jh dxy ed assumption true error rates distinct hypotheses independent random variables 
ed ed jh jh dxy jh ed jh dxy assumption relatively mild implicitly instance calculation values required compare fold cross validation results value gives chance learner better learner problem cross validation results assumptions hold errors independent estimates corresponding true error rates 
chapter 
expected error analysis intuition assumption best captured considering counter example fh maps instances maps instances complementary 
ed known ed pinned gamma ed holds 
ed je ed ed ed independent 
practical hypothesis languages decision trees generally hypothesis languages closed complement violates assumption degree 
section assumption dropped 
theorem scheffer joachims dxy distribution labeled instances finite model 
assumptions error distribution fs ed jh dxy hl function jh fhg ed jh dxy fs ed jh dxy hl ed fs hg jh dxy jh gamma dp fhg ed jh dxy fs hg dxy jh gamma fs hg jh dxy fs hg jh dxy ed dp fhg ed jh dxy 
proof theorem appendix immediate corollary expected true error hl determined 
corollary expected true error hl function jh fhg ed jh dxy fs ed jh dxy hl ddp fs ed jh dxy hl similarly chance hl exceeds generalization error determined just bounded 
corollary chance hl exceeds error function jh fhg ed jh dxy fs ed jh dxy hl gamma dp fs ed jh dxy hl careful implementation theorem corollary runs see appendix 
remaining question get hold distribution fhg ed jh dxy 
distribution known theorem corollary determine expected generalization error hl conduct model selection 
discussing issue general analysis require assumption 
general solution section analysis expected generalization error hl rely assumption 

general solution look model target distribution dxy target dxy defines error ed hypothesis error values define distribution error values write fhg ed jh dxy prior analysis 
fhg ed jh dxy chance drawing hypothesis drawing uniform distribution incurs error 
section study finite models fhg ed jh dxy discrete distribution 
suppose contains hypotheses just easy example 
prior fhg ed jh dxy tells error values occur values chance value chance hypotheses equal errors 
invent names hypotheses ed ed occurring true error values 
sample drawn hypotheses show empirical error values respectively 
happens 
sample size hypothesis incurs empirical error rate case hypotheses respectively governed binomial distribution ed ed respectively 
example classified correctly wrongly chance wrong answer ed ed respectively 
results binomial distribution 
select hypothesis smaller empirical error call hl general case set erm hypotheses learner assumed draw hypothesis hl random uniform distribution set 
chance hl particular error value longer fhg ed jh dxy hl randomly drawn hypothesis 
hypothesis minimizes empirical error 
expected true error hl greater empirical error optimistically biased error randomly drawn hypothesis 
assume sample size fixed priori 
contrast sample random variable governed distribution dxy implies random variable depends hl hl drawn randomly 
leads posterior distribution fs ed jh dxy hl chance drawing sample fixed size consequently hypothesis hl true error hl principle difference prior posterior distribution prior gives distribution error rates hypotheses drawn uniformly posterior gives distribution error values hypotheses generated error minimization process 
posterior fs ed jh dxy hl immediately leads expectation fs ed jh dxy hl 
expected true error hypothesis hl returned erm learner model sample size expected true error hl fs ed jh dxy hl quantified theorem 
crucial part proof determine minimum error number hypotheses jh achieve error 
idea chance particular subset determined factorizing error calculating chances hypothesis empirical error hypothesis outside incurs strictly greater error 
imposes difficulty 
empirical error hypothesis distributed binomially true error 
chance hypotheses incurring certain empirical error determined 
unfortunately probability determined empirical error rates distinct hypotheses assumed independent estimates corresponding true error rates reality empirical error rates slightly dependent measured sample 
assumption assumption relatively mild common statistics 
questionable assumption longer necessary 
theorem scheffer joachims distribution dxy labeled instances finite chapter 
expected error analysis model ed true error hypothesis fixed sample size 
hl hypothesis drawn uniformly set hypotheses empirical error respect sample drawn dxy assumption expected error hl fs ed jm dxy hl jh ed ed jh jh gamma fsg je dxy fsg je dxy proof appendix equation principle evaluated distribution true error values fhg ed jh dxy 
unfortunately straightforward evaluation equation require run time exponential jh additional technical assumption 
assumption assume jh dxy jh dxy assumption means chance set hypotheses empirical error size known hypothesis belongs set dependent hypothesis known set 
assumption reasonable practical cases jh grows doubly exponential boolean functions singly exponential languages conjunctions large 
theorem scheffer joachims dxy distribution labeled instances finite model 
assumptions expected error hypothesis hl returned erm learner sample drawn dxy fs ed jh dxy hl dp fhg ed jh dxy dp fsg jh ed fhg ed jh dxy dp fsg jh ed fsg jh ed ed ees jh jp fhg ed jh dxy jh jp fhg ed jh dxy gamma iff iff arbitrary hypothesis true error ed proof appendix theorem solves primary complexity problem removing product subsets equation 
straightforward evaluation equation run careful implementation see appendix runs 

estimating fhg ed jh dxy 
estimating fhg ed jh dxy 
fhg ed jh dxy depends dxy determined exactly 
information dxy access contained find efficient way obtaining estimate distribution error rates sample 
possible way estimating distribution measure empirical counterpart fhg jh estimate fhg ed jh dxy 
grows fhg jh converges fhg ed jh dxy reasonably large estimate fhg ed jh dxy obtained 
see experimental section small samples impose optimistic bias vanishes sample size grows 
fact experiments reported sections show samples size allow reasonably accurate estimates 
note dimensional distribution dimensionality increase grows 
hypotheses draw uniform distribution order obtain estimate distribution 
fhg jh dxy discrete distribution individual probabilities 
draw log ffi hypotheses chance mis estimating ffi 
statement strong say strongly error distribution influence quality estimate hl generalization error 
consider worst possible case occur 
suppose hypothesis space contains single hypothesis error rate zero exponentially fast growing number hypotheses error rate 
fail hit extremely hypothesis estimate fhg ed jh dxy single point mass 
estimate far true density converges single point exponentially fast impose strong inaccuracy estimated error rate hl true error rate hl zero certainty provided learner exhaustive finds isolated hypothesis exponentially fast growing set bad hypotheses theorem estimate error 
order avoid failures high confidence need draw exponentially fast growing number hypotheses estimate distribution error rates 
hypothesis languages certain property symmetry exploit achieve linear time 
suppose decision tree leaf nodes assume leafs unlabeled 
assigning combinations class labels zero leafs generate distinct trees equal stems differ labelings leafs 
exploit property symmetry construct algorithm prints distribution corresponding empirical error rates 
details algorithm see algorithm estimate fhg ed jh dxy section 
unfortunately estimator fhg ed jh dxy discussed unbiased 
model fhg ed jh dxy zero chance hypothesis incurring empirical error zero greater zero imposes bias 
fortunately bias vanishes sample size grows 
unbiased estimators exist considerable disadvantages estimator prohibitive variance second relies distributional assumption may easily fail 
see scheffer joachims detailed discussion 
fhg ed jh dxy assumed normal distribution parameters oe determined easily observed fhg jh 
assumption holds target boolean function attributes instances governed uniform distribution 
experiments artificial text categorization problem shown assumption fails frequently 
efficient model selection algorithm 
theorem implemented algo chapter 
expected error analysis rithm models record fhg jh drawing small number hypotheses random uniform distribution measuring empirical error 
fhg jh estimate fhg ed jh dxy theorem determine expected error erm hypothesis 
expected error estimated models select model lowest estimated error invoke learner selected model sample fitting 
learning curve function maps model index error hypothesis generated learner model learning problem 
learning curve grows large models generally referred fitting 
frequently considered due high hypothesis complexity models high 
fitting necessarily occur 
boosting algorithms exhibited complementary behavior schapire unpruned decision trees observed outperform pruned decision trees fisher schlimmer schaffer experiments support claim generalization ability learner property learning problem property intrinsic learner 
chernoff bounds guarantee high probability difference true empirical error hypothesis model exceeds certain threshold immediately leads worst case error bounds 
way pac vc theory argue 
empirical error binomially distributed poor hypothesis small chance depending sample size exhibiting low empirical error 
grows chance hypothesis exhibiting large difference true empirical error grows steeply 
hypotheses equal empirical error come distinct models pac theory gives better guarantees comes smaller model 
guarantees extremely pessimistic rely assumption learner selects erm hypothesis greatest true error rate 
expected error analysis implies prior distribution error rates model remains constant expected error returned hypothesis converges grows 
look expected error returned hypothesis hl model size jh approaches infinity formally lim jh fs ed jh dxy hl 
moment assume influential factors particular fhg ed jh dxy stay constant 
theorem fhg ed jh dxy constant fhg ed jh dxy expected error erm hypothesis hl converges jh grows 
lim jh fs ed jh dxy hl theta gamma dp fhg ed jh dxy gamma dp fhg ed jh dxy proof appendix theorem implies limit exists means learning curve converges fixed number diverging 
note consideration subject assumption fhg ed jh dxy remains fixed grows 
study fhg ed jh dxy behaves target boolean function 

fitting 
hypotheses contain attribute attributes attributes attributes attributes attributes various shapes fh dxy ed jh curves models contain boolean attributes target function requires attributes distributions equal models models contain irrelevant attributes incur smaller ratio hypotheses extremal error values causes greater expected error hl study fhg ed jh dxy behaves want learn boolean functions uniform distribution boolean instances 
case prior determined analytically 
target function uses attributes model contains hypotheses attributes prior certain binomial distribution depending function hypothesis agree possible ng instances distinguished target function hypothesis 
plugging exact prior theorem determine learning curve analytically 
appendix gives derivation expected learning curve expected uniform distribution boolean functions contains boolean functions attributes 
shows examples distribution hg ed jh target dxy random variable draw targets random dxy uniform distribution boolean functions attributes set functions attributes impose equal distributions hg ed jh tails distribution intuitively concentration really really bad hypotheses decreases hypotheses incur error close causes learning curve rise shows learning curve boolean functions simulation curve shows error measured experiment averaged randomly drawn target functions 
slight deviation originates sources simulation curve measured experiment subject inaccuracy independence assumption empirical error causes modest bias simplification assumption implementation incurs small error 
learning curve predicted accurately 
knowledge time mathematical model generalization quantitatively predicts shape learning curve 
prediction indicates expected error chapter 
expected error analysis number attributes predicted learning curve learning curve measured simulation learning curve expected generalization error theoretical values values measured simulation target function requires attributes model horizontal axis uses attributes analysis provides understanding nature fitting 
robustness inaccurate estimates jh virtually practically relevant hypothesis languages bounds language size known exact size 
expected error analysis applied cases exact size models known 
assume number independent hypotheses overestimated factor effect estimated error hl shows inaccurate error estimate depending model size 
topmost curve displays error estimate actual size lower curves show estimated error model size estimated factor respectively 
order obtain curves fixed fhg ed jh dxy jh experiment section plotted true error theorem various assumed model sizes fixed sample size 
generalization error converges exponentially fast exponential log jh doubly exponential jh threshold depends sample size error prior 
curves converge threshold difference converges zero exponentially fast 
fast convergence explains knowing precise size models necessary 
empirical studies section study bias variance error estimates obtained expected error analysis 
particular compare estimates estimates obtained fold cross validation 
set experiments section fixed target concept learn decision trees 
empirical studies error actual hypothesis size error hi error hi inaccurate error estimate mis estimate number independent hypotheses 
topmost curve shows true error function jh horizontal axis units log jh vertical axis shows error second curve shows error actual number independent hypotheses half estimated number third curve gives error estimate estimated number hypotheses times true number 
continuous discrete attribute 
set experiments section study boolean functions 
run large number model selection experiments target functions randomly drawn boolean functions 
able claims performance possible boolean target functions 
controlled experiments sections relatively small scale 
order study algorithm scales study text categorization problem section 
learning task relevant attributes examples 
applying cross validation problem feasible 
artificial problem designed model selection problem true error hypothesis calculated 
chose instance space continuous discrete parameter 
distribution dxy consists overlapping gaussians class amount overlap gives rise nonzero intrinsic target noise 
chose hypothesis space consists decision trees fixed split resulting hypotheses 
contains binary decision trees split discretized different values different trees 
entails binary decision trees depth hypotheses consist ary split binary split results 
table shows true error values table predictions fold cross validation 
predictions small samples poor generally variance predictions quite high 
table shows predictions expected error analysis 
predictions sample size poor instances estimate prior accurately bracketed values 
accuracy error rate estimates comparable accuracy fold cross chapter 
expected error analysis sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma table true error rates 
sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma table fold cross validation error rates 
validation estimates provided prior estimated instances 
sample size small fhg ed jh dxy estimated accurately estimate expected error analysis estimate optimistically biased 
learning boolean decision trees results reported previous section due intrinsic properties fixed target distribution 
section report set experiments consider different target functions drawn randomly space boolean functions 
stratification contains boolean decision trees boolean attributes total possible attributes 
assumed uniform distribution dx instances 
figures show learning curves boolean decision trees sample sizes respectively 
figures target function attributes 
figures compares curves 
average learning curve simulation labeled simulation randomly drawn functions 
curve gives unbiased estimate expected learning curve subject variance runs conducted 
second labeled predicted error exact prior learning curve predicted expected error analysis prior fhg ed jh dxy determined analytically curve section error prior boolean functions determined 
curves show small pessimistic bias caused independence assumption 
third labeled predicted sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma table expected error analysis estimates 

empirical studies model predicted learning curve estimated prior learning curve measured simulation predicted learning curve exact prior expected learning curves boolean functions attributes relevant sample size 
error estimated prior predicted learning curve prior distribution error values fhg ed jh dxy estimated recording fhg jh 
difference predicted curves exact prior curves measured simulation due assumption independent empirical errors variance error measurement simulation 
difference predicted curve exact prior predicted curve estimated prior due identification fhg ed jh dxy observed fhg jh 
small sample sizes estimate poor predicted learning curve shows considerable optimistic bias 
sample size grows estimated learning curve converges learning curve exact error prior 
experiments compare expected error analysis fold cross validation 
experiment depth target function drawn random uniform distribution target model target function drawn uniformly 
algorithm cv uses fold cross validation select model minimizes error chosen model complete sample 
true error returned hypothesis averaged runs different randomly drawn target functions 
expected error analysis algorithm predicts error model selects apparently best model minimizes empirical error model sample 
slightly different versions expected error analysis model selection compared version estimates error prior recording empirical error hypotheses model 
computationally expensive run learning algorithm approximately times faster fold cross validation version estimates error prior measuring empirical error rates randomly drawn hypotheses 
accomplished 
shows results table gives errors standard deviation numerically point averaged distinct target functions 
resulting error decreases course growing sample size 
sample size expected error analysis significantly better fold cross validation value cross validation better expected error analysis prior estimated drawing hypotheses 
error rate achieved expected error anal chapter 
expected error analysis model predicted learning curve estimated prior learning curve measured simulation predicted learning curve exact prior expected learning curves boolean functions attributes relevant sample size 
model predicted learning curve estimated prior learning curve measured simulation predicted learning curve exact prior expected learning curves boolean functions attributes relevant sample size 

scaling text categorization sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma cv sigma sigma sigma sigma sigma sigma table true error returned hypotheses target models drawn uniformly 
expected error analysis prior estimate model expected error analysis prior estimate randomly hypotheses fold cross validation model selection true error returned hypothesis uniform distribution target models horizontal axis sample size vertical axis error see table 
ysis model selection accurate error rate obtained cross validation model selection 
expected error analysis yields optimistically biased estimates small samples expected error analysis model selection algorithm performs 
blush may appear surprising 
note goal determine error values lowest harm error rates estimated constant bias 
principle advantage expected error analysis learning done estimate obtained extremely efficient manner 
scaling text categorization section apply expected error analysis framework problem text categorization 
text categorization salton problem mapping texts semantic categories 
important applications classification newspaper articles classification web pages 
cases large number available documents manual classification expensive 
problem difficult high quality result important task frequently performed manually 
instance yahoo web index created maintained manually 
documents represented sparse attribute vectors word stem occurs document feature 
linear classifiers features chapter 
expected error analysis weighted inverse word frequency salton 
clearly representation incurs loss information resulting classification key words 
cases word ordering relevant decide category text 
induced problem mapping feature vectors text categories difficult number attributes typically greater required sample size 
experiment reuters corpus joachims pre classified newspaper articles 
data set split training hold instances 
order apply expected error analysis problem steps carried 
selecting hypothesis language stratification 
experiments decision trees joachims indicated produced trees imbalanced 
order exploit apparent property problem chose decision lists fixed initial experiments dl infeasible due high computational complexity dl performed poor 
decision list rivest list hb arbitrarily rules conjunction literals class label 
way decision list assigns class label instance 
starting list instance passed 
rule body satisfied fires determines class label point instance passed 
rule fires instance assigned valid class label 
sorted attributes information gain accordance results yang petersen 
different 
stratification consists models 
models included decision lists attributes respectively 
second stratification hh contains decision lists monomials cover examples respectively note decision list sequence monomials monomial covers instances 
parameter adapted influences model allowing monomials supported certain sample size 
think number examples covered monomial pruning threshold adapted 
size dl known log gamma dl rivest 
adjusted multiplicative factor trials compared predicted generalization error hold error models 
order bias results different models category adaptation 
factor influences predicted error slightly wide range factors leads acceptable results see section 
defining learning algorithm 
learner minimize empirical error model decision list learner rivest runs theta number attributes number literals monomial sample size trials learner turned slow large sample 
furthermore rivest algorithm finds decision list consistent sample exists fails 
unfortunately minimizing empirical error difficult finding consistent hypothesis 
rivest algorithm greedy coverage algorithm back committed rule order find minimizing decision list conducting back tracking rules necessary 
back tracking rules feasible impose computational effort approximately average length observed lists 
settled greedy algorithm accepted drawback guaranteed find erm hypothesis 
modified algorithm furthermore reduces cost theta theta parameter determines different values empirical error rates algorithm distinguishes 
order determine empirical error rates efficiently algorithm uses inverse 
scaling text categorization indexing technique array words points examples word occurs 
algorithm dl input sample parameters number attributes pruning threshold parameter controls discretization empirical error rates 
output dl approximation erm hypothesis 

start empty list 
error bound steps lit lit rule lit lit complementary rule lit lit incurs empirical error error bound covers examples commit rule append rule remove covered examples 
point instances covered rule cover examples incur empirical error error bound eliminated sample 

point instances covered rule cover examples incur empirical error instances removed sample 

return estimating fhg ed jh dxy 
straightforward estimation prior error values drawing hypotheses require theta theta len 
length lists bounded number distinct rules drawing single decision list random feasible 
problem solved consideration prior estimated distribution fhg jh rules cover instance influence empirical error 
algorithm runs theta estimates prior theta randomly drawn hypotheses 
done drawing bodies lists length len recording error values len possible labelings class values rules 
algorithm estimate fhg ed jh dxy 

initialize 
contain estimate fhg ed jh dxy 

repeat times start empty list full sample 
empty draw example random draw attributes lit lit occur drawn example 
ii 
rule lit lit lit lit covered examples add rule remove covered examples 
randomly drawn decision list covers sample monomial covers examples 
chapter 
expected error analysis initialize 
pos neg refer number positive negative examples covered ith rule 
increment neg pos 
point gives number hypotheses consist body rule incur error 
len increment pos 
increment neg 
ii 
gives number hypotheses consisting bodies rules possible assignments class labels bodies incur empirical error 
iii 
set 
increment len theta 

return trick algorithm counts number hypotheses fixed bodies possible combinations class labels len len hypotheses 
done steps ia ib 
step number hypotheses divided total number hypotheses determine distribution 
results 
assessing model means expected error analysis requires approximately minutes pc running fold cross validation fixed model takes hours 
order able compare predicted error rates hold error rates commit relatively small set models 
set experiments wanted determine attributes relevant 
compared models containing attributes respectively 
measured hold error hypotheses returned learner examples determined error rates predicted theorem 
surprisingly learning curves curves hold errors predicted errors averaged frequent categories flat noise 
hold error predicted error pessimistic bias observed earlier 
attributes generalization error significantly lower attributes paired test 
predicted error model attributes lower predicted error models 
significant differences error rates attributes paired test 
expected error analysis prefers attributes significantly better worse attributes 
indicates attributes relevant single attribute carries little information 
set experiments second stratification 
models consist decision lists monomials cover certain number examples pruning threshold adapted 
table shows hold error incurred frequent concepts table shows error rates predicted expected error analysis 
hold set quite large examples differences significant 
shows predicted error rates error rates estimated fold cross validation averaged frequent categories 

scaling text categorization category acq corn earn crude grain interest money fx ship trade wheat average table hold error rates depending pruning threshold attributes 
category acq corn earn crude grain interest money fx ship trade wheat average table predicted error rates depending pruning threshold attributes 
predicted values subject pessimistic bias observed previous experiments shapes learning curves fairly similar 
note constant bias undesirable wants know just accurate particular hypothesis harmless wants know hypotheses best 
blush model appears incur lower averaged hold error model average predicted incur lower generalization error expected error analysis 
closer look turns categories expected error analysis hold testing agree models incurs smaller error 
furthermore paired test shows averaged categories model superior model differences hold error rates models significant individual categories 
results promising expected error analysis provides estimate generalization error 
estimate obtained extremely efficiently 
assessing large number models problem feasible means hold testing expected error analysis hundreds models assessed 
chapter 
expected error analysis error pruning threshold hold error rates frequent categories error rates predicted expected error analysis learning curves measured fold cross validation expected error analysis averaged frequent categories 
discussion showed expected true error erm hypothesis characterized function prior distribution error rates uniformly drawn hypotheses 
shows prior fhg ed jh dxy learning problem extremely elaborate sample size model size contains information necessary determine exact probability distribution generalization error hl yields tight error bounds fs ed ffl ffl dp sg ed jm dxy hl expected error fs ed jh dxy hl particular learning problem pac vc style bounds hold worst possible target distribution 
difference classical bayesian analysis expected error analysis clearly kept mind 
classical bayesian analysis posterior chance dxy js chance dxy sought target distribution sample prior dxy unconditioned chance dxy drawn target distribution 
contrast expected error analysis considers chance hypothesis particular error value 
order estimate prior dxy observe learning problems referred empirical bayesian analysis contrast target distribution dxy induces individual prior fhg ed jh dxy estimated sample 
learning curves 
experiments show prior distribution error values fhg ed jh dxy known case target boolean function instances uniformly distributed analysis predicts explains learning curves precision achieved pac theory valiant statistical physics tishby frameworks 
reason analysis worst case anal 
discussion ysis considers distribution error values characterizes particular learning problem distribution learning problems 
explanation fitting expected error analysis provides substantially different explanation provided pac framework 
largest difference true empirical error hypothesis hypothesis language grows necessarily hypothesis language grows 
pac theory gives weaker guarantees hypotheses learned larger models considered justification occam razor principle blumer 
learning algorithms practice shown patterns behavior deviate considerably pac results instance generalization error boosting algorithms freund schapire observed continuously decrease number elementary hypotheses hypothesis complexity increases schapire quinlan similar behavior observed hypothesis languages fisher schlimmer schaffer 
expected error analysis shows generalization error primarily depends prior distribution error values hypothesis language 
distribution stays constant hypothesis space grows generalization error decrease 
variance distribution decreases ratio hypotheses extremal error values decreases causes elevated generalization error 
variance error prior decreases instance irrelevant attributes added 
model selection 
prior estimated sample expected error estimate poor optimistically biased sample small estimate prior distribution 
sample sizes expected error analysis estimates reasonably accurate 
large sample sizes error estimate turned pessimistically biased assumed empirical errors distinct hypotheses independent estimates corresponding true error pessimistic bias constant level 
fold cross validation appear preferable task determine precise accuracy hypothesis 
large samples cross validation unbiased comes approximate confidence bounds 
task determine hypotheses learned distinct models better constant bias harmful 
experiments randomly drawn boolean function expected error analysis estimation model selection turned accurate fold cross validation model selection small samples 
expected error analysis algorithm efficient cross validation learning done 
text categorization problem relevant attributes examples estimate learning curve quite accurate obtained extremely efficient manner fold cross assessment single model single category required hours 
approach model selection particularly interesting large scale model selection problems text categorization knowledge discovery databases hundreds thousands relevant attributes tens thousands examples 
advantage expected error analysis cross validation increases number attributes available sample size increases 
limitations 
analysis subject primary restrictions 
model size required finite 
decision trees decision lists dnf similar languages lie scope approach neural networks lie outside 
second loss function restricted zero loss generalization error 
solution linear cost functions corresponding algorithm slow practical purposes 
far solutions loss functions quadratic loss 
cases analysis applied situations applied 
primarily case additional background knowledge automatic model selection unnecessary 
prior distribution target functions chapter 
expected error analysis distributions known choosing bayes hypothesis sub optimal 
bayes hypothesis computationally intractable heuristics map mdl hypothesis exploit additional knowledge promise better results 
task determine error rate particular hypothesis accurately possible opposed model selection task wants determine hypotheses best leave cross validation able obtain estimate small bias expected error analysis shown bias 
related results 
attempts find efficient means assessing models 
potential benefits analysis ed jh dxy discussed wolpert 
bias variance decompositions analyses error hypothesis hl returned learner split expected loss intuitively meaningful terms actual terms differ slightly breiman 
term bias error rate optimal hypothesis model variance term error error caused choice sub optimal hypothesis due sample 
analysis kohavi wolpert intrinsic target noise oe expected loss bayes optimal hypothesis bias quantifies learner guesses target average variance measures returned hypotheses hl differ possible samples learner invoked repeatedly 
bias variance decompositions known quadratic loss function geman zero loss kong dietterich dietterich kong kohavi wolpert 
bias variance decomposition serves tool analysis learning algorithms efficient means estimating error incurred learner particular problem 
estimating terms decomposition requires run learner repeatedly 
contrast analysis chapter leads solution evaluated efficiently 
domingos solution expected error hypothesis empirical error additional assumptions 
assumes hypotheses model equal true error hypotheses distinct models may distinct errors 
additional assumption simplifies solution considerably depends number hypotheses observed empirical error 
input solution empirical error returned hypothesis number considered hypotheses 
domingos idea process model selection calls approach characterize expected error returned hypothesis hl generated learning process necessarily simple error minimization process 
certainly considerable practical importance find solutions error hypotheses generated instance greedy algorithm back propagation 
far processes solution known error minimization hypotheses equal true independent empirical error error domingos error minimization hypotheses independent true empirical error scheffer joachims error minimization hypotheses arbitrary true error independent empirical error values chapter 
summary ffl expected generalization error hypothesis minimizes empirical error expressed function fhg ed jh dxy distribution error values hypotheses model 
ffl estimate error prior fhg ed jh dxy obtained recording 
summary fhg jh empirical counterpart 
estimate asymptotically consistent 
resulting generalization errors models compared model chosen efficiently 
ffl resulting model selection algorithm comparable fold cross validation terms accuracy superior cross validation experiments boolean decision trees show experiments text categorization show easily scales problems attributes examples 
ffl increasing size hypothesis space se cause fitting 
fact growing hypothesis language distribution error rates stays constant leads decrease error 
increase error referred fitting occurs distribution error values changes frequency hypotheses extremely low high error decreases variance fhg ed jh decreases 
instance occur irrelevant attributes added hypothesis language 
chapter assumptions justify model selection pac vc theory quantify worst case bounds difference true empirical error hypothesis terms size vc dimension hypothesis space sample size vapnik chervonenkis valiant haussler 
larger hypothesis language greater largest difference true empirical error hypothesis language 
motivates practical learners give preference restricted subspaces hypothesis space 
occam algorithms weak implementations idea hypotheses consistent sample return respect particular ordering stratification hypotheses 
occam algorithms obtain better estimate returned hypothesis true error provided consistent hypothesis model small index 
practical learners accept higher empirical error reduces complexity hypothesis leads better estimated generalization error 
technically done employing pruning regularization techniques neural weight decay mingers cun conducting cross validation stone kohavi john 
model selection approaches usually lead tighter bounds error impose risk missing hypotheses excluding hypothesis space 
positive results power model selection known kearns ng unfortunately strong prove conducting model selection opposed simply minimizing empirical error beneficial 
empirical results kohavi john show cross validation improves performance learners studied problems experiments schaffer theoretical analysis wolpert property particular problems studied property model selection 
chapter study occam algorithms cross validation beneficial quantify performance gains losses obtained cross validation 
results chapter hoffmann scheffer 
bounds performance model selection section presents results limitations power model selection strategies 
results contradict general belief usefulness model selection guides construction practical learners 
claims free lunch theorem see section wolpert 
intuition theorem sample instances just concepts classify instance concepts classify instance 
averaged possible target concepts predictions 
bounds performance model selection class instance included sample equally bad 
remaining free lunch theorems implies kind generalization relies heavily meta physical assumptions physical reality assumed patterns behavior natural concepts unobserved samples corresponds partial alignment assumed actual prior 
immediate corollary theorem true errors hypotheses consistent sample equally bad averaged target concepts 
corollary rewrites claim free lunch theorems model selection algorithms generalization error 
definition consistent hypothesis hh stratification sample 
consistent hypothesis consistent model occurs contains consistent hypothesis 
corollary 
hl consistent hypotheses respect orderings hh hh respectively uniformly averaged target concepts generalization error rates hl hl equal jf edx jf edx 
proof 
consistent hypotheses equally accurate sample instances 
claim theorem explains hypotheses equally accurate averaged target concepts 
choose equal potential hypothesis language stuck single model corollary claims conducting model selection learning available hypothesis language just occam algorithm 
appears idea return hypothesis consistent sample originates restricted subset early stratification hypothesis space error estimate better small sets hypotheses intuition misleading restricted completely relative term 
whichever hypotheses chose inhabit models small indexes difference information prior 
note situation description length defined prior chose arbitrary encoding scheme 
corollary major impact consistent learning algorithms support vector machine support vector machine uses stratification defined terms hyper plane positive negative examples 
vc dimension structural risk minimization framework corresponds model index chapter 
consistent hyper planes inflated feature space svm returns maximizes margin positive negative instances 
corollary claims consistent hypotheses just equally average 
implies propositions equivalent svm better average learner concepts characterized terms hyper plane space polynomials wide margin distinct centers positive negative instances frequent nature 
study general case learners restricted hypothesis consistent sample learners employ cross validation pruning techniques 
theorem claims prior target concepts known better just minimize error hypothesis space 
theorem hoffmann scheffer return arbitrary hypothesis hl empirical error 
arbitrary learner cross validation learner returns hypothesis hl 
uniformly averaged target functions ff sg ed jm ff sg ed jm 
chapter 
assumptions justify model selection proof 
sample errors hl hl average equal follows theorem generalization error hl equal true error hl sample error hl minimal 
second statement theorem follows corollary 
occam algorithms corollary claims bayesian prior known return hypothesis consistent sample 
hand prior known bayes rule find bayes hypothesis guaranteed generalization error 
partial knowledge prior available situation studied robust bayesian learning berger prove occam algorithms better pac learners return arbitrary consistent hypotheses 
fact follows free lunch theorems knowledge prior learner perform better algorithm returns random consistent hypothesis 
order prove need tweak definition pac learning include distributions target concepts 
learner required produce high probability guess drawn respect distribution 
require learner find hypothesis high probability learner prior information addition concept class pac learner learner provided class distributions class may contain element case bayesian scenario known prior potentially infinitely 
learner required perform respect dx definition class target functions class distributions learner accepts values ffi sample learner learns sample size iff distribution dx fs fg edx jm dx ffi drawn dx learns polynomially sample size runtime bounded polynomially size parameter ffi relationship pac learning 
definition valiant haussler class target functions 
pac learner accepts values ffi sample size learns sample size iff dx fsg edx jm dx ffi drawn dx pac learns polynomially runtime sample size bounded polynomially size parameter ffi theorem hoffmann scheffer pac learner class target functions learner class distributions functions proof 
distribution produce functions pac learner guaranteed learn function high probability error 
section assume implies consistent sample positive results apply class target functions stratified chunks chance target function originating chunk small index higher chance lying chunk high index 
actual prior 
occam algorithms concepts chunks constrained need known learner 
results refer learner produces consistent hypothesis respect stratification hypothesis consistent sample comes model smallest index contains consistent hypothesis 
theorem hoffmann scheffer stratification ffg jh learner produces consistent hypothesis learn fs fg edx jm dx ffi sample size log ffi proof 
shall prove situation theorem edx jm dx gamma hl consistent hypothesis respect stratification sample size 
chance hypothesis consistent sample model obviously chance hl probability target concept implies hypothesis consistent sample 
generally chance earlier model contains consistent hypothesis gamma edx jm dx gamma jh gamma jg gamma gamma proves edx jm dx gamma order find sample size bound choose edx jm dx equation ffi 
bound theorem suffices gamma log ffi ffi theorem error required sample size bounded size model number models actual size hypothesis space infinite infinite vc dimension irrelevant error 
decrease required sample size due stratification simply due fact learner allowed fail ffi target functions pac learner reliably target functions 
consider naive learner allowed incur high error chance ffi argue ignore ffi functions learn remaining functions confidence gamma ffi know ffg restrict ourself learning log ffi models 
means learner picks arbitrary hypothesis consistent sample set theta log ffi hypotheses 
learner constrained models obtain error bound theta gamma learner incurs error chance due ignoring functions outside models chance theta gamma due choosing suboptimal hypothesis chapter 
assumptions justify model selection standard pac theory consistent sample 
order incur error probability ffi need sample size ffl log log ffi ffi note bound theorem lower size hypothesis space depend ffi 
choosing arbitrarily small ffi implied upper bound sample size ehrenfeucht exceed lower bound theorem 
shows significant part saving required sample size due partial alignment stratification prior 
theorem assures learning models equally small focus setting models growing ith model size finitely different models 
resembles typical model selection situation increasing description length hypotheses adding attribute doubles cardinality hypothesis language finitely attributes 
theorem hoffmann scheffer stratification hm ffg ffg jh learner produces consistent hypothesis learn edx jm dx ffi log mk ffi proof 
shall prove ed jm dx gamma analogously proof theorem argue chance containing hypothesis consistent sample gamma edx jm dx gamma gamma gamma gamma mk gamma sample size log suffices mk gamma log mk ffi ffi study setting models grow exponentially infinitely models 
case instance pattern languages angluin 
idea learner cares log ffi models 
target concept fall models chance happening ffi learner returns arbitrary hypothesis 
incurs certain error potential sources errors exceed probability gamma ffi 
theorem hoffmann scheffer stratification ffg ffg gamma jh learner produces consistent hypothesis hypothesis consistent sample log ffi models arbitrary hypothesis learn dc log log ffi ffi 
cross validation proof 
intuitively learn concept class size log ffi confidence ffi cause error ffi ignoring concepts outside restricted class occur probability ffi 
formally show chance exceeding error resolves ffi assign specified theorem set log ffi ignoring functions models causes large error probability ffi edx jm dx log ffi gamma log log ml ffi ffi ffi ffi completes proof 
cross validation theorems section claim prior target function alignment stratification doing error minimization hypothesis language harmful occam algorithms just algorithm 
theorems section show partial alignment stratification partially known prior occam algorithms perform considerably better error minimization algorithms bias 
far studied cases beneficial accept higher empirical error traded smaller model index 
section quantify expected error fold cross validation model selection referred training test hold testing erm learners identify cases cross validation beneficial 
previous section showed alignment prior preference particular hypothesis guarantees generalization 
general cross validation setting target function guaranteed member model may lie outside 
may occur hypothesis low generalization error posterior probability sought target function zero inconsistent data fsg sjf 
central analysis section prior distribution error values model fhg ed jh dxy 
sample split training set hold set assume considering model learner perceives jh different empirical error values hypotheses empirical errors learner draws uniformly written 
analogously chapter models assumed finite learner assumed minimize empirical error 
determining hypotheses empirical error training sample learner determines hold errors 
model incurred hold error selected empirical error minimized learner sample hl determined 
arbitrary distribution dxy jx dx training sample size holdout sample size stratification hh models fixed 
dxy define distribution fhg ed jh dxy true error values hypotheses dxy induces distribution samples drawn dxy fhg ed jh dxy distribution samples lead distribution em chapter 
assumptions justify model selection errors fs hg jm dxy hypotheses model 
true error corresponding empirical error binomially distributed distribution empirical errors determined integrating true errors fs hg jm dxy ed dp fhg ed jh dxy 
learner draws hypothesis uniformly set erm hypotheses 
defines distribution error rates erm hypothesis write fs ed jh dxy 
expectation error fs ed jh dxy dp fs ed 
point learner determined hypotheses model true errors distributed fs ed jh dxy fs ed jh dxy 
distributions distinct 
distribution hold examples size induces distribution hold errors fs jh dxy ed dp fs ed jh dxy 
learner selects model hypothesis incurred hold error breaking ties favor small indexes 
returns hypothesis hl minimizes error sample selected model induces distribution true error values hypothesis hl fs ed jh dxy hl 
expected error returned hypothesis hl fs ed jh dxy hl 
shall quantify abovementioned distributions 
theorem chapter quantifies true error erm hypotheses theorem quantifies error returned hypothesis hl theorem hoffmann scheffer distribution true error rates hypothesis hl minimizes sample error size model selected hold testing training hold set function distributions errors fhg ed jh dxy models hh jh assumptions 
fs ed jh dxy hl fs ed jh dxy fs fixed random variables drawn dxy dxy fs ed jh dxy distinct distributions defined theorem 
furthermore fs fs jh dxy theta fs jh dxy theta fs jh dxy fs jh dxy ed dp fs ed jh dxy fs ed jh dxy theorem 

case study boolean functions proof 
equation follows training test learning procedure true error hl equal error minimizes error sample model times chance model selected 
equation quantifies chance model selected 
happens hold error learned sample model hold error models strictly greater hold error model greater equation incorporates assumption hold errors erm hypotheses independent depending corresponding true error 
practical benefit theorem 
learning problem theorem provides answers questions 

potential hypothesis language language stratified models hh expected error rate minimized 
obtain answer trying different 
stratification estimate distribution error values model sample input distributions theorem 
decision estimates generalization error rate hl returned theorem 

stratification learning problem split sample training hold set 
decision generalization error rates estimated theorem split 

stratification hh conduct hold testing model selection simply minimize empirical error rate union models comparing outcomes theorems estimate option lead lower error rate 
case study boolean functions theorem quantifies error rate learner uses training test model selection 
unfortunately solution relatively complicated provide simple answers questions regarding construction learner 
error priors known theorem predicts resulting error rate 
particular learning problem error priors estimated data 
certain problems problem learning randomly drawn boolean function error priors determined analytically 
section study problem learning randomly drawn boolean functions 
model selection task guess number relevant attributes 
problem similar situation studied section 
target distribution dxy induces set error priors fhg ed jh dxy model jp dxy chance observing error priors dxy case uniform distribution boolean functions attributes 
fs hl dxy ed hl jh hl fs hlg ed hl jh hl dp jp dxy chapter 
assumptions justify model selection ratio data training attributes attributes stage learner attributes attributes stage learner attributes attributes stage learner expected error cross validation model selection learning model selection depending training hold split 
uniformly drawn boolean functions dxy fs dxy ed hl determined appendix plugging theorem determine ed jm hl expected error possible targets distributed dxy 
shows expected error values various sample sizes training test splits 
clearly see optimal training test split ratio fixed varies number attributes sample size accordance results kearns 
see error rates model selection learning uniformly greater error rates obtained learner uses potentially available hypothesis space 
irrelevant attributes setting surprising 
best cross validation select attributes learner conduct model selection place 
picture changes attributes irrelevant see 
error rates model selection learning independent number attributes error learner conduct model selection increases dramatically number irrelevant attributes grows 
demonstrates model selection learners presence irrelevant attributes 
similar observations instance ng 
ng gives error bound cross validation model selection contains hypotheses attributes number irrelevant attributes occurs logarithmically 
discussion related results model selection techniques pruning regularization moody cross validation structural risk minimization vapnik generally considered lead generalization errors 

discussion related results ratio data training attributes relevant cross validation attributes relevant stage learner attributes relevant cross validation attributes relevant stage learner attributes relevant cross validation attributes relevant stage learner expected error attributes irrelevant 
general belief empirically supported claims kohavi john cun mingers 
empirical studies schaffer theoretical analysis wolpert contradict general belief render model selection particular learning bias fail problems perform 
chapter picked qualitative results specified learning problems model selection appropriate bias quantified gain model selection yields situations 
characterized distributional assumptions target problems occam algorithms cross validation appropriate bias quantified performance gains losses occam algorithms cross validation 
turns occam algorithms perform better pac learners knowledge prior distribution target functions encoded stratification 
general cross validation setting target function assumed model best hypothesis may consistent sample 
bayesian prior posterior fsg sjf helpful situation hypothesis inconsistent sample certainly sought target function small true error may solution learning problem 
prior fhg ed jh dxy error values model extremely elaborate situation 
prior theorem quantifies expected error fold cross validation model selection compares error learning model selection 
distribution error values equal models learner simple error minimization superior learner conducts cross validation model selection 
ratio hypotheses higher models case attributes irrelevant model selection performs 
note contrast known positive results model selection kearns kearns ng prove model selection better simple error minimization 
expected error analysis quantifies expected error hypothesis returned cross validation function error chapter 
assumptions justify model selection distributions model 
theorem claim optimal training test split particular problem specified terms distributions fhg ed jh dxy 
problem finding optimal training test split studied intensely 
unfortunately answer optimal training test split simple 
surprising previous results showed resulting error rate depends factors learning problem kearns muller 
contributing better understanding model selection benefits results practical implications 
theorem decide learning scenario described terms distributions fhg ed jh dxy fold cross validation learning model selection preferable 
second decision model selection theorem determine optimal training test split 
summary ffl occam algorithms require ordering hypotheses 
hypotheses consistent sample prefer respect order 
enables occam algorithms give better bounds largest difference true empirical error hypothesis model 
order chosen arbitrarily order aligned prior distribution target functions hypotheses consistent sample incur equal error average renders occam algorithms useless 
similar theorems proven provide evidence model selection absence additional distributional assumptions 
ffl partial knowledge distribution target functions available known member particular class occam algorithms give preference hypotheses perform far better learners conduct model selection 
ffl conducted analysis expected error hypothesis returned learner conducts fold cross validation called training test hold testing 
solution provides optimal training test split problem allows determine learning problem better conducting model selection 
input analysis set distributions fhg ed jh dxy error rates hypotheses models hh estimated sample determined analytically 
chapter assessment learning algorithms empirical assessment performance learning techniques respect sets benchmark problems topic experienced attention ml community 
performance translates expected generalization accuracy 
estimating generalization error respect particular set benchmark problems collection statlog data sets uci repository machine learning data sets murphy aha implies assumption studied learning algorithms applied similar problems 
remember free lunch theorems see section imply uniform distribution target function learning algorithms distinct learning bias perform just equally 
means learner better learner problem problem better empirical studies help reveal learning problems particular learning technique suited 
learning techniques usually assessed empirically means hold testing fold cross validation stone toussaint 
discussed cross validation section 
available data set small error estimated reliably fold cross validation hold testing 
quantifying performance best learners 
virtually practical learning algorithm possesses number parameters learning rates number learning steps pruning thresholds 
selecting values parameters model selection task considered part training process 
unfortunately parameters adjusted error hold set case fold cross validation averaged error hold sets minimized 
error test set quality criterion model selection task hold set influences training process 
assumption hold sets learning essential result fold cross validation bias free violated 
happens similar happens learning algorithm selects hypothesis set potential hypotheses minimizes empirical error sample 
hypotheses resulted parameter settings assessed optimistically hypotheses certain parameter settings assessed pessimistically 
parameter setting minimized hold error cross validation error optimistically assessed parameter setting 
hold cross validation error estimate learner expected generalization error incur optimistic bias 
chapter quantify bias 
quantification practical relevance bias turns negligibly small results obtained particular experimental setting reliable 
expensive experimental setting uses nested cross validation chapter 
assessment learning algorithms discussed section 
chapter assume generalization error learner binary parameters estimated parameter adapted time 
note results easily adapted learner parameters take values setting log note learner may continuous parameters finitely settings tried 
chernoff bounds section simple chernoff bounds bound true generalization error best parameterizations assessed hold set 
simple result resembles pac style bounds error best hypotheses 
theorem learner possess binary parameters 
parameter setting leads error hold set size error best learners bounded follows ed gamma jm dxy theta gamma consequently probability gamma ffi hold error guaranteed ed gamma jm dxy ffi hold sample size log log ffi proof 
equation follows chernoff inequality 
regarding equation theta exp ae gamma log log ffi oe theta exp ae gamma log ffi oe theta exp ae gamma log ffi oe ffi completes proof 
suffices hold sample grows linearly number parameters 
required hold samples relatively large estimates reliable 
consider examples 
ffi theta ffi theta ffi theta ffi theta ffi theta ffi theta 
information theoretic approach training set learning algorithm hypothesis test set parameter adaptation fold cross validation error test set parameters channel information allows test set influence learning process accuracy model selection accuracy estimation mixed parameters communication channel delivering information hold set learning algorithm 
large hold samples usually available data mining text categorization problems 
discouraging results criticized reasons 
results cover advantage fold cross validation hold testing 
difference may significant particular sample small 
second theorem worst case result 
particular problem bias may considerably smaller 
order overcome problems pursue information theoretic approach treat dependencies results distinct cross validation folds explicitly refer properties data set worst case 
information theoretic approach parameter adaptation performed hold sample error estimation parameters form communication channel hold set learning algorithm see resulting bias depends capacity channel number parameter settings number different parameter settings tested 
sections consider different experimental settings section quantifies bias shot training test section quantifies bias fold cross validation different parameter settings trials section dedicated cross validation equal parameter settings trials 
section discuss experimental setting yields unbiased ranking experiments 
results chapter scheffer herbrich 
section assume setting 
learning algorithm accepts set parameters distinct parameter settings algorithm possesses log parameters possible values 
set parameters viewed communication channel parameter optimizer learner capacity bits 
illustrates learner set parameters training set generates hypothesis determine accuracy hold set size parameter optimizer told accuracy responds new set parameters training 
cycle repeated times best observed accuracy hold chapter 
assessment learning algorithms set submitted publication 
accuracy measured hold set parameter optimizer send bits information hold set learning algorithm 
entropy hold set confuse entropy hypothesis space abbreviated preceding chapters capacity parameter channel allows transmit information class labels hold examples bits required encode class label example hold set contains uniformly distributed class labels entropy allowing parameter channel width distinct settings transmit class labels hold objects 
intuitively parameter channel bits accounts bias strong told learner class labels hold examples 
knowledge improve result hits knowledge learner necessarily failed examples 
quantify actual gain hits hold examples separately hold testing fold cross validation 
describe theoretic algorithm achieve difference true hold error rate 
parameter adjustment parameter optimizer guesses parameters obtains accuracy hold set return 
assume learning algorithm passes parameters hypothesis uses information classify hold objects encounters particular way parameters encode class labels hold objects 
parameter optimizer strategy 
objects labels jy tell learner classify ith example class determine empirical error hold set 
keep label class value resulted lowest hold error 
algorithm tries assignment possible class label hold examples leaves class labels remaining examples fixed underlying hypothesis resulting complexity delta jy learning trials set class labels finds parameter setting encodes correct class labels samples 
view algorithm greedy search optimal learning parameters correct class label example independent assigned class label sample greedy algorithm find optimal assignment delta jy trials 
shot training test told class labels hold objects knowledge improve accuracy hold set 
assume initial classifier learned note lower bound number trials algorithm needs gamma gamma trials worst case algorithm essentially performs gradient search parameter space faster algorithm behaves expect parameter optimizer behave result somewhat closer behavior real learning algorithm 

shot training test quinlan say classifies objects hold set size correctly 
classifying instances reflect correct class labels determined parameter adaptation procedure 
classify remaining gamma instances number hits gamma number hits obtained hold instances classified correctly 
probability gamma procedure increases number hits 
drawing examples total know hits classified correctly 
number hits drawn examples follows hyper geometric distribution note finite 
drawn examples replaced hits parameter optimizer discloses class labels parameter channel 
gamma gamma gamma gamma gamma gammak delta gamma gammap delta gamma delta leads bias computational effort explained section required achieve bias shot training test 
expected number learning experiments need conducted order obtain hits hold examples probability gamma true hit rate provided parameter channel bits capacity 
gamma gamma gammak delta gamma gamma delta gamma delta jy delta affected benchmark problems section sections quantify bias concrete data sets 
assume uses real learning algorithm cases tuned additional parameters order pretend outperform learning algorithm rank higher initial learner statlog performance chart michie 
answer questions parameters needed succeed probability performing sufficiently trials trials needed average obtain result provided parameter optimizer performs gradient search parameter space may inexact result depends actual optimizer strong parameters influence result 
land sat satellite images data set contains training hold instances 
default error rate 
michie ranked th error hits hold set 
ranked th outperform bay tree error needs extra hits hold set 
class labels instances chapter 
assessment learning algorithms known gamma 
need delta trials different parameter settings need parameter channel bits 
automatic parameter adjustment system may run trials parameter channel bits fairly uncommon achieved parameters possible values 
examples parameter channel bits experiments chance ranked 
dna data set described michie possesses training hold instances 
ranked th hits 
ranked th outperform requiring extra hits 
need told class labels achieve gamma 
classes need delta trials 
need bit parameters parameters possible values 
data sets eager scientist may obtain modest ranking algorithm incremental algorithm automatic parameter adjustment procedure 
fold cross validation parameter adjustment section study bias caused parameter adaptation fold cross validation conducted parameter values chosen differently runs learning algorithm 
example number learning steps crucial performance back propagation hinton 
optimal number learning steps determined observing error hold set selecting point error rate starts increasing 
minimum errors occurred learning curves averaged published number learning steps may fixed value folds 
setting splits training set explicitly stated mesh vehicle silhouettes 
achieve average extra hits fold way theta equal gamma number hits lost examples fold nc gamma nz nc nz nc gamma nc nz nc gamma unfortunately explicit formula distribution sum hyper geometric random numbers 
probability sum random numbers split considering possible combination yields sum gamma 
equation decompose summand resulting recursive equation gamma gamma xm 
instantiated situation yields nc gamma nc gammak gamma nc gamma gamma nc gammak gamma nc gamma gamma nc gammak gamma nc gamma gamma gamma delta gamma gammap gammal delta gamma delta 
fold cross validation fixed parameters straightforward evaluation recursive equation instantiation expensive evaluation gamma calculated 
iteratively filling array indexed formula evaluated quickly 
affected benchmark problems fem mesh design muggleton relational problem popular inductive logic programming lavrac dzeroski 
explicitly split learning problems 
examples classes entropy 
foil quinlan achieves accuracy hits 
achieve accuracy probability foil needs class labels achieve probability foil need class labels 
achieve need conduct trials parameter channel bits achieve require trials parameter channel bits 
parameter adaptation bias strong accuracy results easily pushed data set 
prove actual accuracy learning algorithm lower claimed clearly shows accuracy claim validly empirically supported experiment 
results achieved parameter optimization different parameter settings folds strongly biased 
diabetes examples classes 
achieves hits rank need outperform additional hits fold 
need class labels succeed probability need parameter channel bits conduct trials 
experimental setting parameter optimization folds arbitrarily results easily achievable 
ranking results achieved setting distorted high probability 
fold cross validation fixed parameters section parameter optimizer able communicate class labels hold objects learner best parameter optimizer communicate frequently observed class label hold object folds objects 
example folds conducted hold object class parameter optimizer may tell learner hold object class results extra hit averaged folds 
probability number extra hits gained way takes value 
study problem class labels 
case position hold set choose class majority examples drawn position folds belongs 
gamma yg representatives class 
assuming significantly probability choosing examples binomially distributed probability default class frequent class data set gamma assumption probability hyper geometrically distributed number parameters needed determine class labels number examples gets close size hold set 
chapter 
assessment learning algorithms total number nz additional hits folds difference number extra hits explained paragraph number lost hits hypothesis calculated section gamma nz nc nz gamma nc nz nc gamma gamma determined follows gamma gamma gamma gammak gamma gamma gamma gammak gamma gamma gamma gammak gamma gamma gamma gamma delta gamma gammap gammaq delta gamma delta note situation probability ranked high depends hit rate default classifier hit rate initial hypothesis default hit rate high initial classifier performs poorly probability ranked high 
affected benchmark problems diabetes setting need parameters achieve extra hits fold probability 
increasing number parameters decreases probability expected number examples equal class labels position folds divided number folds small compared hit rate initial hypothesis 
experimental setting diabetes safe 
heart disease data set michie examples classes 
performs poorly problem need extra hits parameter channel bits trials sufficient succeed 
nn true learning algorithm hits fold need extra hits ranked rank better 
examples succeed probability 
due fact default probability worse initial hypothesis 
parameter channel bits required trials conducted 
strong bias data set 

unbiased assessment learning algorithm training hypothesis set parameter adaptation hypothesis fold cross validation estimated accuracy error training subset parameters fold cross validation hold set fold cross validation yields unbiased estimate resulting generalization error outer hold set single hypothesis 
unbiased assessment section want review unbiased ranking experiments conducted 
key confuse parameter adaptation error estimation 
possible way obtain estimate small bias fold triple cross validation norman 
setting split available data chunks repeat procedure times 
parameter settings wish try gamma chunks generate hypothesis chunk hold chunk obtain unbiased estimate generalization error 
select hypothesis obtained parameter setting imposed error rate hold chunk assess hypothesis second hold chunk available chunk 
repeat procedure times average error rates incurred second hold chunks 
procedure estimates generalization error hypothesis learned hold testing adapt parameters 
runs theta try parameter settings 
estimate subject small pessimistic bias gamma training data learning data hold testing learn adapt parameters data set slightly larger sets second hold set longer needed 
adapt parameters fold cross validation expect find better parameter settings hold testing 
algorithm shall refer fold cross validation instance applied kohavi john obtains unbiased estimate expected generalization rate learner problem sample size parameter setting minimizes fold cross validation error rate parameter settings 
algorithm fold cross validation 

split sample chunks 

repeat times minus ith chunk 
split chunks repeat times gamma chunks ii 
possible parameter settings tried invoke learner evaluate resulting hypothesis remaining chunk training 
determine parameter setting leads error hold chunk 
chapter 
assessment learning algorithms parameter setting determined parameter optimizer invoke learner providing 
determine ith hold error resulting hypothesis 
return average measured hold error rates 
illustrates parameter adaptation needs performed considering hold set 
order get reliable estimate optimal parameter settings outer loop fold cross validation evaluates hypothesis fold 
inner cross validation loop parameters optimized 
discussion results chapter clearly show adapting parameters accuracy hold set optimized causes optimistic bias depends number parameters number trials 
quantified bias observed sufficiently trials conducted learner optimal available parameters calculated expected number trials needed assuming parameter optimizer follows gradient descent search 
shows hard ranked shot training test situation hold set large 
fold cross validation situation probability ranked high difference default hit rate true accuracy low 
fold cross validation setting different parameter values parameters optimized locally highly ranked result achieved experiments conducted way yield valid results 
considerations prove learning algorithm perform worse claimed show claims validly supported experiments naive setting 
results constructive sense provided equations easily proven situations depending properties data set naive inexpensive experimental setting yields perfectly valid results 
results validation performance evaluations new learning algorithms difficult empirical results naive setting distorted compared obtained unbiased experiments 
heuristic modifications add new parameters may easily estimated 
modification improve true accuracy hypothesis new parameter may improve ranking results 
summary ffl averaged hold error cross validation error obtained particular learner sample slightly pessimistically biased estimate expected error learner problem 
subject small pessimistic bias sample training cross validation conducted 
bias minimized choosing leave cross validation 
ffl learner started distinct parameter setting observed cross validation error optimistically biased estimate corresponding learner performance just training error optimistically biased estimate returned hypothesis true error 

summary ffl parameters seen communication channel hold set learning process 
information theoretic approach quantify optimistic bias cross validation error 
ffl bias considered depends factors determined data set 
cases bias neglected shot training test situations hold sample large 
fold cross validation setting bias considerable default error rate higher error returned hypothesis 
cross validation fold allowed distinct parameter settings bias extremely strong 
earlier empirical results appear questionable results 
ffl order obtain unbiased estimate generalization error learner parameters optimized sample conduct nested loops cross validation 
parameters optimized inner loop error rate resulting hypothesis estimated outer loop 
cases expensive procedure necessary 
results show single loop cross validation yield reliable result 
chapter complexity issues model selection learning learner constrained fixed model determined model selection strategy 
complexity process finding minimizing element set hypotheses studied intensely see nilsson 
minimizing empirical error hypothesis language usually requires effort jh 
hypothesis languages jh polynomial size parameter number attributes 
case shallow decision trees dobkin auer pattern languages bounded length mitchell 
finding hypothesis consistent sample language exists difficult 
general worst case complexity jh algorithms run poly log jh languages conjunctive concepts valiant dnf valiant dl valiant linear threshold units blumer 
blush results show learning large model easier learning small model model selection learning algorithms complexity point view 
chapter discuss case 
demonstrate referring multilayer perceptrons fixed number hidden units example model selection learning boosting 
boosting technique growing hypotheses dynamically compounding elementary hypotheses majority voting 
show primarily restriction hypothesis space small model learning difficult 
support claim showing worst case time bound log adaboost algorithm solving problem static hypothesis space np complete 
results chapter scheffer stephan 
boosting suppose learning problem strategies available guaranteed perform just slightly better randomly guessing 
situation majority voting algorithms provide scheme combine weak learners powerful system achieve arbitrarily high accuracies 
intuition majority voting system give incorrect answer query half weak hypotheses wrong query 
weak learners perform slightly better random guessing chance half wrong time vanishes number weak hypotheses grows 
adaboost freund schapire implementation idea algorithm subsequently trains weak hypotheses combined hypothesis guaranteed produce 
definitions arbitrarily low empirical error 
stage adaboost gives high weight examples misclassified majority hypotheses gamma trains hypothesis weighted sample 
error incurred majority hypotheses error respect sample drops exponentially fast 
empirical studies uci repository machine learning data sets boosting algorithms related bagging algorithms breiman proven boost generalization accuracy decision tree learners quinlan order learners quinlan neural networks drucker 
empirical success boosting algorithms considered due particular geometrical bias induced majority voting adding weak hypotheses error training sample zero boosting maximizes margin positive negative instances space inflated new weak hypotheses added schapire 
resembles learning bias support vector machines vapnik cortes vapnik algorithm wysotzki wide margin classifiers 
support vector machines stratify hypothesis space hypotheses centered hypothesis maximizes margin polynomially inflated instance space positive negative instances 
hypotheses consistent sample svm returns approximately closest center measured terms vc dimension 
unfortunately analysis adaboost depends better random guessing weak hypotheses 
consequently known error bounds terms difference accuracy weak hypotheses accuracy random guessing 
worstcase analysis adaboost weak hypotheses perceptrons decision trees 
main result boosted perceptrons learned log sample size boosted decision tree guaranteed converge tree depth log considering result training neural network hidden output perceptron np complete blum rivest clearly points unnoticed benefit boosting growing hypothesis space dynamically adaboost maps search space larger space greedy algorithm exists decide existence hypothesis consistent sample consequently find hypothesis polynomial time 
analogous results obtained instance learnability boolean functions term cnf polynomially pac learnable polynomial algorithm find hypothesis consistent sample dnf learnable term cnf ae dnf pitt valiant 
reason greedy algorithm exists decides existence consistent hypothesis dnf 
definitions learning problem 
chapter assume unknown target function 
learner perceives sample consisting points corresponding returns hypothesis 
learner may access distribution sample points 
distribution confused underlying unknown distribution dx somewhat artificial distribution objective put higher weight examples misclassified previous elementary hypotheses 
putting higher weight instances maximize chance classified correctly newly learned elementary hypotheses eventually majority elementary hypotheses 
empirical error respect defined xs xs chapter 
complexity issues zero loss function 
adaboost 
algorithm adaboost freund schapire receives sample distribution points integer indicates desired number iterations 
algorithm initializes weight vector step proceeds follows 

weak learner invoked distribution returns hypothesis 
fi ffl gammaffl ffl error respect 
new weight vector set fi gammaf final hypothesis consists final weight vector fi weak hypotheses compound hypothesis weighted majority weighted fi individual hypothesis formally log fi log fi perceptrons 
perceptron discriminating hyperplane 
perceptron specified vectors define function fa bg ax accordingly boosted perceptron consists collection perceptrons weight vector weights may equal perceptrons 
outcome boosted perceptron point weighted majority perceptrons outputs 
shallow decision trees 
decision tree fixed depth minimizes error sample polynomial time decision trees depth learned log auer dobkin 
surprisingly positive finding error minimizing hypotheses half spaces conjunctive concepts kearns polynomial time 
shallow decision trees particularly interesting weak hypotheses boosting algorithms 
empirical studies decision trees depth decision stumps studied freund schapire breiman 
worst case bound adaboost perceptrons section study worst case behavior boosted perceptrons 
discuss worst case behavior weak learner perceptron 
examples linearly separable simple known greedy algorithm construct separating perceptron rosenblatt 
unfortunately general case finding half space minimizes error rate sample np complete necessarily mean learning boosted perceptron possible polynomial time 
suffices weak hypothesis finding hyper plane minimizes zero loss np complete finding plane maximizes squared distance positive negative examples easier unger wysotzki gives rise efficient piecewise linear classifiers wysotzki 
worst case bound adaboost perceptrons error slightly constructed polynomial time 
see easily done efficient manner 
prove demonstrating algorithm 
algorithm picks arbitrary example point constructs hypothesis classifies half sample correctly classifies correctly 
number correctly classified points exceeds number mis classified sample points 
lemma show hyper plane touches point constructed efficiently sample 
possible finitely examples fill space 
lemma point ir finite set points containing efficiently find hyper plane ir gamma containing point proof lemma 
loss generality assume coordinates obtained easily coordinate transformation 
constructed linear mapping subspace generated coordinates plane contains points 
contain definition guarantees plane disjoint points form 
linear function defined inductively values coordinates th takes 
goal construction define forced contain points th step contains points th coordinate non zero linear hull fz linear hull fz gamma number unique iff compute finite numbers define jf jf numbers follows elements induction disjoint lemma show efficiently construct perceptron achieves hit rate essentially pick point maximal weight sample construct hyper plane touches example 
switching sign perceptron achieve error rate sample fixing sign shift plane slightly forward backward falls right side plane sample point changes side 
obtain extra hit 
theorem scheffer stephan fx xm set data points class labels distribution efficiently find perceptron err xs gamma proof theorem 
point maximally large points maximize may gammap fx know lemma implies select hyper plane ax efficiently touches fx loss generality jaj normalize 
define perceptrons ax gamma complementary sample points look error gammap respect 
follows complementarity gammap jf gamma jf gamma 
chosen gammap jf gamma gammap jf gamma gamma chapter 
complexity issues implies minf gammap jf gamma gammap jf gamma jg gamma gamma hypothesis fh incurs error gamma points hypothesis 
disregarded possible misclassification smallest distance plane data point ffl lemma implies distance non zero 
define hypotheses ffl gamma ffl 
jaj definition ffl imply hypotheses behave equally points complementary classify correctly 
total error respect gamma unfortunately bound improved 
observation learning algorithm exists guaranteed find boosted perceptron error gamma data points respect distribution particular worst case complexity boosting learner gamma invocations weak learning algorithm 
proof observation 
assume assume odd mg 
define 
hypotheses possible gamma effectively split 
odd numbers gamma numbers error gamma 
gamma 
equally odd numbers gammac odd numbers gammac numbers hypothesis incurs error 
odd 
odd numbers equally odd numbers hypothesis incurs error gamma affine hypotheses space 
second statement due fact majority vote perceptrons identify error perceptron outputs different values final hypothesis needs gamma perceptrons boosting algorithm invoke weak learner gamma times 
clear better accuracy randomly guessing weak learners guaranteed achieve give worst case bounds error combined hypothesis respect sample 
theorem scheffer stephan adaboost perceptrons weak hypotheses requires log iterations order produce hypothesis error 
proof theorem 
theorem showed perceptron incurs error gamma bound difference weak learner randomly guessing fl equation freund schapire bounds number invocations weak learner adaboost required reach error fl log yields log fl shows adaboost converges reaching error bound polynomially steps 
corollary adaboost produce error respect sample zero log iterations 

boosting decision stumps proof corollary 
follows theorem bounds theorem corollary assumption number hits weak hypotheses exceeds number failures just sample point 
bounds give borderline weak learner boosting converges 
corollary adaboost find hypothesis consistent sample polynomial time hypothesis may consist log elementary hypotheses 
hand layer neural networks fixed number hidden units 
unfortunately known algorithm training problem neural networks sub exponential time worst case complexity 
theorem blum rivest deciding neural network input hidden output neuron exists consistent sample finding neural network hidden unit minimizes empirical error np complete 
fact training neural network number hidden units bounded polynomially number input terminals np complete 
observation expressive neural networks hidden output units boosted perceptrons learned polynomial time neural networks np 
proof 
expressive mean boosted perceptron consistent arbitrary arbitrarily large set points follows theorem boosted hypothesis consist log perceptrons 
contrast vc dimension neural network fixed number units fixed number means consistent possible function set points vc dimension 
theorem claims boosted perceptron learned theorem shows training neural network np complete 
boosting decision stumps empirical error shallow decision trees minimized efficiently depth algorithm finds minimizing tree variables log auer finding optimal shallow decision tree easier finding optimal hyper plane 
raises question shallow decision trees decision stumps weak hypotheses boosting algorithms 
unfortunately shallow decision trees guaranteed drop error theorem scheffer stephan examples adaboost general terminate weak hypotheses decision trees depth log proof theorem 
consider boolean space dimensions 
give equal weight points 
possible data points number occurring equals 
define odd intuitively resembles dimensional chess board 
claim decision tree chapter 
complexity issues depth log achieve error rate points reach arbitrary leaf 
corresponding branch perform sequence tests log gamma boolean variables 
points reach leaf defined values attributes arbitrary values remaining gamma attributes gammak points leaf differ value gamma attributes 
points corresponds binary number gamma bits 
equally binary numbers odd number number leaf number points equal number points 
decision tree achieve exactly hits 
voting hypotheses error yields total error chance 
adaboost decision trees guaranteed terminate yielding arbitrarily small error trees depth number attributes 
maximally deep trees match intuition boosted decision stumps note single tree depth consistent hypothesis 
course theorem refers worst case 
boosting shallow decision trees may terminate problem malicious 
intermediate decision trees branch log times 
programs try combine learnability intuitive idea small concepts 
allow adaboost learn decision trees boosting algorithm needs slightly super polynomial exponential number iterations 
theorem scheffer stephan examples adaboost learns hypothesis log iterations weak hypotheses decision trees depth log 
proof scheffer stephan 
lower bound gamma iterations weak learner perceptrons established showing full interpolation problems needs gamma perceptrons 
case lower bound improved super linear value gap lower upper bound worst case complexity adaboost learning decision trees logarithmic depth logarithmically branching points 
discussion related showed boosted perceptron learned log 
contrast layer neural network hidden output neuron learned polynomial algorithm np function boosted perceptron consistent function finite sample points expressive power boosted perceptrons greater expressive power fixed sized neural network 
shows increasing expressiveness hypothesis eases difficulty finding consistent hypothesis consistent boosted perceptrons greedy algorithm neural networks 
known analogous results different domains 
element term cnf conjunction boolean disjunctions boolean variables 
dnf contains disjunctions conjunction literals boolean variables 
term cnf turned dnf vice versa term cnf ae dnf 
sample complexity required pac learn term cnf dnf polynomial fixed 
order decide existence hypothesis consistent sample dnf essentially exhaust hypothesis space exponential dnf hypothesis language 
summary greedily search conjunction literals covers positive example consistent negative examples remove covered positive instances recur 
greedy algorithm find consistent hypothesis exists runs time polynomial returned dnf necessarily contain number conjunctions note contrast term cnf number terms conjunctions restricted dnf 
similar result holds support vector machine 
np complete find hyper plane minimizes mis classification rate support vector machine inflates space adding polynomials original attributes kernel trick assures added attributes need represented explicitly 
inflated space hyperplane consistent sample greedy algorithm polynomial time 
cases drawback inflating hypothesis space larger sample required generalization 
situations may reasonable trade 
sample required dnf polynomial practical problems appropriateness maximally wide margin bias compensates complex hypothesis space 
summary ffl model selection framework studied previous chapter model selection algorithm pre selects model invokes learner required minimize error model 
contrast boosting setting learner constructs hypothesis may grow hypothesis language dynamically hypothesis consistent sample 
ffl growing hypothesis dynamically done adaboost algorithm efficient 
adaboost perceptrons operates log comparable task static hypothesis language multilayer network fixed number units np complete 
ffl hypotheses learned adaboost complex 
consist log elementary hypotheses 
proving meaningful bounds largest difference true empirical error hypothesis impossible 
imply boosted hypotheses worse static hypotheses 
chapter depends adequate hypothesis language problem measured terms distribution fhg ed jh dxy 
chapter problem error minimization revolves problem 
hypothesis incurs unknown error error estimated sample usually yields unbiased estimate true error 
hypotheses error rate estimated sample 
unbiased means expected empirical error hypothesis true error ed just true error ed 
empirical error linked true error binomial distribution 
want minimize error focus hypothesis minimizes observed empirical error 
expected empirical loss hypothesis hl minimizes empirical error unfortunately distinct true error ed 
empirical error rates optimistic estimates pessimistic estimates corresponding true error rates going smallest observed loss choose hypothesis true loss estimated optimistically 
order know particular learning bias performs particular problem find just optimistically biased empirical loss hl possible answer problem provided cross validation 
small part sample held back asses single hypothesis estimate subject small pessimistic bias 
pac vc theory provide different answers bounding largest difference empirical true loss hypothesis chernoff bounds 
pac bounds overly general hold problem just particular problem difficult bound error hypotheses just hl 
expected error analysis provides different answer finite models 
quantifies expected error hypothesis hypothesis minimizes empirical error fixed sample size natural widely target criterion terms prior distribution error rates model fhg ed jh dxy 
error prior estimated efficiently sample recording empirical counterpart fhg jh leads efficient way estimate error rate hypothesis result error minimization process process having carried 
hypotheses output parameterized learner assessed hold sample similar situation arises 
lowest observed hold error learners optimistic estimate learner true error interesting know just optimistically biased hold error rate hypotheses assessed means training test pac style results elementary statistic results confidence estimates applied 
hypotheses evaluated fold cross validation approaches require additional assumption estimates obtained distinct folds independent 
assumption reasonable imply possible increase available information 
chapter 
expected error analysis information theoretic approach treated dependencies distinct cross validation folds explicitly quantified bias 
solution depends elementary properties data set easily measured entropy hit rate default classifier worst case result 
expected error analysis main contributions thesis analysis expected error hl hl hypothesis drawn random uniform distribution hypotheses empirical error model posterior distribution error values hl reduced prior distribution fhg ed jh dxy error values hypotheses practical analysis prior estimated recording fhg ed jh empirical errors randomly drawn hypotheses gives efficient means estimating error hypothesis error minimizing learner operates return 
learning conducted order obtain estimate opposed cross validation 
understanding learning curves 
learning curves express relationship hypothesis language generalization error 
order construct learners incur low generalization error important understand learning curves 
pac vc theory provides explanation shape learning curves taken guideline construction learners leads sub optimal learning algorithms 
pac model learning curves coupled largest difference true empirical error hypothesis inevitably increasing pac theory predict greater errors number hypotheses grows 
experiments learning curves behave complementary 
instance observed context boosting schapire decision trees hypothesis languages fisher schlimmer schaffer 
boosted hypothesis weighted majority elementary hypotheses 
increase number elementary hypotheses causes learning curve decrease contradicts pac theory 
expected error analysis explains happen error prior fhg ed jh dxy stays constant jh grows 
happens instance relevant attributes added hand irrelevant attributes added variance prior decreases causes predicted error increase 
predicted learning curves match measured simulations nicely small fairly constant bias caused assumed independence empirical errors distinct hypotheses 
empirical results 
conducted experiments artificial problem randomly drawn boolean functions large scale text categorization problem 
unfortunately error estimates turn unbiased 
distinct types bias interfere 
assumed independence assumption caused small pessimistic bias 
bias distort shape learning curves considerably 
importantly location optimal model predicted quite accurately 
second optimistic bias due way prior fhg ed jh dxy estimated 
small sample sizes true prior distribution fhg jh differ considerably 
note fs hg jh ed dp fhg ed jh dxy 
means fhg ed jh dxy zero chance empirical error zero strictly greater zero 
turn means ratio hypothesis estimated 
note fs hg jh converges fhg ed jh dxy grows estimate asymptotically consistent 
empirical comparison expected error analysis model selection fold cross validation showed small sample sizes error rates obtained chapter 
expected error analysis model selection equal error rates obtained means cross validation cases better 
experiments text categorization problem demonstrated expected error analysis scales large problems 
assessing model required minutes required hours attributes 
excerpt learning curve estimated hold testing matched reasonably predicted learning curve usual pessimistic bias 
expected error analysis selected models frequent categories lead hypotheses average hold error close hold error observed model 
limitations 
framework subject constraints 
important assumed finiteness analysis applied regression problems classification problems hypothesis language consists instance linear units 
number models finite 
usually problem number attributes finite 
learning curve assumed shaped convex infinite processed 
case search stopped minimum passed 
important constraint expected generalization error minimized 
far efficient practical solutions loss functions 
assumed independence empirical errors distinct hypotheses thought source modest bias constraint applicability 
experiments shown assumption accounts bias bias strong importantly observed distort position minimum strongly 
extensions linear cost models continuous models 
far expected zero loss loss function expected error analysis conducted 
raises question solution loss functions linear cost functions quadratic loss regression 
loss function plugged derivation 
proof theorem empirical error zero loss true error fsg je ed 
distinct cost function distribution empirical loss values true loss plugged point 
turns similar solution linear cost functions 
learning problems cost matrix theta ir available 
specifies costs imposed hypothesis assigns class label instance instance really belongs class medical diagnosis applications cost function zero loss minimized 
missing serious disease considerably worse making overly pessimistic diagnosis 
unfortunately solution prior distribution fhg jf fhg ed jd xy 
distribution jy dimensions hypotheses estimate fhg ed jd xy jy hypotheses required linear cost case 
usually expensive 
extending analysis cover regression turns difficult 
meaningful equivalent fhg ed jd xy infinite spaces require uniform distribution infinite approach lead solution consider error values hypotheses considered greedy learner 
approach faces problem recorded empirical errors unbiased estimate true errors occurred search greedy search guided low empirical errors 
sample size small fhg jh estimate fhg ed jh dxy estimate biased 
error rate expected error analysis model selection worse error rate cross validation model selection small sample sizes 

error rate intrinsic property domains sample size small interesting obtain better solution 
problem obtaining fhg ed jh dxy fhg jh equivalent problem estimating mixing density 
solutions intensely studied problem exist non parametric approaches deconvolution kernel estimator observed data carroll hall liu taylor furthermore discrete maximum likelihood estimator jsj mass points laird 
parametric approaches assume model mixing density ed adapt model parameter error observed predicted minimized instance em algorithm vardi lee 
results applied expected error analysis promise accurate results small sample sizes see section scheffer joachims 
error rate intrinsic property hypothesis language 
pac vc results bound error rate hypothesis learned hypothesis language terms hypothesis language size similarly vc dimension bound possible error difference empirical generalization error hypothesis learned hypothesis language model impose 
best hypothesis small hypothesis language incur relatively high error rate error rate best hypothesis hypothesis language referred bias term resulting error 
hand greatest difference true empirical error rate hypothesis hypothesis language grows model size 
know true error returned hypothesis 
difference error best hypothesis hypothesis language error rate hypothesis returned learner referred variance term error rate 
size vc dimension hypothesis language grows know error rate misinterpreted meaning variance term error increases 
leads false idea language biases intrinsically better conducting model selection trading bias term variance term intrinsically beneficial 
experiments schaffer theoretical analysis wolpert show learning biases se equally long empirical error minimized far expected generalization error concerned 
better average result set problems achieved implementing additional assumptions problems learner narrowing learner problems 
considerations provide evidence generalization error meaningfully quantified problem hypothesis language provides motivation error analysis chapter 
crucial property problem influences generalization error prior distribution error rates fhg ed jh dxy 
distribution explained cases curve simply decreases bound variance term increases cases learning curve increases overfitting occurs 
expected error analysis taken step see section explain cases conduction training test model selection beneficial 
case depends prior distributions error rates fhg ed jh dxy models stratification hh typical situation learning bias conducting model selection beneficial known attributes irrelevant 
results implications learning algorithms applied practical prob chapter 
lems 
impossible construct learner general accurate specialize learner strongly possible problem order obtain result problem 
specializing learner means encode available background knowledge learning bias 
known focused problem best construct learner solves possible problem equally 
hand target distribution determined exactly learning extreme special case construct trivial learner returns function approximates distribution optimally referring potentially available data 
learner solve particular problem optimally solve learning problem reasonably 
typically background knowledge available focused learning problem possible narrow hypothesis space model contains high density hypotheses problem 
instance realized choosing constructing small set attributes carry necessary information relevant classification task 
model selection 
model selection considered technique generally improves performance learners 
fact virtually practical learners employ type model selection technique pruning weight decay 
known exactly learning problems model selection beneficial compared choosing maximally complex model place 
results book indicate class problems solved means cross validation learning smaller generally assumed 
generally speaking theorems say occam algorithms useful stratification aligned prior distribution target functions 
theorem quantifies generalization error hypothesis generated fold cross validation learning compared error hypothesis generated simple error minimization specific stratification quantified theorems 
errors lower depends problem error priors fhg ed jh dxy principle estimated decision estimates theorems 
fact prior arrays estimated distinct decision optimal stratification 
error priors estimated imposes risk mis estimating errors making wrong decision 
greater number considered optimistically biased lowest estimated error meta level fitting occurs 
complexity model selection 
blush learning large hypothesis language appears difficult learning small hypothesis language error minimization usually requires jhj 
surprisingly cases finding hypothesis consistent sample easier larger hypothesis languages 
reason redundant representation greedy algorithms construct consistent hypotheses time logarithmically time required enumerate hypothesis space 
chapter demonstrated phenomenon referring adaboost multilayer networks 
learning hypothesis languages larger necessary result lower generalization performance 
contrast pac results results section show case 
learning bias chosen adequately problem techniques boosting find hypotheses low error efficiently 
expected error analysis 
section chapter discussed expected error analysis applied leaves question open 
applicability learning algorithms applied 
prior distribution known ideal conditions function class small optimal bayes hypothesis determined 
returning hypothesis hypothesis determined expected error analysis lead sub optimal result 
known bayes hypothesis determined map mdl hypotheses reasonable clear hypotheses perform better model selection algorithms stratification aligned prior 
expected error analysis algorithm exploits prior weak form means stratification aligned prior expect better map mdl hypothesis 
prior known decision complexity penalization cross validation expected error analysis 
complexity penalization algorithms require parameter trades empirical error model complexity 
parameter effectively determines model selected 
usually adjusted means hold testing 
complexity penalization methods schuurmans certain heuristic parameter 
hand means parameter adjusted hand heuristic certainly problems just certainly fail 
advantage cross validation expected error analysis cross validation estimates unbiased 
contrast expected error analysis yields pessimistically biased estimates sufficiently large samples necessarily cause expected error analysis chose sub optimal model cross validation 
sample size error prior fhg ed jh dxy estimated sufficient accuracy renders cross validation preferable 
principle drawback cross validation high computational complexity 
longer run learner takes preferable expected error analysis promising areas application expected error analysis large scale learning tasks text categorization knowledge discovery databases large scale classification tasks 
applicability learning algorithms virtually loss functions zero loss studied book quadratic loss regression assume distribution instances relatively loss defined observations required independent identically distributed distribution 
aware restriction assumption imposes application learning algorithms 
hypothesis small expected loss guaranteed incur little loss average provided fed input drawn respect distribution 
applications assumption met degree 
domains assumptions reasonable 
think instance databases customer transactions 
assume transaction observation 
transactions conducted customer day means independent second distribution transactions changes time old products vanish new products appear products get advertised 
think share prices 
distribution ticks ticks changes dramatically national global economic situation changes stationary 
situations assumption natural distribution appears questionable principle 
instance case scientific inquiry hypothesis sought explains certain area discourse completely exactly just respect particular distri appendix bution observations 
cases available data generated conducting experiments experiments guided scientists natural distribution 
identification limit framework hand requires distributional assumptions 
guarantees target identified exactly eventually 
unboundedness learning time constrains applicability algorithms emerged framework average case complexity analyses algorithms shinohara 
appendix proof theorem equation refer definition replace hl inf fe notational conventions relabel distribution change equations 
note erm hypothesis incurs empirical error necessarily true error 
equation factorize empirical error 
equation bayes equation order swap posterior probabilities 
distributed dxy distributed uniformly happens equation ajb fe fe gg fed idea equation bjc ajc bja 
equation state empirical error binomially distributed true error mean value 
dp fs ed jm dxy hl dp fs ed dxy inf fe dp fs hg ed jm dxy inf fe fs hg dxy inf fe fs hg inf fe dxy fs hg eje inf fe dxy dp fhg ed jh dxy fs hg inf fe dxy fs hg inf fe dxy dp fhg ed jh dxy fs hg gamma dxy fs hg eje dp fhg ed jh dxy fs hg inf fe dxy fs hg inf fe ed dxy dp fhg ed jh dxy fs hg inf fe dxy appendix fs hg inf fe je ed dxy order determine chance finding hypothesis empirical error randomly picking hypothesis factorize empirical error equation 
fs hg inf fe dxy fs hg inf fe dxy fs hg dxy exploiting assumption observed empirical errors independent random events assumptions equation state empirical error errors iff jh times error observed know hypothesis incur 
equation reflects perspective viewing process exhausting hypothesis space stochastic process perceived empirical error values outcomes random experiment 
assumed independence write conjunction events jh gamma st power individual event equation 
fs hg inf fe ed dxy fs hg jh je ed dxy fs jm dxy jh gamma es fs dxy jh gamma equations reduce prior empirical errors prior true errors 
fs dxy fs eje dxy dp fh ed jh dxy dp fh ed jh dxy eliminated unknown terms fhg ed jh dxy 
efficient implementation theorem section assemble corollary theorem coherent algorithm 
algorithm subsequently give documenting remarks 
order gain substantial understanding algorithms details reader need study proof theorem proof theorem appendix straightforward implementation corollary theorem incur computational effort algorithm exploits intimate details derivation calculates error estimate 
algorithm expected error analysis model selection input sample stratification hh output number estimated expected error erm hypothesis estimated expected error erm hypotheses models 

initialize variables zero 

models draw fixed number hypotheses measure empirical error 
theta number hypotheses incurred empirical error drawn hypotheses 
identify fhg ed jh dxy fhg jh 
jh gamma chance inf fe hypothesis empirical error erm hypothesis 
theta chance inf fe randomly drawn hypothesis 
increment exp theta gamma theta theta delta theta exp estimated expected true error model 

return minimizes exp 
algorithm runs 
step algorithm identifies estimates fhg jh fhg ed jh dxy see section discussion 
step algorithm calculates probability fs hg inf fe ed dxy described equation appendix 
note fs hg inf fe ed dxy fs hg inf fe dxy follows assumption 
step implementation equation 
step jointly implements sum corollary equation note equation theorem just expansion equation 
implementation binomial distribution uses pascal triangle small values refers normal distribution large values 
actual implementation resolution true error values restricted arbitrary constant reduces time complexity theta 
proof theorem expected error fs ed jh dxy hl expressed sum errors ed times chance selected learner 
appendix fs ed jh dxy hl jh ed fs jh dxy hl chance hypothesis selected set erm hypotheses zero equation 
factorize number erm hypotheses equation 
chance chosen hl minimum error hypothesis equation 
factorize empirical error fs jh dxy hl fs jh dxy hl fs jh dxy jh fs jjh dxy hl fsg jh dxy jh fsg jh dxy fsg je jh fsg jh dxy ed jh fsg jh dxy empirical error true error binomially distributed fsg je equation equals ed equation 
need determine unknown term fsg jh dxy 
equation factorize possible size hypotheses incur empirical error hypotheses incur strictly higher error definition 
equation exploit independence assumption resolve quantifiers 
fsg jh dxy jh es nh es jes ed ed dxy jh es jes ed es jes ed proof theorem jh gamma ed es es ed completes proof 
proof theorem remember learner draws hypothesis uniform distribution assigning error values individual hypothesis names notational trick place 
means ed ed fsg jh fsg jh 
arbitrary hypothesis ed equation fs ed jh dxy hl ddp fhg ed jh dxy fs jh dxy hl take care fs jh dxy hl 
insert equation equation 
fs ed jh dxy hl ddp fhg ed jh dxy min jh fsg jh min dxy je min ed min exploiting assumption claim const jh jh min dxy constant hypotheses fs ed jh dxy hl integrate 
fixes scaling factor const const dp fhg ed jh dxy min ed min je min gamma abbreviate min ed min je min fsg jh ed arrive equation 
focus equation 
equation follows straightforward observation iff incurs empirical error incur error note equation exploits assumption 
equation group hypotheses equal true error factor take factor appendix number hypotheses error 
note hypothesis assigned empirical error included product 
fsg jh ed ed jh ees ed ed ees gj equations rewritten equation 
completes proof 
efficient implementation theorem shall give efficient algorithm evaluates theorem 
algorithm expected error analysis input sample stratification hh output number estimated expected error erm hypothesis estimated expected error erm hypotheses 

initialize variables zero 

models draw fixed number hypotheses measure empirical error 
theta number hypotheses incurred empirical error drawn hypotheses 
identify fhg ed jh dxy fhg jh 
geq es ed ed geq 
table fsg je 
emin prod emin ii 
prod ed prod theta pow geq emin jh theta 
es increment ed ed es theta prod es fsg jh ed 
ed increment numerator theta theta 
ii 
increment denominator theta 
exp numerator denominator estimated expected true error model 

return minimizes exp 
algorithm generates tables avoid double computations 
runs 
proof theorem proof theorem equation write expected error integral error values equation factorize empirical error hl note ajb 
equation apply bayes theorem ajb bja fed fe hl fe 
happens equation ajb fe fed idea equation bjc ajc bja 
fe fed equation state empirical error binomially distributed remove sum fs je zero jh 
equation claim hl incurs empirical error rate zero 
note hypothesis error strictly nonzero chance incurring empirical error rate 
jh approaches infinity chance hypothesis incurring empirical error assumption approaches 
lim jh fs ed jh dxy hl lim jh ddp fs ed jh dxy hl lim jh dp fs ed jh dxy hl fs dxy hl lim jh fs hl je dxy fs dxy hl dp ed jh dxy fs hl jh dxy lim jh fs hl je dxy dp ed jh dxy fs jh dxy lim jh fs dxy ed dp ed jh dxy fs jh dxy fs jh dxy ed db ed dp ed jh dxy fs jh dxy theta gamma dp fhg ed jh dxy fs hg jh dxy theta gamma dp fhg ed jh dxy gamma dp fhg ed jh dxy note ed gamma completes proof 
appendix expected learning curve boolean functions boolean functions attributes section focus ed jh dxy hl expected error erm hypothesis hl dxy governed uniform distribution boolean concepts 
assume learning domain characterized prior dxy target distributions 
expected error hl theorem claims fhg ed jh dxy known knowledge exact dxy necessary 
dxy yields fhg ed jh dxy 
fhg ed jh dxy jp dxy probability particular error prior fhg ed jh dxy distribution targets dxy 
order determine expected error target functions integrate possible error priors 
ed jh hl fs ed jh hl dp dxy fs ed jh hl expected error hl error prior fhg ed jh dxy theorem 
boolean functions equation explicit solution 
dxy jx dx dx uniform distribution boolean instances jx governed uniform distribution boolean concepts boolean variables set boolean functions boolean variables sample size hl drawn uniformly hypotheses empirical error 
cases distinguished 
dxy splits boolean space instances hypotheses split space distinguishable subspaces 
gammai boolean instances potentially distinct class labels fall subspace 
hypothesis assign class label subspace 
target function uniformly distributed assigning class label mis classify number instances distributed ffl gammai class label incur error gammai gamma ffl 
ffl ffl number instances mis classified corresponding subspace assigned class label ffl lie gammai 
ffl parameters distribution fhg ed jh dxy 
target functions parameters distributed ffl ffl gammai ffl theta theta gammai ffl set parameters ffl ffl fhg ed jh dxy sum errors incurred subspace distributed fhg ed jh dxy ffl ffl ffl ffl expected learning curve boolean functions ffl ffl gamma ffl ffl ffl gamma gammai ffl ffl iff ffl gammai gamma iff ffl iff ffl gammai gamma equation recursive intuition equation error incurred subspace error ffl class label incurred subspace error gamma ffl incurred subspace error gammai gamma ffl incurred subspace class label remaining error gamma gammai gamma ffl incurred subspaces case ed jh hl takes form ed jh hl ffl ffl fs ed jh fhg ed jh dxy hl ffl ffl fhg ed jh dxy depends ffl ffl equation ffl ffl equation fs ed jh fhg ed jh dxy hl theorem 
case target function assigns class label gamman instances distinguished hypothesis 
target function dxy fhg ed jh dxy distribution distributions fhg ed jh dxy dxy uniform distribution boolean functions attributes dx uniform distribution boolean instances 
key observation situation fhg ed jh dxy equation depend concrete dxy fh dxy ed jh fhg ed jh dxy 
ed jh hl takes form ed jh hl fs ed jh fhg ed jh dxy hl error prior fhg ed jh dxy equation fs ed jh fhg ed jh dxy hl theorem 
note expected error hl depends number relevant attributes number attributes model sample size expected learning curve boolean functions attributes section discuss solution equation contains boolean functions attributes possible attributes target boolean function appendix attributes 
contrast previous section included hypotheses particular attributes error prior situation 
look relevant irrelevant attributes randomly drawn hypothesis contains 
relevant attributes total attributes number relevant attributes hyper geometrically distributed 
assume gamma attributes relevant attributes irrelevant 
fh dxy ed jh gamma theta fhg ed jn part refer particular distribution shall call gives chance sum random numbers chance random number 
equals sum theta occurs probability 
occur certain number times 
occurs times occurs gamma times result gamma occur gammap times 
characterized follows 
iff theta gamma mp gamma iff binomial distribution individual probability relevant attributes split space instances gammau subspaces 
subspaces split gammai parts target function 
similar situation studied previous subsection 
hypothesis assign class label gammai parts 
correct class label uniformly distributed part assigning class label misclassify number instances ffl distributed gammai 
target function gammau subspaces error values ffl gammai gammai gammaffl gammai possible 
ffl gammau parameters prior distribution fhg ed jh dxy 
gammau subspaces say subspace generated gamma relevant attributes split subspaces irrelevant attributes 
remember error values ffl gammai gammai gammaffl gammai possible subspace assume hypothesis assigns class subspace error ffl gammai occurs error gammai gammaffl gammai 
number possible assignments class labels subspaces incurs particular error values distributed 
random experiments possible outcomes ffl gammai gamma ffl chance possible outcome carried 
sum outputs governed distribution 
gammai ffl gammai gamma ffl 
consequently chance sum error values gammau subspaces generated relevant attributes fhg ed jn ffl ffl gammau gammau gammau ffl ffl gammai notation ffl gammai gamma ffl ffl ffl gamma gammai ffl gammai ffl gammai gamma ffl distribution governs parameters ffl ffl gammau error prior analogous equation 
ffl ffl gammau gammai ffl theta theta gammai ffl gammau parametric error prior prior distribution parameters determined plugged theorem 
dxy uniform distribution boolean functions attributes contains boolean functions attributes possible attributes fs dxy ed jh hl ffl ffl gammau fs ed jh fhg ed jn hl ffl ffl gammau gamma completes derivation 
notation section provide quick table notation book 

binomial distribution density 
chance achieving hits trials chance hit trial trials independent 
gamma delta gamma gammax bc learner identifies target function correct corresponding learnability class abbreviated bc finitely erroneous predictions finite point keeps making true predictions may change conjecture target function 
dxy target distribution 
dxy jx dx jx yjx chance class instance dx probability mass instance see section definition 
ed 
true error respect target distribution dxy hypothesis see section definition 
erm hypothesis 
set hypotheses empirical error respect sample called erm hypotheses 
single hypothesis drawn set uniform distribution erm hypothesis 
appendix 
empirical error hypothesis sample see section definition 
ex class explanatory learnable languages 
language class explanatory learnable exists learner identifies language class certainty eventually 
new example learner outputs new hypothesis learner may change mind finitely bound required number example needed 
fxg 
expectation distributed usually context 
class finitely identifiable languages 
learner identifies language class finitely outputs single correct hypothesis finitely examples change mind 
arbitrary hypothesis 
usually refers hypotheses drawn random uniform distribution chapter 
th model 
model set hypotheses chapter models assumed finite 

set hypotheses empirical error hl hypothesis returned learner cnf 
class conjunctive normal forms conjunctions disjunctions boolean attributes disjunction literals 
dl 
class decision lists boolean attributes rule consists body literals head labeled class symbol 
rule fires instance assigns class label instance 
see section 
dnf 
class disjunctive normal forms disjunctions conjunctions boolean attributes conjunction literals 
lh 
result deterministic learner operates model sample particular hypothesis 
pl jh 
chance learner returning hypothesis hl operating model sample fxg 
probability drawing set instances 
finite set class labels 
bibliography agrawal imielinski swami 

mining association rules sets items large databases 
acm sigmod conference management data pp 

angluin 

inductive inference formal languages positive data 
inform 
control 
angluin 

inductive inference formal languages positive data 
information control 
angluin 

learning queries 
baum 
ed computational learning cognition pp 

siam 
angluin smith 

inductive inference theory methods 
computing surveys 

overview minimum description length principle 
lectures complex systems proceedings complex systems summer school 
lectures volumes ii iii santa fe institute studies sciences complexity addison wesley 
auer holte maass 

theory applications agnostic pac learning small decision trees 
th icml pp 

barzdin 

theorems limiting synthesis functions 

gos 
univ uch 

russian 
baxter 

bayesian information theoretic model learning learn multiple task sampling 
machine learning 
baxter 

model bias learning 
submitted jacm 
bayes 

essay solving problem doctrine chances 
phil 
trans 
royal soc 
london 
berger 

statistical decision theory bayesian analysis 
springer verlag 
berger 

overview robust bayesian analysis 
tech 
rep department statistics purdue university 
berger berliner 

robust bayes empirical bayes analysis contaminated priors 
annals statistics 
bibliography blum rivest 

training node neural network np complete 
neural networks 
blumer ehrenfeucht haussler warmuth 

occam razor 
information processing letters 
blumer ehrenfeucht haussler warmuth 

learnability dimension 
acm 
bose 

bayesian robustness mixture priors 
tech 
rep dept statistics george washington university 
bose 

bayesian robustness class 
journal statistical planning inference 
breiman 

bias variance arcing classifiers 
tech 
rep statistics department university california berkeley 
breiman friedman olshen stone 

classification regression trees 
pacific grove 
breiman 

stacked regressions 
machine learning 
cleve 

oracles queries sufficient exact learning 
proc 
th annual acm workshop computational learning theory pp 

cano hernandez moreno 

posterior measures partial prior information 
statistica 
carroll hall 

alternative method cross validation smoothing density estimates 
journal american statistical association 
case smith 

comparison identification criteria machine inductive inference 
theoretical computer science 
cohen 

empirical methods artificial intelligence 
mit press cambridge 
cortes vapnik 

support vector networks 
machine learning 
cun denker solla 

optimal brain damage 
nips pp 

de finetti 

probability induction statistics 
wiley new york 


partial prior knowledge bayesian inference 
ph thesis yale university new haven 
dietterich kong 

machine learning bias statistical bias statistical variance decision tree algorithms 
tech 
rep department computer science oregon state university 
dietterich 

statistical tests comparing supervised classification learning algorithms 
unpublished manuscript submitted 
bibliography dobkin kasif 

computing optimal shallow decision trees 
proc 
international workshop mathematics artificial intelligence 
dobkin fulton kasif salzberg 

induction shallow decision trees 
ieee trans 
pattern analysis machine intelligence 
review 
muggleton 

application inductive logic programming finite element mesh design 
muggleton 
ed inductive logic programming pp 

academic press 
domingos 

process oriented heuristic model selection 
icml pp 

drucker schapire simard 

improving performance neural networks boosting algorithm 
hanson cowan giles 
eds advances neural information processing systems vol 
pp 

morgan kaufmann 
efron 

bootstrap methods look 
annals statistics 
efron 

estimating error rate prediction rule 
journal american statistical association 


maximum smoothed likelihood density estimation inverse problems 
annals statistics 
ehrenfeucht haussler kearns valiant 

general lower bound number examples needed learning 
information computation 
appeared proc 
st annu 
workshop comput 
learning theory 
steger 

learning variable pattern languages efficiently average parallel asking queries 
algorithmic learning theory eighth international workshop alt vol 
lecture notes artificial intelligence pp 

fayyad shapiro smyth 

knowledge discovery data mining unifying framework 
kdd 
fisher schlimmer 

concept simplification prediction accuracy 
icml pp 

fisher 

multiple measurements taxonomic problems 
ann 

freund schapire 

experiments new boosting algorithm 
machine learning proceedings thirteenth international conference 
freund schapire 

decision theoretic generalization line learning application boosting 
journal computer system sciences 
gaines compton 

induction ripple rules 
th australian conference artificial intelligence 
smith 

learning queries 
proc 
th annual symposium computer science focs 
bibliography geman bienenstock doursat 

neural networks bias variance dilemma 
neural computation 
gold 

language identification limit 
information control 


thinking foundations probability theory applications 
university minnesota press minneapolis 


ranges posterior measures classes priors specified moments 
research report dept statistical science university college london 


nonparametric estimation mixing density kernel method 
journal american statistical association 
greiner 

palo probabilistic hill climbing algorithm 
artificial intelligence 
grunwald 

minimum description length approach grammar inference 
wermter riloff 
eds connectionist statistical symbolic approaches learning natural language processing vol 
lnai pp 

springer verlag berlin 
hartigan 

bayes theory 
springer verlag new york 
haussler 

quantifying inductive bias ai learning algorithms valiant learning framework 
artificial intelligence 
haussler 

learning conjunctive concepts structural domains 
machine learning 
haussler kearns schapire 

bounds sample complexity bayesian learning information theory vc dimension 
machine learning 
haussler 

decision theoretic generalizations pac model neural net learning applications 
information computation 


design analysis pattern recognition experiments 
bell syst 
tech 
journal 
simon horn 

robust single neurons 
journal computer system sciences 
hoffmann scheffer 

hidden assumptions model selection algorithms 
submitted 
siebes 

data mining 
search knowledge databases 
tech 
rep cs cwi amsterdam box gb amsterdam netherlands 
jain kasturi schunck 

machine vision 
mcgraw hill 
jain osherson royer sharma weinstein 

systems learn 
mit press 
jain sharma 

language learning team 
automata languages programming pp 

bibliography jeffreys 

theory probability 
oxford university press 
joachims 

text categorization support vector machines 
proceedings european conference machine learning 
joachims freitag mitchell 

webwatcher tour guide world wide web 
proceedings th international joint conference artificial intelligence ijcai pp 
san francisco 
morgan kaufmann publishers 
kang compton preston 

multiple classification ripple rules evaluation possibilities 
gaines musen 
eds proc 
th banff knowledge acquisition knowledge systems workshop pp 

kearns mansour ng ron 

experimental theoretical comparison model selection methods 
machine learning journal 
kearns pitt 

polynomial time algorithm learning variable pattern languages 
colt 
kearns schapire sellie 

efficient agnostic learning 
int 
conference computational learning theory pp 

kearns vazirani 

computational learning theory 
mit press 
kearns 

bound error cross validation approximation estimation rates consequences training test split 
advances neural information processing systems vol 
pp 

kietz dzeroski 

inductive logic programming learnability 
sigart bulletin 
lindner 
oracles sigma sufficient exact learning 
proc 
international workshop algorithmic learning theory 
kohavi 

wrappers performance enhancement oblivious decision graphs 
ph thesis stanford university 
kohavi john 

automatic parameter selection minimizing expected error 
icml 
kohavi john 

wrappers feature subset selection 
artificial intelligence 
kohavi wolpert 

bias plus variance decomposition zero loss functions 
icml 
kong dietterich 

error correcting output coding corrects bias variance 
icml pp 

kononenko 

inductive bayesian learning medical domains 
applied artificial intelligence 
bibliography koza rice 

genetic generation weights architecture neural network 
international joint conference neural networks ijcnn vol 
ii pp 



unbiased method obtaining confidence intervals probability misclassification discriminant analysis 
biometrics 
laird 

nonparametric maximum likelihood estimation mixing distribution 
journal american statistical association 
lang 

newsweeder learning filter netnews 
icml pp 

lange 

monotonic versus non monotonic language learning 
proceedings second international workshop nonmonotonic inductive logic vol 
lecture notes artificial intelligence pp 

springer verlag 


note bounding monte carlo variances 
communications statistics theory methods 
lavrac dzeroski 

inductive logic programming 
ellis horwood 
lee 

automatic speech recognition development sphinx system 
kluwer academic publishers 
lewis 

representation learning information retrieval 
phd thesis university massachusetts 
lindner 


ph thesis friedrich schiller universitat jena 
liu taylor 

consistent nonparametric density estimator deconvolution problem 
canadian journal statistics 
mcallester 

pac bayesian theorems 
colt pp 

morgan kaufmann 
mehta rissanen agrawal 

mdl decision tree pruning 
proceedings international conference knowledge discovery data mining kdd pp 

michalski hong lavrac 

multi purpose incremental learning system aq testing application medical domains 
proc 
fifth national conference artificial intelligence pp 
san mateo ca 
morgan kaufmann 
michie spiegelhalter taylor 

machine learning neural statistical classification 
ellis horwood 
milne 

attribute selection neural networks classify remotely sensed data 
proc 
visual information processing workshop sydney 
mingers 

empirical comparison selection measures decision tree induction 
machine learning 
mitchell 

learnability subclass extended pattern languages 
proceedings eleventh annual conference computational learning theory 
acm press 
bibliography mitchell scheffer sharma 

pac learnability subclasses pattern languages 
unpublished manuscript 
mitchell 

machine learning 
mcgraw hill 
mitchell 

generalization search 
ai journal 
moody 

effective number parameters analysis generalization regularization non linear learning systems 
nips 
moody 

principled architecture selection neural networks 
nips 
moreno cano 

robust bayesian analysis partially known 
journal royal statistical society series 


problems designs cross validation 
educational psychological measurement 
mosteller tukey 

data analysis including statistics 
aronson 
eds revised handbook social psychology vol 
pp 

addison wesley 
muggleton 

inductive logic programming 
volume series 
academic press london 
muller finke schulten murata amari 

numerical study learning curves stochastic multi layer feed forward networks 
neural computation 
muller wysotzki 

automatic construction decision trees classification 
annalen ur operations research 
muller wysotzki 

automatic synthesis control programs combination learning problem solving methods extended 
lavrac wrobel 
eds machine learning ecml lnai pp 

springer verlag 
murphy aha 

uci repository machine learning databases 
www ics uci edu mlearn mlrepository 
scheffer 

term genetic code artificial neural networks 
hopf 
ed genetic algorithms framework evolutionary computation 
max fur informatik 
neyman 

selection early statistical papers neyman 
university california press berkeley 
ng 

preventing overfitting cross validation data 
proc 
int 
conference machine learning pp 

morgan kaufmann 
ng 

feature selection learning exponentially irrelevant features 
icml pp 

nilsson 

machine learning 
mit press 
norman 

double split cross validation extension design undesirable alternatives results 
journal applied psychology 
bibliography oliver baxter 

mdl mml similarities differences 
tr dept computer science monash university clayton victoria australia 
available www www cs monash edu au 
oliver 

unsupervised learning mml 
machine learning proceedings thirteenth international conference icml pp 

morgan kaufmann publishers san francisco ca 
available www www cs monash edu au jon pitt valiant 

computational limitations learning examples 
acm 
pomerleau 

alvin land vehicle neural network 
technical report cmu cs carnegie mellon university pittsburgh 
quinlan 

learning logical definitions 
machine learning 
quinlan 

programs machine learning 
morgan kaufmann publisher 
quinlan rivest 

inferring decision trees minimum description length principle 
technical memo mit lcs tm massachusetts institute technology laboratory computer science 
quinlan 
ed 

applications expert systems 
addison wesley 
quinlan 

bagging boosting 

proceedings thirteenth national conference artificial intelligence eighth innovative applications artificial intelligence conference pp 
menlo park 
aaai press mit press 
quinlan 

boosting order learning 
proc 
international workshop algorithmic learning theory pp 
sydney 
richter 

als fur 
rad 
rissanen 

modelling shortest data descriptions 
automatica 
rissanen 

minimum description length principle 
ann 
statist 
rissanen 

stochastic complexity statistical inquery 
world scientific 
rivest 

learning decision lists 
machine learning 
rosenblatt 

perceptron probabilistic model information storage organization brain 
psych 
rev 
reprinted neurocomputing mit press 
hinton 

learning internal representations error propagation 
mcclelland 
eds parallel distributed processing vol 
pp 

mit press cambridge ma 
salomaa 

patterns formal language theory column 
eatcs bulletin 
salomaa 

return patterns formal language theory column 
eatcs bulletin 
bibliography salton 

term weighting approaches automatic text retrieval 
information processing 
sammut hurst michie 

learning fly 
proceedings ninth international conference machine learning aberdeen 
morgan kaufmann 
schafer 

constructive probability 
synthese 
schaffer 

overfitting avoidance bias 
machine learning 
schaffer 

selecting classification method cross validation 
machine learning 
schaffer 

conservation law generalization performance 
icml pp 

schapire freund bartlett lee 

boosting margin new explanation effectiveness voting methods 
machine learning proceedings fourteenth international conference pp 

schapire 

pattern languages learnable 
proc 
rd annu 
workshop comput 
learning theory pp 
san mateo ca 
morgan kaufmann 
scheffer 

algebraic foundation improved methods induction ripple rules 
proc 
pacific knowledge acquisition workshop 
scheffer greiner darken 

experimentation better perfect guidance 
proc 
th international conference machine learning pp 

morgan kaufmann 
scheffer herbrich 

unbiased assessment learning algorithms 
ijcai pp 

scheffer herbrich wysotzki 

efficient theta subsumption graph algorithms 
inductive logic programming th international workshop selected papers 
springer verlag 
scheffer joachims 

estimating expected error empirical minimizers model selection 
aaai 
scheffer joachims 

estimating expected error empirical minimizers model selection 
tech 
rep tr technische universitaet berlin 
scheffer joachims 

expected error analysis model selection 
proceedings international conference machine learning icml 
scheffer joachims 

expected error analysis model selection 
preprint tu berlin 
available ki cs tu berlin de scheffer 
scheffer stephan 

worst case analysis boosting 
unpublished 
wysotzki 

piecewise linear classifier 
bergadano raedt 
eds machine learning ecml lnai 
springer verlag 
bibliography schuurmans 

new metric approach model selection 
aaai 
schuurmans ungar foster 

characterizing generalization performance model selection strategies 
icml pp 

shinohara arikawa 

pattern inference 
jantke lange 
eds algorithmic learning knowledge systems vol 
lecture notes artificial intelligence pp 

springer verlag 
shinohara 

polynomial time inference extended regular pattern languages 
goto furukawa nakajima nakata 
ed proceedings rims symposia software science engineering vol 
lncs pp 
kyoto japan 
springer 
simon 

machines learn 
michalski mitchell 
eds machine learning artificial intelligence approach pp 

tioga palo alto ca 


bounds posterior expectations density bounded classes constant bandwidth discussion 
journal statistical planning inference 
berger 

robust analysis binomial empirical bayes problem 
canadian journal statistics 
smith 

examples discrimination 
ann 

smith 

decades team learning 
arikawa 
eds proc 
th international workshop analogical inductive inference th international workshop algorithmic learning theory pp 

stephan 

learning queries oracles 
journal pure applied logic 
stone 

cross choice assessment statistical predictions 
journal royal statistical society 
sutton barto 

reinforcement learning 
mit press 
tesauro 

practical issues temporal difference learning 
machine learning 
tesauro 

temporal difference learning td gammon 
communications acm 
tishby 

statistical physics models supervised learning 
wolpert 
ed mathematics generalization 
addison wesley 
toivonen 

sampling large databases association rules 
proc 
vldb conference 
toussaint 

bibliography estimation misclassification 
ieee transactions information theory 
wysotzki 

zur von und ihre simulation auf 
kybernetik forschung 
bibliography unger wysotzki 


akademie verlag berlin 
valiant 

learning disjunctions conjunctions 
proc 
th ijcai vol 
pp 

morgan kaufmann 
valiant 

theory learnable 
communications acm 
vapnik 

estimation dependencies empirical data 
springer 
vapnik 

nature statistical learning theory 
springer 
vapnik 

statistical learning theory 
wiley 
vapnik chervonenkis 

uniform convergence relative frequencies events probabilities 
theory probability applications 
vardi lee 

image deblurring optimal investments maximum likelihood solutions positive linear inverse problems 
journal royal statistical society 


theory learning generalization 
springer 


theorie der 
akademie verlag berlin 
wexler 

formal principles language acquisition 
mit press cambridge ma 
wolpert 

overfitting avoidance bias 
working santa fe institute 
wolpert 

relationship pac statistical physics framework bayesian framework vc framework 
wolpert 
ed mathematics generalization sfi studies sciences complexity pp 

addison wesley 
wolpert 

connection sample testing generalization error 
complex systems 
yang petersen 

comparative study feature selection text categorization 
icml 
