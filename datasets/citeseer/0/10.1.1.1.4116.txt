semi supervised learning classifiers theory algorithms application human computer interaction ira cohen fabio cozman marcelo thomas huang ira cohen hewlett packard labs palo alto ca usa ira cohen hp com fabio cozman marcelo polit cnica universidade de paulo paulo brazil 
usp br marcelo poli usp br faculty science university amsterdam netherlands 
science uva nl thomas huang beckman institute university illinois urbana champaign usa 
huang ifp uiuc edu partially funded nsf iis 
draft automatic classification basic tasks required pattern recognition human computer interaction application 
discuss training probabilistic classifiers labeled unlabeled data 
provide new analysis shows conditions unlabeled data learning improve classification performance 
show conditions violated unlabeled data detrimental classification performance 
discuss implications analysis specific type probabilistic classifiers bayesian networks propose new structure learning algorithm utilize unlabeled data improve classification 
show resulting algorithms successfully employed applications related human computer interaction pattern recognition facial expression recognition face detection 
index terms semi supervised learning generative models facial expression recognition face detection unlabeled data bayesian network classifiers 
pattern recognition human computer interaction applications require design classifiers 
classifiers designed expert knowledge training data 
training data labeled unlabeled 
applications obtaining fully labeled training sets difficult task labeling usually done human expertise expensive time consuming error prone 
obtaining unlabeled data usually easier involves collecting data known belong classes having label 
example facial expression recognition easy collect videos people displaying expressions tedious difficult label video corresponding expressions 
learning labeled unlabeled data known semi supervised learning 
start general analysis semi supervised learning probabilistic classifiers 
goal analysis show conditions unlabeled data improve classification performance 
review maximum likelihood estimation learning labeled unlabeled data 
provide asymptotic analysis value unlabeled data show unlabeled data help reducing estimator variance 
show assumed probabilistic model matches data generating distribution reduction variance leads improved classification accuracy situation analyzed 
show assumed probabilistic model match true data generating distribution unlabeled data detrimental classification accuracy phenomenon generally ignored misinterpreted previous researchers observed empirically :10.1.1.1.5684
new result emphasizes importance correct modeling assumption learning unlabeled data 
analysis semi supervised learning classifiers bayesian networks 
classification problems simple structures learned just labeled data successfully naive bayes classifier structures fail trained labeled unlabeled data 
bayesian networks probabilistic classifiers joint distribution features class variables specified graphical model 
graphical representation advantages 
existence algorithms inferring class label ability intuitively represent fusion different modalities graph structure ability perform classification learning complete data importantly ability learn labeled unlabeled data 
discuss possible strategies choosing graphical structure argue problems necessary search structure 
structure search algorithms driven likelihood cost functions potentially inadequate classification due attempt maximize likelihood data largely ignoring important quantity classification class posteriori likelihood :10.1.1.30.9978
propose classification driven stochastic structure search algorithm sss combines labeled unlabeled data train classifier search better performing bayesian network structure 
new understanding limitations imposed properties unlabeled data equipped algorithm overcome limitations apply bayesian network classifiers human computer interaction problems facial expression recognition face detection 
applications obtaining unlabeled training data relatively easy 
cases labeling data difficult 
facial expression recognition accurate labeling requires expert knowledge applications labeling large amount data time consuming human labeler 
show bayesian network classifiers trained structure search benefit semi supervised learning problems 
remainder organized follows 
section ii discuss value unlabeled data illustrate possibility unlabeled data degrade classification performance 
section iii propose possible solutions bayesian network classifiers benefit unlabeled data learning network structure 
introduce new stochastic structure search algorithm empirically show ability learn labeled unlabeled data datasets uci machine learning repository 
section iv describe components real time face recognition system including real time face tracking system features extracted classification facial expressions 
perform experiments facial expression recognition system databases show ability utilize unlabeled data enhance classification performance small labeled training set 
experiments bayesian network classifiers face detection section iv 
concluding remarks section ii 
learning classifier labeled unlabeled training data goal classify incoming vector observables instantiation sample 
exists class variable values classes 
want build classifiers receive sample output class 
assume loss consequently objective minimize probability error classification error 
knew exactly joint distribution optimal rule choose class value maximum posteriori probability 
classification rule attains minimum possible classification error called bayes error 
take probabilities functions probabilities estimated data plugged optimal classification rule 
assume parametric model adopted 
estimate denoted denote asymptotic value 
distribution belongs family say model correct say model incorrect 
estimation bias loosely mean expected difference estimated consider scenario 
sample generated 
value revealed sample labeled value hidden sample unlabeled 
probability sample labeled denoted fixed known independent samples underlying distribution generates labeled unlabeled data 
worth noting assume revealed label correct corrupted noise case noisy labels studied various works chapter 
extending analysis noisy labeled case scope 
set nl labeled samples nu unlabeled samples maximum likelihood estimating 
consider distributions decompose depend explicitly 
known generative model 
log likelihood function generative model dataset labeled unlabeled data ll lu log nl nu lu nl nu nl log xj ll nl log xi ci ia indicator function 
ll lu likelihoods labeled unlabeled data respectively 
statistical intuition suggests reasonable expect average improvement classi fication performance increase number samples labeled unlabeled 
existing literature presents empirical theoretical findings indicate positive value unlabeled data 
cooper freeman optimistic unlabeled data title asymptotic improvement outcome supervised learning provided additional learning early studies strengthened assertion unlabeled data available 
castelli venkatesh showed unlabeled data asymptotically useful classification 
krishnan extended results provide efficiency results discriminant logistic normal models samples labeled stochastically 
noted previous theoretical critical assumption belongs family models model correct 
applied semi supervised learning 
publications advance optimistic view labeled unlabeled data problem unlabeled data profitably available 
different parameter set :10.1.1.1.5684
probability error labeled labeled labeled number unlabeled records labeled labeled labeled number unlabeled records fig 

naive bayes classifier data generated naive bayes model tan model 
point summarizes runs classifier testing data bars cover percentiles 
probability error detailed analysis current applied results reveal puzzling aspects unlabeled data 
researchers reported cases addition unlabeled data degraded performance classifiers compared case unlabeled data 
cases specific type data different kinds sensory data computer vision text classification 
explain phenomenon began performing extensive experiments providing em evidence degradation performance directly related incorrect modeling sumptions 
consider shows typical results 
estimated parameters naive bayes classifier features expectation maximization em algorithm varying numbers labeled unlabeled data 
shows classification performance underlying model naive bayes structure left underlying model naive bayes right 
result clear estimate naive bayes classifier data generated naive bayes model unlabeled data help estimate naive bayes classifier data come corresponding model unlabeled data degrade performance case labeled unlabeled samples 
provide theoretical explanation empirical evidence derived asymptotic prop erties maximum likelihood estimators labeled unlabeled case 
analysis remainder section provides unified explanation behavior classifiers cases model correct 
value unlabeled data maximum likelihood estimation base analysis white properties maximum likelihood estimators properties hold case model correctness model incorrectness 
theorems showed suitable regularity conditions maximum likelihood estimators converge parameter set minimizes kullback liebler kl distance assumed family distributions true distribution 
white shows estimator asymptotically normal cy number samples goes infinity 
cy covariance matrix equal ay ay evaluated ay matrices th element number parameters ay log log log 
definitions general result obtain theorem consider supervised learning samples randomly labeled proba bility 
assuming identifiability marginal distributions value limiting value maximum likelihood estimates arg max log log expectations respect 
additionally ax bx evaluated ax bx defined expression replaced proof 
denote random variable assumes values plus unlabeled value 
observed samples realizations conditions ensure existence derivatives defined expectations theorem 
write probability distribution sample compactly follows mixture density obtained 
accordingly parametric model adopted 
white results stated know maximizes log expectation respect 
log log log log log log log log log terms expression irrelevant maximization respect 
terms equal log log expression expression equal log log expectations respect 
obtain expression 
expression follows directly white theorem expression replacing appropriate observations theorem 
expression indicates semi supervised learning viewed asymptotically convex combination supervised unsupervised learning 
objective function semi supervised learning combi nation objective function supervised learning log objective function unsupervised learning log 
second asymptotic covariance matrix positive definite positive definite ay symmetric ay see asymptotically increase number labeled unlabeled samples lead reduction variance 
guarantee basis optimistic view unlabeled data improve classification accuracy 
show view valid model correct valid model incorrect 
model correct suppose family distributions contains distribution 
condition maximum likelihood estimator consistent identifiability 

shahshahani landgrebe suggested taylor expansion classification error link decrease variance associated unlabeled data decrease classification error 
show smaller variance estimator smaller classification error variance estimator smaller number samples increases labeled unlabeled adding unlabeled data reduce classification error 
formal general argument mclachlan compare relative efficiency labeled unlabeled data 
castelli derives taylor expansion classification error study estimation mixing factors derivation precise states required assumptions 
model incorrect study realistic scenario distribution belong family distributions 
view theorem surprising unlabeled data deleterious effect observed occasionally literature 
suppose example section observe large number labeled samples classification error handle difficulty unlabeled data information decide labels decision regions classification error 
defining error define error approaching 
approximately 
collect samples unlabeled eventually reach point classification error approaches 
net result started classification error close adding large number unlabeled samples classification performance degraded 
basic fact estimation classification bias affected differently different values 
necessary condition kind performance degradation sufficient condition 
example bivariate gaussians spurious correlation previous discussion alluded possibility model incorrect 
skeptical reader may think occur practice numerical algorithms em cause performance degradation analytically show occurs example obvious practical significance 
examples provided 
assume bivariate gaussian samples observed 
modeling error ignored dependency observables 
type modeling error quite common practice studied context supervised learning 
argued ignoring dependencies positive decision may see reduction number parameters estimated reduction variance estimates 
example consider real valued observations taken classes know gaussian variables know means variances class mean conditional conditional 
variances conditional equal 
know estimate mixing factor 
data sampled distribution mixing factor equal 
want obtain naive bayes classifier approximate naive bayes classifiers assumption independent suppose independent conditional dependent conditional 
dependency manifested correlation knew value obtain optimal classification boundary plane optimal classification boundary shown defined function log 
incorrect assumption classification boundary linear log consequently decreasing function 
labeled data easily obtain sequence bernoulli trials 
classification boundary note linear boundary obtained labeled data best possible linear boundary 
fact find best possible linear boundary form 
classification error diag 
interchanging differentiation respect integration possible obtain de closed form 
second derivative positive consequently single minimum solving de 
find minimizing log 
line best linear boundary problem 
consider set lines form see farther go best line larger classification error 
shows linear boundary obtained labeled data best possible linear boundary 
boundary labeled data best linear boundary 
consider computation theorem asymptotic estimate unlabeled data arg max log diag diag diag 
second derivative double integral negative seen interchanging differentiation integration function concave single maximum 
search zero derivative double integral respect 
obtain value numerically 
estimate linear boundary unlabeled data 
line linear boundary labeled data previous discussion leads larger classification error boundary labeled data 
best linear labeled unlabeled fig 

graphs example 
contour plots mixture optimal classification boundary quadratic curve best possible classification boundary form 
contour plots best linear boundary lower line linear boundary obtained labeled data middle line linear boundary obtained unlabeled data upper line classification error classifier obtained unlabeled data larger classifier obtained labeled data 

boundary obtained unlabeled data shown 
example suggests situation 
suppose collect large number nl labeled samples 
labeled estimates form sequence bernoulli trials probability estimates quickly approach variance decreases nl 
add large amount unlabeled data data approaches classification error increases 
finite sample effects asymptotic analysis semi supervised learning suffices show fundamental problem occur learning unlabeled data 
focus asymptotics adequate want eliminate phenomena vary dataset dataset 
smaller large labeled dataset larger unlabeled dataset classification error dataset larger classification error labeled data 
occurs finite sample size datasets 
performed extensive experiments real artificial datasets various sizes 
experiments em algorithm maximize likelihood expression started classification error log classification error unlabeled records number records log unlabeled unlabeled unlabeled probability error labeled labeled labeled number unlabeled records probability error labeled number unlabeled records labeled fig 

lu graphs example gaussian observables 
sample graph average trials classification error obtained testing labeled samples drawn correct model 
naive bayes classifiers data generated tan model introduced section iii observables variable values points graphs summarize runs testing data bars cover percentiles 
graph enlarged 
note unlabeled data lead significant improvement performance added labeled samples 
performance degradation presence labeled samples 
em algorithm parameters obtained labeled data starting points obtained closed form 
visualize effect labeled unlabeled samples suggest profitable strategy fix percentage unlabeled samples training samples 
plot classification error number training samples 
call graph lu graph 
example consider situation binary class variable values 
real valued observables distributions 
dependency conditional 
suppose build naive bayes classifier problem 
shows lu graphs unlabeled sam ples unlabeled samples unlabeled samples averaging large ensemble classifiers 
asymptotes converge different values 
suppose started labeled samples training data 
classification error see lu graph unlabeled data 
suppose added labeled samples obtain classification error 
suppose added unlabeled samples 
move lu graph lu graph 
classification error increase 
added unlabeled samples move lu graph classification error twice error just labeled samples 
noted difficult classification problems lu graphs decrease slowly unlabeled data may improve classification performance certain regions lu graphs 
problems large number observables parameters require training data expect problems benefit consistently unlabeled data 
figures illustrate possibility naive bayes classifier features 
possible phenomenon addition substantial number unlabeled samples may reduce variance decrease classification error additional larger pool unlabeled data eventually add bias increase classification error 
situation happened results reported nigam classification errors go unlabeled samples added 
summary semi supervised learning displays odd failure robustness certain mod eling errors unlabeled data degrade classification performance 
estimation bias central factor phenomenon level bias depends ratio labeled unlabeled samples 
existing theoretical results semi supervised learning assumption modeling error consequently bias issue far 
iii 
semi supervised learning bayesian network classifiers turn attention implication previous analysis bayesian network classifiers 
stated chose bayesian network classifiers reasons classi fication possible missing data general unlabeled data particular graphical representation intuitive easily expanded add different features modalities efficient algorithms inference 
bayesian network composed directed acyclic graph node associated variable xi conditional distribution xi denotes parents xi graph 
joint probability distribution factored collection conditional probability distributions node graph xn xi directed acyclic graph structure distributions xi represent param eters network 
consider data generated distribution collected 
say assumed structure network correct possible find distribution matches data generating distribution structure incorrect maximum likelihood estimation main methods learn parameters network 
missing data training set em algorithm maximize likelihood 
direct consequence analysis previous section bayesian network correct structure correct parameters optimal classification posteriori distribution class variable accurately represented 
great moti vation obtaining correct structure conducting semi supervised learning 
somewhat surprisingly option searching better structures proposed researchers previously witnessed performance degradation learning unlabeled data 
sections describe different strategies learning bayesian network classifiers labeled unlabeled data 
switching simple models structure learning observe performance degradation may try find correct structure bayesian network classifier 
alas learning bayesian network structure trivial task 
attempt simplest overcome performance degradation unlabeled data assume simple model naive bayes typically correct structure switch complex model soon degradation detected 
family models tree augmented naive bayes tan 
strategy guarantees find correct structure existence efficient algorithm learning definitions follow directly definitions correct incorrect models described previous section 
necessarily unique correct structure structure correct defined structures markov equivalent class correct causality issue 
tan models supervised case semi supervised case switching tan models attractive :10.1.1.112.7737
naive bayes tan classifiers observed successful supervised case success observed semi supervised case section iii 
simple strategies fail performing unconstrained structure learning alternative 
various approaches learning structure bayesian networks different criteria attempt find correct structure 
class structure learning methods consider class independence methods known constraint test methods 
algo rithms obtain correct structure fully reliable independence tests available appropriate classification 
cheng bell liu algorithms cbl cbl particularly suited classification strive keep number edges bayesian networks small possible performance cbl labeled data reported surpass performance tan :10.1.1.43.7564
independence algorithms cbl explicitly optimize metric handle unlabeled data directly optimization scheme em 
handle unlabeled data strategy derived denoted em cbl start learning bayesian network available labeled data em process unlabeled data followed independence tests probabilistic labels generated em obtain new structure 
em new structure cycle repeated subsequent networks identical 
noted scheme intuitively reasonable convergence guarantees test displayed oscillating behavior 
second class structure learning algorithms score methods 
heart score methods likelihood training data penalty terms avoid overfitting 
comparison different methods 
existing methods form handle missing data general unlabeled data particular 
structural em sem algorithm attempt learn structure missing data :10.1.1.24.1555
algorithm attempts maximize bayesian score em scheme space structures parameters method performs increasing search space structures guarantee attainment local maximum 
learning structure classifier score structure learning approaches strongly criticized 
problem finite amounts data posteriori probability class variable small effect score dominated marginal observables leading poor classifiers :10.1.1.30.9978
friedman showed tan surpasses score methods fully labeled case learning classifiers 
point unlabeled data score methods sem go astray reported supervised case marginal observables dominates likelihood portion score ratio unlabeled data increases 
classification driven stochastic structure search sss score independence methods try find correct structure bayesian network fail data reliable independence tests search yields classifier 
consider alternative 
interested finding structure performs classifier natural design algorithms classification error guide structure learning 
leverage properties semi supervised learning know unlabeled data indicate incorrect structure degradation classification performance know classification performance improves correct structure 
structure higher classification accuracy indicates improvement finding optimal classifier 
learn structure classification error adopt strategy searching space structures efficient manner avoiding local maxima 
section propose method effectively search better structures explicit focus classification 
essentially need find search strategy efficiently search space structures 
simple closed form expression relates structure classification error difficult design gradient descent algorithm similar iterative method 
gradient search algorithm find local minimum size search space 
define measure space structures want maximize definition inverse error measure structure ps ps summation space possible structures ps probability error best classifier learned structure metropolis hastings sampling generate samples inverse error measure having compute possible structures 
constructing metropolis hastings sampling define neighborhood structure set directed acyclic graphs transit step 
transition done predefined set possible changes structure transition change consists single edge addition removal reversal 
define acceptance probability candidate structure snew replace previous structure st follows min new new new min pnew nt error nnew transition probability nt nnew sizes neighborhoods st snew respectively choice corresponds equal probability transition member neighborhood structure 
choice neighborhood transition probability creates markov chain aperiodic irreducible satisfying markov chain monte carlo mcmc conditions 
summarize algorithm name stochastic structure search sss 
add temperature factor acceptance probability 
roughly speaking close allow acceptance structures higher probability error previous structures 
close allows acceptance structures improve probability error 
fixed amounts changing distribution sampled mcmc decreasing simulated annealing run aimed finding maximum inverse error measures 
rate decrease temperature determines rate convergence 
asymptotically number data logarithmic decrease guarantees convergence global maximum probability tends 
sss algorithm logarithmic cooling schedule find structure close minimum probability error 
caveats 
logarithmic cooling schedule slow 
faster cooling schedules starting point best nb classifier tan classifier 
second access true probability error structure error 
empirical error training data denoted error 
procedure stochastic structure search sss fix network structure initial structure 
estimate parameters structure compute probability error error 
set 
repeat maximum number iterations reached sample new structure snew neighborhood st uniformly probability nt 
learn parameters new structure maximum likelihood estimation 
compute probability error new classifier new error 
accept snew probability eq 
snew accepted set st snew error new error change temperature decrease schedule 
st st 
return structure sj arg min error 
fig 

stochastic structure search algorithm avoid problem overfitting approaches possible 
cross validation labeled training data split smaller sets tests performed smaller sets test sets 
approach significantly slow search suitable labeled training set moderately large 
approach penalize different structures complexity measure 
bic mdl complexity measure chose multiplicative penalty term derived structural risk minimization directly related relationship training error generalization error 
define modified error term eq 
error error mod hs log log hs hs vapnik chervonenkis vc dimension classifier structure number training records 
approximate vc dimension hs ns ns number free parameters markov blanket class variable network assuming variables discrete 
point reader shown vc dimension naive bayes classifier linearly proportional number parameters 
possible extend result networks features descendants class variable 
general networks features markov blanket class variable effect value classification assuming missing values feature justifying approximation 
initial experiments multiplicative penalty outperformed holdout method mdl bic complexity measures 
evaluation uci machine learning datasets evaluate structure learning methods labeled unlabeled data started empirical study involving simulated data 
artificially generated data investigate sss algorithm finds structure close structure generated data algorithm uses unlabeled data improve classification performance 
typical result follows 
generated data tan structure features 
dataset consisted labeled unlabeled records 
estimated bayes error rate learning correct structure large fully labeled dataset 
obtained classification accuracy 
learned naive bayes classifier labeled records labeled unlabeled records likewise learned tan classifier labeled records labeled unlabeled records em tan algorithm learned bayesian network classifier sss algorithm labeled unlabeled records 
results row table correct structure adding unlabeled data improves performance significantly columns tan em tan 
note adding unlabeled data degraded performance error error learned naive bayes classifier 
structure search algorithm comes close performance classifier learned correct structure 
shows changes test train error search process 
graph shows moves search initialized naive bayes structure 
error usually decreases new structures accepted occasionally see increase error allowed metropolis hastings sampling 
performed experiments uci datasets relatively small labeled table classification results naive bayes tan em cbl stochastic structure search 
xx indicates learning available labeled data 
dataset train test nb em tan em em sss tan lab nb tan cbl artificial satimage shuttle adult chess sets large unlabeled sets table 
results suggest structure learning holds promise utilizing unlabeled data 
clear winner approach sss yields better results cases 
see performance degradation nb dataset 
em tan improve performance tan just labeled data shuttle 
chess dataset discarding unlabeled data tan best approach 
compared likelihood structure learning methods mcmc datasets showing allow algorithms large labeled datasets learn structure resultant networks suffer performance degradation learned unlabeled data 
illustrating iterations sss algorithm shows changes error shuttle dataset 
iv 
learning bayesian network classifiers hci applications experiments previous section discussed commonly machine learning datasets 
sections discuss hci applications benefit unlabeled data 
start facial expression recognition 
probability error test error train error iteration probability error iteration fig 

train test error structure search artificial data shuttle data labeled unlabeled data experiments 
facial expression recognition bayesian network classifiers early paul ekman colleagues performed extensive studies human facial expressions evidence support universality facial expressions 
universal facial expressions representing happiness sadness anger fear sur prise disgust 
ekman inspired researchers analyze facial expressions means image video processing 
tracking facial features measuring amount facial movement attempt categorize different facial expressions 
facial expression analysis recognition basic expressions subset 
pantic rothkrantz provide depth review research done automatic facial expression recognition years 
challenges facing researchers attempting design facial expression recognition systems relatively small amount available labeled data 
construction labeling database images videos facial expressions requires expertise time training subjects 
databases available cohn kanade database :10.1.1.41.8048:10.1.1.41.8048
collecting labeling data humans displaying expressions difficult 
beneficial classifiers learned combination labeled data large amount unlabeled data 
generative bayesian network classifiers 
developed real time facial expression recognition system 
system uses model non rigid face tracking algorithm extract motion features seen serve input bayesian network classifier recognizing different facial expressions 
main motivations bayesian network classifiers problem 
ability learn unlabeled data infer class label features missing due failure tracking occlusion 
second motivation possible extend system fuse modalities audio principled way simply adding subnetworks representing audio features 
experimental design different databases database collected chen huang cohn kanade au code facial expression database :10.1.1.41.8048:10.1.1.41.8048
database subjects instructed display facial expressions corresponding types emotions 
tests algorithms performed set people displaying sequences emotions starting neutral expression 
video sampling rate hz typical emotion sequence samples long 
upper row shows frame subject 
cohn kanade database consists expression sequences subjects starting neutral expression peak facial expression :10.1.1.41.8048:10.1.1.41.8048
subjects database 
subjects facial expressions sequences available subset subjects sequences available 
subject sequence expression average frames expression 
lower row shows examples experiments 
summary databases table ii 
measure accuracy respect classification result frame frame video sequence manually labeled expressions including neutral 
manual labeling introduce noise classification boundary neutral expression sequence necessarily optimal frames near boundary cause confusion expression neutral 
experimental results labeled data start experiments labeled data 
viewed upper bound performance classifiers trained labels removed 
labeled case compare results training artificial neural network ann test bayesian network classifiers compare different kind classifier problem 
perform person independent tests partitioning data sequences subjects test sequences database subjects table ii summary databases sequences sequences subject average frames expression expression expression chen huang db cohn kanade db fig 

examples images video sequences experiment 
top row shows subjects chen huang db bottom row shows subjects cohn kanade db printed permission researchers 
sequences remaining subjects training sequences 
table iii shows recognition rate test classifiers 
classifier learned sss algorithm outperforms nb tan classifiers ann perform compared 
experiments labeled unlabeled data perform person independent experiments labeled unlabeled data 
partition data training set test set training testing choose random portion training set remove labels 
procedure ensures distribution labeled unlabeled sets 
train naive bayes tan classifiers just labeled part training data combination labeled unlabeled data 
sss em cbl algorithms train classifier labeled unlabeled data search fig 

motion units extracted face tracking 
table iii recognition rate person independent test 
nb tan sss ann chen huang database cohn database structure just labeled part small performing full structure search 
table iv shows results experiments 
see nb tan labeled samples adding unlabeled data degrades performance classifiers better unlabeled data 
see em cbl performs poorly cases 
sss algorithm able improve results utilize unlabeled data achieve performance higher just labeled data nb tan 
fact performance lower case training set labeled compared implies relative value labeled data higher unlabeled data shown castelli 
unlabeled data performance expected improve 
applying bayesian network classifiers face detection apply bayesian network classifiers problem face detection purpose showing proposed methods semi supervised learning learn face detectors 
take appearance approach intensity image pixels table iv classification results facial expression recognition labeled unlabeled data 
dataset train test nb em tan em em sss lab nb tan cbl cohn kanade chen huang features classifier 
learning defining bayesian network classifiers look fixed size windows learn face appears windows assume face appears window pixels 
goal classifier determine pixels fixed size window face non face 
note numerous appearance approaches face detection considerable success see yang detailed review state art face detection 
attempt knowledge semi supervised learning face detection 
labeled databases face images available universally robust face detector difficult construct 
main challenge faces appear different different lighting conditions expressions glasses facial hair makeup classifier trained labeled images large number unlabeled images enable incorporating facial variations need label huge datasets 
experiments training set consisting faces non faces obtained mit cbcl face database 
face image cropped resampled window classifier features 
randomly rotate translate face images create training set face images 
addition available non face images 
leave images faces non faces testing train bayesian network classifier remaining 
experiments learn naive bayes tan general generative bayesian network classifiers em cbl sss algorithms 
compare results classifiers receiving operating characteristic roc detection tan nb sss false detection detection sss tan tan nb nb cbl false detection detection sss tan tan nb nb cbl false detection fig 

roc curves showing detection rates faces compared false detection faces different sss tan nb classifiers different ratios labeled unlabeled data data labeled unlabeled data data unlabeled data unlabeled 
curves 
roc curves show different classification thresholds ranging probability detecting face face image pd face face probability falsely detecting face non face image pf face face 
learn training data labeled 
shows resultant roc curve case 
classifier learned sss algorithm outperforms tan nb classifiers perform quite achieving detection rates low rate false alarm 
remove labels training data leaving labeled images train classifiers 
shows resultant roc curve case 
see nb classifier labeled unlabeled data performs poorly 
tan labeled images tan labeled unlabeled images close performance significant degradation performance adding unlabeled data 
classifier data sss outperforms rest roc curve close best roc curve 
shows roc curve labeled data 
nb labeled unlabeled performs poorly sss outperforms classifiers great reduction performance compared roc curves 
experiment shows structure search unlabeled data utilized successfully achieve classifier data labeled 
summary discussion unlabeled data enhance performance classifiers trained labeled data applications pattern recognition computer vision data mining text recognition 
fully utilize potential unlabeled data abilities limitations existing methods understood 
main contributions summarized follows derived studied asymptotic behavior semi supervised learning maximum likelihood estimation 
detailed analysis performance dation unlabeled data showing directly related modeling assumptions regardless numerical instabilities finite sample effects 
discussed implications analysis semi supervised learning bayesian network classifiers importance structure unlabeled data training 
listed possible shortcomings likelihood structural learning algorithms learning classifiers especially unlabeled data 
introduced classification driven structure search algorithm metropolis hastings sampling showed performs fully labeled datasets labeled unlabeled training sets 
note practitioners sss algorithm appears relatively large datasets difficult classification problems represented complex structures 
large datasets labeled data reliable estimation empirical error allowing search complex structures unlabeled data reduce estimation variance complex structures 
real time facial expression recognition system model face tracking algorithm bayesian network classifiers 
showed experiments labeled unlabeled data 
bayesian network classifiers learning detect faces images 
note finding classifier major part face detection system components need designed system natural images ability detect multi scales highly varying illumination large rotations faces partial occlusions 
goal step designing system show feasibility approach training labeled unlabeled data 
discussion semi supervised learning bayesian networks suggests path faced option learning bayesian networks labeled unlabeled data start naive bayes tan classifiers learn labeled data test model correct learning unlabeled data 
result satisfactory sss attempt improve performance computational resources 
methods unlabeled data improve performance supervised tan naive bayes discard unlabeled data try label data active learning example 
investigation semi supervised learning important open theoretical questions research directions possible find necessary sufficient conditions performance degradation occur 
finding conditions great practical significance 
knowing conditions lead design new useful tests indicate unlabeled discarded different model chosen 
important question semi supervised learning methods tive svm training exhibit phenomenon performance degradation :10.1.1.114.9164
extensive studies performed results literature suggest realistic conjecture 
zhang oles demonstrated transductive svm cause degradation performance unlabeled data added 
ghani described experiments phenomenon occurred training 
causes formance degradation similar different algorithms possible unified theory semi supervised learning 
performance guarantees semi supervised learning finite amounts data labeled unlabeled 
supervised learning guarantees studied extensively 
pac risk minimization bounds help determining minimum amount labeled data necessary learn classifier generalization performance 
existing bounds classification performance training labeled unlabeled data 
finding bounds derived principles estimation theory asymptotic covariance properties estimator 
bounds derived pac theoretical approaches 
existence bounds immediately lead new algorithms approaches better utilizing unlabeled data 
fact unlabeled data indicate model incorrectness actively learn better models 
active learning promising possible possible extend active learning learn better models just enhancement parameter estimation 
closing viewed combination main components 
theory showing limitations unlabeled data motivate design algorithms search better performing structures bayesian networks successful application real world problems interested solving learning labeled unlabeled data 
acknowledgments authors acknowledge alex hp labs support discussions analysis semi supervised learning 
portions done hp labs palo alto 
ira cohen funded hp fellowship 
financial support granted hp brazil research 
authors tu face tracking roy wang ming yang help face detection database jeffery cohn larry chen facial expression databases 
moises goldszmidt garg ism nia michael lew discussions various parts reviewers helpful comments 
coded naive bayes tan classifiers java language libraries system freely available www cs cmu edu 
shahshahani landgrebe effect unlabeled samples reducing small sample size problem mitigating hughes phenomenon ieee transactions geoscience remote sensing vol 
pp 

zhang oles probability analysis value unlabeled data classification problems international conference machine learning icml pp 

nigam mccallum thrun mitchell text classification labeled unlabeled documents em machine learning vol 
pp 

bruce semi supervised learning prior probabilities em international joint conference ai workshop text learning supervision seattle washington 
baluja probabilistic modelling face orientation discrimination learning labeled unlabeled data neural information processing systems nips pp 

kohavi scaling accuracy naive bayes classifiers decision tree hybrid proc 
second int 
conference knowledge discovery data mining pp 

cohen cozman value unlabeled data semi supervised learning maximum likelihood estimation tech 
rep hpl hp labs 
pearl probabilistic reasoning intelligent systems networks plausible inference 
san mateo california morgan kaufmann 
garg pavlovic rehg boosted learning dynamic bayesian networks multimodal speaker detection proceedings ieee vol 
pp 
sep 
oliver horvitz garg hierarchical representations learning inferring office activity multimodal information international conference multimodal interfaces 
friedman geiger goldszmidt bayesian network classifiers machine learning vol 
pp 

greiner zhou structural extension logistic regression discriminative parameter learning belief net classifiers proc 
annual national conference artificial intelligence aaai pp 

ekman friesen facial action coding system investigator guide 
palo alto consulting psychologists press 
blake merz uci repository machine learning databases university california irvine dept information computer sciences 
devroye lugosi probabilistic theory pattern recognition 
new york springer verlag 
jaakkola continuations methods mixing heterogeneous sources uncertainty artificial intelligence uai pp 

linear discriminant analysis training samples journal american statistical association vol 
pp 

learning imperfectly labeled examples pattern recognition vol 
pp 

krishnan efficiency discriminant analysis initial samples classified stochastically pattern recognition vol 
pp 

krishnan efficiency logistic normal supervision pattern recognition vol 
pp 

pal pal pattern recognition classical modern approaches 
new jersey world scientific 
cooper freeman asymptotic improvement outcome supervised learning provided additional learning ieee transactions computers vol 
pp 
november 
comparison iterative maximum likelihood estimates parameters mixture normal distributions different types sample biometrics vol 
pp 
december 
neill normal discrimination unclassified observations journal american statistical association vol 
pp 

mclachlan efficiency linear discriminant function unclassified initial samples biometrika vol 
pp 
december 
castelli relative value labeled unlabeled samples pattern recognition 
phd thesis stanford university palo alto ca 
venkatesh learning mixture labeled unlabeled examples parametric side information proceedings eigth annual conference computational learning theory pp 

mitchell role unlabeled data supervised learning sixth international colloquium cognitive science san sebastian spain 
miller uyar mixture experts classifier learning labelled unlabelled data neural information processing systems nips pp 

collins singer models named entity classification international conference machine learning icml pp 

denis gilleron positive unlabeled examples help learning proc 
th international conference algorithmic learning theory watanabe yokomori eds berlin pp 
springer verlag 
goldman zhou enhancing supervised learning unlabeled data international conference machine learning icml pp 

cozman cohen unlabeled data degrade classification performance generative classifiers fifteenth international florida artificial intelligence society conference pp 

cohen semisupervised learning classifiers application human computer interaction 
phd thesis university illinois urbana champaign 
cozman cohen semi supervised learning mixture models international conference machine learning icml pp 

dempster laird rubin maximum likelihood incomplete data em algorithm journal royal statistical society series vol 
pp 

white maximum likelihood estimation misspecified models econometrica vol 
pp 
january 
cozman cohen effect modeling errors semi supervised learning mixture models unlabeled data degrade performance generative classifiers tech 
rep university paulo 
www poli usp br fabio cozman publications ps gz 
ahmed discriminant analysis scale contamination initial sample classification clustering new york pp 
academic press 
mclachlan discriminant analysis statistical pattern recognition 
new york john wiley sons 
friedman bias variance loss curse dimensionality data mining knowledge discovery vol 
pp 

meila learning mixture trees 
phd thesis massachusetts institute technology boston ma 
spirtes glymour scheines causation prediction search 
cambridge mit press nd ed 
pearl causality models reasoning inference :10.1.1.41.8048
cambridge cambridge university press 
cheng greiner kelly bell liu learning bayesian networks data information theory approach artificial intelligence journal vol 
pp 
may 
cheng greiner comparing bayesian network classifiers uncertainty artificial intelligence uai pp :10.1.1.43.7564

allen greiner model selection criteria learning belief nets empirical comparison international conference machine learning icml pp 

friedman bayesian structural em algorithm uncertainty artificial intelligence uai pp :10.1.1.24.1555

metropolis rosenbluth rosenbluth teller teller equation state calculation fast computing machines journal chemical physics vol 
pp 

madigan york bayesian graphical models discrete data int 
statistical review vol 
pp 

hajek cooling schedules optimal annealing mathematics operational research vol 
pp 
may 
roth learning natural language international joint conference artificial intelligence pp 

ekman strong evidence universals facial expressions reply russell mistaken critique psychological bulletin vol 
pp 

pantic rothkrantz automatic analysis facial expressions state art ieee trans pami vol 
pp 

kanade cohn tian comprehensive database facial expression analysis automatic face gesture recognition fg pp :10.1.1.41.8048

cohen garg huang facial expression recognition video sequences proceedings international conference multimedia expo icme pp 

tao huang connected vibrations modal analysis approach non rigid motion tracking ieee conference computer vision pattern recognition pp 

chen joint processing audio visual information recognition emotional expressions human computer interaction 
phd thesis university illinois urbana champaign urbana il 
yang kriegman ahuja detecting faces images survey ieee trans pami vol 
pp 

mit cbcl face database 
mit center biological computation learning www ai mit edu projects cbcl 
bennett demiriz semi supervised support vector machines neural information processing systems nips pp 

blum mitchell combining labeled unlabeled data training proceedings eleventh annual conference computational learning theory pp 

ghani combining labeled unlabeled data multiclass text categorization international conference machine learning icml pp 

ira cohen earned sc 
ben gurion university israel phd 
university illinois urbana champaign electrical computer engineering respec tively 
research scientist hewlett packard research labs palo alto ca works machine learning theory application computer system performance modeling 
research interests probabilistic models computer vision human computer interaction system modeling 
received ph degree leiden university netherlands 
currently faculty science university amsterdam netherlands doing research areas multimedia information retrieval human computer interaction computer vision applications 
author book robust computer vision theory applications kluwer april upcoming book computer vision machine learning approach kluwer summer 
guest editor cviu special issue video retrieval summarization december chair th acm multimedia information retrieval workshop mir conjunction acm multimedia chair human computer interaction workshop hci conjunction eccv 
technical program chair international conference image video retrieval 
published technical papers areas computer vision content retrieval pattern recognition human computer interaction served program committee conferences areas 
member ieee acm 
fabio cozman associate professor university paulo brazil 
earned phd degree robotics school computer science carnegie mellon university worked theory applications sets probability measures bayesian networks semi supervised learning 
marcelo electrical graduate student univ paulo interests areas machine learning computer vision particularly applications require manipulation labeled unlabeled data 
thomas huang received degree electrical engineering national taiwan university taipei taiwan china sc degrees electrical engineering massachusetts institute technology cambridge massachusetts 
faculty department electrical engineering mit faculty school electrical engineering director laboratory information signal processing purdue university 
joined university illinois urbana champaign william everitt distinguished professor electrical computer engineering research professor coordinated science laboratory head image formation processing group beckman institute advanced science technology chair institute major research theme human computer intelligent interaction 
dr huang professional interests lie broad area information technology especially transmission processing multidimensional signals 
published books papers network theory digital filtering image processing computer vision 
member national academy engineering foreign member chinese engineering sciences fellow international association pattern recognition ieee optical society american received fellowship humboldt foundation senior scientist award fellowship japan association promotion science received ieee signal processing society technical achievement award society award 
awarded ieee third millennium medal 
received honda lifetime achievement award contributions motion analysis 
received ieee jack medal 
received king sun fu prize international association pattern recognition pan wen yuan outstanding research award 
founding editor international journal computer vision graphics image processing editor springer series information sciences published springer verlag 
