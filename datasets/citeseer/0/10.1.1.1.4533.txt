unsupervised feature selection multi objective genetic algorithms handwritten word recognition morita sabourin andc suen cole de technologie sup rieure montreal canada centre pattern recognition machine intelligence montreal canada cia universidade cat lica brazil mail concordia ca methodology feature selection unsupervised learning proposed 
multiobjective genetic algorithm minimization number features validity index measures quality clusters guide search discriminant features best number clusters 
proposed strategy evaluated synthetic data sets applied handwritten month word recognition 
comprehensive experiments demonstrate feasibility efficiency proposed methodology 
choice features represent patterns affects aspects pattern recognition problem accuracy required learning time necessary number samples 
way selection best discriminative features plays important role constructing classifiers 
trivial task especially dealing lot features 
order choose subset original features reducing irrelevant redundant ones automated feature selection algorithms 
literature contains studies feature selection supervised learning 
feature selection unsupervised learning investigated 
objective unsupervised feature selection search subset features best uncovers natural groupings clusters data criterion 
difficult task find subset features maximizes performance criterion clusters defined 
problem difficult number clusters unknown happens real life situations 
necessary proceedings seventh international conference document analysis recognition icdar ieee explore different numbers clusters traditional clustering methods means algorithm variants 
light clustering trial 
result may promising especially number clusters large easy estimate 
context feature selection presents optimization function number features validity index measure quality clusters 
genetic algorithm ga offers particularly attractive approach solve kind problems generally quite effective rapid global search large nonlinear poorly understood spaces 
decade ga largely applied feature selection problem 
approach combines different optimization objectives single objective function 
main drawback kind strategy lies difficulty exploring different possibilities trade offs objectives 
order overcome kind problem authors propose multi objective genetic algorithm perform feature selection 
propose methodology feature selection unsupervised learning handwritten month word recognition see section 
nondominated sorting genetic algorithm nsga proposed srinivas deb deals multi objective optimization :10.1.1.26.8615
objective find set nondominant solutions contain discriminant features pertinent number clusters 
criteria guide search minimization number features minimization validity index measures quality clusters 
standard means algorithm applied form number clusters selected features 
proposed strategy assessed synthetic data sets significant features appropriate clusters feature subspace known 
applied handwritten month word recog nition order optimize word classifiers 
experimental results show efficiency proposed methodology 
methodology feature selection unsupervised learning nsga objective functions stated criteria minimization validity index minimization number features 
order measure quality clusters scatter cluster separation widely various researchers 
kim objective functions compute measurements independently 
bandyopadhyay combine objective function davies db index proposed davies 
indices suitable problem normalized number selected features 
due fact geometric distance metrics directly applicable biased dimensionality space variable feature selection problems 
experiments considered normalized db index 
criteria described follows 
db index index function ratio sum scatter cluster separation 
scatter ith cluster computed follows si ci zi ci distance clusters ci cj defined dij zi zj si average euclidean distance vectors cluster ci respect centroid zi 
dij euclidean distance centroids zi zj clusters ci cj respectively 
subsequently compute ri max si sj dij db index defined idb ri corresponds number selected clusters number selected features 
objective achieve proper clustering minimizing db index 
number features observed value db index decreases number features increases see section 
correlated effect normalization index order compensate considered objective minimization number features 
case feature set 
implementation nsga proceedings seventh international conference document analysis recognition icdar ieee experiments nsga bit representation binary codification point crossover bit flip mutation roulette wheel selection elitism 
cases parameters nsga tuned experimentation 
idea nsga ranking selection method emphasize points niche method maintain stable subpopulations points 
varies simple ga way selection operator works 
crossover mutation remain usual 
selection performed population ranked basis individual 
nondominated individuals population identified current population 
individuals assumed constitute nondominated front population assigned large dummy fitness value 
fitness value assigned give equal reproductive potential nondominated individuals 
order maintain diversity population classified individuals share dummy fitness values 
sharing achieved performing selection operation degraded fitness values obtained dividing original fitness value individual quantity proportional number individuals 
sharing nondominated individuals ignored temporarily process rest population way identify individuals second nondominated front 
new set points assigned new dummy fitness value kept smaller minimum shared dummy fitness previous front 
process continued entire population classified fronts 
population reproduced dummy fitness values 
individuals front maximum fitness value get copies rest population 
efficiency nsga lies way multiple objectives reduced dummy fitness function nondominated sorting procedures 
details nsga :10.1.1.26.8615
proposed strategy tries find set solutions discriminant features proper value number clusters chromosome population encodes types information 
positions encode features remaining devoted number clusters 
order find high quality solutions considered objectives minimization number features minimization db index 
computing objective simple bits equal part chromosome provide number selected features 
second evaluated performing clustering 
case standard means algorithm applied form clusters selected features number selected clusters obtained computing bits equal second part chromosome 
evaluation methodology synthetic data sets easy evaluate quality unsupervised learning algorithm especially performing feature selection time due fact clusters depend dimensionality selected features 
order assess proposed methodology improve insight carried experiments synthetic data sets distributions points significant features appropriate clusters feature subspace known 
cases evaluate solutions pareto optimal front examining selected features number selected clusters 
experiment synthetic data set points significant features points form defined clusters 
clusters formed generating points pseudo gaussian distribution standard deviation equal 
illustrates data set 
feature feature 
data set experiment experiment chromosomes composed bits bits encode features remaining position encode number clusters vary 
cases zero cluster meaningless application 
nsga parameters population size number generations probability crossover probability mutation niche distance 
table shows set nondominant solutions nsga 
notice table solutions describe data set depicted 
solutions extremely criteria 
solution clusters feature feature final population dominated solution db index criterion 
table 
solutions experiment sol 
db index 
features clusters features experiment ii proceedings seventh international conference document analysis recognition icdar ieee second synthetic data set points features constructed follows 
clusters formed features way data set 
features similar feature 
features points distributed uniformly 
clusters formed generating points pseudo gaussian distribution standard deviation equal 
illustrates data set projecting points feature subspaces dimensions 
chromosomes represented bits bits encode features remaining position encode number clusters vary 
nsga parameters population size number generations probability crossover probability mutation niche distance 
table shows set nondominant solutions nsga 
observe table features significance selected 
solutions describe data set depicted 
experiment interaction optimization criteria visualized discussed section 
proposed methodology applied handwritten month word recognition regarding results achieved experiments synthetic data sets concluded proposed feature feature similar feature feature feature similar feature similar feature feature feature feature 
dimensional projections data set experiment ii table 
solutions experiment ii sol 
db index 
features clusters features strategy capable identifying significant features relevant number clusters 
step lies evaluating effect month word recognition 
word classifier experiment word verifier date recognition system 
case word image segmented segments graphemes consists correctly segmented segmented segmented character 
feature sets extracted sequence graphemes feed classifiers 
order better assess approach considered feature set mixture segmentation primitives concavity contour features 
grapheme concavity contour feature vector components extracted 
working discrete hidden markov models hmms feature proceedings seventh international conference document analysis recognition icdar ieee vectors contain real values low level features convert symbols high level features clustering technique 
traditional strategy considers entire feature set tries exhaustively various number clusters propose foregoing methodology find automatically proper number clusters discriminant features 
basically proposed methodology works follows nsga produces automatically set nondominant solutions called pareto optimal corresponds best trade offs number features quality clusters 
applying strategy month word recognition solution pareto optimal front chosen system 
order perform task firstly train solution pareto optimal front validate best solutions nsga 
classifiers system solution supplies best word recognition result validation set chosen 
evaluate results achieved approach compare results obtained traditional strategy 
evaluation methodology month word recognition section devoted experiments conducted database knowledge clusters relevant features 
database contains isolated images handwritten brazilian month words janeiro mar maio divided sets images training validation testing respectively 
order increase training validation sets considered word images respectively extracted legal amount database 
possible considering character models 
clustering feature vectors extracted training set words 
chromosomes represented bits bits encode features remaining position encode number clusters vary 
nsga parameters population size number generations probability crossover probability mutation niche distance 
illustrates pareto optimal front nsga selected solution features clusters supplied best word recognition result validation set 
case recognition rate zero rejection level test set 
visualize graphic value db index decreases number features increases 
recognition rates similar results reached traditional strategy tries empirically various number clusters performing feature selection 
case solution brought better results composed features clusters 
achieved validation test sets respectively 

pareto optimal front confirms efficiency proposed methodology selecting powerful subset features proper value number clusters 
reduced number features number clusters keeping recognition rates level traditional strategy 
time required training hmms decreased number clusters significantly reduced 
methodology feature selection unsupervised learning multi objective optimization 
generates set nondominant solutions called pareto optimal corresponds best tradeoffs number features quality clusters 
proposed strategy evaluated synthetic data sets applied handwritten month word recognition order optimize word classifiers 
results achieved show efficiency proposed methodology number features clusters reduced recognition rates kept level traditional strategy 
successfully applied problem feature selection unsupervised learning 
proceedings seventh international conference document analysis recognition icdar ieee bandyopadhyay 
genetic clustering automatic evolution clusters application image classification 
pattern recognition 
davies 
cluster separation measure 
ieee pami 
gy brodley 
feature subset selection order identification unsupervised learning 
proc 
th international conference machine learning 
huang andm jack 
hidden markov models speech recognition 
edinburgh univ press 
kim street menczer 
feature selection unsupervised learning evolutionary search 
proc 
th acm sigkdd international conference knowledge discovery data mining pages 
morita sabourin suen 
segmentation recognition handwritten dates 
proc 
th pages 
oliveira sabourin suen 
feature selection multi objective genetic algorithms handwritten digit recognition 
th icpr pages 
siedlecki sklansky :10.1.1.26.8615
note genetic algorithms large scale feature selection 
pattern recognition letters 
srinivas deb :10.1.1.26.8615
multiobjective optimization nondominated sorting genetic algorithms 
evolutionary computation 

clustering selforganization map 
ieee transactions neural networks may 
