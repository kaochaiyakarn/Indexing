generalized maximum entropy approach bregman clustering matrix approximation banerjee dhillon joydeep ghosh university texas austin tx usa clustering powerful data mining technique varied applications text clustering microarray analysis recommender systems 
informationtheoretic clustering approach applicable empirical joint probability distributions proposed 
situations clustering general matrices desired 
substantially generalized clustering framework bregman divergence objective function various conditional expectation constraints considered statistics need preserved 
analysis problem leads minimum bregman information principle generalizes maximum entropy principle yields elegant meta algorithm guaranteed achieve local optimality 
methodology yields new algorithms encompasses previously known clustering clustering algorithms alternate minimization 
categories subject descriptors artificial intelligence learning general terms algorithms keywords clustering matrix approximation bregman divergences 
clustering bi clustering problem simultaneously clustering rows columns data matrix :10.1.1.26.5045
problem clustering arises diverse data mining applications simultaneous clustering genes experimental conditions bioinformatics documents words text mining users movies recommender systems order design clustering framework need characterize goodness clustering :10.1.1.13.9802
existing clustering techniques achieve quantifying goodness clustering terms approximation error original permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page :10.1.1.26.5045
copy republish post servers redistribute lists requires prior specific permission fee 
kdd august seattle washington usa 
copyright acm 
modha ibm almaden research center san jose ca usa data matrix reconstructed matrix clustering 
techniques efficient scalable algorithms alternate minimization schemes restricted distortion measures kl divergence squared euclidean distance specific matrix reconstruction schemes :10.1.1.13.9802
limitations restrict applicability techniques small range data matrices 
address questions class distortion functions admit efficient algorithms alternate minimization different possible matrix reconstruction schemes clustering algorithms 
show alternate minimization clustering algorithms large class distortion measures called bregman divergences include squared euclidean distance kl divergence itakura saito distance special cases 
demonstrate clustering large variety approximation models possible type summary statistics need preserved 
analysis general clustering problem leads minimum bregman information principle simultaneously generalizes maximum entropy squares principles 
principle related results develop elegant meta algorithm bregman clustering problem number desirable properties 
previously known parametric clustering clustering algorithms alternate minimization follow special cases methodology 
motivation start reviewing information theoretic clustering motivating need general clustering framework :10.1.1.13.9802
denote index running discrete random variables take values sets xu yv respectively 
suppose idealized situation joint probability distribution known 
practice may estimated contingency table occurrence matrix 
suppose want cluster simultaneously cluster disjoint row clusters xg disjoint column clusters yh 
denote corresponding clustered random variables range sets 
information theoretic formulation finding optimal clustering solve problem min mutual information 
shown distribution form denotes kullback leibler kl divergence 
search optimal clustering may conducted searching nearest approximation form 
note depends kl independent parameters smaller mn parameters determine general 
call low complexity low parameter matrix approximation 
viewpoint 
alternate viewpoint highlights key maximum entropy property low complexity low parameter approximation 
lemma fixed clustering consider set joint distributions preserve statistics input distribution distributions distribution maximum shannon entropy 
distributions preserve marginals cluster statistics maximum entropy distribution form 
lemma problem equivalent problem finding nearest kl divergence maximum entropy distribution preserves marginals cluster statistics original data matrix 
formulation applicable data matrix directly corresponds empirical joint distribution 
important situations data matrix general example matrix may contain negative entries distortion measure squared euclidean distance itakura saito distance appropriate 
addresses general situation extending information theoretic clustering directions 
nearness measured bregman divergence 
second allow specification larger class constraints preserve various statistics data 
lastly accomplish generalize maximum entropy approach guide clustering generalization appealing minimum bregman information principle shall introduce shortly 
optimal clustering guided search nearest bregman divergence matrix approximation minimum bregman information satisfying desired constraints 

formulation analysis section formulate bregman clustering problem terms bregman divergence matrix approximation clustering 
proofs omitted due lack space see details :10.1.1.1.2064
start defining bregman divergences 
real valued strictly convex function defined convex set dom domain differentiable int interior bregman divergence int defined gradient 
example divergence log log 
example squared euclidean distance bregman divergences possible define useful concept called bregman information captures spread information random variable 
precisely random variable values dom bregman information defined expected bregman divergence expectation 
example divergence real non negative random variable bregman information corresponding divergence log 
uniformly distributed set xu yv pr xu yv mn mn bregman information proportional constant kl divergence uniform distribution shannon entropy 
example squared euclidean distance real random variable bregman information corresponding squared euclidean distance uniformly distributed elements matrix proportional squared frobenius norm matrix constant 
focus problem clustering data matrix entries take values convex set dom slight abuse notation consider matrix random variable known deterministic function underlying random variables take values set row indices set column indices respectively 
uv denote joint probability measure pair pre specified set uniform distribution 
expectations respect 
example divergence jointly distributed random variables values xu yv respectively 
written form matrix xu yv deterministic function example uniform measure corresponds setting described section originally example squared euclidean distance denote data matrix elements may assume note kl divergence special case divergence applicable probability distributions 
real values 
example uniform measure corresponds setting described 
clustering data matrix pair maps 
natural way quantify goodness clustering terms accuracy approximation obtained clustering quality clustering defined mx nx 
uniquely determined clustering 
clustering number different matrix approximations information choose retain 
random variables row column clusterings respectively values 
clustering involves underlying random variables corresponding various partitionings matrix obtain different matrix approximations solely statistics corresponding non trivial combinations denotes power set element set constraints leads possibly different matrix approximation 
considered class matrix approximation schemes 
sake illustration consider examples corresponding non trivial constraint sets symmetric specified constraint set set possible approximations ma consists depend relevant statistics precisely satisfy conditional independence condition approximations function 
define best approximation corresponding clustering constraint set class ma minimizes approximation error argmin ma 
minimum bregman information interestingly shown best matrix approximation turns minimum bregman information matrix class random variables mb consisting preserve relevant statistics precisely satisfy linear constraints 
best approximation original matrix specified clustering constraint set argmin ma argmin mb 
leads new minimum bregman information principle best estimate certain statistics minimum bregman information subject linear constraints preserving statistics 
easy see widely maximum entropy principle special case proposed principle divergence entropy joint distribution negatively related bregman information example 
fact squares principle obtained special case bregman divergence squared euclidean distance 
theorem characterizes solution minimum bregman information problem 
proof see 
theorem bregman divergence random variable specified clustering specified constraint set solution optimal lagrange multipliers corresponding set linear constraints cr cr cr furthermore exists unique 
bregman clustering problem quantify goodness clustering terms expected bregman divergence original matrix minimum bregman information solution bregman clustering problem concretely defined follows definition bregman divergence data matrix set constraints underlying probability measure wish find minimizes argmin argmin mb 
problem np complete reduction kmeans problem 
difficult obtain globally optimal solution 
section analyze problem detail prove possible come iterative update scheme provides locally optimal solution 
example divergence bregman clustering objective function log log minimum bregman information solution table 
note constraint set joint distribution objective function reduces form example squared euclidean distance bregman clustering objective function minimum bregman information solution table 
note constraint set reduces identical objective function :10.1.1.26.5045

meta algorithm section shall develop alternating minimization scheme general bregman clustering problem 
scheme shall serve meta algorithm number special cases new previously known derived 
section suppose underlying measure bregman divergence data matrix number row clusters number column clusters constraint set specified 
outline essence scheme 
step start arbitrary row column clustering say 
set 
respect clustering compute matrix approximation solving minimum bregman information problem 
step repeat steps till convergence step hold column clustering fixed find new row clustering say set respect clustering compute matrix approximation solving minimum bregman information problem 
set 
step hold row clustering fixed find new column clustering say set respect clustering compute matrix approximation solving minimum bregman information problem 
set 
note time step algorithm may choose perform step 
updating row column clusters outline clear key steps algorithm involve finding solution minimum bregman information problem appropriately updating row column clusters 
focus task 
consider matrix approximations functional form minimum bregman solution 
exist unique set optimal lagrange multipliers uniquely specifies minimum bregman information solution general provides unique approximation say set lagrange multipliers necessarily optimal monotonic function 
underscore dependence lagrange multipliers shall notation ps 
basic idea considering approximations form alternately optimizing clustering lagrange multipliers leads efficient update scheme require solving minimum bregman information problem anew possible clustering 
matrix approximations form nice separability property enables decompose matrix approximation error terms rows columns eu ev ev eu :10.1.1.1.2064
separability property efficiently ob table minimum bregman information solution divergence leads multiplicative models 
constraints approximation table minimum bregman information solution squared euclidean distance leads additive models 
constraints approximation tain best row clustering optimizing individual row assignments keeping column clustering fixed vice versa 
particular optimizing contribution row approximation error leads row cluster update step argmin ev similarly obtain column cluster update step argmin eu far considered updating row column clustering keeping lagrange multipliers fixed 
row column updates approximation closer original matrix earlier minimum bregman information solution necessarily best approximation form 
need optimize lagrange multipliers keeping clustering fixed 
turns lagrange multipliers result best approximation optimal lagrange multipliers minimum bregman information problem new clustering 
observation set minimum bregman information solution steps 
algorithm state meta algorithm generalized bregman clustering see algorithm concrete implementation outline section 
row column cluster update steps minimum bregman solution steps progressively decrease matrix approximation error bregman clustering objective function alternate minimization scheme shown algorithm guaranteed achieve local optimality 
theorem general bregman clustering algorithm algorithm converges solution locally optimal bregman clustering problem objective function improved changing row clustering column clustering 
table row column cluster updates 
ev log log ev log log ev log log ev log log table row column cluster updates squared euclidean distance 
ev eu ev eu ev eu ev eu bregman divergence divergence squared euclidean distance minimum bregman information problem closed form analytic solution shown tables 
straightforward obtain row column cluster update steps tables implement bregman clustering algorithm algorithm 
resulting algorithms involve computational effort linear size data scalable 
general minimum bregman information problem need closed form solution update steps need determined numerical computation 
lagrange dual minimum bregman information problem convex lagrange multipliers possible obtain optimal lagrange multipliers convex optimization techniques 
minimum bregman information solution row column cluster update steps obtained optimal lagrange multipliers 

experiments number experimental results existing literature illustrate usefulness particular instances bregman clustering framework :10.1.1.46.2857:10.1.1.26.5045
fact large class parametric partitional clustering algorithms including kmeans shown special cases proposed framework rows columns clustered 
years clustering successfully applied various application domains text mining analysis microarray gene expression data :10.1.1.13.9802
experimentally re evaluate bregman clustering algorithms methods 
brief case studies demonstrate salient features proposed clustering algorithms dimensionality reduction missing value prediction 
dimensionality reduction dimensionality reduction techniques widely text clustering handle sparsity high dimensionality text data 
typically dimensionality reduction step comes clustering step steps independent 
practice clear dimen algorithm bregman clustering algorithm input matrix sm probability measure bregman divergence number row clusters number column clusters constraint set output clustering locally optimizes objective function 
method initialize repeat step update row clusters argmin ev optimal lagrange multipliers updates 
step update column clusters argmin eu optimal lagrange multipliers updates 
convergence table effect implicit dimensionality reduction clustering classic 
fixed number document word clusters 
reduction technique order get clustering 
clustering interesting capability interleaving dimensionality reduction clustering 
implicit dimensionality reduction results superior results regular clustering techniques 
bag words model text column input matrix represent document row represent word 
keeping number document clusters fixed results varying number word clusters 
ran experiments classic dataset document collection smart project cornell university classes 
clustering performed looking class labels 
confusion matrices cluster labels assigned clustering true class labels various numbers word clusters 
number document clusters fixed experiments reported 
clearly see table classic implicit dimensionality reduction clustering gives better document clusters sense cluster labels agree true class labels fewer word clusters 
missing value prediction illustrate missing value prediction consider collaborative filtering recommender system 
main problem setting predict preference user item known preferences users 
popular approach handle computing pearson correlation user users table mean absolute error movie ratings algo 
idiv idiv pearson error known preferences predict unknown rating proportionately combining users ratings 
adopt clustering approach address problem 
main idea simultaneously compute user item clusters assigning zero measure missing values 
result clustering algorithm tries recover original structure data disregarding missing values reconstructed approximate matrix prediction 
experimental results subset movie dataset consisting users movies containing ratings rating integer bad excellent 
ratings clustering training data ratings test data prediction 
applied different algorithms corresponding constraint sets squared euclidean distance divergence idiv training data reconstructed matrix predicting test ratings 
implemented simple collaborative filtering scheme pearson correlation 
table shows mean absolute error predicted ratings actual ratings different methods 
table observe clustering techniques achieve superior results 
constraint set individual biases users row average movies column average accounted resulting better prediction 
clustering algorithms computationally efficient processing time linear number known ratings 

related primarily related main areas matrix approximation learning bregman divergences 
clustering topic interest years applications problems microarray analysis text mining 
fact exist formulations clustering problem hierarchical clustering model bi clustering model involves finding best clusters time focussed partitional clustering formulation introduced 
matrix approximation approaches singular value decomposition svd widely studied 
quite inappropriate data matrices occurrence contingency tables svd decompositions difficult interpret necessary data mining applications 
alternative techniques involving non negativity constraints approximation loss function proposed :10.1.1.46.2857:10.1.1.127.6264
approaches apply special types matrices 
general formulation interpretable applicable various classes matrices invaluable 
proposed bregman clustering formulation attempts address requirement 
research shown results involving kl divergence squared euclidean distance www research compaq com src eachmovie fact certain convexity properties generalize bregman divergences 
intuition motivated consider clustering bregman divergences 
similarities maximum entropy squares principles prompted explore general minimum bregman information principle bregman divergences 

discussion main contributions 
generalized parametric clustering loss functions corresponding bregman divergences 
generality formulation technique applicable practically types data matrices 
second showed approximation models various complexities possible depending statistics preserved 
third proposed extensively minimum bregman information principle generalization maximum entropy principle 
bregman divergences focussed viz divergence squared euclidean distance proposed algorithm linear time complexity scalable 
research supported nsf iis iis nsf career award aci ibm phd fellowship banerjee 

banerjee dhillon ghosh modha 
generalized maximum entropy approach bregman clustering matrix approximation 
technical report tr ut austin 
banerjee dhillon ghosh 
clustering bregman divergences 
sdm 
censor zenios 
parallel optimization theory algorithms applications 
oxford university press 
cheng church 
biclustering expression data 
pages 
cho dhillon guan sra 
minimum sum squared residue clustering gene expression data 
sdm 
cover thomas 
elements information theory 
wiley interscience 
csiszar 
squares maximum entropy 
axiomatic approach inference linear inverse problems 
annals statistics 
dhillon modha 
information theoretic clustering 
kdd pages 
hartigan 
direct clustering data matrix 
journal american statistical association 
hofmann puzicha 
unsupervised learning dyadic data 
technical report tr icsi berkeley 
lee seung :10.1.1.127.6264
algorithms non negative matrix factorization 
nips 

