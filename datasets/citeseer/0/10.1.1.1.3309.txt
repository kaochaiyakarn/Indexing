manifold learning applications recognition zhang stan li wang intelligent information processing laboratory university shanghai china edu cn face group microsoft research asia beijing china microsoft com key lab complex systems intelligent science institute automation chinese academy sciences beijing china 
wang mail ia ac cn large number data images characters varying intrinsic principal features thought constituting highly nonlinear manifolds high dimensional observation space 
visualization exploration high dimensional vector data focus current machine learning research 
recognition systems linear method bound ignore subtleties manifolds concavities protrusions bottleneck achieving highly accurate recognition 
problem solved high performance recognition system 
years seen progress modeling nonlinear manifolds 
rich literature exists manifold learning 
basis different representations manifold learning roughly divided major classes projection methods generative methods embedding methods mutual information methods 

find principal surfaces passing middle data principal curves 
geometrically intuitive difficulty generalize global variable arc length parameter higher dimensional surface 

second adopts generative topology models hypothesizes observed data generated evenly spaced low dimensional latent nodes 
mapping relationship observation space latent space modeled 
resulting inherent insufficiency adopted em expectation maximization algorithms zhang stan li wang generative models fall local minimum easily slow convergence rates 

third generally divided global local embedding algorithms 
isomap global algorithm presumes isometric properties preserved observation space intrinsic embedding space affine sense 
extensions conformal mappings discussed 
hand locally linear embedding lle laplacian focus preservation local neighbor structure :10.1.1.111.3313

fourth category assumed mutual information measurement differences probability distribution observed space embedded space stochastic nearest neighborhood henceforth sne manifold charting 
impressive results discover features manifold fewer reports practical applications manifold learning especially object recognition 
literature negative lle useful small numbers dimensions classifiers performs better large numbers dimensions pca mapped data 
possible explanation practical data includes large number intrinsic features high curvature observation space embedded space manifold learning methods strongly depends selection parameters 
furthermore data different classes belong similar category example face images recognition implemented subspace manifold learning approaches 
data example character mapped different subspace recognition 
assuming data drawn independent identically distributed underlying unknown distribution propose recognition algorithms processing mentioned cases section 
experiments image character data show advantages proposed recognition approaches 
discuss potential problems researches 
manifold learning algorithm dimensionality reduction establish mapping relationship observed data corresponding low dimensional data locally linear embedding lle algorithm obtain corresponding low dimensional data training set :10.1.1.111.3313
dataset modeling subsequently mapping relationship 
main principle lle algorithm preserve local order relation data embedding space intrinsic space 
sample manifold learning applications recognition observation space linearly weighted average neighbors 
basic lle algorithm local covering numbers described follows step define xi consider constraint wij xi xj neighbor wij compute weighted matrix square 
step define yi arg minw 
consider constraint yi number local covering set 
calculate arg miny 
step algorithm equivalent approximate nonlinear manifold point xi linear hyperplane passes neighbors xi 
xik 
considering objective invariant translation constraint term yi added step 
term avoid degenerate solution 
step reduces eigenvector decomposition problem follows arg min yi arg min arg min optimal solution formula smallest eigenvectors matrix 
obviously eigenvalues zero need removed 
need compute bottom eigenvectors matrix discard smallest eigenvector considering constraint term 
obtain corresponding low dimensional dataset embedding space 
completed set subsequent modeling mapping relationship 
disadvantage lle algorithm difficult compute mapping test samples due computational cost 
respect approximation theorem gaussian rbf kernel approximate relationship ik xi zhang stan li wang xi xi exp xi parameter depends dimensionality usually predefined user directly computed complete data 
name procedure manifold learning algorithm mla means manifold learning approaches employed reducing highdimensional data low dimensional space 
linear discriminant analysis assuming data different classes similarity categories instance facial images sampled difference persons viewed having cognitive concept 
data different classes reduce subspace manifold learning approaches 
mla capable recovering intrinsic low dimensional space may optimal recognition 
highly nonlinear manifolds mapped low dimensional subspace mla example reason believe optimal classification hyperplane exists manifolds 
principal axes low dimensional mapping classes manifolds acute angle classification ability may impaired 
linear discriminant analysis lda introduced maximize separability data different classes 
suppose class equal probability event class scatter matrix defined sw ni yj mi yj mi ni samples class class means mi 
mean samples classes class scatter matrix defined sb mi mi 
maximize class distances minimizing distances manifolds column vectors discriminant matrix eigenvectors sw associated largest eigenvalues 
projection matrix play role projects vector low dimensional face subspace discriminatory feature space 
combination lle lda mla lda avoid problem dimensionality curse recognition task realized basis reduced dimensions 
nonlinear auto associative model data classified remarkable different categories example characters mla lda inefficient recognition data manifold learning applications recognition commonly mapped single subspace mla 
corresponding strategy extract principal features manifolds dimensionality reduction methods separately unknown sample auto associated light principal features 
light eq 
construct reconstructed formula manifold learning follows ni jk yj yj yj exp yj weighted inverse mapping matrix reconstruction matrix 
choosing appropriate eq 
data reasonably reconstructed model representing category 
example frey face database pixels examples explain proposed method :10.1.1.111.3313
firstly mla learning stage cluster centers extracted vector quantization mapped space lle 
face examples mapped space mapping learned stage shown 
thirdly randomly sample points upper left lower right corner points rectangle sample evenly spaced points boundary diagonal lines rectangle points reconstructed eq displayed 
observe continuous change vertical axes pose change right side left side 
approximately recovered intrinsic principal features expression pose frey database proposed method 
compare dif examples mapped mla subspace 
corresponding reconstruct faces fig 

dimensional mapping reconstruction frey face data zhang stan li wang ference original images reconstructed images points dim reduced data randomly sampled reconstructed proposed method 
original facial images shown top corresponding reconstructed facial images bottom 
seen proposed method effectively reconstructs images 
origin data sampling data dimensional samples origin images corresponding reconstruction images corresponding reconstruct faces fig 

dimensional mapping reconstruction frey face data procedure foundation cognitive sciences autoassociation argues recalling objects concepts achieved preserving underlying principal features objects concepts 
call proposed model nonlinear auto associative modeling nam 
obvious frey data belong class 
implement recognition nam assume represents ith nam ith class model total different example character corresponds th nam th nam 
assumption data different classes represented different mld lda completed data realized mla separately 
consider auto associative properties corresponding auto associative sample nam class different class higher similarity original sample 
obvious variety similarity measure techniques adopted 
recognition achieved comparing probability metric unknown sample corresponding auto associative sample different 
loss generality probability metric gaussian function sample auto associative sample ith nam exp manifold learning applications recognition means auto associative sample ith nam unknown sample difficult see reconstructed sample original sample nam equal reconstructed sample far away original sample decreased zero rapidly respect properties gaussian function 
guarantee consistency probability metric normalization performed 
corresponding equation exp exp consider formula nam probability metric autoassociative samples original sample highest viewed criterion recognition 
equation recognition cl arg max proposed nam obvious merits firstly nam avoids problems local minimum convergence rate 
secondly proposed nam constructive geometrically intuitive 
thirdly find unlabeled sample predefined threshold 
suggests semisupervised learning characteristics partially labelled data needed learning 
add new redesigning original 
experiments experiments performed object face databases olivetti database umist database jaffe database character databases uci character database ocr optical character recognition database evaluate feasibility proposed nonlinear dimension reduction mla lda method nam method respectively 
image recognition object database provided cambridge laboratories known olivetti database consists different images people female male subjects 
images taken different time varying lighting facial expressions open closed eyes zhang stan li wang smiling non smiling facial details glasses glasses 
images taken dark homogeneous background people right frontal position 
unstructured intermediate changes degrees head pose 
examples orl database shown 
crop images pixels dimensions 
fig 

examples orl face database training set test set divided way images persons randomly partitioned sets training images test images overlapping sets 
second umist database consists images people varied poses 
images subject cover range poses right profile degree frontal degree 
examples umist database shown 
images umist database cropped size dimensions 
main difficulty umist database face data observation space may higher curvature stronger nonlinearity multiple views frontal views 
aspect computer vision variations images face due illumination viewing direction larger image variations due change face identity 
multi view face recognition great challenge 
experiment randomly select images person training set remaining images test set 
jaffe database facial expression recognition consists images japanese females 
head frontal pose 
number image represent categories expressions neutral happiness sadness surprise anger disgust fear 
experiment database oriental face recognition manifold learning applications recognition examples umist face database examples jaffe face database expression recognition 
images jaffe database cropped size pixels 
face recognition jaffe database partitioned sets images persons randomly extracted training set remaining images test images 
expression recognition images categories randomly extracted training set remaining images test set 
examples jaffe database shown 
dimension lle reduced data set jaffe face recognition dimension 
nd mapping lda reduction reduced dimension 
eigenvalues eigenvectors complex values 
remain real value part complex values th reduced dimensions higher 
order compare performance mla lda dimensionality reduction introduce classical linear dimensionality reduction algorithm pca principal component analysis design combinational algorithms face recognition combination nearest neighborhood classifiers pca lda pca lda nn combination means classifiers pca lda pca lda combination nearest neighborhood mla lda mla lda nn combination means classifiers mla lda mla lda 
experimental data normalized 
experimental results average runs 
experiments parameters neighbor factor lle algorithm kernel function need predefined 
loss generality set orl umist jaffe expression database jaffe face database set orl umist databases jaffe expression face database 
zhang stan li wang experimental results illustrated respectively 
error rates er face recognitions tabulated table 
means mla lda nn mla lda pca lda nn pca lda table respectively 
error rates error rates orl face recognition 
th reduced dimensions mla pca 
th reduced dimensions lda 
neighbor factor lle 
variance mla mla lda nn mla lda pca lda nn pca lda number dimension orl face recognition jaffe face recognition 
th reduced dimensions mla pca 
th reduced dimensions lda 
neighbor factor lle 
variance mla number dimension jaffe face recognition mla lda nn mla lda pca lda nn pca lda fig 

recognition comparison error rates error rates mla lda nn mla lda pca lda nn pca lda umist face recognition 
th reduced dimensions mla pca 
th reduced dimensions lda 
neighbor factor lle 
variance mla number dimension umist face recognition mla lda nn mla lda pca lda nn pca lda jaffe express recognition 
th reduced dimensions mla pca 
th reduced dimensions lda 
neighbor factor lle 
variance mla number dimension jaffe express recognition table 
error rates mla lda database dim mla lda nn mla lda pca lda nn pca lda orl umist jaffe express manifold learning applications recognition figures table seen dimension face manifolds remarkable reduced proposed mla lda methods 
example ratio original dimensions th biggest reduced dimension orl database 
compared pca lda recognition obtain better results reduced dimensions face databases umist database 
example reduced dimensions error rates mla lda algorithm mla lda nn pca lda nn pca lda orl face database 
worthy noting parameters influence final experimental results 
example influences variance gaussian rbf kernel training samples orl face recognition illustrated respectively 
observe curve error rate parameter appear valley corresponding lowest error rates adjusting parameter assume parameter may selected automatically 
training sample person proposed mla lda methods lowest error rates 
error rates orl face recognition mla lda mla lda nn logarithm variances variance influences fig 

influences parameters error rates mla lda nn mla lda pca lda nn pca lda orl face recognition 
th reduced dimensions mla pca 
th reduced dimensions lda 
neighbor factor lle 
variance mla training samples influences training samples comparing recognition performance proposed mla lda mla cite experimental results literature table nm means nearest manifold approach 
detail nm approach seen 
difficult see proposed mla lda greater reduced dimensions mla note reason total number classes mentioned data reduced dimensions data 
comparison recognition ability zhang stan li wang table 
error rates mla database dim mla nm mla nn pca nm pca nn umist orl jaffe express recognition results mla lda better mla average sense 
character recognition character dataset uci repository comprises total labelled samples average examples class total number classes 
character images different fonts character fonts randomly distorted produce file unique stimuli 
stimulus converted numerical attributes statistical moments edge counts scaled fit range integer values 
examples character images illustrated 
wide diversity different fonts primitive nature attributes recognition task especially challenging 
database randomly partitioned disjoint fig 

examples uci character databases ing sets training samples classes training set remaining samples test set 
second database created national institute standards technology nist contains handwritten characters 
average characters class classes 
character represented dimensional feature vector edge tangents 
dimension datasets linearly scaled interval 
experiment randomly manifold learning applications recognition select samples character concept training set remaining samples test set 
error rates uci letter recognition neighbor factor lle variance nam reduced dimension uci database fig 

character recognition error rates ocr letter recognition neighbor factor lle variance nam reduced dimension ocr database biggest reduced dimensions extracted spectral properties lle algorithm biggest reduced dimensions uci ocr respectively dimensions gradually decreased descending order eigenvalues lle algorithm :10.1.1.111.3313
furthermore parameters need predefined 
loss generality neighbor factor lle set 
consider dimensionality difference mentioned databases parameters equal uci character database equal ocr database 
independent established different classes 
experimental results average runs 
experimental results databases illustrated 
seen principal dimensions equal lowest error rates character databases obtained 
assume possible number principal features character manifolds extracted 
comparing recognition performance proposed known state art algorithms cite experimental results literature table 
table proposed nam classifiers better recognition rates 
instances uci character database error rates nn mlp ocr character database error rates nn mlp 
furthermore proposed character databases fewer features model feature spaces 
noticeable experimental results average runs results average runs 
investigate error rates character recognition top matches zhang stan li wang table 
average error rates algorithms character databases nam nearest neighbor nn maximum likelihood classifier mlc bayesian pairwise classifier single gaussian voting combination method bpc map estimate combination bpc bayesian pairwise classifier mixture gaussian voting bpc map estimate bpc combination error rates classifier uci ocr nam dim dim nn mlp mlc bpc bpc bpc bpc uci letter recognition neighbor factor lle variance nam rank uci database error rates fig 

rank performance ocr letter recognition neighbor factor lle variance nam rank ocr database proposed nam 
lets know characters examined get desired level performance 
performance statistics reported cumulative error rates plotted 
horizontal axis graph rank vertical axis percentage error rates 
example top matches error rate nam uci error rate nam ocr database 
top matches error rate nam uci database error rate nam ocr database 
worthy noting parameters neighbor factor lle algorithm variance reconstructed variance number training sample influence recognition results 
observe curve error rates parameter appear valley corresponding lowest error rates adjusting parameter example manifold learning applications recognition influences variance character recognition illustrated 
lowest error rates obtained equal uci ocr respectively 
observe parameter independent selection parameter similar phenomenon curve error rates 
parameter may automatically selected 
influences error rates uci letter recognition neighbor factor lle nam logarithm variance variance influence uci database error rates error rates uci letter recognition neighbor factor lle variance dim ocr letter recognition neighbor factor lle nam logarithm variance variance influence ocr database nam vq nam number training samples class influence training samples uci character recognition fig 

parameter influences number training samples recognition rates investigated results uci database illustrated 
noted remarkable improvement error rates recognition tasks number training samples increases 
experiments influence selecting gaussian rbf centers proposed nam vector quantization techniques vq carried uci character databases difficult see combination nam vq error rate remarkable decreased nam vq obtain error rates fewer rbf centers training samples 
zhang stan li wang propose recognition approaches mla lda nam manifold learning 
data classified belongs similar cognitive category face mla lda employed 
nam approach implemented 
assuming objects manifolds different classes lie feature subspace object manifolds different classes reduced intrinsic principal feature subspace proposed mla 
distances enlarged classes decreased lda 
final classification task completed reduced dimensions mla lda 
data classified remarkable different categories example character recognition achieved common feature subspace unreasonable 
proposed nonlinear dimension reduction method mla lda effective case 
propose new constructive nonlinear auto associative modeling manifold learning 
proposed nam principal features extracted preserving principal structure manifold reconstruction achieved 
auto associative mechanism reconstructed data nam having cognitive concept having different cognitive concept deviation 
probability metric recognition naturally established 
nam obvious merits firstly avoids problems local minimum convergence 
secondly constructive geometrically intuitive 
nonlinear auto associative modeling construction mapping inverse mapping relationship observation space corresponding feature spaces dimensionality limitation 
potential problems remains 
parameters influence experimental results 
select parameter automatically worthy research 
second number principal features manifold related error rates recognition 
necessary find effective approach estimate number principal features 
proposed nam semi supervised learning characteristics find unlabelled sample predefined threshold facilitates adding new redesigning original utilize properties modelling unknown concepts designing new algorithms 

hastie stuetzle curves journal american statistical association pp 
manifold learning applications recognition 
gl linder zeger learning design principal curves ieee transactions pattern analysis machine intelligence vol 
pp 


bishop williams gtm generative topographic mapping neural computation pp 

chang ghosh unified model probabilistic principal surfaces ieee transactions pattern analysis machine intelligence pp 

smola mika regularized principal manifolds computational learning theory th european conference vol lecture notes artificial intelligence new york springer pp 

tenenbaum de silva langford global geometric framework nonlinear dimensionality reduction science pp 

silva tenenbaum unsupervised learning curved manifolds nonlinear estimation classification springer verlag new york 

roweis nonlinear dimensionality reduction locally linear embedding science pp 

mikhail belkin niyogi 
laplacian eigenmaps dimensionality reduction data representation neural computation 

hinton roweis stochastic neighbor embedding neural information proceeding systems synthetic vancouver canada december 

brand merl charting manifold neural information proceeding systems natural synthetic vancouver canada december 

dick de ridder robert duin locally linear embedding classification technical reports delft university technology netherlands 
daniel swets john weng discriminant eigenfeatures image retrieval ieee transactions pattern analysis machine vol 
pp 

face recognition hidden markov models phd thesis university cambridge 

wechsler phillips bruce fogelman soulie huang eds em characterizing virtual general purpose face recognition daniel graham nigel 
face recognition theory applications nato asi series computer systems sciences vol 
pp 


michael lyons julien shigeru akamatsu automatic classification single facial images ieee transactions pattern analysis machine intelligence vol pp 


frey slate 

letter recognition holland style adaptive classifiers 
machine learning 

kumar ghosh crawford 
bayesian pairwise classifier character recognition 
cognitive neural models word recognition document processing 
ed world scientific press 

steve lawrence lee giles tsoi back face recognition convolutional neural network approach ieee transactions neural networks vol 
pp 

zhang stan li wang 
moses ullman face recognition problem compensating changes illumination direction proceedings european conference computer vision vol 
pp 

turk pentland eigenfaces recognition journal cognitive neuroscience vol pp 
zhang stan li wang nearest manifold approach face recognition th ieee international conference automatic face gesture recognition may seoul korea 
