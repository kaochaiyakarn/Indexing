leveraging margin carefully nir krause cs huji ac il yoram singer singer cs huji ac il school computer science engineering hebrew university jerusalem israel boosting popular approach building accurate classifiers 
despite initial popular belief boosting algorithms exhibit overfitting sensitive label noise 
part sensitivity boosting algorithms outliers noise attributed unboundedness margin loss functions employ 
describe leveraging algorithms build boosting techniques employ bounded loss function margin 
algorithm interleaves expectation maximization em algorithm boosting steps 
second algorithm decomposes non convex loss difference convex losses 
prove algorithms converge stationary point 
analyze generalization properties algorithms rademacher complexity 
describe experiments synthetic data natural data ocr text demonstrate merits framework particular robustness outliers 

focus problem supervised classification learning 
setting receive training set instance label pairs xi yi concreteness assume xi rn denote coordinate xij 
confine discussion binary problems yi 
class classifiers consider set threshold linear classifiers sign ir assume data linearly separable exist examples xi yi yi xi ir problem finding linear classifier attains minimal number classification errors np hard 
learning techniques employ smooth convex bound classification error widely 
specifically boosting techniques losses appearing proceedings st international conference machine learning banff canada 
copyright author owner 
tial loss logistic loss see instance unified account losses boosting :10.1.1.135.1357
problem convex losses diverge margin example yi xi tends 
overcome problem approaches employ non convex losses proposed 
unfortunately methods yield boosting algorithm classical notion due impossibility theorem duffy helmbold 
boosting style methods achieve pac boosting property referred leveraging algorithms duffy helmbold 
leveraging methods yield effective accurate classifiers robust label noise 
propose different leveraging algorithms employ non convex losses margin 
proposed algorithms simple implement original adaboost algorithm freund schapire closely related variants :10.1.1.135.1357
algorithms employed sequential manner single feature weak hypothesis chosen boosting step fully parallel mode weights features updated :10.1.1.135.1357
classical sequential mode algorithms induce criterion objective function guides weak learner search new hypothesis add 
loss criterion directly related decrease loss due addition new hypothesis 
algorithm described sec 

start giving probabilistic account assumes existence unobserved label noise 
derive algorithm combines expectation maximization em estimation procedure boosting steps 
algorithm weights examples margin probability outliers 
boosting style steps update weights 
prove algorithm converges stationary point 
second approach sec 
decomposes non convex loss function difference logistic functions clearly convex 
derive leveraging algorithm additive update boosting 
analyze loss functions algorithms employ sec 

derive generalization bound algorithms rademacher complexity sec 

conclude experiments synthetic natural data demonstrate robustness algorithms label noise 
experiments described sec 

various previous research papers building blocks 
baum em algorithm maximum likelihood incomplete data formally introduced 
boosting steps boosting algorithms described :10.1.1.135.1357
algorithmic approach minimizing difference convex functions described thoroughly employed classification learning devise robust support vector machines 
generalization analysis employ generalization bounds 
remotely related numerous papers machine learning statistics 
em missing labels corrupted labels mentioned discussion dempster laird rubin 
example application em missing labels 
idea combining em iterative minimization procedure different context different analysis explored wang 
works try improve boosting give algorithms robust label noise 
survey 
algorithm directly minimizes loss steps weak learners known accuracy described 
idea enhanced adaptive algorithm complex algorithm 
pac boosting algorithm developed smooth distributions 
algorithm tolerate low malicious noise rates requires access noise tolerant weak learner known accuracy 
similar approach papers 
employ non convex normalized sigmoid function loss function leveraging algorithm doom ii 
algorithm includes sequential version objective function weak learner minimize directly related decrease loss 
algorithm decreases weight examples large influence previous rounds 
extensions solve lp problem lp equivalent svm problem 
algorithms require line searches compute weights weak learners algorithms described simple implement updates computed analytically 

logistic mixture model 
probabilistic model probabilistic model assume noise free true label ti generated logistic model depends xi vector assume noise bernoulli variable parameter probability correct label inverted ob served label yi obtained 
conditional probabilities due assumptions follows ti xi yi ti xi ti xi ti xi ti yi ti yi define probability observed label yi different noise free label ti 
fact probability label noise occurred xi yi def ti yi xi yi goal devise classifier resistant noise boosting incorporating noise model 
cast task finding parameters minimize negative log likelihood observed data call logistic mixture loss llm ln xi ey xi algorithm description experiments allow fixed estimate value 

combining boosting updates em em algorithm standard tool choice parameter estimation incomplete data 
algorithm composed steps expectation step maximization step 
repeating steps converge stationary point log likelihood model 
specify em iterations settings 
step replace hidden variable unobserved value ti expectation def denote 
step set parameters maximize expectation log likelihood unobserved data observed data probabilities calculated argmax ti xi yi ln yi ti xi ln ln ln xi yi ln maximization respect gives minimizer loss ln xi yi ln expression implies maximizing respect equivalent minimizing weighted logistic loss examples original example xi yi replaced weighted examples xi yi weight xi yi weight llm update requirements aj mij aj init qi mij mij dj ln argmax update qi mij qi mij aj 
llm algorithm 
approximate step focus adapting boosting algorithm finding minimizer eq 

analytical solution minimizer need devise iterative minimization procedure 
iterations interleaved step em computes auxiliary variables derive approximate step briefly describe algorithms described collins 
algorithm belongs family algorithms finding minimizer log loss ln yi xi 
find minimizer log loss collins devised iterative procedure 
denote estimate minimizer log loss th iteration 
example xi yi assigned weight qi estimate qi eyi weight example reflects probability misclassifying example logistic model defined eq 

ease reading define matrix mi matrix weights qi denote ij feature qi mij ij qi mij informally speaking weights reflect correlation probability misclassify examples absolute value th feature example 
estimate computed current estimate weights follows ln 
update derived noise free classification settings log loss 
setting involved 
observe noisy version labels need minimize loss respect hidden true labels 
example associated possible true labels yi yi probabilities respectively 
possible label need take account weight probability logistic model eq 
reflected qi 
result need go indices compute split contribution example yields calculation loss defined eq 
qi mij ij mij ij ij qi mij qi mij qi mij proceed compute eq 

step em algorithm requires finding minimizer eq 

easily done iterations update rule 
show section sufficient perform single update approximate step 
case update decreases minimizes fact gem algorithm 
performing single approximate step simplify boosting update 
case rewrite eq 
eq 
avoid ad ditional computation imposed computing due fact value employed computation qi 
describe efficient single approximate step 
define mij qi mij mij qi mij 
fixed easy verify qi qi 
rewrite original variables wj pseudo code version alternates step single boosting step fig 

term algorithm leveraging logistic mixture algorithm llm 
version experiments variants extensions multiclass derived 
details variants extensions omitted due lack space 
parametric family algorithms algorithm receives input template set template set describe unified view parallel algorithm sequential 
choice algorithm implemented done setting appropriate template set 
setting obtain parallel algorithm algorithm updates indices step 
ei ei denotes ith unit vector receive sequential boosting algorithm 
algorithm simple criterion choosing weak learner step choose weak learner maximizes proof convergence holds change iteration long constraints defined kept satisfied 
implement combined version sequentially update update weight weak learners accumulated far parallel 

convergence analysis section discuss convergence llm algorithm 
short llm algorithm belongs class generalized em algorithms 
lemma shows approximate step guaranteed increase value auxiliary function proof combines proof techniques standard analysis em 
lemma set parameters obtained update 
equality stationary point proof prove theorem view loss induced sample examples examples xi yi weights examples xi yi denoted xi xi yi yi weights 
representation rewrite ln yi xi constant depends denote qi weight example fore update respectively qi get equations qi equality inequality get ln ln qi qi 
rewrite follows ln 
note rewrite weights follows ij mij ij mij 
denoting sij sign mij lower bound change step follows eq 
eq 
eq 
ln ln qi exp mij aj mij aj dj wj dj aj def eq 
follows jensen inequality applied convex function condition aj mij get eq 
dj ln algorithm 
function auxiliary function bounds increase clearly wj get iff simple derivation means stationary point reached stationary point value continue increase approximate step 
definition lemma see series monotonically increasing converges value stationary point 
directly follows loss llm converges value stationary point 

logistic difference model section seemingly different approach obtain bounded margin loss function corresponding leveraging algorithm 
devising mixture model logistic function devise bounded margin loss difference logistic functions follows 
denote losses values log exp log exp mu ld loss loss 
construction ld loss 
margin example 
logistic loss example margin ln 
positive constant 
shifted version loss obtained lld requirements ajm aj init qi gi wj mi qi gi argmax 
lld algorithm 
adding ln 
define loss margin difference logistic loss shifted version ln ln 
construction loss described fig 

term loss logistic difference ld loss 
logistic mixture model find vector sign xi equal yi possible 
logistic difference loss sample lld yi xi yi xi ln xi yi xi ln leveraging algorithm ld loss additive update boosting 
algorithm example weight equal minus derivative loss evaluated example margin 
formally dz ez weight example attaining margin zi yi xi zi zi 
pseudo code resulting leveraging algorithm called leveraging logistic difference algorithm lld fig 

intuitively leveraging iteration lld approximates concave part ld loss linear function 
difference convex function linear function convex boosting technology 
specifically lld performs additive boosting step analogous described function 
exploit construction derive sequel lower bound progress lld prove convergence 
convergence analysis convergence proof divided parts 
part show simple convex upper bound logistic difference loss 
second part show th iteration decreases upper bound reached stationary point 
get logistic difference loss decreases iteration converges stationary point bounded 
recall differentiable concave function bounded affine point applying concave part loss get yi xi yi xi yi xi yi xi yi xi yi xi 
upper bound linear convex 
add convex logistic loss get convex bound loss yi xi yi xi yi xi yi xi 
deduce lld equality concentrate efforts showing decreases stationary point 
lemma decrease satisfies proof prove lemma quadratic function upper bounds fixed prove progress upper bound 
define order prove upper bounds define difference show 
showing convex minimum attained 
note 
stationary point 
second derivative get routine calculations yield xi plug definitions algorithm 
note 
cauchy schwartz inequality inequality constraint inequality 
shown upper bounds bound progress follows corollary decrease loss step satisfies lld lld lemma algorithm converges value stationary point loss 
proof long lld decrease loss positive amount lld 
sequence losses decreases 
bounded converge 
continuity convergence point point lld 
unified view loss functions section show simple bijection losses llm lld identical 
describe simple affine transformation normalizes loss resulting loss approaches margin goes bounding loss 
fix 
add constant loss divide result constant order normalize losses 
resulting normalized losses ln lm ez ln ln ln ld ln ln ln ln examining carefully losses see setting algebraic manipulations obtain ld ln ln ln ln ez ln ln losses identical subject transformation variables ln equivalently losses equivalent refer analysis logistic mixture loss 
calculate lipschitz constant loss find constant ln lm constant note ln ln ln lm bounded 
generalization analysis ln ln ln generalization bounds classes functions rademacher complexity 
short rademacher complexity class functions denoted rm defined rm rm rm sup xi xm independent random variables sampled uniformly 
bounds suited decision theoretic settings attempt minimize combinatorial loss function classification error minimization dominating cost function range 
case loss function classification error dominating cost function loss lm defined eq 
range 
order rademacher bounds need slightly generalize thm 
cost functions range 
theorem bartlett mendelson thm 
consider loss function dominating cost function 
class functions mapping xi yi dependently selected probability measure integer probability samples length satisfies el rm ln 
derive bound problem need bound rademacher complexity linear classifiers 
bartlett mendelson proved bound kernel functions 
bound rewritten setting tation follows assume xi rm algorithm assumed xi follows xi 
standard regularization techniques described implies means combining norm inequalities get case rm conclude adapting result theorem setting 
bound lipschitz constant test error test error boosting lld llm number noisy quarters boosting lld llm number noisy quarters test error boosting lld llm number noisy quarters test error boosting lld llm number noisy quarters 
comparison boosting llm lld synthetic data different noise rates 
calculated sec 
cost function lm applying theorem get corollary probability satisfies ml ln lm ln ln ln ln ln 
experiments synthetic data experiment generated random points ir multivariate normal distribution 
randomly picked hyper plane ir normal distribution assigned th point label yi sign xi 
divided points groups margin 
groups numbered group contains quarter points attaining largest margin fourth group contains quarter points attaining smallest margin 
conducted experiments 
experiment different subset groups contaminated label noise 
experiment group contaminated label noise 
second experiment group contaminated label noise distributed bernoulli 
third set experiments contaminated second group groups contaminated label noise resulting uniform bernoulli noise points 
contaminated subset points noise rate experiment 
experimental setup compared log loss version parallel boosting algorithm llm algorithm set lld algorithm set ln 
compare performances algorithms generated test set containing points 
test set contrast train set noise free 
compatible assumption sec 
data linearly separable train set contaminated label noise 
repeated experimental setup different values experiment repeated times 
average results fig 
plot corresponds different value clear boosting sensitive malicious noise flips labels points attain large margin values 
seen high increase error caused contamination quarter high margin examples 
contaminate quarters increase error smaller 
contrast llm leverages type noise 
error llm low contaminate quarters 
quarter contaminated significant increase error 
error llm lower boosting 
error lld increases linearly amount contaminated quarters 
lld sensitive margin outliers boosting effective llm 
poor performance boosting surprising quite papers noted boosting algorithms highly sensitive label noise see instance 
type noise far crucial llm lld conveys information examples labels incorrect 
error rate llm lld increases noise uniform 
bernoulli noise llm lld exhibit far lower test error boosting 
usps usps postal service dataset known challenging classification task particularly training set test set collected different manner 
dataset contains training examples test examples 
example represented matrix entry matrix pixel take value 

example associated label 
digit content image 
broke class problem binary problems 
th problem discriminate digit rest 
binary problem compared test error algorithms log loss boosting llm fixed llm variable starting modified approximate steps lld algorithm 
comparative representation results seen left side fig 

show difference test error percentage wise boosting test error algorithms 
group bars describes results algorithms digits 
see boosting steps lld algorithm best performing algorithm 
increase number iterations llm especially fixed value takes charge 
llm clearly outperforms boosting error advantage boosting error advantage boosting error advantage boosting iterations llm var llm llm lld iterations llm var llm llm lld iterations llm var llm llm lld error advantage boosting error advantage boosting error advantage boosting iterations llm llm llm llm llm iterations llm llm llm llm llm iterations llm llm llm llm llm 
comparison llm lld algorithms various parameters boosting algorithm 
parallel version usps dataset left sequential version usenet data set right 
show difference boosting test error test error algorithms various parameters 
algorithm 
may partially contributed margin loss function prone outliers discussed sec 

fig 
see llm improves boosting result small margin digits error rate lower boosting 
usenet dataset consists usenet articles collected lang different newsgroups 
articles collected newsgroup articles entire collection 
experiments randomly divided articles newsgroup training set articles test set articles 
binary classification tasks checked discriminate articles pairs topics 
due lack space show results pairs newsgroups topics 
compared log loss boosting sequential version llm single words features 
ith entry vector representing document ith word appears document 
comparison results obtained llm boosting right side fig 

number rounds llm outperforms boosting 
error rate llm cases lower 
number rounds gets results llm vanilla boosting indistinguishable 
possible explanation type behavior llm finds better base hypotheses words boosting due improved criterion choosing features 
alas number rounds grows boosting opportunity choose features close gap 
plan examine angle thoroughly research 
bartlett mendelson 
rademacher gaussian complexities risk bounds structural results 
colt 
collins schapire singer 
logistic regression adaboost bregman distances 
machine learning 
dekel shalev shwartz singer 
smooth epsilon insensitive regression loss symmetrization 
colt 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
royal statistical society ser 

dietterich 
experimental comparison methods constructing ensembles decision trees bagging boosting randomization 
machine learning 
duffy helmbold 
potential nips 
freund 
boosting weak learning algorithm majority 
information computation 
freund 
adaptive version boost majority algorithm 
colt 
freund schapire 
decision theoretic generalization line learning application boosting 
computer system sciences 
van horn simon 
robust single neurons 
jcss 
lang 
newsweeder learning filter netnews 
icml 
mason baxter bartlett frean 
functional gradient techniques combining hypotheses 
advances large margin classifiers 

nigam mccallum thrun mitchell 
text classification labeled unlabeled documents em 
machine learning 
tsch 
robust boosting convex optimization 
doctoral dissertation university potsdam 
schapire singer 
improved boosting algorithms confidence rated predictions 
ml 

smooth boosting learning malicious noise 
colt eurocolt 
tuy 

optimization theory methods algorithms 
horst pardalos ed handbook global opt 
wang rosenfeld zhao schuurmans 
latent maximum entropy principle 
ieee international symposium information theory 
zhang shen tseng wong 
american statistical association 
