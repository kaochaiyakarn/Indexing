natural interface virtual environment computer vision estimated pointing gestures thomas moeslund st erik granum laboratory computer vision media technology aalborg university niels vej dk aalborg east denmark email tbm mst auc dk 
describes development natural interface virtual environment 
interface natural pointing gesture replaces pointing devices normally interact virtual environments 
pointing gesture estimated kinematic knowledge arm pointing monocular computer vision 
extract position user hand map 
line tests system show promising results average errors mm pointing screen away 
implementation real time system currently progress expected run hz 
years concept virtual environment emerged 
virtual environment computer generated world imaginable appear 
known virtual world virtual reality vr 
visual entrance vr screen acts window vr 
ideally may feel immersed virtual world 
believable user wear display located front large screen better completely surrounded large screens 
application areas vr numerous training doctors training simulated operations flight simulators collaborative games chat rooms virtual museums product development presentations architecture construction cars urban planning data mining research art 
applications user needs interact environment pinpoint object indicate direction select menu point 
number pointing devices advanced space developed support interactions 
technical devices surrounded interfaces computer terms times natural intuitive 
general problem human computer interaction hci active research area 
trend develop interaction methods closer human human interaction speech body language gestures 
authors department virtual environment form sided vr cube see installed 
stylus pointing device interacting different applications vr cube 
position orientation stylus registered magnetic tracking system generate bright line virtual world indicating user pointing direction similar laser pen 
propose replace pointing devices stylus computer vision system capable recognising natural pointing gestures hand markers special assumptions 
interaction cumbersome intuitive 
choose explore may achieved just camera 
focus interaction sides vr cube 
sufficient initial feasibility usability studies sides cameras 
crt projector camera screen user crt projector camera crt projector fig 

vr cube schematic view vr cube 
size 
note projectors cameras shown 
user inside vr cube interacting pointing stylus held right hand 
pointing gesture pointing gesture belongs class gestures known deictic gestures describes gestures pointing somebody concrete 
gesture depends context person 
mainly usages indicate direction pinpoint certain object 
direction mainly indicated orientation lower arm 
direction pinpointing object depends user distance object 
object close user direction index finger 
idea vr cube comparable installation cave tm cave automatic virtual environment electronic visualization laboratory university illinois chicago 
active contour estimate direction index finger 
stereo setup identify object user pointing 
extreme case user touches object index finger 
mainly objects user point located surface computer screen close user 
user points text images projected desk 
tip index finger infra red camera 
desk pointed larger length user arm pointer index finger 
tip pointer background subtraction 
object pointing approximately meter away pointing direction indicated line spanned hand index finger visual focus defined centre point eyes 
experiments shown direction consistently individual users placed just lateral hand eye line 
done avoid occluding object result unknown 
hand eye line approximation 
top point head index finger estimated extreme points belonging silhouette user 
information available object pointing searching triangular area image defined extreme points 
dense depth map scene user pointing 
depth background subtraction data classified points belonging arm points belonging rest body 
index finger top head extreme points classes 
cameras estimate position index finger extreme point silhouette produced utilising ir cameras 
initialisation phase user asked point different marks positions known screen 
visual focus point estimated convergence point lines spanned index finger different marks 
means location visual focus adapted individual users pointing habit 
means user allowed change body position arm naturally pointing 
context scenario distance user screen approximately meter 
objects displayed appear close far user meters away cases mentioned occur 
pointing mainly objects appear meters away pointing direction indicated line spanned hand visual focus 
user vr cube wearing stereo glasses see 
magnetic tracker mounted glasses 
measures position orientation user head update images screen user point view 
simply position orientation tracker pointing direction 
conscious head movements pointing shown unnatural possibly transform tunnel syndrome problem neck region 
furthermore due midas touch problem practical sounds 
position tracker estimate visual focus position hand needs estimated order calculate pointing direction 
replace pointing devices natural intuitive action pointing gesture 
estimating exact position hand just camera difficult task 
required precision reduced making user part system feedback loop 
user see pointing direction indicated line starting hand pointing direction system thinks pointing 
user adjust pointing direction fly 
content remaining part structured follows 
section method estimate pointing gesture 
section presents experiments carried test proposed method 
method results discussed section 
method focus interaction side assume user torso fronto parallel respect screen 
allows estimation position shoulder position head glasses 
vector glasses shoulder called displacement vector 
discussed section 
pointing direction estimated line spanned hand visual focus 
order estimate position hand single camera exploit fact distance shoulder hand denoted pointing independent pointing direction 
implies hand pointing located surface sphere radius centre user shoulder coordinates originate cave coordinate system origin centre floor cave axes parallel sides cave 
rest cave coordinate system 
camera system calibrated cave coordinate system 
calibration enables map image point pixel line cave coordinate system 
estimating position hand image obtain equation straight line tsai calibration method full optimisation optical centre camera direction unit vector line 
position hand point line intersects sphere 
obtained inserting rows equation equation resulting second order equation complex solutions indicate intersection ignored 
real solution exist unique solution eliminate solutions 
solution field view respect orientation tracker eliminated 
elimination required prediction choose position previous positions 
done simple order predictor 
pointing direction line spanned non eliminated intersection point visual focus point 
line expressed line space similar equation 
pointing direction valid position tracker hand need constant certain amount time 
estimating position hand image vr cube authors department equipped miniature video cameras placed upper corners 
may usability studies computer vision user interfaces 
illumination sources image capture crt projectors back projecting images hz sides vr cube see 
gives diffuse ambient illumination inside vr cube changes colour depending displayed images 
brightness inside vr cube determined displayed images 
average brightness normal application lux little colour machine vision 
auto gain cameras set maximum sensitivity shutter switched maximum opening results noisy images little colour variations 
hirose proposed system segment user vr cube background order generate video avatar 
infrared cameras cope poor light conditions simulate background image subtracted infrared image containing user 
get satisfying results 
simulation background gives information illumination user exposed 
estimate intensity threshold segmenting user 
due orientation cameras vr cube calculation intensive cameras field view covers parts sides means background image synthesised 
furthermore image processing place computer lot data transfered 
project video cameras priori knowledge scenario camera field view user time vr cube cathode ray tube projector 
projector consists crts 
red green blue respectively 
vr cube equipped projectors position orientation user head known magnetic tracker background brighter user image back projected side sides especially shorter wavelengths higher reflectance human skin skin reflectance long wavelengths shows algorithm segment user hand estimate position image 
firstly image areas user hand appear pointing estimated position orientation user head magnetic tracker model human motor system kinematic constraints related camera parameters calculating field view 
furthermore order predictor estimate position hand position previous image frame 
describe algorithm entire image illustrative purposes 
head position orientation magnetic tracker mapped image plane rgb constrain adaptive increase threshold camera search area thresholding saturation red channel image intensity image label objects determin position largest objects fig 

segmentation algorithm position estimation hand camera image 
histogram intensity image bimodal distribution brighter pixels originate background darker originate user 
segment user background 
optimal threshold distributions minimising weighted sum group variances 
estimated threshold indicated dashed line 
resulting binary image applying threshold 
colour variations camera image poor 
colours close gray vector 
saturation image colours increased empirical factor 
red channel segmented pixels maxima skin areas long user wearing clothes high reflectance long red wavelengths 
histogram red channel bimodal thresholded minimising weighted sum group variances 
thresholding labelling applied 
shows segmentation result largest object 
position head known skin areas excluded 
remaining object user hand 
position image calculated central moments centre mass 
fig 

segmentation user 
histogram intensity image 
dashed line threshold minimisation weighted sum group variance 
thresholded image 
fig 

red channel pre segmented camera image 
thresholded red channel labelling largest objects 
gray values images inverted representation purpose 
experimental evaluation section presents experimental evaluation different parts system 
accuracy pointing described section tested 
secondly segmentation hand section tested 
implementation real time system currently progress test visual feedback user available 
segmentation hand camera image image sequences users race pointing inside vr cube taken different applications different backgrounds illumination conditions 
position estimation hand tested line sequences 
qualitative results available 
position estimation works robustly mixture colours displayed case majority applications 
skin segmentation fails displayed images dark colour predominant red crt projector display measurements red channel camera noisy 
implementation real time system currently progress 
calculation intensive part estimation hand position working version entire images reducing regions interest hz pixels images mhz pentium iii tm expect get hz introducing reduced search area optimising code 
pointing experiments visual feedback subsection describes pointing experiments results done evaluate accuracy pointing direction estimation described section 
user asked point different points displayed screen shown 
visual feedback experiments user unbiased show natural pointing gesture 
experiments different user done 
image pointing gesture taken data magnetic head tracker 
displacement vector head tracker shoulder measured user 
evaluation data turned uncertainty position estimate head magnetic tracker cm direction 
moment possible calibrate device order achieve higher accuracy 
error large head position information method described previous section 
order get accurate position users heads visual focus point segmented image data position tracker position visual focus point estimated 
position estimate position shoulder displacement vector described section 
shows results representative pointing experiment 
circles real positions displayed screen asterisks connected dashed line respective estimated positions user pointing 
error 
estimates column axis axis axis axis fig 

experimental setup pointing experiments visual feedback vr cube 
user distance approximately screen points raster displayed 
axis axis axis axis axis fig 

results pointing experiments 
circles figures real positions screen 
asterisks estimated pointing directions system 
results representative user constant displacement vector 
results representative user lut displacement vector 
inner circle shows average error experiments 
outer circle shows maximum error experiments 
left intersection sphere equation line spanned camera hand user 
error increasing user points left 
mainly due incorrect assumption section displacement vector constant 
direction magnitude displacement vector tracker shoulder varying 
illustrated 
tracker displacement vector shoulder hand position estimated hand position shoulder position estimated shoulder position fig 

user displacement vector tracker shoulder seen right side 
illustration error introduced assuming torso fronto parallel 
illustrate direction magnitude displacement vector tracker shoulder user head looking straight ahead 
head rotated left shoulder rotated illustrated results wrong centre sphere wrong estimation hand position 
error illustrated angle rotation shoulder squeezed relation tracker head rotation displacement vector non linear 
shows components displacement vector test points representative user estimated shoulder position image data tracker data 
user lookup table lut displacement vectors function head rotation build 
shows result representative pointing experiment lut displacement vectors estimate position shoulder 
notice position shoulder correction estimates left column available 
table shows average errors maximum errors pointing experiments mm respective points screen 
errors illustrated inner circle indicates average errors outer circle maximum errors 
average error points experiments mm 
mm mm mm point fig 

components displacement vector function test points 
table 
average errors maximum errors mm respective points screen 
axis axis discussion demonstrated technical interface devices replaced natural gesture finger pointing 
pointing gesture estimated line spanned position hand visual focus defined centre point eyes 
visual focus point moment estimated image data measure 
position orientation electro magnetic tracker mounted stereo glasses worn user 
position hand estimated intersection line spanned hand camera sphere centre shoulder user radius equal length user arm pointing pointing experiments different user done 
user asked point points screen distance 
due especially movements shoulder pointing errors mm estimated real position screen observed 
reduce errors lut correct position shoulder 
reduced average error mm maximum error mm 
find accurate result user standing meters away 
error large depends application 
final system estimated pointing direction indicated bright line seen stereo glasses starting finger user object pointed 
error critical user part system loop correct fly 
words effect error hinder user accurate pointing feedback line may acceptable 
system applications feedback non virtual world need know effect different sources errors compensate 
error originates different sources tracker image processing definition pointing direction assumption torso fronto parallel respect screen assumption constant 
currently deriving explicit expressions error sources setting test scenarios measure effect errors 
experiments done vr cube characterise accuracy usability soon real time implementation finished 
experiments show method allows replace traditional pointing devices suggested line tests 
issue intend investigate midas touch problem inform system pointing gesture 
simple test scenario gesture pointing relatively easy determine performed 
mentioned see gesture recognised position hand constant number frames 
realistic scenarios multiple gestures appear problem difficult 
type solution thumb mouse bottom 
natural gesture spoken input select point object 
path follow decided 


improving human computer interaction adding speech gaze tracking agents wimp environment 
master thesis aalborg university 

bar shalom fortmann 
tracking data association 
academic press 

hlen granum lauritzen 
visual data mining 
www cs auc dk dvdm 

br larsen moeslund olesen 
workbench environment building multimodal systems 
second international conference cooperative multimodal communication 

browning cruz defanti 
virtual reality design implementation cave 
siggraph computer graphics conference pages 
acm siggraph aug 

cipolla 
uncalibrated stereo vision pointing man machine interface 
iapr workshop machine vision applications yokohama japan december 

fukumoto mase 
finger pointer pointing interface image processing 
computer graphics 

gonzalez 
digital image processing 
addison wesley publish ing 

hirose ogi yamada 
integrating live video immersive environments 
ieee multimedia july 

jojic brumitt meyers harris huang 
detection estimation pointing gestures dense disparity maps 
fourth international conference automatic face gesture recognition grenoble france march 

kahn swain 
understanding people pointing perseus system 
international symposium computer vision coral florida november 


creating models purpose planning 
th international conference computers urban planning urban management venice italy sept 

larsen haase hansen nielsen 
virtual brain project development simulator 
accepted th annual medicine meets virtual reality january 

ritter 
neural recognition human pointing gestures real images 
neural processing letters 

macintyre feiner 
multimedia user interfaces 
multimedia systems 


hand mind gestures reveal thought 
university chicago press 

mase 
gesture interface virtual walk 
workshop perceptual user interface 


thresholding selection method gray level histograms 
ieee transactions systems man cybernetics 

polhemus 
stylus magnetic tracker 
www polhemus com htm 

sato kobayashi koike 
fast tracking hands fingertips infrared images augmented desk interface 
fourth international conference automatic face gesture recognition grenoble france march 

taylor mccloskey 
pointing 
behavioural brain research 

tsai 
versatile camera calibration technique high accuracy machine vision metrology shelf tv cameras lenses 
ieee journal robotics automation august 

