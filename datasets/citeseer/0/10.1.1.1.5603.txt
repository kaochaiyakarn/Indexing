scheduling bag task grids pau case cirne francisco costa daniel santos neto andrade universidade federal de grande dsc edu br discuss difficulties involved scheduling applications computational grids 
highlight main sources difficulties firstly size grid rules possibility centralized scheduler secondly resources managed different parties scheduler consider different policies 
argue scheduling applications grid require orchestration schedulers possibly conflicting goals 
discuss addressed issue context pau grid bag tasks applications parallel applications tasks independent currently deploying brazil 
computational grids platform execute parallel applications promising research area 
possibility allocate amounts resources parallel application lower cost traditional alternatives parallel supercomputers main grid computing 
hand grid characteristics high heterogeneity complexity wide distribution traversing multiple administrative domains create new technical challenges 
particular area scheduling faces entirely new challenges grid computing 
traditional schedulers operating system supercomputer scheduler control resources interest 
grid central control possible 
grid just big single entity control 
second resources comprise grid owned different entities rendering administratively unacceptable single entity controls resources 
grid scheduler strive traditional goals improve system application performance realizing system control 
fact system control schedulers 
scheduler sar de rose derose inf br miranda scheer jo jornada hewlett packard miranda scheer joao jornada hp com interact consider schedulers order achieve goals 
way multiple schedulers grid form ecology individual schedulers compete collaborate schedulers system behavior emerges decisions schedulers 
discusses scheduling ecology pau node grid supports execution bag tasks applications 
bag tasks bot applications parallel applications tasks independent 
despite simplicity bot applications variety scenarios including data mining massive searches key breaking parameter sweeps simulations fractal calculations computational biology computer imaging 
due independence tasks bot applications successfully executed widely distributed computational grids demonstrated seti home 
fact argue bot applications applications suited computational grids communication easily bottleneck parallel applications 
focusing bot applications interesting problem simplified remains useful relevant 
major simplification introduced focusing bot applications need quality services guarantees 
tasks compose bot application independent having task making progress slowly stopping 
dealt major problems 
worst task resubmitted 
scheduling ecology pau designed respect site autonomy ii cater user priorities iii enable multilateral collaboration contrast common bilateral collaboration iv support dedicated resources explicitly separate archi components implementation easing addition new schedulers ecology 
design goals achieved separating grid scheduling concerns main aspects improve performance application grid ii manage resources site set resources single administrative domain iii gain access resources grid 
concern responsibility job scheduler concern ii responsibility site scheduler 
grid job schedulers site schedulers 
user rights account sites 
sites dynamically access foreign users peer peer mechanism called network favors addressing concern iii 
section surveys area discusses related 
section describes experience building pau community 
pau scheduling ecology section 
section presents set experiments gauges performance expect pau 
section contains delineates 
related grid computing active area research 
started high performance computing people realized grid technology deliver computational services ondemand 
observation brought merge grid web services technologies seen standards ogsa successor wsmf 
standards currently implemented academia industry 
notably standards implemented globus project greatest visibility grid computing 
important realize wsmf technologies address scheduling resource management directly 
provide grid building blocks common foundation grids built 
scheduling thought happen pervasively grids service making scheduling decisions may delegated specialized services 
grid scheduling result scheduling decisions multiple autonomous related entities 
particular scheduling decisions service take account quality service provided services invoked idea single scheduler deal entire grid dates mid berman seminal application level scheduling 
number works aspects scheduling grids 
aspects include example coping dynamicity grid resource availability impact large data transfers coordination schedulers deliver combined service virtualization way ease scheduling 
closer scheduling efforts target bot applications apst nimrod condor 
particular apst nimrod similar mygrid job scheduler intent architecture 
require information mygrid scheduling 
differ mygrid assumptions application grid 
apst targets divisible workloads mygrid user responsible breaking application tasks 
nimrod assumes user going pay resources scheduling grid economy model 
condor initially conceived campus wide networks extended run grids 
mygrid apst nimrod schedulers condor system centric scheduler 
condor closer ourgrid site scheduler 
major difference ourgrid condor ourgrid designed encourage people donate resources community resources received proportional resources donated condor issue taken line altruism administrative orders lead people condor pool 
condor ourgrid create grids resource providers resource consumers roles played people 
alternative public computing efforts suggest asymmetrical view people voluntarily donate resources projects great public appeal 
arguably public computing originated huge success achieved seti home harvested close years cpu far 
seti home distinction application search intelligence evidence radio signals grid support 
introduced seti home sequel promising exactly separation 
aims create public computing infrastructure different applications 
project aims create public computing infrastructure carries interesting contribution tolerating sabotage bogus volunteer results 
pau community pau means ancient language spoken native initiative created hp brazil build brazilian grid 
pau currently involves different universities research centers collaborate hp brazil call hp brazil research ecosystem 
goals pau twofold 
goal take advantage number computational resources available different research centers hp brazil creating wide geographically distributed grid country 
second goal foster grid research solution currently developed constantly improved usage experience 
responsible mygrid ourgrid research independent auditing slas service level agreements grids integration supercomputers 
instituto atl focuses security aspects grid 
focusing security management aspects 
instituto adding windows support helping community configuration management training 
hewlett packard brazil doing research idle cycle exploitation applications execution security sandboxing 
ipt sp working testing applications development web services 
puc rs performing research clusters integration cluster resources transparent manner 
cap puc rs developing grid applications 
working field bioinformatics applications 
doing research grids perform data mining 
challenges arise coping decentralized administration geographically distributed community 
just cite take account evolution research carried synchronize correct time research institution joins community increases decreases resources allocated institution plan integration new piece technology community common software 
cope challenges grid policies managed defined general committee formed research center representatives 
committee responsible establishing defining flexible dynamic non community synchronizing different activities developed 
committee regular tele conference meetings track integration activities define steps 
number different people involved having regular tele conference meeting committee meets face face couple times year 
scheduling pau grid pau poses challenges schedulers 
resources widely spread making difficult efficient global snapshot grid 
multiple users multiple resource owners particular wishes priorities 
scenario creates need system multiple schedulers 
designed implemented set schedulers collectively responsible scheduling pau 
discussed schedulers respect autonomy site considering priorities associated different users 
support dedicated non dedicated resources 
interaction needs facilitates addition new schedulers grid 
achieved design goals separating grid scheduling concerns main aspects 
key aspect improvement performance application grid 
achieved job scheduler efficient lightweight approach explained shortly 
aspect definition concept site resources managed particular policy site comprises set resources single administrative domain 
site scheduler charge imposing site policy 
final aspect providing way gain access resources grid resources foreign site 
responsibility peer topeer resource exchange network involving site schedulers 
user submits bot job job scheduler sends request resources sites user account 
sites controlled site scheduler allocates resources job scheduler best effort basis 
resources may local resources controlled site scheduler foreign resources obtained network favors 
job scheduler begins receive resources site schedulers starts farm tasks compose application 
goal job scheduler minimize application execution time 
note resources offered job scheduler best effort basis 
resources may disappear time 
job scheduler guarantee tasks finish necessary 
pau parlance peer peer resource exchange network favors called ourgrid 
site scheduler ourgrid peer 
job scheduler termed mygrid 
job scheduler despite simplicity bot applications scheduling bot applications grids difficult 
grids introduce issues complicate matters 
efficient schedulers depend lot information application estimated execution time resources processor speed network topology kind information typically difficult obtain 
second important bot applications data intensive applications considering data transfers paramount achieve performance 
order achieve efficient schedules provide coordinated data computation scheduling non trivial task 
mygrid scheduler workqueue replication wqr dealt issue 
wqr uses task replication recover bad task machine allocations inevitable uses information 
wqr performance traditional knowledge schedulers fed perfect information cost consuming cycles 
wqr take data transfers account 
version mygrid released alternative scheduler mygrid storage affinity tackle problems simultaneously 
note wqr available mygrid quite job cpu intensive bot applications 
grid schedulers take data transfers account order improve performance applications 
greater visibility 
name suggest extension sufferage scheduling heuristic knowledgebased scheduler 
order cope lack information environment data placement concerns developed novel scheduling heuristic data intensive bot applications 
heuristic named storage affinity 
idea exploit data avoid unnecessary data transfers 
data appears basic flavors inter job inter task 
arises job uses data produced job executed previously appears applications tasks share input data 
order take advantage data improve performance data intensive bot applications introduce storage affinity metric 
metric determines close site task close mean bytes task input dataset stored specific site 
storage affinity task site number bytes task input dataset stored site 
claim information data size data location obtained priori difficulty loss accuracy example cpu network loads completion time tasks 
instance information obtained data server particular site able answer requests data elements stores large data element 
alternatively implementation storage affinity heuristic easily store history previous data transfer operations containing required information 
naturally storage affinity dynamic information grid application inefficient task processor assignments may occur 
order circumvent problem storage affinity uses task replication strategy similar wqr 
replicas chance submitted faster processors processors assigned original task increasing chance task completion time decreased 
total simulations performed investigate efficiency storage affinity heuristics 
simulation consisted sequence executions job 
executions repeated analyzed scheduling heuristics wqr storage affinity 
execution time values scheduling heuristic analyzed 
table presents summary simulation results 
possible note average storage affinity achieve comparable performances 
results show data aware heuristics attain better performance wqr 
data transfer delays dominate execution time application account severely hurts performance application 
case wqr execution task preceded costly data transfer operation inferred large bandwidth small cpu waste 
impairs improvement replication strategy wqr bring 
hand replication strategy storage affinity able cope lack dynamic information yields performance similar 
main inconvenience need knowledge dynamic information drawback storage affinity consumption extra resources due replication strategy average extra cpu cycles negligible amount extra bandwidth 
naturally report wasting values heuristic apply replication strategy 
storage affinity wqr execution time mean sec std dev wasted cpu mean std dev wasted bandwidth mean std dev result state storage affinity task replication strategy feasible technique obviate need dynamic information scheduling data intensive bot applications expenses consuming cpu 
refer reader complete performance analysis storage affinity 
site scheduler computational grids composed resources sites 
resources different processor architectures diverse operating systems 
consequently may differ performance ranging desktop machines supercomputers 
bot grids consider types resources space shared parallel machines massive parallel processors clusters ii time shared resources desktop machines accessed time 
access site resources request 
example mpp system access nodes request resource manager 
happens resource manager decides job executed machine nodes run 
strictly speaking true desktop machines resource shared intervention resource scheduler 
case operating system resource scheduler 
global grid forum scheduling dictionary working group types resource schedulers involved site level 
local scheduler determines system processes job queue case space shared resources mpp cluster 
implemented usually system resource manager 
machine scheduler resource just machine desktop computer 
type scheduler uses criteria schedule jobs priority length time job queue available resources 
implemented operating system machine 
table storage affinity simulations results introducing new type resource scheduler called site scheduler 
represents site resources grid making available higher schedulers 
access schedulers described local machine go site scheduler 
main responsibilities site scheduler verification access rights grid jobs ii abstraction site resource types grid iii arbitration site demand grid demand 
verification access rights needed security reasons block grid users eligible site resources 
access rights impose limitations related maximum number allocated resources exclusion specific resource types 
time related restrictions possible exclusion grid accesses peak hours 
resource abstraction interesting feature grid users care site resource types 
negotiations site manager local machine schedulers allocate site resources transparent grid 
arbitration key aspect local users grid users competing resources 
site manager find balance maintain external interference reasonable level delay local users 
priority policy guarantee better response special users 
additional services may include caching tasks executables site scheduler function proxy resource monitoring performance prediction site higher resource levels 
interesting issue consider site scheduler known ip site 
case grid see site virtual resource sum site available resources 
network favors form grid shares resources multiple organizations necessary infrastructure allows site schedulers resources 
necessary resource owners resources available grid 
obvious statement making happen straightforward 
experience shows making people contribute resources community hardest tasks assembling grid 
experience backed empirical studies peer peer resource sharing communities showing absence incentives resource donation users consume resources system donating back 
provide incentives donating resources grid ourgrid implements scheme called network favors 
site offers community access idle resources expecting gain access idle resources participants exceeds local capacity 
motivate sites share resources possible network favors designed promote fairness resource sharing site community expect receive community 
network favors site consumes resources owned site regarded favor paid resource owner consumer 
site system stores local balance favors received minus favors known site past interactions site 
balance updated providing consuming favors 
conflicting requests resources resource owner prioritizes requests sites higher balances 
quantification favor value done locally independently negotiations agreements affects decisions resource allocations sites involved 
sites reciprocate favors satisfactorily time lower priority community 
non may happen reasons local resource failures absence resources site desired resources locally users moment request 
free riding sites may choose reciprocate favors 
case non favors gradually diminishes ability site access grid resources 
behavior illustrated 
shows results simulation site community different proportions 
possible see fraction community resources obtained free riders epsilon diminishes time tending small value 
resources obtained free riders verified simulations amount resources collaborator receives divided amount denoted fr approximately 
illustrates site community amount site uniform distribution 
reasonable assume cost donating resource smaller utility gained receiving 
interest sites donate resources 
distribution favor ratio running pau just started running bot applications pau 
results attained simple experiment conducted 
application cpu bound bot application finds divisors large number 
experiment conducted small subset pau composed different sites 
site peer acting site scheduler providing nodes grid users 
table shows sites maximum number nodes available nodes configuration peers names 
idle resources available grid 
idle mean resources local users 
shows site schedulers site location max 
num peers ber nodes names porto lsd grande lcc grande lab petri grande table testbed configuration site schedulers communicate resource schedulers sites order obtain resources 
peers lsd lcc deal machine schedulers machines 
site scheduler complex major resources controlled local scheduler clusters 
way peer communicates types resource schedulers desktop machines local scheduler cluster nodes 
experiments executed follows 
mygrid running desktop machine site acts job scheduler 
performed sets experiments environment described order analyze speed attained grid compared execution application standalone setting 
set experiments composed jobs small tasks minutes dedicated pentium iii mhz 
second set tasks times longer tasks task 
set obtained speedup ranging average 
peak number machine gained site scheduler job scheduler fastest job executed 
set longer tasks speedup ranging average 
set reached peak machines utilization 
expected obtained improvement application performance 
highlighted overhead impact pau structure 
set small tasks speedup second set due latency gain grid machine communication configured order maintain performance local resources local users 
reason lsd site peers acts site scheduler acts relay 
required connection lack due firewall configuration 
peer connections lcc machine application relay grid lcc resources 
peers prepare environment transfer binary task execute tasks 
implies moment application reasonable large granularity execute pau 
strategy deal scheduling bot application large scale grids 
strategy proposed supported set schedulers divided distinct classes peer peer resource harnessing mechanism 
site schedulers responsible providing grid resources job schedulers turn provide efficient scheduling bot applications available resources 
currently job schedulers replication mechanism achieve efficient scheduling requiring information grid applications scheduled 
site schedulers ensure implementation policies set resource owner 
try find remote resources peer peer resource trading protocol network favors protocol 
results show ecology schedulers intended require parallel applications course grain fully benefit grid 
important point scheduling problem just problems need tackled order deploy grid pau 
particular security issues challenging 
currently working vi machine sandbox technology address issues 
clear avenue improvement relaxation bot requirement currently pose application 
strategy relax application constraints incrementally supporting broader class applications step 
step call workflow applications parallel applications tasks input comes task output 
described pau currently deployed institutions compose grid 
institutions currently mygrid form grid local remote resources account 
installing ourgrid moving architecture described months 
pau closed grid software described open source 
mygrid ourgrid available www ourgrid org available sourceforge net projects 
abramson giddy 
high performance parametric modeling nimrod killer application global grid 
ipdps pp 

anderson cobb seti home experiment public resource computing 
comm 
acm vol 
pp nov 
anderson 
public computing people science 
conference shared knowledge web madrid spain 
andrade cirne 
ourgrid approach easily assemble grids equitable resource sharing 

andrade cirne 
discouraging free riding peer peer cpu sharing grid hpdc june 
berman application level scheduling distribute heterogeneous networks 
supercomputing pittsburgh 
berman fox hey editors 
grid computing making global infrastructure reality 
john wiley sons 
virtual resource manager architecture sla aware resource management 
th international symposium cluster computing grid 
buyya abramson giddy 
economy driven resource management architecture global computational power grids 
pdpta 
casanova heuristics scheduling parameter sweep applications grid environments 
th hcw pp 
casanova hayes yang 
algorithms software schedule deploy independent tasks grid environments 
workshop distributed computing metacomputing resource globalization 
france 
december 
casanova berman 
parameter sweeps grid apst 
april 
czajkowski resource management architecture metacomputing systems 
pp 

czajkowski open grid services infrastructure ws resource framework refactoring extension 
www ibm com developerworks library ws resource pdf plank wolski 
data staging effects wide area task farming applications 
ieee international symposium cluster computing grid pp 
may frey condor computation management agent multi institutional grids 
th hpdc august 
foster kesselman 
grid blueprint new computing infrastructure 
morgan kaufmann 
global grid forum 
grid scheduling dictionary terms keywords 
www fz de zam rd coop ggf sched sd html globus web site 
www globus org ripeanu 
deconstructing kazaa network rd ieee workshop internet applications june 
litzkow livny mutka 
condor hunter idle workstations 
th icdcs 
de rose 
configurable easy maintain resource manager optimized small mid size gnu linux cluster 
nd icpp pp 

cirne 
trading cycles information replication schedule bag tasks applications computational grids 
euro par 
santos neto cirne lima 
exploiting replication data reuse efficiently schedule data intensive applications grids 
th june 
saroiu gummadi gribble 
measurement study peer peer file sharing systems mmcn jan 
sabotage tolerance mechanisms volunteer computing systems 
generation computer systems elsevier 
seti home statistics page 

ssl berkeley edu totals html combining workstations supercomputers support grid applications parallel tomography experience 
hcw 
casanova berman 
applying scheduling tuning line parallel tomography 
supercomputing nov 
smith foster taylor 
scheduling advanced reservations 
ipdps 
stiles monte carlo simulation transmitter release general simulator cellular physiological processes 
comp 
neuroscience pp 

tuecke open grid services infrastructure version 
global grid forum draft recommendation 
www globus org research papers html yang schopf foster 
conservative scheduling predicted variance improve scheduling decisions dynamic environments 
supercomputing nov 
