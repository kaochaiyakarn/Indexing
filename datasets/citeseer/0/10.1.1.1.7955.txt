submitted ieee tnn global optimization rbf networks shimon cohen nathan intrator school computer science tel aviv university www math tau ac il nin modifications parameter estimation radial basis functions network introduced 
include better initializing clustering algorithm full gradient descent centers weights weights matrix inversion 
performance comparison rbf algorithms data sets 
proposed method superior bishop em training algorithm orr method conventional implementation 
radial basis functions extensively interpolation regression classification due universal approximation properties simple parameter estimation 
parameter estimation requires pseudo inversion possibly sparse matrix 
possible numerical instability inversion aggravated number training patterns small compared dimensionality may partially alleviated parameter estimation 
usefulness rbf architecture interpolation ensured micchelli theorem approximation properties clear especially high dimensional noisy data 
particular approximation interpolation properties extensively studied problem optimal radial basis function centers resolved 
poggio girosi suggest regularization approach problem proposed approaches related different estimation cluster centers 
gaussian kernels rbf network trained maximum likelihood goal identical mixture gaussians model 
kernel centers determination forward weights kernels output done analytically pseudo inverse hidden units activity matrix 
flake output means algorithm computed weights net pseudo inverse 
setting radius values cluster distance nearest cluster center obtained results vowel data set 
linear problem optimal determination cluster centers non linear difficult problem 
sensitive dimensionality input space initial starting positions clustering algorithm 
bishop suggested optimize cluster centers em algorithm initial values clustering algorithm 
intended reduce sensitivity algorithm clustering problems 
expectation maximization em approach includes expectation part calculates likelihood model maximization part maximizes likelihood data respect centers radii 
experimental investigation clustering algorithms shown em approach clustering outperforms naive center initialization random perturbations output hierarchical agglomerative clustering partially supported israeli ministry science hermann minkowski minerva center geometry tel aviv university 
analysis seismic data supported atomic energy commission council higher education dr gideon leonard 
part done affiliated institute brain neural systems brown university supported part onr 

refinement random initialization improved performance means clustering small subsamples training set applied means algorithm subsample 
solution applying means algorithm centers previous phase 
algorithm may useful large training sets 
possible apply means algorithm data class separately 
radius cluster center calculated standard deviation data vicinity cluster 
cluster centers calculated class separately clusters combined larger neighboring clusters belonged single class 
assumption radial symmetry cluster relaxed estimating different radius data axis 
orr uses regression tree find centers radii rbf units 
done creating large tree pruning terminal nodes criterion general predication error gpe bayesian information criterion bic 
members remaining terminal nodes create clusters diagonal part covariance matrix members radii cluster different axes 
problems estimating optimal cluster centers alleviated performing post parameter estimation full model estimation forward weights 
initial step direction perform gradient descent cluster centers forward weights change cluster radius 
argue performance network relatively insensitive radius value argument contradiction findings 
introduce modifications estimation rbf model parameters 
show clustering algorithms sensitive initial choice cluster centers propose better cluster initialization method 
introduce gradient descent full rbf architecture parameters centers radii forward weights 
done parameters clustering pseudo matrix inversion 
performance comparison benchmark data sets 
comparison includes conventional rbf implementation bishop em implementation orr regression trees approach 
ii 
modified clustering algorithm radial basis functions approximation network rbf composed set kernel functions located cluster centers mi input space width gaussian kernels approaches compare results 
output network yk exp 
yk wkj xj rj number rbfs 
rbfs radially symmetric architecture far flexible number estimated parameters grows mn mn case estimation full covariance 
input dimensionality large complexity larger 
expected added flexibility may useful weigh added flexibility reduced robustness due increased variance model 
means algorithm seeks partition data set disjoint subsets sj containing nj patterns implemented matlab tm toolbox 
criterion minimized nj xj mi mi ni xj ni 
classical approach 
starts random selection data initial cluster centers 
batch version pattern re assigned new cluster nearest center 
procedure repeated change grouping patterns occurs 
approaches attempt better random initialization cluster centers 
initial vectors chosen cluster means algorithm may get stuck local minimum ignores clusters 
demonstrated cluster centers initially chosen cluster eventually cluster 
suggest start clustering algorithm initial centers different clusters 
demonstration bad local minima clustering algorithms fig 

data points red circles black crosses blue rectangles initial cluster centers green stars final clusters 
initial cluster centers cluster means algorithm left em clustering algorithm right get stuck bad local minima 
choosing initial vectors procedure choose initial centers 
choose cluster center random 

number clusters vector remaining set computed nearest cluster center 
choose vector largest distance nearest cluster center initial cluster center 
procedure increases probability cluster centers smaller number clusters 
procedure may iterated times different random initial cluster ensure robust solution 
computational complexity added algorithm nk number data points number clusters searched data 
observe added complexity compensated accelerated convergence new algorithms 
initial computation forward weights iii 
full architecture parameter estimation cluster centers regular means modified means algorithm forward weights analytically pinv design matrix nj pinv pseudo inverse 
practice solutions linear matrix inversion problem ill conditioned due small variances projections data 
solved singular value decomposition svd 
gradient descent optimizing centers radii weights summarize clustering algorithms may find suboptimal solutions due reasons 
bad initial conditions 

wrong number searched clusters 

search take class labels account possibly concentrating search data clusters classification problem 
purpose estimate architecture parameters performing gradient descent search initial parameter estimation 
search include cluster centers clusters radii forward weights 
concurrent search parameters non trivial appears force driving cluster radii zero stronger optimization forces 
wang zhu addressed problem assuming fixed size radii performing optimization remaining parameters cluster centers forward weights 
address problem applying constrained optimization penalizes small radii 
optimization objective rk rk radius cluster note assume radially symmetric cluster assumption relaxed performing local transformation cluster 
small regularizing parameter estimated cross validation training data 
surprisingly obtained faster convergence optimization algorithm replacing gradient descent conjugate gradient algorithm 
polak algorithm find slightly better optimization algorithms suggested 
gradient optimization formulae appendix 
pruning unnecessary units gradient descent algorithm described produces units relatively ineffective 
units easily pruned evaluating contribution error 
done simply removing connections hidden unit output units measuring change objective function 
units increase error increase error value smaller predetermined regularization constant determined cross validation training data removed 
application pruning process reduced average number hidden units significant reduction performance 
mackay sine sine friedman rbf orr rbf matlab rbf bishop table mean squared error results test data regression data sets 
averaged results runs shown including standard deviation 
iv 
results section describes regression classification results variants rbf proposed gradient descent rbfn method data sets 
results test data average runs include standard deviation 
start comparison simulated regression data sets orr asses performance rbf 
results summarized table data set sine wave 
sin 
gaussian noise added outputs standard deviation 
sets train test patterns randomly sampled data additive noise 
second data set sine wave 
sin sin training patterns sampled random input range 
clean data corrupted additive gaussian noise 
test set contains noiseless samples arranged grid pattern covering input ranges 
orr measured error total squared error samples 
measure 
third data set dimensional hermite polynomial input values sampled randomly gaussian noise standard deviation added output 
fourth data set simulated alternating current circuit input dimensions resistance frequency inductance capacitance output impedance training set contained points sampled random certain region details 
additive noise added outputs 
experimental design friedman evaluation mars 
friedman results include division variance test set targets 
follow friedman report nmse test set 
orr regression trees method outperforms methods data set 
classification classification data set sonar returns attempt distinguish mine rock 
gorman sejnowski study feasibility neural networks sonar signal discrimination 
data continuous inputs binary output classes 
algorithm sonar vowel seismic seismic breast cancer rbf orr rbf matlab rbf bishop table ii percent classification results rbf different architectures data sets 
average results runs including standard deviations 
divided training patterns test patterns 
task train network discriminate sonar signals reflected metal cylinder reflected similar shaped rock 
gorman sejnowski report results feed forward architectures hidden units 
achieved correct classification aspect independent test data 
result outperforms results obtained different rbf methods surpassed proposed rbf training algorithm 
vowel recognition data widely studied benchmark 
problem may indicative type problems real neural network faced 
data consists auditory features steady state vowels spoken british english speakers 
training patterns test patterns 
pattern consists features belongs classes correspond spoken vowel 
speakers genders 
best score far reported flake units 
hybrid neural network uses nonlinear linear input features resulting expansion euclidean distance cluster center center radial function input pattern vector 
flake shown kind network approximate projection units radial units 
inputs approximate radial functions projection regular inputs 
average best score achieved hidden units 
algorithm achieved correct classification hidden units 
far know best result achieved data set 
orr method included available multi class problem 
seismic seismic different representations seismic signals 
data sets include waveforms types explosions task distinguish types 
data evaluation various approaches including artificial neural networks learning course tel aviv university dimensionality seismic representing time frames frequency bands dimensionality seismic representing time frames frequency bands 
principal component analysis pca reduce data representation dimensions 
data sets training patterns test patterns chosen difficult desired discrimination 
table ii summarizes percent correct classification results data sets different rbf classifiers 
test set carefully chosen include difficult discrimination tasks 
seismic data due single test set std zero single classification data obtained runs 
details see www math tau ac il nin learn summary demonstrated possibly strong deficiency familiar means algorithm variants including em variant 
suggested simple way amend problem 
analysis performed appendix demonstrates problem bad initial cluster centers worse number training patterns increases 
important correct problem cases expects size data set sufficient robust parameter estimation 
improve results clustering algorithm suggested perform constrained optimization parameters radial basis function architecture 
optimization constrained avoid trivial strongly overfitting solution cluster centers converge zero 
demonstrated improved performance resulting algorithm data sets compared approach current state art rbf training algorithms 
results demonstrate proposed training algorithm outperforms competing algorithms remarkable result achieved vowel data set 
appendix full gradient formulae provide equations full error gradient rbf networks including gradient radii forward weights cluster centers 
en error due training pattern total error 
define derivatives error respect weights yk 
yk 
wij derivatives error respect cluster center mi 
mi number network outputs 
derivatives error function respect radius ri ri derivatives rbf function respect center mi derivatives rbf function respect radius ri xn mi 
xn mi ri dimension input pattern vector 
ii 
upper bound probability means get stuck bad minima derivation holds clustering done em algorithm 
derive probability vectors chosen cluster 
assuming exactly distinct clusters event initial vectors chosen distinct clusters 
assuming cluster roughly number members rounded smallest integer different ways choose initial vectors 
different ways choose vectors total number vectors 
nk kk 

proceed stirling approximation 
large stirling approximation gives 
nn 
nk ke 
arrangement arrive nk 
nnn arrive nk kk 
probability start bad local initial cluster centers approaches number data points large 
starting bad initial cluster centers guarantee final solution include clusters analysis gives upper bound probability arriving bad cluster solutions 
em algorithm alleviate problem demonstrated 
example upper bound 
orr regularisation selection rbf centres neural computation vol 
pp 

hardy equations topography irregular surfaces aircraft vol 
pp 

elsevier amsterdam 
dyn levin numerical procedures surface fitting scattered data radial functions siam sci 
statist 
comput vol 
pp 

powell radial basis functions multivariable interpolation review algorithms approximation mason cox eds pp 

clarendon press oxford 
alternative assumption reduce probability broomhead lowe multivariate functional interpolation adaptive networks complex systems vol 
pp 

dyn micchelli interpolation sums radial functions numerische math vol 
pp 

micchelli scattered data distance matrices conditionally positive definite functions constructive approximations vol 
pp 

dyn ron radial basis function approximation gridded centers scattered centers proc 
london math 
soc 
poggio girosi networks approximation learning ieee proceedings vol 
pp 

moody darken fast learning networks locally tuned processing units neural computation vol 
pp 

bishop improving generalization properties radial basis function neural networks neural computation vol 
pp 

titterington smith makov statistical analysis finite mixture distributions john wiley sons 
bishop training noise equivalent tikhonov regularization neural computation vol 
pp 

flake square unit augmented radially extended multilayer neural networks tricks trade orr ller eds pp 

springer 
speaker normalisation automatic speech recognition ph thesis university cambridge 
dempster laird rubin maximum likelihood incomplete data em algorithm proceedings royal statistical society vol 
pp 

meila heckerman experimental comparison clustering methods technical report tr microsoft www research microsoft com heckerman 
celeux classification em algorithm clustering versions computational statistics data analysis vol 
pp 

banfield raftery model gaussian non gaussian clustering biometrics vol 
pp 

bradley fayyad refining initial points means clustering international conference machine learning 
july morgan kaufmann www research microsoft com fayyad papers icml htm 
prieto technique selection kernel function parameters rbf neural networks classification remote sensing images ieee trans 
vol 
pp 

orr hallam murray leonard assessing rbf networks delve 
zheng ou wang tao zhu efficient learning algorithm improving generalization performance radial basis function neural networks neural vol 

wasserman advanced methods neural computing van nostrand reinhold new york 
duda hart pattern classification scene analysis john wiley new york 
lloyd squares quantization pcm ieee transactions information theory vol 
pp 

kaufman rousseeuw finding groups data john wiley sons new york 
rasmussen clustering algorithms information retrieval data algorithms yates yates eds pp 

prentice hall nj 
polak computational methods optimization 
unified approach new york academic press 
mackay bayesian interpolation neural computation vol 
pp 

friedman adaptive regression splines annals statistics vol 
pp 

gorman sejnowski analysis hidden units layered network trained classify sonar targets neural network pp 
vol 

rumelhart hinton williams learning internal representations error propagation parallel distributed processing rumelhart mcclelland eds vol 
pp 

mit press cambridge ma 

