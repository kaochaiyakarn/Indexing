probabilistic latent semantic indexing proceedings second annual international sigir conference research development information retrieval probabilistic latent semantic indexing novel approach automated document indexing statistical latent class model factor analysis count data 
fitted training corpus text documents generalization expectation maximization algorithm utilized model able deal domain speci synonymy polysemous words 
contrast standard latent semantic indexing lsi singular value decomposition probabilistic variant solid statistical foundation de nes proper generative data model 
retrieval experiments number test collections indicate substantial performance gains direct term matching lsi 
particular combination models di erent dimensionalities proven advantageous 
advent digital databases communication networks huge repositories textual data available large public 
today great challenges information sciences develop intelligent interfaces human machine interaction support computer users quest relevant information 
elaborate ergonomic elements computer graphics visualization proven extremely fruitful facilitate enhance information access progress fundamental question machine intelligence ultimately necessary ensure substantial progress issue 
order computers interact naturally humans deal potential vagueness user requests recognize di erence user say meant 
typical scenario human machine interaction information retrieval natural language queries user formulates request providing number keywords free form text expects system thomas hofmann international computer science institute berkeley ca eecs department cs division uc berkeley hofmann cs berkeley edu return relevant data amenable representation form ranked list relevant documents 
retrieval methods simple word matching strategies determine rank relevance document respect query 
known literal term matching severe drawbacks mainly due words unavoidable lack precision due personal style individual di erences word usage 
latent semantic analysis lsa approach automatic indexing information retrieval attempts overcome problems mapping documents terms representation called latent semantic space :10.1.1.108.8490
lsa usually takes high dimensional vector space representation documents term frequencies starting point applies dimension reducing linear projection 
speci form mapping determined document collection singular value decomposition svd corresponding term document matrix 
general claim similarities documents documents queries reliably estimated reduced latent space representation original representation 
rationale documents share frequently occurring terms similar representation latent space terms common 
lsa performs sort noise reduction potential bene detect synonyms words refer topic 
applications proven result robust word processing 
lsa applied remarkable success di erent domains including automatic indexing latent semantic indexing lsi number de mainly due unsatisfactory statistical foundation :10.1.1.20.2241:10.1.1.108.8490
primary goal approach lsa factor analysis called probabilistic latent semantic analysis plsa solid statistical foundation likelihood principle proper generative model data 
implies particular standard techniques statistics applied questions model tting model combination complexity control 
addition factor representation obtained plsa allows deal polysemous words explicitly distinguish di erent meanings di erent types word usage 
aspect model core plsa statistical model called aspect model :10.1.1.46.2857
latent variable model general occurrence data associates unobserved class variable fz observation occurrence word fw wmg document fd 
terms generative model de ned way select document probability pick latent class probability zjd generate word probability wjz 
result obtains observed pair latent class variable discarded 
translating process joint probability model results expression wjd wjd wjz zjd essentially derive sum possible choices generated observation 
aspect model statistical mixture model independence assumptions observation pairs assumed generated independently essentially corresponds bag words approach 
secondly conditional independence assumption conditioned latent class words generated independently speci document identity number states smaller number documents acts bottleneck variable predicting conditioned notice contrast document clustering models document speci word distributions wjd obtained convex combination aspects factors wjz 
documents assigned clusters characterized speci mixture factors weights zjd 
mixing weights er modeling power conceptually di erent posterior probabilities clustering models unsupervised naive bayes models cf 
:10.1.1.46.2857
likelihood principle determines zjd wjz maximization log likelihood function log denotes term frequency number times occurred noticing equivalent symmetric version model obtained inverting conditional probability zjd help bayes rule results wjz djz just re parameterized version generative model described 
model fitting tempered em standard procedure maximum likelihood estimation latent variable models expectation maximization em algorithm 
em alternates steps expectation step posterior probabilities computed latent variables current estimates parameters ii maximization step parameters updated posterior probabilities computed previous step 
aspect model symmetric parameterization bayes rule yields step zjd djz wjz djz wjz probability word particular document explained factor corresponding standard calculations arrives step re estimation equations wjz djz zjd zjd zjd zjd zjd alternating de nes convergent procedure approaches local maximum log likelihood 
far focused maximum likelihood estimation equivalently word perplexity reduction 
distinguish predictive performance model training data expected performance unseen test data 
particular naive assume model generalize new data just fact achieve low perplexity training data 
derive conditions generalization unseen data guaranteed fundamental problem statistical learning theory 
propose generalization maximum likelihood mixture models called tempered em tem entropic regularization closely related method known deterministic annealing 
principled derivation tem scope interested reader referred necessary modi cation standard em ad hoc manner 
essentially introduces control parameter inverse computational temperature modi es step djz wjz zjd djz wjz notice results standard step likelihood part bayes formula discounted additively log scale 
shown tem minimizes objective function known free energy de nes convergent algorithm 
temperature generalizations em related algorithms optimization homotopy continuation method avoid unfavorable local extrema main advantage tem context avoid tting 
somewhat contrary spirit annealing continuation method propose utilize temper em heating 
order determine optimal value propose held portion data 
idea implemented scheme set perform em performance held data deteriorates early stopping 
ii 
decrease setting rate parameter 
iii 
long performance held data improves continue tem iterations value iv 
decreasing yield improvements goto step ii 
perform nal iterations training held data 
experiments typical number iterations tem performed starting randomized initial conditions iteration requires pass data order arithmetical operations 
probabilistic latent semantic analysis latent semantic analysis mentioned key idea lsa map documents symmetry terms vector space reduced dimensionality latent semantic space :10.1.1.108.8490
mapping computed decomposing term document matrix svd orthogonal matrices diagonal matrix contains singular values lsa approximation computed thresholding largest singular values zero rank optimal sense matrix norm known linear algebra obtains approximation note norm approximation prohibit entries negative 
geometry aspect model consider class conditional multinomial distributions jz vocabulary aspect model represented points dimensional simplex possible multinomials 
convex hull set points de nes dimensional sub simplex 
modeling assumption expressed conditional distributions jd approximated representable convex combination class conditionals jz 
geometrical view mixing weights zjd correspond exactly coordinates document sub simplex 
simple sketch geometry shown 
demonstrates despite discreteness latent variables introduced aspect model continuous latent space obtained space multinomial distributions 
dimensionality sub simplex opposed complete spanned sub simplex kl divergence projection embedding simplex sketch probability sub simplex spanned aspect model 
probability simplex thought terms dimensionality reduction sub simplex identi ed probabilistic latent semantic space 
mixture decomposition vs singular value decomposition stress point clarify relation lsa rewrite aspect model parameterized matrix notation 
de ne matrices diag zk joint probability model written matrix product comparing decomposition svd decomposition lsa point re interpretation concepts linear algebra weighted sum outer products rows re ects conditional independence plsa 
ii 
left right eigenvectors svd seen correspond factors wjz component distributions djz aspect model 
iii 
mixing proportions plsa substitute singular values svd lsa 
despite similarity fundamental difference plsa lsa objective function utilized determine optimal decomposition approximation 
lsa norm frobenius norm corresponds implicit additive gaussian noise assumption counts 
contrast plsa relies likelihood function multinomial sampling aims explicit maximization predictive power model 
modeling side ers important advantages example mixture approximation cooccurrence table de ned probability distribution factors clear probabilistic meaning terms mixture component distributions 
kullback leibler projection vs orthogonal projection returning geometrical view aspect model sketched interesting reveal projection principle implicitly aspect model 
plane space shuttle family hollywood plane space home lm airport shuttle family movie crash mission music ight love new safety launch kids best aircraft station mother hollywood air crew life love passenger nasa happy actor board satellite friends entertainment airline earth cnn star table factors factor decomposition tdt corpus 
factor represented probable words words ordered wjz 
bosnia iraq kobe un iraq building aid city sanctions people bosnia kuwait relief rescue un people buildings council camps workers nato gulf kobe camp victims nations food area peace hussein earthquake table additional factors factor decomposition tdt corpus cf 
table 
rewriting log likelihood arrives log wjd log rst term brackets corresponds negative kullback leibler kl divergence cross entropy empirical distribution words document wjd model distribution wjd 
xed factors wjz maximizing log likelihood mixing proportions zjd amounts projecting wjd subspace spanned factors kl divergence 
di erent type squared deviation result orthogonal projection cf 
details geometry statistical models 
factor representation example order visualize factor solution plsa elucidating example 
performed exper aid food medical people un war aid food medical people un war aid food medical people un war aid food medical people un war folding query terms aid food medical people un war evolution posterior probabilities mixing proportions rightmost column bar plot factors depicted table rst row second row third row fourth row iterations 
iments tdt collection contains documents broadcast news stories 
words eliminated standard word list stemming preprocessing performed 
table shows reduced representation factors factor solution 
rst factors selected ones highest generate word ight factors highest probability generate word love 
interesting see rst factors capture di erent types usage term ight planes space ships 
similarly factors capture distinguishable contexts word love occurs tdt collection contains documents topics events readers familiar collection preferred test collections utilized section 
med cran cacm cisi precision improvement precision improvement precision improvement precision improvement cos tf lsi plsi plsi plsi plsi cos df lsi plsi plsi plsi plsi table average precision results relative improvement baseline method cos tf cos df respectively standard test collections 
compared lsi plsi plsi variants plsi plsi results obtained combining plsi models plsi plsi respectively 
lsi indicates performance gain achieved baseline result dimensions combination baseline score reported case 
tdt collection real love context family life opposed staged love sense hollywood 
folding queries folding refers problem computing representation document query contained original training collection 
lsa approach simply done linear mapping ectively represents document query center constituent terms appropriate term weighting :10.1.1.108.8490
plsa mixing proportions computed em iteration factors xed mixing proportions adapted step 
table shows factors tdt collection clearly re ect vocabulary dealing certain events war bosnia iraq crisis earthquake kobe 
factors computed representation test query consisting terms aid food medical people un war 
visualizes evolution posterior probabilities mixing proportions course em procedure 
query designed factor matching query terms un involved kobe earthquake medical aid provided iraq gulf war 
seen factor highest weight rst iteration notice factors account half probability 
changes em iterations aspect model introduces feedback terms 
example term un best explained bosnia factor context query terms drastically increases probability particular occurrence un related events 
mechanism able detect true :10.1.1.33.1187
probabilistic latent semantic indexing vector space models lsi popular families information retrieval techniques vector space model vsm documents 
vsm variant ingredients transformation function called local term weight ii term weighting scheme called global term weight iii similarity measure 
experiments utilized representation untransformed term frequencies tf combined ii popular inverse document frequency idf term weights iii standard cosine matching function 
representation applies queries matching function baseline methods written pp pp idf weighted word frequencies 
latent semantic indexing original vector space representation documents replaced representation low dimensional latent space similarity computed representation 
queries documents part original collection folded simple matrix multiplication cf 
details :10.1.1.108.8490
precision precision med tf recall cos tf lsi plsi cos tfidf lsi plsi med tfidf recall cran tf recall cos tf lsi plsi cos tfidf lsi plsi cran tfidf recall cacm tf recall cos tf lsi plsi cos tfidf lsi plsi cacm tfidf recall cisi tf recall cos tf lsi plsi cos tfidf lsi plsi cisi tfidf recall precision recall curves test collections idf term weighting lower row upper row 
depicted curves direct term matching lsi best performing plsi variant 
experiments considered linear combinations original similarity score weight derived latent space representation weight suggested cf 
detailed empirical investigation linear combination schemes information retrieval systems 
variants probabilistic latent semantic indexing di erent schemes exploit plsa indexing investigated context dependent unigram model empirical word distributions documents plsi ii latent space model provides dimensional document query representation plsi plsi document collection plsa provides multinomial distribution wjd vocabulary 
distribution general smooth version empirical distribution wjd 
propose utilize wjd thought document vector order compute matching score document query 
notice wjd representation original word space obtained back projection probabilistic latent space 
vector jd optionally weighted inverse document frequencies compared weighted query cosine 
wehave considered ways combining plsa standard vsm linearly combining cosine similarities discussed lsi ii additively combining multinomials methods language modeling representation wjd wjd wjd 
methods empirically shown identical performance report results variant scheme case lsi 
folding queries possible empirically shown ad plsi scheme 
perplexity model performance cran eld collection terms perplexity upper plot precision lower plot absolute gain vs baseline di erent recall levels di erent values model annealed 
trained convergence early stopping performed 
plsi scheme low dimensional representation zjd toevaluate similarities 
queries folded done xing wjz parameters calculating weights tem 
optimally take account global term weights plsi partially resolved problem 
ad hoc approach reweight di erent model components quantities wjz idf may optimal term weight priors 
advantage statistical models vs svd techniques allows systematically combine di erent models 
optimally done bayesian model combination scheme simpler approach experiments shown excellent performance robustness 
plsi probability estimates wjd models di erent number components additively uniform weights 
plsi scheme combined cosine scores models uniform weight 
resulting methods referred plsi plsi respectively 
empirically performance robust di erent non uniform weights weight combination original cosine score 
due noise reducing bene ts model averaging 
notice lsa representations di erent form nested sequence true statistical models expected capture larger variety reasonable decompositions 
experimental results performance plsi systematically compared standard term matching method raw term frequencies tf combination inverse document frequencies df lsi 
utilized medium sized standard document collection med document abstracts national library medicine ii cran document abstracts aeronautics cran eld institute technology iii cacm abstracts cacm journal iv cisi abstracts library science institute scienti information 
condensed results terms average precision recall recall levels summarized table 
selection average precision recall curves 
details experimental setup plsa models trained tem data set held data 
plsi plsi report best result obtained models lsi report best result obtained optimal dimension exploring dimensions step size 
combination weight cosine baseline score coarsely optimized hand med cran cacm cisi general slightly smaller weights utilized combined models 
experiments consistently validate advantages plsi lsi 
substantial performance gains achieved data sets term weighting schemes 
particular plsi plsi particularly raw term frequencies lsi hand may fail completely accordance results reported :10.1.1.108.8490
explain fact large frequencies dominate squared error deviation svd dampening idf weighting necessary get reasonable decomposition term document matrix 
plsi take advantage term weighting scheme plsi plsi performs slightly better case 
suspect better results achieved improved integration term weights plsi 
bene ts model combination substantial 
cases uniformly combined model performed better best single model 
ect model averaging selecting optimal model dimensionality 
terms computational complexity despite iterative nature em computing time tem model tting roughly comparable svd standard implementation 
larger data sets may consider speeding tem line learning 
notice plsi scheme advantage documents represented low dimensional vector space lsi plsi requires calculation high dimensional multinomials wjd ers advantages terms space requirements indexing information stored 
wehave performed experiment stress importance tempered em standard em model tting 
plots performance factor model trained cran terms perplexity terms precision function seen crucial control generalization performance model precision inversely correlated perplexity 
particular notice model obtained maximum likelihood estimation deteriorates retrieval performance 
outlook novel method automated indexing statistical latent class model 
approach important theoretical advantages standard lsi likelihood principle de nes generative data model directly minimizes word perplexity 
take advantage statistical standard methods model tting tting control model combination 
empirical evaluation clearly con rmed bene ts probabilistic latent semantic indexing achieves signi cant gains precision standard term matching lsi 
investigation needed take full advantage prior information provided term weighting schemes 
shown bene ts plsa extend document indexing similar approach utilized language modeling collaborative ltering 
acknowledgment supported postdoctoral fellowship 
deerwester dumais furnas landauer harshman indexing latent semantic analysis :10.1.1.108.8490
journal american society information science 
dempster laird rubin maximum likelihood incomplete data em algorithm 
royal statist 
soc 
dumais latent semantic indexing lsi trec report 
proceedings text retrieval conference trec harman ed pp 

gildea hofmann topic language models em 
proceedings th european conference communication technology eurospeech 
hofmann latent class models collaborative ltering 
proceedings th international joint conference onarti cial intelligence ijcai 
hofmann probabilistic latent semantic analysis 
proceedings th conference uncertainty ai 
hofmann puzicha jordan unsupervised learning dyadic data :10.1.1.46.2857
advances neural information processing systems vol 

linguistic data consortium 
tdt pilot study corpus 
catalog 
ldc 
mclachlan basford mixture models 
marcel dekker new york basel 
murray rice di erential geometry statistics 
monographs statistics applied probability 
chapman hal 
neal hinton view em algorithm justi es incremental variants 
learning graphical models jordan ed 
kluwer academic publishers pp 

pereira tishby lee distributional clustering english words 
proceedings acl pp 

rose gurewitz fox deterministic annealing approach clustering 
pattern recognition letters 
salton mcgill modern information retrieval 
mcgraw hill 
saul pereira aggregate mixed order markov models statistical language processing 
proceedings nd international conference empirical methods natural language processing 
vogt cottrell predicting performance linearly combined ir systems 
proceedings st acm sigir international conference development information retrieval melbourne australia 
