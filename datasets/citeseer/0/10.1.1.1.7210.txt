ieee transactions neural networks vol 
july learning trade direct reinforcement methods optimizing portfolios asset allocations trading systems direct reinforcement dr 
approach investment decision making viewed stochastic control problem strategies discovered directly 
adaptive algorithm called recurrent reinforcement learning rrl discovering investment policies 
need build forecasting models eliminated better trading performance obtained 
direct reinforcement approach differs dynamic programming reinforcement algorithms td learning learning attempt estimate value function control problem 
find rrl direct reinforcement framework enables simpler problem representation avoids bellman curse dimensionality offers compelling advantages efficiency 
demonstrate direct reinforcement optimize risk adjusted investment returns including differential sharpe ratio accounting effects transaction costs 
extensive simulation real financial data find approach rrl produces better trading strategies systems utilizing learning value function method 
real world applications include intra daily currency trader monthly asset allocation system stock index bills 
index terms differential sharpe ratio direct reinforcement dr downside deviation policy gradient learning recurrent reinforcement learning td learning trading risk value function 
investor trader ultimate goal optimize relevant measure trading system performance profit economic utility risk adjusted return 
describe direct reinforcement dr methods optimize investment performance criteria 
investment decision making viewed stochastic control problem strategies discovered directly 
adaptive algorithm called recurrent reinforcement learning rrl 
need build forecasting models eliminated better trading performance obtained 
methodology applied optimizing systems designed trade single security allocate assets manage portfolio 
investment performance depends sequences interdependent decisions path dependent 
optimal trading portfolio rebalancing decisions require account current system state includes market conditions currently held positions 
market real manuscript received march revised march 
supported nonlinear prediction systems darpa contract daah daah 
authors computational finance program oregon graduate institute science technology beaverton usa nonlinear prediction systems beaverton usa 
publisher item identifier 
john moody matthew ieee world costs trading arbitrarily frequent trades large changes portfolio composition prohibitively expensive 
optimal decisions establishing new positions consider current positions held 
proposed rrl algorithm dr rrl adaptive policy search algorithm learn investment strategy line 
demonstrated papers direct reinforcement provides elegant effective means training trading systems portfolio managers market considered standard supervised approaches 
contrast dr policy search approach commonly value function approaches 
term dr refer algorithms learn value function order derive policy 
dr methods date back pioneering farley clark received little attention reinforcement learning community past decades 
notable exceptions williams reinforce algorithm baxter bartlett 
methods dynamic programming td learning learning focus modern research 
methods attempt learn value function closely related function 
value function methods natural problems checkers backgammon immediate feedback performance readily available point time 
actor critic methods received substantial attention 
algorithms intermediate dr value function methods critic learns value function update parameters actor theoretical progress years area value function learning relatively widely cited successful applications techniques 
notable examples include td gammon elevator scheduler space shuttle payload scheduler 
due inherently delayed feedback applications td learning learning value function rl methods 
financial decision making problems results accrue gradually time immediately measure short term performance 
enables dr approach market include taxes variety transaction costs commissions bid ask spreads price slippage market impact 
baxter bartlett independently proposed term dr policy gradient algorithms markov decision process framework 
term spirit generally refer reinforcement learning algorithm require learning value function 
reviews depth presentations value function actor critic methods see 
ieee transactions neural networks vol 
july provide immediate feedback optimize strategy 
class performance criteria frequently financial community measures risk adjusted investment returns 
rrl learn trading strategies balance accumulation return avoidance risk 
describe commonly measures risk review differential forms sharpe ratio downside deviation ratio formulated enable efficient online learning rrl 
empirical results discovering structure dollar british pound foreign exchange market dr addition compare performance rrl trader trader learn switching strategies stock index treasury bills 
traders results demonstrate presence predictable structure stock prices year test period 
find rrl trader performs substantially better trader 
relative learning observe rrl enables simpler problem representation avoids bellman curse dimensionality offers compelling advantages efficiency 
foreign exchange results previously 
discuss relative merits dr value function learning provide arguments examples value function methods may result unnatural problem representations 
results suggest dr offers powerful alternative reinforcement algorithms learn value function problem domains immediate estimates incremental performance obtained 
conclude note computational finance offers interesting challenging potential applications reinforcement learning methods 
emphasizes dr applications finance date dynamic programming type methods 
elton gruber provide early survey dynamic programming applications finance 
problems optimum consumption portfolio choice continuous time formulated merton dynamic programming stochastic control 
extensive body intertemporal multi period portfolio management asset pricing reviewed 
duffie describes stochastic control dynamic programming methods finance depth 
dynamic programming provides basis cox widely binomial option pricing methods 
see strategic asset allocation brennan 
due curse dimensionality approximate dynamic programming required solve practical problems longstaff schwartz pricing american options 
past years applications value function reinforcement learning methods 
van roy uses td approach valuing options performing portfolio optimization 
uses learning approach asset allocation decisions incorporate notion risk sensitivity construction function 
derivatives pricing applications studied tsitsiklis van roy 
moody compare dr learning asset allocation explore minimization downside risk dr 
ii 
trading systems performance criteria structure trading systems consider agents trade fixed position sizes single security 
methods described generalized sophisticated agents trade varying quantities security allocate assets continuously manage multiple asset portfolios 
see discussion multiple asset portfolios 
traders assumed take long neutral short positions constant magnitude 
long position initiated purchasing quantity security short position established selling security 
price series traded denoted position established maintained time interval period trade possible time period nonzero trading costs discourage excessive trading 
trading system return realized time interval includes profit loss resulting position held interval transaction cost incurred time due difference positions order properly incorporate effects transactions costs market impact taxes trader decision making trader internal state information recurrent 
single asset trading system takes account transactions costs market impact decision function denotes learned system parameters time denotes information set time includes past values price series arbitrary number external variables denoted simple example long short trader autoregressive inputs sign price returns defined system parameters weights trader form simulations described section iv 
formulation describes discrete action deterministic trader easily generalized 
simple generalization continuously valued example replacing discrete values imposed decision function differentiable 
gradient optimization methods may developed considering differentiable stocks short sale involves borrowing shares selling borrowed shares third party 
profit shorted shares time lower price 
short sales securities including stocks bonds futures options foreign exchange contracts common place 
moody learning trade direct reinforcement outputs example replacing learning discretizing outputs trading 
models extended stochastic framework including noise variable random variable induces joint probability density discrete actions model parameters model inputs noise level measured generally scale varied control exploration vs exploitation behavior trader 
differentiability probability distribution actions enables straightforward application gradient learning methods 
profit wealth trading systems trading systems optimized maximizing performance functions profit wealth utility functions wealth performance ratios sharpe ratio 
simplest natural performance function risk insensitive trader profit 
additive profits appropriate consider trade fixed number shares contracts security case example trading small stock futures accounts trading standard fx contracts foreign currencies 
define price returns risky traded asset risk free asset bills respectively denote transactions cost rate additive profit accumulated time periods trading position size defined term trading returns typically risk free rate interest ignored simplified expression obtained wealth trader defined multiplicative profits appropriate fixed fraction accumulated wealth invested long short trade 
short sales allowed leverage factor set fixed wealth time risk free rate interest ignored second simplified expression obtained relaxing constant magnitude assumption realistic asset allocations portfolios enables better risk control 
related expressions portfolios 
performance criteria general performance criteria consider functions profit wealth sequence time steps generally time sequence trades simple form includes standard economic utility functions 
second case general form path dependent performance functions include inter temporal utility functions performance ratios sharpe ratio sterling ratio 
case performance criterion time expressed function sequence trading returns brevity denote general form optimizing traders interested marginal increase performance due return time step note depends current trading return 
strategy derive differential performance criteria capture marginal utility trading return period 
differential sharpe ratio maximizing profits modern fund managers attempt maximize risk adjusted return suggested modern portfolio theory 
sharpe ratio widely measure risk adjusted return 
denoting trading system returns period including transactions costs sharpe ratio defined average standard deviation average standard deviation estimated periods note ease exposition analysis suppressed inclusion portfolio returns due risk free rate capital substituting excess returns strictly speaking performance criteria commonly financial industry true utility functions term utility colloquial sense 
ieee transactions neural networks vol 
july equation produces standard definition 
caveat mind discussion purposes loss mathematical generality 
proper line learning requires compute influence sharpe ratio marginal utility trading return time accomplish derived new objective function called differential sharpe ratio line optimization trading system performance 
obtained considering exponential moving averages returns standard deviation returns expanding order adaptation rate note zero adaptation rate corresponds infinite time average 
expanding amounts turning adaptation 
order term expansion depends return time define differential sharpe ratio quantities exponential moving estimates second moments treating numerical constants note update equations controls magnitude influence return sharpe ratio differential sharpe ratio represents influence trading return realized time marginal utility sharpe ratio criterion 
influences risk return differential sharpe ratio readily apparent 
current return enters expression numerator term numerator positive exceeds moving average past returns increased reward second term negative exceeds moving average past squared returns increased risk 
differential sharpe ratio rrl algorithm see section iii current contribution performance function depend optimizing trading system relevant derivatives simple form systems trade futures forward place risk free rate accounted relation forward prices spot prices 
differential sharpe ratio attractive properties facilitates recursive updating incremental nature calculations updating exponential moving sharpe ratio straightforward 
necessary recompute average standard deviation returns entire trading history order update sharpe ratio time period 
enables efficient line optimization cheaply calculated previously computed moving averages current return enables efficient stochastic optimization 
weights returns exponential moving average sharpe ratio returns receive stronger weightings older returns 
provides interpretability differential sharpe ratio isolates contribution current return exponential moving average sharpe ratio 
simple form clear risk reward affect sharpe ratio 
difficulty sharpe ratio variance risk measure distinguish upside downside risk assuming largest possible improvement occurs sharpe ratio penalizes gains larger counter intuitive relative investors notions risk reward 
downside risk symmetric measures risk variance viewed inadequate measures due asymmetric preferences investors price changes 
investors consider large positive returns risky large positive negative returns penalized symmetric measure risk variance 
investors term risk refers intuitively returns portfolio decrease profitability 
markowitz father modern portfolio theory understood 
focussed mean variance framework portfolio optimization proposed semivariance means dealing downside returns 
long lasting decades vigorous industry financial community modeling minimizing downside risk 
criteria interest include double deviation dd second lower partial moment th lower partial moment 
measure risk adjusted performance widely professional fund management community especially hedge funds sterling ratio commonly defined sterling ratio annualized average return maximum drawn maximum draw peak trough account equity net asset value defined relative standard period example years 
mini moody learning trade direct reinforcement somewhat cumbersome focus dd measure downside risk 
dd defined square root average square negative returns dd dd measure risk define utility function similar sharpe ratio call downside deviation ratio ddr ddr average dd ddr rewards presence large average positive returns penalizes risky returns risky refers downside returns 
order facilitate recurrent reinforcement learning algorithm section iii need compute influence return time ddr 
similar manner development differential sharpe ratio define exponential moving averages returns squared dd dd dd dd define ddr terms moving averages 
obtain performance function considering order expansion adaptation rate ddr ddr ddr ddr define order term ddr differential downside deviation ratio 
form ddr dd dd dd obvious utility increases increases penalty large positive returns exists variance risk measure 
see detailed experimental results ddr build rrl trading systems 
iii 
learning trade reinforcement learning adjusts parameters system maximize expected payoff reward generated due actions system 
accomplished trial white dd tracks sterling ratio effectively 
error exploration environment space strategies 
contrast supervised learning system examples desired actions 
receives reinforcement signal environment reward provides information actions bad 
compared supervised learning dr approach 
supervised methods discussed included trading forecasts market prices training trader labeled data 
supervised frameworks difficulties encountered transaction costs included 
supervised learning methods effective solving structural credit assignment problem typically address temporal credit assignment problem 
structural credit assignment refers problem assigning credit individual parameters system 
reward produced depends series actions system temporal credit assignment problem encountered 
assigning credit individual actions taken time 
reinforcement learning algorithms offer advantages supervised methods attempting solve problems simultaneously 
reinforcement learning algorithms classified dr called policy search value function methods 
choice best method depends nature problem domain 
discuss issue greater detail section section recurrent reinforcement learning algorithm dr review value function methods specifically learning refinement learning called advantage updating 
section iv compare rrl value function methods systems learn allocate assets stock index bills 
recurrent reinforcement learning section describe recurrent reinforcement learning algorithm dr algorithm originally 
trading system model goal adjust parameters order maximize traders form trading returns form gradient respect parameters periods system sequence system optimized batch mode repeatedly computing value forward passes data adjusting trading system parameters gradient ascent learning rate optimization method 
note due inherent recurrence quantities total derivatives depend entire sequence previous time periods 
correctly compute optimize total derivatives effi ieee transactions neural networks vol 
july cient manner requires approach similar backpropagation time bptt 
temporal dependencies sequence decisions accounted recursive update equation parameter gradients expressions assume differentiability long short traders thresholds described section ii reinforcement signal backpropagated outputs manner similar learning rule 
equations constitute batch rrl algorithm 
ways batch algorithm described extended stochastic framework 
exploration strategy space induced incorporating noise variable stochastic trader formulation 
tradeoff exploration strategy space exploitation learned policy controlled magnitude noise variance noise magnitude annealed time simulation order arrive strategy 
second simple line stochastic optimization obtained considering term depends realized return forward pass data parameters updated line algorithm performs stochastic optimization system parameters varied forward pass training data 
stochastic line analog equations constitute stochastic adaptive rrl algorithm 
reinforcement algorithm closely related recurrent supervised algorithms real time recurrent learning rtrl dynamic backpropagation 
see discussion backpropagating utility werbos 
differential performance criteria described section ii differential sharpe ratio differential downside deviation ratio stochastic update equations line algorithms recurrent reinforcement learning type simulations section iv 
note find noise variable provides little advantage real financial applications consider data series contain significant intrinsic noise 
find simple greedy update adequate 
description rrl algorithm traders optimize immediate estimates performance specific actions taken 
presentation thought special case general markov decision process mdp policy gradient formulation 
straightforward extension formulation obtained traders maximize discounted rewards 
experimented approach little advantage problems consider 
second extension formulation consider stochastic trader expected reward framework probability distribution actions differentiable 
approach joint density 
expected reward framework appealing theoretical perspective provide practical basis simulations 
focussed discussion traders single risky asset scalar algorithms described section trivially generalized vector case portfolios 
optimization portfolios described 
value functions learning explicitly training trader take actions implicitly learn correct actions technique value iteration 
value iteration uses value function evaluate improve policies see tutorial full overview algorithms 
value function estimate discounted rewards received starting state policy 
value function satisfies bellman equation probability action state probability transitioning state state action immediate reward differential utility action transitioning state state discount factor weighs importance rewards versus immediate rewards 
policy optimal policy value function greater equal value functions policies set states 
optimal value function defined tesauro finds similar result td gammon 
greedy update works dice rolls game provided uncertainty induce extensive strategy exploration 
moody learning trade direct reinforcement satisfies bellman optimality equation value iteration update guaranteed converge optimal value function certain general conditions 
optimal policy determined optimal value function arg learning technique named learning uses value function estimates rewards current state current action taken 
write function version bellman optimality equation similarly function learned value iteration approach iteration shown converge optimal function certain constraints 
advantage function need know system model order choose best action 
calculates best action arg update rule training function approximator gradient error advantage updating refinement learning algorithm provided advantage updating 
advantage updating developed specifically deal continuous time reinforcement learning problems applicable discrete time case 
designed deal situation relative advantages individual actions state small compared relative advantages different states 
advantage updating shown able learn faster rate learning presence noise 
advantage updating learns separate functions advantage function value function advantage function measures relative change value choosing fig 

trading system dr approach taken 
system trades target series making trading decisions set input variables current positions held 
intermediate steps making forecasts labeling desired trades required necessary learn value function 
trader learns strategy trial error exploration actions receiving positive negative reinforcement results 
trading performance function profit utility risk adjusted return directly optimize trading system parameters system recurrent feedback system state current positions portfolio weights enables trading system learn correctly incorporate transactions costs trading decisions 
traders considered dr policy search method recurrent reinforcement learning optimize trader 
action state versus choosing best possible action state 
value function measures expected discounted rewards described previously 
advantage updating relationship learning similarly learning optimal action take state arg see baird description learning algorithms 
iv 
empirical results section presents empirical results problems 
controlled experiments artificial price series done test rrl algorithm ability learn profitable trading strategies maximize risk adjusted return measured sharpe ratio respond appropriately varying transaction costs 
second problem demonstrates ability rrl discover structure real financial price series half hourly dollar british pound exchange rate see fig 

problem rrl trader attempts avoid downside risk maximizing downside deviation ratio 
compare performance traders rrl learning second real world problem trading monthly stock index 
year test period find rrl trader outperforms trader outperform buy hold strategy 
discussion trader versus rrl trader performance section 
trader simulation section demonstrate rrl algorithm optimize trading behavior differential sharpe ratio presence transaction costs 
extensive results ieee transactions neural networks vol 
july fig 

artificial prices top panel trading signals second panel cumulative sums profits third panel moving average sharpe ratio bottom panel 
system performs poorly learning scratch time periods performance remains 

find maximizing differential sharpe ratio yields consistent results maximizing profits methods outperform trading systems forecasts 
rrl traders studied take long short positions recurrent state similar described section ii 
enable controlled experiments data section artificial price series designed structure 
experiments demonstrate rrl effective means learning trading strategies trading frequency reduced expected transaction costs increase 
data generate log price series random walks autoregressive trend processes 
parameter model constants normal random deviates zero mean unit variance 
define artificial price series scale defined range simulation samples 
results set parameters price process artificial price series short time scales high level noise 
realization artificial price series shown top panel fig 

simulated trading results figs 
show results single simulation artificial market described 
slightly number hours year series thought representing hourly prices hour artificial market 
alternatively series length represent slightly years hourly data market trades hours week 
fig 

expanded view time periods fig 

exponential moving sharpe ratio forgetting time scale ia periods 
smaller smooth fluctuations 
fig 

histograms price changes top trading profits time period middle sharpe ratios bottom simulation shown fig 

left column time periods right column time periods 
transient effects time periods real time recurrent learning evident lower left graph 
experiments rrl traders single threshold units autoregressive input representation 
inputs time constructed previous returns 
rrl traders initialized randomly adapted real time recurrent learning optimize differential sharpe ratio 
transaction costs fixed half percent real time learning trading process 
transient effects initial learning trading process seen time steps fig 
distribution differential sharpe ratios lower left panel fig 

fig 
shows box plots summarizing test performances ensembles experiments 
simulations data samples partitioned initial training set consisting samples subsequent test data set containing samples 
rrl traders moody learning trade direct reinforcement optimized training data set epochs adapted line test data set 
trial different realizations artificial price process different randomly chosen initial trader parameter values 
vary transaction cost observe trading frequency cumulative profit sharpe ratio test data set 
shown experiments positive sharpe ratios obtained 
expected trading frequency reduced transaction costs increase 
dollar british pound foreign exchange trading system long short neutral trading system trained half hourly dollar british pound foreign exchange fx rate data 
experiments described section reported 
dataset consists months quotes hour days week foreign exchange market 
bid ask prices dataset trading system required incur transaction costs trading bid ask prices 
trader trained rrl algorithm maximize differential downside deviation ratio measure risk adjusted return 
top panel fig 
shows dollar british pound price series month period 
trading system initially trained data points produces trading signals week period data points 
training window shifted forward include just tested data retrained trading signals recorded week sample time period 
process generating sample trading signals continues rest data set 
second panel fig 
shows sample trading signal produced trading system third panel displays equity curve achieved trader 
bottom panel shows moving average calculation sharpe ratio trading period time constant 
trading system achieves annualized return annualized sharpe ratio approximately month long test period 
average system trade hours 
fx simulations demonstrate ability rrl algorithm discover structure real world financial price series 
cautious extrapolating simulated performance achieved actual real time trading 
problem data set consists indicative quotes necessarily representative price system able transact 
related possibility system discovering market microstructure effects real time 
simulation assumes pound hours day day trading week 
certainly real time trading system suffer additional penalties trying trade peak low liquidity trading times 
accurate test trading system require live trading foreign exchange broker directly data part olsen associates dataset obtainable contacting www olsen ch 
fig 

boxplots trading frequency cumulative sums profits sharpe ratios versus transaction costs 
results obtained trials various realizations artificial data initial system parameters 
increased transaction costs reduce trading frequency profits sharpe ratio expected 
trading frequency percentage number time periods trades occur 
figures computed points data set 
fx market order verify real time prices profitability 
ieee transactions neural networks vol 
july fig 

long short neutral trading system dollar british pound uses bid ask spread transaction costs 
data consists half hourly quotes day week hour fx market 
time period shown months 
trader optimized recurrent reinforcement learning maximize differential downside deviation ratio 
data points approximately months training validation 
trading system achieves annualized return annualized sharpe ratio approximately month sample test period 
average system trade hours 
fig 

time series influence return attainable asset allocation system 
top panel shows series dividends 
bottom panel shows annualized monthly treasury bill dividend yields 
bill asset allocation section compare recurrent reinforcement learning advantage updating formulation learning algorithm building trading system 
comparative results previously nips 
long short trading systems trade stock index effect allocating assets month treasury bills 
traders long bill interest earned traders short stocks standard leverage earn twice bill rate 
advantage updating refinement standard learning algorithm yield better trading results 
see section iii description representational advantages approach 
target series total return index computed monthly dividends 
indexes moody learning trade direct reinforcement dividends shown fig 
month treasury bill dividend yields 
monthly input series trading systems include financial macroeconomic data 
data obtained macroeconomic series lagged month reflect reporting delays 
total years monthly data january december 
years data initial training system 
test period year period january december 
experimental results year test period true ex ante simulated trading results 
simulation details year system trained moving window previous years data 
system initialized random parameters 
subsequent years previously learned parameters initialize training 
way system able adapt changing market economic conditions 
moving training window rrl trader systems years stochastic optimization system parameters subsequent years validating early stopping training 
rrl trader networks single unit regularized quadratic weight decay training regularization parameter 
trader systems bootstrap sample year training window training final years training window validating early stopping training 
results reported networks layer feedforward networks units hidden layer 
networks trained initially discounting factor set zero 
set 
find decreasing performance value adjusted higher values 
investigate bias variance tradeoff traders tried networks size hidden units 
unit traders performed significantly better sample traders smaller larger networks 
unit traders significantly better unit traders suggesting smaller networks represent function adequately high model bias 
degradation performance observed unit nets suggests possible overfitting increased model variance 
experimental results fig 
shows box plots summarizing test performance full year test period trading systems various realizations initial system parameters trials rrl trader system trials trader system 
transaction cost set 
profits trading multiplicative profits calculating wealth 
box plots indicate robust estimates confidence intervals hypothesis median equal performance buy hold strategy 
horizontal lines show performance rrl trader voting trader voting buy hold strategies test period 
total profits buy hold strategy trader voting strategy rrl trader voting strategy historical data obtainable www fame com 
trials done trader system due amount computation required training systems 
fig 

test results ensembles simulations stock index month treasury bill data time period 
boxplots show performance ensembles rrl trader trader trading systems 
horizontal lines indicate performance systems buy hold strategy 
solid curves correspond rrl trader system performance dashed curves trader system dashed dotted curves indicate buy hold performance 
systems significantly outperform buy hold strategy 
respectively 
corresponding annualized monthly sharpe ratios respectively 
remarkably superior results rrl trader networks single thresholded unit trader required networks hidden units 
fig 
shows results strategy positions majority vote ensembles trading systems compared buy hold strategy 
see trading systems go short critical periods oil price shock tight money periods early market correction crash 
ability take advantage high treasury bill rates avoid periods substantial stock market loss major factor long term success trading models 
exception rrl trader trading system remains long stock market correction associated persian gulf war political event trader system fortunately short correction 
trader system trades frequently rrl trader system perform data set 
results find trading systems outperform buy hold strategy measured accumulated wealth sharpe ratio 
differences statistically significant support proposition predictability stock treasury bill markets year period 
detailed presentation rrl trader results appears 
discussion trader versus rrl trader performance section 
sharpe ratios calculated returns excess treasury bill rate 
discussed section iv care taken avoid underfitting overfitting trader case smaller nets performed substantially worse 
ieee transactions neural networks vol 
july fig 

test results ensembles simulations stock index month treasury bill data time period 
shown equity curves associated systems buy hold strategy trading signals produced systems 
solid curves correspond rrl trader system performance dashed curves trader system dashed dotted curves indicate buy hold performance 
systems significantly outperform buy hold strategy 
cases traders avoid dramatic losses buy hold strategy incurred 
model insight sensitivity analysis sensitivity analysis rrl trader systems performed attempt determine economic factors traders basing decisions 
fig 
shows absolute normalized sensitivities salient input series function time averaged members rrl trader committee 
sensitivity input defined trading output policy function denotes input time varying sensitivities fig 
emphasize nonstationarity economic relationships 
example yield curve slope measures inflation expectations important factor trends long term interest rates measured month difference aaa bond yield important trends short term interest rates measured month difference treasury bill yield dominate early 
learn policy learn value 
mentioned section iii reinforcement learning algorithms classified called policy search value function methods actor critic methods 
choice best method depends nature problem domain 
immediate versus rewards reinforcement signals received environment immediate delayed 
problems checkers backgammon navigating maze fig 

sensitivity traces inputs rrl trader trading system averaged ensemble traders 
nonstationary relationships typical economic variables evident time varying sensitivities 
maneuvering obstacles reinforcement environment occurs game task 
final rewards received success failure win lose tasks temporal credit assignment problem extreme 
usually priori assessment performance available course game trial 
forced learn value function system state time 
accomplished doing runs trial error basis discounting ultimate reward received back time 
discounting approach basis dynamic programming td learning learning 
value function methods action taken time offers largest increase expected value 
policy represented directly 
intermediate class reinforcement algorithms actor critic methods 
actor module provides direct representation policy methods relies critic module feedback 
role critic learn value function 
contrast direct reinforcement methods represent policy directly immediate feedback adjust policy 
approach appealing possible specify instantaneous measure performance need learn value function bypassed 
trading asset allocation portfolio management problems example performance accrues gradually time 
financial decision making problems immediate measure incremental performance available time step 
total performance usually involves integrating averaging time possible adaptively update strategy investment return received time step 
domains offer possibility immediate feedback include wide range control applications 
standard formulation optimal control problems involves time integrals instantaneous performance measure 
examples common loss functions include average squared deviation desired trajectory average squared jerk 
jerk rate change acceleration 
moody learning trade direct reinforcement related approach represents improves policies explicitly policy gradient approach 
policy gradient methods gradient expected average discounted reward respect parameters policy function improve policy 
expected rewards typically estimated learning value function single sample paths markov reward process 
independent proofs convergence policy gradient methods 
tsitsiklis baxter bartlett show convergence locally optimal policies simulation methodologies approximate expected rewards 
sutton tsitsiklis obtain similar results estimating expected rewards value function implemented function approximator 
application robot navigation provided ungar 
note called policy gradient methods dr methods require estimation value function 
methods properly classified actor critic methods 
policies versus values attention reinforcement learning community question learning policies versus learning value functions 
past years value function approach dominated field 
approach worked applications number convergence theorems exist prove approach certain conditions 
value function approach suffers limitations 
original formulation learning context discrete state action spaces 
practical situations suffers curse dimensionality learning extended function approximators shown cases simple markov decision processes algorithms fail converge 
policies derived learning approach tend brittle small changes value function produce large changes policy 
finance particular presence large amounts noise nonstationarity datasets cause severe problems value function approach 
find rrl algorithm simpler efficient approach 
policy represented directly simpler functional form adequate solve problem 
significant advantage rrl approach ability produce real valued actions portfolio weights naturally resorting discretization necessary learning case 
constraints actions easier represent policy representation 
advantages rrl algorithm robust large amounts noise exists financial data able quickly adapt nonstationary market conditions 
baxter bartlett independently coined term dr describe policy gradient methods mdp framework simulating sample paths maximizing average rewards 
intended usage term spirit general referring algorithms need learn value function order derive policy 
brown provides nice example demonstrates brittleness learners noisy environments 
fig 

representation value function learned learning algorithm example text section 
function represents value value action state left shows value function case discrete binary returns 
function form xor problem optimal policy simply 
right shows value function returns real valued note change axes 
function arbitrarily hard represent accurately single function approximator units optimal policy simple sign example example increase complexity occurs policy represented implicitly value function 
start simple trading problem trader decisions buy sell single asset transaction costs trading 
asset returns binomial process matters simple assume period return known advance 
conditions optimal policy require knowledge rewards learning discount parameter set zero 
measure complexity solution counting number units required implement solution single function approximator 
obvious policy function trivial 
optimal policy take action terms model structure single unit suffice 
hand decide learn value function actions find case learn xor function 
shown fig 
value function proposed action sign 
binomial return process solve problem units 
due value function representation problem complexity solution doubled 
doubling model complexity comparison minor problem little realistic allowing returns drawn continuous real valued distribution 
complexity policy function increased sign value function increase complexity potentially enormous 
returns real valued wish approximate value function arbitrarily small precision arbitrarily large model 
discussion bill results bill asset allocation problem described section iv find rrl offers advantages learning performance interpretability computational efficiency 
year test period rrl trader produced significantly higher profits versus sharpe ratios versus trader 
rrl trader learns stable robust trading strategy ieee transactions neural networks vol 
july maintaining positions extended periods 
frequent switches position trader suggests sensitive noise inputs 
strategy learned brittle 
regarding interpretability find value function representation obscure 
change policy implemented rrl algorithm directly related changes inputs value function effect policy clear 
rrl trader linear policy representation net just single unit trader policy layer network policy input 
brittle behavior trader probably due complexity learned function respect inputs actions 
problem representation trader reduces explanatory value 
sensitivity analysis rrl trader strategy section iv easy formulate implement 
enables identify important explanatory variables observe relative saliency varies slowly time 
trader similar analysis straightforward 
possible actions represented inputs function network chosen action determined imagine proxies sensitivity analysis simple action long short framework clear perform sensitivity analysis actions versus inputs general learning framework 
reduces explanatory value trader 
long short trader implemented neural network function approximator bellman curse dimensionality relatively small impact results experiments 
input dimensionality trader increased actions consider 
case portfolio management multi sector asset allocation system dimensionality problem severe 
portfolio management requires continuous weight assets included portfolio 
increases input dimension trader relative rrl trader 
order facilitate discovery actions consider discrete action sets 
number discrete actions considered exponential issue consider possible loss utility results due finite resolution action choices 
terms efficiency advantage updating representation trader required networks units 
order reduce run time simulation code written run required approximately hours complete pentium pro running linux operating system 
rrl networks single unit implemented matlab code 
unoptimized coding rrl simulations times faster min 
vi 
demonstrated train trading systems dr described rrl algorithm optimize financial performance criteria differ encountered obstacle preliminary unpublished experiments 
sharpe ratio differential downside deviation ratio 
provided empirical results demonstrate presence predictability discovered rrl dollar british pound exchange rates monthly stock index year test period 
previous showed trading systems trained rrl significantly outperform systems trained supervised methods 
compared dr approach rrl learning value function method 
find rrl trader achieves better performance trader bill asset allocation problem 
observe relative learning rrl enables simpler problem representation avoids bellman curse dimensionality offers compelling advantages efficiency 
discussed relative merits dr value function learning provided arguments examples value function methods may result unnatural problem representations 
problem domains immediate estimates incremental performance obtained results suggest dr offers powerful alternative 
acknowledgment authors wish wu liao contributions early dr tesauro reviewers helpful comments manuscript 
moody wu optimization trading systems portfolios decision technologies financial engineering abu mostafa weigend eds 
london world scientific pp 

moody wu liao performance functions reinforcement learning trading systems portfolios forecasting vol 
pp 

clark farley simulation self organizing systems digital computer ire trans 
inform 
theory vol 
pp 

generalization pattern recognition self organizing system proc 
western joint comput 
conf pp 

williams theory reinforcement learning connectionist systems college comput 
sci northeastern univ boston ma tech 
rep nu ccs 
williams simple statistical gradient algorithms connectionist reinforcement learning machine learning vol 
pp 

baxter bartlett direct gradient reinforcement learning gradient estimation algorithms comput 
sci 
lab australian nat 
univ tech 
rep 
bellman dynamic programming 
princeton nj princeton univ press 
sutton learning predict method temporal differences machine learning vol 
pp 

watkins learning delayed rewards ph thesis cambridge univ psychol 
dept 
watkins dayan technical note learning machine learning vol 
pp 

barto sutton anderson neuronlike adaptive elements solve difficult learning control problems ieee trans 
syst man cybern vol 
pp 
sept 
barto handbook intelligent control 
new york van nostrand reinhold ch 

handbook intelligent control white eds van nostrand reinhold new york pp 

handbook intelligent control white eds van nostrand reinhold new york pp 

kaelbling littman moore reinforcement learning survey artificial intell 
res vol 

bertsekas tsitsiklis neuro dynamic programming 
belmont ma athena scientific 
moody learning trade direct reinforcement sutton barto reinforcement learning 
cambridge ma mit press 
tesauro td gammon self teaching backgammon program achieves master level play neural comput vol 
pp 

temporal difference learning td gammon commun 
acm vol 
pp 

crites barto improving elevator performance reinforcement learning advances neural information processing systems touretzky mozer hasselmo eds vol 
pp 

zhang dietterich high performance job shop scheduling time delay td 
network advances neural inform 
processing syst touretzky mozer hasselmo eds vol 
pp 

moody reinforcement learning trading advances neural inform 
processing syst solla kearns cohn eds mit press vol 
pp 

minimizing downside risk stochastic dynamic programming computational finance lo abu mostafa lebaron weigend eds 
cambridge ma mit press pp 

merton lifetime portfolio selection uncertainty continuous time case rev economics statist vol 
pp 
aug 
optimum consumption portfolio rules continuous time model economic theory vol 
pp 
dec 
merton continuous time finance 
oxford blackwell 
elton gruber dynamic programming applications finance finance vol 

intertemporal portfolio theory asset pricing finance newman eds 
new york macmillan pp 

duffie security markets stochastic models 
new york academic 
dynamic asset pricing theory nd ed 
princeton nj princeton univ press 
cox ross rubinstein option pricing simplified approach financial economics vol 
pp 
oct 
brennan schwartz strategic asset allocation economic dynamics contr vol 
pp 

longstaff schwartz valuing american options simulation simple squares approach rev financial studies published 
van roy temporal difference learning applications finance computational finance abu mostafa lebaron lo weigend eds 
cambridge ma mit press pp 

optimal asset allocation adaptive dynamic programming advances neural information processing systems touretzky mozer hasselmo eds 
cambridge ma mit press vol 
pp 

risk sensitive reinforcement learning advances neural information processing systems kearns solla cohn eds 
cambridge ma mit press vol 
pp 

tsitsiklis van roy optimal stopping markov processes hilbert space theory approximation algorithms application pricing high dimensional financial derivatives ieee trans 
automat 
contr vol 
pp 
oct 
regression methods pricing complex american style options ieee trans 
neural networks vol 
pp 
july 
sharpe mutual fund performance business pp 
jan 
markowitz portfolio selection efficient diversification investments 
new york wiley 
van der meer downside risk capturing stake investment situations portfolio management vol 
pp 

optimal algorithms lower partial moment ex post results appl 
economics vol 
pp 

characteristics portfolios selected degree lower partial moment int 
rev financial anal vol 
pp 

forsey misuse downside risk portfolio management vol 
pp 

brief history downside risk measures investing pp 
fall 
white private communication 
sutton temporal credit assignment reinforcement learning ph dissertation univ massachusetts amherst 
baird advantage updating wright laboratory wright patterson air force base oh tech 
rep wl tr 
rumelhart hinton williams learning internal representations error propagation parallel distributed processing exploration microstructure cognition rumelhart mcclelland eds 
cambridge ma mit press ch 
pp 

werbos back propagation time proc 
ieee vol 
pp 
oct 
widrow hoff adaptive switching circuits ire convention record pp 

williams zipser learning algorithm continually running fully recurrent neural networks neural comput vol 
pp 

narendra parthasarathy identification control dynamical systems neural networks ieee trans 
neural networks vol 
pp 

samuel studies machine learning game checkers ibm res 
development vol 
pp 

studies machine learning game checkers 
ii progress ibm res 
development vol 
pp 

peng williams efficient learning planning dyna framework adaptive behavior vol 
pp 

moore atkeson prioritized sweeping reinforcement learning data real time machine learning vol 
pp 

tsitsiklis simulation optimization markov reward processes proc 
ieee conf 
decision contr 
simulation optimization markov reward processes ieee trans 
automat 
contr vol 
pp 
feb 
sutton mcallester singh mansour policy gradient methods reinforcement learning function approximation advances neural information processing systems leen solla 
muller eds 
cambridge ma mit press vol 
pp 

tsitsiklis actor critic algorithms advances neural information processing systems solla leen 
muller eds 
cambridge ma mit press vol 
pp 

ungar localizing policy gradient estimates action transitions proc 
th int 
conf 
machine learning 
baird moore gradient descent general reinforcement learning advances neural information processing systems solla kearns cohn eds 
cambridge ma mit press vol 
pp 

brown policy vs value function learning variable discount factors proc 
nips workshop reinforcement learning learn policy learn value function dec 
john moody received degree physics university chicago chicago il ph degrees theoretical physics princeton university princeton nj respectively 
director computational finance program professor computer science electrical engineering oregon graduate institute science technology beaverton 
founder president nonlinear prediction systems specializing development forecasting trading systems 
research interests include computational finance time series analysis machine learning 
dr moody served program chair computational finance london past general chair program chair neural information processing systems nips conference member editorial board quantitative finance 
matthew received sc 
degree computer science engineering minor mathematics university tx sc 
degree computer science engineering university tennessee 
pursuing ph degree computer science engineering department oregon graduate institute beaverton 
consulting scientist nonlinear prediction systems 
