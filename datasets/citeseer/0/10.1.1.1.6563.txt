ieee transactions pattern analysis machine intelligence vol 
september floatboost learning statistical face detection stan li senior member ieee zhang novel learning procedure called floatboost proposed learning boosted classifier achieving minimum error rate 
floatboost learning uses backtrack mechanism iteration adaboost learning minimize error rate directly minimizing exponential function margin traditional adaboost algorithms 
second contribution novel statistical model learning best weak classifiers stagewise approximation posterior probability 
novel techniques lead classifier requires fewer weak classifiers adaboost achieves lower error rates training testing demonstrated extensive experiments 
applied face detection floatboost learning method proposed detector pyramid architecture leads real time multiview face detection system reported 
index terms pattern classification boosting learning adaboost floatboost feature selection statistical models face detection 
nonlinear classification high dimensional data challenging problem 
adaboost methods introduced freund schapire provide simple effective approach stagewise learning nonlinear classification function :10.1.1.32.8918
classifier classification function assigns input class label 
classifier difficult obtain adaboost learns sequence easily learnable weak classifiers performances may poor better random guessing boosts combines strong classifier higher accuracy 
originating pac probably approximately correct learning theory adaboost provably achieves arbitrarily bounds training generalization errors provided weak classifiers perform slightly better random guessing distribution training set :10.1.1.156.2440:10.1.1.156.2440:10.1.1.32.8918:10.1.1.32.8918
shown simple weak classifiers boosted capture complex decision boundaries 
relationships adaboost functional optimization statistical estimation established 
shown adaboost learning procedure minimizes upper error bound exponential function margin training set :10.1.1.31.2869
gradient boosting algorithms proposed provide new insights adaboost learning :10.1.1.29.9093
significant advance friedman :10.1.1.30.3515
shown adaboost 
weak classifier simple classification function example nearest neighbor classifier thresholded feature value likelihood ratio function posterior ratio function 
face detection system described posterior ratio type weak classifiers 
li microsoft research asia beijing sigma center road hai district beijing china 
mail microsoft com 
zhang beckman institute university illinois urbana champaign mathews ave urbana il 
mail uiuc edu 
manuscript received june revised feb accepted mar 
recommended acceptance govindaraju 
information obtaining reprints article please send mail tpami computer org ieeecs log number tpami 
algorithms interpreted stagewise estimation procedures fit additive logistical regression model 
discrete adaboost real version optimize exponential loss function albeit different ways :10.1.1.156.2440:10.1.1.156.2440:10.1.1.156.2440:10.1.1.32.8918
links adaboost advocated machine learning viewpoint statistical theory :10.1.1.30.3515
boosting learning problems associated adaboost motivated investigate effective boosting learning algorithm adaboost minimizes exponential form function margin training set :10.1.1.31.2869
convenience theoretical numerical analysis :10.1.1.30.3515
ultimate goal applications pattern classification usually minimize cost directly usually linearly associated error rate 
strong classifier learned adaboost suboptimal applications terms error rate 
problem noted solutions literature 
second adaboost leaves challenge learning weak classifiers practitioner choice 
learning optimal weak classifier log posterior ratio requires estimation densities feature space :10.1.1.30.3515:10.1.1.30.3515:10.1.1.156.2440:10.1.1.156.2440:10.1.1.156.2440:10.1.1.156.2440
difficult problem especially dimensionality space high 
effective tractable weak learning algorithm needed 
propose novel learning procedure called floatboost section bridge gap goal conventional boosting learning maximizing margin applications minimizing error rate incorporating floating search adaboost 
idea floating search originally proposed feature selection 
incorporation backtrack mechanism floating search boosting learning allows deletions weak classifiers ineffective terms error rate 
deletions backtrack performed error rate improvement classification error guaranteed 
leads strong classifier consisting fewer weak classifiers 
ieee published ieee computer society ieee transactions pattern analysis machine intelligence vol 
september formulate novel statistical model learning weak classifiers section 
stagewise approximation formulated estimate posterior probabilities effective features learned overcomplete feature set high dimensional feature space 
weak classifiers defined logarithm posterior ratio 
provides solution second problem discussed 
learning face detection floatboost learning algorithm applied face detection 
boundary face nonface patterns highly nonlinear face manifold due variations facial appearance lighting head pose expression highly complex 
learning approach far effective constructing face nonface classifiers 
see reader referred handbook face recognition subspace manifold modeling statistical learning face detection recognition aspects face recognition theories algorithms applications 
system viola jones successful application adaboost face detection earlier viola schneiderman 
adaboost adapted solve fundamental problems boosting procedure learning effective features large feature set constructing weak classifiers selected features boosting weak classifiers stronger classifier 
system real time frontal view face detector runs frames second image 
deals primarily frontal faces 
liu presents bayesian discriminating features bdf method 
input image harr wavelet representation amplitude projections concatenated expanded vector input dimensions 
assuming vectors follow single multivariate normal distribution face linear dimension reduction performed obtain pca modes 
likelihood density estimated pca residuals bayesian techniques 
nonface class modeled similarly 
classification decision face nonface density estimates 
bdf classifier reported achieve result compares favorably state art face detection algorithms schneiderman kanade method 
interesting note results achieved single gaussian face nonface bdf trained relatively small data sets feret face images natural nonface images trained classifier generalizes test images 
details needed understand underlying mechanism 
ability deal faces varying head poses termed multiview faces important real applications statistics show approximately percent faces home photos 
reasonable treatment multiview face detection recognition appearance framework view method difficulties explicit modeling avoided 
adopt viewbased representation face detection 
wiskott build elastic bunch graph templates multiview face detection recognition 
gong study trajectories faces linear pca feature spaces rotate kernel support vector machines svms face detection pose estimation 
huang svms estimate facial poses 
system schneiderman kanade multiresolution information different levels wavelet transform 
algorithm consists array face detectors view framework 
constructed statistics products histograms computed examples respective view 
takes minute image octaves candidate sizes reported 
great success achieved frontal view face detection engineering needed real time multiview face detection 
multiview face detection system section extension schneiderman kanade viola jones 
system applies floatboost algorithm learning face nonface classifiers uses coarse fine simple complex architecture called detector pyramid efficient computation detection multiview faces 
leads realtime multiview face detection system world 
runs ms image size pixels pentium iii cpu mhz 
experimental results section demonstrate floatboost learning face detection 
comparisons floatboost adaboost clearly show floatboost yields stronger classifier consists fewer weak classifiers adaboost achieves lower error rates 
effectiveness multiview face detection demonstrated 
floatboost learning section give brief review adaboost learning algorithm notion opposed original discrete adaboost :10.1.1.30.3515:10.1.1.30.3515:10.1.1.156.2440:10.1.1.156.2440:10.1.1.156.2440:10.1.1.32.8918
floatboost learning procedure 
xn yn yi class label associated example xi ir stronger classifier linear combination weak classifiers hm xm hm real version adaboost weak classifiers take real value hm ir absorbed coefficients needed discrete version hm case :10.1.1.30.3515:10.1.1.30.3515:10.1.1.156.2440:10.1.1.156.2440:10.1.1.156.2440
class label obtained sign hm magnitude indicates confidence 
training example associated weight approximates distribution samples 
learning process weights updated iteration way emphasis placed hard examples erroneously classified previously 
reweighting process important original 
revision schneiderman kanade reported improvement speed system coarse search strategy various heuristics reusing wavelet transform coefficients color preprocessing 
improved speed seconds image size pentium ii mhz 
li zhang floatboost learning statistical face detection fig 

algorithm 
adaboost 
noted studies artificial operation explicit reweighting unnecessary incorporated functional optimization procedure boosting :10.1.1.29.9093
error occurs hm 
margin example achieved hm ir training set examples defined 
considered measure confidence prediction hm 
upper bound classification error achieved hm derived exponential loss function hm xi yi pm hm xi adaboost constructs hm stagewise minimization :10.1.1.31.2869
current hm pm hm best hm new strong classifier hm hm hm leads minimum cost hm arg min hm minimizer derived hm log jx jx exp weight iteration labeled example jx ew jx ew stands mathematical expectation true zero :10.1.1.30.3515:10.1.1.30.3515:10.1.1.156.2440:10.1.1.156.2440:10.1.1.156.2440
jx defined similarly 
adaboost algorithm descriptions shown fig :10.1.1.30.3515:10.1.1.30.3515:10.1.1.156.2440:10.1.1.156.2440:10.1.1.156.2440

reweight formula step equivalent multiplicative rule original form adaboost :10.1.1.156.2440:10.1.1.156.2440:10.1.1.32.8918
section statistical model stagewise approximation jx 
floatboost performs backtrack latest weak classifier hm added adaboost 
backtrack deletes set learned weak classifiers mg hm help terms error rate order improve error rate 
idea backtrack originally floating search 
aimed dealing nonmonotonicity problem explained sequential feature selection 
known sequential forward selection sfs sequential backward selection sbs features added deleted improve performance step optimal 
assumption sequential selection strategy entire process monotonicity adding deleting feature leads improvement performance 
nonmonotonicity problem adding additional feature may lead drop performance way correct stages 
solutions proposed 
plus minus combines sfs sbs tackle problem features added deleted step backtrack performed fixed 
floating search procedure allows number backtracking steps controlled fixed 
specifically adds deletes feature backtracks steps depends current situation 
flexibility limitations due nonmonotonicity problem 
improvement quality selected features gained cost increased computation due extended search 
algorithm performs applications ieee transactions pattern analysis machine intelligence vol 
september fig 

floatboost procedure 
hm fh set best weak classifiers learned far hm error rate achieved hm pm hm weighted sum missing rate false alarm rate usually criterion detection problems min minimum error rate achieved far ensemble weak classifiers 

floating search developed allowing flexibility determination 
floatboost uses backtrack remove unfavorable weak classifiers existing classifiers order achieve low error rate 
floatboost procedure shown fig 

step forward inclusion selected weak classifiers best weak classifier added time adaboost 
step conditional exclusion floatboost removes significant weak classifier hm subject condition removal leads error rate lower min repeated removals done 
procedure terminates risk training set maximum number mmax reached 
conditional exclusion incorporated floatboost improves feature selection classifier learning 
results fewer weak classifiers adaboost achieve error rate 
deletions backtrack performed error rate lower error rate reduced feature set guaranteed 
learning weak classifiers optimal weak classifier stage derived 
expressed follows yjx xjy hm lm lm xjy log xjy log log likelihood ratio llr lm learned training examples classes 
threshold determined log ratio prior probabilities 
practice li zhang floatboost learning statistical face detection fig 

types simple haar wavelet features defined face example block consists pixels 
du dv du dv distances blocks 
number inside block weight pixels block 
feature takes value calculated weighted sum pixel values blocks window 
adjusted balance detection false alarm rates choose point roc curve 
learning optimal weak classifiers requires modeling llr 
estimating likelihood highdimensional data nontrivial task 
derive likelihood xjy overcomplete scalar feature set fz kg stagewise characteristics boosting learning approximate likelihood effective features learned stagewise 
specifically approximate xjy zm jy zm features selected previous stages feature selected 
set candidate features method constructing weak classifiers features described 
scalar feature transform dimensional face example size data space real line 
multiview face detection basic types scalar features shown fig 

face example size hundreds thousands different admissible du dv du dv values overcomplete feature set intrinsically low dimensional face pattern block difference features scalar values extensions steerable filters haar wavelets 
features computed efficiently summed area table integral image 
lienhart proposed extended set features dealing plane rotations 
optimal weak classifier associated single scalar feature construct best new weak classifier choose best corresponding feature 
feature selection weak classifier construction stagewise approximation xjy 
approximate xjy conditional distributions zm selected features approximation increasingly accurate grows 
note holds selected jy zm jw jy obtained entire history accounts dependencies zm 
xjy jy jy zm jy jy right hand side equation conditional densities fixed jy 
densities jy positive class negative class estimated histograms computed weighted voting labeled training examples weights 
lm optimal weak classifier estimated 
form approximation formula define component llr target lm lm log selected features zm log jy jy features selected 
target llr function approximated lm log xjy xjy xm lm xjy zm jy jy jy zm jy zm jy zm overcomplete basis set zm learned adaboost weakly dependent stagewise lm lm xm lm best feature corresponding best fits lm 
solution minimization problem ieee transactions pattern analysis machine intelligence vol 
september fig 

plane view partition 
plane head rotations row facial view labels row coarse fine view partitions levels detector pyramid rows 
arg min lm xi xi done steps follows find parallel xn lm lm lm xn amounts finding correlated lm data distribution set zm compute pn lm xi lk xi pn lk xi obtain strong classifier hm xm lm lk lm xm multiview face detection system lm mt face detection problem classify image standard size pixels face nonface 
essentially rest classification problem face nonface 
engineering solutions multiview face detection 
coarse fine view partition strategy leads detector pyramid architecture consisting levels coarse level top fine level bottom 
dealing head rotations system deals types head rotations ranges plane rotations range degrees plane rotations range nodding rotations approximately range 
detector pyramid constructed detect presence right faces certain range plane rotations plane rotations 
design detector pyramid described shortly 
plane rotations handled follows divide subranges 
apply images plane rotated original image 
effectively cover plane rotations 
nodding rotations dealt tolerances face detectors 
detector pyramid design detector pyramid adopts coarse simple complex top pyramid strategy generalizes cascade structure viola jones system suit multiview case 
coarse fine 
full range plane rotations partitioned increasingly narrower ranges face space divided increasingly smaller subspaces 
current implementation consists levels 
partitions outof plane rotation levels illustrated fig 

overlaps partitioned view subranges level face detector trained view usually cope faces extended view range 
detector pyramid architecture illustrated fig 
detection faces plane rotation 
plane rotations dealt applying detector pyramid images rotated discussed earlier 
shows detector pyramid levels 
dlv denotes detector detects faces view range pyramid level top coarsest classification faces range plane rotation 
bottom level finest classification 
current implementation partition fig 
consists detectors 
final result obtained merging subwindows pass channels bottom level 
schematically illustrated fig 

fig 

detector pyramid multiview face detection 
li zhang floatboost learning statistical face detection fig 

schematic illustration merging different channels 
left right outputs frontal left right view channels final result merge 
fig 

face nonface examples 
simple complex 
vast number subwindows result scan input image 
purpose efficiency crucial discard nonface subwindows possible earliest possible stage subwindows possible processed stages 
detectors early stages simpler reject vast number nonface subwindows quickly little computation stage complex spend time 
detector block pyramid consists cascade strong classifiers efficient classification idea viola jones 
summary system summarize construct detector pyramid multiview face detection full range plane rotation partitioned top level pyramid subranges lower levels 
detectors pyramid learned independently face examples corresponding view range bootstrapped nonface examples 
learning detector done follows 
set simple haar wavelet features candidate features 
tens thousands features window 

subset selected corresponding weak classifiers constructed floatboost learning 

strong classifier constructed linear combination weak ones 

detector composed strong classifiers cascade 
detector pyramid built learned detectors 
detailed specifications section 
experimental results experiments compare floatboost fb adaboost ab learning algorithms performances nonlinear classification face detection 
comparisons boosting learning classification single strong classifiers set experiments compares single strong classifiers learned fb ab algorithms classification performance 
cascade stronger classifiers needed achieve low false alarm rate face detection comparison effectiveness boosting learning algorithms system performance 
data set composed face nonface images size 
set frontal face images collected various sources 
faces cropped rescaled images size 
set nonface examples size collected images containing faces 
see fig 
random sample face nonface images 
examples set divided training set examples test set examples 
fig 

false alarm error rates fb ab algorithms frontal face training test sets function number weak classifiers 
fb ab strong classifier composed weak classifiers 
ieee transactions pattern analysis machine intelligence vol 
september performance measured false alarm fa error rate detection rate dr training set fixed percent 
fa curves training test sets algorithms shown fig 

curves number learned features weak classifiers floatboost achieves lower error rates adaboost training test data sets 
example test set fb false alarm rate ensemble weak classifiers opposed ab 
false alarm rate test set consistently lower ab training set 
fb needs fewer weak classifiers ab order achieve false alarm rate 
example ab needs weak classifiers achieve lowest fa rate test set fb needs achieve performance 
clearly demonstrate strength floatboost statistical learning achieve classification performance 
cascades strong classifiers set experiments compare classification performances cascades fb ab strong classifiers 
training face data section 
nonface images collected stagewise bootstrapping images containing faces 
evaluate trained compared cascade face detectors adaboost ab floatboost fb floatboost fb 
ab trained way achieves percent false alarm rate stage 
fb detection false alarm rates ab stage different numbers weak classifiers 
fb detection rate numbers weak classifiers ab different false alarm rate 
table comparison cascades ab fb classifiers table compares cascade classifiers terms number weak classifiers wcs stage total number wcs detection dr false alarm fa rates stage false alarm rate cascade 
ab fb classifiers trained target dr rates corresponding stages allow different fa rates numbers wc 
rates table training sets rates training sets error rates test sets generally higher 
comparing fb ab see floatboost method needs fewer wcs adaboost method achieve dr fa 
comparing fb ab see floatboost achieve lower fa adaboost number wcs dr demonstrate floatboost opposed adaboost 
comment follows table compares results cascades strong classifiers cascade composed strong classifiers stages obtained ab fb fb settings results training set 
differently fig 
compares single strong classifiers stage learned ab fb results training test data sets 
data sets face icons large images experiments meant compare ab fb learning algorithms 
performances resulting face detection systems including cascade strong classifiers postprocessing mergers performed large images 
comparisons ab fb systems experiments compare face detection systems performing large images icons table 
li zhang floatboost learning statistical face detection table comparison face detection rate mit cmu test set frontal faces training systems frontal face examples collected various sources faces subject slight plane rotation range 
aligned setting eye mouth coordinates fixed positions 
aligned face example synthesized face example generated random rotation range 
generates training set face examples 
images cropped rescaled size 
sufficient nonface examples bootstrapped images containing faces 
sets experiments performed mit cmu test set home brew test set 
mit cmu test set consists images containing faces 
data set detectors compared 
floatboost fb 
ab adaboost viola jones implemented training examples size 
ab adaboost training examples size reported 
cmu neural network system rowley baseline system 
table compares detection rates dr systems numbers weak classifiers wc boosting systems numbers false alarms fa 
thresholds fb ab ab classifiers adjusted numbers false alarms reported mit cmu test set 
results show fb achieve higher detection rate fa 
fb system needs fewer weak classifiers ab systems percent required ab percent ab 
fig 

comparison detection rates fb ab methods mit cmu test set 
table frontal face detection comparison home brew test set number features wcs target detection rate fixed fb generally achieved lower fa rates ab 
fig 
shows roc curves fb ab systems 
reader referred performances systems mit cmu data set 
second set system experiments systems built cascade detectors ab fb fb described section tuning tests performed home brew face image set 
set contains faces pictures taken outdoors complex backgrounds arbitrary illumination conditions 
comparison shown table 
fb achieve performance comparable ab fewer weak classifiers 
fb consisting number weak classifiers ab achieve higher detection rate lower false alarm rate 
confirm section 
fig 
shows fb detection results 
multiview faces section demonstrates multiview face detection system method section 
original face samples collected training covering plane rotation range 
total number multiview face images generated randomly shifting rotating original images small amount 
top level detector pyramid trained face examples view range 
second level face training set divided view groups corresponding subranges 
third level full range partitioned view groups 
system consists detectors need trained due symmetry face side view detectors second third levels constructed mirroring features side view detectors 
way number cascade detectors training time reduced level number view groups level 
cascade detectors trained independently 
believe bootstrapped nonface examples training detectors levels lead improvement 
cascades trained way toplevel detector consists cascade strong classifiers features respectively 
rejects percent retaining percent training faces 
second level detector cascade strong classifiers rejects percent passed top level retains percent train faces 
bottom level detector cascade ieee transactions pattern analysis machine intelligence vol 
september fig 

results frontal face detection obtained fb detector 
fig 

multiview face detection results 
li zhang floatboost learning statistical face detection strong classifiers detection rate percent false positive rate cmu profile face set test algorithm 
database downloaded ri cmu 
edu idb html face profile images index html 
data set consists images faces profile views restricted terms subject matter background scenery 
collected various news web sites 
results shown fig 

detector pyramid architecture effective speeding multiview face detection 
image pixels total subwindows classified 
full view detector top pyramid needs ms process subwindows rejects percent 
second level needs total ms process remaining subwindows 
third level needs ms process remaining subwindows 
total time detector pyramid processing ms small fraction subwindows processed third level increase computation full view range partitioned smaller intervals 
contrast view approach applying detectors cost ms system runs ms image size pixels pentium iii cpu mhz 
realtime multiview face detection system world 
face detection tracking demos research microsoft com demos mv html 
contribution summarized novel learning procedure floatboost proposed improve adaboost learning 
novel statistical model provided stagewise approximation needed learning weak classifiers 
floatboost learning algorithm applied face detection detector pyramid architecture efficient detection multiview faces 
incorporating idea floating search adaboost floatboost learning results strong classifier needs fewer weaker classifiers adaboost achieve similar error rate achieves lower error rate number weak classifiers :10.1.1.156.2440:10.1.1.156.2440:10.1.1.32.8918
real time multiview face detection achieved incorporating idea detector pyramid detectors learned floatboost 
performance improvement brought float boost achieved cost longer training time times longer floatboost classifiers evaluated 
methods training efficient 
example noticing examples large weight values influential friedman propose select examples large weights examples wrongly classified previously learned classifiers subsequent training :10.1.1.30.3515
top examples fraction total weight mass may 
currently cascade structure adopted face detector 
computational efficiency run time 
detection rate cascade detector approximately product individual detection rates resulting drop detection rate 
possible amendment cascade single strong classifier consisting long sequence weak classifiers 
detector exits rejecting subwindows possible 
learning classifier inherit sample weights learned previously starting fresh new weights cascade learning 
preliminary results show idea effective 
acknowledgments authors long zhu doing part experiments early development steve lin help proofreading final manuscript 
freund schapire decision theoretic generalization line learning application boosting computer system sciences vol :10.1.1.32.8918
pp 
aug 
valiant theory learnable comm 
acm vol 
pp 

kearns vazirani computational learning theory 
cambridge mass mit press 
schapire singer improved boosting algorithms confidence rated predictions proc :10.1.1.156.2440
th ann 
conf 
computational learning theory pp 

breiman arcing classifiers annals statistics vol 
pp 

schapire freund bartlett lee boosting margin new explanation effectiveness voting methods annals statistics vol :10.1.1.31.2869
pp 
oct 
friedman greedy function approximation gradient boosting machine annals statistics vol :10.1.1.29.9093
oct 
mason baxter bartlett frean functional gradient techniques combining hypotheses advances large margin classifiers smola bartlett scholkopf schuurmans eds pp 
cambridge mass mit press 
zemel gradient boosting algorithm regression problems advances neural information processing systems vol 

friedman hastie tibshirani additive logistic regression statistical view boosting annals statistics vol :10.1.1.30.3515
pp 
apr 
yu invited discussion additive logistic regression statistical view boosting friedman hastie tibshirani annals statistics vol 
pp 
apr 
kittler floating search methods feature selection pattern recognition letters vol 
pp 

li zhu zhang blake zhang shum statistical learning multi view face detection proc 
european conf 
computer vision vol 
pp 

li zhang 
shum zhang floatboost learning classification proc 
neural information processing systems dec 
bichsel pentland human face recognition face image set topology cvgip image understanding vol 
pp 

simard cun denker transformation invariance pattern recognition tangent distance tangent propagation neural networks tricks trade orr 
muller eds springer 
rowley baluja kanade neural network face detection ieee trans 
pattern analysis machine intelligence vol 
pp 
jan 

sung poggio example learning view human face detection ieee trans 
pattern analysis machine intelligence vol 
pp 
jan 
ieee transactions pattern analysis machine intelligence vol 
september osuna freund girosi training support vector machines application face detection computer vision pattern recognition pp 


yang roth ahuja snow face detector proc 
neural information processing systems pp 

handbook face recognition li jain eds 
springer verlag press 
viola jones robust real time object detection ieee iccv workshop statistical computational theories vision july 
viola jones rapid object detection boosted cascade simple features proc 
ieee cs conf 
computer vision pattern recognition dec 
viola boosting image retrieval proc 
ieee cs conf 
computer vision pattern recognition vol 
pp 

schneiderman statistical approach object detection applied faces cars cmu ri tr phd dissertation 
liu bayesian discriminating features method face detection ieee trans 
pattern analysis machine intelligence vol 
pp 
june 
moghaddam pentland probabilistic visual learning object representation ieee trans 
pattern analysis machine intelligence vol 
pp 
july 
pering freeze serra consumer multimedia organization retrieval system proc 
acm sig chi conf may 
pentland moghaddam starner view modular eigenspaces face recognition proc 
ieee cs conf 
computer vision pattern recognition pp 

fast accurate face detector indexation face images proc 
fourth ieee int conf 
automatic face gesture recognition 
wiskott kruger malsburg face recognition elastic bunch graph matching ieee trans 
pattern analysis machine intelligence vol 
pp 
july 
gong mckenna collins investigation face pose distribution proc 
ieee int conf 
face gesture recognition 
ng gong performing multi view face detection pose estimation composite support vector machine view sphere proc 
ieee int workshop recognition analysis tracking faces gestures real time systems pp 
sept 
li gong support vector regression classification multi view face detection recognition ieee int conf 
face gesture recognition pp 
mar 
huang shao wechsler face pose discrimination support vector machines svm proc 
int conf 
pattern recognition 
schneiderman kanade statistical method object detection applied faces cars proc 
ieee cs conf 
computer vision pattern recognition 
schneiderman kanade object detection statistics parts int computer vision vol 
pp 
feb 
stearns selecting features pattern classifiers proc 
int conf 
pattern recognition pp 

kittler feature set search algorithm pattern recognition practice chen ed north holland pp 

jain zongker feature selection evaluation application sample performance ieee trans 
pattern analysis machine intelligence vol 
pp 
feb 
adaptive floating search methods feature selection pattern recognition letters vol 
pp 

papageorgiou oren poggio general framework object detection proc 
ieee int conf 
computer vision pp 

simard bottou haffner cun fast convolution algorithm signal processing neural networks advances neural information processing systems kearns solla cohn eds vol 
mit press pp 

crow summed area tables texture mapping proc 
siggraph vol 
pp 

lienhart extended set haar features rapid object detection proc 
ieee int conf 
image processing vol 
pp 

amit geman wilder joint induction shape features tree classifiers ieee trans 
pattern analysis machine intelligence vol 
pp 

geman coarse fine face detection int computer vision vol 
pp 

erik face detection survey computer vision image understanding vol 
pp 
sept 
stan li received beng degree university meng degree national university defense technology phd degree surrey university worked research fellow 
degrees electrical electronic engineering researcher microsoft research asia beijing 
joined microsoft research china may post associate professor nanyang technological university singapore 
current research interest pattern recognition machine learning image analysis face technologies 
published books including markov random field modeling image analysis springer verlag second edition handbook face recognition editing anil jain springer verlag refereed papers book chapters areas 
senior member ieee currently serves editorial board pattern recognition program committees various international conferences 
zhang received bs degree department electrical engineering tsinghua university china ms degree electrical engineering chinese academy science 
currently phd candidate department electrical computer engineering university illinois urbana champaign 
research interests include computer vision machine learning multimodal human computer interaction 
information computing topic please visit digital library www computer org publications dlib 
