tree discretization continuous state space reinforcement learning reinforcement learning effective technique learning action policies discrete stochastic environments efficiency decay exponentially size state space 
situations significant portions large state space may irrelevant specific goal aggregated relevant states 
tree algorithm generates tree state discretization efficiently finds relevant state chunks large propositional domains 
extend tree algorithm challenging domains continuous state space initial discretization 
continuous tree algorithm transfers traditional regression tree techniques reinforcement learning 
performed experiments variety domains show continuous tree effectively handles large continuous state spaces 
report results different domains gives clear visualization algorithm empirically demonstrates effective state discretization simple multi agent environment 
reinforcement learning technique learning control policy agent moves world receives series rewards actions kaelbling littman moore 
number states need considered traditional reinforcement learning algorithm increases exponentially dimensionality state space 
dimensionality state space turn increases linearly example number agents world 
parts state space exact position state space irrelevant agent 
states region true equivalent replaced single state reducing state space explosion 
environments reduction significant 
concept starting states joining resolution known state aggregation 
techniques non uniform discretization referred variable resolution techniques moore 
parti game algorithm moore algorithm automatically generating variable resolution discretization continuous deterministic domain william uther manuela veloso computer science department carnegie mellon university pittsburgh pa uther veloso cs cmu edu observed data 
algorithm uses greedy local controller move state adjacent states discretization 
greedy controller fails resolution discretization increased state 
algorithm chapman kaelbling tree algorithm mccallum similar algorithms automatically generate variable resolution discretization re discretizing propositional state spaces 
policy new discretization traditional techniques 
parti game start world single state recursively split necessary 
continuous tree algorithm described extends algorithms continuous state spaces propositional state spaces 
scale reinforcement learning techniques standard approach substitute function approximator table states standard algorithm 
done naively lead convergence problems boyan moore 
gordon showed averaging system sub sample state space 
system interpolate sub sampled points extrapolate outside sampled range 
baird describes technique gradient descent function approximator table values 
continuous tree algorithm described viewed regression tree breiman quinlan store state values 
markov decision problems mdps similar reinforcement learning problems assume complete correct prior model world 
mdps solved state aggregation techniques dean lin 
describe continuous tree generalizing variable resolution reinforcement learning algorithm works continuous state spaces 
evaluate continuous tree domains small dimensional domain larger dimensional domain 
small domain robot travelling corridor shows main features algorithm 
larger domain hexagonal grid soccer similar littman investigate markov games 
ordered discrete strictly continuous 
increased size domain number states number actions state test generalization capabilities algorithm 
machine learning formulation reinforcement learning discrete set states actions agent detect current state state choose perform action turn move state 
state action resulting state triple reinforcement signal world assumed markovian 
state action pair probability distribution giving probability reaching particular successor state foundations reinforcement learning bellman equations bellman equations define function value function function function state action pairs expected sum discounted reward watkins dayan 
continuous tree tree mccallum includes method apply propositional decision tree techniques reinforcement learning 
introduce extension tree capable directly handling propositional domains 
resulting algorithm continuous tree uses decision tree learning techniques breiman quinlan find discretization continuous state space 
continuous tree different tree traditional reinforcement learning algorithms require prior discretization world separate states 
algorithm takes continuous ordered discrete state space automatically splits form discretization 
discrete state reinforcement learning algorithm discretization 
tree continuous tree distinct concepts state 
continuous tree type state fully continuous state world agent moving 
term sensory input 
second type state position discretization formed algorithm 
term state second sense 
state area sensory input space sensory input falls state 
previous algorithms concepts identical 
sensory input discrete different sensory input corresponded state 
translation concepts state terms state tree describes discretization 
node binary tree corresponds area sensory input space 
internal node decision attached describes way divide area sensor space 
decisions described terms attribute sensory input split value attribute 
sensory input relevant attribute falls stored value passed left right side tree respectively 
leaf nodes tree correspond states 
state tree enables generalization states areas sensory input space 
refer step agent takes world transition 
saved transition vector starting sensory input action performed resulting sensory input reward obtained transition values fully continuous 
agent forming discretization prior discretization aggregate data 
partial prior discretization exists utilized algorithm initializing state tree discretization required 
subset transitions need saved need saved full sensor accuracy 
deciding subset transitions saved discussed 
transition sensory input vector values 
gives datapoint triple sensory input action value value datapoint expected reward performing transition behaving optimally defined 
continuous tree forms discretization recursively growing state tree 
table summarizes algorithm 
nutshell algorithm follows initially world considered single state expected reward state tree single leaf node describing entire sensory input space 
algorithm loops phase process datapoints accumulated learning data gathering phase processing phase updates discretization 
data gathering phase algorithm behaves standard reinforcement learning algorithm added step state tree translate sensory input state 
bers transitions sees 
processing phase values datapoints re calculated 
significant difference defined distribution datapoint values state state split 
split adds new leaves state tree representing new states formed old state 
mdp defined new discretization solved find state values 
initializing state values values previous discretization incremental algorithm updating values relatively fast 
deciding execute pass processing phase parameter algorithm 
similar recursive partitioning decision regression tree values datapoints change data gathered mdp re solved processing phase 
splitting breadth depth 
value datapoint calculated modification bellman equations 
value resulting state transition recorded reward transition assign value corresponding datapoint data gathering phase pass current sensory input state tree find current state discretization 
values state choose action store transition datapoint optional update value function standard discrete reinforcement learning technique 
processing phase leaf update values datapoints leaf find best split point splitting criterion 
split point satisfies stopping criterion split leaf states 
calculate transition probabilities expected rewards recorded transitions solve mdp specified find table continuous tree algorithm having calculated datapoint values continuous tree tests vary systematically state finds best single decision divide state 
done decision tree techniques 
attribute sensory input considered turn 
datapoints sorted attribute 
algorithm loops sorted list trial split added consecutive pair datapoints 
split divides datapoints sets 
sets compared splitting criterion see returns single number describing different distributions 
trial split leads largest difference distributions remembered 
best split evaluated fixed stopping criterion see check difference significant 
having discretization problem reduced standard discrete reinforcement learning problem finding values new states done calculating state transition probabilities recorded tran expected rewards sitions solving mdp specified 
prioritized sweeping moore atkeson conjugate gradient descent sum squared bellman residuals baird solve mdp defined new discretization 
splitting criteria testing difference data distributions tried different splitting criteria 
nonparametric statistical test kolmogorov smirnov test 
second sum squared error 
splitting criterion mccallum original tree algorithm looked violations markov assumption 
distribution datapoint values new states different violation markov assumption information world available knowing agent old state 
splitting criterion test statistical similarity distributions datapoints side split 
kolmogorov smirnov non parametric test 
test difference cumulative distributions datasets 
illustrates cumulative distributions datasets 
arrow lines marks maximum difference cumulative probability distributions distance distribution approximately calculated independent identically distributed sets datapoints regardless distribution sets drawn 
assuming cumulative distributions dataset sizes respectively probability observed difference generated noise calculated equations table press 
smaller probability evidence true difference 
small pass stopping criterion split introduced 
table kolmogorov smirnov test equations investigated second squared error splitting criterion breiman 
aim approximate cumulative probability datapoint values cumulative probability distributions values transitions val ues states error measure fit sum squared error 
assuming values change values new states equal mean datapoints states 
mean squared error equals expected squared deviation mean variance 
splitting criterion weighted sum variances values transitions side split 
second justification splitting criterion finding williams baird reducing bellman residual value function increases performance 
splitting criteria sensitive performing tests separately action combining results 
mean squared error test weighted sum 
kolmogorov smirnov test multiply probabilities calculated 
importantly efficiency viewpoint tests done partly incrementally 
sum squared error keep track estimate variance time complexity series tests data leaf kolmogorov smirnov test datasets need sorted find cumulative probability distributions 
sorting performed trading space need loop find distributions test making total time stopping criteria gone far 
learning regression tree data standard technique grow tree recursively splitting data stopping criterion met 
obviously datapoints leaf value algorithm 
similarly way separate data example datapoints fall point sensory input space 
generally algorithm detect significant difference datasets side best split leaf 
word significant different meanings 
kolmogorov smirnov test significant mean statistically significant level 
stopping criterion squared error test reduction mean squared error 
difference variance entire dataset weighted mean variance datasets induced split threshold significant difference 
test theoretically test careful adjustment threshold produced reasonable results 
tree learning literature known stopping criteria weak find splits hidden low tree 
overly large trees produced trees pruned 
assuming little error introduced discretizing 
datapoint sampling save 
description algorithm simply noted transitions saved 
notably continuous tree need save transitions 
generalization capabilities allow learn non continuous trajectories state space 
recording transitions expensive terms memory record data processing time testing splits 
fewer transitions recorded harder detect differences leaf 
limiting number saved datapoints simply recorded fixed number datapoints state 
number transitions store set unclear transitions store 
experiments reported remembered transitions preset state limit reached 
room new transitions discarding random transition state 
advantage bias mislead splitting stopping criterion introduced 
corridor domain qualitatively test algorithm introduce simple domain simulating robot moving corridor 
domain state dimensions location temperature 
location ordered discrete attribute agent needs distinguish value represent correct value function 
temperature continuous divided qualitatively different regions cold normal hot 
robot division 
see continuous tree correctly autonomously finds qualitatively different temperature regions 
finds distance corridor needed full accuracy 
domain length corridor divided sections sections sections corridor different base temperature 
cooled normal temperature heated 
robot starts random position corridor rewarded reaches right hand corridor 
robot temperature sensitive limited temperature control board 
robot learn move correct corridor turn heater section turn section robot actions move change temperature control settings 
robot actions perform move forward backward unit heater 
robot moves nondeterministically function temperature 
robot temperature range successfully moves probability moves probability 
robot temperature base temperature section corridor adjusted robot temperature control unit 
experiments corridor units long 
sections units length respectively base temperature ranges respectively 
robot temperature control unit changed temperature robot differ corridor section base temperature heater 
temperatures temperature adjustments independently sampled uniform distribution range time step 
shows part state transition diagram domain 
robot moves high probability temperature range 
temperature timestep determined new location temperature control unit settings previous move 
means move needs set temperature unit correctly unit corridor current unit 
section corridor shown cooled section corridor transitions correspond moving right heater transitions correspond moving right heater 
axis robot temperature 
axis represents consecutive locations cool section corridor 
high probability transitions shown 
sample transitions part corridor domain shows discretization policy task steps world 
axis temperature robot scale axis location robot corridor scale splits policy axis robot temperature 
axis location corridor 
goal right hand edge 
policy move right point 
black areas indicate heater active 
white areas indicate active 
learnt discretization policy corridor task steps tests run corridor domain splitting criterion behaved similarly 
sum squared error testing significantly faster 
hexagonal soccer domain complex domain empirical evaluation continuous tree introduce hexagonal grid soccer simulation 
similar larger game framework littman test minimax learning 
test continuous tree having learn play reinforcement learning algorithm 
uther veloso compare number reinforcement learning algorithms domain 
prioritized sweeping best tradition reinforcement learning algorithms 
compare continuous tree prioritized sweeping 
consists field hexagonal grid cells players ball 
player cell occupied player 
ball center board controlled cell players 
gives total distinct states game 
players start fixed positions board shown 
game proceeds rounds 
round players simultaneous moves neighboring cell possible directions 
players specify direction move player attempts move edge grid remains cell 
player moves ball ball stays player stolen player 
cell contested winner decided non winner getting ball 
players score ball opponent goal 
ball arrives goal game ends 
player guarding goal loses game gets negative reward 
opponent receives equal magnitude positive reward 
possible score goal 
performed empirical comparisons different dimensions fast algorithm learn play level expertise algorithm learn play discounting reasonable initial learning period 
essentially points learning curve 
experiment pair algorithms played games 
wins recorded games second games 
games allowed measure learning speed agents started knowledge game 
second games gave indication level ability algorithm initial learning period 
game test repeated times 
results shown table number games mean standard deviation won algorithm 
percentage row level statistical significance difference 
table shows continuous tree performs better prioritized sweeping 
interestingly looking quite large trees generated continuous tree see managed learn concept opponent prioritized sweeping continuous tree significance games second games total table results prioritized sweeping vs continuous tree discussion continuous tree learns play world experience prioritized sweeping 
despite requiring data continuous tree algorithm slower real time prioritized sweeping algorithm domains tested 
domains data expensive time consuming gather trading algorithm speed efficient data important 
area continuous tree advantage traditional reinforcement learning algorithms 
corridor domain clear number details algorithm 
firstly algorithm successfully reduced continuous space discrete space states see 
algorithm managed find cutoff points reasonable accuracy try represent entire temperature range 
continuous tree perfect splits initially introduces splits refine discretization 
possible enhancement old splits reconsidered light new data optimize directly 
secondly algorithm divided length corridor possible 
temperature axis large numbers different datapoints values grouped state different corridor location different state 
location agent corridor directly relevant achieving goal 
state values fully discretized world change axis 
comparison values different points temperature range corridor location equal 
agent move time information needed 
ongoing attempting generalize cost move states addition value states 
simple form decision recorded continuous tree state tree divide space parallel axes input space 
complex types decision exist regression tree literature utgoff brodley simple decisions shown remarkably effective faster find complex decisions 
adding redundant attributes linear combinations primary attributes allow algorithm find diagonal splits 
tree continuous tree inherits decision tree background ability handle moderately high dimensional state spaces 
original tree capability partially remove markov assumption 
playing markov game implement part tree algorithm continuous tree reason done 
large state spaces affect learning speed exact location space relevant achieving goal 
described effective generalizing reinforcement learning algorithm continuous tree discretize continuous state space leaving equivalent areas single states 
generalizing algorithm performs significantly better non generalizing algorithms 
baird 
residual algorithms reinforcement learning function approximation 
prieditis russell eds machine learning proceedings twelfth international conference icml 
san mateo ca morgan kaufmann 
bellman 
dynamic programming 
princeton nj princeton university press 
boyan moore 
generalization reinforcement learning safely approximating value function 
tesauro touretzky leen eds advances neural information processing systems volume 
cambridge ma mit press 
breiman friedman olshen stone 
classification regression trees 
wadsworth statistics probability series 
monterey california wadsworth brooks cole advanced books software 
chapman kaelbling 
input generalization delayed reinforcement learning algorithm performance comparisons 
proceedings twelfth international joint conference artificial intelligence ijcai 
dean lin 

decomposition techniques planning stochastic domains 
proceedings fourteenth international joint conference artificial intelligence ijcai 
gordon 
online fitted reinforcement learning 
value function approximation workshop ml 
kaelbling littman moore 
reinforcement learning survey 
journal artificial intelligence research 
littman 
markov games framework multiagent reinforcement learning 
machine learning proceedings eleventh international conference icml 
san mateo ca morgan kaufmann 
mccallum 
reinforcement learning selective perception hidden state 
phd 
thesis department computer science university rochester rochester ny 
moore atkeson 
prioritized sweeping reinforcement learning data real time 
machine learning 
moore 
variable resolution dynamic programming efficiently learning action maps multivariate real values state spaces 
birnbaum collins eds machine learning proceedings eighth international workshop 
morgan kaufmann 
moore 
parti game algorithm variable resolution reinforcement learning multidimensional state spaces 
cowan tesauro alspector eds advances neural information processing systems 
morgan kaufmann 
press teukolsky vetterling flannery 
numerical recipes art scientific computing 
cambridge cambridge university press nd edition 
quinlan 
induction decision trees 
machine learning 
sutton 
temporal credit assignment reinforcement learning 
ph dissertation university massachusetts amherst 
utgoff brodley 
linear machine decision trees 
technical report university massachusetts 
uther veloso 
adversarial reinforcement learning 
journal artificial intelligence research 
submitted 
watkins dayan 
learning 
machine learning 
williams baird 
tight performance bounds greedy policies imperfect value functions 
technical report college computer science northeastern university 
