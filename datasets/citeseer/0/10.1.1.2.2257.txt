support vector machines text categorization latent semantic indexing text categorization tc important component information organization information management tasks 
key issues tc feature coding classifier design 
text categorization support vector machines svms approach latent semantic indexing lsi described 
latent semantic indexing method selecting informative subspaces feature spaces goal obtaining compact representation document 
support vector machines powerful machine learning systems combine remarkable performance elegant theoretical framework 
svms fits text categorization task due special properties text 
experiments show lsi svms frame improves clustering performance focusing attention support vector machines informative subspaces feature spaces 

information available internet growing interest assisting people manage huge amount information 
information routing filtering identification objectionable materials junk mail structured search browsing topic identification hot spots current information management 
assignment texts predefined categories content text categorization tc important component tasks 
key issues text categorization feature coding classifier design 
lot aspects done 
feature extraction basically method document coding automatically construct internal representations documents 
basic principles document coding firstly amenable interpretation classifier induction algorithms secondly compactly capture meaning document computationally flexible feasible 
feature selection defined yan huang electrical computer engineering department johns hopkins university huang jhu edu number original feature set number reduced optimal feature set 
higher results lower computational expense may curtail classification performance classifier 
appropriate coding scheme issue tc 
mutual information feature selection filtering approach effective feature selection methods widely tc 
latent semantic indexing lsi coding scheme extracts underlying semantic structure corpus determining significant statistical factors weighted word space 
advantages lie reducing dimensionality digging factors account higher order associations groups words may individual documents 
classifier design problem number statistical classification machine learning applied tc 
include multivariate regression models bayesian models nearest neighbor decision trees adaptive decision trees neural networks symbolic rule learning support vector machine learning :10.1.1.21.466
reported decent results 
due properties text svms fit tc task high dimension property 
svms overfitting protection handle large feature spaces 
sparseness property 
document turned sparse non zero items zeros 
proved theoretically practically svms suited dense sparse problems text categorization problems linearly separable part introduce approach text clustering vector machines document latent semantic indexing lsi 
latent semantic indexing 
svms method clustering 
experiments analysis 


latent semantic indexing latent semantic indexing adopts vector model semantics word occurrences 
assumption words tend occur tend occur similar words considered semantically similar 
documents treated semantically cohesive set words paragraphs articles newspapers newsgroup articles referred bag words model structure documents maintained 
build lsi model matrix representation training document created rows corresponding words vocabulary columns documents 
entry matrix weighted frequency corresponding term corresponding document 
weighting reduce influence frequently occurring terms function words 
details 
step reduce large sparse matrix compressed matrix singular value decomposition usv original matrix decomposed reduced rank term matrix diagonal matrix singular values document matrix row vector matrix column vector matrix projections word vectors document vectors singular value space 
words documents represented compact way compared original representation 
depending different tasks number selected singular value varies 
typical choices regular tasks 
examples word representations examples word representation 
svms method clustering support vector machines structure risk minimization principle computational theory 
basic idea find hypothesis guarantee lowest true error 
true error hypothesis probability error unseen randomly selected test example 
support vector machine method solution data overfitting problem neural networks 
problem occurs traditional backpropagation training gradient descent algorithms mechanism determine optimal point descending error gradient 
kinds svms choice 
polynomial svms clustering 

experiments results data sets data set wsj text data 
randomly select documents training set 
vocabulary frequent words 
second data set famous reuters text categorization collection available downloading www research att com lewis reuters html 
stories classified categories experiment 
splitting data stories training data classifier training stories testing data 
due time limitation frequent categories shown table selected experiment classifier 
categories consist training set 
category name num train num test earn acquisitions money fx grain crude table top categories number training test items experiment data set get word representation space note due time limit experiment 
ideal way extract word representation data set text data reuters text 
sparse matrix generated represent wsj training corpus column vector representation document original space 
detail coding method referred 
svd toolkit singular value decomposition 
word vocabulary represented dimension vector 
encode reuters training text text represented normalized summation words appearing document 
step train svm top categories described table 
simple polynomial svms provide generalization accuracy fast learn 
learning finished feature weights obtained svm corresponding top category 
parameters sigmoid function learned transform binary output svm probability 
testing phase encode test text way described fill svms obtained training procedure 
rank output probabilities svms choose biggest classification result 
results popular measures performance precision recall 
precision proportion items placed really category recall proportion items category category 
average precision recall breakeven point evaluate classification results 
average result measured micro scores equal weight 
considered document average average document category pairs 
shows initial experiments classification results linear svms earn acq money fx gain crude average top initial experiment results experiments include extending categories set done 
initial experiment top categories shows lsi svms performs tc task 

summary introduces support vector machines text categorization latent semantic indexing 
experiments show lsi effective coding scheme 
captures underlying content document semantic sense 
svms fit text task due properties text 
lsi svms shows promising scheme tc task 
due time limit experiments done 
direction include scheme solve amount hand labelled training data problem 

berry dumais computational methods intelligent information access proceedings supercomputing san diego ca december 
michael berry susan dumais gavin brien december 
published siam review pp 

burges tutorial support vector machines pattern recognition data mining knowledge discovery vol 
number luigi fabrizio sebastiani maria simi feature selection negative evidence automated text categorization yang pedersen comparative study feature text categorization proceedings fourteenth international conference machine learning icml pp 
ng goh low 
feature selection perception learning usability case study text categorization proceedings sigir 
gary noel extreme dimensionality reduction text learning cluster generated feature space ph thesis institute technology august 
yang chute example mapping method text categorization retrieval 
acm transactions information systems 
fuhr lustig tzeras air rule multi stage indexing system large subject fields 
processings riao 
schutze hull pedersen comparison classifiers document representations routing problem :10.1.1.21.466
sigir conference research development information retrieval 
yang evaluation statistical approaches text 
cmu technical report cmu cs april 

learning algorithms text categorization 
annual symposium document analysis information retrieval 
weiss maximizing text mining performance ieee intelligent systems july august 
wiener pedersen weigend neural network approach topic spotting 
processings fourth annual symposium document analysis information retrieval sdair 
hull pedersen comparison classifiers document representations routing problem 
sigir 
apte damerau weiss automated learning decision rules text categorization 
acm transactions information system 
warmuth auer 
perceptron algorithm vs window linear vs logarithmic mistake bounds input variables relevant 
conference learning theory cohen singer context learning methods text categorization sigir 
text categorization support vector machines learning relevant features 
proceedings th eurospeech conference machine learning ecml springer verlag 
bellegarda multi span language modeling framework large vocabulary speech recognition ieee trans 
audio press 
berry large scale sparse singular value computations int 

appl vol 
pp 

extreme dimensionality reduction text learning cluster generated feature spaces 
berry version user guide tech rep cs tennessee 
yang evaluation statistical approaches text categorization journal information retrieval vol pp 
