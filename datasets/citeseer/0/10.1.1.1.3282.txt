journal machine learning research submitted revised published boosting regularized path maximum margin classifier rosset ibm com data analytics research group ibm watson research center yorktown heights ny usa ji zhu umich edu department statistics university michigan ann arbor mi usa trevor hastie hastie stat stanford edu department statistics stanford university stanford ca usa editor robert schapire study boosting methods new perspective 
build efron show boosting approximately cases exactly minimizes loss criterion constraint coefficient vector 
helps understand success boosting early stopping regularized fitting loss criterion 
commonly criteria exponential binomial log likelihood show constraint relaxed equivalently boosting iterations proceed solution converges separable case optimal separating hyper plane 
prove optimal separating hyper plane property maximizing minimal margin training data defined boosting literature 
interesting fundamental similarity boosting kernel support vector machines emerges described methods regularized optimization high dimensional predictor space computational trick calculation practical converging margin maximizing solutions 
statement describes svms exactly applies boosting approximately 
keywords boosting regularized optimization support vector machines margin maximization 
outline boosting method iteratively building additive model ft th jt jt large assume finite dictionary candidate predictors weak learners jt basis function selected best candidate modify function stage model ft equivalently represented assigning coefficient dictionary rosset ji zhu trevor hastie 
rosset zhu hastie function selected jt ft jt representation allows interpret coefficient vector vector equivalently hyper plane normal 
interpretation play key role exposition 
examples common dictionaries training variables case leads additive model ft just linear model original data 
number dictionary functions dimension polynomial dictionary degree case number dictionary functions decision trees terminal nodes limit split points data points midway data points cart 
number possible trees bounded trivially np 
note regression trees fit framework give 
boosting idea introduced freund schapire adaboost algorithm 
adaboost boosting algorithms attracted lot attention due great success data modeling tasks mechanism analyzed perspectives 
friedman 
develop statistical perspective ultimately leads viewing adaboost gradient incremental search additive model specifically coordinate descent algorithm exponential loss function exp yf 
gradient boosting friedman anyboost mason generic algorithms approach generalize boosting idea wider families problems loss functions :10.1.1.126.8716
particular friedman 
pointed binomial log likelihood loss log exp yf natural loss classification robust outliers misspecified data 
different analysis boosting originating machine learning community concentrates effect boosting margins yif xi 
example schapire 
margin arguments prove convergence boosting perfect classification performance training data general conditions derive bounds generalization error unseen data 
combine approaches conclude gradient boosting described separable case approximate margin maximizing process 
view develop boosting approximate path optimal solutions regularized problems justifies early stopping boosting specifying value regularization parameter 
consider problem minimizing non negative convex loss functions particular exponential binomial log likelihood loss functions training data bound model coefficients arg min yi xi 
psfrag replacements boosting regularized path xi xi xi hj xi 
hastie 
chapter observed slow gradient boosting set small tends follow penalized path function mild conditions path 
words notation implies vanishes wide range values illustrates equivalence boosting optimal solution real life data set squared error loss loss function 
demonstrate equivalence formally coefficients lasso svi pgg gleason age lcp coefficients forward stagewise iteration svi pgg gleason exact coefficient paths left constrained squared error regression boosting coefficient paths right data prostate cancer study state conjecture 
progress proving conjecture efron 
prove weaker local result case squared error loss mild conditions optimal path 
generalize result general convex loss functions 
combining empirical theoretical evidence conclude boosting viewed approximate incremental method regularized path 
prove separable case exponential logistic log likelihood loss functions converges optimal separating hyper plane described arg max min yi xi 
words maximizes minimal margin vectors norm equal 
result generalizes easily lp norm constraints 
example describes optimal separating hyper plane euclidean sense non regularized support vector machine find 
combining main results get characterization boosting 
notation assumes minimum unique requires mild assumptions 
avoid notational complications slightly abusive notation 
appendix give explicit conditions uniqueness minimum 

margin maximizing hyper plane may unique show case limit defined maximizes second minimal margin 
see appendix details 
age lcp rosset zhu hastie boosting described gradient descent search approximately path constrained optimal solutions loss criterion converging separable case margin maximizer sense 
note boosting large dictionary particular guarantees data separable pathologies separability mild assumption 
case support vector machines high dimensional feature spaces non regularized optimal separating hyper plane usually theoretical interest typically represents fitted model 
want choose regularized model 
results indicate boosting gives natural method doing stopping early boosting process 
furthermore point fundamental similarity boosting svms approaches allow fit regularized models high dimensional predictor space computational trick 
differ regularization approach take exact regularization svms approximate regularization boosting computational trick facilitates fitting kernel trick svms coordinate descent boosting 
related schapire 
identified normalized margins distance normed separating hyper plane 
results relate boosting iterations success minimal margin combined model 
tsch 
take asymptotic analysis adaboost 
prove normalized minimal margin mini yi tht xi asymptotically equal classes 
words prove asymptotic separating hyper plane equally far away closest points side 
property margin maximizing separating hyper plane define 
papers illustrate margin maximizing effects adaboost experimentation 
short proving convergence optimal margin maximizing solutions 
motivated result tsch warmuth asserted margin maximizing properties adaboost different approach 
results relate asymptotic convergence infinitesimal adaboost compared analysis regularized path traced way variety boosting loss functions leads convergence result binomial log likelihood loss 
convergence boosting optimal solution loss function perspective analyzed papers 
tsch 
collins 
give results bounds convergence training set loss ic yi tht xi minimum 
separable case convergence loss inherently different convergence linear separator optimal separator 
solution separates classes perfectly drive exponential log likelihood loss simply scaling coefficients linearly 
papers connection boosting regularization slightly different context 
zhang suggests shrinkage version boosting converges regularized solutions zhang yu illustrate quantitative relationship early stopping boosting constraints 

boosting gradient descent boosting regularized path generic gradient boosting algorithms friedman mason attempt find linear combination members dictionary basis functions optimize loss function sample 
done searching iteration basis function gives steepest descent loss changing coefficient accordingly 
words coordinate descent algorithm assign dimension coordinate coefficient dictionary function 
assume data xi yi xi loss cost function set dictionary functions algorithms follow essential steps algorithm generic gradient boosting algorithm 
set 
fi xi current fit 
set wi yi fi fi identify jt argmax wih xi 
set jt jt wih jt xi jt 
current coefficient vector current step size 
notice wih jt xi ic yi fi jt mentioned algorithm interpreted simply coordinate descent algorithm weak learner space 
implementation details include dictionary weak learners loss function method searching optimal jt way determined 
example original adaboost algorithm uses scheme exponential loss exp yf implicit line search find best direction jt chosen see hastie mason 
dictionary adaboost formulation set candidate classifiers xi usually decision trees practice 
practical implementation boosting dictionaries boosting typically large practically infinite generic boosting algorithm implemented verbatim 
particular practical exhaustively search maximizer step 
approximate usually greedy search conducted find candidate weak learner jt order decline loss large maximal possible models 
common case dictionary weak learners comprised decision trees nodes way adaboost boosting algorithms solve stage building 
sign sign wih jt xi want loss reduced 
cases dictionary negation closed assumed wlog coefficients positive increasing rosset zhu hastie decision tree re weighted version data weights wi 
replace step minimization wi yi jt xi easily shown equivalent original step 
greedy building algorithm cart build node decision tree minimizes quantity achieves low weighted misclassification error weighted data 
tree built greedily split time global minimizer weighted misclassification error node decision trees 
fit re weighted data considered approximation optimal tree 
approximate optimization techniques critical strength boosting approach comes ability build additive models high dimensional predictor spaces 
spaces standard exact optimization techniques impractical approach requires calculation inversion hessian matrices completely question approaches require derivatives coordinate descent implemented approximately 
gradient boosting generic modeling tool friedman mason 
mention view boosting gradient descent allows devise boosting algorithms function estimation problem need appropriate loss appropriate dictionary weak learners 
example friedman 
suggested binomial log likelihood loss exponential loss adaboost binary classification resulting logitboost algorithm 
need limit boosting algorithms classification friedman applied methodology regression estimation squared error loss regression trees rosset segal applied density estimation log likelihood criterion bayesian networks weak learners 
experiments illustrate practical usefulness approach coordinate descent high dimensional predictor space carries classification supervised learning 
view coordinate descent boosting approximate regularized fitting offers insight approach general allows fit regularized models directly high dimensional predictor space 
bears conceptual similarity support vector machines exactly fit regularized model high dimensional predictor space 
loss functions commonly loss functions boosting classification models exponential minus binomial log likelihood exponential ce exp yf loglikelihood cl log exp yf 
loss functions bear important similarities 
friedman 
show population minimizer expected loss point similar loss functions boosting regularized path exponential logistic classification loss functions log ce exponential loss cl binomial loss 
importantly purpose simple proposition illustrates strong similarity loss functions positive margins correct classifications proposition yf ce cl ce 
words losses similar margins positive behave exponentials 
proof consider functions log 

conclude 
set exp get desired result 
negative margins behaviors ce cl different friedman 
noted 
particular cl robust outliers misspecified data 
line search boosting vs boosting mentioned adaboost determines line search 
notation algorithm argmin yi fi jt xi 
rosset zhu hastie alternative approach suggested friedman hastie 
shrink single small value 
may slow learning considerably depending small attractive theoretically order theory underlying gradient boosting implies weak learner chosen best increment locally 
argued approach stronger line search keep selecting jt repeatedly remains optimal boosting dominates line search boosting terms training error 
practice approach slowing learning rate usually performs better line search terms prediction error see friedman 
purposes assume infinitesimally small theoretical boosting algorithm results limit series boosting algorithms shrinking 
regression terminology line search version equivalent forward stage wise modeling infamous statistics literature greedy highly unstable see friedman 
intuitively obvious increasing coefficient saturates destroying signal may help select predictors 

lp margins support vector machines boosting introduce concept margins geometric interpretation binary classification model 
context boosting view offers different understanding adaboost gradient descent view 
sections connect views 
euclidean margin support vector machine consider classification model high dimensional predictor space say model separates training data xi yi sign xi yi geometrical perspective means hyper plane defined separating hyper plane data define euclidean margin min yif xi 
margin maximizing separating hyper plane data defined maximizes 
shows simple example separable data dimensions margin maximizing separating hyper plane 
euclidean margin maximizing separating hyperplane non regularized support vector machine solution 
margin maximizing properties play central role deriving generalization error bounds models form basis rich literature 
margin relation boosting considering euclidean margin define margin concept mp min yif xi 
particular interest case 
shows margin maximizing separating hyper plane simple example 
note fundamental difference boosting regularized path simple data example observations class observations class 
full line euclidean margin maximizing separating hyper plane 
margin maximizing separating hyper plane data set 
difference diagonal euclidean optimal separator vertical optimal separator illustrates sparsity effect optimal separation rosset zhu hastie solutions optimal separator diagonal optimal vertical 
understand relate margin definitions yf yf 
representation observe margin tend big ratio big 
ratio generally big sparse 
see consider fixing norm vector comparing norm candidates small components sparse large components zero components 
easy see second vector bigger norm margin vectors equal bigger margin 
different perspective difference optimal solutions theorem due mangasarian states lp margin maximizing separating hyper plane maximizes lq distance closest points separating hyper plane 
euclidean optimal separator maximizes euclidean distance points hyper plane optimal separator maximizes distance 
interesting result gives intuition optimal separating hyper planes tend coordinate oriented sparse representations projection considers largest coordinate distance coordinate distances may cost decreased distance 
schapire 
pointed relation adaboost margin 
prove case separable data boosting iterations increase boosting margin model defined min yif xi 
words margin model uses incremental representation geometric representation model 
representations give norm sign consistency monotonicity coefficient paths traced model iteration boosting algorithm jt sign sign jt 
see monotonicity condition play important role equivalence boosting regularization 
margin maximization view adaboost schapire 
plethora papers followed important analysis boosting algorithms distinct reasons gives intuitive geometric interpretation model adaboost looking model separates data margin sense 
note view boosting gradient descent loss criterion doesn really give kind intuition data separable model separates training data drive exponential binomial loss scaled yi xi 
boosting regularized path margin behavior classification model training data facilitates generation generalization prediction error bounds similar exist support vector machines schapire 
important quantity context margin normalized margin considers conjugate norm predictor vectors yi xi xi dictionary comprised classifiers xi margin exactly relevant quantity 
error bounds described schapire 
allow margin distribution just minimal margin 
boosting tendency separate sense central motivation results 
statistical perspective suspicious margin maximization method building prediction models high dimensional predictor space 
margin maximization high dimensional space lead fitting bad prediction performance 
observed practice authors particular breiman 
results sections suggest explanation model complexity margin maximization limit parametric regularized optimization models regularization vanishes regularized models path may superior margin maximizing limiting model terms prediction performance 
section return discuss issues detail 

boosting approximate incremental constrained fitting section introduce interpretation generic coordinate descent boosting algorithm tracking path approximate solutions constrained equivalently regularized versions loss criterion 
view serves understanding boosting particular connection early stopping boosting regularization 
view get result asymptotic margin maximization regularized classification models analogy classification boosting 
build ideas hastie 
chapter efron 

convex non negative loss criterion consider dimensional path optimal solutions constrained optimization problems training data arg min yi xi 
varies get traces dimensional optimal curve optimal solution non constrained problem exists finite norm obviously 
case separable class data ce cl optimal solution 
constrained solution different way building solution norm run boosting algorithm iterations 
give vector norm exactly norm geometric representation equal need monotonicity condition hold 
condition play key role exposition 
going argue solution paths similar small 
start observing similarity practice 
shows example lasso rosset zhu hastie example equivalence lasso optimal solution path left boosting squared error loss 
note equivalence breaks path variable non monotone similarity squared error loss fitting lasso penalty 
shows example mold taken efron 

data diabetes study dictionary just original variables 
panel left shows path optimal constrained solutions panel right shows boosting path dimensional dictionary total number boosting iterations 
dimensional path described coordinate curves corresponding variables 
interesting phenomenon observe coefficient traces completely identical 
agree point variable coefficient path non monotone violates point variable comes model see arrow right panel 
example illustrates monotonicity condition implication critical equivalence boosting constrained optimization 
examples seen far squared error loss ask equivalence stretches loss 
shows similar result time binomial log likelihood loss cl spam data set taken uci repository blake merz 
chose predictors plots interpretable computations accommodating 
see perfect equivalence exact constrained solution regularized logistic regression boosting case paths fully monotone 
justify observed equivalence surprising consider locally optimal monotone direction problem finding best monotone increment model min component wise 
values exact constrained solution boosting regularized path values stagewise exact coefficient paths left constrained logistic regression boosting coefficient paths right binomial log likelihood loss variables spam data set 
boosting path generated iterations 
shorthand ic yi xi 
order taylor expansion gives 
constraint increase easy see order optimal solution optimal solution coordinate descent step max assuming signs match sign sign 
get optimal solution monotonicity constraint happens monotone equivalent coordinate descent step 
reasonable expect optimal regularized path monotone figures infinitesimal boosting algorithm follow path solutions 
furthermore optimal path monotone formulation argue boosting tend follow approximate regularized path 
main difference boosting path true optimal path tend delay non monotone observe variable 
understand specific phenomenon require analysis true optimal path falls outside scope discussion efron 
cover subject squared error loss discussion applies continuously differentiable convex loss second order approximations 
employ understanding relationship boosting regularization construct lp boosting algorithms changing coordinate selection criterion coordinate descent algorithm 
get back point section design boosting algorithm 
experimental evidence heuristic discussion lead conjecture connects slow boosting regularized optimization rosset zhu hastie conjecture consider applying boosting algorithm convex loss function generating path solutions 
optimal coefficient paths monotone non decreasing range lim 
efron 
theorem prove weaker local result case squared error loss 
generalize result convex loss 
result prove global convergence conjecture claims empirical evidence implies 
sake brevity readability defer proof concise mathematical definition different types convergence appendix context real life boosting number basis functions usually large making small theory apply require running algorithm forever results considered directly applicable 
taken intuitive indication boosting especially version approximating optimal solutions constrained problems encounters way 

lp constrained classification loss functions having established relation boosting regularization going turn attention regularized optimization problem 
analogy results apply boosting 
concentrate ce cl classification losses defined solution paths lp constrained versions arg min yi xi 
ce cl discussed equation training data separable span values consequently 
may ask convergence points sequence 
theorem shows convergence points describe lp margin maximizing separating hyper planes 
theorem assume data separable yi xi 
ce cl convergence point corresponds lp margin maximizing separating hyper plane 
lp margin maximizing separating hyper plane unique unique convergence points lim arg max min yi xi 
proof proof applies ce cl property 
consider separating candidates 
assume separates better simple lemma min yi xi min yi xi 
boosting regularized path lemma exists incurs smaller loss words yi xi yi xi 
lemma prove convergence point lp margin maximizing separator 
assume convergence point denote minimal margin data data separable clearly loss converge 
assume bigger minimal margin continuity minimal margin exists open neighborhood min yi xi 
lemma get exists incurs smaller loss 
convergence point conclude convergence point sequence lp margin ing separator 
margin maximizing separator unique possible convergence point lim arg max min yi xi 
proof lemma definition ce get loss functions yi xi nexp 
separates better find desired logn log nexp exp 
definition ce write exp combining inequalities get desired result yi xi yi xi 
yi xi 
rosset zhu hastie conclude lp margin maximizing separating hyper plane unique normalized constrained solution converges 
case margin maximizing separating hyper plane unique fact prove stronger result indicates limit regularized solutions determined second smallest margin third 
result mainly technical interest prove appendix section 
implications theorem briefly discuss implications theorem boosting logistic regression 
boosting implications combined results section theorem indicates normalized boosting path ce cl loss approximately converges separating hyper plane attains max min yi xi max di signed euclidean distance training point separating hyper plane 
words maximizes euclidean distance scaled norm 
mentioned implies asymptotic boosting solution tend sparse representation due fact fixed norm norm vectors entries generally larger 
fact mild conditions asymptotic solution number observations non zero coefficients cl ce loss 
see appendix section proof 
logistic regression implications recall logistic regression maximum likelihood solution undefined data separable euclidean space spanned predictors 
theorem allows define logistic regression solution separable data follows 
set high constraint value cmax 
find cmax solution logistic regression problem subject constraint cmax 
problem convex differentiable interior point methods solve problem 

approximately lp margin maximizing solution data described cmax cmax solution original problem sense approximately convergence point normalized lp constrained solutions constraint relaxed 
boosting regularized path course result theorem probably sense simply find optimal separating hyper plane directly linear programming problem separation quadratic programming problem separation 
consider optimal separator logistic regression solution separable data 

examples apply boosting data sets interpret results light regularization margin maximization view 
spam data set know data separable boosting run forever approach optimal separator ce cl early data separable behavior loss functions may differ significantly ce weighs negative margins exponentially cl approximately linear margin large negative margins see friedman 
consequently expect ce concentrate hard training data particular non separable case 
illustrates behavior boosting minimal margin minimal margins exponential logistic adaboost test error test error exponential logistic adaboost behavior boosting loss functions spam data set loss functions adaboost spam data set predictors binary response 
node trees 
left plot shows minimal margin function norm coefficient vector 
binomial loss creates bigger minimal margin initially minimal margins loss functions converging asymptotically 
adaboost initially lags catches nicely reaches minimal margin asymptotically 
right plot shows test error iterations proceed illustrating methods fit eventually separation minimal margin improving 
adaboost significantly fit iterations allowed run obviously allowed run 
emphasize comparison adaboost boosting considers basis comparison norm number iterations 
terms computational complexity represented number iterations adaboost reaches large minimal mar rosset zhu hastie gin prediction performance quickly slow boosting approaches adaboost tends take larger steps 
simulated data educated comparison compelling visualization constructed example separation dimensional data th degree polynomial dictionary functions 
data consists observations class drawn mixture gaussians 
solid line optimal separator data dictionary easily calculated linear programming problem note difference optimal decision boundary section 
optimal separator non zero coefficients 
optimal boost iter boost iter artificial data set margin maximizing separator solid boosting models iterations dashed iterations dotted 
observe convergence boosting separator optimal separator ran boosting algorithm data set logistic log likelihood loss cl shows models generated iterations 
see models converge optimal separator 
different view convergence see measures convergence minimal margin left maximum value obtainable horizontal line norm distance normalized models right optimal separator norm boosting model iterations 
conclude simple artificial example get nice convergence model path margin maximizing separating hyper plane 
example illustrate similarity boosted path path optimal solutions discussed section 
minimal margin boosting regularized path measures convergence boosting model path optimal separator minimal margin left distance normalized boosting coefficient vector optimal model right norm norm norm norm comparison decision boundary boosting models broken optimal constrained solutions norm full shows class decision boundaries models generated boosting path compared optimal solutions constrained logistic regression problem bound norm coefficient vector 
observe clear similarities way solutions evolve converge optimal separator 
fact differ cases significantly surprising recall monotonicity condition section exact correspondence model paths 
case look coefficient paths difference rosset zhu hastie shown observe monotonicity condition consistently violated low norm ranges expect paths similar spirit identical 

discussion summarize learned boosting previous sections boosting approximately follows path regularized models loss criterion loss criterion exponential loss adaboost binomial log likelihood loss logistic regression regularized model converges margin maximizing separating hyper plane data separable span weak learners may ask points key success boosting approaches 
empirical clue answering question breiman programmed algorithm directly maximize margins 
results algorithm consistently got significantly higher minimal margins adaboost data sets fact higher margin distribution minimal margin slightly worse prediction performance 
margin maximization key adaboost success 
statistical perspective embrace reflecting importance regularization highdimensional predictor space 
results previous sections margin maximization viewed limit parametric regularized models regularization vanishes 
generally expect margin maximizing solutions perform worse regularized models 
case boosting regularization correspond early stopping boosting algorithm 
boosting svms regularized optimization high dimensional predictor spaces exposition led view boosting approximate way solve regularized optimization problem min yi xi converges loss ce cl general loss convex differentiable loss defined match problem domain 
support vector machines described solving regularized optimization problem see friedman chapter min yi xi converges non regularized support vector machine solution optimal euclidean separator denoted interesting connection exists approaches allow solve regularized optimization problem high dimensional predictor space 
argued margin maximizing models regularized sense minimize norm criterion separating models 
arguably property allows generalize reasonably cases 
boosting regularized path able solve regularized problem approximately high dimension boosting applying approximate coordinate descent trick building decision tree greedily selecting weak learner re weighted versions data 
support vector machines facilitate different trick solving regularized optimization problem high dimensional predictor space kernel trick 
dictionary spans reproducing kernel hilbert space rkhs theory tells find regularized solutions solving dimensional problem space spanned kernel xi 
fact means limited hinge loss applies convex loss 
concentrate discussion svm hinge loss far common known application result 
view boosting svm methods allow fit regularized models high dimensional predictor space computational shortcut 
complexity model built controlled regularization 
methods distinctly different traditional statistical approaches building models high dimension start reducing dimensionality problem standard tools newton method applied fitting concern 
merits regularization dimensionality reduction ridge regression lasso documented statistics computational issues impractical size problems typically solved boosting svm computational tricks 
believe difference may significant reason enduring success boosting svm data modeling working high dimension regularizing statistically preferable step procedure reducing dimension fitting model reduced space 
interesting consider differences approaches loss flexible vs hinge loss penalty vs type dictionary usually trees vs rkhs 
differences indicate approaches useful different situations 
example true model sparse representation chosen dictionary regularization may warranted form true model facilitates description class probabilities logistic linear model logistic loss cl best loss 
computational tricks svm boosting limit kind regularization fitting high dimensional space 
problems formulated solved different regularization approaches long dimensionality low support vector machines fitted penalty solving norm version svm problem equivalent replacing penalty penalty 
fact norm svm quite widely easily solved linear non rkhs situation linear program compared standard svm quadratic program tends give sparser solutions primal domain 
similarly describe approach developing boosting algorithm fitting approximate regularized models 
methods interesting potentially useful 
lack arguably attractive property standard boosting svm algorithms computational trick allow fitting high dimensions 
boosting algorithm rosset zhu hastie understanding relation boosting regularization theorem formulate lp boosting algorithms approximately follow path lp regularized solutions converge corresponding lp margin maximizing separating hyper planes 
particular interest case theorem implies constrained fitting cl ce build regularized path optimal separating hyper plane euclidean svm sense 
construct boosting algorithm consider equivalent optimization problem change step size constraint constraint 
easy see order solution problem entails selecting modification coordinate maximizes subject monotonicity lead correspondence locally optimal direction 
intuition construct boosting algorithm changing step generic boosting algorithm section identify jt maximizes wih jt xi jt note need consider current coefficient denominator algorithm appropriate toy examples 
situations dictionary weak learner prohibitively large need trick section allow approximate search optimizer step 
problem applying algorithm large problems choose dictionary function twice non coefficients 
due penalty current coefficient value affects rate penalty term increasing 
particular increasing causes penalty term increase rate order algorithm considering 
convergence boosting algorithm artificial data set section illustrated 
observe boosting models approach optimal separator 
interesting note significant difference optimal separator optimal separator section 

summary introduced new view boosting general class boosting particular comprised main points generalized results efron 
hastie 
describe boosting approximate regularized optimization 
shown exact regularized solutions converge margin maximizing separating hyper plane 
boosting regularized path optimal boost iter boost iter artificial data set margin maximizing separator solid boosting models iterations dashed iterations dotted 
observe convergence boosting separator optimal separator hope results help better understanding boosting works 
interesting challenging task separate effects different components boosting algorithm loss criterion dictionary greedy learning method line search slow learning relate success different scenarios 
implicit regularization boosting may contribute success shown situations regularization inherently superior see donoho 
important issue analyzing boosting fitting noisy data case 
deal fitting tsch 
propose regularization methods generalizations original adaboost algorithm achieve soft margin introducing slack variables 
results indicate models boosting path regarded regularized versions optimal separator regularization done directly naturally stopping boosting iterations early 
essentially choice constraint parameter questions arise view boosting 
issues considered similar separator view multi class boosting 
tentative results indicate case boosting problem formulated properly 
constrained optimization view boosting help producing generalization error bounds boosting tight current existing ones 
acknowledgments rosset zhu hastie stephen boyd brad efron jerry friedman robert schapire rob tibshirani helpful discussions 
referees thoughtful useful comments 
partially supported stanford graduate fellowship dms national science foundation roi ca national institutes health 
appendix local equivalence infinitesimal boosting constrained optimization assume set training data xn yn smooth cost function set basis functions hj 
denote optimal solution constrained optimization problem min yi xi subject 
suppose initialize boosting version algorithm described section run algorithm steps 
denote coefficients steps 
global convergence conjecture section implies mild assumptions 
proving global result show local result looking derivative 
proof builds proof efron 
theorem similar result case cost squared error loss theorem shows start boosting algorithm solution constrained optimization problem direction change boosting solution agree constrained optimization problem 
theorem assume optimal coefficient paths monotone coefficient paths monotone boosting proceeds proof introduce notations 

hj xn jth basis function evaluated training data 
xn vector current fit 
yn fn fn boosting regularized path current generalized residual vector defined friedman 
jr current correlation max set indices maximum absolute correlation 
clarity re write boosting algorithm starting special case algorithm follows initialize 
find jt argmax rt 
update update ft rt 
jt jt sign jt notice algorithm start 
proposed efron 
consider idealized boosting case 
monotone paths condition section section efron 
showed satisfy constraints ft rt constraint convex cone generated sign pj 
constraint equal correlation sign sign jv constraint true basis functions ac able catch terms sufficiently small pj non negative coefficient paths monotone 
second constraint seen taylor expansion quadratic term letting go zero applying result squared error loss efron 

constraints established notice vi yi xi ui 
rosset zhu hastie plug constraint constraint get set equations hap ha sign diag yi pj xi rank get back issue details appendix equivalently uniquely determined scale number 
consider constrained optimization problem 
fitted vector corresponding residual vector 
smooth define lim lim lemma monotone coefficient paths assumption satisfy constraints 
proof write coefficient 
constrained optimization problem equivalent corresponding lagrangian dual min yi xi subject 
yi xi lagrange multipliers 
differentiating lagrangian dual get solution needed satisfy karush kuhn tucker conditions boosting regularized path 
max 
see facts karush kuhn tucker conditions fact 
fact example suppose implies 
fact sign sign 
note non zero hold time 
possible happens finite number values basis enter model 
sufficiently small second derivative cost function finite stay 
change fitted vector jh sign sign coefficients change monotonically sign agree sign 

implies satisfies constraint 
claim satisfies constraint follows directly fact satisfy constraint 
completion proof theorem notice boosting case constrained optimization case pj definition monotone coefficient paths condition uniquely determined translate result notice 
efron 
showed defined elements give sufficient conditions true appendix ha hj xi rosset zhu hastie matrix assume rank 
theorem proved 
appendix uniqueness existence results 
appendix give details properties regularized solution paths 
section formulate prove sparseness uniqueness results regularized solutions convex loss 
section extend theorem section proved margin maximizing property limit lp regularized solutions regularization varies case margin maximizing solution unique 
sparseness uniqueness regularized solutions limits consider constrained optimization problem min yi xi 
section give sufficient conditions properties solutions 
existence sparse solution non zero coefficients 
non existence non sparse solutions non zero coefficients 
uniqueness solution 
convergence solutions sparse solution increases 
theorem assume unconstrained solution problem norm bigger exists solution non zero coefficients 
proof lemma appendix prove theorem karush kuhn tucker kkt formulation optimization problem 
chain rule differentiation gives ic yi xi jr defined appendix generalized residual vector 
simple relationship fact lemma write system equations nonzero coefficients optimal constrained solution follows denote set indices non zero coefficients sign 
boosting regularized path words get equations variables corresponding non zero column matrix ha length ha linearly independent columns rank ha assume optimal solution exists substituting th row get hl jh 
jh sign 
know sign meaning re phrase sign sign 
words get hl linear combination columns ha obey specific numeric relation 
construct alternative optimal solution non zero coefficient follows 
start 
define direction coefficient space implied sign sign 
move direction coefficient hits zero define min know 
set get xi xi get sign 
generates fit norm optimal solution non zero coefficient definition 
obviously apply process repeatedly get solution non zero coefficients 
theorem immediate implication rosset zhu hastie corollary set dictionary functions obeys equalities training data solution non zero coefficients 
corollary implies example basis functions come continuous nonredundant distribution means equality hold probability probability solution non zero coefficients 
theorem assume set dictionary functions obeys equalities training data 
addition assume 
loss function strictly convex squared error loss cl ce obviously qualify 
set dictionary functions size linearly dependent training data 
problem unique solution 
proof previous corollary tells solution non zero coefficients 
assume solutions 
strict convexity loss get convexity norm get 
solution 
total number variables non zero coefficients bigger non zero coefficients values contradicting corollary 
ignoring coefficients get represented dimensional maximum sub space leads contradiction assumption 
corollary consider sequence normalized solutions problem 
assume solutions non zero coefficients 
limit point sequence non zero coefficients 
proof trivial consequence convergence 
assume contradiction convergence point non zero coefficients 
argmin 
vector non zero coefficients know get contradiction convergence 
boosting regularized path uniqueness limiting solution theorem margin maximizing separator unique recall interested convergence points normalized regularized solutions theorem proves convergence point corresponds margin maximizing separating hyper plane 
extend case order separator unique extending result consider second smallest margin tie breaker 
show convergence point maximizes second smallest margin models maximal minimal margin 
ties second smallest margin limit point maximizes third smallest margin models remain 
noted minimal margin typically attained observation margin maximizing models 
case ties smallest margins smallest second smallest implies arbitrary tie breaking decision tied margins considered smallest second smallest consequence 
theorem assume data separable margin maximizing separating hyper correspond margin maximizing separating hyper plane maximizes second smallest margin 
plane defined unique 
convergence point proof proof essentially theorem 
outline 
theorem know need consider margin maximizing models limit points 
margin maximizing models lp norm bigger second smallest margin 
assume attains smallest margin observation attains smallest margin observation 
define min yih xi xi 
lemma theorem holds proof exactly ignore smallest margin observation model contribute amount combined loss 
convergence point 
know maximizes margin theorem 
assume maximizes margin bigger second smallest margin proceed exactly proof theorem considering observations model modified lemma conclude convergence point note smallest margin observation contributes loss models 
case smallest margins define unique solution continue list margins applying result recursively 
limit normalized lp regularized models maximizes margins just minimal margin 
case convergence point unique case order statistic optimal separator unique 
interesting research question investigate conditions scenario possible 
rosset zhu hastie blake merz 
repository machine learning databases 
www ics uci edu mlearn mlrepository html 
irvine ca university california department information computer science 
breiman 
prediction games arcing algorithms 
neural computation 
collins schapire singer 
logistic regression adaboost bregman distances 
computational theory pages 
donoho johnstone kerkyacharian picard 
wavelet shrinkage 
statist 
soc 

efron hastie johnstone tibshirani 
angle regression 
annals statistics 
freund schapire 
decision theoretic generalization line learning application boosting 
european conference computational learning theory pages 
friedman 
greedy function approximation gradient boosting machine 
annals statistics 
friedman hastie tibshirani 
additive logistic regression statistical view boosting 
annals statistics 
hastie tibshirani friedman 
elements statistical learning 
springer verlag new york 
mangasarian 
arbitrary norm separating plane 
operations research letters 
mason baxter bartlett frean 
boosting algorithms gradient descent 
neural information processing systems volume 
tsch mika warmuth 
convergence leveraging 
neurocolt technical report royal holloway college london 
tsch onoda 
ller 
soft margins adaboost 
machine learning march 
tsch warmuth 
efficient margin maximization boosting 
submitted jmlr december 
rosset segal 
boosting density estimation 
nips 
schapire freund bartlett lee 
boosting margin new explanation effectiveness voting methods 
annals statistics 
zhang 
sequential greedy approximation certain convex optimization problems 
ieee transactions information theory 
boosting regularized path zhang yu 
boosting early stopping convergence results 
report dept statistics univ california berkeley 

