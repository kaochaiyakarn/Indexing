fibring neural networks avila garcez dov gabbay dept computing city university london ec hb uk soi city ac uk dept computer science king college london wc ls uk dg dcs kcl ac uk neural symbolic systems hybrid systems integrate symbolic logic neural networks 
goal neural symbolic integration benefit combination features symbolic connectionist paradigms artificial intelligence 
introduces new neural network architecture idea fibring logical systems 
fibring allows combine different logical systems principled way 
fibred neural networks may composed interconnected neurons networks forming recursive architecture 
fibring function defines recursive architecture behave defining networks ensemble relate typically allowing activation neurons network influence change weights network 
intuitively seen training network time runs network show addition universal approximators standard feedforward networks fibred neural networks approximate polynomial function desired degree accuracy expressive standard feedforward networks 
keywords neural symbolic integration fibring systems recursion 
neural symbolic integration concerns symbolic knowledge neurocomputing paradigm artificial intelligence ai avila garcez gabbay 
goal benefit integration symbolic connectionist paradigms ai providing logical characterisation connectionist system connectionist massively parallel implementation logic hybrid system bringing features neural networks symbolic ai avila garcez gabbay holldobler 
efficient parallel distributed reasoning learning capabilities core neural symbolic system may argue ai system 
ultimately goal produce effective ai system added reasoning copyright american association artificial intelligence www aaai org 
rights reserved 
learning capabilities pointed valiant valiant key challenge computer science 
neural symbolic systems simple neural networks single hidden layer feedforward recurrent networks haykin typically manage represent reason propositional symbolic knowledge rules avila garcez gabbay fu pinkas towell shavlik 
hand neural symbolic systems capable representing reasoning expressive symbolic knowledge modal logic order logic normally capable learning new concepts efficiently holldobler sun alexandre shastri 
clearly need strike balance reasoning learning capabilities neural symbolic systems 
simple networks example efficient backpropagation learning algorithm applicable rumelhart hinton williams werbos shown represent languages expressive propositional logic complex connectionist systems capable representing order higher order logics example holldobler efficient learning algorithms developed 
necessary real world applications failure diagnosis engineering bioinformatics applications require languages expressive propositional logic 
bioinformatics particular requires ability represent reason relations order logic muggleton 
adopt approach extending simple networks backpropagation order allow higher expressive power 
gabbay fibring methodology gabbay different systems logical systems space time neural networks bayesian networks williamson gabbay avila garcez avila garcez lamb may put ordinated man ner solve particular problem 
know fundamental aspect symbolic computation lies ability implement recursion 
result neural networks behave logic need add recursion allowing networks composed interconnected neurons networks 
exemplifies network embedded recursively network 
course idea fibring organise networks number sub networks 
example hidden neuron network expected neural network network right 
input weights output network may depend activation state neuron known fibring function 
function may multiply weights network input potential neuron 

network network fibring neural networks implement recursion neural networks concentrated recurrent auto associative networks symmetric networks represent formal grammars elman touretzky hinton smolensky pollack 
general networks learn simulate number recursive rules similarity set examples question rules represented network treated secondary 
give different treatment subject looking neural symbolic integration perspective avila garcez gabbay 
idea able represent learn expressive symbolic rules rules containing embedded implication form example robot motion control system may require logic space logic time visual pattern recognition neural networks system 
encoded network encoded network fibred network represents follows introduce define fibred neural networks show addition universal approximators approximate polynomial function unbounded domain expressive standard feedforward networks 
briefly shown noting fibred neural networks compute function exactly input opposed feedforward networks restricted compact closed bounded domains hornik stinchcombe white 
intuitively fibring neural networks seen running training neural networks time 
example time run network perform kind learning network allow weights change fibring function 
words object level network running meta level network training occurring simultaneously system responsible added expressiveness system 
organised follows 
firstly introduce exemplify fibred neural networks 
define architecture dynamics fibred networks precisely show fibred networks approximate polynomials 
conclude discuss directions 
examples fibring main idea fibring neural networks allow single neurons behave entire embedded networks fibring function 
function qualifies function computed embedded network embedded network output depends 
example consider network embedded network network 
wa wb set weights network network respectively 
ia function computed network ib function computed network ia ib input vectors networks respectively 
network embedded neuron network fibring function function computed network gw ib wb output neuron output network example illustrates 
consider simple networks 
assume loss generality input output neurons identity activation function hidden neurons tanh activation function hornik stinchcombe white 
bipolar inputs ij outputs ok 
output network universal approximators feedforward neural networks approximate borel measurable function compact domain desired degree accuracy 
oc output network od 
network embedded network shown 
indicates input potential neuron influence fibring function 
refer input potential 
addition indicates output od influence example output 
suppose wd wd wd multiplies weights input potential oc od denote outputs networks respectively fibred 
od obtained applying wd calculating output network follows od 
oc obtaining od output neuron example oc od 
notice network trained changes weights time network running 
clearly fibred networks trained examples way standard feedforward networks example backpropagation rumelhart hinton williams 
networks example trained separately fibred 
network trained robot visual system network trained planning system 
simplicity assume defined fibring function remain unchanged 
extensions fibring neural networks consider task learning fibring functions 
notice addition different fibring functions networks fibred number different ways far architectures concerned 
networks example fibred embedding network input neuron network say input 
case outputs od oc od function wd say wd wd oc od 
consider simpler example illustrates power fibring neural networks 
consider networks single input neuron ia ib respectively single hidden neuron single output neuron oa ob respectively 
weights networks value identity activation function neurons including hidden neurons 
result simply oa ia ia ob ib ib weights network weights network assume embed network input neuron note particular example oc due identity activation function output layer 
network oc network fibring simple networks network obtain ob ib oa ob 
ob ib oa ob 
fibring function wa ia wb ia wb wb 
equal obtain ob ia ia ib oa ob 
means fix ib output network fibred network square input 
result sequence input fibred corresponding output sequence 
note input changes weights input changes weights back input changes weights input changes weights back 
interest sequence lies fact alternating inputs square input computed exactly network input illustrates important feature fibred neural networks ability approximate functions unbounded domain henderson hines 
results recursive characteristic fibred networks indicated fibring function discussed detail section 
note practice fibring function defined problem domain 
fibred neural networks section define fibred neural networks precisely define dynamics show fibring function changes weights embedded network reset weights back computation sequence 
approximate unbounded functions 
fibring definition sake simplicity restrict definition fibred networks feedforward networks single output neuron 
concentrate networks linear input linear output activation functions linear sigmoid hidden layer activation function 
believe principles fibring applied artificial neural network model 
follows allow networks number embedded networks nested fibred network 
allow unlimited number hidden layers network 
definition fibring function neural networks 
function called fibring function input potential neuron set weights definition fibred neural networks neural networks 
say embedded fibring function output neuron output network resulting network composed networks said fibred neural network 
note networks embedded single network networks nested network embedded network network embedded network 
resulting fibred network constructed applying definition recursively embed embed resulting network example consider identical network architectures containing single linear input neuron single linear hidden neuron single linear output neuron depicted 
denote weight input neuron hidden neuron network weight hidden neuron output neuron assume embed network output neuron network embed resulting network output neuron network definition shown 
denote fibring function denote fibring function usual define wb wc input potential neuron input potential neuron wb denotes weight vector wo ofb weight vector wo ofc initially wherea 
result input update weights network networks fibred input example produce output ob ax network particularly interesting consider fibring recurrent networks networks feedback connections 
oa ax network network embedded system input network fibring function update weights network parameter 
ax weights network changed ax ax output network networks illustrates computation polynomials 
computation odd degree polynomials negative coefficients achieved adding hidden layers networks see sequel 
nesting fibred networks fibring dynamics example illustrates dynamics fibred networks 
define dynamics precisely 
definition nested nn neural networks 
nn form nested fibred network ni embedded neuron ni fibring function say level network nj 
definition dynamics letn nn nested fibred network 
fibring function ni ni input vector network nj wj current weight vector nj ij input potential nj neuron nj nj embedded input vector ij output neuron nj ij function computed network nj wj ij standard way feedforward networks 
output oj network nj defined recursively terms output oj network nj asfollows wj ij wj oj oj ij ij oj denotes function computed nj substituting output neuron nj output network nj 
fibring expressiveness defined proceed show addition universal approximators approximate polynomial function expressive standard feedforward neural networks 
proposition fibred neural networks approximate borel measurable function compact domain desired degree accuracy universal approximators 
proof follows directly proof single hidden layer feedforward neural networks universal approximators hornik stinchcombe white observation level zero networks generalisation single hidden layer feedforward networks 
proposition fibred neural networks approximate polynomial function desired degree accuracy 
proof consider level zero network 
number input neurons ai embed networks input neurons level indicated networks network embedded neuron network network embedded neuron network embedded neuron embedded networks represent xn represents represents xn ensemble networks including contain linear neurons 
network nj represents xj contains input neurons allow representation aj hidden layers layer containing single hidden neuron number hj single output neuron 
addition aj weight input neuron andlet bethe weight connection nj 
need show nj computes definition input wj weights nj multiplied input produce output ajx neuron produce output ajx 
neuron hj produce output output neuron produce definition neuron recall differently functions compact domain polynomial functions bounded 
nj embedded activation ajx output addition straightforward see network completing proof compute aixi 
level level level aix level computing polynomials fibred networks introduced new neural network architecture named fibred neural networks combines number standard feedforward neural networks trained backpropagation fibring function 
shown addition universal approximators approximate polynomial function expressive standard feedforward neural networks 
question logics represented interesting open question 
step recursive expressive architecture perform symbolic computation giving neural symbolic characterisation 
expect able represent variables learn reason relational knowledge 
interesting pursue define recurrent neural networks fibred 
recurrent networks possess limited ability compute unbounded functions henderson 
comparison computational capabilities architectures highly desirable 
questions different networks fibred fibring functions important comes practical applications 
clearly domain dependent empirical evaluation comparison standard neural networks required 
acknowledgments grateful stefan useful discussions 
garcez partly supported foundation 
muggleton 
machine learning metabolic pathway descriptions probabilistic relational representation 
electronic transactions artificial intelligence 
mi 

artificial nonmonotonic neural networks 
artificial intelligence 
eds 

knowledge neurocomputing 
mit press 

approximation superposition sigmoidal functions 
mathematics control signals systems 

avila garcez lamb 
reasoning time knowledge neural symbolic learning systems 
thrun saul eds advances neural information processing systems proceedings nips conference 
vancouver canada mit press 
avila garcez gabbay 
symbolic knowledge extraction trained neural networks sound approach 
artificial intelligence 
avila garcez gabbay 
neural symbolic learning systems foundations applications 
perspectives neural computing 
springer verlag 
avila garcez 
gabbay fibring methodology bayesian neural networks 
laws models science european science foundation esf 
king college london 
elman 
finding structure time 
cognitive science 
fu 
neural networks computer intelligence 
mcgraw hill 
gabbay 
fibring logics 
oxford press 
haykin 
neural networks comprehensive foundation 
prentice hall 
henderson 
estimating probabilities unbounded categorization problems 
proceedings european symposium artificial neural networks 
hines 
logarithmic neural network architecture unbounded non linear function approximation 
proceedings ieee international conference neural networks 
holldobler 
connectionist inference system 
eds parallelization inference systems 
springer 
holldobler 
approximating semantics logic programs recurrent neural networks 
applied intelligence journal special issue neural networks structured knowledge 
hornik stinchcombe white 
multilayer feedforward networks universal approximators 
neural networks 

approximate match rules backpropagation neural networks 
machine learning 
pinkas 
reasoning nonmonotonicity learning connectionist networks capture propositional knowledge 
artificial intelligence 
pollack 
recursive distributed representations 
artificial intelligence 
rumelhart hinton williams 
learning internal representations error propagation 
rumelhart mcclelland eds parallel distributed processing explorations microstructure cognition volume 
mit press 

shastri 
advances neurally motivated model relational knowledge representation rapid inference temporal synchrony 
applied intelligence journal special issue neural networks structured knowledge 
smolensky 
tensor product variable binding representation symbolic structures connectionist networks 
artificial intelligence 
smolensky 
grammar connectionist approaches language 
cognitive science 
sun alexandre 
connectionist symbolic integration 
lawrence erlbaum associates 
touretzky hinton 
distributed connectionist production system 
cognitive science 
towell shavlik 
knowledge artificial neural networks 
artificial intelligence 
valiant 
problems computer science 
journal acm 
werbos 
backpropagation time mean 
proceedings ieee volume 
williamson gabbay 
recursive causality bayesian networks self fibring networks 
laws models science european science foundation esf 
king college london 
