machine learning fl kluwer academic publishers boston 
manufactured netherlands 
text classification labeled unlabeled documents em kamal nigam cs cmu edu andrew mccallum zy mccallum com sebastian thrun thrun cs cmu edu tom mitchell tom mitchell cmu edu school computer science carnegie mellon university pittsburgh pa just research henry street pittsburgh pa received march revised february editor william cohen 
shows accuracy learned text classifiers improved augmenting small number labeled training documents large pool unlabeled documents 
important text classification problems obtaining training labels expensive large quantities unlabeled documents readily available 
introduce algorithm learning labeled unlabeled documents combination expectation maximization em naive bayes classifier 
algorithm trains classifier available labeled documents probabilistically labels unlabeled documents 
trains new classifier labels documents iterates convergence 
basic em procedure works data conform generative assumptions model 
assumptions violated practice poor performance result 
extensions algorithm improve classification accuracy conditions weighting factor modulate contribution unlabeled data multiple mixture components class 
experimental results obtained text different real world tasks show unlabeled data reduces classification error 
keywords text classification expectation maximization integrating supervised unsupervised learning combining labeled unlabeled data bayesian learning 
consider problem automatically classifying text documents 
problem great practical importance massive volume online text available world wide web internet news feeds electronic mail corporate databases medical patient records digital libraries 
existing statistical text learning algorithms trained approximately classify documents sufficient set labeled training examples 
text classification algorithms automatically catalog news articles lewis gale joachims web pages craven dipasquo freitag mccallum mitchell nigam slattery shavlik rad automatically learn reading interests users pazzani muramatsu billsus lang automati nigam mccallum thrun mitchell cally sort electronic mail lewis knowles sahami dumais heckerman horvitz :10.1.1.11.6124:10.1.1.16.3103
key difficulty current algorithms issue addressed require large prohibitive number labeled training examples learn accurately 
labeling done person painfully time consuming process 
take example task learning usenet newsgroup articles interest particular person reading usenet news 
systems filter pre sort articles ones user finds interesting highly desirable great commercial interest today 
lang person read labeled articles learned classifier achieved precision making predictions top documents confident 
users practical system patience label articles especially obtain level precision 
obviously prefer algorithms provide accurate classifications hand labeling dozen articles thousands 
need large quantities data obtain high accuracy difficulty obtaining labeled data raises important question sources information reduce need labeled data 
addresses problem learning accurate text classifiers limited numbers labeled examples unlabeled documents augment available labeled documents 
text domains especially involving online sources collecting unlabeled documents easy inexpensive 
filtering task thousands unlabeled articles freely available usenet example 
labeling collecting documents expensive 
unlabeled data increase classification accuracy 
consideration inclined think gained access unlabeled data 
provide information joint probability distribution words 
suppose example labeled data determine documents containing word homework tend belong positive class 
fact estimate classification unlabeled documents find word lecture occurs frequently unlabeled examples believed belong positive class 
cooccurrence words homework lecture large set unlabeled training data provide useful information construct accurate classifier considers homework lecture indicators positive examples 
explain correlations helpful source information increasing classification rates specifically labeled data scarce 
uses expectation maximization em learn classifiers take advantage labeled unlabeled data 
em class iterative algorithms maximum likelihood maximum posteriori estimation problems incomplete data dempster laird rubin 
case unlabeled data considered incomplete come class labels 
algorithm trains classifier available labeled documents uses classifier assign probabilistically weighted class labels unlabeled document cal text classification labeled unlabeled documents em expectation missing class labels 
trains new classifier documents originally labeled unlabeled iterates 
maximum likelihood formulation em performs hill climbing data likelihood space finding classifier parameters locally maximize likelihood data labeled unlabeled 
combine em naive bayes classifier mixture multinomials commonly text classification 
propose augmentations basic em scheme 
order basic em improve classifier accuracy assumptions data generated satisfied 
assumptions data generated mixture model correspondence mixture components classes 
assumptions satisfied em may degrade improve classifier accuracy 
assumptions rarely hold realworld data propose extensions basic em naive bayes combination allow unlabeled data improve classification accuracy spite violated assumptions 
extension introduces weighting factor dynamically adjusts strength unlabeled data contribution parameter estimation em 
second reduces bias naive bayes modeling class multiple mixture components single component 
course experimental comparisons show unlabeled data significantly increase performance basic em algorithm suffer misfit modeling assumptions unlabeled data extension mentioned reduces effect problem improves classification 
reduction number labeled examples needed dramatic 
example identify source newsgroup usenet article classification accuracy traditional learner requires labeled examples alternatively algorithm takes advantage unlabeled examples requires labeled examples achieve accuracy 
task technique reduces need labeled training examples factor 
labeled documents class accuracy improved adding unlabeled data 
findings illustrate power unlabeled data text classification problems demonstrate strength algorithms proposed 
remainder organized follows 
section describes theoretical point view problem learning labeled unlabeled data 
sections formal framework naive bayes 
section combination em naive bayes extensions algorithm 
section describes systematic experimental comparison classification domains newsgroup articles web pages newswire articles 
domains multi class classification problems class relatively frequent 
third domain treated binary classification positive class having frequency depending task 
related discussed section 
advantages limitations research directions discussed section 
nigam mccallum thrun mitchell class class 
classification mixture gaussians 
unlimited amounts unlabeled data available mixture components fully recovered labeled data assign labels individual components converging exponentially quickly bayes optimal classifier 

argument value unlabeled data unlabeled data useful learning classification 
unlabeled data generally insufficient yield better random classification information class label castelli cover 
unlabeled data contain information joint distribution features class label 
sample labeled data significantly increase classification accuracy certain problem settings 
see consider simple classification problem instances generated gaussian mixture model 
data generated gaussian distributions class parameters unknown 
illustrates bayes optimal decision boundary classifies instances classes shown shaded unshaded areas 
note possible calculate bayes rule know gaussian mixture distribution parameters mean variance gaussian mixing parameter 
consider infinite amount unlabeled data available finite number labeled samples 
known unlabeled data generated mixture gaussians sufficient recover original mixture components mclachlan krishnan section 
impossible assign class labels gaussians labeled data 
remaining learning problem problem assigning class labels gaussians 
instance means variances mixture parameter learned unlabeled data 
labeled data determine gaussian belongs class 
problem known converge exponentially quickly number labeled samples castelli cover 
informally long labeled examples determine text classification labeled unlabeled documents em class component parameter estimation done unlabeled data 
important notice result depends critical assumption data generated parametric model classification certainly untrue real world domains text classification 
raises important empirical question extent unlabeled data useful practice spite violated assumptions 
sections address describing detail parametric generative model text classification presenting empirical results model real world data 

probabilistic framework section presents probabilistic framework characterizing nature documents classifiers 
framework defines probabilistic generative model data embodies assumptions generative process data produced mixture model correspondence mixture components classes 
naive bayes text classifier discuss falls framework example section 
setting document generated probability distribution defined set parameters denoted 
probability distribution consists mixture components fc jcj component parameterized disjoint subset 
document created selecting mixture component mixture weights class prior probabilities having selected mixture component generate document parameters distribution jc 
characterize likelihood document sum total probability mixture components jcj jc document class label 
assume correspondence mixture model components classes time indicate jth mixture component jth class 
class label particular document written document generated mixture component say class label may may known document 

text classification naive bayes section presents naive bayes known probabilistic classifier describes application text 
naive bayes foundation build order incorporate unlabeled data 
learning task section estimate parameters generative model labeled training data 
algorithm uses estimated param nigam mccallum thrun mitchell eters classify new documents calculating class generated document 

generative model naive bayes assumes particular probabilistic generative model text 
model specialization mixture model previous section assumptions discussed 
additionally naive bayes word independence assumptions allow generative model characterized greatly reduced number parameters 
rest subsection describes generative model formally giving precise specification model parameters deriving probability particular document generated class label equation 
introduce notation describe text 
document considered ordered list word events hw write word position document word vocabulary hw jv document generated particular mixture component document length jd chosen independently component 
note assumes document length independent class 
selected mixture component generates word sequence specified length 
furthermore assume generates word independently length 
expand second term equation express probability document mixture component terms constituent features document length words document 
note general setting probability word event conditioned words precede 
jc hw jd jd jd jc standard naive bayes assumption words document generated independently context independently words document class label 
assume probability word independent position document example probability seeing word homework position document seeing position 
express assumptions jc jc combining equations gives naive bayes expression probability document class jc jd jd jc text classification labeled unlabeled documents em parameters individual mixture component multinomial distribution words collection word probabilities written jc jc jc jv jg jc 
assume classes document length identically distributed need parameterized classification 
parameters model mixture weights class prior probabilities written indicate probabilities selecting different mixture components 
complete collection model parameters set multinomials prior probabilities multinomials jc cg 

training classifier learning naive bayes text classifier consists estimating parameters generative model set labeled training data fd jdj subsection derives method calculating estimates training data 
estimate written 
naive bayes uses maximum posteriori estimate finding arg max jd 
value probable evidence training data prior 
parameter estimation formulae result maximization familiar ratios empirical counts 
estimated probability word class jc simply number times word occurs training data class divided total number word occurrences training data class counts numerator denominator augmented pseudo counts word come prior distribution 
type prior referred laplace smoothing 
smoothing necessary prevent zero probabilities infrequently occurring words 
word probability estimates jc jc jc jdj jd jv jv jdj jd count number times word occurs document jd class label 
class prior probabilities estimated manner involve ratio counts smoothing jdj jd jcj jdj derivation ratios counts formulae comes directly maximum posteriori parameter estimation appealed deriving parameter estimation formulae em augmented em 
finding maximizes jd accomplished breaking expression terms bayes rule jd dj 
term calculated nigam mccallum thrun mitchell product document likelihoods equation 
second term prior distribution parameters represent dirichlet distribution gamma ff gamma jc ff gamma delta ff parameter effects strength prior constant greater zero 
set ff maximum posteriori estimation equivalent laplace smoothing 
expression maximized solving system partial derivatives log jd lagrange multipliers enforce constraint word probabilities class sum 
maximization yields ratio counts seen 

classifier estimates parameters calculated training documents equations possible turn generative model backwards calculate probability particular mixture component generated document 
derive application bayes rule substitutions equations jd jc jd jc jcj jd jc task classify test document single class class highest posterior probability arg max jd selected 

discussion note assumptions generation text documents mixture model correspondence mixture components classes word independence document length distribution violated real world text data 
documents mixtures multiple topics 
words document independent grammar topicality 
despite violations empirically naive bayes classifier job classifying text documents lewis ringuette craven yang pederson joachims mccallum rosenfeld mitchell ng :10.1.1.21.7950:10.1.1.35.6633:10.1.1.35.6633
observation explained part fact classification estimation function sign binary classification function estimation domingos pazzani friedman 
word independence assumption causes naive bayes give extreme class probability estimates 
estimates poor classification accuracy remains high 
formulation naive bayes uses generative model accounts number times word appears document 
multinomial lan text classification labeled unlabeled documents em guage modeling terms unigram model classifier mixture multinomials mccallum nigam :10.1.1.13.8629
formulation numerous practitioners naive bayes text classification lewis gale joachims li yamanishi mitchell mccallum lewis :10.1.1.21.7950:10.1.1.21.7950:10.1.1.16.3103:10.1.1.16.3103
formulation naive bayes text classification uses generative model document representation word vocabulary binary feature modeled mixture multi variate robertson sparck jones lewis larkey croft koller sahami 
empirical comparisons show multinomial formulation yields classifiers consistently higher accuracy mccallum nigam :10.1.1.13.8629

incorporating unlabeled data em proceed main topic unlabeled data improve text classifier 
naive bayes just small set labeled training data classification accuracy suffer variance parameter estimates generative model high 
augmenting small set large set unlabeled data combining sets em improve parameter estimates 
em class iterative algorithms maximum likelihood maximum posteriori estimation problems incomplete data dempster 
case unlabeled data considered incomplete come class labels 
applying em naive bayes quite straightforward 
naive bayes parameters estimated just labeled documents 
classifier assign probabilistically weighted class labels unlabeled document calculating expectations missing class labels jd 
new classifier parameters estimated documents originally newly labeled 
steps iterated change 
shown dempster 
iteration process guaranteed find model parameters equal higher likelihood previous iteration 
section describes em extensions probabilistic framework naive bayes text classification 

basic em set training documents task build classifier form previous section 
previously section assume subset documents come class labels rest documents subset class labels unknown 
disjoint partitioning section learning classifier approached calculating maximum posteriori estimate arg max dj 
consider second term maximization probability training data probability data simply product documents document nigam mccallum thrun mitchell ffl inputs collections labeled documents unlabeled documents 
ffl build initial naive bayes classifier labeled documents 
maximum posteriori parameter estimation find arg max dj see equations 
ffl loop classifier parameters improve measured change jd complete log probability labeled unlabeled data prior see equation ffl step current classifier estimate component membership unlabeled document probability mixture component class generated document jd see equation 
ffl step re estimate classifier estimated component membership document 
maximum posteriori parameter estimation find arg max dj see equations 
ffl output classifier takes unlabeled document predicts class label 
table 
basic em algorithm described section 
independent model 
unlabeled data probability individual document sum total probability classes equation 
labeled data generating component labels need refer mixture components just corresponding class 
probability data dj jcj jc theta jy trying maximize jd directly log jd step making maximization solving system partial derivatives tractable 
jd log dj 
equation write jd log log jcj jc log jy notice equation contains log sums unlabeled data maximization partial derivatives computationally intractable 
consider access class labels documents represented matrix binary indicator variables hz ij iff ij express complete log likelihood text classification labeled unlabeled documents em parameters jd log sums term inside sum non zero 
jd log jcj ij log jc replace ij expected value current model equation bounds incomplete log likelihood equation 
shown application jensen inequality log log 
result find locally maximum hill climbing procedure 
formalized expectation maximization em algorithm dempster 

iterative hill climbing procedure alternately recomputes expected value maximum posteriori parameters expected value 
note labeled documents known 
estimated unlabeled documents 
denote estimates iteration algorithm finds local maximum jd iterating steps ffl step set zjd 
ffl step set arg max jd 
practice step corresponds calculating probabilistic labels jd unlabeled documents current estimate parameters equation 
step maximizing complete likelihood equation corresponds calculating new maximum posteriori estimate parameters current estimates jd equations 
iteration process initialized priming step labeled documents estimate classifier parameters equations 
cycle begins step uses classifier probabilistically label unlabeled documents time 
algorithm iterates steps converges point change iteration 
algorithmically determine convergence occurred observing threshold change parameters equation height surface em hill climbing 
table gives outline basic em algorithm section 

discussion summary em finds locally maximizes likelihood parameters data labeled unlabeled 
provides method unlabeled data augment limited labeled data contribute parameter estimation 
interesting empirical question higher likelihood nigam mccallum thrun mitchell parameter estimates improve classification accuracy 
section discusses fact naive bayes usually performs classification despite violations assumptions 
em property 
note justifications approach depend assumptions stated section data produced mixture model correspondence mixture components classes 
assumptions hold certainly case real world textual data benefits unlabeled data clear 
experimental results section show method dramatically improve accuracy document classifier especially labeled documents 
data sets lot labeled lot unlabeled documents case 
experiments incorporation unlabeled data decreases increases classification accuracy 
describe changes basic em algorithm described aim address performance degradation due violated assumptions 

augmented em section describes extensions basic em algorithm described 
extensions help improve classification accuracy face somewhat violated assumptions generative model 
add new parameter modulate degree em weights unlabeled data second augment model relax assumptions generative model 

weighting unlabeled data 
described common scenario labeled documents hand orders magnitude unlabeled documents readily available 
case great majority data determining em parameter estimates comes unlabeled set 
circumstances think em entirely performing unsupervised clustering model positioning mixture components maximize likelihood unlabeled documents 
number labeled data small comparison unlabeled significant effect labeled data initialize classifier parameters determining em starting point hill climbing identify component class label 
mixture model assumptions true natural clusters data correspondence class labels unsupervised clustering unlabeled documents result mixture components useful classification section infinite amounts unlabeled data sufficient learn parameters mixture components 
mixture model assumptions true natural clustering unlabeled data may produce mixture components correspondence class labels detrimental classification accuracy 
effect particularly apparent number labeled documents large obtain text classification labeled unlabeled documents em reasonably parameter estimates classifier orders magnitude unlabeled documents overwhelm parameter estimation badly skew estimates 
subsection describes method influence unlabeled data modulated order control extent em performs unsupervised clustering 
introduce new parameter likelihood equation decreases contribution unlabeled documents parameter estimation 
term resulting method em 
em maximize equation maximize jd log jcj ij log jc jcj ij log jc notice close zero unlabeled documents little influence shape em hill climbing surface 
unlabeled document weighted labeled document algorithm original em previously described 
iterating maximize equation step performed exactly 
step different entails substitutes equations 
define weighting factor unlabeled set labeled set ae new estimate jc ratio word counts counts unlabeled documents decreased factor jc jc jdj jd jv jv jdj jd class prior probabilities modified similarly jdj jd jcj jd jd equations derived solving system partial derivatives lagrange multipliers enforce constraint probabilities sum 
select value maximizes leave crossvalidation classification accuracy labeled training data 
experimental results technique described section 
shown setting value result classification accuracy higher indicating value unlabeled data natural clustering result poor classification 
nigam mccallum thrun mitchell 
multiple mixture components class 
em technique described addresses violated mixture model assumptions reducing effect violated assumptions parameter estimation 
alternative approach attack problem head removing weakening restrictive assumption 
subsection takes exactly approach relaxing assumption correspondence mixture components classes 
replace restrictive assumption correspondence mixture components classes 
textual data corresponds saying class may comprised different sub topics best captured different word distribution 
furthermore multiple mixture components class capture dependencies words 
example consider sports class consisting documents hockey baseball 
documents words ice puck occur words bat base occur 
dependencies captured single multinomial distribution words sports class 
hand multiple mixture components class multinomial cover hockey sub topic baseball sub topic accurately capturing occurrence patterns words 
classes allow multiple multinomial mixture components 
note result missing values labeled unlabeled documents unknown mixture component covering label responsible generating particular labeled document 
parameter estimation performed em labeled document estimate mixture component document came 
introduce notation separating mixture components classes 
denote class corresponding mixture component write ath class topic continue denote jth mixture component 
write jc predetermined deterministic mapping mixture components classes 
parameter estimation done em 
step basic em building maximum posteriori parameter estimates multinomial component 
step unlabeled documents treated calculating probabilistically weighted mixture component membership jd 
labeled documents previous jd considered fixed class label allowed vary mixture components assigned document class 
algorithm calculates probabilistically weighted mixture component membership labeled documents 
note jd jc zero clamped zero rest normalized sum 
multiple mixture components class initialized randomly spreading labeled training data mixture components matching appropriate class label 
components initialized performing ran text classification labeled unlabeled documents em ffl inputs collections labeled documents unlabeled documents 
ffl weighted set discount factor unlabeled data cross validation see sections 
ffl multiple set number mixture components class cross validation see sections 
ffl multiple labeled document randomly assign jd mixture components correspond document class label initialize mixture component 
ffl build initial naive bayes classifier labeled documents 
maximum posteriori parameter estimation find arg max dj see equations 
ffl loop classifier parameters improve deltal jd change complete log probability labeled unlabeled data prior see equation ffl step current classifier estimate component membership document probability mixture component generated document jd see equation 
multiple restrict membership probability estimates labeled documents zero components associated classes renormalize 
ffl step re estimate classifier estimated component membership document 
maximum posteriori parameter estimation find arg max dj see equations 
weighted counting events parameter estimation word document counts unlabeled documents reduced factor see equations 
ffl output classifier takes unlabeled document predicts class label 
table 
algorithm described generate experimental results section 
algorithm enhancements em vary contribution unlabeled data section indicated weighted 
optional multiple mixture components class section indicated multiple 
unmarked paragraphs common variations algorithm 
step jd sampled uniform distribution mixture components jc 
multiple mixture components class classification matter probabilistically classifying documents mixture components summing mixture component probabilities class probabilities jd jc jd jc jcj jd jc select number mixture components class crossvalidation 
table gives outline em algorithm extensions previous section 
experimental results technique described section 
shown data naturally modeled single component class unlabeled data em degrades performance 
multiple nigam mccallum thrun mitchell mixture components class performance unlabeled data em superior naive bayes 

experimental results section provide empirical evidence combining labeled unlabeled training documents em outperforms traditional naive bayes trains labeled documents 
experimental results different text corpora usenet news articles newsgroups web pages webkb newswire articles reuters 
results show improvements accuracy due unlabeled data dramatic especially number labeled training documents low 
example newsgroups data set classification error reduced trained labeled unlabeled documents 
certain data sets especially number labeled documents high incorporation unlabeled data basic em scheme may reduce increase accuracy 
show application em extensions described previous section increases performance naive bayes 

datasets protocol newsgroups data set joachims mccallum mitchell collected ken lang consists articles divided evenly different usenet discussion groups :10.1.1.21.7950
task classify article newsgroup posted 
categories fall confusable clusters example comp discussion groups discuss religion 
words stoplist common short words removed unique words occur feature selection 
tokenizing data skip usenet headers discarding subject line tokens formed contiguous alphabetic characters left unstemmed 
word counts document scaled document constant length potentially fractional word counts 
preliminary experiments newsgroups indicated naive bayes classification better word count normalization 
newsgroups data set collected usenet postings period months 
naturally data time dependencies articles nearby time thread occasional quotations may contain words 
practical classifier data set asked classify articles trained articles past 
preserve scenario create test set documents selecting posting date articles newsgroup 
unlabeled set formed randomly selecting documents remaining 
labeled training sets formed partitioning remaining documents non overlapping sets 
sets created equal numbers text classification labeled unlabeled documents em documents class 
experiments different labeled set sizes create sets size obviously fewer sets possible experiments labeled sets containing documents 
non overlapping training set comprises new trial experiment 
results reported averages trials experiment 
webkb data set craven contains web pages gathered university computer science departments :10.1.1.35.6633:10.1.1.35.6633
collection includes entirety departments additionally assortment pages universities 
pages divided categories student faculty staff course project department 
populous non categories student faculty course project containing pages 
task classify web page appropriate categories 
consistency previous studies data set craven tokenizing webkb data numbers converted time phone number token appropriate sequence length token :10.1.1.35.6633:10.1.1.35.6633
stemming stoplist stoplist hurt performance 
example excellent indicator student homepage fourth ranked word information gain 
limit vocabulary informative words measured average mutual information class variable 
feature selection method commonly text yang pederson koller sahami joachims :10.1.1.21.7950
selected vocabulary size running leave cross validation training data optimize classification accuracy 
webkb data set collected part effort create crawler explores previously unseen computer science departments classifies web pages knowledge base ontology 
mimic crawler intended avoid reporting performance idiosyncrasies particular single department test leave university approach 
create test sets containing pages complete computer science departments 
test set unlabeled set pages formed randomly selecting remaining web pages 
non overlapping training sets formed method newsgroups 
results reported averages trials share number labeled training documents 
reuters distribution data set consists articles topic categories reuters newswire 
studies joachims liere tadepalli build binary classifiers populous classes identify news topic :10.1.1.11.6124
words inside text 
tags including title remove reuter tags occur top bottom document 
stoplist stem 
reuters classifiers different categories perform best widely varying vocabulary sizes chosen average mutual information class variable 
variance optimal vocabulary size unsurprising 
previously noted joachims categories wheat corn known strong correspondence small set words title words cate nigam mccallum thrun mitchell gories categories acq known complex characteristics :10.1.1.21.7950
categories narrow definitions attain best classification small vocabularies broader definition require large vocabulary 
vocabulary size reuters trial selected optimizing accuracy measured cross validation labeled training set 
newsgroups data set time dependencies reuters 
standard modapte train test split divides articles time documents form test set earlier available training 
experiments documents training set randomly selected form unlabeled set 
remaining training documents randomly select non overlapping training sets positively labeled documents negatively labeled documents previously described data sets 
non uniform number labelings classes negative class frequent positive class binary reuters classification tasks 
results reuters reported precision recall breakeven points standard information retrieval measure binary classification 
accuracy performance metric high accuracy achieved predicting negative class 
task data set classification filtering find positive examples large sea negative examples 
recall precision capture inherent duality task defined recall correct positive predictions positive examples precision correct positive predictions positive predictions classifier achieve trade precision recall adjusting decision boundary positive negative class away previous default jd 
precision recall breakeven point defined precision recall value equal joachims :10.1.1.11.6124
algorithm experiments em described table 
section leave cross validation performed conjunction em simplification computational efficiency 
run em convergence training data subtract word counts labeled document turn testing document 
performing cross validation specific combination parameter settings run em required run em labeled example 
note residual effects held document 
computational complexity em prohibitive 
iteration requires classifying training documents step building new classifier step 
experiments em usually converges iterations 
wall clock time read document word matrix disk build em model iterating convergence classify test documents minute text classification labeled unlabeled documents em number labeled documents unlabeled documents unlabeled documents 
classification accuracy newsgroups data set unlabeled documents 
small amounts training data em yields accurate classifiers 
large amounts labeled training data accurate parameter estimates obtained unlabeled data methods converge 
webkb data set minutes newsgroups 
newsgroups data set takes longer documents words vocabulary 

em unlabeled data increases accuracy consider basic em incorporate information unlabeled documents 
shows effect basic em unlabeled data newsgroups data set 
vertical axis indicates average classifier accuracy test sets horizontal axis indicates amount labeled training data log scale 
vary amount labeled training data compare classification accuracy traditional naive bayes unlabeled data em learner access unlabeled documents 
em performs significantly better 
example labeled documents documents class naive bayes reaches accuracy em achieves 
represents reduction classification error 
note em performs small number labeled documents documents single labeled document class naive bayes obtains em 
expected lot labeled data naive bayes learning curve close plateau having unlabeled data help nearly labeled data accurately estimate classifier parameters 
labeled documents class classification accuracy increases 
results statistically significant 
results demonstrate em finds parameter estimates improve classification accuracy reduce need labeled training examples 
example reach classification accuracy naive bayes requires labeled examples em requires labeled examples achieve accuracy 
nigam mccallum thrun mitchell number unlabeled documents labeled documents labeled documents labeled documents labeled documents labeled documents 
classification accuracy varying number unlabeled documents 
effect shown newsgroups data set different amounts labeled documents varying amount unlabeled data horizontal axis 
having unlabeled data helps 
note dip accuracy small amount unlabeled data added small amount labeled data 
hypothesize caused extreme estimates component membership jd unlabeled documents caused naive bayes word independence assumption 
consider effect varying amount unlabeled data 
different quantities labeled documents hold number labeled documents constant vary number unlabeled documents horizontal axis 
naturally having unlabeled data helps helps labeled data 
notice adding small amount unlabeled data small amount labeled data hurts performance 
hypothesize occurs word independence assumption naive bayes leads overly confident jd estimates step small amount unlabeled data distributed sharply 
bias naive bayes step spread unlabeled data evenly classes 
number unlabeled documents large problem disappears unlabeled set provides large sample smooth sharp discreteness naive bayes overly confident classification 
move different data set 
provide intuition em works detailed trace example webkb data set 
table shows evolution classifier course em iterations 
column shows ordered list words model indicates predictive course class 
words judged predictive weighted log likelihood ratio 
symbol indicates arbitrary digit 
iteration parameters estimated randomly chosen single labeled document class 
notice course document specific artificial intelligence course dartmouth 
em iterations unlabeled documents see em unlabeled data find words generally text classification labeled unlabeled documents em table 
lists words predictive course class webkb data set change iterations em specific trial 
second iteration em common course related words appear 
symbol indicates arbitrary digit 
iteration iteration iteration intelligence dd dd dd artificial lecture lecture understanding cc cc dd dd dist dd dd due identical handout rus due homework arrange problem assignment games set handout dartmouth tay set natural hw cognitive exam logic homework problem proving kfoury prolog sec postscript knowledge postscript solution human exam quiz representation solution chapter field ascii indicative courses 
classifier corresponding column achieves accuracy em converges classifier achieves accuracy 

varying weight unlabeled data graphing performance data set see incorporation unlabeled data decrease increase classification accuracy 
graph shows performance basic em unlabeled documents webkb 
em improves accuracy significantly amount labeled data small 
labeled documents class traditional naive bayes attains accuracy em reaches 
lot labeled data em hurts performance slightly 
labeled documents naive bayes obtains accuracy em worse 
differences performance statistically significant university test sets respectively 
discussed section hypothesize em hurts performance data fit assumptions generative model mixture components best explain unlabeled data precise correspondence class labels 
surprising unlabeled data throw parameter estimation considers number unlabeled documents greater number labeled documents versus points largest amounts labeled nigam mccallum thrun mitchell number labeled documents unlabeled documents unlabeled documents 
classification accuracy webkb data set unlabeled documents 
small numbers labeled documents em improves accuracy 
labeled documents em degrades performance slightly indicating misfit data assumed generative model 
data great majority probability mass step estimate classifier parameters comes unlabeled data 
remedy dip performance em reduce weight unlabeled data varying equations 
plots classification accuracy varying achieve relative weighting indicated horizontal axis different amounts labeled training data 
bottom curve obtained labeled documents vertical slice point em unlabeled data gives higher accuracy naive bayes 
best weighting unlabeled data high indicating classification improved augmenting sparse labeled data heavy reliance unlabeled data 
middle curve obtained labeled documents slice near point em naive bayes performance cross 
best weighting middle indicating em performs better naive bayes basic em 
top curve obtained labeled documents slice unweighted em performance lower traditional naive bayes 
weight unlabeled data point 
note inverse relationship labeled data set size best weighting factor smaller labeled data set larger best weighting unlabeled data 
trend holds amounts labeled data 
intuitively em little labeled training data parameter estimation desperate guidance em unlabeled data helps spite somewhat violated assumptions 
labeled training data sufficiently estimate parameters weight unlabeled data 
note best performing values extremes remembering right point corresponds em weighting generate left regular naive bayes 
paired tests trials test universities show points curves statistically significantly higher text classification labeled unlabeled documents em weight unlabeled data labeled documents labeled documents labeled documents 
effects varying weighting factor unlabeled data em 
curves webkb data set correspond different amounts labeled data 
labeled data accuracy highest weight unlabeled data 
amount labeled data large accurate parameter estimates attainable labeled data unlabeled data receive weight 
moderate amounts labeled data accuracy better middle extreme 
note magnified vertical scale 
point difference maxima basic em labeled documents 
practice value tuning parameter selected cross validation 
experiments select leave cross validation labeled training set trial discussed section 
shows accuracy best possible accuracy selecting cross validation 
basic em naive bayes accuracies shown comparison 
perfectly selected accuracy dominates basic em naive bayes curves 
crossvalidation selects small amounts labeled documents perform em 
large amounts labeled documents cross validation selects suffer degraded performance seen basic em performs naive bayes 
example document level seen picked cross validation gives weight unlabeled data basic em 
doing provides accuracy compared naive bayes basic em 
statistically significantly different naive bayes statistically significantly higher basic em test sets 
results indicate automatically avoid em degradation accuracy large training set sizes preserve benefits em seen small labeled training sets 
results indicate training set size small improved methods selecting significantly increase practical performance em 
note cases cross validation documents choose section suggests methods may perform better cross validation 
nigam mccallum thrun mitchell number labeled documents unlabeled documents best em lambda unlabeled documents cv em lambda unlabeled documents basic em unlabeled documents 
classification accuracy webkb data set modulation unlabeled data weighting factor top curve shows accuracy best value second curve chosen cross validation 
small amounts labeled data results similar basic em large amounts labeled data results accurate basic em 
weighting factor large amounts unlabeled data longer degrades accuracy algorithm retains large improvements small amounts labeled data 
note magnified vertical axis facilitate comparisons 

multiple mixture components class faced data fit assumptions model tuning approach described addresses problem allowing model incrementally ignore unlabeled data 
direct approach described section change model naturally fits data 
flexibility added mapping mixture components class labels allowing multiple mixture components class 
expect improve performance data class fact multi modal 
eye testing hypothesis apply em reuters corpus 
documents data set multiple class labels category traditionally evaluated binary classifier 
negative class covers distinct categories expect task strongly violate assumption data negative class generated single mixture component 
reason model positive class single mixture component negative class mixture components unlabeled data 
table contains summary results test set modeling negative class multiple mixture components 
nb column shows precision recall breakeven points standard naive bayes just labeled data models negative class single mixture component 
nb column shows results modeling negative class multiple mixture components just labeled data 
nb column number components selected optimize best precision recall breakeven point 
median number components selected trials indicated parenthesis text classification labeled unlabeled documents em table 
precision recall breakeven points showing performance binary classifiers reuters traditional naive bayes nb multiple mixture components just labeled data nb basic em em labeled unlabeled data multiple mixture components em labeled unlabeled data em 
nb em number components selected optimally trial median number components trials negative class shown parentheses 
note multi component model natural reuters negative class consists topics 
unlabeled data multiple mixture components class increases performance naive bayes 
category nb nb em em em vs nb em vs nb acq corn crude earn grain interest money fx ship trade wheat breakeven point 
note consider effect unlabeled data complex representation data improves performance traditional naive bayes 
column labeled em shows results basic em single negative component 
notice performance worse naive bayes nb 
hypothesize negative class truly multi modal fitting single naive bayes class em data accurately capture negative class word distribution 
column labeled em shows results em multiple mixture components selecting best number components 
performance better nb traditional naive bayes nb naive bayes multiple mixture components class 
increase measured trials reuters statistically significant 
indicates multiple mixture components increases performance traditional naive bayes combination unlabeled data multiple mixture components increases performance 
furthermore interesting note average em uses mixture components nb suggesting addition unlabeled data reduces variance supports expressive model 
tables show complete results experiments multiple mixture components unlabeled data respectively 
note general mixture components hurts performance 
components assumptions overly restrictive 
components parameters estimate amount data 
table shows results table classification accuracy breakeven 
general trends accuracy 
accuracy optimal number mixture components negative class greater precision recall nature precision nigam mccallum thrun mitchell table 
performance em different numbers mixture components negative class unlabeled documents 
precision recall breakeven points shown experiments mixture components 
note mixture components results poor performance 
category em em em em em em acq corn crude earn grain interest money fx ship trade wheat table 
performance em different numbers mixture components negative class unlabeled data 
precision recall breakeven points shown experiments mixture components 
category nb nb nb nb nb nb acq corn crude earn grain interest money fx ship trade wheat table 
classification accuracy reuters traditional naive bayes nb multiple mixture components just labeled data nb basic em em labeled unlabeled data multiple mixture components em labeled unlabeled data em table 
category nb nb em em em vs nb em vs nb acq corn crude earn grain interest money fx ship trade wheat text classification labeled unlabeled documents em table 
performance multiple mixture components number components selected cross validation em cv compared optimal selection em straight naive bayes nb 
note cross validation usually selects components 
category nb em em cv em cv vs nb acq corn crude earn grain interest money fx ship trade wheat recall focuses modeling positive class accuracy focuses modeling negative class frequent 
allowing mixture components negative class accurate model achieved 
obvious question select best number mixture components having access test set labels 
selection weighting factor leave cross validation computational short cut entails running em described section 
results technique em cv compared naive bayes nb best em em shown table 
note cross validation perfectly select number components perform best test set 
results consistently show selection cross validation chooses smaller number components best 
cross validation computational short cut bias model held document hypothesize favors fewer components 
computationally expensive complete cross validation perform better 
model selection methods may perform better remaining computationally efficient 
include robust methods cross validation ng minimum description length rissanen metric approach uses unlabeled data 
research improved methods model selection algorithm area 

related expectation maximization known family algorithms long history applications 
application classification new statistics literature 
idea em procedure improve classifier treating unclassified data incomplete mentioned little published responses original em dempster 
discussion partial classification paradigm descriptions mclachlan basford book mixture models page 
nigam mccallum thrun mitchell studies machine learning literature em combine labeled unlabeled data classification miller uyar shahshahani landgrebe 
naive bayes shahshahani landgrebe mixture gaussians miller uyar mixtures experts 
demonstrate experimental results non text data sets features 
contrast textual data sets orders magnitude features hypothesize exacerbate violations independence mixture model assumptions 
shahshahani landgrebe theoretically investigate utility unlabeled data supervised learning quite different results 
analyze convergence rate assumption unbiased estimators available labeled unlabeled data 
bounds fisher information gain show linear exponential value labeled versus unlabeled data 
unfortunately analysis assumes unlabeled data sufficient estimate parameter vectors assume target concept recovered target labels 
assumption unrealistic 
shown castelli cover unlabeled data improve classification results absence labeled data 
example applying em fill missing values missing values class labels unlabeled training examples 
ghahramani jordan example machine learning literature em mixture models fill missing values 
focus data class labels missing focus data features class labels missing 
autoclass project cheeseman stutz investigates combination expectation maximization naive bayes generative model 
emphasis research discovery novel clustering unsupervised learning unlabeled data 
multiple mixture components class example mixtures improve modeling probability density functions 
jaakkola jordan provide general discussion mixtures improve mean field approximations naive bayes example 
paradigm reduces need labeled training examples active learning 
scenario algorithm repeatedly selects unlabeled example asks human labeler true class label rebuilds classifier 
active learning algorithms differ methods selecting unlabeled example 
examples applied text query committee dagan engelson liere tadepalli relevance sampling uncertainty sampling lewis gale lewis :10.1.1.16.3103:10.1.1.16.3103:10.1.1.16.3103
authors combines active learning expectationmaximization mccallum nigam :10.1.1.13.8629
em applied unlabeled documents help inform algorithm choice documents labeling requests boost accuracy documents remain unlabeled 
experimental results show combination active learning em requires slightly half labeled training examples achieve accuracy active learning em 
text classification labeled unlabeled documents em effort unlabeled data support supervised learning training blum mitchell 
consider particular subclass learning problems distinct problems addressed 
particular consider problems target function learned attributes describing instance partitioned sets sufficient calculate redundancy attributes describing allows learning distinct classifiers train unlabeled data 
experimental results showing success approach web page classification task proof certain conditions target function pac learned initial weak classifier unlabeled data 
variety statistical techniques naive bayes applied text classification including support vector machines joachims nearest neighbor yang tfidf rocchio salton rocchio exponential gradient covering algorithms cohen singer :10.1.1.11.6124
naive bayes strong probabilistic foundation expectationmaximization efficient large data sets 
furthermore thrust straightforwardly demonstrate value unlabeled data similar approach apply unlabeled data complex classifiers 

summary family algorithms address question unlabeled data may supplement scarce labeled data especially learning classify text documents 
important question text learning high cost hand labeling data availability large volumes unlabeled data 
algorithm takes advantage experimental results show significant improvements unlabeled documents training classifiers real world text classification tasks 
assumptions data generation correct basic em effectively incorporate information unlabeled data 
full complexity realworld text data completely captured known statistical models 
interesting consider performance classifier generative models incorrect assumptions data 
cases data inconsistent assumptions model method adjusting relative contribution unlabeled data em prevents unlabeled data degrading classification accuracy 
augmentation basic em scheme study effect multiple mixture components class 
effort relax assumptions model generative model better match data 
experimental results show improvements classification suggest exploration complex mixture models correspond better textual data distributions 
results recommend study improvements current nigam mccallum thrun mitchell cross validation methods selecting unlabeled data weight number mixture components class 
believe algorithm unlabeled data require closer match data generative model labeled data 
intended target concept model differ actual distribution data strongly unlabeled data hurt help performance 
intend closer theoretical empirical study tradeoffs unlabeled data inherent model inadequacies 
see interesting directions unlabeled data 
task formulations benefit em active learning explicit model unlabeled data incorporate em improve selection examples request label improve classification accuracy examples remain unlabeled initial study area begun mccallum nigam incremental learning algorithm re trains testing phase unlabeled test data received early testing phase order improve performance test data :10.1.1.13.8629
furthermore problem domains share similarities text domains limited expensive labeled data abundant inexpensive unlabeled data 
robotics vision information extraction domains 
applying techniques improve performance areas 
acknowledgments larry wasserman extensive help theoretical aspects 
anonymous reviewers helpful suggestions corrections 
doug baker helped format reuters data set 
john lafferty provided insightful discussions em 
jason rennie comments earlier draft 
research supported part darpa hpkb program contract 
notes 
assumption relaxed section making correspondence 
li yamanishi relaxes assumption fashion 

standard notational shorthand random variables jy written jy random variables values 
previous naive bayes formalizations include document length effect 
general case document length modeled parameterized class class basis 

dirichlet commonly conjugate prior distribution multinomials 
dirichlet distributions discussed detail example stolcke omohundro 

data sets available internet 
see www cs cmu edu www research att com lewis 
text classification labeled unlabeled documents em 
statistical results number labeled examples small multiple trials paired tests 
number labeled examples large single trial report results mcnemar test 
tests discussed dietterich 

weighted log likelihood ratio rank words jc log jc understood information theoretic terms word contribution average inefficiency encoding words class code optimal distribution words sum quantity words kullback leibler divergence distribution words distribution words cover thomas 
blum mitchell 

combining labeled unlabeled data training 
proceedings th annual conference computational learning theory colt pp 

castelli cover 

exponential value labeled samples 
pattern recognition letters 
cheeseman stutz 

bayesian classification autoclass theory results 
fayyad piatetsky shapiro smyth uthurusamy 
eds advances knowledge discovery data mining 
mit press 
cohen singer 

context sensitive learning methods text categorization 
sigir proceedings nineteenth annual international acm sigir conference research development information retrieval pp 

cover thomas 

elements information theory 
john wiley sons new york 
craven dipasquo freitag mccallum mitchell nigam slattery 

learning extract symbolic knowledge world wide web 
proceedings fifteenth national conference artificial aaai pp 

dagan engelson 

committee sampling training probabilistic classifiers 
machine learning proceedings twelfth international conference icml pp 

dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
dietterich 

approximate statistical tests comparing supervised classification learning algorithms 
neural computation 
domingos pazzani 

optimality simple bayesian classifier zero loss 
machine learning 
friedman 

bias variance loss curse dimensionality 
data mining knowledge discovery 
nigam mccallum thrun mitchell ghahramani jordan 

supervised learning incomplete data em approach 
advances neural information processing systems pp 

jaakkola jordan 

improving mean field approximation mixture distributions 
jordan 
ed learning graphical models 
kluwer academic publishers 
joachims 

probabilistic analysis rocchio algorithm tfidf text categorization 
machine learning proceedings fourteenth international conference icml pp 

joachims 

text categorization support vector machines learning relevant features 
machine learning ecml tenth european conference machine learning pp 

koller sahami 

hierarchically classifying documents words 
machine learning proceedings fourteenth international conference icml pp 

lang 

newsweeder learning filter netnews 
machine learning proceedings twelfth international conference icml pp 

larkey croft 

combining classifiers text categorization 
sigir proceedings nineteenth annual international acm sigir conference research development information retrieval pp 

lewis 

evaluation phrasal clustered representations text categorization task 
sigir proceedings fifteenth annual international acm sigir conference research development information retrieval pp 

lewis 

sequential algorithm training text classifiers corrigendum additional data 
sigir forum 
lewis 

naive bayes independence assumption information retrieval 
machine learning ecml tenth european conference machine learning pp 

lewis gale 

sequential algorithm training text classifiers 
sigir proceedings seventeenth annual international acm sigir conference research development information retrieval pp 

lewis knowles 

threading electronic mail preliminary study 
information processing management 
lewis ringuette 

comparison learning algorithms text categorization 
third annual symposium document analysis information retrieval pp 

li yamanishi 

document classification finite mixture model 
proceedings th annual meeting association computational linguistics pp 

liere tadepalli 

active learning committees text categorization 
proceedings fourteenth national conference artificial intelligence aaai pp 

mccallum nigam 

comparison event models naive bayes text classification 
aaai workshop learning text categorization 
tech 
rep ws aaai press 
www cs cmu edu mccallum 
text classification labeled unlabeled documents em mccallum rosenfeld mitchell ng 

improving text shrinkage hierarchy classes 
machine learning proceedings fifteenth international conference icml pp 

mccallum nigam 

employing em pool active learning text classification 
machine learning proceedings fifteenth international conference icml pp 

mclachlan krishnan 

em algorithm extensions 
john wiley sons new york 
mclachlan basford 

mixture models 
marcel dekker new york 
miller uyar 

mixture experts classifier learning labelled unlabelled data 
advances neural information processing systems pp 

mitchell 

machine learning 
mcgraw hill new york 
ng 

preventing overfitting cross validation data 
machine learning proceedings fourteenth international conference icml pp 

pazzani muramatsu billsus 

syskill webert identifying interesting web sites 
proceedings thirteenth national conference artificial intelligence aaai pp 

rissanen 

universal prior integers estimation minimum description length 
annals statistics 
robertson sparck jones 

relevance weighting search terms 
journal american society information science 
rocchio 

relevance feedback information retrieval 
salton 
ed smart retrieval system experiments automatic document processing 
prentice hall englewood cliffs nj 
sahami dumais heckerman horvitz 

baysian approach filtering junk mail 
aaai workshop learning text categorization 
tech 
rep ws aaai press 
robotics stanford edu users sahami papers html 
salton 

developments automatic text retrieval 
science 
schuurmans 

new metric approach model selection 
proceedings fourteenth national conference artificial intelligence aaai pp 

shahshahani landgrebe 

effect unlabeled samples reducing small sample size problem mitigating hughes phenomenon 
ieee transactions geoscience remote sensing 
shavlik rad 

intelligent agents web tasks advice approach 
aaai workshop learning text categorization 
tech 
rep ws aaai press 
www cs wisc edu shavlik publications html 
stolcke omohundro 

best model merging hidden markov model induction 
tech 
rep tr icsi university california berkeley 
www icsi berkeley edu techreports html 
nigam mccallum thrun mitchell yang 

expert network effective efficient learning human decisions text categorization retrieval 
sigir proceedings seventeenth annual international acm sigir conference research development information retrieval pp 

yang 

evaluation statistical approaches text categorization 
journal information retrieval 
appear 
yang pederson 

feature selection statistical learning text categorization 
machine learning proceedings fourteenth international conference icml pp 

