collaborative adversarial learning case study robotic soccer peter stone manuela veloso school computer science carnegie mellon university pittsburgh pa cs cmu edu veloso www cs cmu edu running title soccer collaborative adversarial soccer rich domain study multiagent learning issues 
players learn low level skills learn adapt behaviors different opponents 
robotic soccer system study different types multiagent learning low level skills collaborative adversarial 
describe detail experimental framework 
learned robust low level behavior necessitated multiagent nature domain shooting moving ball 
discuss issues arise extend learning scenario require collaborative adversarial learning 
soccer rich domain study multiagent learning issues 
teams players order put ball opposing goal time defending 
learning essential task dynamics system change opponents behaviors change 
players able adapt new situations 
research sponsored wright laboratory aeronautical systems center air force materiel command usaf advanced research projects agency arpa number 
views contained document authors necessarily representing official policies endorsements expressed implied wright laboratory government 
players learn adapt behaviors different opponents learn 
soon play young soccer players learn team order win 
players learn collaborative adversarial techniques acquire low level skills allow manipulate ball 
low level skills dribbling entirely individual nature passing receiving passes necessitated multiagent nature domain 
approach multiagent learning complex domain develop increasingly complex layers learned behaviors bottom 
individual skills appropriate multiagent environment identify appropriate behavior parameters learning methods parameters 
incorporate learned individual skills higher level multiagent learning scenarios 
ongoing goal create full team agents learned behaviors different levels reason strategically real time environment 
article detailed experimental results successful neural networks learning low level behavior 
learned behavior shooting moving ball crucial successful action multiagent domain 
clients skill necessary learn higher level collaborative adversarial behaviors 
illustrate learned individual skill basis higher level multiagent learning discussing issues arise extend learning task 
multiagent learning multiagent learning intersection multiagent systems machine learning subfields artificial intelligence see 
described wei learning done agents possible agents wei 
fact certain circumstances clause definition necessary 
claim possible engage multiagent learning agent learning 
particular agent learning acquire skills interact agents environment regardless agents learning simultaneously agent learning multiagent learning 
especially learned behavior enables additional multiagent behaviors agent learn behavior multiagent behavior 
notice situation certainly satisfies second clause wei definition learning possible agent isolated 
multiagent systems artificial intelligence multiagent learning machine learning multiagent learning intersection multiagent systems machine learning subfields artificial intelligence 
traditional machine learning typically involves single agent trying maximize utility function knowledge care agents environment 
examples traditional machine learning tasks include function approximation classification problem solving performance improvement empirical data 
subfield multiagent systems surveyed stone veloso deals domains having multiple agents considers mechanisms interaction independent agents 
multiagent learning includes situation agent learns interact agents agents behaviors static 
main justification considering situations single agent learns multiagent learning learned behavior basis complex interactive behaviors 
example article reports development low level learned behavior multiagent domain 
single agent learning behavior possible presence agents importantly enables agent participate higher level collaborative adversarial learning situations 
multiagent learning accomplished layering learned behaviors top case levels learning involve interaction agents contribute part multiagent learning 
previous examples single agents learning multiagent environment included multiagent learning literature 
earliest multiagent learning papers describes reinforcement learning agent incorporates information gathered agent tan 
considered multiagent learning learning agent cooperating agent adversary agent learns interact 
example negotiation scenario agent learns negotiating techniques bayesian learning methods zeng sycara 
situation considered multiagent learning learning agent learning interact agent situation sense due presence multiple agents 
example represents class multiagent learning learning agent attempts model agents 
final example multiagent learning agents learns training scenario novice agent learns knowledgeable agent clouse 
novice learns drive simulated race track expert agent behavior fixed 
thing learning systems common learning agent interacting agents 
learning possible due presence agents may enable higher level interactions agents 
characteristics define type multiagent learning described article 
robotic soccer robotic soccer emerging challenging topic artificial intelligence re searchers interested machine learning multiagent systems reactive behavior strategy tion areas artificial intelligence 
section briefly summarizes previous research describes research platform 
related ground breaking system robotic soccer served inspiration dynamo system developed university british columbia sahota mackworth barman 
system designed capable supporting robots team done vs scenario 
sahota system introduce decision making strategy called reactive deliberation choose hard wired behaviors sahota 
approach worked specific task ml needed order avoid cumbersome task hard wiring robots new situation expanding complex multiagent environment 
modeled closely dynamo system authors developed real world robotic soccer system achim stone veloso 
main differences dynamo system robots smaller team infra red communication radio frequency 
robotic soccer system developed asada lab different dynamo system asada noda hosoda asada noda hosoda 
asada robots larger equipped board sensing capabilities 
develop low level behaviors shooting avoiding rl technique combining behaviors asada 
reducing state space significantly able rl learn shoot stationary ball goal 
best result simulation scoring rate 
done combining different learned behaviors separate learned decision mechanism top asada 
goals research similar approach different 
asada developed sophisticated robot system advanced capabilities chosen focus producing simple robust design enable concentrate efforts learning low level behaviors high level strategies 
believe approaches valuable advancing state art robotic soccer research 
real robotic systems mentioned new ones built robotic soccer tournaments kim kitano asada kuniyoshi noda osawa needed studying certain robotic issues possible conduct research efficiently designed simulator 
researchers previously simulated robotic soccer study ml applications 
soccer simulator sahota ford reinforcement learning rl approach sensory predicates learn choose low level behaviors ford boutilier kanazawa 
simulator described stone veloso memory learning allow player learn shoot pass ball stone veloso 
robocup soccer server noda neural network allow player learn shoot pass matsubara noda 
described article builds previous learning difficult behavior shooting moving ball goal 
behavior specifically designed useful complex multiagent behaviors described section 
simulator pursuing research directions real robotic system achim simulator facilitates extensive training testing learning methods 
research reported article conducted simulation 
simulator real world system closely systems designed laboratory computational intelligence university british columbia sahota 
particular simulator code adapted code michael sahota sahota personal correspondence sahota motivating invaluable 
simulator facilitates control number agents ball designated playing area 
care taken ensure simulator models real world responses friction conservation momentum closely possible 
sensor noise variable magnitude included model precise real systems 
graphic display allows researcher watch action progress graphics toggled speed rate experiments 
shows simulator graphics 
ball agents theta client program client program gas steering simulator includes world model graphic view simulator 
eventually teams agents compete real time game robotic soccer 
interface clients simulator 
simulator client server model server models real world reports state world clients control individual agents see 
moving objects world agents ball position orientation simulator describes current state world clients reporting coordinates objects indicating positions orientations 
clients periodically send throttle steering commands simulator indicating agents move 
simulator job correctly model motion agents commands motion ball collisions agents walls 
parameters modeled underlying physics described detail appendix clients equipped path planning capability allows follow shortest path positions latombe reeds shepp 
purposes article path planning needed ability steer straight line 
task trivial client control agent motion discrete time intervals 
algorithm controls agent steering offset correct heading distance line followed 
algorithm allows agent steering exactly right direction small distance line short adjustment period 
agent able reliably strike ball direction 
details line method see appendix learning low level multiagent behavior low level skill focus article ability shoot moving ball 
single agent shooting consider multiagent learning scenario ball typically moving result pass teammate possible agents 
furthermore discussed section skill necessary creation higher level multiagent behaviors 
order skill variety situation robust situation independent possible 
section describe detail created robust learned behavior multiagent scenario 
experiments agents passer accelerates fast possible stationary ball order propel shooter goal 
resulting speed ball determined distance passer started ball 
shooter task time acceleration intercepts ball path redirects goal 
constrain shooter accelerate fixed constant rate steering fixed line decided approach 
behavior learned consists decision moving action opportunity shooter starts waits 
having started decision may retracted 
key issue shooter decision observed field ball shooter coordinates reported simulated rate method shooter decision called shooting policy 
experiments shooter initial position varies randomly continuous range initial heading varies degrees initial coordinates vary independently units shown 
pictured show extreme possible starting positions terms heading location 
ball momentum initially front goal shooter compensate aiming wide goal making contact ball see 
approach shooter chooses point wide goal aim 
deciding start steers imaginary line point shooter initial position continually adjusting heading moving right direction line see appendix 
line shooter steers steering line 
method shooter chooses steering line called aiming policy 
task learning shooting policy parameters control level difficulty 
ball moving speed training examples different speeds 
second ball coming trajectory different trajectories 
third goal place testing training change locations think parameter possibility aiming different parts goal 
fourth training testing occur location testing moved different action quadrant symmetrical location field 
acquire robust behavior perform dynamo system reported coordinates rate hz overhead camera color coded objects sahota 
width field side goal shown units 
width goal units 
contact point steering line shooter initial position experiments 
agent lower part picture passer accelerates full speed ahead hits ball 
agent shooter attempts redirect ball goal left 
agents top illustrate extremes range angles shooter initial position 
square agents indicates range initial position center shooter 
diagram illustrating paths ball agents typical trial 
report series experiments increase difficulty task incrementally 
develop learning agent test far training limited scenario extend 
table indicates article organized parameters varied experiments 
illustrates variations 
passer fixed variable ball speed section sections ball trajectory sections sections goal location sections section action quadrant sections training sections testing table parameters control difficulty task sections varied 
supervised learning technique learn task hand 
article training done simulated sensor noise units degrees reported success rates trials 
variations initial setup initial position opposite corner field different action quadrant varied ball trajectory line indicates ball possible initial positions passer starts directly ball facing fixed point 
passer initial distance ball controls speed ball passed varied goal position placement higher lower goals pictured 
fixed ball motion began experimentation ball passed trajectory speed training testing examples 
condition fixed ball motion shooter aim point wide goal guaranteeing contact ball propelled right direction 
say shooter constant aiming policy 
determined trajectory speed units sec ball initially shooter score contacting ball steering line aimed units wide center goal illustrated 
point remains constant section section 
setting learning experiments simple fixed shooting policy allow shooter score consistently starting exact center range initial positions 
starting position shooter score consistently began accelerating ball distance projected point intersection agent path reached units 
call policy simple shooting policy 
simple policy clearly appropriate entire range shooter positions considered policy starting random positions shooter scored time 
choosing inputs convinced ml technique provide significantly better shooting policy decided try neural network initial attempt 
plan experiment ml techniques task 
considered structure neural network order learn function current state world indication shooter start accelerating remain wait 
output function fairly straightforward 
indicate starting accelerate world state described input values lead goal outputs close outputs close 
deciding represent world state inputs neural network represented core part research 
option coordinates shooter ball 
inputs generalized limited training situation 
furthermore led higher dimensional function turned necessary 
chose just easily computable coordinate independent predicates 
line agent steered computed started moving line connecting agent initial position point units wide goal ball trajectory estimated error due noise getting distinct position readings shooter able determine point hoped strike ball contact point 
cheaply compute certain useful predicates ball distance ball distance contact point agent distance agent distance contact point heading offset difference agent initial heading desired heading 
physical meaning inputs illustrated 
inputs proved sufficient learning task hand 
furthermore contained coordinate specific information enabled training narrow setting apply widely shown section 
gathering training data recall shooter started center range score simple shooting policy began moving ball distance units 
get diverse training sample replaced shooting policy random shooting policy form opportunity moving probability help choose determined shooter decision opportunities ball moved units contact point 
wanted shooter start moving decision cycles roughly equal probability get balanced training sample solved equation 
random shooting policy shooter started moving probability decision point 
shooting policy collected training data 
instance consisted numbers inputs ball distance agent distance heading offset time shooter began accelerating indicate shot successful 
shot successful went directly front shooter goal illustrated trial halted unsuccessfully ball hit corner side shooter ball hit wall goal 
running trials manner gave sufficient training data learn shoot moving ball goal 
success rate random shooting policy 
particular training examples positive instances 
training data able train nn shooter part learned shooting policy enabled score consistently 
tried configurations neural network settling single layer hidden units learning rate 
layer bias unit constant input 
normalized inputs fall roughly see 
target outputs positive examples successful trials negative examples 
weights initialized randomly 
resulting neural network pictured 
neural network result exhaustive search optimal configuration suit task quickest successful alternatives different numbers hidden units different learning rates 
settling configuration changed concentrating research issues 
heading offset ball distance steering line shooter agent distance contact point passer ball distance agent distance heading offset radians move stay predicates describe world purpose learning shoot moving ball illustrated 
neural network learn shooting policy 
neural network hidden units bias unit input hidden layers 
training neural network entire training set epochs resulted mean squared error examples misclassified output closer wrong output correct 
training epochs help noticeably 
due sensor noise training concept perfectly learned 
testing training neural network cheaply just single forward pass decision point decide accelerating 
notice single trial input neural network varied ball distance decreased ball approached contact point 
output neural network tended vary fairly regularly 
ball began approaching output began increasing slowly sharply 
reaching peak output began decreasing slowly sharply 
optimal time shooter accelerating peak function function peaked different values different trials input neural network shooting policy accelerating output requiring output output previous output 
ensured shooter start moving believed score output previous output true output neural network just past peak 
requiring output decrease previous output ensured decrease due simply sensor noise 
learned input neural network shooting policy shooter scored time 
results reported section summarized table 
initial shooter position shooting policy success constant simple varying simple varying random varying input nn table results learning fixed ball motion 
important high success rate achieved learned shooting policy fact shooter achieved success rate symmetrical reflections training situation action quadrants 
training shooter able score side goal side field 
illustrates symmetrical scenarios 
world description input neural network contained information specific location field captured information relative positions shooter ball goal 
flexible inputs training situation applicable situations 
varying ball speed encouraged success flexibility initial solution varied parameter initial setup test solution extend 
section passer started units away ball accelerated full speed ahead striking 
process consistently propelled ball units sec 
task challenging varied ball speed starting passer randomly range units away ball 
ball travel shooter speed units sec 
making changes shooter shooting policy tested policy trained section new task 
input neural network sufficient handle varying speed giving success rate due acceleration 
order accommodate added fourth input neural network order represent speed ball ball speed shooter computed ball speed ball change position amount time 
due sensor noise change position single time slice give accurate reading 
hand ball slowed time shooter take ball total change position time 
compromise shooter computed ball speed ball change position time slices fewer positions observed 
accommodate additional quantity describe world state gathered new training data 
shooter random shooting policy training trials 
time training instance consisted inputs describing state world plus output indicating trial successful 
samples gathered training positive examples 
purposes training new neural network scaled ball speed ball fall fourth input new neural network looked pictured 
hidden units bias unit level learning rate 
training neural network epochs resulted mean squared error instances misclassified 
new neural network decision function output input neural network shooting policy shooter able score time ball moving different speeds 
success rate observed action quadrants 
results section summarized table 
shooting policy success input nn random input nn table ball speed varies additional input needed 
varying ball trajectory point shown inputs train neural network allowed training particular part field apply parts field 
section describe experiments show input neural network shooting policy described section flexible 
experiments section exactly shooting policy retraining 
inputs relative contact point predicted trajectory ball approached shooter affect performance neural network adversely 
order test hypothesis changed initial positions ball passer ball cross shooter path different heading opposed 
variation initial setup illustrated 
ball speed varied section 
shooter able consistently contact ball redirect goal 
scored steering line aiming wide goal 
due ball changed trajectory aiming units wide goal longer appropriate shooter ball approached trajectory shooter change aiming policy aim units wide goal units wide 
aiming policy 
passer ball coming trajectory simply change steering line shooter aiming units wide center goal 
doing led success rate better actual training situation 
improved success rate accounted fact ball approaching agent directly slightly easier hit 
remained difficult task successfully accomplished retraining 
learned shooting policy generalize different areas field generalized different ball trajectories 
having shooting policy successfully shoot balls moving different knew vary ball trajectory continuous range policy score 
problem altering shooter aiming policy 
far chosen steering line hand want different possible trajectory ball 
fact problem gave opportunity put principal espoused article 
experiments described remainder section ball initial trajectory ranged randomly 
illustrates range 
policy shooter decide accelerating exactly learned section retraining neural network 
top neural network added new determine direction shooter steer 
steering line determined direction shooter current position compute predicates needed input original neural network 
new neural network chose parameters allow generalize training situation 
new input chose angle ball path line connecting shooter initial position center goal ball agent angle 
output angle wide second line shooter steer 
quantities illustrated 
performance improved slightly added second input estimate ball speed 
neural network input neural network shooting policy shooter determined steering line estimate ball speed just position readings 
shooter steering line chosen learned aiming policy input neural network shooting policy time shot 
gather training data new neural network ran trials ball passed different speeds different trajectories angle wide set random number radians 
positive examples trained neural network learned aiming policy 
inputs outputs scaled fall roughly hidden units bias units weights initialized randomly learning rate 
resulting neural network pictured 
training neural network epochs gave mean squared error 
input neural network aiming policy decide aim old input neural network shooting policy decide accelerate shooter scored time ball speed trajectory varied 
neural network just input omitting speed estimate hidden unit input neural network aiming policy gave success rate 
satisfied performance experiment neural ball agent angle steering line shooter angle wide passer ball speed ball agent angle radians angle wide predicates describe world purpose learning aiming policy 
neural network learn aiming policy 
neural network hidden units bias unit input hidden layers 
network configurations 
moving goal test learned aiming policy generalize training situation moved goal goal width side field see 
think variation shooter aiming different parts larger goal see section 
changing shooter knowledge goal located shooter scored time lower goal time upper goal see table 
discrepancy values explained greater difficulty shooting ball direction close direction travelling 
expect initial situation flipped shooter began lower quadrant success rates flipped shooter scored frequently shooting higher goal 
notice case representation output important inputs generalization training situation 
aiming policy goal position success input nn middle input nn middle input nn lower goal width input nn higher goal width table ball trajectory varied new aiming policy needed 
results reasonable goal moved new position 
higher level multiagent extensions shooting moving ball crucial high level collaborative adversarial action soccer domain 
learned shooting behavior robust works different ball speeds trajectories situation independent works different action quadrants goal locations 
qualities enable basis higher level behaviors 
soccer players learn low level skills soccer inherently strategic high level task team best skilled individual players world easily beaten 
similarly unskilled team works exploit weakness better team able prevail 
section describes robust shooting template developed section built collaborative adversarial directions 
particular collaborative standpoint learned shooting skill passer 
passing stationary ball shooter passer redirect moving ball exactly way shooter aiming point front shooter goal 
hand adversarial issues studied introducing defender tries block shooter attempts 
cooperative learning task shooting moving ball passer behavior predetermined accelerated fast hit ball 
varied velocity speed trajectory ball simply starting passer ball different positions 
real game passer rarely opportunity pass stationary ball placed directly front 
learn deal ball motion 
particular passer need learn pass ball way shooter chance putting ball goal see 
shooter passer collaborative scenario passer shooter learn tasks way interact successfully 
approach problem low level template learned section shooter passer 
fixing behavior agents learn entirely new behavior level worrying low level execution 
notice passer shooter learned cooperate effectively number passes chained 
receiver pass case passer simply aim receiver chain shooter 
parameters learned passer shooter point aim pass point position respectively 
section shooter fixed goal aim passer task defined 
goal redirect ball way shooter best chance hitting 
similarly ball passed shooter get position gives passer best chance executing pass 
illustrates inputs outputs passer shooter behavior scenario 
passer shooter angle passer choose lead distance distance shooter line connecting shooter goal aim pass 
notice input passer learning function manipulated shooter passer aims pass 
passer relative position goal shooter affect passer shooter angle positioning appropriately 
inputs outputs tasks similar task learned section similar neural network techniques 
lead distance passer goal angle shooter passer shooter angle passer goal distance ball passer angle passer passer shooter passer lead distance angle passer goal distance angle shooter passer shooter angle ball passer angle parameters learning functions passer shooter 
passer shooter learning scenario satisfies aspects wei definition multiagent learning agent learning situation multiple agents necessary 
phenomenon different agents learning parameters interacting directly common multiagent systems 
novel part approach layering learned multiagent behavior top 
continuing layer passing behavior incorporated player faced decision teammate pass 
chaining passes described point passer aim pass analogous goal location section 
passer choose trajectory way section 
assumes player knows pass ball 
game situation positioning players field receiver pass choose send ball 
richer widely simulator environment noda authors decision tree learning enable passer choose possible receivers presence defenders stone veloso 
addition current simulator successfully reimplemented neural network approach learning intercept moving ball described article 
adversarial learning time teammates cooperating passing ball consider best defeat opponents 
shooting template developed section incorporated adversarial situation adding defender 
section shooter trained aim different goal locations 
small goal train shooter accuracy 
competitive situations goal larger player single player entirely block goal 
order adversarial situation fair goal widened times see 
collaborative case approach problem involves holding learned shooting template fixed allowing clients learn behaviors higher level 
defender position shooter choose shoot upper middle lower portion goal 
shooter ball positions current velocity defender choose direction accelerate 
simplicity defender may set throttle full forward full backwards 
outputs shooter defender learning functions discrete reinforcement learning techniques learning kaelbling littman moore possible alternatives neural networks 
defender may able learn judge shooter aiming shooter strikes ball observing shooter approach 
similarly defender starts moving defender defender position shooter ball position defender ball velocity shooter defender shooter passer shot direction acceleration direction adversarial scenario defender learns block shot shooter simultaneously tries learn score 
shooter may able adjust aim different part goal 
time goes opponents need evolve order adjust changing strategies 
note sophisticated agent may able influence opponent behavior acting consistently period time drastically changing behaviors fool opponent 
higher level behavior adversarial scenario extended combining collaborative passing behavior 
receiving ball passer decide pass ball shoot immediately defender motion 
extra option complicate defender behavior 
related results pertaining decision pass shoot appear stone veloso 
discussion robotic soccer rich domain study multiagent learning issues 
opportunities study collaborative adversarial situations 
order study situations agents learn basic behaviors necessitated multiagent nature domain 
similar human soccer players learn contact moving ball learn aim start thinking trying beat opponent team level strategies 
article presents robust low level learned behavior presents ways extended incorporated collaborative adversarial situations 
ongoing research agenda includes improving low level behaviors simultaneously working collaborative adversarial learning issues 
goal create high level learned strategic behaviors continuing layer learned behaviors 
achim stone veloso 

building dedicated robotic soccer system 
working notes iros workshop robocup 
asada noda hosoda 

purposive behavior acquisition real robot vision reinforcement learning 
proc 
mlc colt machine learning computer learning theory workshop robot learning pp 

asada noda hosoda 

coordination multiple behaviors acquired vision reinforcement learning 
proc 
ieee rsj gi international conference intelligent robots systems iros pp 

clouse 

learning automated training agent 
wei sen 
eds adaptation learning multiagent systems 
springer verlag berlin 
ford boutilier kanazawa 

exploiting natural structure reinforcement learning experience robot soccer playing 
unpublished manuscript 
kaelbling littman moore 

reinforcement learning survey 
journal artificial intelligence research 
kim 

third call participation micro robot world cup soccer tournament 
accessible vivaldi kaist ac kr 
kitano asada kuniyoshi noda osawa 

robocup robot world cup initiative 
ijcai workshop entertainment ai alife pp 
montreal quebec 
latombe 

fast path planner car indoor mobile robot 
proceedings ninth national conference artificial intelligence pp 

matsubara noda 

learning cooperative actions multi agent systems case study pass play soccer 
adaptation coevolution learning multiagent systems papers aaai spring symposium pp 
menlo park ca 
aaai press 
aaai technical report ss 
noda 

soccer server simulator robocup 
proceedings ai symposium pp 

japanese society artificial intelligence 
press 

numerical recipes art scientific computing pp 

cambridge university press cambridge 
reeds shepp 

optimal paths car goes forward backward 
pacific journal mathematics 
sahota 

personal correspondence 
sahota 

user guide 
available www cs ubc ca nest lci soccer 
sahota 

real time intelligent behaviour dynamic environments soccer playing robots 
master thesis university british columbia 
sahota mackworth barman 

real time control soccer playing robots board vision dynamite testbed 
ieee international conference systems man cybernetics pp 

stone veloso 

beating defender robotic soccer memory learning continuous function 
touretzky mozer hasselmo 
eds advances neural information processing systems cambridge ma 
mit press 
stone veloso 

layered approach learning client behaviors robocup soccer server 
submitted applied artificial intelligence aai journal 
stone veloso 

multiagent systems survey machine learning perspective 
submitted ieee transactions knowledge data engineering tkde 
tan 

multi agent reinforcement learning independent vs cooperative agents 
proceedings tenth international conference machine learning pp 

wei 

distributed reinforcement learning 
robotics autonomous systems 
zeng sycara 

bayesian learning negotiation 
adaptation coevolution learning multiagent systems papers aaai spring symposium pp 
menlo park ca 
aaai press 
aaai technical report ss 
simulator physics physics simulator largely embedded simulator dynamo group sahota minor adjustments 
ball radius units mass units drag units sec value drag typical ping pong ball cm sec 
agents units long units wide mass units 
agent position updated steering throttle commands receives fourth order runge kutta formula press 
collisions objects world handled follows 
ball bounces walls elastically stops enters goal agents collide wall 
collisions ball agent take account momenta colliding objects corners agents 
line clients article path planning needed ability steer straight line 
line periodically position client goal eventually travelling precisely parallel line certain distance line 
distance tolerance parameter article set units 
agent width units tolerance allows client strike ball expected 
client begins computing distance line offset correct heading slope line 
client distance greater tolerance agent steers sharply line 
particular sets steering proportion distance line minus tolerance 
distance tolerance client steers line force proportional heading offset 
client heading parallel line wheel centered 
perpendicular distance center client target line 
client heading line slope 
tolerance set steering angle line proportion 
tolerance set steering angle line proportion 
method force client travel exactly line allow client moving precisely right direction 
able reliably strike ball particular direction 

