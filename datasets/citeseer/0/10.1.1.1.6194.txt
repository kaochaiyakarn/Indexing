improving multi class text classification naive bayes jason rennie computer science carnegie mellon university submitted department engineering computer science partial requirements degree master science electrical engineering computer science massachusetts institute technology september massachusetts institute technology 
rights reserved 
signature author 
department electrical engineering computer science september certified 
tommi jaakkola assistant professor electrical engineering computer science thesis supervisor accepted 
arthur smith chairman committee graduate students department electrical engineering computer science improving multi class text classification naive bayes jason rennie submitted department electrical engineering computer science september partial fulfillment requirements degree master science numerous text documents available electronic form 
available day 
documents represent massive amount information easily accessible 
seeking value huge collection requires organization organizing documents automated text classification 
accuracy understanding systems greatly influences usefulness 
seek advance understanding commonly text classification techniques understanding improve tools available text classification 
clarifying assumptions derivation naive bayes noting basic properties proposing ways extension improvement 
investigate quality naive bayes parameter estimates impact classification 
analysis leads theorem gives explanation improvements multiclass classification naive bayes error correcting output codes 
experimental evidence commonly data sets exhibit application theorem 
show fundamental flaws commonly feature selection algorithm develop statistics framework text feature selection 
greater understanding naive bayes properties text allows better text classification 
thesis supervisor tommi jaakkola title assistant professor electrical engineering computer science contents naive bayes ml naive bayes 
map naive bayes 
expected naive bayes 
bayesian naive bayes 
bayesian naive bayes performs worse practice 
naive bayes linear classifier 
naive bayes outputs 
analysis naive bayes parameter estimates consistency 
bias 
variance 
danger imbalanced class training data 
error correcting output coding 
additive models 
relation boosting 
support vector machine 
experiments 
success failure naive bayes 
multiclass error function binary performance 
non linear loss ect ecoc performance 
feature selection information gain 
hypothesis testing 
generalization advantage significance level 
undesirable properties ig ht 
simple discriminative feature selection 
new feature selection framework 
data sets list figures document generation model 
distribution variance log 
classification output variance 
comparison multiclass classification schemes 
binary error poor indicator multiclass performance 
multiclass vs binary performance 
vs multiclass error vs roc breakeven 
new statistics framework feature selection 
list tables bayesian naive bayes classification results 
maximum term frequency bayesian map naive bayes 
map naive bayes produces posteriors 
newsgroup words high log odds ratio 
ecoc multiclass performance 
binary confusion matrix 
ecoc binary performance 
linear loss function ective hinge 
chapter numerous text documents available electronic form 
available constantly 
web contains documents 
millions people send mail day 
academic publications journals available electronic form 
collections represent massive amount information easily accessible 
seeking value huge collection requires organization 
web sites er hierarchically organized view web 
mail clients er system filtering mail 
academic communities web site allows searching papers shows organization papers 
organizing documents hand creating rules filtering painstaking labor intensive 
greatly aided automated classifier systems 
accuracy understanding systems greatly influences usefulness 
aim advance understanding commonly text classification techniques understanding improve tools available text classification 
naive bayes de facto standard text classifier 
commonly practice focus research text classification 
chakrabarti naive bayes organizing documents hierarchy better navigation understanding text corpus er mccallum naive model estimate word distribution node hmm extract information documents dumais naive bayes text classifiers automate process text classification naive bayes commonly important reason gain better understanding 
naive bayes tool works particular cases important able identify ective techniques appropriate 
thorough understanding naive bayes easier extend naive bayes tune particular application 
naive bayes text classification 
lewis gives review naive bayes information retrieval lewis text classification information retrieval practitioners usually assume independence features ignore word frequency document length information 
multinomial model text classification di erent treated 
domingos pazzani discuss conditions naive bayes optimal classification probability assessments incorrect domingos pazzani domingos pazzani clarify point show simple cases naive bayes optimal classification 
analysis naive bayes domingos pazzani important little exists 
berger ghani individually ran experiments ecoc naive bayes 
able improve performance regular naive bayes berger ghani adequately explains regular naive bayes performs poorly compared ecoc 
yang pedersen conduct empirical study feature selection methods text classification yang pedersen give evaluation di erent feature selection techniques provide analysis di erences 
need better understanding feature selection method 
yang pedersen say common terms informative text classification certainly factors 
application naive bayes multiclass text classification understood 
important factor ecting performance naive bayes quality parameter estimates 
text special large number features usually features provide information classification occur handful times 
poor estimates due insu cient examples class ect classifier 
approach problem analyzing bias variance naive bayes parameter estimates 
naive bayes suited perform multiclass text classification reason believe schemes ecoc multiclass boosting yield improved performance naive bayes component 
regular naive bayes cient schemes important understand improve performance merely add ine cient baggage multiclass system 
show ecoc yield improved performance regular naive bayes give experimental evidence back claims 
multitude words english languages drives practitioners reduce number feature selection 
feature selection improve generalization error eliminating features poor parameter estimates 
interaction feature selection algorithms naive bayes understood 
commonly algorithms properties appropriate multiclass text classification 
point flaws suggest new framework text feature selection 
chapter naive bayes says naive bayes clear meant 
mccallum nigam clarify picture defining di erent naive bayes event models provide empirical evidence multinomial event model preferred text classification multiple methods obtaining parameter estimates 
interest clarity carefully step multinomial derivation naive bayes distinguish variations model 
fully bayesian derivation naive bayes new advertised algorithm text classification 
careful presentation hope clarify basis naive bayes give insight extended improved 
simplify assume class 
unknown parameter vector generates documents independently 
documents observed part particular class known training documents designated test documents 
model depicted 
assume generation model multinomial ignore document length concerns 
ml naive bayes formulation naive bayes choose parameters produce largest likelihood training data 
predictions estimated parameter vector method obvious flaws includes strong assumptions generation data 
example feature occur training data class assumed occur document generated class 
method known maximum likelihood ml ective practice cient implement 
regularly domains 
call multinomial version ml naive bayes 
ml parameter class argmax 
training data class class parameter vector multi graph independence relations variables classification 
class multinomial parameter vector 
set training documents class test document classified 
nomial model 
multinomial likelihood 
notate number times word occurs class training data 
th component multinomial parameter vector probability word appear single event multinomial trial 
ml estimate maximizes ml naive bayes assume estimated parameter vector vector generated assess test document generated class infer parameter vector prediction test document implicitly depends training data setting bottlenecks information may provide bayes optimal decision rule classification argmax argmax 

set training data 
class prior uniform classification rule simply chooses class test document argmax argmax 
number times word occurs decision rule augmented text classification 
ensure happen training data counts fictitious counts 
rationale adding counts varies 
fictitious count word arrive modified decision rule argmax 
uniform fictitious counts words 
common choice 
map naive bayes ml naive bayes leaves desired include framework explain fictitious counts 
result know fictitious counts represent 
know assumptions parameter estimation underpins inclusion decision rule 
turn generalization ml estimation maximum posteriori map estimation 
map estimation produces fictitious counts thorough particular choice parameter prior distribution 
change way estimate parameters map naive bayes identical ml naive bayes 
select best parameter vector vector classification 
map estimation estimate parameter vector argmax argmax parameter prior term 
map estimation generalization ml estimation ml map appropriate constant 
choose dirichlet general form prior 
hyper parameters 
density dirichlet dir 
gamma function 
satisfies 


valuable property dirichlet conjugate prior multinomial distribution 
posterior distribution dirichlet dir 
setting maximizes expression 
size vocabulary 
setting gives fictitious counts equation ad hoc reasoning 
map derivation clear fictitious counts represent particular prior distribution parameter space 
particular common choice represents prior distribution uniform parameters preferred 
expected naive bayes map nb decision rule commonly derived di erent way chakrabarti maximizing aspect data expected value parameter 
estimate parameter number times word appears class training documents 
uniform prior get map nb decision rule 
size vocabulary 
maximizing posterior prior prefers uniform parameters gives parameter estimates uniform prior expected values 
bayesian naive bayes map naive bayes chooses particular parameter vector classification 
simplifies derivation bottlenecks information training data classification 
alternative approach distribution parameters data 
complicates derivation somewhat don evaluate 
integrate possible parameters belief particular set parameters generated ml map naive bayes start bayes optimal decision rule argmax argmax 
expand 
multinomial likelihood 
expand posterior bayes law dirichlet prior map naive bayes 
gives dirichlet posterior dir 
substituting equation selecting get argmax 
fully bayesian derivation distinct map ml derivations shares similarities 
particular approximations get decision rule similar map naive bayes argmax 
lone di erence map naive bayes uses di erent dirichlet hyper parameters achieve rule 
bayesian naive bayes distinct map naive bayes decision rule 
shown modifications identical modifications generally appropriate 
fact modifications exhibit di erences map bayesian naive bayes 
compared map bayesian naive bayes emphasizes words appear test document 
consider binary classification test document word appears twice 
contribution map naive bayes similar contribution bayesian naive bayes 
bayesian term larger terms identical 
di erence greater word occurs frequently 
bayesian naive bayes performs worse practice data sets tried bayesian nb dirichlet prior performed worse map nb dirichlet prior 
sign bayesian derivation bad far 
poor empirical performance indication dirichlet prior poor choice dirichlet hyper parameter settings chosen 
estimated hyper parameters di erent prior dirichlet process may yield better performance bayesian nb ferguson show empirical di erence give statistics exhibiting conditions classification di erences occur 
conducted classification experiments newsgroups industry sector data sets 
table shows empirical test error averaged test train splits 
see appendix full description data sets preparations industry sector training examples class map bayesian newsgroups training examples class map bayesian table shown results naive bayes multi class classification bayesian map nb newsgroups industry sector data sets 
errors average trials 
di erences newsgroups results statistically significant 
bayesian nb higher error rates industry sector data set 
industry sector correct label bayesian map max 
term freq 
newsgroups correct label bayesian map max 
term freq 
table shown maximum term frequencies test documents classification algorithms disagree 
bayesian column gives maximum frequency averaged test documents bayesian nb gives correct label map nb 
map gives statistic case map nb label correct bayesian nb label wrong 
disagreements map correct frequent word occurs bayesian correct frequent word occurs 

techniques perform equally newsgroups data set 
di erences way technique classifies documents di erences result large di erences error 
technique consistently outperforms vary amount training data 
case industry sector data set 
di erences error map bayesian nb larger sided map nb lower error levels training data 
additional analysis shows cases bayesian map nb don agree distinct di erence frequency words test document 
map produces correct label word largest term frequency occurs word largest term frequency documents bayesian labels correctly 
trend seen newsgroup results correlate di erence error 
table summarizes statistics 
bayesian nb word occur class training data emphasized classification output compared map nb 
choice corresponds prior 
corresponds preference uniform parameter vectors vectors words probability 
isn reasonable prior english languages 
appropriate prior cause novel words emphasized 
poor performance bayesian nb fault classification algorithm sign choice prior model poor 
bayesian derivation provides classification rule directly incorporates information training data may sensitive choice prior 
better choice model prior improve performance bayesian nb 
naive bayes linear classifier map naive bayes known linear classifier 
case classes classification output log log log log corresponds classification corresponds classification 
represent linear weight th word vocabulary 
identical manner way logistic regression linear svms score documents 
logistic regression classifies classification classification 
exp similarly linear svm classifies assigning class class 
algorithms operationally identical terms classify documents 
di erence way weights trained 
similarity extends multi class linear classifiers 
softmax standard extension logistic regression linear case 
multi class problem classes 
softmax computes class assigns probabilities exp exp class largest largest probability declared label example percentile min 
posterior percentile digits table shown max values produced map naive bayes newsgroup data 
shows smallest value percentile levels 
naive bayes produced value majority test data 
shows percentile rounding posterior number digits produce value 
posteriors tend rapidly 
similarly map naive bayes decision rule argmax argmax log log argmax 
naive bayes softmax operationally identical 
extension linear svm multi class shares form 
distinction algorithms way weights trained 
naive bayes outputs consider pair unfair coins 
comes heads time 
count times coins show side heads appears time 
coins marginally show heads time heads time coins show side 
consider casting spell heads coins second coin lands side 
model coins independent observe large number flips estimate coins land side heads shows time 
fact probability event 
ect occurs map naive bayes 
rare words serve exact duplicates coin example 
distinguishing classes requires mere word vocabulary terms document correct classification remaining information class variable noisy redundant 
text databases frequently distinct vocabulary words documents contain terms 
great opportunity duplication 
get sense duplication trained map naive bayes model newsgroups documents 
produced posterior values remaining data show statistics max table 
values highly 
test documents assigned posterior rounded decimal digits 
logistic regression naive bayes optimized produce reasonable probability values 
logistic regression performs joint optimization linear coe cients converging appropriate probability values su cient training data 
naive bayes optimizes coe cients 
produces realistic outputs independence assumption holds true 
features include significant duplicate information usually case text posteriors provided naive bayes highly 
chapter analysis naive bayes parameter estimates having understanding map naive bayes parameter estimates ect classification important 
quality parameter estimates directly ects performance 
show naive bayes estimates consistent investigate behavior finite training data analyzing bias variance 
bias estimate direct product prior tends monotonically zero training data 
variance peaks word expected occur times training data falls 
analysis shows insu cient training examples class negatively ect performance 
variance sum variances individual components 
single class variance large variance high 
consistency map naive bayes estimates vector parameters multinomial model 
individual parameter estimated probability word appearing particular position class document 
parameters dirichlet prior number occurrences word training documents 
map estimate 
basic desirable property parameter estimates consistency convergence estimates true values amount data estimates grows large 
cover thomas describe method types way describe properties empirical distributions cover thomas multinomial random variable parameters 
represent distribution parameters 
represent empirical distribution samples taken resulting counts 
map estimates probability observing counts probability making estimates mean estimate goes 
variance estimate goes zero 
map estimates consistent limit unbiased zero variance 
size observed data grows large estimates converge true parameters 
bias infinite training data practice important understand behavior estimates finite training data 
particular number observed words bias estimate word bias words expected estimate smaller expected estimate larger true value 
natural consequence choice dirichlet prior 
bias lessens amount training data grows large 
variance variance parameter estimate yields little insight ect estimates classification 
naive bayes linear classifier useful variance quantity examine variance individual term classification output 
frequency word test document 
log log classification score class assigned class largest score 
individual terms sum independent assuming log plot pmf log 
plots variance log vary note axis log scale 
var log peaks word expected occur times 
representative plot var log peaks near shape shown 
fixed var var log 
assume fixed may vary 
variance individual term var log log log 
treating binomial parameter get log log 
equation di cult compute approximate poisson stirling formula 
arrive log log exp log exp log log 
formula graphs 
poisson approximation generally case text 
shows plots pmf variance word 
var log maximized expected occur times training data 
incorporate word occurs times training data theta shown word variance contribution classification output various values assume var log largest larger values yield larger word variance contributions 
class occur test documents generated class give ability compare variances classes 
true parameters classes 
training data classes consists words contribution variance classification output greater class class 
shows variance contribution individual tokens assuming words largest contribute largest variance classification output 
reasonable class independent words test documents drawn class words large values contribute greatest amount variance classification outputs word small easily contribute great deal variance occurs frequently test document 
glean ect additional training data classification 
widely believed additional training data improves classification 
plot variance log shows word point variance contribution word diminishes additional training data 
point passed words variance classification output decreases monotonically 
point output variance may increase additional training data amount training data relatively small bias significant factor 
word estimate may times actual parameter value 
amount training data small bias plays greater role ecting classification performance 
analysis variance shows point variance decreases monotonically word 
lessening variance contributes improved classification number training examples increases 
category word log odds ratio alt atheism atheism comp graphics jpeg comp os ms windows misc windows comp sys ibm pc hardware scsi comp sys mac hardware mac comp windows window misc sale rec autos car rec motorcycles bike rec sport baseball baseball rec sport hockey hockey sci crypt sci electronics circuit sci med patients sci space space soc religion christian god talk politics guns gun talk politics mideast talk politics misc talk religion misc god table category newsgroups dataset word highest log odds ratio 
larger score indicates word commonly specified category rarely categories 
words high log odds ratios discriminants vs problem 
danger imbalanced class training data observation classes little observed training data documents words yield high variance outputs 
words useful classification 
table gives list frequent class predictive words newsgroups data set 
gives sense frequency words occur 
table shows word greatest log odds ratio class newsgroups data set 
define log odds ratio logodds log log 
words high log odds ratio occur unusually frequently class occur class 
words correspond var log relatively large variances 
contrast var log 
larger amounts observed data yield smaller variances words occur frequently 
class little training data variance may greater classes 
theorem consider class classification problem 
log log 
assume var var 
var var var 
proof log log equation 
terms independent variance sum variances terms 
class higher variance classes variance dominate variance classification outputs 
ample training data yield estimates contribute little variance output dearth examples class contribute great variance 
performance naive bayes classifier easily dictated class smallest number examples 
benefit naive bayes receives additional training data marginal data distributed evenly classes 
chapter error correcting output coding error correcting output coding ecoc approach solving multiclass categorization problems originally introduced dietterich bakiri reduces multiclass problem group binary classification tasks combines binary classification results predict multiclass labels 
experimentally shown ecoc improve text classification naive bayes ghani berger give detailed results newsgroups industry sector data sets 
explain parameter estimate analysis predicts success failure map naive bayes conjunction ecoc 
certain ecoc classifiers outperform naive bayes 
performance binary classifiers ecoc scheme great impact multiclass performance 
perform su er examples relatively binary performance 
additionally experiment linear loss function find yields performance comparable best non linear loss function tried 
evidence text classification bag words representation linear problem 
note section say naive bayes referring map naive bayes dirichlet hyper parameters code matrix 
defines data splits binary classifier learn 
th row matrix defines code class th column matrix defines split classifier learn 
number classes number partitionings length code 
particular column represent assignment classes partitions 
di erent matrices vs ova matrix column filled entries dense matrix entries independently determined flipping fair coin assigning heads tails bch codes matrix construction technique yields high column row separation ghani bch codes ghani available line www cs cmu edu ecoc 
classifiers trained partitionings indicated code matrix 
furthermore chosen loss function 
multiclass classification new example argmin ci 
allwein give full description code matrix classification framework give loss functions various models hinge loss svm loss function svm optimized 
svm naive bayes optimize loss function 
find hinge loss function yields lower error logistic loss functions hinge loss naive bayes ecoc classifier 
additive models ecoc resides greater class models known additive models 
additive model classification form argmin ic ic arbitrary function data weights 
ecoc uses uniform weights 
name comes fact final output determined weighted summing outputs possibly non linear functions 
algorithms determine final output voting fall class algorithms 
fact ective way collection experts vote 
similar ecoc works 
creates handful experts specializes partitioning set classes particular way 
ecoc allows expert vote set classes believes example lie 
non binary loss functions votes weighted confidence expert 
additive aspect imposes linear constraint final output 
restricts expressiveness output function experts final output 
constraint expressiveness classifier long experts su ciently expressive 
relation boosting algorithms logistic regression softmax linear svm multiclass extension map naive bayes trivially additive models linear classifiers 
loss function ecoc may non linear individual classifiers linear 
model non linear additive model boosting 
boosting shares similarity ecoc composed separately trained binary classifiers 
original formulation boosting adaboost designed perform binary classification freund schapire adaboost composes binary classifiers experts di erent parts example space training classifier di erent weighted set examples 
multiclass case creation experts done partitioning class weighting individual examples 
ecoc specifies partitioning class multiclass boosting schemes adaboost oc adaboost ecc specify partitionings classes example space freund schapire guruswami multiclass boosting ecoc closely related multiclass boosting extension ecoc 
multiclass boosting specifies particular binary learner underlying weak learner unspecified imposes weights loss output binary learner 
multiclass boosting algorithms train binary classifiers function previous classifiers 
usually done ecoc 
main thrust boosting creation various meaningful binary sub problems 
multiclass case ecoc partitioning examples class 
classes give meaningful locations draw boundaries 
clear multiclass boosting schemes er advantage strong binary classifier ecoc 
support vector machine support vector machine classifier originally proposed vapnik finds maximal margin separating hyperplane classes data non linear extensions svm yang linear kernel outperform non linear kernels text classification 
informal experiments linear performs non linear kernels 
linear svm results 
smart ltc transform package running experiments rifkin introduce svm show results svm contrast naive bayes performance 
svm known perform case imbalanced training data theorem gives reason believe naive bayes handle imbalanced training data 
svm results give baseline grade naive bayes performance 
experiments table shows results ecoc experiments 
appendix describes preparations data set 
results averaged random train test splits data 
svm consistently performs better naive bayes binary classifier ecoc scheme 
degree di erence depends matrix type data set 
newsgroups svm nb svm nb svm nb svm nb ova dense bch dense bch dense bch industry sector svm nb svm nb svm nb svm nb ova dense bch dense bch dense bch table results multiclass classification experiments newsgroups top industry sector bottom data sets 
top row table indicates number documents class training 
second row indicates binary classifier 
far left column indicates multiclass technique 
entries table classification error 
ryan rifkin providing svm results 
number training examples newsgroups classification error bch ova nb number training examples industry sector classification error bch ova nb shown multiclass errors di erent classification algorithms 
ova refers ecoc vs matrix 
bch refers ecoc bch matrix 
naive bayes binary classifier ova bch plot 
nb refers regular naive bayes 
note ova nb follow similar trends ova outperforms nb small margin 
bch greatly outperforms ova nb industry sector marginally outperforms newsgroups 
note log scale axes 
success failure naive bayes compares performance ecoc ova regular nb ecoc bch 
note data sets performance ecoc ova regular nb follows consistent pattern di erent train set sizes regular nb consistently performs slightly worse ecoc ova 
back berger claim ecoc ova classification naive bayes similar regular naive bayes classification berger fact components binary classifiers simply individual components regular naive bayes classifier 
ova adds outputs compare 
additional information allows ova outperform nb somewhat 
ova tied performance regular naive bayes 
causes regular naive bayes perform poorly 
understand performance regular naive bayes return theorem 
theorem gives intuition regular naive bayes classifier worst component 
additional training examples reduce variance naive bayes classifier class fewest examples dictate performance classifier 
newsgroups training data industry sector classes 
class fewest training examples 
class training examples 
training levels classes fewer training examples respectively 
correlates improved performance ecoc bch 
bch matrix shows greatest gains ova nb largest number training examples 
case largest disparity number training examples di erent classes case theorem applicable 
number training examples class newsgroups ova performance nb binary error nb multi error number training examples class industry sector ova performance nb binary error nb multi error multiclass error improves number training examples increases binary error improves marginally industry sector degrades newsgroups 
shown performance ecoc ova naive bayes binary classifier 
ova binary classifiers lop sided example distribution guessing achieves binary error newsgroups industry sector 
binary error loosely tied binary classifier strength 
note log scale axes 
guess true tp fn label fp tn table performance binary classifier described confusion matrix shown 
letters describe entry 
stands true 
false 
positive 
negative 
detection rate tp tp fn 
false alarm rate fn tp fn 
rate fp tn fp 
roc breakeven average alarm rates di erence minimized 
multiclass error function binary performance performance ecoc classifier ected number factors binary classifier performance independence binary classifiers loss function 
find binary performance influential multiclass text classification 
error measure multiclass performance 
avoid binary error measure binary performance 
shows 
additional training examples yields improved multiclass error binary error rises falls training examples class newsgroups data set 
ova matrix partitions examples unevenly assigning examples single class 
error mainly judges classifiers performance examples class 
better measure evenly weights performance classes 
propose roc breakeven measure 
table shows terms describe output classifier 
define roc breakeven average number training examples class newsgroups bch performance svm roc nb roc svm multi error nb multi error number training examples class industry sector bch performance svm roc nb roc svm multi error nb multi error shown comparison roc breakeven multiclass error ecoc bch matrix svm naive bayes binary classifier 
see roc breakeven largely dictates multiclass error 
trends roc breakeven curves reflected multiclass error curves 
maximum number examples class 
note log scale axes 
false alarm rates point di erence false alarm rate rate minimum 
note precision recall breakeven roc breakeven achievable 
achieve di erent rates modifying bias term classifier 
roc breakeven selects bias classifier performs examples class examples class 
roc breakeven allows better judge strength binary classifier example distribution uneven 
example distribution roc breakeven nearly identical binary error 
gives comparison multiclass error roc breakeven ecoc classification bch matrix 
svm achieves lower roc breakeven data set correspondingly achieves lower multiclass error 
relationship roc breakeven multiclass error clear 
newsgroups relatively consistent relationship svm nb roc breakeven 
gap remains constant number training examples increases 
mirrored multiclass error 
svm outperforms nb consistent margin 
industry sector roc breakeven close training examples class quickly diverges 
multiclass error shows pattern 
svm nb multiclass errors close examples class examples class svm multiclass error just half nb multiclass error 
performance binary classifier great impact multiclass performance 
trends seen ecoc classification bch matrix repeated ova matrix results 
shows results 
industry sector svm roc breakeven improves quickly nb roc breakeven number training examples increases 
multiclass error follows suit decreasing error binary roc breakeven 
naive bayes lags multiclass error binary roc breakeven 
results newsgroups similar large di erences binary roc ect multiclass error 
number training examples class newsgroups vs performance svm roc nb roc svm multi error nb multi error number training examples class industry sector vs performance svm roc nb roc svm multi error nb multi error shown roc breakeven multiclass error ecoc ova matrix 
changes roc breakeven directly reflected multiclass error 
multiclass error changes gradually newsgroups trends roc breakeven evident multiclass error 
maximum number examples class 
note log scale axes 
lower roc breakeven yields lower multiclass error roc svm nb converge multiclass errors 
plots show clearly factors binary performance 
example roc breakeven naive bayes industry sector data set examples class yields multiclass error roc breakeven svm examples yields multiclass error 
svm higher multiclass error roc breakeven lower 
due correlation binary classifiers 
examples class svm classifiers produce identical labels training data available 
example average pair example svm binary classifiers trained ova split data produce label time 
average pair nb binary classifiers trained examples produce label time 
greater independence classifiers allows lower multiclass error ecoc scheme binary classifiers show higher roc breakeven scores 
full binary error roc breakeven results table 
seen figures seen table roc breakeven correlated multiclass error 
factors identical nb svm roc yield identical multiclass errors 
trends roc breakeven clearly reflected multiclass error 
case binary error ova matrix roc breakeven binary error di er 
roc breakeven clearly indicator multiclass performance better judges strength classifier example distribution skewed 
newsgroups svm nb svm nb svm nb svm nb ova error ova roc bch error bch roc industry sector svm nb svm nb svm nb svm nb ova error ova roc bch error bch roc table shown binary errors roc breakeven points binary classifiers trained matrix columns 
results dense matrix omitted nearly identical bch results 
table entries averaged matrix columns train test splits 
error poor judge classifier strength ova matrix 
error increases examples newsgroups 
note error roc breakeven numbers similar bch matrix 
newsgroups hinge linear ova svm ova nb bch svm bch nb industry sector hinge linear ova svm ova nb bch svm bch nb table shown multiclass errors data sets variety ecoc classifiers 
errors nearly identical hinge linear loss functions 
ecoc provides opportunity non linear decision rules loss function non linear loss function provides practical benefit 
non linear loss ect ecoc performance factor greatly impact ecoc multiclass error loss function 
hinge function experiments exhibits nonlinearity 
loss function allows ecoc express functions linear classifiers naive bayes linear svm express 
fact ecoc non linear provide empirical benefit experiments 
table shows results experiments ran compare hinge loss function trivial linear loss function 
find practically di erence multiclass error compared hinge loss function 
results show maximum number training examples class industry sector class newsgroups results similar fewer training examples 
confidence information contributed loss function important text classification non linearity provides practical benefit 
linear loss function yields completely linear system nb svm classifiers linear 
contributes evidence text classification bag representation linear problem 
chapter feature selection feature selection essential part text classification 
document collections unique words 
words useful classification 
restricting set words classification classification cient improve generalization error 
describe application information gain feature selection multiclass text classification fundamentally flawed compare statistics algorithm exhibits similar di culties 
text feature selection algorithm select features drawn distribution distant class neutral distribution 
algorithms 
describe framework feature selection encapsulates notion exposes free parameters inherent text feature selection 
framework provides basis new feature selection algorithms clarifies intent design algorithms 
information gain information gain ig commonly score selecting words text classification joachims mccallum nigam yang pedersen mitchell derived information theoretic notions 
word ig measures entropy di erence unconditioned class variable class variable conditioned presence absence word ig log 
score equivalent mutual information class word variables ig 
score called mutual information 
probabilities correspond individual word occurrences 
corresponds occurrence word corresponds occurrence word 
treat token data binomial event estimate probabilities equation maximum likelihood 
number occurrences word class 

ig log log 
feature selection ig computed word words larger scores retained 
hypothesis testing desirable property feature distribution highly dependent class 
words occur independent class give information classification 
natural approach developing metric filtering features determine word class independent distribution eliminate word distribution 
statistics problem determining data generated particular distribution known hypothesis testing 
proposes model parameters ranks data likelihood 
text feature selection call feature selection score ht 
consider single word treat appearances training data draws multinomial event class label 
hypothesized parameters 
parameters correspond word occurrence irrelevant class multinomial model 
test statistic determines ordering data di erence log likelihoods maximum likelihood estimate hypothesized parameters ht log 
ht larger ht values correspond data generated proposed model 
keep words large ht values discard words small ht values 
note score similar ig score 
generalization advantage significance level common feature selection performed terms number features 
example ig score usually select ig cuto eliminate words ig score 
ranks words ig score retains top scoring words 
number words retained particular application varies data set 
example mccallum nigam best multinomial classification accuracy newsgroups data set achieved entire vocabulary words contrast best multinomial performance interest cate gory reuters data set achieved words 
advantage ht score number words selected specified terms significance level 
ht cut chosen cuto ht score 
significance level corresponding ht cut sl pr ht ht cut sample estimate 
fixed variable 
sl selects words empirical distributions occur draws hypothesis distribution selected words atypical class neutral distribution 
intuitive simply selecting ht ig cuto may allow generalization di erent data sets conditions 
significance level choose number words feature selection gives easy interpret understanding words retained 
undesirable properties ig ht application ig ht text classification ignores critical aspects text 
words occur sparsely provide information occur 
ig expects word provide information occur 
ig ht tendency give higher scores words occur 
example ht 
draws ht score 
words devoid class information empirical distributions 
high score ig ht provide significant reduction entropy little chance drawn hypothesis distribution 
fact true distribution probably close hypothesized distribution ignored ig ht 
word occurs just times high ig ht score non occurrences provide little information extreme empirical distribution relatively common draw hypothesis distribution 
example chance observing draws multinomial parameters 
appearance single word predict class baseball document 
non appearance rarely informative won appear baseball documents 
text feature selection algorithm retain words appearance probably highly predictive class 
sense want words discriminative 
simple discriminative feature selection simple score selecting discriminative features argmax new feature selection framework views text feature selection problem finding words empirical distribution represented true distribution class independent distribution dashed arrows point distributions drawn 
probability class appearance word gives largest score words appear single class 
word appears document know doubt class document belongs 
find estimate map estimate dirichlet prior gives argmax 
setting hyper parameters encode preference uniform distribution reason believe choices appropriate 
choice prior important serves measure confidence empirical distribution 
prior dirichlet prefers class neutral distribution cn estimate word lies line connecting prior dictates close estimate number draws 
new feature selection framework simple score describe selects discriminative features limiting imposes specific distance metric 
describe framework text feature selection exposes parameters feature selection method explicit 
gives visual description framework 
develop framework extend ht important ways 
introduce ball hypothesis distribution 
serves define distributions nearly 
second define metric measuring distances distributions 
determine distribution ball nearest empirical distribution 
near distribution ball nearest ht judges possibility drawn new framework evaluate probability drawn near select words distributions outside ball distributions far class independent distribution 
new feature selection framework parameters define set distributions close metric determine near significance level sl comparing empirical true distributions 
hypothesis test score define significance level near near 
cuto choice significance level defined sl pr near cut sample estimate near 
word selected near cut cut defined chosen significance level near distribution ball closest defined near 
words empirical distributions near discarded smaller cuto larger significance level 
include discriminative words ig ht number selected features 
new framework exposes fundamental parameters variables text feature selection scheme word appearances 
compensates fact words truly drawn multinomial eliminating words close class neutral distribution 
sl allows user select amount evidence required show word drawn class neutral distribution 
defines closeness distributions specifies distribution empirical distributions compared 
new framework selects words drawn discriminative distribution 
ht ig accounts fact text multinomial empirical distributions close class neutral distribution informative respect class variable 
chapter focus thesis application naive bayes multiclass text classification resulted new insights 
parameter estimate analysis shows naive bayes performs poorly class relatively examples 
empirically showed ecoc performance mainly result binary performance 
binary classifiers ecoc su cient examples ecoc performs better regular naive bayes 
furthermore showed commonly text feature selection algorithm multiclass text classification judges words non appearances bias words appear 
proposed select features distribution discriminative gave framework exposes free parameters scheme 
terms choice prior greatly ect classification especially words observations choice understood 
better selection prior may lead improved classification performance 
observed linear classifiers perform better non linear classifiers text classification bag words representation 
determining generally true understanding case important 
ecoc experiments performance particular matrix varied data set amount training data 
additional gains may possible developing algorithms successively tune columns ecoc matrix specific problem 
envision able unlabeled data em counter limiting ect classes labeled examples 
appendix data sets experiments di erent commonly data sets mccallum nigam slonim tishby berger ghani mccallum rainbow pre process documents newsgroups data set collected originally text classification lang lang contains non empty documents evenly distributed categories representing newsgroup 
remove headers uu encoded blocks words occur data 
vocabulary size 
randomly select documents class training remaining testing 
pre processing splitting mccallum nigam newsgroups experiments mccallum nigam industry sector data collection corporate web pages organized categories produces nigam non empty documents categories 
remove headers prune stoplist words words occur 
include html experiments 
find regular naive bayes ecoc ova naive bayes better html removed 
di erence change note 
vocabulary size 
randomly select documents class training remaining testing 
create subsets training set observe ects varying amounts training data 
similar pre processing splitting ghani industry sector experiments ghani di erence ghani excluded html pre processing 
text classification experiments include feature selection step may improve classification 
mccallum nigam performed feature selection experiments modified version industry sector data set newsgroups data set case feature selection significantly improve classification experiments information gain feature selection improve classification 
full vocabulary experiments 
bibliography allwein erin allwein robert schapire yoram singer 
reducing multiclass binary unifying approach margin classifiers 
journal machine learning research 
berger adam berger 
error correcting output coding text classification 
proceedings ijcai workshop machine learning information filtering stockholm 
chakrabarti soumen chakrabarti byron dom rakesh agrawal prabhakar raghavan 
taxonomy discriminants signatures navigating text databases 
proceedings rd vldb conference 
cover thomas thomas cover joy thomas 
elements information theory 
john wiley sons 
dietterich bakiri tom dietterich bakiri 
error correcting output codes general method improving multiclass inductive learning programs 
proceedings ninth national conference artificial intelligence pages anaheim ca 
aaai press 
domingos pazzani pedro domingos michael pazzani 
independence conditions optimality simple bayesian classifier 
proceedings thirteenth international conference machine learning icml 
dumais dumais john platt david heckerman mehran sahami 
inductive learning algorithms text classification 
seventh international conference information knowledge management 
ferguson ferguson 
bayesian analysis nonparametric problems 
annals statistics pages 
freund schapire yoav freund robert schapire 
experiments new boosting algorithm 
proceedings thirteenth international conference machine learning 
freund schapire yoav freund robert schapire 
short boosting 
journal japanese society artificial intelligence 
mccallum dayne andrew mccallum 
information extraction hmms shrinkage 
proceedings aaai workshop machine learning information extraction 
ghani ghani 
error correcting codes text classification 
proceedings seventeenth international conference machine learning 
guruswami venkatesan guruswami amit 
multiclass learning boosting error correcting codes 
proceedings twelfth annual conference computational learning theory 
joachims thorsten joachims 
text categorization support vector machines learning relevant features 
technical report university dortmund computer science department 
lang ken lang 
newsgroups 
www ai mit edu people newsgroups 
lang ken lang 
newsweeder learning filter netnews 
proceedings twelfth international conference machine learning pages 
lewis david lewis 
naive bayes independence assumption information retrieval 
proceedings tenth european conference machine learning 
mccallum nigam andrew mccallum kamal nigam 
comparison event models naive bayes text classification 
proceedings aaai workshop learning text categorization 
mccallum andrew mccallum 
bow toolkit statistical language modeling text retrieval classification clustering 
www cs cmu edu mccallum bow 
mitchell tom mitchell 
machine learning 
mcgraw hill companies 
nigam kamal nigam 
industry sector data 
www cs cmu edu datasets html 
rifkin ryan rifkin 

percent nation mit edu 
eric 
natural law succession 
technical report cs tr princeton university 
slonim tishby noam slonim naftali tishby 
agglomerative information bottleneck 
neural information processing systems nips 
vapnik vladimir vapnik 
nature statistical learning theory 
springerverlag 
yang pedersen yiming yang pedersen 
comparitive study feature selection text categorization 
proceedings fourteenth international conference machine learning 

