efficient routing peer peer overlays gupta barbara liskov rodrigo rodrigues mit computer science artificial intelligence laboratory liskov rodrigo csail mit edu current peer peer lookup schemes keep small amount routing state node typically logarithmic number overlay nodes 
design assumes routing information member node kept small bookkeeping required respond system membership changes small aggressive membership dynamics expected 
consequence lookups high latency lookup requires contacting nodes sequence 
question assumptions presenting peer peer routing algorithms small lookup paths 
hop routing scheme 
show disseminate information membership changes quickly nodes maintain accurate routing tables complete membership information 
deduce analytic bandwidth requirements scheme demonstrate feasibility 
propose hop routing scheme large scale systems nodes bandwidth requirements hop routing large 
scheme keeps fixed fraction total routing state node chosen hop low latency additional delay small 
validate analytic model simulation results show algorithms maintain routing information sufficiently date large fraction queries succeed re routed 
structured peer peer overlays chord pastry tapestry provide substrate building large scale distributed applications :10.1.1.111.1818:10.1.1.142.752:10.1.1.105.3673
overlays allow applications locate objects stored system limited number overlay hops 
peer peer lookup algorithms strive maintain small amount node routing state typi cally log designers expect system membership changes frequently 
expectation confirmed successfully deployed systems 
study shows average session time gnutella hours 
equivalent saying system nodes membership change events second 
maintaining small tables helps keep amount bookkeeping required deal membership changes small 
price pay having small amount routing state node lookups high latency lookup requires contacting nodes sequence 
questions need keep routing state small 
take position maintaining full routing state complete description system membership viable large system containing nodes 
techniques show systems size nodes maintain membership information accurately communication costs low 
results imply peer peer system route efficiently system large membership changing rapidly 
novel peer peer lookup system maintains complete membership information node 
show analytic results prove system meets goals reasonable accuracy bandwidth usage 
course easy achieve goals small systems 
algorithm designed scale large systems 
analysis shows hop routing systems millions nodes 
analysis shows nodes bandwidth requirements scheme large 
design hop lookup scheme overcomes problem provides faster lookups existing peer peer routing algorithms 
analytic model hop system conclude bandwidth requirements reasonable systems tens millions nodes 
presents simulation results corroborate analytic models predict 
show performance degrade significantly system larger smaller due aggressive system dynamics 
rest organized follows 
section presents system model 
sections describe hop hop routing schemes respectively 
section evaluates system 
conclude discussion accomplished 
system model consider system nodes large number assume dynamic membership behavior gnutella representative open internet environment 
study gnutella napster deduce systems nodes show membership changes second respectively 
call rate refer membership changes events rest 
node overlay assigned random bit node identifier 
identifiers ordered identifier ring modulo assume identifiers generated resulting set uniformly distributed identifier space example setting node identifier cryptographic hash network address 
node predecessor successor identifier ring periodically sends keep alive messages nodes 
similarly item key identifier ring 
responsibility item providing storage rests successor node identifier ring clockwise key 
mapping keys nodes chord changing system mappings straightforward :10.1.1.105.3673
clients issue queries try reach successor node particular identifier 
intend system satisfy large fraction queries correctly attempt attempt requires hops depending scheme 
goal support high values 
query may fail attempt due membership change notification change reached querying node 
case query rerouted succeed higher number hops 
define failed queries answered correctly attempt objec tive hop lookups depending algorithm 
hop lookups section presents design analysis hop scheme 
scheme node maintains full routing table containing information node overlay 
actual query success rate depends accuracy information 
section describes algorithm handles membership changes convey information changes nodes ring 
section explains algorithm reacts node failures presents informal correctness argument approach 
section discusses issues asymmetry load individual nodes 
section presents analysis bandwidth requirements scheme 
membership changes membership changes nodes joining leaving ring raise important issues algorithm address 
update local information membership change order node system determine precisely interval id space responsible 
second issue conveying information change nodes ring nodes maintain correct information system membership consequently manage route single hop 
maintain correct local information information node successor predecessor node node runs stabilization routine periodically sends keep alive messages successor predecessor node checks predecessor notifies existence node 
similarly checks successor notifies respond pings repeatedly time period decides node unreachable dead 
joining node contacts system node get view current membership protocol similar chord protocol :10.1.1.105.3673
membership information enables get touch predecessor successor informing presence 
maintain correct full routing tables notifications membership change events joins leaves reach node system specified amount time depending frac slice leader unit leader ordinary node 
flow event notifications system tion failed queries deemed acceptable 
goal way low notification delay reasonable bandwidth consumption bandwidth resource system 
achieve goal superimposing welldefined hierarchy system 
hierarchy form dissemination trees propagate event information 
impose hierarchy system dynamic membership dividing bit circular identifier space equal contiguous intervals called slices 
ith slice contains nodes currently overlay node identifiers lie range 
nodes uniformly distributed random identifiers slices number nodes time 
slice slice leader chosen dynamically node successor mid point slice identifier space 
example slice leader ith slice successor node key new node joins system learns slice leader neighbors information data responsible routing table 
similarly slice divided equal sized intervals called units 
unit unit leader dynamically chosen successor mid point unit identifier space 
depicts information flows system 
node labeled detects change membership successor failed new successor sends event message slice leader 
slice leader collects event notifications receives slice aggregates seconds sending message slice leaders 
spread bandwidth utilization communication different slice leaders synchronized slice leader ensures communicates individual slice leader seconds 
messages different slice leaders sent different points time contain different sets events 
slice leaders aggregate messages receive short time period twait dispatch aggregate message unit leaders respective slices 
unit leader piggybacks information keep alive messages successor predecessor 
nodes propagate information direction receive information predecessors send successors vice versa 
information piggy backed keepalive messages 
way nodes system receive notification events unit information flowing unit leader ends unit 
nodes unit boundaries send information neighboring nodes outside unit 
result redundancy communications node get information neighbor step closer unit leader 
get benefits choosing design 
imposes structure system defined event dissemination trees 
structure helps ensure redundancy communications leads efficient bandwidth usage 
second aggregation events message allows avoid small messages 
small messages problem protocol overhead significant relative message size leading higher bandwidth usage 
effect analyzed detail section 
scheme level hierarchy 
choice number levels hierarchy involves tradeoff large number levels implies larger delay propagating information small number levels generates large load nodes upper levels 
chose level hierarchy low delay bandwidth consumption top level nodes reasonable 
fault tolerance query fails attempt return error application 
queries rerouted 
lookup query node node fails longer system retry query sending successor 
query failed joined node new successor key looking reply identity knows contact second routing step 
scheme dependent correct functioning slice leaders need recover failure 
relatively slice leaders failures infrequent 
aggressive replacing order maintain query success target 
slice unit leader fails successor soon detects failure new leader 
time slice unit leader fails new node takes event notification messages may lost information membership changes reflected system nodes membership tables 
issue routing correctness node maintains correct information predecessor successor 
lead routing hops allowed errors accumulate eventually lead degradation hop lookup success rate 
avoid accumulation lookups detect propagate inaccuracies 
node performs lookup detects routing entry incorrect lookup timed re routed new successor new information pushed system nodes normal channels notifies slice leader event 
correctness protocols fact successor predecessor pointers correct 
ensures remainder membership information contains errors query eventually succeed re routing 
words complete membership description seen optimization successor pointers way chord fingers optimization successors similarly peerto peer routing schemes 
furthermore argue successor predecessor pointers correct due fact essentially follow protocol chord maintain proven correct 
scalability slice leaders nodes problem poorly provisioned node low bandwidth connection internet 
overcome problem identify connected provisioned nodes supernodes entry system 
parallel ring supernodes successor supernode ring midpoint slice identifier space slice leader 
require sufficient number supernodes ensure slice 
show section bandwidth requirements small participants system potential supernodes sized system system slice leaders require kbps upstream bandwidth 
system may require supernodes academic corporate users bandwidth requirements increase kbps 
section presents hop scheme may required wish system accommodate larger memberships 
analysis section presents analysis parameterize system satisfy goal fast propagation 
achieve desired success rate need propagate information events time period section showing compute quantity 
require performance especially respect bandwidth utilization 
section show satisfy requirement controlling number slices units 
analysis considers non failure situations 
take account overheads slice unit leader failure events rare 
ignores message loss delay simplifies presentation overhead introduced message delays retransmissions small compared costs system 
analysis assumes query targets distributed uniformly ring 
worst case pattern events queries notifications assume events happen just slice leader notifications queries happen immediately affected routing table entries corrected queries targeted nodes nodes causing events fail 
real deployment queries interleaved events notifications fewer fail 
scenario illustrated timeline 
twait frequency slice leaders communicate unit leaders time takes propagate information unit time slice leader events queries wait resp 
slices know slice leaders know unit leaders know small big wait small nodes know 
timeline worst case situation waits communications slice leader 
twait seconds point slices events occurred correct entries nodes affected respective events 
seconds events point slice leaders notify slice leaders 
twait seconds point nodes system receive notification events 
twait 
quantity represents delay time event occurs leader slice learns 
configuration parameters parameters characterize system deployment 
acceptable fraction queries fail routing attempt 
expected number nodes system 
expected rate membership changes system parameters compute 
assumption query targets distributed uniformly ring implies fraction failed queries proportional expected number incorrect entries querying node routing table 
worst case assumption entries concerning events occurred seconds incorrect fraction failed queries ensure fraction queries fail need system nodes rate events get time interval large propagate information 
note linearly proportional independent function desired success rate 
slices units system performance depends number slices units 
number slices ring divided 

number units slice 
parameters determine expected unit size 
turn determines time takes information propagate unit leader members unit assumption frequency keep alive probes 
determine calculated value choices values twait 
recall twait 
simplify analysis choose values twait 
result analysis concerned just independent variables particular choice values second twait 
reasonable decision amount data sent probes messages unit leaders large overhead messages small information events sent system nodes 
note choice half unit size 
seconds account delay detecting missed keep alive message probes confirm event 
cost analysis goal choose values way reduces bandwidth utilization 
particular concerned minimizing bandwidth slice leaders approach 
bandwidth consumed propagate actual data message overhead 
bytes required describe event overhead message types communication system 

keep alive messages keep alive messages form base level communication node predecessor successor 
messages include information events 
described section system avoids sending redundant information messages controlling direction information flow unit leader unit members sending information unit boundaries 
upstream downstream slice leader unit leader nodes table 
summary bandwidth keep alive messages sent second node edge unit send acknowledge aggregate message containing average events 
size message size acknowledgment 
event notification slice leaders node detects event sends notification slice leader 
expected number events second slice downstream bandwidth utilization slice leaders message acknowledged upstream utilization 
messages exchanged slice leaders message sent slice leader batches events occurred seconds slice 
typical message size bytes 
period slice leader sends message slice leaders receives acknowledgment 
slice leader receives gets average upstream downstream bandwidth symmetric 
bandwidth tion upstream downstream 
messages slice leaders unit leaders messages received slice leader batched second forwarded unit leaders 
second events happen aggregate message size bandwidth utilization table summarizes net bandwidth node 
clarify presentation removed insignificant terms expressions 
formulas compute load non slice leaders particular configuration 
computations bytes bytes 
system nodes see bandwidth kbps upstream downstream 
bandwidth slice leader load ordinary node kbps load unit leader kbps upstream kbps downstream 
system nodes numbers kbps kbps kbps respectively 
table clear upstream bandwidth required slice leader dominating limiting term 
shall choose parameters minimize bandwidth 
simplifying expression interrelationship explained section get function depends independent variables analyzing function deduce minimum achieved values twait formulas allow compute values example system nodes want roughly slices containing units 
system nodes units slice slices 
values compute unit size turn allows compute 
find bandwidth choose seconds seconds 
values formulas table plot bandwidth usage slice leader overhead 
aggregate bandwidth overhead scheme percentage theoretical optimum systems various sizes 
results calculation shown 
note load increases linearly size system 
load quite modest system nodes kbps upstream bandwidth nodes cable modems act slice leaders system 
system nodes upstream bandwidth required slice leader approximately kbps 
appropriate limit slice leaders machines reasonably provisioned local area networks 
larger networks bandwidth increases point slice leader need provisioned node 
shows percentage overhead scheme terms aggregate bandwidth system respect hypothetical optimum scheme zero overhead 
scheme scheme cost just total bandwidth sending events node system second note overhead system comes message protocol overhead 
scheme propagate redundant information 
note overhead approximately system containing nodes goes system containing nodes 
result reasonable messages get larger overhead significant system size increases 
hop lookups scheme previous section works systems large nodes 
systems larger size bandwidth requirements scheme may large significant fraction nodes 
section propose hop scheme 
scheme keeps fixed fraction total routing state node consumes bandwidth scales larger system size 
presenting algorithm design section 
section analyzes bandwidth requirements scheme 
system design design routing algorithm routes hops structure hop scheme slices units respective leaders described previously 
addition slice leader chooses group nodes slice groups 
group size groups may chosen randomly may proximity metrics group may chosen members dispersed terms network location way approximates network spread members slice 
slice leader sends routing information group exactly slice leader 
information group disseminated members slice hop scheme 
node routing information exactly nodes slice 
node maintains ordering sending probes nodes network distance 
maintains ordering slice builds table nodes close slice 
addition node keeps full routing information nodes slice 
node wants query successor key sends lookup request chosen node slice containing key 
chosen node examines routing table identify successor key forwards request node 
rest shall refer chosen intermediate nodes forwarding nodes 
information flow system similar saw hop lookup algorithm 
difference occurs slice leader sends slice leaders step 
message sends ith slice leader empty nodes previously sent slice leader left system 
case sends information nodes left system nodes chooses replace 
node learns different membership slice probes nodes just heard updates proximity information slice 
tolerating slice unit leader failure works similarly hop case 
analysis section presents analysis parameterize system satisfy goal fast propagation 
analysis take account overheads slice unit leader failure events rare 
ignores message loss delay proximity probes simplifies presentation overhead introduced probes message delays retransmissions small compared time constants system 
analysis assumes query targets located uniformly random ring 
worst case pattern queries notifications 
ways query fail 
forwarding node failed querying node aware event 
second successor key queried changed forwarding node aware event 
probability query failing depends events may independent 
assume upper bound failure probability sum probabilities events 
time taken spread information event slice depends unit size call 
time taken spread information event nodes system twait 
average locations ring probability query failure leave forwarding node approximately twait average probability query failure change key successor twait expected fraction failed queries upper bounded twait 
ensure fraction queries fail need twait example system nodes rate events require twait seconds 
note linearly proportional inequality independent function desired success rate 
choose seconds twait seconds seconds 
choice leaves interval seconds detection join leave event 
keep alive messages exchanged second implies expected size unit 
control upstream bandwidth utilization slice leaders fix number units slice 
implies expected size slice ring divided slices 
terms bandwidth costs need account fact dealing small messages need consider protocol overheads 
assume bytes required describe event overhead message types communication system 
keep alive messages keep comprise base level communication node predecessor successor 
messages include information events node slice exported nodes slices 
described section system avoids sending redundant information messages controlling direction information flow unit leader unit members sending information unit boundaries 
keep alive messages sent second node edge unit send acknowledge aggregate message containing average events 
size message size 
event notification slice leaders identical hop case 
node detects event sends notification slice leader 
expected number events second slice downstream bandwidth utilization slice leaders fore message ac upstream utilization 
messages slice leaders message sent slice leader contains information changes exported nodes 
expected message size bytes 
seconds period slice leader sends message slice leaders receives acknowledgment 
slice leader receives gets average upstream downstream bandwidth symmetric 
bandwidth utilization upstream stream upstream downstream slice leader mbps kbps unit leader kbps bps ordinary node bps bps table 
summary bandwidth system size 
messages slice leaders unit leaders messages received slice leader batched second forwarded unit events happen leaders 
second slice events exported 
units slice band width utilization formulas compute load nodes system size appropriate choice system size may choose approximately 
slices large expect group size allow node able find node slice close terms network proximity groups populated randomly 
hop lookup low latency hop bringing total routing delay 
algorithms clustering nodes basis network proximity may fixed depending size number clusters 
computations bytes bytes 
table summarizes net bandwidth node system size having events second 
load slice leaders increases linearly size system 
scheme scale system half nodes 
evaluation section experimental results obtained simulations hop hop schemes 
set experiments coarse grained simulator understand behavior hop systems tens thousands nodes 
simulation scales approximately nodes 
second set experiments fine grained simulation hop system simulation environment support nodes 
experiments derived inter host la query failure rate observed failure rate interval successive inter slice communications seconds 
query failure rate hop system size changing inter slice communication frequency internet measurements set dns servers 
note experimental results largely independent topology measure lookup latency influenced distance forwarding node hop scheme impact inter node latency query failure rates minimal latencies order magnitude smaller timeouts propagating information 
coarse grained simulations experiments coarse grained simulator aimed validating analytic results concerning query success rate 
coarse grained simulator simplifying assumptions allow scale larger network sizes 
synchronous simulation proceeds series rounds representing second processing nodes receive messages perform local processing messages send messages nodes 
second case simulate slice leader failures 
packet losses simulated 
set experiments shows fraction successful queries function communication rate 
expected number nodes system mean join rate nodes second mean leave rate nodes second 
node lifetime exponentially distributed 
new nodes queries distributed uniformly id space 
query rate query node second 
hop scheme number slices chosen units slice query failure rate interval successive inter slice communications seconds 
query failure rate hop system size changing inter slice communication frequency choices analysis 
frequency inter slice communication varied seconds seconds 
results shown 
see query failure rate grows steadily time inter slice communication 
note relatively infrequent communication seconds obtain average failure rate 
simulation confirms expectation failed query rate computed analytically conservative 
see inter slice communication set value suggested analysis query failure rate conservatively predicted section 
reason actual failure rate lower analysis assumed worse case queries issued right membership events occur events propagated 
reality queries distributed time interval takes propagate information time queries issued nodes received todate information 
shows similar experiment simulation hop scheme 
expected number slices system chosen bandwidth optimal slice count 
similarly number units slice chosen choice comes analysis 
comparing figures see hop scheme causes lower fraction failed queries hop scheme 
happens reasons 
hop scheme hop fails fraction query failures hop query failure rate query failure rate rerouting time seconds 
query failure rate hop system size forwarding node fails node joins cause routing failures case 
second hop succeed single hop hop case choice target node forwarding node date information slice 
points explain hop system lower sensitivity change frequency inter slice communications hop system 
fine grained simulations section report simulations hop routing algorithm psim peerto peer protocol simulator implement complete functionality hop protocol 
simulator able explore bandwidth utilization impact slice unit leader failures 
experiment measure evolution query failure rate system grows fast initially stabilizes typical membership dynamics 
simulate system dynamic nodes slices units slice 
results shown 
seconds simulation nodes join rapidly 
system shows gnutella churn events minute 
nodes join obtaining routing table node routing tables continue grow new nodes come 
approximately minutes query failure rate stayed consistently 
experiments determine failure rate observed query re routed 
case failure rate settles approximately failure queries 
fraction query failures hop query failure rate time seconds 
query failure rate hop system size nodes crash local information slice especially knowledge predecessors successors gets transmitted rapidly 
queries correctly satisfied fewer hops 
examine behavior system presence burst membership departures 
simulated system nodes slices units slice 
query rate lookup second node 
time instant seconds nodes system crash 
nodes chosen randomly 
shows fraction lookups failed subsequent crash 
takes system seconds return reasonable query success rate doesn stabilize query success rate prior crash seconds 
happening interval recovery slice leader failures 
query rate important role slice leader recovery queries help restoring stale state regenerating event notifications lost slice leader failures 
example query rate lookup seconds system stabilize length simulation seconds query rate lookups second system stabilized seconds 
indicates may useful artificially insert queries system periods inactivity 
shows bandwidth system period 
aggregate bandwidth entire system kbps crash settles kbps approximately seconds 
steady state bandwidth decreases due decrease system size 
see duration bandwidth kbps time seconds bandwidth 
bandwidth hop system size nodes crash spike similar spike query failure rate bandwidth settles expected levels faster successful lookups 
happens lookups continue fail routing table entries notifications lost slice leader failures 
failures re discovered lookups fixed slice slice takes longer 
failed lookup generates notifications increases maintenance bandwidth system size messages spike settles dominated udp ip packet overhead 
effect bandwidth significantly lower 
ran experiments simulated bursts crashes different fractions nodes 
observed time periods taken lookups bandwidth settle cases 
expect happen time taken stabilize system dependent system size chosen parameters unit size remain cases 
computed average spike bandwidth 
measured computing average bandwidth entire system seconds took system settle cases 
see bandwidth grows approximately linearly size crash 
cases bandwidth consumption reasonable fact bandwidth split nodes 
discussion section discuss features incorporate algorithms may 
bandwidth kbps average spike bandwidth fraction nodes crashing burst 
average spike bandwidth fraction nodes crash burst proximity existing peer peer routing algorithms pastry tapestry carefully exploit internode proximity choosing node routing table entries :10.1.1.111.1818:10.1.1.142.752:10.1.1.142.752
trying populate node routing tables nearby nodes routing process simplified shorter routing hops 
hop scheme require proximity routing proximity information practice scheme 
hop scheme mentioned section proximity exploited improve routing 
improve algorithms proximity information forming dissemination trees case formed randomly chosen slice unit leaders 
main improvement comes improving dissemination information slice 
think inter slice communication small part load slice leaders chosen connected nodes point trying improve situation 
disseminating information slice improve current scheme application level multicast technique takes proximity account 
peer peer technique scribe bayeux traditional technique srm rmtp appropriate 
caching load balancing previous peer peer systems exploited fact queries key different clients lookup paths overlap final segments perform caching objects returned nodes contacted lookup path 
provided natural way perform load balancing popular content cached longer client obtain content cached copy lookup path 
hop scheme similar scheme provide load balancing caching 
lead popular items cached forwarders accessed hop note added benefit querying node usually proximity forwarder 
hop scheme doesn extra routing steps load balancing 
hop routing combined caching schemes achieve load balancing nearby access desired 
addition load balancing achieved application level advantage replication 
peer peer system data replicated order avoid loss nodes depart 
query take advantage replication retrieve item replica proximate querying node 
related rodrigues proposed single hop distributed hash table assumed smaller peer dynamics corporate environment deal difficulties rapidly handling large number membership changes efficient bandwidth usage 
douceur system routes constant number hops design assumes smaller peer dynamics searches lossy 
kelips uses sized tables node gossip mechanism propagate event notifications provide constant time lookups 
lookups constant time routing table entries reasonably accurate 
seen systems highly dynamic accuracy tables depends long takes system converge event 
expected convergence time event kelips log 
tens seconds small systems nodes systems having nodes takes hour event propagated system 
rate large fraction routing entries table stale correspondingly large fraction queries fail attempt 
mahajan derive analytic models cost maintaining reliability pas try peer peer routing algorithm dynamic setting :10.1.1.142.752
differs substantially nature routing algorithms quite different pastry uses log state requires log hops lookup focus techniques reduce low maintenance cost 
liben nowell provide lower bound cost maintaining routing information peer peer networks try maintain topological structure 
designing system requires significantly larger bandwidth lower bound aim achieve lower lookup latency 
alternative routing scheme 
scheme queries routed equivalent slice leaders ordinary nodes exchange state 
hop scheme gives querying node different possibilities forwarding node allows employ clever techniques decide forwarding node proximity 
questions necessity multi hop lookups peer peer routing algorithms 
introduce design novel peer peer lookup algorithms 
algorithms route hops respectively lookup fails routes need attempted 
designed algorithms provide small fraction lookup failures 
analytic results show parameterize system obtain reasonable bandwidth consumption despite fact dealing highly dynamic membership 
simulation results support analysis system delivers large fraction lookups hops depending algorithm 
previous peer peer systems exploited fact queries id different clients lookup paths overlap final segments perform caching objects returned nodes contacted lookup path 
provided natural way perform load balancing popular content cached longer client obtain content cached copy lookup path 
hop algorithm similar scheme provide load balancing caching 
investigating ways obtain similar advantages hop scheme 
currently peer peer systems high lookup latency suited applications mind high latency store retrieve operations backups store retrieve massive amounts data source tree distribution 
moving efficient routing removes constraint 
way enable larger class applications peer peer systems 
acknowledgments shepherd ion stoica anonymous referees helpful comments drafts 
research supported darpa contract nsf cooperative agreement ani 
rodrigues supported fellowship foundation previously supported praxis xxi fellowship 
castro druschel 
kermarrec rowstron 
scribe large scale decentralised application level multicast infrastructure 
ieee journal selected areas communications jsac 
douceur adya bolosky simon theimer 
reclaiming space duplicate files serverless distributed file system 
proceedings nd ieee international conference distributed computing systems july 
floyd jacobson 
liu mccanne zhang 
reliable multicast framework light weight sessions application level framing 
ieee acm transactions networking 
gil kaashoek li morris stribling 
psim simulator peer peer protocols 
www pdos lcs mit edu psim 
gummadi saroiu gribble 
king estimating latency arbitrary internet hosts 
proceedings sigcomm internet measurement workshop marseille france november 
gupta birman demers van renesse 
kelips building efficient stable dht increased memory background overhead 
proceedings nd international workshop peer peer systems iptps feb 
liben nowell balakrishnan karger 
analysis evolution peer peer systems 
proceedings annual acm symposium principles distributed computing podc 
lin paul 
rmtp reliable multicast transport protocol 
proceedings ieee pages san francisco ca mar 
mahajan castro rowstron 
controlling cost reliability peer peer overlays 
proceedings nd international workshop peer peer systems iptps feb 
cheng kumar savage 
structured superpeers leveraging heterogeneity provide constant time lookup 
ieee workshop internet applications 
ratnasamy francis handley karp shenker 
scalable content addressable network 
acm sigcomm aug 
rodrigues liskov shrira 
design robust peer peer system 
proceedings th sigops european workshop sept 
rowstron druschel :10.1.1.142.752
pastry scalable decentralized object location routing largescale peer peer systems 
ifip acm middleware nov 
saroiu gummadi gribble 
measurement study peer peer file sharing systems 
proceedings multimedia computing networking mmcn jan 
stoica morris karger kaashoek balakrishnan :10.1.1.105.3673
chord scalable peer peer lookup service internet applications 
acm sigcomm aug 
stoica morris karger balakrishnan 
chord scalable peer topeer lookup service internet applications 
technical report mit lcs tr mit mar 
zhao duan huang joseph kubiatowicz 
landmark routing overlay networks 
proceedings st international workshop peer peer systems iptps cambridge ma usa mar 
zhao kubiatowicz joseph :10.1.1.111.1818
tapestry infrastructure fault tolerant widearea location routing 
technical report ucb csd uc berkeley apr 
zhuang zhao joseph katz kubiatowicz 
bayeux architecture scalable fault tolerant wide area data dissemination 
network operating system support digital audio video th international workshop nossdav june 
