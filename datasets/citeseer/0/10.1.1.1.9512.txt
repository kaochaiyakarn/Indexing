exploration versus exploitation topic driven crawlers gautam pant srinivasan 
menczer department management sciences school library information science university iowa iowa city ia gautam pant srinivasan menczer uiowa edu dynamic nature web highlights scalability limitations universal search engines 
topic driven crawlers address problem distributing crawling process users queries client computers 
context available topic driven crawler allows informed decisions prioritize links visited 
focus balance crawler need exploit information focus promising links need explore links appear suboptimal lead relevant pages 
investigate issue different tasks seeking new relevant pages starting known relevant subset ii seeking relevant pages starting links away relevant subset 
framework number quality metrics developed evaluate topic driven crawling algorithms fair way find mix exploitation exploration essential tasks spite penalty early stage crawl 
projection estimates size visible web today march static pages 
largest search engine google claims searching pages 
fraction web cov ered search engines improved past years 
increasing hardware bandwidth resources disposal search engines keep growth web rate change 
scalability limitations stem search engines attempt crawl web answer query user 
decentralizing crawling process scalable approach bears additional benefit crawlers driven rich context topics queries user profiles interpret pages select links visited 
comes surprise development topic driven crawler algorithms received significant attention years :10.1.1.22.3686:10.1.1.43.1111
topic driven crawlers known focused crawlers respond particular information needs expressed topical queries interest profiles 
needs individual user query time online crawlers community shared interests topical search engines portals 
evaluation topic driven crawlers difficult due lack known relevant sets web searches presence conflicting page quality measures need fair gauging crawlers time space algorithmic complexity 
research evaluation framework designed support comparison topic driven crawler algorithms specified resource constraints 
line research investigating relative merits exploration versus exploitation defining characteristic crawling mechanism 
issue exploitation versus exploration universal machine learning artificial intelligence presents task search guided quality estimations 
regularity assumption assume measure quality point search space provides information quality nearby points 
greedy algorithm exploit information concentrating search vicinity promising points 
strategy lead missing equally better points reasons estimates may noisy second search space may local optima trap algorithm keep locating global optima 
words may necessary visit bad points order arrive best ones 
extreme algorithms completely disregard quality estimates continue explore uniform random fashion risk getting stuck local optima available information bias search may spend time exploring suboptimal areas 
balance exploitation exploration clues obviously called heuristic search algorithms optimal compromise point unknown topology search space understood typically case 
topic driven crawlers fit picture views web search space pages points neighborhoods defined hyperlinks 
crawler decide pages visit cues provided links nearby pages 
assumes relevant page higher probability near relevant pages random page quality estimate pages provide cues exploited bias search process 
short range relevance clues web relevant page links apparently irrelevant 
balancing exploitation quality estimate information exploration suboptimal pages crucial performance topic driven crawlers 
question study empirically respect different tasks 
seek relevant pages starting set relevant links 
applications task query time search agents results search engine starting points provide user personalized results 
start relevant links may expect exploratory crawler perform reasonably 
second task involves seeking relevant pages starting crawl links links away relevant subset 
task may part web mining competitive intelligence applications search starting competitors home pages 
start known relevant subset appropriate balance exploration vs exploitation empirical question 
evaluation framework topics examples neighbors order evaluate crawler algorithms need topics corresponding relevant examples neighbors 
neighbors urls extracted neighborhood examples 
obtain topics open directory dmoz 
ran randomized breadth crawls starting main categories dmoz site 
crawlers identify dmoz leaves pages children category nodes 
leaves external links derive topics 
collected topics 
topic represented types information derived corresponding leaf page 
words dmoz hierarchy form topic keywords 
second external links form topic examples 
third concatenate text descriptions anchor text target urls written dmoz human editors form topic description 
difference dmoz org table sample topic 
description truncated space limitations 
topic keywords topic description examples recreation hot air ballooning organizations society australia varied collection photos facts ballooning australia balloon building 
includes article theory flight 
albuquerque ascension association comprehensive site covering range ballooning topics including balloon local education safety programs flying events club activities committees club history 
arizona hot air balloon club 
tween topic keywords topic descriptions give crawlers models short query topics detailed representations topics gauge relevance crawled pages post hoc analysis 
table shows sample topic 
neighbors obtained topic process 
examples obtain top inlinks returned google 
get top inlinks inlinks obtained earlier 
examples start may maximum unique urls 
subset urls picked random set 
links subset called neighbors 
architecture previously proposed evaluation framework compare different crawlers 
framework allows easily plug modules implementing arbitrary crawling algorithms share data structures utilities optimize efficiency affecting fairness evaluation 
mentioned crawlers different tasks 
task crawlers start examples second starting points neighbors 
case pages fetched component urls added list call frontier 
crawler may topic keywords google yahoo com www com au www org www com www aristotle net www pair com abac abac htm www org www com au balloon club html communities msn com www bfa net www ask ne jp html guide selection frontier urls fetched iteration 
topic crawler allowed crawl max pages pages 
crawl may sooner crawler frontier empty 
timeout seconds web downloads 
large pages chopped retrieve kb 
protocol allowed redirection allowed filter static pages text html content 
stale links yielding error codes removed links analysis 
constrain space resources crawler algorithm restricting frontier size max buffer urls 
buffer full crawler decide links replaced new links added 
crawling algorithms study notion exploration versus exploitation 
single family crawler algorithms single greediness parameter control exploration exploitation behavior 
previous experiments naive best crawler displayed best performance crawlers considered 
study explore variants best crawler 
generally examine best family crawlers parameter controls characteristic interest 
best crawlers studied :10.1.1.22.3686
basic idea frontier best topic starting urls foreach link starting urls enqueue frontier link frontier visited max pages links crawl dequeue top links frontier foreach link randomize links crawl doc fetch new document link score sim topic doc foreach outlink extract links doc frontier max buffer dequeue link min score frontier enqueue frontier outlink score pseudocode best crawlers 
links best link estimation criterion selected crawling 
best generalization iteration batch top links crawl selected 
completing crawl pages crawler decides batch 
mentioned topic keywords guide crawl 
specifically done link selection process computing lexical similarity topic keywords source page link 
similarity page topic estimate relevance pages linked urls best estimates selected crawling 
cosine similarity crawlers links minimum similarity score removed frontier necessary order exceed max buffer size 
offers simplified pseudocode best crawler 
best offers ideal context study 
parameter controls greedy behavior crawler 
increasing results crawlers greater emphasis exploration consequently reduced emphasis exploitation 
decreasing reverses selecting smaller set links exploitative evidence available regarding potential merits links 
experiments test mutants crawler setting 
refer values 
evaluation methods table depicts methodology crawler evaluation 
rows table indicate different methods gauging page quality 
purely lexical approach similarity topic description assess relevance 
second method primarily linkage approximation retrieval ranking method google uses pagerank discriminate pages containing number topic keywords :10.1.1.109.4049
columns table show measures static dynamic perspective 
static approach examines crawl quality assessed full set pages crawled query 
contrast dynamic measures provide temporal characterization crawl strategy considering pages fetched crawl progress 
specifically static approach measures coverage ability retrieve pages quality page assessed different ways corresponding rows table 
static plots show ability crawler retrieve fewer highly relevant pages 
analogous plotting recall function generality 
dynamic approach examines quality retrieval crawl progresses 
dynamic plots offer trajectory time displays dynamic behavior crawl 
measures built average quality ranks generally inversely related precision 
average rank decreases increasing proportion crawled set expected relevant 
noted scores ranks dynamic measure computed calculations point time crawler done data generated full crawl 
instance pagerank scores calculated full set retrieved pages 
strategy quite reasonable want best possible evidence judging page quality 
table evaluation schemes measures 
static scheme coverage top pages ranked quality metric crawled pages 
set pages visited crawler 
dynamic scheme ranks quality metric crawled pages averaged crawl sets time 
static scheme dynamic scheme lexical linkage lexical page quality smart system rank retrieved pages lexical similarity topic 
smart system allows pool pages crawled crawlers topic rank topic description 
system utilizes term weighting strategies involving term frequency inverse document frequency computed pooled pages topic 
smart computes similarity query topic dot product topic page vectors 
outputs ranked set pages topic similarity scores 
page get rank refer cf 
table 
topic percentage top pages ranked smart varies retrieved crawler may calculated yielding static evaluation metric 
dynamic view values pages calculate mean different points crawl 
denote set pages retrieved time calculate mean 
set pages increases size proceed time 
approximate number pages crawled 
trajectory mean values time displays dynamic behavior crawler 
linkage page quality observed content give fair measure quality page :10.1.1.120.3875
algorithms hits pagerank linkage structure web rank pages :10.1.1.120.3875:10.1.1.109.4049
pagerank particular estimates global popularity page 
computation pageranks done iterative process 
pageranks calculated crawls completed 
pool pages crawled topics crawlers calculate pageranks algorithm described :10.1.1.18.5084
sort pages crawled topic crawlers number topic keywords contain sort pages number keywords pagerank 
process gives page crawled topic 
static evaluation metric measures percentage top pages ranked crawled crawler topic 
dynamic metric mean plotted number pages crawled 
results evaluation schemes metrics outlined table analyzed performance crawler tasks 
task starting examples task crawlers start relevant subset links examples hyperlinks navigate discover relevant pages 
results task summarized plots 
readability plotting performance selected subset best crawlers 
behavior remaining crawled crawled static lexical performance bfs bfs number top pages static linkage performance bfs bfs number top pages average rank average rank dynamic lexical performance bfs bfs pages crawled dynamic linkage performance bfs bfs pages crawled static evaluation left dynamic evaluation right representative crawlers task 
plots correspond lexical top linkage bottom quality metric 
error bars correspond standard error topics plots 
crawlers bfs bfs bfs extrapolated curves corresponding bfs bfs 
general observation draw plots bfs achieves significantly better performance static evaluation schemes superior coverage highly relevant pages quality metrics different numbers top pages cf 

difference coverage crawlers different increases considers fewer highly relevant pages 
results indicate exploration important locate highly relevant pages starting relevant links exploitation harmful 
dynamic plots give richer picture 
recall lowest average rank best 
bfs significantly better crawlers lexical metric cf 

linkage metric shows bfs pays large penalty early stage crawl cf 

crawler quality longer run 
better coverage highly relevant pages crawler cf 
may help interpret improvement observed second phase crawl 
conjecture exploring suboptimal links early bfs capable eventually discovering paths highly relevant pages escape greedy strategies 
task starting neighbors success exploratory algorithm task may come surprise start known relevant pages 
crawled crawled static lexical performance bfs bfs breadthfirst number top pages static linkage performance bfs bfs breadthfirst number top pages average rank average rank dynamic lexical performance bfs bfs breadthfirst pages crawled dynamic linkage performance bfs bfs breadthfirst pages crawled static evaluation left dynamic evaluation right representative crawlers task 
plots correspond lexical top linkage bottom quality metric 
second task links obtained neighborhood relevant subset starting points goal finding relevant pages 
take worst bfs best bfs crawlers task task 
addition add simple breadth crawler uses limited size frontier fifo queue 
breadth crawler added observe performance blind exploratory algorithm 
summary results shown plots 
task find exploratory algorithm bfs performs significantly better bfs static evaluations lexical linkage quality metrics cf 

dynamic plots cf 
bfs bear initial penalty exploration recovers long run 
breadth crawler performs poorly evaluations 
general result find exploration helps exploitative algorithm exploration guidance goes astray 
due availability relevant subsets examples topics current task plot average recall relevant examples number pages crawled 
plot illustrates target seeking behavior crawlers examples viewed targets 
find bfs outperforming bfs breadth trails 
related research research design effective focused crawlers 
different types crawling algorithms developed 
example chakrabarti classifiers built training sets positive negative average recall bfs bfs breadthfirst pages crawled average recall examples crawls start neighbors :10.1.1.43.1111
example pages guide focused crawlers 
focused crawling starting points generated clever search engines 
crawlers follow fixed strategies adapt course crawl learning estimate quality links :10.1.1.1.7474
question exploration versus exploitation crawler strategies addressed number papers directly 
fish search limited exploration bounding depth path appeared suboptimal 
cho exploratory crawling behaviors implemented breadth algorithm lead efficient discovery pages pagerank :10.1.1.22.3686
discuss issue limiting memory resources buffer size crawler impact exploitative behavior crawling strategy forces crawler frequent filtering decisions 
breadth crawlers find popular pages early crawl 
exploration versus exploitation issue continues studied variations major classes breadth best crawlers 
example research breadth focused crawling diligenti address crawlers assessing potential value links crawl 
particular look avoid short term gains expense obvious larger long term gains 
solution build classifiers assign pages different classes expected link distance current page relevant documents 
area crawler quality evaluation received attention research 
instance alternatives assessing page importance explored showing range sophistication 
cho simple presence word computer indicate relevance :10.1.1.22.3686
amento compute similarity page centroid seeds 
fact contentbased similarity assessments form basis relevance decisions examples research :10.1.1.43.1111
exploit link information estimate page relevance methods degree degree pagerank hubs authorities :10.1.1.4.6938:10.1.1.22.3686:10.1.1.102.9301
example cho consider pages pagerank score threshold relevant :10.1.1.22.3686
najork wiener crawler fetch millions pages day calculate average pagerank pages crawled daily assumption pagerank estimates relevance 
combinations link content relevance estimators evident approaches 
evaluation framework topic driven crawlers study role exploitation link estimates versus exploration suboptimal pages 
experimented family simple crawler algorithms varying greediness limited memory resources different tasks 
number schemes quality metrics derived lexical features link analysis introduced applied gauge crawler performance 
consistently exploration leads better coverage highly relevant pages spite possible penalty early stage crawl 
obvious explanation exploration allows trade short term gains longer term potentially larger gains 
blind exploration starting neighbors relevant pages leads poor results 
mix exploration exploitation necessary performance 
starting relevant examples task better performance crawlers higher exploration attributed better coverage documents close relevant subset 
performance bfs starting away relevant pages shows exploratory nature complements greedy side finding highly relevant pages 
extreme exploitation bfs blind exploration breadth impede performance 
exploitation better 
results short crawls pages 
may hold longer crawls issue addressed research 
dynamic evaluations suggest short crawls best greedy lesson incorporated algorithms query time online crawlers 
observation higher exploration yields better results motivate parallel distributed implementations topic driven crawlers complete orderings links frontier required greedy crawler algorithms necessary performance 
crawlers local decisions hold promise performance exploratory strategies efficiency scalability distributed implementations 
particular intend experiment variations crawling algorithms allow adaptive distributed exploratory strategies 
crawler algorithms intend study research include best strategies driven estimates lexical ones 
example plan implement best family link estimates local versions metric evaluations purposes 
plan test sophisticated lexical crawlers shark search prioritize links single page 
biz uiowa edu goal research identify optimal trade offs exploration exploitation exploration greediness degrade performance 
large buffer size constrain range exploration exploitation strategies happened experiments described due small max buffer 
identifying optimal exploration exploitation trade step development adaptive crawler attempt adjust level greediness crawl 
things done analyze time complexity crawlers topic specific performance strategy 
regarding clearly greedy strategies require frequent decisions may impact efficiency crawlers 
regarding considered quality measures aggregate topics 
useful study appropriate trade offs exploration exploitation depend different characteristics topic heterogeneity 
issues object ongoing research 
aggarwal yu 
intelligent crawling world wide web arbitrary predicates 
proc 
th intl 
world wide web conference pages 
amento terveen hill 
authority mean quality 
predicting expert quality ratings web documents 
proc 
rd acm sigir conf 
research development information retrieval pages 
ben shaul maarek pelleg ur 
adding support dynamic focused search 
computer networks 
bharat henzinger 
improved algorithms topic distillation hyperlinked environments 
proc 
st acm sigir conf 
research development information retrieval pages 
cybenko 
dynamic web 
proc 
th international world wide web conference 
brin page :10.1.1.109.4049
anatomy large scale hypertextual web search engine 
computer networks 
chakrabarti dom raghavan rajagopalan gibson kleinberg 
automatic resource compilation analyzing hyperlink structure associated text 
computer networks 
chakrabarti van den berg dom :10.1.1.43.1111
focused crawling new approach topic specific web resource discovery 
computer networks 
cho garcia molina page :10.1.1.22.3686
efficient crawling url ordering 
proc 
th intl 
world wide web conference brisbane australia 

sizing internet 
white july 
www com web corporate white papers htm 
de bra post 
information retrieval world wide web making searching feasible 
proc 
st intl 
world wide web conference 
diligenti coetzee lawrence giles gori 
focused crawling context graphs 
proc 
th international conference large databases vldb pages cairo egypt 
haveliwala :10.1.1.18.5084
efficient computation pagerank 
technical report stanford database group 
maarek pelleg ur 
shark search algorithm application tailored web site mapping 
proc 
th intl 
world wide web conference 
kleinberg :10.1.1.120.3875
authoritative sources hyperlinked environment 
journal acm 
lawrence giles 
accessibility information web 
nature 
menczer 
links tell lexical semantic web content 
arxiv cs ir 
menczer belew 
adaptive retrieval agents internalizing local context scaling web 
machine learning 
menczer pant ruiz srinivasan 
evaluating topic driven web crawlers 
proc 
th annual intl 
acm sigir conf 
research development information retrieval 
najork wiener 
breadth search crawling yields high quality pages 
proc 
th international world wide web conference 
pant menczer 
evolve intelligent web crawlers 
autonomous agents multi agent systems 
rennie mccallum 
reinforcement learning spider web efficiently 
proc 
th international conf 
machine learning pages 
morgan kaufmann san francisco ca 
salton 
smart retrieval system experiments automatic document processing 
prentice hall englewood cliffs nj 
