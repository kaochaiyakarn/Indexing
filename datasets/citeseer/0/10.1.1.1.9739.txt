intelligenz learning hidden markov models information extraction actively partially labeled text tobias er stefan wrobel popov christian susanne university magdeburg fin po box magdeburg germany ai lab christo blvd bulgaria er wrobel cs uni magdeburg de bg may vast range information expressed unstructured semi structured text form hard decipher automatically 
consequently enormous importance construct tools allow users extract information textual documents easily extracted structured databases 
information extraction systems identify predetermined relevant information text documents speci domain ll structured form 
approach solving problem hidden markov models hmms generated automatically manually labeled example documents 
consider challenging task learning hmms partially sparsely labeled documents available training 
order reduce amount data labeling ort user invest describe algorithm naturally extended active learning algorithm selects dicult unlabeled tokens asks user label 
study empirically active learning reduces required labeling ort 
enormous amounts information available unstructured semi structured textual documents tools information extraction enormously important see overview 
tools identify relevant information documents convert structured format database xml document 
rst algorithms hand crafted sets rules researchers soon turned learning extraction rules hand labeled documents 
unfortunately rule approaches fail provide necessary robustness inherent variability document structure led interest hidden markov models hmms purpose 
markov model algorithms part speech tagging known hidden markov models require training documents labeled completely token manually appropriate label 
clearly expensive process 
concentrate task learning information extraction models particular hidden markov models partially labeled texts develop appropriate em style algorithms 
show additional unlabeled documents usually readily available applications perform active learning hmms 
idea active learning algorithms identify dicult unlabeled observations ask user label 
algorithms known classi cation clustering regression algorithm active learning hidden markov models 
organized follows 
give account task loss functions section followed description hidden markov models section 
discuss hmm applied problem section hmms learned example documents section 
discuss active learning approach section empirical results section section concludes 
information extraction systems identify predetermined relevant information text documents speci domain extract convert structured format database xml document 
example consider problem processing ight con sent airline email automatically 
order identify ight database record con rmed details ight numbers outbound inbound dates names passengers extracted unstructured text 
interesting application domain computational biology 
great amount biological data collections maintained internet repositories 
data collections queried internet 
answers generally provided form html pages 
order process pertinent information html documents necessary identify example enzyme functional data reaction type inhibitors molecular weight queried enzymes semi structured html output 
give de nition task considered 
document sequence observations 
observation corresponds token document 
token vector attributes generated collection nlp tools 
attributes may include word stem part speech html context properties word sentence paragraph 
task attach semantic tag token observations left untagged special tag 
extraction algorithm function maps observation sequence single sequence tags fx multi valued assignments handled models label 
problem unknown joint distribution tags observations 
example documents assumed drawn resulting marginal distribution documents learning problem nd extraction function set example documents minimizes chosen error criterion 
depending application ways de ne error criterion minimized 
applications costs may arise false tag assigned token 
cases de ne token error token probability tokens false tags 
precision recall popular error criteria de ned problems tag fx available 
precision refers amount correct tags assigned relative number correctly incorrectly assigned tags recall re ects amount tags correctly assigned relative total amount tags assigned 
hidden markov models hidden markov models hmms see robust statistical method structural analysis sequential data 
hmms nite state machines stochastic state observation sequences 
hmm consists nitely states fs sn state hmm time denoted random variable sn possible values 
time step hmm generates observation array denotes probabilities starting state ij jq quanti es probability transition state state characterized probability distribution jq observations 
time step hmm changes state observation generated states model passes invisible outside observation sequence probabilistic function state sequence known name hidden markov model 
hmms useful model underlying events probabilistically induce surface events suitable model natural language 
context hmm parameters seen stochastic grammar focused class documents describes probabilistically sequences words documents occur class 
observations individual words document hmm state word emitted represents semantic role meaning 
hmm information extraction possesses state tags want attach words possesses number background states representing meaning words interested extracting 
denote target states corresponding tags background states represent information interested denoted sn underlying hmms markov assumption says state conditionally depends direct predecessor higher order hmms generalize preceeding states jq jq 
words states summarize past information relevant applying hmms suppose hidden markov model describes class documents interested new document instance class 
word want identify hidden state meaning corresponding 
identi ed states simply extract tokens correspond target states label corresponding tags important step process applying hmm determine probabilities jo hmm state time probability usually abbreviated 
determined optimal extraction strategy depends precise optimization criterion 
order minimize number false labels assign tag state maximizes 
certain precision recall balance achieved assigning tag word exceeds threshold 
threshold balances precision recall 
focus problem calculating jo 
problem special case slightly general problem solve order nd learning algorithm hmms section 
general case tokens may possess labels user attached 
immediately restricts state labeled tokens states consistent user assigned label labels user de ned labels impact nearby unlabeled words 
denote set state indices consistent user de ned label word apply hmm new document labels ng means states possible word 
solution inspired message passing algorithm bayesian networks extends standard message passing algorithm constraints account 
light solution close relation forward backward algorithm hidden markov models message passing algorithm bayesian networks recognized 
decompose xje xje jx evidential support causal support 
normalization jo jq please note order extended problem de ned chosen di erently corresponding probabilities classical hmm algorithm 
straightforward recursive evaluation prohibitively expensive 
theorems dynamic programming algorithm calculates eciently 
de nition de ne probability arriving state time initial observation sequence labels equation probability encountering remaining observations labels state time equation 
probability state time partially labeled observation sequence 
jo jq proofs theorems refer reader 
theorem computed recursively equations 
ij theorem computed recursively equations 
ij theorem computed equation 
theorems say possible determine linear time 
forward backward algorithm achieves follows 

calculate theorem 
calculate theorem pass calculate theorem 
learning hmms partially labeled data seen hmm information extraction study learn hmm set example sequences 
assume set initially unlabeled documents available user labels words sequences appropriate tag 
express labels terms sequences possible states ng 
user may label token tag means hmm state emitting fig 
tokens may labeled possessing tag implies hmm background states fn ng may left unlabeled says state ng 
extends setting part speech tagging problem token label corresponds exactly state 
baum welch algorithm see example estimate model parameters set observation sequences 
principle diculty algorithm addresses order determine transition emission probabilities states correspond observations known 
unfortunately states unlabeled tokens hidden constraints imposed labels 
called step algorithm calculates token probability distribution states current estimate transition emission probabilities labels account 
distributions states called step update emission transition probabilities em algorithm 

start random model 

forward backward algorithm estimate state probabilities current model 

calculated state probabilities estimate transition probabilities ij counting transitions occur 

calculate state frequently observations observed state 
estimate 

recur step parameters stay nearly constant iteration 
baum welch algorithm known contribution instantiation information extraction handles partially labeled observation sequences mathematically sound way 
possible theorems say eciently calculate modi ed variable takes including non local ects labels state probabilities account 
algorithm desirable properties 
regular baum welch algorithm converges local optimum parameter space 
document completely labeled reaches stability rst iteration behaves straightforward parameter estimation procedure hidden markov models 
active revision hmms unlabeled documents obtained easily information extraction problems labeling tokens document imposes ort 
active learning approach utilizing available amount user ort ectively select available unlabeled tokens potentially interesting 
objective minimize number false tags bayes optimal extraction algorithm label token tag maximizes jo observation sequence 
deviate optimal strategy parameter estimates di er true parameters state state really max jo max jo 
see di erence probability second state con dence margin state de ne margin jo token read time di erence highest second highest probability state equation jo max fp jo max fp jo max max intuitively margin seen quantifying dicult low margin easy large margin token active hmm learning algorithm rst learns initial model set partially labeled documents 
determines margins tokens starts asking user label tokens particularly low margin 
baum welch algorithm restarted previous parameters initial model adapting new data 
experiments empirically test active learning algorithm performed sets experiments synthetic arti cial data data real life problem ight con rmation mails 
synthetic experiments generated hmms variable numbers background target states random 
hmms generate unlabeled observation sequences labeled number initial tokens drawn random 
studied error develops number additional labels added observation sequences strategies 
rst active learning algorithm proposed asking user labels observations smallest margins refer approach simply margin strategy 
contrast random strategy label randomly drawn observations large margins strategy opposite proposed strategy selecting tokens largest margins 
proposed strategy margins really better random expect large margins perform worse 
di erent hmm sizes 
easy hidden markov model consists background target states 
state emits observations randomly drawn probabilities 
generated sequences initially unlabeled observations 
medium size hmm possesses nodes large hmm consists states state nonzero transition probabilities states 
curves averages learning problems 
initial sample contains unlabeled tokens figures easy medium size hard learning problem labels figures respectively tokens figures drawn random 
figures see slight signi cant advantage random token selection selecting tokens small margins 
dicult tokens bene cial average 
phase learning token selection small margins gains small advantage 
bene margin strategy clearly visible initial sample contains labels figures figures tokens drawn random tokens smallest margins selected 
small hmm learning problem unlabeled data relative problem complexity available bottom line error reached labels margin strategy labeled tokens random strategy 
active learning small margin examples initial random tokens bene cial 
case base level error reached examples active examples regular learning times fewer labels needed 
choosing easy examples large margins clearly bad strategy cases 
experiments show classes hmm generated dicult low margin tokens learning results higher error rates 
training started labeling randomly drawn tokens active hmm chooses dicult low margin tokens initial phase signi cant improvement achieved regular hmms result suciency times fewer labeled examples 
real life experiments task analyze ight con rmation emails written airline 
order process ight con automatically necessary identify passenger names ight numbers outbound inbound dates locations 
problem hard human customer support agent emails unstructured ungrammatical imprecise 
shows precision recall curves depending number example documents 
gure shows reasonably small numbers documents total labeling ort hours suce obtain precision recall rates 
discussion related range possible problems addressed ectively robust generic information extraction tool wide ranging automatically generating databases price comparisons corporate intelligence 
discussed hidden markov model framework adapted deal sparsely labeled observation sequences naturally occur information extraction problem 
proposed strategy active selection unlabeled dicult tokens labeling 
experiments show active data selection reduce data labeling ort substantially 
labeled tokens random margins large margins labeled tokens random margins large margins labeled tokens random margins large margins labeled tokens random margins large margins labeled tokens random margins large margins labeled tokens random margins large margins labeled tokens random margins large margins labeled tokens random margins large margins labeled tokens random margins large margins error rate active regular learning number labeled tokens 
easy hmm initial sample contains labeled tokens drawn random 
medium size hmm initial sample initial labels 
large hmm initial labels 
alternative non generative hidden markov model information extraction described 
usual distributions ij alternative model uses conditional probability jq 
consequently non generative model possesses jqj joj probabilities estimated contrast jqj jqj joj probabilities generative model 
set models describing di erent categories web pages observation sequence calculate oj generative model hmms text classi cation tasks 
idea promising information lies usual bag words representation text classi cation 
probability determined non generative model 
furthermore forward backward baum welch algorithm non generative model assumptions believe dicult motivate see detailed discussion 
durbin proposed modi cation baum welch algorithm partially labeled observation sequences probabilities events inconsistent labels simply set zero calculated labels 
procedure simple may cases lead desired results deviates mathematically strict approach calculating state probability labels 
part supported dfg german science foundation tp part project cluster information fusion 
brants 
cascaded markov models 
proceedings ninth conference european chapter association computational linguistics 
precision recall rates obtained extraction ight details customer emails depending number examples 
cohn atlas ladner 
improving generalization active learning 
machine learning 
mark craven dan dipasquo dayne freitag andrew mccallum tom mitchell kamal nigam se slattery :10.1.1.18.5535
learning construct knowledge bases world wide web 
arti cial intelligence 
durbin eddy krogh mitchison 
biological sequence analysis probabilistic models proteins nucleic acids 
cambridge university press 

information extraction world wide web survey 
technical report norwegian computing center 
ralph grishman beth sundheim 
message understanding conference brief history 
proceedings international conference computational linguistics 
thomas hofmann joachim buhmann 
active data clustering 
advances neural information processing systems volume 
hsu dung 
generating nite state transducers semistructured data extraction web 
journal information systems special issue semistructured data 
anders krogh jesper vedelsby 
neural network ensembles cross validation active learning 
advances neural information processing systems volume pages 
kushmerick 
wrapper induction eciency expressiveness 
arti cial intelligence 
andrew mccallum dayne freitag fernando pereira 
maximum entropy markov models information extraction segmentation 
proceedings seventeenth international conference machine learning 
rabiner 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
er popov wrobel 
learning hidden markov models information extraction actively partially labeled text 
technical report university magdeburg 
tobias er stefan wrobel 
active learning partially hidden markov models 
proceedings ecml pkdd workshop instance selection 

