invariant feature extraction classi cation kernel spaces sebastian mika gunnar jason weston bernhard sch olkopf alex smola klaus robert uller gmd :10.1.1.103.1189
berlin germany bioinformatics waters av gr usa microsoft research street cambridge cb nh uk australian national university canberra act australia rst gmd de dcs rhbnc ac uk bsc microsoft com alex smola anu edu au incorporate prior knowledge construct nonlinear algorithms invariant feature extraction discrimination :10.1.1.103.1189
employing uni ed framework terms nonlinear variant rayleigh coecient propose non linear generalizations fisher discriminant oriented pca support vector kernel functions 
extensive simulations show utility approach 
common practice preprocess data extracting linear nonlinear features 
known feature extraction technique principal component analysis pca 
aims nd orthonormal ordered basis th direction describes variance possible maintaining orthogonality directions 
pca linear technique limited capture interesting nonlinear structure data set nonlinear generalizations proposed kernel pca computes principal components data set mapped nonlinearly high dimensional feature space prior information instance know sample corrupted noise invariances classi cation change 
feature extraction concepts known noise transformation invariance certain degree equivalent interpreted causing change feature ought minimized 
clearly invariance sucient condition feature simply take constant function 
obtain feature invariant possible covering information necessary describing particular data 
considering linear feature vector restricting rst second order statistics data arrives maximization called rayleigh coecient feature vector sn matrices describing desired undesired properties feature respectively information noise 
data covariance sn noise covariance obtain oriented pca 
leave eld data description perform supervised classi cation common choose separability class centers class variance sn class variance 
case recover known fisher discriminant 
ratio maximized cover information coded avoiding coded sn problem known solved analogy pca generalized symmetric eigenproblem sn corresponding biggest eigenvalue :10.1.1.103.1189
generalize setting nonlinear 
analogy rst map data nonlinear mapping high dimensional feature space optimize avoid working mapped data explicitly impossible nite dimensional introduce support vector kernel functions known kernel trick 
kernel functions compute dot product feature space 

formulating algorithms dot products replace occurrence dot product kernel function possible choices proven useful support vector machines kernel pca gaussian rbf exp kx yk polynomial kernels :10.1.1.103.1189
positive constants respectively :10.1.1.103.1189
remainder organized follows section shows formulate optimization problem induced feature space 
section considers various ways nd fisher discriminant conclude extensive experiments section discussion ndings 
rayleigh coecient optimize kernel feature space need nd formulation uses dot products images :10.1.1.103.1189
numerator denominator scalars done independently 
furthermore matrices sn basically covariances sum outer products images 
due linear nature solution written expansion terms mapped training data de ne common choices fx training sample appropriate subclasses jx :10.1.1.103.1189
get full covariance sb sw operators nite dimensional subspace spanned possibly nite space :10.1.1.103.1189
span span :10.1.1.103.1189
sw sb symmetric hw swi hv sv lies span operates subspace exist expansion maximizes :10.1.1.103.1189
oriented kernel pca 
sn estimate noise covariance analogous de nition mapped patterns sampled assumed noise distribution 
standard formulation fisher discriminant yielding kernel fisher discriminant kfd sw sb class scatter sw sn class scatter sb :10.1.1.103.1189
sample mean patterns class incorporate known invariance oriented kernel pca tangent covariance matrix small local parameter transformation :10.1.1.103.1189
nite di erence approximation covariance tangent point details :10.1.1.103.1189
sn oriented kernel pca impose invariance local transformation crucially matrix constructed training patterns argument nd expansion slightly incorrect :10.1.1.103.1189
assume reasonable approximation describing variance induced multiplying matrices left right expansion nd formulation uses dot products :10.1.1.103.1189
sake brevity give explicit formulation kfd cf 
details 
de ning write kfd kk ij :10.1.1.103.1189
results choices sn cases oriented kernel pca transformation invariance obtained lines 
note maximize rayleigh coecient 
quotient terms expansion coecients terms potentially nite dimensional space :10.1.1.103.1189
furthermore known solution special eigenproblem direction solved cholesky factorization projection new pattern computed :10.1.1.103.1189
algorithms estimating covariance matrix rank samples ill posed 
furthermore performing explicit centering covariance matrix loses dimension rank worse kfd matrix rank :10.1.1.103.1189
ratio de ned anymore denominator zero 
propose ways deal problem kfd 
furthermore tackle question solve optimization problem kfd eciently 
far eigenproblem size 
large numerically demanding 
reformulations original problem allow overcome limitations 
describe connection kfd rbf networks 
regularization solution subspace noted matrix rank :10.1.1.103.1189
numerical problems cause matrix positive think imposing regularization control capacity simply add multiple identity matrix replace viewed di erent ways problem feasible numerically stable positive ii seen decreasing bias sample estimation eigenvalues cf 
iii imposes regularization favoring solutions small expansion coecients :10.1.1.103.1189
furthermore regularization type additives penalizing kwk analogy svm adding kernel matrix ij :10.1.1.103.1189
optimize need solve eigenproblem intractable large 
solutions sparse directly ecient algorithms chunking support vector machines cf 

restrict solution lie subspace expanding write patterns subset training patterns estimated clustering algorithm :10.1.1.103.1189
derivation change matrices advantage increases rank relative size need regularization 
quadratic optimization cation full rank maximizing underdetermined optimal multiple thereof :10.1.1.103.1189
rank :10.1.1.103.1189
seek vector minimal xed 
solution unique nd optimal solving quadratic optimization problem min subject quadratic optimization problem easier solve eigenproblem appealing interpretation 
constraint ensures average class distance projected direction discrimination constant intra class variance minimized maximize average margin 
contrarily svm approach optimizes large minimal margin :10.1.1.103.1189
considering able overcome shortcoming kfd 
solutions sparse evaluating expensive 
solve add regularizer objective function regularization parameter allowing adjust degree sparseness 
connection rbf networks interestingly exists close connection rbf networks kfd 
add regularization expand training patterns nd optimal symmetric positive matrix kernel elements label vector rbf network see note written rank vector patterns class zero :10.1.1.103.1189
rbf ab abr svm kfd banana cancer diabetes german heart image ringnorm sonar splice thyroid titanic twonorm waveform table comparison kfd single rbf classi er adaboost ab :10.1.1.103.1189
adaboost abr svms see text 
best result bold face second best italics 
kernel sample xed kernel width gives solution mean squared error labels output minimized 
case restricted expansions exists connection rbf networks smaller number centers cf 

experiments kernel fisher discriminant shows illustrative comparison features kfd kernel pca 
kfd feature discriminates classes rst kernel pca feature picks important nonlinear structure 
evaluate performance kfd real data sets performed extensive comparison state art classi ers details reported 
compared kernel fisher discriminant support vector machines gaussian kernel adaboost regularized adaboost cf 
table 
kfd regularized class scatter computed projections optimal direction means :10.1.1.103.1189
classi cation estimate threshold 
done trying thresholds outputs training set selecting median smallest empirical error computing threshold maximizes margin outputs analogy support vector machine deal errors set svm soft margin approach 
disadvantage control regularization constant slack variables 
results table show average test error standard full rank null space spanned null space get :10.1.1.103.1189
free constraint positive constant just feasible 
breast cancer domain obtained university medical center inst 
oncology ljubljana yugoslavia 
data 
data sets experiments obtained www gmd de 
comparison feature kfd left rst kernel pca feature right 
depicted classes information kfd dots crosses levels feature value 
polynomial kernel degree kfd regularized class scatter 
deviation averages estimation runs di erent realizations datasets 
estimate necessary parameters ran fold cross validation rst realizations training sets took model parameters median estimates see details experimental setup 
prior knowledge 
toy example gure shows comparison kernel pca oriented kernel pca full covariance noise matrix sn tangent covariance rotated patterns ii axis translated patterns :10.1.1.103.1189
toy example shows imposing desired invariance yields meaningful invariant features 
experiment incorporated prior knowledge kfd 
usps database handwritten digits consists training test patterns dimensional gray scale images digits :10.1.1.103.1189
regularized class scatter sn added multiple tangent covariance sn invariance transformations chosen horizontal vertical translation rotation thickening cf 
simply averaged matrices corresponding transformation :10.1.1.103.1189
feature extracted restricted expansion patterns rst training samples 
kernel chosen gaussian width 
optimal svms :10.1.1.103.1189
class trained kfd classi ed class rest computed class error scheme 
threshold estimated minimizing empirical risk normalized outputs kfd 
invariances achieved test error slightly better plain svm kernel :10.1.1.103.1189
tangent covariance matrix led slight improvement 
result signi cantly better corresponding kfd attributed fact expansion coecients cases 
tangent covariance matrix lives slightly di erent subspace 
subsequent experiment vectors obtained clustering larger dataset including virtual examples generated appropriate invariance transformation led comparable svm prior knowledge best svm result local kernel virtual support vectors :10.1.1.103.1189
task learning data equivalent prior knowledge invariances speci sources noise 
case feature extraction seek features suciently noise invariant describing interesting structure 
oriented pca closely related fisher discriminant particularly simple features consider rst second order statistics maximizing rayleigh coecient 
linear methods restricted real world applications support vector kernel functions obtain nonlinear versions algorithms oriented kernel pca kernel fisher discriminant analysis 
experiments show kernel fisher discriminant competitive comparison rst features kernel pca oriented kernel pca see text left right kpca rotation translation invariance gaussian kernel :10.1.1.103.1189
cases superior state art algorithms tested 
interestingly svm kfd construct hyperplane sense optimal 
cases solution kfd superior svms 
encouraged preliminary results digit recognition believe reported results improved incorporating di erent invariances local kernels :10.1.1.103.1189
research focus improvements algorithmic complexity new algorithms far larger svm algorithm connection kfd support vector machines cf 

acknowledgments partially supported dfg ja ec storm project number carried bs gmd :10.1.1.103.1189
bishop 
neural networks pattern recognition 
oxford univ press 
boser guyon vapnik :10.1.1.103.1189
training algorithm optimal margin classi ers 
haussler editor proc 
colt pages :10.1.1.103.1189
acm press :10.1.1.103.1189
kung 
principal component neural networks 
wiley new york 
fang dawid 
comparison full bayes bayes squares criteria normal discrimination 
chinese journal applied probability statistics :10.1.1.103.1189
freund schapire 
decision theoretic generalization line learning application boosting 
eurocolt 
lncs 
friedman 
regularized discriminant analysis 
journal american statistical association 
fukunaga 
statistical pattern recognition 
academic press san diego nd edition :10.1.1.103.1189
mika weston sch olkopf 
uller 
fisher discriminant analysis kernels 

hu larsen wilson douglas editors neural networks signal processing ix pages 
ieee 
moody darken 
fast learning networks locally tuned processing units 
neural computation :10.1.1.103.1189
onoda 
uller 
soft margins adaboost 
technical report nc tr royal holloway college university london uk :10.1.1.103.1189
saitoh 
theory reproducing kernels applications 
longman scienti technical harlow england 
sch olkopf :10.1.1.103.1189
support vector learning 
oldenbourg verlag 
sch olkopf burges smola editors 
advances kernel methods support vector learning 
mit press 
sch olkopf smola 
uller 
nonlinear component analysis kernel eigenvalue problem 
neural computation :10.1.1.103.1189
shashua 
relationship support vector machine classi cation ed linear discriminant 
neural processing letters april :10.1.1.103.1189
tong koller 
bayes optimal hyperplanes maximal margin hyperplanes 
submitted ijcai workshop support vector machines robotics stanford edu koller 
