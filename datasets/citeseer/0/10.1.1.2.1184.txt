combining naive bayes gram language models text classi cation peng dale schuurmans school computer science university waterloo university avenue west waterloo ontario canada ff peng cs uwaterloo ca 
augment naive bayes model gram language model address shortcomings naive bayes text classi ers 
chain augmented naive bayes classi ers propose advantages standard naive bayes classi ers 
chain augmented naive bayes model relaxes independence assumptions naive bayes allowing local markov chain dependence observed variables permitting ecient inference learning 
second smoothing techniques statistical language modeling recover better estimates laplace smoothing techniques usually naive bayes classi cation 
experimental results real world data sets show achieve substantial improvements standard naive bayes classi cation achieving state art performance competes best known methods cases 
naive bayes classi ers proven successful domains especially text classi cation despite simplicity model restrictiveness independence assumptions :10.1.1.144.7475:10.1.1.11.8264
domingos point naive bayes classi ers obtain near optimal misclassi cation error independence assumption strongly violated :10.1.1.144.7475
commonly thought relaxing independence assumption naive bayes ought allow superior text classi cation shown practice functionally dependent attributes improve classi cation accuracy cases :10.1.1.131.5458
signi cant amount research conducted relaxing naive bayes independence assumption machine learning research 
popular extension tree augmented naive bayes classi er tan allows tree structured dependence observed variables addition traditional dependence hidden root variable :10.1.1.20.1365
learning tree structured bayesian network trivial model rarely text classi cation applications 
investigate convenient alternative lies pure naive bayes tan bayes models strength assumptions chain augmented naive bayes model 
bayes model simpli es tan model restricting dependencies observed variables form markov chain tree 
interestingly turns model results closely related gram language models widely studied statistical natural language modeling speech recognition 
augment naive bayes text classi er including attribute dependencies form markov chain techniques statistical gram language modeling learn apply models 
result combination naive bayes statistical gram methods yields simple surprisingly ective classi ers 
addition proposing model investigate advanced smoothing techniques statistical language modeling obtain better parameter estimates 
standard naive bayes approach laplace smoothing commonly avoid zero probability estimates 
laplace smoothing usually ective smoothing techniques statistical language modeling 
consider advanced smoothing techniques commonly gram language modeling improve naive bayes classi er 
rst describe naive bayes classi er section basic gram language model section 
models form basis chain augmented naive bayes classi er propose section :10.1.1.144.7475
experimentally evaluate proposed classi er real world data sets section conclude brief discussion section 
naive bayes text classi er text classi cation problem assigning document set jcj pre de ned categories fc jcj normally supervised learning framework train text classi er learning algorithm provided set labeled training examples ng produce classi cation function maps documents categories 
denotes ith training document corresponding category label random variables denote document category values respectively 
popular learning algorithm text classi cation simple application bayes rule chapter cjd djc simplify presentation re write eq 
cjd djc bayes rule decomposes computation posterior probability computation likelihood prior probability 
text classi cation document popular learning algorithms text classi cation include decision tree learning support vector machines regression methods neural network rule learning methods line learning :10.1.1.119.8039:10.1.1.131.5458
normally represented vector attributes 
computing djc case generally trivial space possible documents vast 
simplify computation naive bayes model introduces additional assumption attribute values independent category label conditionally independent assumption greatly simpli es computation reducing eq 
cjd jc eq 
maximum posterior map classi er constructed seeking optimal category maximizes posterior cjd arg max fp cjd arg max jc arg max jc note going eq :10.1.1.144.7475
eq 
valid constant category map classi er eq 
optimal sense minimizing zero loss misclassi cation error :10.1.1.144.7475
independence assumption holds classi er eq 
optimal 
prior distribution incorporate additional assumptions 
commonly prior distributions dirichlet distribution uniform distribution 
uniform distribution prior map classi er equivalent maximum likelihood ml classi er 
arg max jc eq 
called maximum likelihood naive bayes classi er 
variants naive bayes classi ers including binary independence model multinomial model poisson model negative binary independence model 
shown text categorization applications multinomial model best choice consider multinomial naive bayes model :10.1.1.144.7475
fig 
gives graphical representation multinomial naive bayes model showing attribute node independent attributes class label attributes called features papers 
feature selection important procedure classi ers 
vn 
fig 

graphical model naive bayes classi er parameters multinomial naive bayes classi er jc 
likelihood set documents category frequency attribute occurring maximum likelihood estimate yields parameter estimates note eq 
puts zero probability attribute values occur 
unfortunately zero estimate create signi cant problems classify new document example encounter new attribute value observed training corpus overcome problem laplace smoothing usually avoid zero probability estimates practice special case laplace smoothing add smoothing chapter obtained setting 
laplace smoothing ective language modeling smoothing techniques :10.1.1.131.5458
show advanced smoothing techniques improve naive bayes classi ers play important role developing ective text classi ers naive bayes models 
going back equ 
see naive bayes classi er considers selected attributes ignoring attributes oov attributes 
infrequent attributes contribute information common attributes individually cumulative ect important ect classi cation accuracy 
language modeling classi er alleviates problem implicitly consider possible attributes 
experimentally show considering oov attributes notable ects classi cation performance 
describe main component models statistical gram language models augment naive bayes classi ers 
markov gram language modeling dominant motivation language modeling come speech recognition statistical language models widely application areas including information retrieval :10.1.1.54.6410
goal language modeling predict probability natural word sequences simply put high probability word sequences occur low probability word sequences occur 
word sequence test corpus quality language model measured empirical perplexity entropy corpus perplexity jw entropy log perplexity goal language modeling obtain small perplexity 
simplest successful basis language modeling gram model note chain rule probability write probability sequence jw gram model approximates probability assuming words relevant predicting jw previous words assumes markov gram independence assumption jw jw straightforward maximum likelihood estimate gram probabilities corpus observed frequency jw :10.1.1.144.7475
number occurrences speci ed gram training corpus 
unfortunately grams length entails estimating probability events size word vocabulary 
quickly overwhelms modern computational data resources modest choices 
heavy tailed nature language zipf law encounter novel grams witnessed training 
mechanism assigning non zero probability novel grams central unavoidable issue 
standard approach smoothing probability estimates cope sparse data problems cope potentially missing grams sort back estimator jw jw jw jw discount discounted probability normalization constant calculated discounted probability computed di erent smoothing approaches including linear smoothing absolute smoothing turing smoothing witten bell smoothing :10.1.1.131.5458
note basic unit language models described word 
consider text concatenated sequence characters words 
formulation character language model size vocabulary smaller greatly reduces sparse data problem 
character level models avoid word segmentation problems occur asian languages chinese japanese 
allow comparison laplace smoothing eq 
brie describe standard smoothing techniques gram language modeling 
absolute smoothing frequency word subtracted constant discounted probability eq 
calculated jw de ned upper bound number events occur exactly times training data 
notation applies smoothing techniques 
linear smoothing discounted probability calculated jw number uni grams corresponds number words training data 
turing smoothing turing smoothing discounts frequency gt nr nr discounted probability calculated jw gt witten bell smoothing witten bell smoothing similar laplace smoothing reserves probability mass oov values laplace smoothing 
discounted probability calculated jw number distinct words follow training data 
uni gram model corresponds size vocabulary 
show naive bayes models statistical gram models combined form chain augmented naive bayes classi er 
gram language models text classi ers text classi ers attempt identify attributes distinguish documents di erent categories :10.1.1.144.7475
attributes may include vocabulary terms word average length local grams global syntactic semantic properties 
language models attempt capture regularities provide natural avenue constructing text classi ers 
gram language model applied text classi cation similar manner naive bayes model 
case arg max fp cjd arg max fp djc arg max fp djc arg max jw step eq 
eq 
assumes uniform prior categories step eq 
eq 
uses markov gram independence assumption 
likelihood related perplexity entropy eq 
eq 

principle gram language model text classi er determine category document generated category model eq 

train separate language model category classify new document evaluating likelihood category choosing category eq 

parameters model jw jt 
di erent naive bayes classi er equ 
equ 
considers words testing may vocabulary 
difference naive bayes text classi er language modeling text classi er 
see di erence noticeable improvements 
naive bayes text classi er attributes words considered independent category 
language modeling approach enhanced considering markov dependence adjacent witten bell smoothing misnomer invented alistair moffat called method ppm text compression 
words 
due similarity refer gram augmented naive bayes classi er chain augmented naive bayes classi er 
graphical model bi gram augmented naive bayes text classi er fig 

vn 
fig 

graphical model bi gram chain augmented naive bayes classi er experimentally evaluate performance chain augmented naive bayes classi er real word data sets 
empirical evaluation experimental results di erent text classi cation problems di erent languages 
consider greek authorship attribution problem greek documents classi ed written modern greek authors 
consider english newsgroup topic detection problem documents classi ed belonging newsgroups 
consider chinese trec topic detection problem chinese documents classi ed topics 
data sets performance measure greek authorship attribution data previously investigated downloaded www site modern greek weekly newspaper 
corpus consists document collections containing works di erent authors totaling documents 
experiments divided collections training testing documents author 
authors data sets shown table 
considered concatenated character string delimited white space word introduces noise character strings concatenated punctuation marks 
perform extra pre processing remove noise 
number unique words observed training corpus 
select frequent words form vocabulary comprises total word occurrences 
english newsgroup data investigated research text categorization :10.1.1.144.7475:10.1.1.20.1365
contains non empty documents evenly distributed newsgroups form categories :10.1.1.144.7475
randomly select documents training leave remaining testing 
experiments selected frequent words form vocabulary comprises total word available www ai mit edu newsgroups code author name code author name table :10.1.1.144.7475
authors greek data set occurrences 
greek data set selected words noise remove 
chinese data set previously investigated 
corpus subset trec people daily news corpus published linguistic data consortium ldc 
entire trec data set contains documents variety topics including international domestic news sports culture 
corpus originally intended research information retrieval 
data set suitable text categorization rst clustered documents groups shared headline indicated sgml tag 
frequent groups selected chinese text categorization data set shown table 
group documents randomly selected training documents reserved testing 
experiments selected frequent chinese characters serve vocabulary comprised total character occurrences training corpus 
code topic politics law society literature arts education science culture sports theory academy economics table :10.1.1.144.7475
topics chinese trec data set sake consistency previous research measure categorization performance accuracy number correctly identi ed texts divided total number texts considered 
measure performance macro measure average measures categories 
measure combination precision recall :10.1.1.109.2516
experimental results results greek authorship attribution shown table 
upper half shows results considering vocabulary oov words lower half shows results considering oov words 
oov rate average rate words vocabulary testing documents 
rst column order gram model column represents di erent smoothing technique 
considering oov words adding absolute turing linear witten bell acc 
mac acc 
mac acc 
mac acc 
mac acc 
mac considering oov words oov rate table :10.1.1.144.7475
results greek authorship attribution results topic detection english newsgroup data set shown table :10.1.1.144.7475
considering oov words adding absolute turing linear witten bell acc 
mac acc 
mac acc 
mac acc 
mac acc 
mac considering oov words oov rate table :10.1.1.144.7475
topic detection results english newsgroup data word models chinese topic detection dicult words white space delimited english greek 
normally word segmenter required chinese text classi cation 
avoid need explicit segmentation character level bayes classi er 
table shows experimental results chinese topic detection 
greek english apply character level bayes classi ers 
results character level models greek english shown table table respectively 
size character vocabulary greek english 
discussion performance language model text classi er depends factors including order gram models smoothing techniques considering oov words adding absolute turing linear witten bell acc 
mac acc 
mac acc 
mac acc 
mac acc 
mac considering oov words oov rate table :10.1.1.144.7475
topic detection results chinese trec data character models laplace absolute turing linear witten bell acc 
mac acc 
mac acc 
mac acc 
mac acc 
mac table :10.1.1.144.7475
results greek authorship attribution character models oov characters encountered considering oov words character level model word level model 
discuss uence factors 
ects order gram models relaxing naive bayes independence assumption consider local context dependencies main motivations bayes model 
table table obviously see increase classi cation accuracy longer context 
increase stops point 
due sparseness data larger 
additional information provided longer context compromised data sparseness 
data sparseness explains context information help chinese data grams 
performance increase gram gram increase :10.1.1.144.7475
commonly characters chinese compared upper case lower case punctuation 
english 
data sparseness serious chinese english 
chinese words consist characters 
bi grams ective chinese ir 
longer context information help improve decrease signi cantly adding smoothing classi cation accuracies greek english laplace absolute turing linear witten bell acc 
mac acc 
mac acc 
mac acc 
mac acc 
mac table :10.1.1.144.7475
topic detection results english newsgroup data character models oov characters word level witten bell smoothing may due poor performance uni gram models sparse data problem word level serious character level 
relaxing independence assumptions help cases sucient training data reduce sparse data problem character level models 
sucient training data important short context models avoid sparse data problems 
results may er explanation naive bayes classi er preferred practice normally get sucient labeled training data train large context model 
greek authorship attribution english topic detection chinese topic detection accuracy order gram models absolute turing linear witten bell adding fig :10.1.1.144.7475

classi cation performance curves greek authorship attribution entropy english topic detection entropy chinese topic detection entropy order gram models absolute turing linear witten bell adding fig :10.1.1.144.7475
:10.1.1.144.7475
entropy curves ect smoothing technique results comprehensible illustrate upper parts table :10.1.1.144.7475
illustrate corresponding entropy curves :10.1.1.144.7475
entropy averaged testing documents 
obvious phenomena di erent smoothing techniques behave quite di erently experiments 
adding smoothing performs worse smoothing uni gram model greek dataset 
strange phenomena adding smoothing ts earlier smoothing 
absolute smoothing linear smoothing generally perform 
witten bell smoothing mo smoothing perform word level model despite performs best character level models table 
absolute linear smoothing performed best laplace smoothing performed worst experiments 
ect considering oov words mentioned discarding oov words loses information provided values 
weakness revealed experiments 
comparing upper parts lower parts tables see oov rate low example table performance obtained ignoring oov words roughly considering oov words performance :10.1.1.144.7475
oov rate high example table greek data set oov words ignoring oov words noticeably damage performance smoothing techniques reduction 
implies considering oov words better ignoring sparse data problem serious 
character level versus word level asian languages chinese japanese word segmentation hard character level bayes model suited text classi cation avoids need word segmentation 
western languages greek english word character levels 
experiments character level models worked slightly better word level models english newsgroup data set vs 
appears character level models capturing regularity word level models missing case 
relaxing context character level models capture regularities word level phrase level regularities 
experiments greek data character level models performed worse word level models vs 
far remains inconclusive level best text classi cation western languages 
suggests combining levels result robust better results 
performance compared state art results reported comparable better start art results data sets 
accuracy greek authorship attribution better reported complicated analysis 
accuracy newsgroups data set better best results reported combination svm error correcting output coding ecoc 
accuracy chinese trec data level results reported svm word segmentation feature selection 
chain augmented naive bayes classi er works simpler technique methods specialized particular data set 
relationship gram text categorization considered :10.1.1.144.7475:10.1.1.162.2994
grams features traditional feature selection process deployed classi ers calculating feature vector similarities 
bayes models perform feature selection course decide factors order smoothing technique vocabulary size 
grams remain model importance implicitly considered contribution perplexity 
language modeling techniques text categorization new research area ir done 
teahan harper ppm compression method text categorization seek model obtains best compression new document :10.1.1.144.7475
ppm compression model deals oov words escape method essentially weighted linear interpolation di erent gram models 
gram models related phrase weighting information retrieval 
gram considered phrase weight calculated smoothed frequency counting 
relaxing independence assumption naive bayes model researched idea machine learning researchers considered learning tree augmented naive bayes classi ers data 
learning hidden tree structure problematic chain augmented naive bayes model special case tan bayes better preserves simplicity naive bayes classi er introducing additional dependencies tan bayes 
chain augmented naive bayes classi er statistical gram language modeling 
bayes model captures dependence adjacent attributes markov chain 
better smoothing techniques laplace smoothing obtain performance improvements 
bayes modeling approach able character level word level provides language independent abilities handle eastern languages chinese japanese just easily western languages english greek 
experimental results support utility approach 
currently investigating data sets reuters data set 
acknowledgments supplying greek authorship attribution data ji chinese topic detection data anonymous reviewers constructive comments william teahan clari cation witten bell smoothing 

bell cleary witten 

text compression 
prentice hall 

chen goodman 

empirical study smoothing techniques language modeling 
technical report tr harvard university 


:10.1.1.144.7475
gram text categorization 
proceedings sdair :10.1.1.144.7475
:10.1.1.144.7475
domingos pazzani 

independence conditions optimality simple bayesian classi er 
machine learning 
duda hart 

pattern classi cation scene analysis 
wiley ny 

lewis madigan 

naive bayes model text categorization 
appear arti cial intelligence statistics 

friedman geiger goldszmidt 

bayesian network classi ers 
machine learning 

tan tan 

comparative study chinese text categorization methods 
proceedings pricai international workshop text web mining :10.1.1.144.7475

hiemstra 

language models information retrieval 
ph thesis centre telematics information technology university twente 

keogh 

learning augmented bayesian classi ers comparison distribution classi cation approaches 
arti cial intelligence statistics 
kwok 

employing multiple representations chinese information retrieval jasis 

lewis 

naive bayes independence assumption information retrieval 
proceedings ecml 

manning sch 

foundations statistical natural language processing mit press cambridge massachusetts 
:10.1.1.144.7475
mccallum nigam 

comparison event models naive bayes text classi cation 
proceedings aaai workshop learning text categorization aaai 

ney essen kneser 
:10.1.1.144.7475
structuring probabilistic dependencies stochastic language modeling 
comput 
speech lang 

pazzani billsus 

learning revising user pro les identi cation interesting web sites 
machine learning 

ponte croft 

language modeling approach information retrieval 
proceedings sigir 

rennie 

improving multi class text classi cation naive bayes 
master thesis 
ai technical report :10.1.1.144.7475


rish 

empirical study naive bayes classi er 
proceedings ijcai workshop empirical methods arti cial intelligence 

robertson sparck jones 

relevance weighting search terms 
jasis :10.1.1.144.7475

scott matwin 

feature engineering text classi cation 
proceedings icml pp 


sebastiani 

machine learning automated text categorization 
acm computing surveys :10.1.1.144.7475

kokkinakis 

automatic text categorization terms genre author 
comput 
ling pp :10.1.1.144.7475
:10.1.1.144.7475
teahan harper 

compression language models text categorization 
proceedings workshop 

mo 

statistical phrases vector space information retrieval 
proceedings sigir pp 


yang 

evaluation statistical approaches text categorization 
information retrieval vol 
pp 

