linguistic feature extraction independent component analysis timo honkela helsinki university technology neural networks research center laboratory computer information science www cis hut fi tho aim find syntactic semantic relationships words analysis corpora 
propose application independent component analysis clear advantages classic methods latent semantic analysis self organizing maps 
latent semantic analysis simple method automatic generation concepts useful encoding documents information retrieval purposes 
concepts easily interpreted humans 
self organizing maps generate explicit diagram characterizes relationships words 
resulting map reflects syntactic categories organization semantic categories local level 
self organizing map provide explicit distinct categories words 
independent component analysis applied word context data gives distinct features reflect syntactic semantic categories 
independent component analysis gives features categories explicit easily interpreted humans 
result obtained human supervision tagged corpora predetermined morphological syntactic semantic information 
word belong syntactic categories simultaneously 
number categories higher takes account semantic categories 
traditionally categorization determined hand categories word belongs described dictionary 
study emergence linguistic representations analysis words contexts 
give general description approach describe methods widely analysis latent semantic analysis self organizing map 
introduce novel approach independent component analysis 
analysis words contexts contextual information widely statistical analysis natural language corpora consider 
handling computerized form written language rests processing discrete symbols 
symbolic input word numeric algorithm 
similarity appearance words usually correlate content refer 
simple example may consider words window glass widow 
words window widow phonetically close hyv rinen hiit basic research unit department computer science university helsinki www cs helsinki fi hyvarinen semantic relatedness words window glass reflected simple metric 
useful numerical representation obtained account sentential context words occur 
represent word vector ndimensional space code context average vectors representing words context 
simplest case dimension taken equal number different words word represented vector element equal equal zero 
context vector simply gives frequency word context 
information retrieval similar approach called bag words cf 
vector space model 
computational reasons dimension may reduced different methods 
classic method reducing dimension vector space model latent semantic analysis described 
latent semantic analysis latent semantic analysis technique known singular value decomposition svd create latent semantic space 
term document matrix generated 
term represented row matrix document represented column 
individual entry aij represents frequency term document svd decompose matrix separate matrices 
matrix term concept matrix second matrix concept concept matrix third matrix concept document matrix special case coding contexts explained context document lsa 
lsa described terms learning cognitive science 
claim lsa acquired knowledge full vocabulary english comparable rate school children 
development lsa motivated practical applications 
problem lsa concept space difficult understand humans 
self organizing map introduced section creates visual display analysis results readily understandable human viewer 
self organizing map words self organizing map analysis word context data artificially generated short sentences grimm tales 
selforganizing map analysis word contexts performed dimensional map order find synonymous words 
result called self organizing map words word category map 
earlier name self organizing semantic map 
similar results miikkulainen 
consider thorough analysis explanation methodology 
areas local regions word category map considered implicit categories classes emerged learning process 
single nodes map considered adaptive prototypes 
prototype involved adaptation process neighbors influence map gradually finding form best represent input 
emergent categories word category map implicit 
categories determined separately 
beneficial find categories automated analysis 
word appears location map 
means things map characteristics categories word represented categories overlap accordingly corresponding areas map overlap 
cases case possible see area modal verbs inside area verbs map 
wish find sparse encoding words way collection features associated word 
instance word verb copula verb connects subject complement past tense 
old idea linguistics associate words features 
features syntactic semantic proposed 
traditional linguistic analysis features hand membership crisp 
data collection analysis ica propose independent component analysis ica extraction linguistic features text corpora detailed methodological description 
ica learns features unsupervised manner 
features word ica gives explicit values feature word 
expect features coincide known syntactic semantic categories instance expect ica able find feature shared words may 
earlier studies independent component analysis document level analysis texts see 
data collection ii 
data methods data experiments consists collection mails sent connectionists mailing list texts concatenated file 
punctuation marks removed uppercase letters replaced corresponding lowercase letters 
resulting corpus consists tokens words running text types different unique words 
analysis common words manually selected contextual information calculated common types way 
formed context matrix cij denotes number occurrences jth word immediate context ith word ith word followed jth word words 
provided matrix illustrated fig 

fig 

illustration matrix contextual data 
logarithm number occurrences taken order reduce effect common words analysis 
independent component analysis give brief outline basic theory independent component analysis 
classic version ica model expressed xn vector observed random variables vector independent latent variables denoted 
sn independent components unknown constant matrix called mixing matrix 
denote columns matrix aj model written goal ica learn decomposition eq 
unsupervised manner 
observe want estimate ica seen extension principal component analysis factor analysis www cs cmu edu afs cs cmu edu project connect connect archives underlie lsa 
ica powerful technique capable finding underlying factors classic methods fail 
starting point ica simple assumption si statistically independent 
variables independent information value give information value vice versa 
need hold observed variables xi 
case variables independence holds 
definition extends number random variables 
properties ica taken account considering analysis results 
determine variances independent components si 
reason unknown scalar multiplier sources si canceled dividing corresponding column ai scalar 
normalization step assume component unit variance si 
ambiguity sign remains multiply component affecting model 
second property remembered determine order components 
unknown freely change order terms eq 
call components 
third important property ica independent components nongaussian ica possible 
mixing matrix estimated order sign discussed 
stark contrast techniques principal component analysis factor analysis able estimate mixing matrix rotation quite insufficient purposes 
ica analyses applied fastica software package matlab 
fed word context matrix fastica algorithm column considered data point row random variable 
standard maximum likelihood estimation setting nonlinearity tanh function symmetric orthogonalization 
dimension data reduced principal component analysis implemented part software reduction dimension reduce noise overlearning 
number independent components reduced 
iii 
linguistic features extracted ica results ica analysis corresponded cases reasonably known intuitively plausible linguistic categories 
system able automatically create distributed representations www cis hut fi projects ica fastica matlab code operations follows lc log fastica lc approach symm tanh epsilon meaningful collection emergent linguistic features independent component feature 
show examples analysis results 
considering feature distributions keep mind sign features arbitrary 
mentioned earlier ambiguity sign multiply component affecting model see section 
numbering order components arbitrary 
fig 
shows third component strong case nouns singular form 
similar pattern nouns exceptional cases additional strong fourth component indicated fig 

reason appears psychology neuroscience share semantic feature science scientific discipline 
similar pattern words engineering biology 
group words provide clear example distributed representation case components involved 
model pattern problem result fig 

ica features model problem pattern results 
word show values independent components bar plot 
neuroscience psychology fig 

ica features neuroscience psychology 
interesting point comparison fig 
collection plural forms nouns fig 

third component strong singular nouns strong component fifth 
models problems fig 

ica features models problems 
fig 
shows possessive pronouns share feature number 
fig 

ica features 
modal verbs represented clearly component number shown fig 

slightly modal verbs directly linked verbs general shared component 
may distinct nature modal verbs 
remember analysis number ica features sets limit complexity feature encoding 
limit order demonstrate usefulness method simple manner 
higher number features order obtain detailed feature distinctions 
fig 
shows adjectives related shared feature number number opposite direction 
quite interestingly component number associated ing verbs see fig 
modeling training naturally serve position adjective noun consider instance training set versus network training 
fig 
shows articles feature dimensions sixth seventh 
may fig 

ica features may 
adaptive fig 

ica features adaptive artificial 
individual words particularly verbs result clear words 
fig 
shown copula features distributed manner 
word shares clearly feature number word 
collection particles similar common words excluded analysis unique considering contexts appear 
phenomenon discernable analysis word contexts self organizing map 
categorical nature component illustrated listing words give strongest response modeling artificial training fig 

ica features modeling training 
fig 

ica features 
fig 

ica features 
value component see fig 
fig 

result shows clear components considered noun categories 
components discussed earlier 
component number responds adjectives number contains modal verbs 
verbs different forms component 
see certain kind component overloading components 
explained limited number component 
larger number components detailed categories gained ambiguity inside category avoided 
science networks information university systems engineering learning papers research models system psychology processing neuroscience algorithms networks technology recognition 
fig 

representative words features components order representativeness top highest 
nouns network control component fig 
corpus noun phrases neural network society 
general area style texts corpus course reflected analysis results 
neural computational cognitive may network adaptive learning control research learning processing 
fig 

representative words features components order representativeness top highest 
iv 
discussion article started discussing advantages limitations latent semantic analysis self organizing maps analysis word contexts 
latent semantic analysis suffers limitation underlying semantic space remains implicit 
self organizing map able explicate semantic space relationships map 
categories remain implicit position word map limitation considering intuitive idea word may belong categories simultaneously 
shown independent component analysis bring additional advantage finding explicit features characterize words intuitively appealing manner 
considered methods analysis words appear text corpora 
methods beneficial automatic statistical methods linguistic analysis 
independent component analysis appears possible qualitatively new kind result earlier obtainable hand analysis 
analysis results show ica analysis able reveal underlying linguistic features solely contextual information 
results include emergence clear distinctive categories features distributed representation 
fact word may belong categories simultaneously 
illustration purposes kept number features low 
similar approach scales higher numbers dimensions 
research directions include analysis larger corpora extracting larger number independent components 
various options determining contextual window tested 
qualitative level words similar meanings considered 
component values applied degrees membership word category question analysis 
interpret estimated components linguistic features necessary measure capture linguistic information 
study closeness match emergent components manually determined linguistic categories 
optimistic approach relevant areas general cognitive linguistics language technology 
potential practical application areas include information retrieval machine translation 
distributed representation motivated low dimensional encoding words different applications 
limited number dimensions brings computational efficiency meaningful interpretation component provides basis intelligent processing 
fact features obtained automated analysis cost effective solution compared traditional manual development dictionaries linguistic knowledge bases 
cognitive linguistics wish model provide additional understanding potential cognitive mechanisms natural language learning understanding 
approach assumption linguistic knowledge emergent nature specific learning mechanisms 
shown independent component analysis additional qualitative advantages compared traditional artificial neural network statistical machine learning methods 
bingham lagus 
ica som text document analysis 
proceedings th acm sigir conference research development information retrieval pages 
church hanks 
word association norms mutual information lexicography 
computational linguistics 
comon 
independent component analysis new concept 
signal processing 
deerwester dumais landauer furnas harshman 
indexing latent semantic analysis 
journal american society information science 
ch fillmore 
universals linguistic theory chapter case case pages 
holt rinehart winston 
finch chater 
unsupervised methods finding linguistic categories 
aleksander taylor editors artificial neural networks pages ii 
north holland 
furnas landauer gomez dumais 
vocabulary problem human system communication 
communications acm 
honkela 
learning understand general aspects self organizing maps natural language processing computing anticipatory systems pages 
american institute physics woodbury new york 
honkela kohonen 
contextual relations words grimm tales analyzed self organizing map 
proceedings icann international conference artificial neural networks volume pages 
ec cie 
hyv rinen 
fast robust fixed point algorithms independent component analysis 
ieee transactions neural networks 
hyv rinen karhunen oja 
independent component analysis 
john wiley sons 
isbell viola 
restructuring sparse high dimensional data effective retrieval 
advances neural information processing systems volume pages 
jutten 
blind separation sources part adaptive algorithm neuromimetic architecture 
signal processing 
hansen 
independent components text 
advances neural information processing systems volume pages 
landauer dumais 
solution plato problem latent semantic analysis theory acquisition induction representation knowledge 
psychological review 
manning sch tze 
foundations statistical natural language processing 
mit press cambridge ma 
miikkulainen 
distributed feature map model lexicon 
proceedings th annual conference cognitive science society pages hillsdale nj 
lawrence erlbaum 
miikkulainen 
subsymbolic natural language processing integrated model scripts lexicon memory 
mit press cambridge ma 
miikkulainen 
self organizing feature map model lexicon 
brain language 
ritter kohonen 
self organizing semantic maps 
biological cybernetics 
salton wong yang 
vector space model automatic indexing 
communications acm 
sch tze 
dimensions meaning 
proceedings supercomputing pages 
