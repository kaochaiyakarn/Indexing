factored frontier algorithm approximate inference dbns kevin murphy yair weiss computer science department university california berkeley ca cs berkeley edu factored frontier ff algorithm simple approximate inference algorithm dynamic bayesian networks dbns 
similar fully factorized version boyen koller bk algorithm doing exact update step followed marginalisation projection works factored distributions 
applied models exact update step intractable 
show ff equivalent iteration loopy belief propagation lbp original dbn bk equivalent iteration lbp dbn cluster nodes 
show empirically iterating lbp improve accuracy ff bk 
compare algorithms real world dbns rst model water treatment plant second coupled hmm model freeway trac 
dynamic bayesian networks dbns directed graphical models stochastic processes 
generalise hidden markov models hmms representing hidden observed state terms state variables complex interdependencies 
graphical structure provides easy way specify conditional independencies provide compact parameterization model 
see figures examples 
concerned task ine probabilistic inference dbns computing jy th hidden node time evidence vector time simple adapt algorithms online ltering computing jy prediction computing jy 
assume hidden nodes discrete possible values 
observed nodes discrete continuous 
simplest way perform exact inference dbn convert model hmm apply forwards backwards algorithm :10.1.1.131.2084
takes tq time 
exploiting conditional independencies slice possible reduce time maximum fan node 
unfortunately exponential fact show case assuming graph connected direct connection nodes neighboring time slices correlated time virtue sharing common uences past 
case static networks need approximations sparse models 
section new approximation called factored frontier ff algorithm represents belief state product marginals 
ff algorithm similar fully factorized version boyen koller bk algorithm summarise section 
ff aggressive approximation applied bk intractable ff take time bk take depending graph 
addition ff simpler bk explicitely rely junction tree algorithm 
section show ff bk related loopy belief propagation lbp method applying pearl message passing algorithm bayes net contains undirected cycles loops :10.1.1.32.5538:10.1.1.27.8431:10.1.1.115.4360
section experimentally compare algorithms exact ff bk lbp number problems section conclude 
coupled hmm chains 
square nodes discrete round continuous 
clear nodes hidden shaded nodes observed 
freeway trac application section represents hidden trac status free owing congested location freeway time assumed generate local noisy measurement trac speed depend previous state previous state upstream downstream neighbors 
dbn designed monitor waste water plant 
model originally modi ed include discrete evidence nodes :10.1.1.119.6111
exact inference start reviewing forwards backwards fb algorithm hmms frontier algorithm dbns form basis generalisation :10.1.1.131.2084
forwards backwards algorithm basic idea fb algorithm compute def forwards pass def backwards pass combine produce nal answer def def jjx transition matrix def jx diagonal matrix containing conditional likelihood evidence time algorithm just matrix vector multiplication 
speci cally forwards pass compute backwards pass compute constants proportionality simply normalizing constants 
boundary conditions def prior 
possible states fb algorithm clearly takes time 
frontier algorithm vector hidden nodes possible values possible states fb algorithm intractable 
frontier algorithm way computing similarly needing form transition matrix multiply 
basic idea sweep markov blanket dbn rst forwards backwards 
shall call nodes markov blanket frontier set denote nodes left right frontier denoted step algorithm separates maintain joint distribution nodes advance frontier slice follows 
move node soon parents keep frontier small possible move node soon children adding node entails multiplying conditional probability table cpt jpa frontier removing node entails frontier 
best explained example see 
consider coupled hmm chmm shown 
frontier initially contains nodes slice 
add add remove remove add frontier algorithm applied chmm observed leaves omitted clarity 
nodes inside box frontier 
node operated shown shaded connections parents children shown arcs omitted clarity 
see text details 
def jy 
advance frontier moving multiply cpt jx jy jx add jy jx nodes depend frontier marginalize move jy process continues way compute jy weight factor likelihood jy jx clear example exact inference takes time space frontier contains nodes takes steps sweep frontier general running time frontier algorithm exponential size largest frontier quantity known induced width underlying moral graph 
keep frontiers small possible 
unfortunately computing order add remove nodes minimize sum frontier sizes equivalent nding optimal elimination ordering known np hard 
heuristics methods greedy search perform exhaustive search branch bound 
special case frontier algorithm applied factorial hmms published appendix :10.1.1.16.2929
fhmm cross links hidden nodes constraints order nodes added removed frontier 
regular dbns frontier algorithm equivalent junction tree algorithm applied unrolled dbn :10.1.1.150.82
particular frontier sets correspond maximal cliques moralized triangulated graph junction tree cliques connected chain possibly smaller cliques hanging backbone accomodate non persistent observed leaves 
despite equivalence junction tree frontier algorithm simple form basis approximation algorithm discussed section 
approximate inference factored frontier algorithm problem frontier junction tree algorithms need exponential space just represent belief states need time compute 
idea factored frontier ff algorithm approximate belief state product marginals jy regular dbn certain restrictions topology 
denote hidden nodes time slice observed nodes 
regular dbn connections 
particular intra slice connections nodes 
furthermore assume node connects nodes persistent 
dbns regular 
jy 
backward messages approximated similar way 
algorithm proceeds follows add node frontier multiply cpt product factors corresponding parents creates joint distribution family 
immediately marginalize parent nodes 
node order long add parents children 
backwards pass analogous 
frontier algorithm maintain joint distribution frontier nodes factored form 
algorithm clearly takes time matter topology 
boyen koller algorithm boyen koller algorithm represents belief state jy product marginals clusters jy jy subset variables fx :10.1.1.119.6111
clusters need disjoint 
factored prior step exact bayesian updating compute posterior general factored need project space factored distributions computing marginal cluster 
product marginals gives approximate posterior similar method computing backward messages ecient manner 
boyen koller prove roughly speaking error introduced projection step isn greater error incurred approximate prior errors relative true uncomputable distribution error bounded 
accuracy bk algorithm depends size clusters approximate belief state 
exact inference corresponds single cluster containing hidden variables timeslice 
aggressive approximation corresponds clusters variable call fully factorized approximation 
clear fully factorized version bk similar ff algorithm important di erence bk assumes update factored prior exactly say junction tree computing marginals ff computes approximate marginals directly 
bk obviously accurate ff step exact updating expensive 
cost bk determined size maximal cliques moralized triangulated version slice dbn 
unrolling dbn slices induces long distance correlations results cliques span time slice saw 
coupled hmm chmm model cliques just correspond families nodes parents algorithm takes time ff 
water model see get extra non local cliques due triangulation 
complex models generalisation chmm time slice lattice cell depends nodes receptive eld previous slice neighbors slice largest clique size running time bk fully factorized case 
bk ff special cases loopy belief propagation pearl belief propagation algorithm way computing exact marginal posterior probabilities graphs undirected cycles loops 
essentially generalises forwards backwards algorithm trees 
applied graph loops algorithm called loopy belief propagation lbp case resulting posteriors may correct oscillate 
outstanding empirical success turbo decoding shown equivalent lbp created great interest algorithm :10.1.1.115.4360
lbp empirically shown kinds bayesian networks quite di erent turbo codes :10.1.1.32.5538:10.1.1.27.8431
addition number theoretical results proved networks nodes gaussian networks single loop general networks max product viterbi version sum product forwards backwards version algorithm 
key assumption lbp messages coming node independent 
exactly assumption ff algorithm 
show algorithms equivalent speci order send messages 
normally implement lbp decentralized message passing protocol step node computes parallel incoming message previous step sends messages neighbors 
imagine forwards backwards fb protocol node rst sends messages left right sends messages right left 
single pass fb pro illustration clustering process 
modi ed version chmm chains 
big mega nodes contain joint distribution slice 
omitted observed leaves clarity 
lbp applied graph equivalent bk 
created overlapping clusters size additional accuracy 
tocol equivalent ff 
lbp converges protocols give results di erent behavior short term 
particular dbn fact hmm single fb iteration tn message computations result exact posteriors requires iterations decentralized protocol iteration computing tn messages parallel reach result centralized algorithm ecient 
loopy graphs clear protocol better depends local global information important computing posteriors 
easy see fully factorized version bk equivalent single fb pass lbp applied modi ed dbn shown 
slice create mega nodes contains hidden nodes slice 
messages coming rst mega node assumed independent multiplied form approximate prior single message sent second mega node corresponding exact update step transition matrix nally individual marginals computed process repeated 
course bk construct mega nodes exact update junction tree algorithms functionally equivalent 
simulate bk clusters contain node simply create new clustered nodes addition mega nodes run lbp case noisy nodes ecient ways compute messages having exponential number parents 
reduces complexity ff 
directed graph naive pearl take time compute mega node qn time exploiting fact cpt factorizes 
alternatively undirected graph computation messages takes time linear number neighbors 
new graph illustrated 
ff bk equivalent iteration lbp regular clustered graphs respectively improve algorithms iterating 
gives algorithm opportunity recover incorrect independence assumptions 
see section small number iterations help dramatically 
free energy iterated bk equivalence bk single iteration lbp clustered graph allows utilize result yedidia obtain free energy iterated bk 
de ne iterated bk algorithm running lbp clustered graph fb schedule convergence 
rst iteration iterated bk equivalent bk subsequent iterations messages interact improve quality approximation 
analysis shows iterated bk converge zero gradient points bethe free energy 
sheds light relationship iterated bk mean eld mf approximation 
mf free energy iterated bk free energy joint distributions pairs nodes replaced product marginal beliefs individual nodes iterated bk captures dependencies nodes subsequent slices mf 
result holds iterated bk ordinary bk thought rst approximation iterated bk 
experimental results compare accuracy lbp bk ff chmm model chains trained real freeway trac data exact em 
de ne error 
jp iterations error results applying lbp trac chmm chains 
lower solid horizontal line error incurred bk 
oscillating line error incurred lbp damping factor lowest curve corresponds highest 
upper horizontal line corresponds updating messages gives indication performance local evidence 

exact posterior 
approximate posterior 
plot iterations lbp 
clearly posteriors oscillating happens sequences model 
damping trick described :10.1.1.119.6111
case new message de ned convex combination usual expression old weight old message 
corresponds propagation corresponds updating messages local evidence 
easy show xed points reached algorithm xed points original set equations 
clear damping helps considerably 
results summarised see small number iterations lbp doing better bk 
sequences give similar behavior 
check results speci model data set compared algorithms water dbn shown 
generated observation sequences length model random parameters binary nodes compared marginal posteriors function iterations water network 
number iterations damping factor 
results typical sequence shown 
time see oscillation iterations lbp outperform bk 
addition accuracy interesting see algorithms compare terms speed 
generated random data chmms chains computed posteriors di erent methods 
running times shown 
clear bk ff lbp running time linear chmm model constant factors bk higher due complexity algorithm particular need perform repeated 
bk slower exact inference asymptotically ecient 
described simple approximate inference algorithm dbns shown equivalent single iteration loopy belief propagation lbp 
elucidated connection bk algorithm lbp free energy lbp compare bk mean eld approximations 
showed empirically lbp improve accuracy ff bk 
algorithms implemented matlab included bayes net toolbox downloaded www cs berkeley edu bayes bnt html 
error error iter iter error error iter iter error marginal posteriors vs timeslice iterations lbp applied trac chmm 
oscillations error imply oscillations underlying marginals 
error error iter iter error error iter bk iterations lbp damping factor iteration bk trac chmm 
elapsed time slice seconds num 
chains jtree bk loopy loopy loopy loopy running time chmms function number chains 
dotted curve junction tree steep straight line bk shallow straight lines lbp 
boyen koller 
approximate learning dynamic models 
nips 
boyen koller :10.1.1.119.6111
tractable inference complex stochastic processes 
uai 
cowell dawid lauritzen spiegelhalter 
probabilistic networks expert systems 
springer 
freeman weiss 
xed points max product algorithm 
technical report merl 
appear ieee trans 
info 
theory 
frey 
turbo factor analysis 
neural computation 
submitted 
ghahramani jordan :10.1.1.16.2929
factorial hidden markov models 
machine learning 
jensen olesen pedersen 
expert system control waste water treatment pilot project 
technical report univ aalborg 
danish 
triangulation graphs algorithms giving small total state space 
technical report dept math 
comp 
sci aalborg univ denmark 
computational scheme reasoning dynamic probabilistic networks 
uai 
kwon murphy 
modeling freeway trac coupled hmms 
submitted nips 
mceliece mackay cheng 
turbo decoding instance pearl belief propagation algorithm 
ieee areas comm 
murphy weiss jordan :10.1.1.119.6111
loopy belief propagation approximate inference empirical study 
uai 
pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
peot shachter 
fusion propogation multiple observations belief networks 
arti cial intelligence 
rabiner :10.1.1.131.2084
tutorial hidden markov models selected applications speech recognition 
proc 
ieee 
smyth heckerman jordan 
probabilistic independence networks hidden markov probability models 
neural computation 
weiss 
correctness local probability propagation graphical models loops 
neural computation 
weiss freeman 
optimality solutions max product belief propagation algorithm 
ieee transactions information theory 
appear 
weiss freeman 
correctness belief propagation gaussian graphical models arbitrary topology 
nips 
yedidia freeman weiss 
generalized belief propagation 
nips 
zweig 
forward backward algorithm inference bayesian networks empirical comparison hmms 
master thesis dept comp 
sci berkeley 
