chapter unscented kalman filter eric wan rudolph van der merwe 
book extended kalman lter ekf standard technique performing recursive nonlinear estimation 
ekf algorithm provides approximation optimal nonlinear estimation 
chapter point underlying assumptions aws ekf alternative lter performance superior ekf 
algorithm referred unscented kalman lter ukf rst proposed julier developed wan van der merwe :10.1.1.32.9011
basic di erence ekf ukf stems manner gaussian random variables represented propagating system dynamics 
ekf state distribution approximated propagated analytically rst order linearization nonlinear system 
introduce large errors true posterior mean covariance transformed may lead sub optimal performance divergence lter 
ukf addresses problem deterministic sampling approach 
state distribution approximated represented minimal set carefully chosen sample points 
sample points completely capture true mean covariance propagated true nonlinear system captures posterior mean covariance accurately nd order taylor series expansion nonlinearity 
ekf contrast achieves rst order accuracy 
explicit jacobian hessian calculations necessary ukf 
remarkably computational complexity ukf order ekf 
julier demonstrated substantial performance gains ukf context state estimation nonlinear control 
number theoretical results derived 
chapter reviews presents extensions broader class nonlinear estimation problems including nonlinear system identi cation training neural networks dual estimation problems 
additional material includes development unscented kalman smoother uks speci cation ecient recursive square root implementations novel ukf improve particle lters :10.1.1.32.9011
unscented kalman lter ukf represents derivative free alternative extended kalman lter ekf provides superior performance equivalent computational complexity 
presenting ukf cover number application areas nonlinear estimation ekf applied 
general application areas may divided state estimation parameter estimation learning weights neural network dual estimation expectation maximization em algorithm 
areas place speci requirements ukf ekf developed turn 
overview framework areas brie reviewed 
state estimation basic framework ekf involves estimation state discrete time nonlinear dynamic system represent unobserved state system known exogenous input observed measurement signal 
process noise drives dynamic system observation 
discrete time nonlinear dynamic system noise note assuming additivity noise sources 
system dynamic model assumed known 
simple block diagram system shown 
state estimation ekf standard method choice achieve recursive approximate maximum likelihood estimation state completeness review ekf underlying assumptions section help motivate presentation ukf state estimation section 
parameter estimation parameter estimation referred system identi cation machine learning involves determining nonlinear mapping input output nonlinear map 
parameterized vector nonlinear map example may feedforward recurrent neural network weights numerous applications regression classi cation dynamic modeling 
learning corresponds estimating parameters typically training set provided sample pairs consisting known input desired outputs fx error machine de ned goal learning involves solving parameters order minimize expectation function error 
number optimization approaches exist gradient descent backpropagation ekf may estimate parameters writing new state space representation parameters correspond stationary process identity state transition matrix driven process noise choice variance determines convergence tracking performance discussed detail section 
output corresponds nonlinear observation ekf applied directly ecient second order technique learning parameters 
ekf training neural networks developed singhal wu covered chapter text 
ukf role developed section 
dual estimation special case machine learning arises input unobserved requires coupling state estimation parameter estimation 
dual estimation problems consider 
optimal recursive estimation ekf discrete time nonlinear dynamic system system states set model parameters dynamic system simultaneously estimated observed noisy signal example applications include adaptive nonlinear control noise reduction speech image enhancement determining underlying price nancial time series general theoretical algorithmic framework dual kalman estimation chapter text 
expectation maximization approach covered chapter 
approaches dual estimation utilizing ukf developed section 
section review optimal estimation explain basic assumptions aws ekf 
motivate ukf method amend aws 
detailed development ukf section 
remainder chapter divided application areas reviewed 
conclude chapter section unscented particle filter ukf improve sequential monte carlo ltering methods 
appendix provides derivation accuracy ukf 
appendix details ecient square root implementation ukf 
optimal recursive estimation ekf observations goal estimate state assumptions nature system dynamics point 
optimal estimate minimum mean squared error mmse sense conditional mean jy sequence observations time evaluation expectation requires knowledge posteriori density jy density determine mmse estimator best estimator speci ed performance criterion 
problem determining posterior density general referred bayesian approach evaluated recursively relations jy jy jx jy jy jx jy dx normalizing constant jy jy jy jx dx note write implicit dependence observed input random variable 

optimal recursive estimation ekf recursion speci es current state density function previous density measurement data 
state space model comes play specifying state transition probability jx measurement probability likelihood jx 
speci cally jx determined innovations noise density state update equation example additive noise model gaussian density jx 
similarly jx determined observation noise density measurement equation principle knowledge densities initial condition jy jx determines jy unfortunately multi dimensional integration indicated equations closed form solution intractable systems 
general approach apply monte carlo sampling techniques essentially convert integrals nite sums converge true solution limit 
particle lter discussed section chapter example approach 
basic assumption densities remain gaussian bayesian recursion greatly simpli ed 
case conditional mean jy covariance need evaluated 
straightforward show leads recursive estimation prediction 
prediction xk xk yk linear recursion assumed linearity model 
optimal terms recursion yk yk optimal prediction prior mean written corresponds expectation nonlinear function random variables similar interpretation optimal prediction 
optimal gain term expressed function posterior covariance matrices 
note evaluation covariance terms require expectations nonlinear function prior state variable 
xk prediction covariance yk covariance celebrated kalman lter calculates terms equations exactly linear case viewed ecient method analytically propagating linear system dynamics 
nonlinear models ekf approximates optimal terms yk yk predictions approximated simply function prior mean value expectation taken covariances determined linearizing dynamic equations ax noise means denoted usually assumed equal zero 

unscented kalman filter bv cx dn determining posterior covariance matrices analytically linear system 
words ekf state distribution approximated propagated analytically rst order linearization nonlinear system 
explicit equations ekf table 
ekf viewed providing rst order approximations optimal terms approximations introduce large errors true posterior mean covariance transformed gaussian random variable may lead sub optimal performance divergence lter aws addressed section ukf 
initialize px time update equations extended kalman lter xk measurement update equations xk xk xk xk xk covariances respectively 
table extended kalman lter ekf equations unscented kalman filter ukf addresses approximation issues ekf 
state distribution represented speci ed minimal set carefully chosen sample points 
sample points completely capture true mean covariance propagated second order versions ekf exist increased implementation computational complexity tend prohibit 
popular technique improve rst order approach iterated ekf ectively iterates ekf equations current time step rede ning nominal state estimate measurement equations 
capable providing better performance basic ekf especially case signi cant nonlinearity measurement function 
performed comparison ukf time similar procedure may adapted iterate ukf 

unscented kalman filter true non linear system captures posterior mean covariance accurately nd order taylor series expansion nonlinearity 
elaborate explaining unscented transformation 
unscented transformation unscented transformation ut method calculating statistics random variable undergoes nonlinear transformation 
consider propagating random variable dimension nonlinear function 
assume mean covariance calculate statistics form matrix sigma vectors px px scaling parameter 
constant determines spread sigma points usually set small positive value 
constant secondary scaling parameter usually set see details incorporate prior knowledge distribution gaussian distributions optimal 
ith column matrix square root lower triangular cholesky factorization 
sigma vectors propagated nonlinear function mean covariance approximated weighted sample mean covariance posterior sigma points py fy yg fy yg weights block diagram illustrating steps performing ut shown 
note method di ers substantially general monte carlo sampling methods require orders magnitude sample points attempt propagate accurate possibly non gaussian distribution state 
deceptively simple approach taken ut results approximations accurate rd order gaussian inputs nonlinearities 
non gaussian inputs approximations accurate nd order accuracy third higher 
unscented kalman filter xx cgg weighted sample mean weighted sample covariance gl block diagram ut order moments determined choice proof provided appendix valuable insight ut gained relating numerical technique called gaussian quadrature numerical evaluation integrals 
ito xiong showed relation ut gauss hermite quadrature rule context state estimation 
close similarity exists ut central di erence interpolation ltering cdf techniques developed separately ito xiong ravn 
van der merwe wan shows ukf cdf uni ed general family derivative free kalman lters nonlinear estimation 
simple example shown dimensional system left plot shows true mean covariance propagation monte carlo sampling center plots show results linearization approach done ekf right plots show performance ut note sigma points required 
superior performance ut clear 
unscented kalman filter unscented kalman lter ukf straightforward extension ut recursive estimation equation state rv rede ned concatenation original state noise variables ut sigma point selection scheme equation applied new augmented state rv calculate corresponding sigma matrix ukf equations table 
note explicit calculation jacobians hessians necessary implement algorithm 
furthermore number computations order ekf 
scalar case gauss hermite rule dx equality holds polynomials 
degree quadrature points weights determined rule type see detail 
higher dimensions gauss hermite rule requires order functional evaluations dimension state 
scalar case ut coincides point gauss hermite quadrature rule 

unscented kalman filter actual sampling linearized ekf ut sigma points true mean ut mean covariance weighted sample mean mean ut covariance covariance true covariance transformed sigma points example ut mean covariance propagation 
actual rst order linearization ekf ut implementation variations special case process measurement noise purely additive computational complexity ukf reduced 
case system state need augmented noise rv 
reduces dimension sigma points total number sigma points 
covariances noise sources incorporated state covariance simple additive procedure 
implementation table 
complexity algorithm order dimension state 
complexity ekf 
costly operation forming sample prior covariance matrix depending form may simpli ed univariate time series parameter estimation see section complexity reduces order number variations numerical purposes possible 
example matrix square root implemented directly cholesky factorization general order 
covariance matrices expressed recursively square root computed order dimension output performing recursive update cholesky factorization 
details ecient recursive square root ukf implementation appendix 
unscented kalman filter initialize calculate sigma points time update kjk kjk kjk kjk kjk kjk kjk measurement update equations yk yk kjk kjk xk yk kjk kjk xk yk yk yk yk yk composite scaling parameter dimension augmented state process noise cov measurement noise cov weights calculated eqn 

table unscented kalman filter ukf equations 
unscented kalman filter initialize calculate sigma points time update kjk kjk kjk kjk kjk kjk kjk measurement update equations yk yk kjk kjk kjk kjk yk yk yk yk composite scaling parameter dimension state process noise cov measurement noise cov weights calculated eqn 

table ukf additive zero mean noise case 
unscented kalman filter double inverted pendulum state estimation examples ukf originally designed state estimation applied nonlinear control applications requiring full state feedback 
provide example double inverted pendulum control system 
addition provide new application example corresponding noisy time series estimation neural networks 
double inverted pendulum 
double inverted pendulum see states corresponding cart position velocity top bottom pendulum angle angular velocity 
system parameters correspond length mass pendulum cart mass 
dynamic equations cos cos sin sin cos cos gl sin sin xl cos cos gl sin sin continuous time dynamics discretized sampling period seconds 
pendulum stabilized applying control force cart 
case state dependent equation controller stabilize system state estimator run controller designed formulating dynamic equations note representation linearization reformulation nonlinear dynamics pseudo linear form 
state space representation design optimal lqr controller solution standard equations state dependent matrices 
procedure repeated time step current state provides local asymptotic stability plant 
approach far robust lqr controllers standard linearization techniques alternative advanced nonlinear control approaches 

unscented kalman filter outside control loop order compare ekf ukf estimates states feedback control evaluation purposes 
observation corresponds noisy measurements cart position cart velocity angle top pendulum 
challenging problem measurements bottom pendulum angular velocity top pendulum 
experiment pendulum initialized jack knife position degrees cart set meters 
resulting state estimates shown 
clearly ukf better able track unobserved states estimated states feedback control loop ukf system able stabilize pendulum ekf system crashes 
return double inverted pendulum problem chapter model estimation dual estimation 
observed observed un observed un observed observed time true state noisy observation ekf estimate ukf estimate un observed time state estimation double inverted pendulum problem 
noisy states observed cart position cart velocity angle top pendulum 
db snr alpha beta kappa note states observed noise performance ekf ukf comparable 

unscented kalman filter estimation mackey glass time series ekf clean noisy ekf estimation mackey glass time series ukf clean noisy ukf estimation error ekf vs ukf mackey glass ekf ukf estimation mackey glass time series ekf ukf known model 
bottom graph shows comparison estimation errors complete sequence 
noisy time series estimation example ukf estimate underlying clean time series corrupted additive gaussian white noise 
time series mackey glass chaotic series 
clean times series rst modeled nonlinear autoregression model parameterized approximated training feedforward neural network clean sequence 
residual error convergence taken process noise variance 
white gaussian noise added clean mackey glass series generate noisy time series corresponding state space representation 








unscented kalman filter 

estimation problem noisy time series observed input ekf ukf algorithms utilize known neural network model 
shows sub segment estimates generated ekf ukf original noisy time series db snr 
superior performance ukf clearly visible 
unscented kalman smoother discussed kalman lter recursive algorithm providing conditional expectation state observations current time contrast kalman smoother estimates state observations past nal time 
kalman smoothers commonly applications trajectory planning noncausal noise reduction step em algorithm 
thorough treatment kalman smoother linear case 
basic idea run kalman lter forward time estimate mean covariance state past data 
second kalman lter run backward time produce backward time predicted mean covariance data 
estimates combined producing smoothed statistics data nonlinear case ekf replaces kalman lter 
ekf forward lter straightforward 
implementation backward lter achieved linearized backward time system 
bv forward nonlinear dynamics linearized inverted backward model 
linear kalman lter applied 
proposed unscented kalman smoother uks replaces ekf ukf 
addition consider nonlinear backward model derived rst principles training backward predictor neural network model illustrated time series case 
nonlinear backward model allows take full advantage ukf requires linearization step 
illustrate performance reconsider noisy mackey glass time series problem previous section second time series generated chaotic autoregressive neural network 
table compares smoother performance 
case network models trained clean time series tested noisy data standard extended kalman smoother linearized backward model eks extended kalman smoother second nonlinear backward model eks unscented kalman smoother uks 
forward backward smoothed estimation errors reported 
performance bene ts unscented approach clear 

ukf parameter estimation forward backward neural network prediction training 
mackey glass norm 
mse algorithm eks eks uks chaotic ar nn norm 
mse algorithm eks eks uks ukf parameter estimation recall parameter estimation involves learning nonlinear mapping corresponds set unknown parameters 

may neural network parameterized function 
ekf may estimate parameters writing new state space representation correspond stationary process identity state transition matrix driven process noise desired output corresponds nonlinear observation linear case relationship kalman filter kf popular recursive squares rls 
nonlinear case ekf training corresponds modi ed type method see chapter 
optimization perspective prediction error cost minimized noise covariance constant diagonal matrix fact cancels algorithm shown explicitly set arbitrarily 
alternatively set specify weighted mse cost 
innovations covariance hand ects convergence rate tracking performance 
roughly speaking larger covariance quickly older data discarded 
options choose set arbitrary xed diagonal value may annealed zero training continues 
set rls rls referred forgetting factor de ned recursive squares rls algorithm 
provides approximate 
ukf parameter estimation exponentially decaying weighting past data described fully 
note rls confused sigma point calculation 
set 
stochastic approximation scheme estimating innovations 
method assumes covariance kalman update model consistent actual update model 
typically constrained diagonal matrix implies independence assumption parameters 
note similar update may experience indicates robbins monro method provides fastest rate absolute convergence lowest nal mmse values see experiments section 
xed combination annealing achieve nal mmse performance requires monitoring greater prior knowledge noise levels 
problems mmse zero covariance lower bounded prevent algorithm stalling potential numerical problems 
forgetting factor xed methods appropriate online learning problems tracking time varying parameters necessary 
case parameter covariance stays lower bounded allowing data emphasized 
leads keeps kalman gain suciently large maintain tracking 
general various trade di erent approaches area open research 
ukf represents alternative ekf parameter estimation 
function linear advantage ukf may obvious 
note observation function nonlinear 
furthermore ekf essentially builds approximation expected hessian outer products gradient 
ukf may provide accurate estimate direct approximation expectation hessian 
ekf ukf expected achieve similar nal mmse performance convergence properties may di er 
addition distinct advantage ukf occurs architecture error metric di erentiation respect parameters easily derived necessary ekf 
ukf ectively evaluates jacobian hessian precisely sigma point propagation need perform analytic di erentiation 
speci equations ukf parameter estimation table 
simpli cations relative state ukf accounting speci form state transition function 
table provided options function output achieved 
rst option output kjk corresponding direct interpretation ukf equations 
output expected value mean function random variable second option corresponding typical interpretation output function current best set parameters 
option yields convergence performance indistinguishable ekf 
rst option di erent convergence characteristics requires explanation 
state space approach parameter estimation absolute convergence achieved parameter covariance goes zero forces kalman gain zero 

ukf parameter estimation initialize pw time update sigma point calculation wk kjk kjk kjk option kjk option measurement update equations dk dk kjk kjk kjk kjk dk dk wk dk dk composite scaling parameter dimension state process noise cov measurement noise cov weights calculated eqn 

table ukf parameter estimation point output option identical 
prior nite covariance provides form averaging output function turn prevents parameters going minimum error surface 
method may help avoid falling local minimum 
furthermore provides form built regularization short noisy data sets prone tting exact speci cation level regularization requires study 
note complexity ukf algorithm order number parameters due need compute matrix square root time step 
order complexity ekf achieved recursive square root formulation appendix 
ukf parameter estimation epochs learning curves mackay robot arm nn parameter estimation ekf ukf epochs learning curves ikeda nn parameter estimation ekf ukf top mackay robot arm problem comparison learning curves ekf ukf training mlp annealing noise estimation 
bottom ikeda chaotic time series comparison learning curves ekf ukf training mlp robbins monro noise estimation 
parameter estimation examples performed number experiments illustrate performance ukf parameter estimation approach 
rst set experiments correspond benchmark problems neural network training serve illustrate di erences ekf ukf di erent options discussed 
parametric optimization problems included corresponding model estimation double pendulum benchmark rosenbrock banana optimization problem 
benchmark nn regression time series problems mackay robot arm dataset ikeda chaotic time series benchmark problems compare neural network training 
illustrates di erences learning curves ekf versus ukf option 
note slightly lower nal mse performance ukf weight training 
option ukf output see equation learning curves ekf ukf indistinguishable consistent experiments show explicit learning curves ukf option 
illustrates performance di erences choice process noise covariance 
ukf parameter estimation mackey glass ikeda time series 
plots show comparisons ukf di erences similar ekf 
general robbins monro method robust approach fastest rate convergence 
examples seen faster convergence annealed approach requires additional insight heuristic methods monitor learning 
re iterate xed lambda approaches appropriate line tracking problems 
epochs learning curves ikeda nn parameter estimation fixed lambda anneal robbins monro epochs learning curves mackey glass nn parameter estimation fixed lambda anneal robbins monro neural network parameter estimation di erent methods noise estimation 
top ikeda chaotic time series 
bottom mackey glass chaotic time series ukf state dimension regions classi cation example consider benchmark pattern classi cation problem having interlocking regions 
layer feedforward network mlp nodes trained inputs randomly drawn pattern space desired output value pattern fell assigned region 
illustrates classi cation task learning curves ukf ekf nal classi cation regions 
learning curve epoch represents randomly drawn input samples 
test set evaluated epoch corresponds uniform grid points 
see superior performance ukf 

ukf parameter estimation true mapping nn classification ekf trained nn classification ukf trained learning curves test set epochs ekf ukf singhal wu region classi cation problem 
ukf settings state dimension mlp robbins monro epoch random examples 

ukf parameter estimation iteration inverted double pendulum parameter estimation ekf ukf inverted double pendulum parameter estimation 
ukf settings state dimension robbins monro double inverted pendulum returning double inverted pendulum section consider learning system parameters 
parameter values treated unknown initialized 
full state observed 
shows total model mse versus iteration comparing ekf ukf 
iteration represents pendulum crash di erent initial conditions state control applied 
nal converged parameter estimates true model ukf estimate ekf estimate case ekf converged biased solution possibly corresponding local minimum error surface 
rosenbrock banana function parameter estimation example turn pure optimization problem 
banana function thought dimensional surface saddle curvature bends origin 
speci cally wish nd values minimizes function true minimum 
banana function known test problem compare convergence rates competing minimization techniques 
order ukf ekf basic parameter estimation equations need reformulated minimize non mse cost function 
write state space equations observed error form 
ukf parameter estimation target observation xed zero error term resulting optimization sum instantaneous costs mse cost optimized setting 
arbitrary costs cross entropy minimized simply specifying appropriately 
discussion approach chapter book 
reformulation ukf equations requires changing ective output setting desired response zero 
example hand set furthermore optimization problem special case noiseless parameter estimation actual error minimized zero equation option calculate output ukf algorithm 
allow ukf reach true minimum error surface rapidly set scaling parameter small value appropriate zero mse problems 
circumstances performance ukf ekf indistinguishable illustrated 
performance lters comparable superior number alternative optimization approaches levenberg marquardt see matlab 
main purpose example illustrate versatility ukf general optimization problems 
function value ekf ukf model error ekf ukf rosenbrock banana optimization problem 
ukf settings state dimension fixed note option expected value function output essentially involves averaging output current parameter covariance 
slows convergence case zero mse possible convergence state covariance zero necessary proper annealing state noise innovations 
ukf dual estimation ukf dual estimation recall dual estimation problem consists simultaneously estimating clean state model parameters noisy data see equation 
number algorithmic approaches exist problem including joint dual ekf methods recursive prediction error maximum likelihood versions expectation maximization em approaches 
thorough coverage algorithms chapter text 
section results dual ukf prediction error joint ukf methods 
dual extended kalman lter separate state space representation signal weights 
run simultaneously signal weight estimation 
time step current estimate weights signal lter current estimate signal state weight lter 
dual ukf algorithm state weight estimation done ukf 
joint extended kalman lter signal state weight vectors concatenated single joint state vector estimation done recursively writing state space equations joint state 


running ekf joint state space produce simultaneous estimates states approach ukf ekf 
dual estimation experiments noisy time series results time series provide clear illustration ukf ekf 
rst series mackey glass chaotic series additive noise snr db 
second time series chaotic comes autoregressive neural network random weights driven gaussian process noise corrupted additive white gaussian noise snr db 
standard mlp tanh hidden activation functions linear output layer lters mackey glass problem 
mlp second problem 
process measurement noise variances associated state assumed known 
note contrast state estimation example previous section noisy time series observed 
clean provided training 
example training curves di erent dual joint kalman estimation methods shown 
nal estimate mackey glass series shown dual ukf 
superior performance ukf algorithms clear 

ukf dual estimation epoch chaotic ar neural network dual ukf dual ekf joint ukf joint ekf epoch mackey glass chaotic time series dual ekf dual ukf joint ekf joint ukf estimation mackey glass time series dual ukf clean noisy dual ukf comparative learning curves results dual estimation experiments 
curves averaged runs respectively di erent initial weights 
fixed innovation covariances joint algorithms 
annealed covariances weight lter dual algorithms 

ukf dual estimation mode estimation example illustrates joint ukf estimating modes mass spring system see 
performed university washington mark campbell 
system linear direct estimation natural frequencies jointly states nonlinear estimation problem 
compares performance ekf ukf 
note ekf converge true value experiment input process noise snr approximately db measured positions additive noise db snr settings ectively turn task pure parameter estimation problem 
xed innovations parameter estimation joint algorithms 
sampling done nyquist emphasizes ect linearization ekf 
faster sampling rates performance ekf ukf similar 
xx xx xx xx pos pos un un mass spring system 
ekf ukf actual time ekf ukf actual linear mode prediction 
ukf dual estimation ight simulation example performed university washington joint estimation done aircraft model 
simulation includes vehicle nonlinear dynamics engine sensor noise modeling atmospheric modeling densities pressure look tables 
incorporated aerodynamic forces data wright patterson afb 
closed loop system gain scheduled controller control model 
simulated mission test ukf estimator involved quick descent short tactical run deg turn ascent possible failure horizontal control surface tail aircraft 
measurements consisted states additive noise db snr 
turbulence approximately rms 
mission joint ukf estimated state positions orientations derivatives parameters corresponding aerodynamic forces moments 
done line estimated states control loop 
illustrative results shown estimation altitude velocity lift parameter lift force aircraft 
left column shows mission failure 
right column includes failure seconds 
note failure ukf capable tracking state parameters 
pointed black box nature simulator conducive jacobians necessary running ekf 
implementation ekf comparison performed 
actual ukf time sec actual ukf failure time sec model joint estimation note estimated true values state indistinguishable resolution 
ukf dual estimation double inverted pendulum nal dual estimation example consider double inverted pendulum time estimate states system parameters joint ukf 
observations correspond noisy measurements states 
estimated states fed back closed loop control 
addition parameter estimates time step design controller approach 
illustrates performance adaptive control system showing evolution estimated actual states 
start simulation states parameters unknown control system unstable point 
trial ukf enables convergence stabilization pendulum single crash 
time angle estimates rad angle angle true states noisy observations ukf estimates time cart mass pendulum length mass pendulum length mass true model parameters ukf estimates double inverted pendulum joint estimation 
estimated states parameters 
angle angle plotted radians 

unscented particle filter unscented particle filter particle lter sequential monte carlo method allows complete representation state distribution sequential importance sampling resampling 
standard ekf ukf gaussian assumption simplify optimal recursive bayesian estimation see section particle lters assumptions form probability densities question full nonlinear non gaussian estimation 
section method utilizes ukf augment improve standard particle lter speci cally generation importance proposal distribution 
chapter review background fundamentals necessary introduce particle ltering extension ukf 
material done van der merwe de freitas doucet wan provides thorough review treatment particle lters general :10.1.1.32.9011
monte carlo simulation sequential importance sampling particle ltering monte carlo simulation sequential importance sampling sis 
goal directly implement optimal bayesian estimation see equations recursively approximating complete posterior state density 
monte carlo simulation set weighted particles samples drawn posterior distribution map integrals discrete sums 
precisely posterior ltering density approximated empirical estimate jy random samples fx ng drawn jy 
denotes dirac delta function 
posterior ltering density jy marginal full posterior density jy 
consequently expectations form jy dx may approximated estimate example letting yields optimal mmse estimate jy 
particles assumed independent identically distributed approximation hold 
goes nity estimate converges true expectation surely 
sampling ltering posterior special case monte carlo simulation general deals complete posterior density jy 
general form derive particle lter algorithm 
impossible sample directly posterior density function 
circumvent diculty making importance sampling alternatively sampling known proposal distribution jy 
exact form distribution critical design issue usually chosen order facilitate easy sampling 
details discussed 
proposal distribution substitution 
unscented particle filter jy jy jy dx jx jy jy dx jy dx variables known unnormalized importance weights jx jy get rid unknown normalizing density follows jy dx jy dx jx jy jy dx jy dx jy dx 
jy 
jy notation 
jy emphasize expectations taken proposal distribution 
jy 
sequential update importance weights achieved expanding proposal distribution jy jy jx making assumption current state dependent observations 
furthermore assumption states corresponds markov process observations conditionally independent states arrive recursive update jx jx jx equation provides mechanism sequentially update importance weights appropriate choice proposal distribution jx 
sample proposal distribution evaluate likelihood jx transition probabilities jx need generate prior set samples iteratively compute importance weights 
procedure allows evaluate expectations interest estimate normalized importance weights denotes th sample trajectory drawn proposal distribution jx 
estimate asymptotically 
unscented particle filter converges expectation variance exist bounded support proposal distribution includes support posterior distribution 
tends nity posterior density function approximated arbitrarily point mass estimate jy posterior ltering density jy case ltering need keep history sample trajectories current set samples time needed calculate expectations form equations 
simply set 
point mass estimates approximate general distribution arbitrarily limited number particles mentioned importance sampling conditions met 
contrast posterior distribution calculated ekf minimum variance gaussian approximation true distribution inherently capture complex structure multi modalities skewness higher order moments 
resampling mcmc step sequential importance sampling sis algorithm discussed far serious limitation variance importance weights increases stochastically time 
typically iterations normalized importance weights tends remaining weights tend zero 
large number samples ectively removed sample set importance weights numerically insigni cant 
avoid degeneracy resampling selection stage may eliminate samples low importance weights multiply samples high importance weights 
followed markov chain monte carlo mcmc move step introduces sample variety ecting posterior distribution represent 
selection scheme associates particle number children selection schemes proposed literature including sampling importance resampling sir residual resampling minimum variance sampling 
sampling importance resampling sir involves mapping dirac random measure fx equally weighted random measure fx words produce new samples equal weighting accomplished sampling uniformly discrete set fx ng probabilities ng 
gives graphical representation process 
procedure ectively replicates original particle times may zero 
residual resampling step process sir 
rst step number children set oor function particle replicated times 
second step sir select remaining samples new weights samples form second set drawn described previously 
total 
unscented particle filter sampling index resampled index cdf resampling process random measure fx mapped equally weighted random measure fx index drawn uniform distribution 
number children particle set procedure computationally cheaper pure sir lower sample variance 
residual resampling experiments section general speci choice resampling scheme signi cantly ect performance particle lter 
selection resampling step time obtain particles distributed marginally approximately posterior distribution 
selection step favors creation multiple copies ttest particles particles may having children having large number children extreme case particular value case severe depletion samples 
additional required introduce sample variety selection step ecting validity approximation infer 
achieved performing single mcmc step particle 
basic idea particles distributed posterior jy case applying markov chain transition kernel invariant distribution particle results set new particles distributed posterior interest 
new particles may move interesting areas state space 
details mcmc step :10.1.1.32.9011
experiments section need mcmc step 
assumed general 
particle filter algorithm pseudo code generic particle lter algorithm 
implementing algorithm choice proposal distribution jx critical design issue 
optimal proposal distribution minimizes variance importance weights jx jx true conditional state density previous state history observations 
sampling course impractical arbitrary densities recall motivation importance sampling rst place 
consequently transition prior popular choice proposal 
unscented particle filter likelihood prior including current observation proposal distribution allows move samples prior regions high likelihood 
paramount importance likelihood happens lie tails prior distribution narrow low measurement error 
distribution jx jx example additive gaussian process noise model transition prior simply jx ectiveness approximation depends close proposal distribution true posterior distribution 
sucient overlap particles signi cant importance weights likelihood evaluated 
ekf ukf particle filter improvement choice proposal distribution simple transition prior address problem sample depletion accomplished moving particles regions high likelihood observation see 
ective approach accomplish ekf generated gaussian approximation optimal proposal jx jy accomplished separate ekf generate propagate gaussian proposal distribution particle jy time uses ekf equations new data compute mean covariance importance distribution particle previous time step 
redraw th particle time new updated distribution 
making gaussian assumption approach provides better approximation optimal conditional proposal distribution shown improve performance number applications 
replacing ekf ukf accurately propagate mean covariance gaussian approximation state distribution 
distributions generated ukf notation denotes chosen indicate subtle di erence versus approximation 

unscented particle filter generic particle filter 
initialization draw states prior 

importance sampling step sample xk jx evaluate importance weights normalizing constant yk jx jx jx normalize importance weights selection step resampling multiply suppress samples high low importance weights respectively obtain random samples approximately distributed jy 
set mcmc move step optional output output algorithm set samples approximate posterior distribution follows xk jy xk optimal mmse estimator xk xk jy similar expectations function xk calculated sample average 
algorithm generic particle lter 
greater support overlap true posterior distribution overlap achieved ekf estimates 
addition scaling parameters sigma point selection optimized capture certain characteristic prior distribution known algorithm modi ed distributions heavier tails gaussian distributions cauchy student distributions 
new lter results ukf proposal distribution generation particle lter framework called unscented particle filter upf 
referring 
unscented particle filter algorithm generic particle lter rst item importance sampling step sample jx replaced ukf update update prior distribution particle ukf calculate sigma points propagate particle time update kjk kjk kjk kjk kjk kjk kjk kjk kjk kjk kjk kjk incorporate new observation measurement update yk yk kjk kjk kjk kjk kjk kjk kjk kjk yk yk kjk kjk kjk yk yk sample jx steps particle lter formulation remain unchanged 
upf experiments performance unscented particle filter compared estimation problems 
rst problem synthetic scalar estimation problem second real world problem concerning pricing nancial instruments 

unscented particle filter synthetic experiment experiment time series generated process model sin gamma ga random variable modeling process noise scalar parameters 
non stationary observation model 
observation noise drawn gaussian distribution 
noisy observations di erent lters estimate underlying clean state sequence 
experiment repeated times random re initialization run 
particle lters particles residual resampling 
ukf parameters set 
parameters optimal scalar case 
table summarizes performance di erent lters 
table shows means variances mean square error mse state estimates 
compares estimates generated single run di erent particle lters 
superior performance unscented particle lter upf clearly evident 
pricing nancial options derivatives nancial instruments value depends basic underlying cash product interest rates equity indices commodities foreign exchange bonds 
call option allows holder buy cash product speci ed date price determined advance 
price option exercised known strike price date option lapses referred maturity time 
put options hand allow holder sell underlying cash product 
seminal black scholes derived industry standard equations pricing european call put options sn xe rtm sn xe rtm algorithm mse mean var extended kalman lter ekf unscented kalman filter ukf particle filter generic particle filter mcmc move step particle filter ekf proposal particle filter ekf proposal mcmc move step particle filter ukf proposal unscented particle filter particle filter ukf proposal mcmc move step table state estimation experiment results 
plot shows mean variance mse calculated independent runs 

unscented particle filter time filter estimates posterior means vs true state true pf estimate pf ekf estimate pf ukf estimate plot estimates generated di erent lters synthetic state estimation experiment 
denotes price call option price put option current value underlying cash product desired strike price time maturity cumulative normal distribution ln unknown volatility cash product risk free interest rate 
volatility usually estimated small moving window data days 
risk free interest rate estimated monitoring interest rates bond markets 
approach treat hidden states output observations 
treated known control signals input observations 
represents parameter estimation problem nonlinear observation equations 
allows compute daily complete probability distributions decide current value option market priced priced 
see details 
example shows implied probability density function volatility strike prices pairs call put option contracts british ftse index february december 
shows estimated volatility interest rate contract strike price 
table compare normalized square errors pair options strike price 
square errors measured days trading allow algorithms converge 
experiment repeated times particles particle lter mean value reported variance essentially zero 
example ekf ukf approach improving proposal distribution leads signi cant improvement standard particle lters 
main advantage ukf ekf ease implementation avoids need analytically di erentiate black scholes equations 

implied volatility distributions strike prices probability smile options ftse index 
volatility smile indicates option strike price equal priced shape probability gives warning hypothesis option priced 
posterior mean estimates obtained black scholes model particle lter th order polynomial hypothesized volatility 
option type algorithm mean nse trivial extended kalman filter ekf unscented kalman filter ukf call particle filter generic particle filter ekf proposal unscented particle filter trivial extended kalman filter ekf unscented kalman filter ukf put particle filter generic particle filter ekf proposal unscented particle filter table step ahead normalized square errors runs 
trivial prediction obtained assuming price day corresponds current price 
ekf widely accepted standard tool control machine learning communities 
chapter alternative ekf unscented kalman 
time days estimated interest rate volatility 
lter 
ukf addresses approximation issues ekf consistently achieves equal better level performance comparable level complexity 
performance bene ts ukf algorithms demonstrated number application domains including state estimation dual estimation parameter estimation 
number clear advantages ukf 
firstly mean covariance state estimate calculated second order better opposed rst order ekf 
provides accurate implementation optimal recursive estimation equations basis ekf ukf 
equations specifying ukf may appear complicated ekf actual computational complexity equivalent 
state estimation algorithms general order dimension state 
parameter estimation algorithms order number parameters 
ecient recursive square root implementation see appendix necessary achieve level complexity case 
furthermore distinct advantage ukf ease implementation 
contrast ekf analytical derivatives jacobians hessians need calculated 
utility especially valuable situations system black box model internal dynamic equations unavailable 
order apply ekf systems derivatives principled analytic re derivation system costly inaccurate numerical methods perturbation 
hand ukf relies functional evaluations inputs outputs deterministically drawn samples prior distribution state random variable 
coding perspective allows general modular implementation 
ukf clear advantages ekf number limitations 
ekf gaussian assumption probability density state random variable 
assumption valid numerous real world applications successfully 
implemented assumption 
certain problems multi modal object tracking gaussian assumption suce ukf ekf applied applied con dence 
examples resort powerful computationally expensive ltering paradigms particle lters see section 
implementation limitation leading uncertainty necessity choose unscented transformation parameters 
attempted provide guidelines choose parameters optimal selection clearly depends speci cs problem hand fully understood 
general choice settings appear critical state estimation greater ect performance convergence properties parameter estimation 
current focuses addressing issue developing uni ed adaptive way calculating optimal value parameters 
areas open research include utilizing ukf estimation noise covariances extension ukf recurrent architectures may require dynamic derivatives see chapter ukf smoother expectation maximization algorithm see chapter 
clearly begun scratch surface numerous applications bene ukf 
sponsored part nsf ecs iri darpa 
appendix accuracy unscented transformation section show unscented transformation achieves nd order accuracy prediction posterior mean covariance random variable undergoes nonlinear transformation 
purpose analysis assume nonlinear transformations analytic domain possible values condition implies nonlinear function expressed multi dimensional taylor series consisting arbitrary number terms 
number terms sum tend nity residual series tends zero 
implies series converges true value function 
consider prior variable perturbed mean zero mean disturbance covariance px taylor series expansion nonlinear transformation 

de ne operator 
rx taylor series expansion nonlinear transformation written 


accuracy mean true mean 

assume symmetrically distributed random variable odd moments zero 
note xx mean reduced 

ut calculates posterior mean propagated sigma points equation 
sigma points denotes th column matrix square root implies px formulation sigma points write propagation point nonlinear function taylor series expansion 

equation ut predicted mean 



sigma points symmetrically distributed odd moments zero 
results simpli cation 

rf rf rf rf includes probability distributions gaussian student see section details exactly sigma points calculated 

ut predicted mean simpli ed 

compare equation equation clearly see true posterior mean mean calculated ut agrees exactly third order errors introduced fourth higher order terms 
magnitude errors depend choice composite scaling parameter higher order derivatives contrast linearization approach calculates posterior mean lin agrees true posterior mean rst order 
julier shows term term basis errors higher order terms ut consistently smaller linearization 
accuracy covariance true posterior covariance yy expectation taken distribution substituting equations equation recalling odd moments zero due symmetry write true posterior covariance py 
jacobian matrix evaluated shown similar approach posterior mean posterior covariance calculated ut ut axp comparing equations clear ut calculates posterior covariance accurately rst terms errors introduced fourth higher order moments 

julier shows absolute term term error higher order moments consistently smaller ut linearized case truncates taylor series rst term lin derivation assumed value parameter ut 
prior knowledge shape prior distribution known set non zero value minimizes error higher order moments 
julier shows error kurtosis posterior distribution minimized gaussian 
appendix ecient square root ukf implementations standard kalman implementation state parameter covariance recursively calculated 
ukf requires matrix square root time step cholesky factorization 
square root ukf sr ukf propagated directly avoiding need time step 
algorithm general state estimation improved numerical guaranteed positive semide niteness state covariances similar standard square root kalman lters 
special state space formulation parameter estimation implementation possible equivalent complexity ekf parameter estimation 
square root form ukf powerful linear algebra techniques qr decomposition cholesky factor updating ecient squares brie review qr decomposition 
qr decomposition factorization matrix qr orthogonal upper triangular upper triangular part transpose cholesky factor aa aa shorthand notation 
donate qr decomposition matrix returned 
computational complexity qr decomposition nl 
note performing cholesky factorization directly aa plus nl form aa cholesky factor updating 
original lower triangular cholesky factor aa cholesky factor rank update uu denoted matrix vector result consecutive updates cholesky factor columns algorithm available matlab update 
ecient squares 
solution equation aa corresponds solution overdetermined squares problem ax solved eciently qr decomposition pivoting implemented matlab operator 
complete speci cations new square root lters table parameter estimation 
describe key parts square root algorithms contrast standard implementations 
experimental results discussion 
see theoretical implementation details 

initialize sigma point calculation time update kjk kjk qr kjk kjk kjk kjk measurement update equations yk qr yk yk xk yk kjk kjk xk yk yk yk yk table square root ukf state estimation 
square root state estimation original ukf lter initialized calculating matrix square root state covariance cholesky factorization eqn 

propagated updated cholesky factor subsequent iterations directly form sigma points 
eqn 
time update cholesky factor calculated qr decomposition compound matrix containing weighted propagated sigma points matrix square root additive process noise covariance 
subsequent cholesky update eqn 
necessary 
zeroth weight may negative 
steps replace time update eqn 

step approach applied calculation cholesky factor observation error covariance eqns 

step lm observation dimension 
contrast way kalman gain calculated standard ukf see eqn 
nested inverse squares solutions expansion eqn 
yk yk square triangular ecient back substitutions solve directly need matrix inversion 
posterior measurement update cholesky factor state covariance calculated eqn 
applying sequential cholesky vectors columns yk replaces posterior update eqn 
lm 
initialize sw time update sigma point calculation wk rls swk wk swk rk kjk wk wk kjk kjk kjk measurement update equations sdk qr sdk sdk kjk kjk dk sdk sdk swk wk rk diag swk diag swk diag table square root ukf parameter estimation 

square root parameter estimation parameter estimation algorithm follows similar framework state estimation square root ukf 
ml algorithm opposed possible advantage linear state transition function 
speci cally time update state covariance simply wk see section discussion selecting 
square root lters swk may updated directly eqn options wk rls swk corresponding exponential weighting past data 
wk swk rk diagonal matrix rk chosen approximate ects annealing diagonal process noise covariance options avoid costly qr cholesky updates necessary state estimation lter 
updates ensures main diagonal pw exact 
additional diagonal cross terms sw dr introduced ect appears negligible 
bibliography 
stochastic simulation bayesian approach multitarget tracking 
iee proceedings radar sonar navigation 
djuri fast weighted bayesian bootstrap lter nonlinear model state estimation 
ieee transactions aerospace electronic systems 
black scholes 
pricing options corporate liabilities 
journal political economy 

aircraft model aiaa controls design challenge 
prc edwards ca 
souza 
nonlinear regulation nonlinear nity control state dependent riccati equation technique part theory 
proceedings international conference nonlinear problems aviation aerospace beach fl may 
de freitas 
bayesian methods neural networks 
phd thesis cambridge university engineering department 
de freitas niranjan gee doucet 
sequential monte carlo methods train neural network models 
neural computation 
doucet 
sequential simulation methods bayesian ltering 
technical report cued infeng tr department engineering cambridge university 
doucet de freitas gordon 
sequential monte carlo methods 
doucet de freitas gordon editors sequential monte carlo methods practice 
springer verlag 

development nonlinear simulation mcdonnell douglas eagle longitudinal control law 
master thesis dept aeronautics astronautics university washington 
efron 
bootstrap resampling plans 
siam philadelphia 
ghahramani roweis 
learning nonlinear dynamical systems em algorithm 
kearns solla cohn editors advances neural information processing systems proceedings conference 
mit press 
gordon salmond smith 
novel approach nonlinear non gaussian bayesian state estimation 
iee proceedings april 
bibliography bibliography haykin 
adaptive filter theory 
prentice hall edition 
higuchi 
monte carlo lter genetic algorithm operators 
journal statistical computation simulation 
hull 
options futures derivatives 
prentice hall third edition 
ikeda 
multiple valued stationary state instability light ring cavity system 
opt 
commun 
isard blake 
contour tracking stochastic propagation conditional density 
european conference computer vision pages cambridge uk 
ito xiong 
gaussian filters nonlinear filtering problems 
ieee transactions automatic control may 

stochastic processes filtering theory 
academic press new york 
julier 
scaled unscented transformation 
appear automatica 
julier uhlmann 
general method approximating nonlinear transformations probability distributions 
technical report rrg dept engineering science university oxford nov 
www robots ox ac uk publications letter size unscented zip 
julier uhlmann 
new extension kalman filter nonlinear systems 
proc 
aerosense th int 
symp 
aerospace defence sensing simulation controls 
julier uhlmann durrant whyte 
new approach ltering nonlinear systems 
proceedings american control conference pages 
kalman 
new approach linear ltering prediction problems 
trans 
asme journal engineering pages 
kitagawa 
monte carlo lter smoother non gaussian nonlinear state space models 
journal computational graphical statistics 
kong liu wong 
sequential imputations bayesian missing data problems 
journal american statistical association 
lapedes farber 
nonlinear signal processing neural networks prediction system modelling 
technical report los alamos national laboratory 
lewis 
optimal estimation 
john wiley sons new york 
liu chen 
blind deconvolution sequential imputations 
journal american statistical association june 
liu chen 
sequential monte carlo methods dynamic systems 
journal american statistical association 
ljung om 
theory practice recursive identi cation 
mit press cambridge ma 
bibliography bibliography mackay 
wol ra phy cam ac uk mackay html 
www 
mackay 
practical bayesian framework backpropagation networks 
neural computation 
mackey glass 
oscillation chaos physiological control system 
science 
matthews 
state space approach adaptive nonlinear ltering recurrent neural networks 
proceedings iasted internat 
symp 
arti cial intelligence application neural networks pages 
nelson 
nonlinear estimation modeling noisy time series dual kalman filtering methods 
phd thesis oregon graduate institute 
niranjan 
sequential tracking pricing nancial options model neural network approaches 
mozer jordan petsche editors advances neural information processing systems volume pages 
ravn 
advances derivative free state estimation nonlinear systems 
technical report imm rep department mathematical modelling department automation technical university denmark lyngby denmark april 
press teukolsky vetterling flannery 
numerical recipes art scienti computing 
cambridge university press edition 

decoupled extended kalman filter training feedforward layered networks 
ijcnn volume pages 

extensions enhancements decoupled extended kalman filter training 
icnn volume pages 
rosenbrock 
automatic method finding greatest value function 
computer journal 
rubin 
sir algorithm simulate posterior distributions 
bernardo degroot lindley smith editors bayesian statistics pages cambridge ma 
oxford university press 
kailath 
state space approach adaptive rls filtering 
ieee signal processing magazine pages jul 
shumway sto er 
approach time series smoothing forecasting em algorithm 
time series analysis 
singhal wu 
training multilayer perceptrons extended kalman lter 
advances neural information processing systems pages san mateo ca 
morgan kau man smith gelfand 
bayesian statistics tears sampling resampling perspective 
american statistician 
bibliography bibliography van der merwe de freitas doucet wan :10.1.1.32.9011
unscented particle filter 
technical report cued infeng tr cambridge university engineering department cambridge england august 
van der merwe wan 
ecient derivative free kalman lters online learning 
appear european symposium arti cial neural networks esann bruges belgium april 
van der merwe wan 
square root unscented kalman lter state parameter estimation 
appear international conference acoustics speech signal processing salt lake city utah may 
wan nelson 
neural dual extended kalman ltering applications speech enhancement monaural blind signal separation 
proc 
neural networks signal processing workshop 
ieee 
wan van der merwe 
unscented kalman filter nonlinear estimation 
proceedings symposium adaptive systems signal processing communication control lake louise alberta canada october 
ieee 
wan van der merwe nelson 
dual estimation unscented transformation 
solla leen 
uller editors advances neural information processing systems pages 
mit press 

monte carlo techniques problems optimal information processing 
automation remote control 

