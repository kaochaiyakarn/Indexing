analyzing neural codes information bottleneck method elad noam slonim naftali tishby rob de van william bialek school computer science engineering center neural computation hebrew university jerusalem israel molecular biology princeton university princeton nj usa nec research institute independence way princeton nj usa cs huji ac il research nj nec com basic aspect understanding neural code neuron neural system ability form dictionary stimuli neuron system patterns spikes neuron responds 
neurons may respond stimuli dictionary stochastic nature 
neuron responds di erent stimuli similar way number stimulus features neuron cares small dictionary compressed signi cant loss properties 
apply agglomerative information bottleneck algorithm study properties dictionary neural code identi ed neuron visual system 
nd neural code dictionaries di erent ies highly compressible suggesting small number features key components neural code 
compare encoded features di erent ies nd similar general structure di erences details 
problem neural coding understand neurons patterns action potentials spike trains represent transmit information 
ideally form kind dictionary stimuli neuron animal resulting spike trains vice versa 
dictionary probablistic stimulus repeatedly neurons may respond unreliable spike trains vitro vivo 
neuronal encoding stimulus spike train ft described conditional distribution ft gjs 
dictionary stimuli response contain set possible stimuli corresponding conditional response distributions 
set distributions relations capture fundamental characteristics neural code 
example describes set codewords neuron uses ects noise coding process information stimulus encoded spike trains 
similarly decoding described response conditional ensemble 
general dictionary stimulus segments conditional responses back may large due size possible stimuli space 
conditional response distributions similar means neuron point view stimuli similar possible construct reduced version dictionary approximation full dictionary 
clear clustering response distributions replacing actual response neuron average response cluster belongs result small loss encoded information stimulus 
notion clustering responses complementary idea dimensionality reduction space stimuli 
obvious question regarding nature neural code large dictionary need capture relation stimuli neuron responses 
introduced information bottleneck method provides general information theoretic framework building compact dictionaries sort 
approach joint distribution random variables looks compact representation preserves information possible context stimuli neural response seek clustering stimulus features times stimulus maximizes information resulting spike trains 
apply agglomerative information bottleneck algorithm hard clustering approximation general method study nature neural code motion sensitive neuron visual system :10.1.1.122.8863
nd small number response clusters captures information stimulus compared information carried full set responses 
average stimulus associated cluster di erent cluster triggered averages may interpreted rst order approximations di erent features stimulus encodes 
average response distributions clustered re ect nature noise model coding mechanism 
compare clustering results di erent ies nd di er dictionary content details features 
experimental setup construction neural code dictionary flies repeatedly second long movie responses neuron recorded 
stimulus ies rigidly moving image random black white bars pattern pattern position de ned pseudorandom sequence simulating di random walk 
spike trains discretized time bins size 
msec 
resolution spikes single bin think neural responses binary strings 
examine responses windows length individual responses binary words 
letters 
choice speci values arbitrary msec previous shown long words re ect temporal structure spike train see discussion short sample relevant probability distributions 
recordings neuron ies standard methods 
ies freshly caught female average intensity stimulus mw 
sr 
movie repeatedly responses stimulus described conditional probability distribution binary words speci times stimulus jt 
movie result underlying statistical source suciently long set time conditional distributions jt fair replacement stimulus segment conditioned distribution js stimulus portion preceding time 
shows portion spike trains ies responds construction conditional word distribution 
responses fly 

stimulus pattern velocity sec sample spike trains estimation jt 
ies view random vertical bar pattern moving visual eld time dependent velocity part shown left panel scale bars mark msec horizontally sec vertically 
experiment sec waveform repeatedly times 
responses ies part stimulus shown top 
example construction time dependent word distributions jt letter words shown times arrows marking spots stimulus responses bottom panel 
spike trains divided contiguous msec bins vertical lines value bin number spikes bin stimulus repeats separately 
full version stimulus response dictionary set stimulus segments times corresponding word distributions 
order investigate compressible dictionary cluster conditional word distributions quantify ect compression quality dictionary compressed dictionary stimuli average cluster responses 
apply wide range clustering algorithms dictionary quality measures problem question hand falls scope general question relation random variables relation inputs outputs system 
agglomerative information bottleneck clustering neural responses finding features input system relevant prediction output shown natural information theoretic formulation termed information bottleneck method nd compact representation denoted xed amount meaningful information minimizing mutual information maximizing compression 
shown formulation amounts minimization lagrangian xjx respect xjx positive lagrange multiplier 
minimization yields set self consistent equations xjx yj solved iteratively 
agglomerative information bottleneck algorithm introduces simple bottomup hard clustering approach information bottleneck problem 
greedy agglomerative procedure nds set clusters directly maximizes locally information prescribed number clusters current context formulation translates seeking set stimuli features clusters times stimulus information features neural responses maximized 
framework agglomerative algorithm means iteratively group times respect stimulus distribution responses similar 
start full set conditional distributions assign unique cluster 
jt assigned cluster cluster probability distribution jt 
size clusters 
measure pairwise distance clusters jensen shannon divergence ij js jjp nd nearest clusters 
merge clusters new cluster ck new cluster size recalculate set pairwise distances clusters distance new cluster remaining clusters needs recalculated iteratively merge nearest ones 
process proceeds left single cluster 
shown number clusters algorithm approximates full bottleneck problem searching clusterings times stimulus locally maximize amount mutual information clusters neural responses 
alternatively interpret information stimulus neuron convey times clustered neuron responded stimulus centroid distribution cluster 
number clusters compare information information words length conveys stimulus vice versa stimulus spike times derived full set conditional distributions 
shows fraction information contained stimulus compact representation responses calculated responses di erent ies separately msec 
msec 
note ies di er considerably amount information encode stimulus average ring rate 
evidently small number clusters limit information bottleneck 
jensen shannon divergence probability distributions djs jjq dkl jjr dkl jjr relative weight distribution similarly dkl kullback leibler divergence de ned dkl jjr log clusters information clusters response information stimulus response fly fly fly information bottleneck curve di erent ies 
information clusters convey neural response normalized information full set jt conveys stimulus shown function number clusters algorithm uses see text details 
time points bins experiment initial set fp jt time points chosen randomly experiment di erent choices time points give similar results shown 
information curve shown di erent ies time points ies 
fly similar ring rate information rates roughly twice high ring rate information rates 
captures information full spike trains holds stimulus suggesting number stimulus features encodes relatively small 
example clusters reduction factor orders magnitude preserve original information preserved exact value depends speci 
extracting features neural code comparing neural codes clustering construction times stimulus belong cluster similar response distributions 
distributions really originate underlying distribution average distribution describes noise model neuron 
shows centroids clusters ies average jt clusters 
ask common features stimulus segments preceding set word distributions times assigned cluster 
generalizing spike word triggered average stimulus calculate average stimulus segments preceding times clustered 
average stimuli waveform cluster triggered average clusters shown case clusters left panel 
repeating analysis ies information curve shown compared clustering results ies 
case clusters ies identify corresponding clusters ones largest number shared times ies watched movie 
centroid word probability binary words ordered spikes probability binary words ordered spikes average wjt clusters ies 
average jt clusters shown case clusters 
words ordered number spikes vertical broken lines mark boundary words di erent number spikes shown top panel 
average jt 
note resulting distributions sensitive di erent randomized choice time points 
distributions similar general structure large cluster spiking sporadic single spike di er details distributions shown 
right panel shows cluster triggered averages ies corresponding clusters re ecting similar general structure large di erence details 
discussion neuron dictionary described neural coding complex dynamic stimuli highly compressible 
correspondingly selective small number stimulus features high dimensional stimulus space 
neural codes di erent ies may di er considerably ring rates information rates compressibility time ms stimulus velocity sec cluster triggered stimulus averages 
clusters fig 
compute average stimulus waveform preceded times clustered left panel 
comparison waveforms corresponding clusters ies right panel 
waveforms lowpass ltered presentation clarity 
universal function fig 

nd considerable overlap features di erent ies ies di er speci features details 
explore range stimulus parameters responses 
compare results di erent stimuli response segment 
analysis stimulus features include go mean stimulus waveform 
interest apply problem full bottleneck method agglomerative version 
object identi ed neuron results typical vertebrate especially cortical codes 
empirical question 
believe approach compressibility coding dictionary considerably questions structure neural code system hope computational methods introduced contribute answering questions 
rieke de van bialek spikes exploring neural code mit press cambridge 
de van bialek real time performance movement sensitive neuron blow visual system coding information transfer short spike sequences proc 
soc 
london ser 

sejnowski reliability spike timing neocortical neurons 
science 
koch temporal precision spike trains cortex behaving macaque monkey 
neural computation 
reich victor knight kaplan response variability timing precision neuronal spike trains vivo neurophysiol 

strong de van bialek entropy information neural spike trains phys 
rev lett 

tishby pereira bialek information bottleneck method proceedings th annual allerton conference communication control computing university illinois 
slonim tishby agglomerative information bottleneck advances neural information processing systems nips 
lobular complex vision invertebrates ed ali plenum 
cover thomas elements theory wiley 
lin divergence measures shannon entropy ieee trans 
inf 
theory 
brenner tishby de van bialek individuality neural code advances neural information processing systems nips press 
