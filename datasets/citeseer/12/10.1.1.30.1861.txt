sub cubic cost algorithms pairs shortest path problem takaoka department computer science ibaraki university hitachi ibaraki japan mail takaoka cis ibaraki ac jp 
give sub cubic cost algorithms pairs shortest distance path apsp problems 
parallel algorithm solves problem directed graph unit edge costs log time ffi log processors erew pram 
second parallel algorithm solves apsp consequently problem directed graph non negative general costs real numbers log time cost 
previously cost greater 
improve respect complexity mn sequential algorithm graph edge costs log log log problem 
pairs shortest path apsp problem compute shortest paths pairs vertices directed graph non negative real numbers edge costs 
pairs shortest distance problem defined similarly word paths replaced distances 
traditionally called apsp problem 
alon galil margalit gave sub cubic algorithm problem graph small integer edge costs 
alon galil margalit naor distinguished problems faced higher complexity apsp problem class graphs 
best time complexities apsp problems undirected graphs unit edge costs seidel 
hand complexity pairs shortest distance problem general edge costs slightly improved takaoka fredman divided polylog close algorithm solve apsp problem time need distinguish apsp 
technique obtaining paths concept witnesses boolean matrix multiplication 
compute witnesses boolean matrix multiplication matrices log 
result solve shortest path problems attaching log factors complexities corresponding distance problems 
partially supported aid scientific research program research hitachi engineering design parallel algorithm problem directed graph unit edge costs log time worst case log processors 
result compared parallel algorithm directed graph general edge costs log log expected time processors small positive real number 
improve parallel algorithm apsp problem general edge costs cost number processors time slightly 
cost log dekel sahni improved han pan reif 
cost new algorithm slightly time polylog nc 
sequential algorithm graph edge cost complexity sub cubic 
basic definitions directed graph subset edge cost denoted ij matrix element ij assume ij ii edge ij 
cost distance path sum costs edges path 
length path number edges path 
shortest distance vertex vertex minimum cost paths denoted ij fd ij call size matrices 
matrices 
products defined elements follows ordinary multiplication ring ab ij ik kj boolean matrix multiplication ij ik kj distance matrix multiplication ij min kn fa ik kj best algorithm computes time 
compute regard boolean values integers algorithm convert non zero elements resulting matrix 
complexity 
algorithm td tp time solve apsp problem td log log time repeated squaring method described repeated description tp time algorithm compute witnesses giving gives minimum ij witnesses matrix fw ij ij ik kj 
ij 
definition computing shortest paths give path matrix size give shortest path time length path 
specifically ij path witness matrix fw ij means path goes recursive function path defined path path path nil path path defined list vertices excluding endpoints 
sections record ij find path modified newly set paths pairs shortest paths review algorithm section 
costs edges graph ones 
th approximate matrix defined ij ij ij ij 
compute algorithm 
algorithm shortest distances boolean matrix multiplication fa ij ij ij identity matrix ij ij ij ij ij ij 
algorithm computed increasing order 
note compute line time computing time algorithm rn 
algorithm problem uses algorithm accelerating phase repeated squaring cruising phase 
algorithm solving compute algorithm dlog re scan th row find smallest set equal ij ij set corresponding indices ij min fd ik kj ij ij ij ij ij ij ij fm ij ij algorithm computes accelerating phase spending rn time computes repeated squaring cruising phase smallest integer series key observation cruising phase need check line size larger 
computing time iteration line 
time cruising phase dlog re balancing phases rn yields time algorithm 
directed graph edge costs positive integer convert graph adding auxiliary vertices edge set modified connected obviously solve problem applying algorithm takes mn time 
witnesses kept lines algorithm log time 
line algorithm witnesses obtained storing witness matrix 
increase order complexity cruising phase 
complexity apsp log graph edge costs complexity mn log mn parallelization graphs unit costs design parallel algorithm erew pram directed graph unit edge costs 
section section mainly describe algorithm crew pram simplicity 
overhead time copy data log time certain number processors depending phase absorbed dominant complexities 
adjacency matrix algorithm 
ij edge 
diagonal elements 
path length element th power boolean matrix multiplication 
repeated squaring get dlog ne boolean matrix multiplications smallest integer series matrices give kind approximate estimation path lengths 
element time say shortest path length 
gets large gap gets large 
miller observe fill gap increasing order way 
shortest paths computed 
shortest path length consists shortest path length path length formally algorithm approximation phase shown explanation purposes 
algorithm shortest distances matrix filling matrix ij ij ij ij min fb ij 
algorithm arrays fb ij working space compute ij line 
fill gaps interested computing time algorithm 
dlog ne solve time efficient 
substitute algorithm algorithm algorithm solve problem time par algorithm 
merit algorithm easy parallelize 
known multiply matrices ring log time processors parallel 
substitute algorithm algorithm algorithm call resulting algorithm algorithm perform multiplications parallel line 
compute matrices lines parallel 
line find minimum time processors 
turning attention cruising phase algorithm find minimum line log time log processors 
rest absorbed complexities 
summarize complexities parallel algorithm 
time number processors 
accelerating phase log cruising phase log log log log complexity follows log log graph edges costs replace mn complexities 
parallelization graphs general costs edges costs non negative real numbers apply techniques previous sections 
previous section magnitude edge costs efficiencies sequential parallel algorithms get worse primitive methods 
fredman gave algorithm problem log log log time showing distance matrix multiplication solved complexity 
takaoka improved log log log pointed apsp problem solved complexity 
algorithm parallelized 
parallel version takes repeated squaring approach 
parallel algorithm distance matrix multiplication complexities log log log log erew pram 
apsp problem solved log log log log 
algorithm keep track witnesses easily apsp problem solved complexities 
cost pt log log log slightly 
major open problem nc algorithm cost 
section show stronger result exists nc algorithm apsp problem cost 
definition 
parallel algorithm time complexity processors th step 
cost complexity defined time interval number processors fixed called processor phase 
equal interval divided number processor phases 
size interval brent theorem states processor phases simulated smaller number processors expense increasing computing time mentioning overhead time rescheduling processors 
suggest number processor phases finite rescheduling cause overhead time 
parallel algorithm number processor phases 
engine speak acceleration phase algorithm fast algorithm boolean matrix multiplication 
fast distance matrix multiplication algorithm engine modify cruising phase slightly fit parallel algorithm 
algorithm difference distances lengths paths edge costs ones 
line algorithm choose set distances ij satisfying ij guarantee correct computation distances 
observe computation essentially path lengths distances 
keep track path lengths adapt algorithm problem 
definition ij gives cost shortest path length greater 
algorithm follows new data structure array ij length path gives ij algorithm fq ij ij dlog re matrix multiplication fq ij ij ik kj ij updated ik kj ij dlog re scan th row find smallest set equal ij ij set corresponding indices ij min fd ik kj gives minimum satisfies ik kj minimum ik kj fs ij ij ij ij ij ij ij ij ij 
described parallelize distance matrix multiplication line erew pram 
index time number processors accelerating phase cruising phase 
log log log log log computation done log time processors 
dominant complexity cruising phase line 
part computed log time log processors 
log log log letting log log log yields log log log log log log log log log log cost log log log log log log log log log note solve apsp problem order cost algorithm 
need keep track witnesses distance matrix multiplication minimum operation line 
perform accelerating phase processors time phase log cost computation 
keep number processors uniform claim algorithm complexities traditional definition cost pt algorithm graphs small edge costs edge costs bounded positive integer better saw previous sections 
briefly review algorithm distance matrix multiplication 
distance matrices elements bounded infinite 
diagonal elements 
convert ij ae ij ij ij ij ae ij ij ij product ordinary matrix multiplication distance matrix multiplication 
ij ik kj ij blog ij compute arithmetic operations integers values expressed log bits strassen algorithm multiplying bit numbers takes log log log bit operations compute log log log log log log time 
replace accelerating phase algorithm call resulting algorithm algorithm 
algorithm accelerating fq ij ij matrix multiplication fq ij ij ij ij updated ij algorithm 
note bound replaced distance matrix multiplication 
time accelerating phase log log rm log log log rm log assume constant balancing complexity cruising phase yields total computing time log log nm log log log nm log choice log log nm log log log nm log complexity simplified log log log value keep complexity sub cubic 
bound considerable improvement 
solved problem 
algorithm keep track witnesses 
able replace accelerating phase repeated squaring better complexity apsp problem small edge costs 
concluding remarks balancing parameters accelerating cruising phases change depending engine accelerating phase 
may find results algorithms engine accelerating phase 
acknowledgment 
comment definition zhi zhong chen greatly appreciated 

alon galil margalit exponent pairs shortest path problem proc 
th ieee focs pp 

alon galil margalit naor witnesses boolean matrix multiplication shortest paths proc 
th ieee focs pp 


coppersmith winograd matrix multiplication arithmetic progressions journal symbolic computation pp 


dekel sahni parallel graph algorithms siam jour 
comp 
pp 


fredman new bounds complexity shortest path problem siam jour 
comput 
pp 


miller improved parallel algorithm computes bfs numbering directed graph info 
proc 
lett 
pp 


gibbons rytter efficient parallel algorithms cambridge univ press 

han pan reif efficient parallel algorithms computing pairs shortest paths directed graphs proc 
th acm spaa pp 


shortest path problem harder matrix multiplications info 
proc 
lett 
pp 

seidel pairs shortest path problem proc 
th acm stoc pp 


strassen zahlen computing pp 


takaoka new upperbound complexity pairs shortest path problem info 
proc 
lett 
pp 


takaoka efficient parallel algorithm pairs shortest path problem wg lecture notes computer science springer berlin pp 

article processed macro package llncs style 
