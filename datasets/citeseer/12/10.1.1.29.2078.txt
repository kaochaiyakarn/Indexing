international journal computer vision kluwer academic publishers 
manufactured netherlands 
joy sampling forsyth ioffe computer science division university california berkeley berkeley ca usa daf cs berkeley edu cs berkeley edu ioffe cs berkeley edu received november revised august accepted january 
standard method handling bayesian models markov chain monte carlo methods draw samples posterior 
demonstrate method core problems computer vision structure motion colour constancy 
examples illustrate samplers producing useful representations large problems 
demonstrate sampled representations trustworthy consistency checks experimental design 
sampling solution structure motion strictly better factorisation approach reports uncertainty structure position measurements direct way identify tracking errors estimates covariance marginal point position reliable 
colour constancy solution strictly better competing approaches reports uncertainty surface colour illuminant measurements direct way incorporates available constraints surface reflectance illumination direct way integrates spatial model reflectance illumination distribution rendering model natural way 
advantage sampled representation resampled take account information 
demonstrate effect knowing colour constancy example surface viewed different images fact object 
conclude general discussion strengths weaknesses sampling paradigm tool computer vision 
keywords markov chain monte carlo colour constancy structure motion 
bayesian philosophy information model captured posterior distribution obtained bayes rule posterior world observations observations world world prior world probability density state world absence observations 
examples suggest computational difficulties bayesian philosophy leads excellent effective data expositions carlin louis gelman 
grenander grenander examples bayesian inference vision literature include binford levitt chou brown huang jolly maybank sturm noble mundy pavlovic sarkar boyer sullivan yuille zhu 
probability distribution essence device computing expectations 
problems interested typically involve important continuous component meaning computing expectations involves estimating integrals usually high dimensional domains 
useful technique represent posterior drawing large number samples distribution 
samples estimate expectation respect posterior 
forsyth ioffe example wished decide fight flee attacker draw samples posterior outcome estimate expected utilities decision averages utilities samples choose decision best utility 
sampling algorithms general random search map interpretations precisely results give approximate representation entire posterior 
means example estimate covariance posterior resample samples incorporate new information engage multiple calculations different decisions distinct utilities sampling principle simple general samples drawn posterior efficiently 
demonstrates strengths weaknesses sampling methods core vision problems examples structure motion section colour constancy section 
notation write vector th component matrix th component mij 
sampler jargon may unfamiliar shown italics introduced 

simple sampling algorithms probability distributions direct algorithms exist drawing samples ripley seldom lucky posterior type 
rejection sampling appropriate distributions 
assume wish draw samples proposal distribution draw samples easily 
assume know constant kq wecan draw sample drawing sample accepting sample probability kq 
sampling sample draw large number independent samples sn proposal distribution set si probability pro wi asn distribution sample approach 
rejection sampling importance sampling methods wildly inefficient approximates poorly usual case high dimensions cases collection different pasted obtain better approximation 

markov chain monte carlo metropolis hastings algorithm markov chain monte carlo methods gilks standard methods sampling complex distributions 
method constructs markov chain stationary distribution target distribution 
new sample obtained old advancing markov chain 
metropolis hastings algorithm technique constructing markov chain particular desired stationary distribution 
assume distribution generate samples 
build markov chain stationary distribution 
algorithm produce sequence samples xn sample xi proposing revised version element sequence xi probability xi xi xi 
give form 
proposal process random 
particular proposal distribution gives probability proposing xi 
written xi 
note proposal distribution function may function xi 
assume non zero values non zero 
case max xi xi xi notice expression qualitatively sensible 
chain point low value new point high value forward backward proposal probabilities equal new point accepted high probability 
chain point high value proposal process high probability suggesting points low value itis stay point 
point high value proposed disproportionately accepted 
way think metropolis hastings algorithm improved version hypothesize test process common vision 
metropolis hastings suggests various hypotheses depending result bookkeeping exercise accepted rejected 
process yields sequence xn 
metropolis hastings sequence hypotheses significant semantics assuming technical conditions proposal process expounded example gilks roberts tierney conditions usually fairly easily met samplers meet sufficient iterations completed subsequent xi samples drawn 

burn mixing generally mcmc method needs produce number samples forget start point 
number iterations required achieve called burn time 
burn may extremely long poorly designed sampler 
small number samplers known short burn time jerrum sinclair 
sampler burnt sequence samples produces may may correlated correlation low method said mix 
desirable algorithm burns quickly mixes 
burn mixing related dynamics underlying markov chain 
way show sampler mixes quickly prove decomposition domain disjoint sets conditional probability sampler goes set high 
proofs fast mixing exist small number cases require substantial art jerrum sinclair 
aware proof sampler vision problems fast mixing 
examples vast majority cases algorithm proofs show variety consistency checks suggest algorithm converged 

attractions mcmc known apply algorithm domain support complicated example samples may drawn domain support posterior consists different spaces different dimensions green richardson green 
numerous variants basic algorithm combine deterministic dynamics random search hope better mixing see review neal 
advantage viewing metropolis hastings algorithms hypothesize test process suggests build proposal mechanisms 
joy sampling natural strategy take current vision algorithms produce probabilistic outputs 
approach illustrated section zhu 
successfully recognition problems 
really attractive feature different possibly incompatible algorithms distinct sources proposals samples obtain represent posterior incorporating available measurements 
quite practice easy come function proportional posterior 
case posterior du integral normalizing constant difficult compute best way sampling method 
attractive feature metropolis hastings algorithm need know normalizing constant distribution constant cancelled ratio 

techniques building practical mcmc samplers easy build sampler metropolis hastings algorithm 
hard build sampler burns quickly mixes gives trustworthy picture posterior algorithm 
describe variety techniques building samplers conclude discussion possible sanity checks 

gibbs samplers 
quite common encounter situations target distribution non standard form standard groups variables fixed values occurs vision problems see sections 
case natural adopt proposal mechanism fixes set variables draws sample full conditional distribution set vice versa 
useful technique known gibbs sampling named geman geman apparently due statistical physics literature known heat bath algorithm gilks 
usually group variables sampled chosen random sufficient samples drawn group variables visited times 
gibbs sampling easy implement 
considerable danger quite difficult avoid 
groups variables strongly forsyth ioffe 
correlated variables cause gibbs samplers behave badly 
top left shows samples drawn gibbs sampler independent normal random variables variance variance 
stars indicate samples line segments indicate order samples drawn 
note sampler quite large vertical moves variance direction large 
top right shows samples drawn distribution rotated gibbs sampler 
case sampler relatively small vertical horizontal moves position samples changes relatively slowly samples graph bottom left consist graph rotated give better picture distribution 
bottom right coordinate samples drawn second sampler solid line coordinates third dashed line 
solid curve correctly suggests samples drawn second sampler quite strongly correlated 
correlated gibbs sampler mix badly 
effect known full discussion see example gilks roberts easily illustrated see fig 


hybrid monte carlo method 
common difficulty sampling methods state sampler appears perform slightly biased random walk 
difficulty random walk takes long time move distance domain meaning sampler started point long way mode distribution take long time reaches mode 
perspective extremely important representation distribution mode 
hybrid monte carlo method making proposals causes state sampler move quickly mode explore 
method due duane 
described detail neal 
write state sampler method requires target distribution written exp think potential function state sampler state particle mass subject potential function 
state determined considering momentum particle writing hamiltonian particle ptp need integrate hamilton equations qu determine state particle 
temporary excursion mechanics justified negative hamiltonian particle get exp exp pt new target distribution larger set random variables 
proposal moves 
advance time particle model randomly chosen amount forwards backwards 
updates long symplectic integrator extent advance uniform random choice forward backward random accept probability 

fix draw sample full conditional 
easy full conditional distribution normal independent sampler attractive qualitative behaviour 
state relatively large value type move travel quickly gradient smaller values building momentum 
second move discards momentum sampler move quickly mode small move exploring mode influence random choice momenta 
values particle mass range time values chosen experiment 
practice hybrid method useful continuous problems 
easy implement colour constancy example joy sampling successfully variety continuous problems neal 

mcmc random search vision markov chain monte carlo appeared vision literature various forms 
common attempt obtain map estimate random search usually metropolis hastings algorithm geman geman geman 
markov random field model spatial model gives posterior image labellings measurements function measurement values local patterns pixel labels called clique potentials topic reviewed li 
standard method estimating map labellings annealed version metropolis hastings algorithm posterior sampled function parameter changes sampling process 
parameter thought temperature intent high values parameter posterior mode temperature reduced state sampler get stuck mode obtaining global extremum 
possible guarantee practice occurs algorithm mixed reputation collins golden 
notion sampling method perform inference generative model image pattern appears due grenander 
successful examples appear literature 
jolly 
annealing method estimate map solution configuration motion motor car template image 
zhu random search method find medial axis transform 
zhu 
mcmc method find simple shapes road signs 
green mcmc perform inference various vision situations including reconstruction single photon emission computed tomography data finding polygonal template duck heavy spatial noise 
phillips smith inference performed hierarchical model find faces version mcmc find unknown number disks 
templates restoration amit 

gibbs samplers quite widely reconstruction geman geman geman zhu 
random search standard method estimating fundamental matrix structure forsyth ioffe motion problems review appears torr murray 
ransac algorithm robust fitting due fischler bolles appearing statistical literature rousseeuw proposes small sets correspondences uniformly random fits fundamental matrix set accepts set fit gives largest number correspondences sufficiently small residual 
number sets chosen ensure high probability correct set 
main advantage mcmc method ransac mcmc method produce series hypotheses meaningful semantics indicating example posterior probability particular point outlier posterior probability pair measurements come single point 

particle filtering condensation survival fittest resampling 
substantial impact sampling algorithms vision resampling algorithms tracking 
best known algorithm known condensation vision community blake isard survival fittest ai community kanazawa particle filtering statistical signal processing community originated carpenter kitagawa 
wide range variants applications particle filtering described forthcoming book doucet 
algorithm modification factored sampling draws samples prior represents state world th measurement propagates samples dynamical model weights posterior incorporating th measurement 
set weighted samples provides representation prior iteration 
algorithm fast efficient quite widely applied low dimensional problems 
attraction resampling algorithms incorporate new information tracking applications new information comes new frame new measurements arrived 
new information may come sources 
colour constancy example assume algorithm told patches different images colour occur recognition algorithm match geometry knows patches represent object 
information strongly constrains inferred colours patches view section 
recognition applications encounters form hierarchical model suggests resampling 
ioffe forsyth sampler label groups image segments consistency observed human kinematics 
human model segments 
foolish attempt label segment groups algorithm uses sampler label individual segments frequency proportional posterior probability label image data 
set individual segment labels resampled propose pairs labels pairs segments 
case new information enhanced prior prior pairs labels emphasizes pairs segments lie particular configurations property meaningless single segments 

example large scale sampling bayesian structure motion structure motion problem inferring description geometry sequence images 
problem long history huge literature space allow comprehensive review see beardsley 
faugeras 
faugeras robert gool zisserman hartley zisserman 
accurate solutions structure motion attractive technique generate models rendering virtual environments debevec faugeras gool zisserman tomasi kanade 

structure motion matrix factorisation assume distinct views points correspondences known 
influential tomasi kanade formulation structure motion tomasi kanade data arranged matrix measurements factor uv represents camera positions represents point positions 
affine transform determined ua minimises set constraints associated camera represents euclidean structure 
practice factorisation achieved singular value decomposition 
maximum likelihood method isotropic gaussian error model adopted anisotropic gaussian error model see morris kanade 
formalism applied various camera models tomasi kanade triggs missing data points interpolated known points jacobs tomasi kanade methods motion segmentation exist costeira kanade methods lines similar primitives known morris kanade 
noise estimates recovered structure morris kanade 
assume errors estimates structure independent assumption authors acknowledge sustainable 
factorisation method important weakness 
algorithm separate stages allow payoff model error extent recovered model violates required set camera constraints measurement error extent model predictions correspond data observations 
means model identify measurement problems example tracker errors fig 
subject reconstruction errors caused incorporating erroneous measurements 
property algorithm problem relatively degrees freedom compared possible identify ignore unreliable measurements full force model employed 
dellaert shown strongly model constrains data sampling method average correspondences weighting consistency measured data obtaining satisfactory reconstruction 
method removes need compute correspondences structure motion problems dellaert 

posterior structure motion useful think bayesian models generative models grenander 
generative structure motion model drawn appropriate priors 
obtained adding noise uv 
assume noise obtained mixture model large probability gaussian noise small probability measurement value replaced uniform random variable 
priors obtained constraints camera structure 
fix origin coordinate system represent points homogenous coordinates dimensions respectively 
assume scaled orthographic viewing model unknown scale varies frame frame 
joy sampling yields vector constraint equations contains elements form ui ui expressing fact camera basis consists elements length ui expressing fact camera basis elements perpendicular homogenous coordinates 
natural prior proportional exp constraint prior penalises violations constraints quite strongly allows constraint violations paid 
approach essence penalty method 
alternative insist prior uniform constraints satisfied zero practice involve constructing parametrisation domain prior non zero working parametrisation 
approach numerically complex implement disadvantage imposing constraints may fact violated scaled orthography model may sufficient imaging element may misaligned respect lens camera basis consists elements slightly different length 
write posterior model 
recall noise process mixture processes adds gaussian noise second replaces measurement value uniform random variable 
introduce set discrete mask bits measurement matrix mask bits determine noise model measurement affected 
mask bit measurement affected isotropic gaussian noise bad measurement contains information model 
bits compared mask bits fitting mixture models em forsyth ioffe see discussion mclachlan krishnan boundary processes blake zisserman mumford shah 
introduce prior zero matrices fewer non zero elements row column uniform prior ensures attempt inference situations insufficient measurements 
likelihood proportional exponential dij mij meas mij posterior proportional constraint bad exp notice maximum posterior occur maximum likelihood factorisation fit data factor may satisfy camera constraints poorly 

sampling structure motion model formulation contains discrete continuous component 
natural consider gibbs sampler sampling full conditional point positions fixed camera positions full conditional camera positions fixed point positions 
works poorly variables highly correlated tiny shift point position fixed camera positions tends result large error 
continuous variables sampled hybrid method described section discrete variables sampled full conditional strategy proposes inverting bits randomly chosen time 
hybrid mcmc moves proposed probability discrete variable moves proposed probability 
example sampling unknown number components bayesian colour constancy image appearance set surfaces affected reflectance surfaces spectral radiance illuminating light 
recovering representation surface reflectance image information called colour constancy 
computational models customarily model surface reflectances illuminant spectra finite weighted sum basis functions variety cues recover reflectance including limited specular reflections lee constant average reflectance buchsbaum illuminant spatial frequency land mccann low dimensional families surfaces maloney physical constraints reflectance illumination coefficients forsyth finlayson 
cue known strengths weaknesses 
complete study appears freeman uses cues bayesian decisions maximise expected utility compares quality decision inaccurate decisions confound recognition funt 

probabilistic model assume surfaces flat shading variation due surface orientation interreflection 
components model viewing model assume perspective view flat frontal surface focal point positioned center surface 
spatial resolution major issue pixel grid speed 
spatial model surface reflectances spatial statistics primary focus model reflectances constant grid boxes grid edges known advance 
natural improvement random polygon tesselation green 
spatial model illumination described assume single point source position uniformly distributed volume viewed surface 
rendering model determines receptor responses resulting particular choice illuminant surface reflectance follows standard considerations 

rendering model 
model surface reflectances sum basis functions assume reflectances piecewise constant ns set coefficients vary space spatial model 
similarly model sum basis functions assume spatial variation presence single point source positioned diffuse component due source ne ed coefficients basis function gain term represents change brightness source area viewed 
specular component due source ne em gain term represents change specular component area viewed 
standard considerations yield model th receptor response pk hik hik sensitivity th receptor class 
illuminant terms follow point source model obtained phong model specularities 
write prior probability distribution 
model process image generated sample number reflectance steps kx ky respectively prior kx ky kx ky 
sample position steps ex ey respectively prior ex ey kx ky ex kx ey ky tile sample reflectance th tile tile prior joy sampling sample illuminant coefficients prior sample illuminant position prior image adding gaussian noise known standard deviation cc value pixel 
gives likelihood image kx ky ex ey posterior proportional image kx ky ex ey ex kx ey ky kx ky tiles 
priors 
spatial model specify spatial model giving number edges direction separately position edges reflectances block 
assume edges patches direction purely efficiency 
prior poisson distribution censored ensure values greater zero prior rescaled 
edge positions chosen model edge position chosen uniformly second chosen uniformly number pixels fewer third chosen uniformly number pixels second fewer 
model ensures edge close pixel evidence edges moot 
priors reflectance illumination surface reflectance functions zero greater 
means coefficients functions lie compact convex set 
easy obtain representative subset family planes bounds set sampling basis functions set wavelengths 
similarly illuminant functions zero meaning coefficients functions lie convex cone 
cone easily approximated 
constraints reflectance illuminant coefficients encoded prior 
prior constant constraint set falls exponentially estimate distance constraint set 
constraint sets convex expressed set linear inequalities surface reflectance forsyth ioffe cs illuminant ci ifthe coefficients inequalities normalised rows matrices unit vectors largest negative value inequalities estimate distance constraint set 
basis elements illumination reflectance example surfaces look different light light 
phenomenon known occurs real world exploration ambiguities represent possibility 
represent surface colour colour surface rendered known white light 

sampling colour constancy model proposals mixture distinct moves chosen random 
probability proposing particular type move uniform exception edges deaths proposed number edges particular direction maximum births proposed 
important advantage approach move assume values variables changing correct apply standard algorithms estimate values 
calculations straightforward lines green 
moving light proposals new position light obtained filtering image 
apply filter kernel shape typical specularity zero mean components separately responses divided mean intensity sum squared responses rescaled form proposal distribution 
kernel obtained averaging large number specularities obtained draws prior illuminant position 
image data construct proposal distributions appears lead quite efficient samplers quite generally applicable zhu 
call data driven mcmc point 
proposals move light uniform small range current position 
real dataset specularities moves demonstrated synthetic data 
birth edge direction apply derivative gaussian filter red green blue components image divide response weighted average local intensity result squared summed direction interest 
normalised uniform distri bution added 
process produces proposal distribution strong peaks edge specularity completely exclude legal edge point fig 

image information construct appropriate proposal process 
state proposal distribution zeroed points close existing edges consistency hard core model proposed new edge position chosen result 
position chosen choose new reflectances new patches created birth edge 
generally give new patches reflectances similar old patch expect small change posterior advantageous encourages exploration 
currently average receptor responses new patch known illuminant estimate reflectance comes close possible achieving average value lying constraint set 
add gaussian random variable estimated reflectance value currently vector independent gaussian components standard deviation choice depend basis fitted 
death edge edge death proposed chosen uniformly random 
death edge causes pairs surface patches fused new reflectance fused region obtained mechanism birth receptor responses averaged known illuminant estimate reflectances patch vector independent gaussian components standard deviation added result 
moving edge edge move chosen uniformly random 
region available points governed hard core model edge get close edges side new position proposed uniformly random 
somewhat inefficient compared filter energies proposal distribution 
mechanism avoid problem posed hard core model difficult sampler move state edges placed close side real edge 
edge moved real edge new edge proposed right side furthermore may little advantage killing edges 
proposing uniform moves alleviates problem increasing possibility edges move away move right spot 
joy sampling 
proposal distribution edge birth direction mondrian image shown 
proposal distribution obtained filtering image dividing response weighted average local intensity summing direction 
result normalised uniform distribution added 
note filtering process leads strong peaks near edges means proposal process relatively efficient completely rule edges away strong responses evidence presence likelihood component posterior 
forsyth ioffe change reflectance illumination tempting gibbs sampler chain moves extremely slowly 
sample reflectance illumination simultaneously hybrid method section 
poor behaviour gibbs sampler explained follows 
assume sampler burnt means current choice surface reflectance illuminant coefficients yields quite approximation original picture 
assume fixed surface reflectance coefficients wish change illuminant coefficients 
expect normal distribution illuminant coefficients mean close current value fairly narrow covariance substantial change illuminant coefficients lead image different original picture 
means change illuminant coefficients results small 
similarly fix illuminant coefficients sample surface reflectance coefficients expect changes result small 

experimental procedures case sampler started state chosen random state chosen start procedure described detail section 
main difference methods choosing start point tends lead sampler appears burn quickly 

structure motion results obtained hotel dataset courtesy modeling group robotics institute carnegie mellon university 
report types experiment sampler run dataset second small percentage points dataset replaced uniform random numbers range image coordinates 
represents large noise effects 
coordinates dataset appear lie range 
algorithm appears quite behaved rang choices constant 
values constants figs 
meas constraint bad slightly larger meas allowing points range distance measurement measurement disallowed meas constraint figures 
experience suggests possible constraint smaller apparently affecting freedom sampler mixes 

colour constancy fig 
indicates sampler runs synthetic images reasonable estimates position edges specularity illuminant surface colours 
case basis constraints known advance 
applying sampler real data interesting 
data set shown fig 
consists images originally forsyth 

left typical synthetic mondrian rendered linear intensity scale thresholds specularity 
center proposal distribution position specularity obtained image filtering shown highest value white 
right rendering typical sample case sample illuminant successful sampler produces samples look image 
results real images shown colour fig 

images set patches mondrian coloured patches photographed white blue yellow purple red cyan light 
specularities diffuse model data set 
original data lost versions scanned images displayed crt photographed display subjected colour printing scanned remarkable constancy possible circumstances 
basis obtained bilinear fitting procedure 
determining appropriate constraint regions difficult obtained natural coordinate system principal components constructed bounding box coordinate system 
box grown axis understanding colours forsyth deeply saturated 
red green blue receptor responses represented numbers range zero cc implying top bits receptor response reliable 

assessing experimental results sections phrased standard vision problems inference problems 
quite nasty inference problems large numbers continuous discrete variables 
possible sections indicated extract representation posterior problems 
believe representations helpful 
compare representations methods offer 
observed making comparisons 
firstly important apply reality check representations sampler produces determine reason believe sampler burnt 
secondly comparing representation posterior data result method reports minimum error solution offers error check 
nature information produced algorithms different 
meaningful comparison possible reports properties posterior 
gold standard tests available methods known produce accurate representations posterior density test sampler 
compare representation produced sampler methods significantly cheaper computationally 
joy sampling 
reality checks sampler burnt mixing 
convergence diagnostics mcmc methods see besag roberts suggest convergence exists easy produce chain pass tests having burnt 
rely general methods 
firstly check ensure sampler move value posterior start position reasonable number moves 
secondly check state sampler moves freely domain represented 
third built various consistency checks experiments 

structure motion 
shows series samples drawn posterior structure motion problem indication order samples drawn indicating sampler mixing relatively 
sampler mixing rate appear sufficient give reasonable estimate structure posterior mode clear sampler move domain freely 
posterior contains discrete symmetry fixed value mask bits multiply square root identity left square root identity right obtain value posterior 
creates particular difficulty practice solutions widely isolated 
sampler move peak peak probability hybrid method obtain sufficient momentum cross large regions low probability effectively zero 
fact desirable property symmetry means accurate estimates mean value zero 
consistency checks general expect sampler behaving properly able identify correspondence errors produce stable representation 
fact number subtle tracker errors hotel sequence 
shows sampler identify tracker errors 
illustrates large tracker errors artificially inserted dataset purpose identified 

colour constancy 
sampler described run synthetic images ground truth known case reaches small neighbourhood ground truth randomly forsyth ioffe 
plots illustrate path taken state space structure motion sampler 
plot connects position point tenth sample starting th 
paths coded grey level clarity early samples light path moves darker grey levels 
fact paths repeatedly cross return regions suggests sampler mixing freely 
selected start point burns samples 
experimental data shown suggests sampler mixes wide spread marginal densities reflectances 
consistency checks sampler run images scene fact images scene built model 
spread samples surface reflectance coefficients recovered particular surface particular image quite wide see fig 

compare spread samples surface different images clusters overlap 
means representation correctly encoding fact surfaces similar 
fact shall see section representation encodes fact surface patches similar 

attractive properties sampled representations attractive properties sampled representations derived provide covariance estimate inferred state resampled incorporate new information appear stable perturbations input data set 
describe properties 

covariance 
samplers described produce representation posterior probability distribution joy sampling 
cropped frames hotel sequence showing single sample reconstruction 
squares correspond measurements mask bit measurement point frame believed correct white cross dark background corresponds measurement mask bit zero measurement point frame believed incorrect grey diamonds correspond model predictions 
extent diamond centered square gives extent model prediction supported data 
right frame locations tracker skipped feature unknown reasons 
case reconstruction identifies data point erroneous point significantly different position measurement reported tracker lying correct measurement seen position relative surface texture object 
data set 
particularly attractive feature special datasets require additional analysis 
example element image colour expect colour constancy sampler produce wide spread samples surface reflectance similarly structure motion data set obtained camera translating plane sampler return set samples substantial variance perpendicular plane ado 
second attractive feature expectations marginal probability distributions easily available compute expectation function average function value samples compute marginal drop irrelevant terms state sample 
illustrates kind information sampler produce structure motion data particular sampler reflects scatter possible inferred values single point 
show set typical results sampler produce real images colour constancy problem 
spatial model identifies edges correctly 
groups samples drawn surface reflectance different lights intersect expect 
furthermore groups samples drawn different surface reflectances light tend intersect meaning surfaces generally seen different 
shows rendering samples white light give impression variation descriptions results 

resampling incorporate new information 
assume engaged colour constancy 
construct representation surface colour new forsyth ioffe 
perturb hotel sequence replacing data points draws uniform distribution image plane 
bayesian method started section easily discounts noise points shows frames sequence fig 
show noise sample reconstruction indicated notation squares correspond measurements mask bit measurement point frame believed correct white cross dark background corresponds measurements mask bit zero measurement point frame believed incorrect grey diamonds correspond model predictions 
extent diamond centered square gives extent model prediction supported data 

black points show overhead view single sample reconstruction obtained frames points hotel sequence rotated hand show right angled structure model indicating structure qualitatively correct cloud grey points samples position single point scaled show small uncertainty available single point measurement 
information arrives 
representation probabilistic answer relatively straightforward adjust representation convey posterior incorporating new information 
ex ample assume sampled representation posterior distinct images 
told patch image patch impact interpretation images 
sampled representation suited determining effect information 
particular samples state image state image suppressed details rest state notation 
interpret mean patch sample gaussian distribution unknown mean known standard deviation 
obtain samples state state image image image abbreviated im 
im im state state proportional im state im state term inside integral state image state image sets samples samples independent identically distributed shuffling remove correlations introduced mcmc 
means conditional density th sample state image 
construct new sampler state 
ensure produces samples distribution indexes previous set samples 
marginalise respect simply dropping values sample 
joy sampling result set samples distributed desired distribution im state im state building sampler obtains samples space desired distribution involves technical difficulties scope 
approach essentially chooses pairs consisting sample set image sample set image pairs chosen frequency higher values inferred particular patch similar 
course trick extends images 
shows results obtained assuming single surface patch images 
typically small number sets samples higher probability sampled representation consists large number copies samples interspersed 
results reduced variance rendering patch known similar images error balls surface patch intersect relatively small region 
mean variance inferred reflectances patches reduced 
reduced fig 
representations recovered separate input image correctly captures possibility surface patches 
important reality check strongly suggests sampled representation trustworthy algorithm able information patch image obtain representation strongly suggests patches 

stability recovered representations 
reconstructions compared basis accuracy ground truth available 
demonstrate sampled representations stable various perturbations input 
structure motion small errors tracker response points lead significant perturbations reconstruction points reconstructed point positions independent coupled reconstructed camera configurations 
small errors tracker response occur frames hotel sequence point measurements frames affected forsyth ioffe small tracker errors shown fig 

small errors affect reconstruction obtained factorisation method factorisation matrix function entries equivalently reconstructed point positions coupled reconstructed camera configurations 
compare stability methods introduce larger tracker errors small percentage data points randomly selected replaced draws uniform distribution image plane 
points included factorisation results essentially meaningless 
provide fair comparison obtained method section start points sampler 
reconstructions guaranteed ignore large error points ignore significant percentage data 
comparison sampler quickly points consistent model gives significantly stable measurements cf torr zisserman uses maximum likelihood identify correspondences 
reconstruction unknown scaled euclidean frame reconstructions best compared comparing angles subtended corresponding triples points comparing distances corresponding points scaled minimize errors 
sampled representation significantly stable tracker errors noise factorisation method figs 

joy sampling 
comparing different algorithms obtaining covariance estimates probability distributions devices computing expectations 
computing expectation integration problem high dimensional problems described curse dimensionality applies quadrature methods appropriate review numerical integration methods evans swartz 
leaves possibilities random quasi random method analytic approximation integral 
applying quasi random methods problems described appears pose substantial technical difficulties refer interested reader evans swartz traub 
analytic approximation currently computer vision laplace method described evans swartz form ripley shall call approximation laplace method follows 
approach models unimodal posterior distribution normal distribution mean mode posterior covariance matrix inverse hessian posterior mode 
essence approximation notes main contribution expectation computed probability distribution mode contribution tails estimated hessian mode 

images set patches mondrian coloured patches photographed white blue purple red aqua yellow light scanned forsyth inputs sampler 
renderings typical representations obtained sampler case shown coloured light inferred successful result inferred representation looks image 
note accuracy spatial model robustness image noise 
renderings typical representations white light successful result implies similar renderings 
components surface reflectance samples plotted axes different surfaces 
sample colour keyed image obtained red samples red image black corresponding white image 
circles show samples reflectance coefficients blue surface top left corner mondrian stars yellow surface second row show samples orange surface top row mondrian crosses red surface bottom row 
surface generates samples represent uncertainty inferred surface reflectance particular image input 
important consistency check data 
notice samples corresponding particular surface image intersects corresponding surface 
means representation possibility commit 
components surface reflectance samples plotted axes different surfaces 
come samples shown resampled assumption blue surface top left hand corner mondrian image 
representation axes 
notice single piece information reduces ambiguity representation 
samples reflectances returned patch mondrian images shown light rendered white light 
samples patch illuminant rendered small square patch little information shows salt pepper style texture 
rows show samples patch different column corresponds illuminant order aqua blue purple red white yellow 
notice substantial variation appearance white pixels denote samples saturated 
notice patch samples look similar 
samples obtained samples resampled assuming right blue patch patch image 
samples obtained samples resampled assuming sixth yellow patch patch image 
notice substantial reduction variance constraint force patches look fact surface 
forsyth ioffe 
factorisation method relatively unstable noise 
compare reconstructions obtained uncorrupted data set reconstructions obtained entries replaced draws uniform distribution image plane represent factorisation method fairly start points obtained algorithm section masks suspect measurements 
left shows histogram relative variations distances corresponding pairs points right shows histogram differences angles subtended corresponding triples points 
note scales interpoint distances factor angles 

bayesian method stable noise 
compare reconstructions obtained uncorrupted data set reconstructions obtained entries replaced draws uniform distribution image plane 
left shows histogram relative variations distances corresponding pairs points right shows histogram differences angles subtended corresponding triples points 
note significant increase stability factorisation method relative errors distance order angular errors order 
laplace method natural linearisation estimates covariance structure motion literature morris kanade 
fig 
indicates estimates produces differ substantially estimates produced sampler 
seen section sampler appears mix acceptably samples significantly covariance com pare fig 
fig 
shows order samples drawn samples fig 

laplace method approximates probability density function poorly 
log posterior consists largely terms degree 
cases hessian significantly poor guide structure log posterior long way mode 
joy sampling 
compare sampled representation posterior structure motion problem representation obtained analytic approximation 
plots depict different estimates marginal posterior probabilities point position plane parallel optical axis 
points points fig 
samples shown scatter plot 
case standard deviation ellipse covariance estimate obtained laplace approximation largest shown substantially overestimates covariance orientation misleading plotted light grey 
case second largest ellipse standard deviation ellipse obtained laplace approximation assuming point camera positions independent overestimate better estimate laplace approximation plotted dark grey 
smallest ellipse case obtained sample mean covariance plotted darkest grey 
laplace approximation appears significantly overestimate covariance certainly hessian mode poor guide behaviour tails posterior problem 
particular overestimates weight tails overestimates covariance 
purely local estimate structure posterior rely second derivative function point necessarily convey helpful information function doing long way away point 
comparison sample involves 
comparison values posterior sample previous sample samples relying local estimate structure posterior 
really useful comparison available case colour constancy 
current colour con algorithms report exact solutions minimum error solutions 
laplace method produce absurd covariance estimates domain integration heavily truncated constraints section tails contribution unreasonable expect sensible approximation method 

speed samplers relatively slow 
samples take longer draw structure motion problem samples views points day forsyth ioffe mhz macintosh system compiled matlab colour constancy problem samples hour compiled matlab computer 
slow technology 
particular important keep mind cheaper technologies laplace approximation estimate covariance section comes mind may offer significantly inaccurate representations 
possibilities speedups intelligent choice start point particular reason start samplers random start point wait gradient descent component hybrid mcmc find mode 
start sampler decent estimate mode describe relevant methods 
faster mixing rate generally better sampler mixes fewer samples needs draw samples increasingly mimic iid samples 
isn clear build truly fast mixing sampler 
best strategy appears image data structure proposal distribution section zhu proofs leads fast mixing sampler 
lower sample cost decent representation covariance available fewer samples 
means sample cheap obtain 
current possibilities include faster integrator hybrid mcmc method symplectic runge kutta nystrom method sanz serna calvo effort choose fastest integrator grouping variables allows efficient gibbs sampler separating cameras points leads standard form sampler minuscule changes state sample reason illustrated fig 
fitting gaussian sample gaussian propose new state 

starting sfm sampler 
sampler state 
show examples 
means domain sampler resp 
copies resp 

relations discrete continuous variables complex small errors sampler started random point burns relatively quickly large errors burn slow 
values depend strongly corresponding signifi cant tracker error error strongly affect values effect slows convergence sampler incorrect values continuous parameters mean data points lie long way values predicted model little distinction points correspond model points 
start sampler fair initial estimate mode 
obtain initial value mask ma sampling independent distribution bits tends points distant corresponding points previous frames 
particular th bit ma probability exp ij exp ij ij di di di di di di di di problem quantity data number parameters model choice fairly unimportant main issue choose value small large tracker errors masked certainly 
va maximise dij ij ik va kj ij obtained sweep algorithm fixes resp 
solves linear system resp 
swaps variables sweeps continue convergence guaranteed 
compute affine transformation minimised draw sample full conditional bit mask matrix obtain start state 

starting colour constancy sampler 
sampler converges started random sample prior slow unnecessarily inefficient 
guess edge positions follows choosing set edges maxima edge proposal distributions censored ensure model applies 
similarly start point light position follows choosing maximum likelihood position proposal distribution specular position known estimate illuminant colour follows 
patch obtain reflectance estimate average colour patch illuminant colour 
yields start point sampler converges relatively quickly 

discussion ups downs sampling methods samplers fast burn quickly mix 
proven samplers theory obviously bad merely mysterious behaviour 
possible build samplers yield representations pass wide range sanity checks fairly fast 
probably best hoped near 

points favour sampled representations points favour sampled representations strongest simple management uncertainty comes methods 
samples available managing information simple 
computing expectations marginalization useful activities particularly easy 
incorporating new information principle simple 
output properly built sampler excellent guide inferences drawn ambiguities dataset 
example fig 
show uncertainty position single point space determined structure motion method result image noise 
independence assumptions required obtain information furthermore required specialised methods camera motion degenerate example camera translates plane effect appear scatter plots vary widely axis perpendicular plane 
main benefit results simple information integration 
building vision systems reasonable scale requires cue integration example happens colour reports region blue shape says fire engine 
contradiction resolved understanding reliability reports 
properly built bayesian model incorporates available information particularly attractive natural likelihood prior models available examples sections 
principle sampling arbitrary posteriors 
joy sampling feature sampled methods handle complex spatial models 
main difficulty models domains complicated topologies 
example simple deal domain consists components different dimension green 
means spatial model part posterior 
example section model layout mondrian grid rectangles position number horizontal vertical edges grid known 
inferred data 
offers prospect unifying information coherence spatial layout model appearance performing segmentation explicit spatial models 
sampling methods standard approach performing inference spatial models geyer moller 

problems samplers samplers principle generic practice building sampler requires significant degree skill 
number samples required large 
vision problems typically consist large numbers discrete continuous variables 
posterior complicated function high dimensional space important modes extremely large number samples may required support useful representation samples mixture model simplified parametric model fitted samples 
phrased vision problems expect see small number quite tight modes posterior suggesting relevant portion posterior represented manageable numbers samples furthermore accurate representation tails significant need reasonable description modes 
samplers currently relatively slow 
possible build samplers fast useful solutions real vision problems obtained reasonable amounts time 
generally prospect understanding build better systems precedes understanding build faster systems 
sampled representations claim universality 
conceivable representation scheme appears rest presence samples 
example wish approximate posterior mixture model 
fit model set samples compute various integrals representing error numerical integrators high dimensions sampling methods form forsyth ioffe 
suggests problem persuaded take series manageable parametric forms deterministic algorithms computing fits available stuck difficulties come sampling methods 
vision problems form adapted sampling methods 
particular usually preponderance evidence meaning posterior large isolated peaks location estimated 
furthermore commonly case computer vision algorithms compute values variables known 
metropolis hastings algorithm gives framework algorithms integrated easily produce series hypotheses meaningful semantics 
samplers poorly adapted problems lead large domains essentially uniform probability 
occur example mrf model may large number states essentially near maximal posterior probability small number label flips away extremum 
difficulty sampler representation produces 
quite easy set examples require large numbers samples represent regions particularly dimension domain large 
fair case problems properly imposing parametric form strategy adopted addressing firstly large domains essentially uniform probability suggest problem parameters don significant effect outcome secondly estimates mode extremely unstable thirdly estimator expectation problem high variance 
samples trusted 
typically samples discarded allow sampler burn 
rest represent posterior 
usual approach start different sequences different points confirm give comparable answers gelman rubin geweke roberts 
approach prove proposal process rapid mixing properties extremely difficult jerrum sinclair 
rapid mixing desirable faster sampler mixes lower variance expectations estimated samples geyer 
mechanism available practical problems structure experimental give checks behaviour sampler 
example structure motion sampler able identify bad measurements gave stable reconstructions section similarly colour constancy resampling algorithm correctly reduced variance inferred colour patches informed patches colour section 

reasons interesting vision problems behaved samplers quite practical tools 
firstly vision problems overwhelming quantity data compared number parameters studied result usual expect posterior small number quite peaked modes exploration domain sampler restricted small subsets 
secondly substantial body algorithms estimates position modes derivative filters estimating position edges factorisation estimating structure motion sampler started state 
vision problems display kind conditional independence property allows large problem decomposed sampling resampling problem section ioffe forsyth 
acknowledgments research supported part adobe systems part nsf fellowship si part nsf digital library award nsf iis 
hotel sequence appears courtesy modeling group robotics institute carnegie mellon university 
stuart russell pointing significance mcmc inference technique 
note 
indebted andrew zisserman suggestion 
amit grenander 
structural image restoration deformable templates 
am 
statist 
ass 
beardsley zisserman murray 
sequential updating projective affine structure motion 
int 
computer vision 
besag green higdon mengersen 
bayesian computation stochastic systems 
statistical science 
binford levitt 
model recognition objects complex scenes 
image understanding workshop pp 

blake isard 
condensation conditional density propagation visual tracking 
int 
computer vision 
blake zisserman 
visual reconstruction 
cambridge ma mit press 
freeman 
bayesian colour constancy 
opt 
soc 
am 
buchsbaum 
spatial processor model object colour perception 
franklin inst 
carlin louis 
bayes empirical bayes methods data analysis 
chapman hall 
carpenter clifford 
improved particle filter non linear problems 
ieee proc 
radar sonar navigation 
chou brown 
theory practice bayesian image labeling 
int 
computer vision 
collins golden 
simulated annealing annotated bibliography 
technical report university maryland college park college business management 
costeira kanade 
multibody factorisation method independently moving objects 
int 
computer vision 
debevec taylor malik 
modeling rendering architecture photographs hybrid geometry image approach 
siggraph pp 

dellaert seitz thorpe thrun 
structure motion correspondence 
ieee conf 
computer vision pattern recognition pp 

doucet de freitas gordon 
sequential monte carlo methods practice 
springer verlag new york 
duane kennedy 
hybrid monte carlo 
physics letters 
evans swartz 
approximating integrals monte carlo deterministic methods 
oxford university press new york 
faugeras robert 
images tell third 
int 
computer vision 
faugeras robert laveau csurka zeller 
reconstruction urban scenes image sequences 
computer vision image understanding 
finlayson colour perspective 

ieee pattern analysis machine intelligence 
fischler bolles 
random sample consensus paradigm model fitting application image analysis automated cartography 
comm 
acm 
forsyth 
novel algorithm colour constancy 
int 
computer vision 
funt barnard martin 
machine colour constancy 
eccv pp 

joy sampling 
markov chain monte carlo 
chapman hall new york 
gelman rubin 
inference iterative simulation multiple sequences 
statistical science 
gelman carlin stern rubin 
bayesian data analysis 
chapman hall london 
geman geman 
stochastic relaxation gibbs distributions bayesian restoration images 
ieee pattern analysis machine intelligence 
geman 
markov random field image models application computer vision 
proc 
int 
congress math 
geweke 
evaluating accuracy sampling approaches calculation posterior moments 
bayesian statistics bernardo berger dawid smith eds 
oxford clarendon press 
geyer 
likelihood inference spatial point processes 
stochastic geometry likelihood computation 
barndorff nielsen kendall van eds chapman hall boca raton 
gilks roberts 
strategies improving mcmc 
markov chain monte carlo practice gilks richardson spiegelhalter eds chapman hall new york 
gilks richardson spiegelhalter 
introducing markov chain monte carlo 
markov chain monte carlo practice gilks richardson spiegelhalter eds chapman hall new york 
gilks richardson spiegelhalter 
markov chain monte carlo 
markov chain monte carlo practice gilks richardson spiegelhalter eds chapman hall new york 
gilks richardson spiegelhalter 
eds 

markov chain monte carlo practice 
chapman hall new york 
golden 
simulated annealing solve routing location problems 
naval res 
log 
quart 
van gool zisserman 
automatic model building video sequences 
european transactions telecommunications 
green 
reversible jump markov chain monte carlo computation bayesian model determination 
biometrika 
green 
mcmc image analysis 
markov chain monte carlo practice gilks richardson spiegelhalter eds chapman hall new york pp 

grenander 
tutorial pattern theory 
technical report brown university providence rhode island 
grenander ulf 

general pattern theory 
oxford university press new york 
hartley zisserman 
multiple view geometry 
cambridge university press new york 
huang koller malik rao russell weber 
automatic symbolic traffic scene analysis belief networks 
aaai pp 

ioffe forsyth 
finding people sampling 
int 
conf 
computer vision pp 

forsyth ioffe jacobs 
linear fitting missing data applications structure motion characterizing intensity images 
ieee conf 
computer vision pattern recognition 
jerrum mark sinclair alistair 

markov chain monte carlo method approach approximate counting integration 
approximation algorithms np hard problems hochbaum ed pws publishing boston 
jolly lakshmanan jain 
vehicle segmentation classification deformable templates 
ieee pattern analysis machine intelligence 
kanazawa koller russell 
stochastic simulation algorithms dynamic probabilistic networks 
proc uncertainty ai 
kitagawa 
non gaussian state space modelling nonstationary time series discussion 
am 
stat 
assoc 
land mccann 
lightness retinex theory 
opt 
soc 
am 
lee 
method computing scene illuminant chromaticity specular highlights 
opt 
soc 
am 
li 
markov random field modeling computer vision 
springer verlag new york 
maloney 
computational model color constancy 
opt 
soc 
am 

linear models surface illuminant spectra 
opt 
soc 
am 
maybank sturm 
minimum description length inference scene structure images 
iee colloquium applied statistical pattern recognition pp 

mclachlan krishnan 
em algorithm extensions 
john wiley sons new york 
moller 
markov chain monte carlo spatial point processes 
stochastic geometry likelihood computation barndorff nielsen kendall van eds chapman hall boca raton 
morris kanade 
unified factorization algorithm points line segments planes uncertainty models 
int 
conf 
computer vision pp 

mumford shah 
optimal approximations piecewise smooth functions associated variational problems 
comm 
pure appl 
math 
neal 
probabilistic inference markov chain monte carlo methods 
computer science tech report crg tr university toronto 
noble mundy 
template bayesian viewpoint 
ieee conf 
computer vision pattern recognition pp 

pavlovic frey huang 
time series classification mixed state dynamic bayesian networks 
ieee conf 
computer vision pattern recognition pp 

pavlovic rehg cham tat jen murphy 
dynamic bayesian network approach tracking learned dynamic models 
int 
conf 
computer vision pp 

phillips smith 
bayesian model comparison jump diffusion 
markov chain monte carlo practice gilks richardson spiegelhalter eds chapman hall 

projective factorisation method recovering shape motion 
cmu cs carnegie mellon university 
richardson green 
bayesian analysis mixtures unknown number components 
proc 
roy 
stat 
soc 

ripley 
stochastic simulation 
wiley 
ripley 
pattern recognition neural networks 
cambridge university press 
roberts 
convergence diagnostics gibbs sampler 
bayesian statistics bernardo berger dawid smith eds oxford clarendon press 
roberts 
markov chain concepts related sampling algorithms 
markov chain monte carlo practice gilks richardson spiegelhalter eds chapman hall new york 
rousseeuw 
robust regression outlier detection 
wiley new york 
sanz serna calvo 
numerical hamiltonian problems 
chapman hall new york 
sarkar boyer 
perceptual organization bayesian networks 
ieee conf 
computer vision pattern recognition pp 

sarkar boyer 
automated design bayesian perceptual inference networks 
ieee conf 
computer vision pattern recognition pp 

sullivan blake isard maccormick 
object localization bayesian correlation 
int 
conf 
computer vision pp 

tierney 
general state space markov chain theory 
markov chain monte carlo practice gilks richardson spiegelhalter eds chapman hall new york 
tomasi kanade 
shape motion image streams orthography factorization method 
int 
comp 
vision 
torr murray 
development comparison robust methods estimating fundamental matrix 
int 
computer vision 
torr zisserman 
robust computation parametrization multiple view relations 
int 
conf 
computer vision pp 

traub 
complexity information 
cambridge university press 
triggs 
factorization methods projective structure motion 
ieee conf 
computer vision pattern recognition pp 

yuille 
high level generic models visual search high level knowledge help 
ieee conf 
computer vision pattern recognition pp 

zhu wu mumford 
filters random fields maximum entropy frame unified theory texture modelling 
int 
computer vision 
zhu 
stochastic computation medial axis markov random fields 
ieee conf 
computer vision pattern recognition pp 

zhu zhang tu 
integrating bottom topdown object recognition data driven markov chain monte carlo 
ieee conf 
computer vision pattern recognition pp 

