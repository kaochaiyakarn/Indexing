keywords reinforcement learning neural networks adaptive multi agent systems agent economies pricing agent economies neural networks multi agent learning gerald tesauro ibm watson research center saw mill river rd hawthorne ny mail tesauro watson ibm com investigates adaptive software agents may utilize reinforcement learning algorithms learning economic decisions setting prices competitive marketplace 
single adaptive agent facing fixed strategy opponents ordinary learning guaranteed find optimal policy 
population agents trying adapt presence adaptive agents problem non stationary history dependent known global convergence obtained solutions optimal 
studies simultaneous learning competing seller agents moderately realistic economic models 
simplest case interesting multi agent phenomena occur state space small lookup tables represent functions 
despite lack theoretical guarantees simultaneous convergence self consistent optimal solutions obtained model small values discount parameter 
cases convergence large discount parameters 
furthermore derived policies increase profitability damp eliminate cyclic price wars compared simpler policies zero short term lookahead 
function approximators neural nets lookup tables investigated preliminary findings indicate reasonably policies obtained absolute accuracy function approximation may poor 
reinforcement learning rl procedures established powerful practical methods solving markov decision problems 
significant actively investigated rl algorithms learning watkins 
learning algorithm learning estimate long term expected reward state action pair 
nice property need model environment line learning 
number powerful convergence proofs showing learning guaranteed converge probability cases state space small lookup table representations watkins dayan 
furthermore large state spaces lookup table representations infeasible rl methods combined function approximators give practical performance despite lack theoretical guarantees convergence optimal policies 
real world problems fully markov nature non stationary history dependent fully observable 
order rl methods generally useful solving problems need extended handle non markovian properties 
important application domain non markovian aspects paramount area multi agent systems 
area expected increasingly important due potential rapid emergence agent economies consisting large populations interacting software agents engaged various forms economic activity 
problem multiple agents simultaneously adapting general non markov agent provides effectively non stationary environment agents 
existing convergence guarantees hold general known global convergence obtained solutions optimal 
progress analyzing certain special case multi agent problems 
example problem teams agents share common utility function studied example crites barto 
likewise purely competitive case zero sum utility functions studied littman algorithm called minimax proposed player zero sum games shown converge optimal value function policies players 
sandholm crites studied simultaneous learning players iterated prisoner dilemma game sandholm crites learning procedure generally converged stationary solutions 
extent solutions optimal unclear 
hu wellman proposed algorithm calculating optimal functions player arbitrary sum games hu wellman 
algorithm important step 
appear useable practical problems assumes policies followed players nash equilibrium policies address equilibrium coordination problem multiple nash equilibria agents decide equilibrium choose 
may serious problem folk theorem iterated games kreps proliferation nash equilibria sufficiently high emphasis rewards large value discount parameter fl 
furthermore may inconsistencies assumed nash policies policies implied functions calculated algorithm 
examines simultaneous learning economically motivated player game 
players assumed sellers similar identical products compete basis price 
time step sellers alternately take turns setting prices account seller current price 
price set consumers respond instantaneously deterministically choosing seller product seller product product current price pair leading instantaneous reward utility sellers respectively 
assumed sellers full knowledge expected consumer response price pair fact full knowledge utility functions 
builds prior research reported tesauro kephart tesauro kephart 
papers examined effect including foresight ability anticipate longer term consequences agent current action 
different algorithms agent foresight generalization minimax search procedure player zero sum games ii generalization policy iteration method dynamic programming players policies simultaneously improved self consistent policy pairs obtained optimize expected reward time steps 
including foresight agents pricing algorithms generally improved agent profitability usually damped eliminated pathological behavior cyclic price wars long episodes repeated undercutting sellers alternate large jumps price 
price wars prior studies agent economy models kephart hanson sairamesh sairamesh kephart agents optimal pricing algorithms optimize immediate reward anticipate longer term consequences agent current price setting 
primary motivations studying simultaneous learning 
functions learned simultaneously players policies implied functions self consistently optimal 
words agent able correctly anticipate longer term consequences actions agents actions correctly model agents having equivalent capability 
classic problem infinite recursion opponent models avoided 
contrast approaches adaptive multi agent systems issues problematic 
example hu wellman study situation single strategic agent able anticipate market impact pricing actions population reactive agents anticipatory capability 
likewise vidal durfee propose recursive opponent modeling scheme level agents opponent modeling level agents model opponents level level agents model opponents level 
approaches effective way agent model agents equivalent level depth complexity 
second advantage learning solutions correspond deep lookahead principle function represents expected reward looking infinitely far ahead time exponentially weighted discount parameter fl 
contrast prior tesauro kephart shallow finite lookahead 
comparison directly modeling agent policies function approach extensible situation large economies competing sellers 
approximating functions nonlinear function approximators neural networks intuitively feasible approximating corresponding policies 
furthermore function approach agent needs maintain single function policy modeling approach agent needs maintain policy model agent infeasible number sellers large 
remainder organized follows 
section describes structure dynamics model seller economy presents economically models seller utility information filtering shopbot known prone price wars agents optimize short term payoffs 
system parameters chosen place systems price war regime 
section describes implementation details qlearning model economies 
step simple case ordinary learning considered sellers uses learning seller uses fixed pricing policy optimal policy 
section examines interesting novel situation simultaneous learning sellers 
section studies single agent learning models neural networks compares results section lookup tables 
section summarizes main discusses promising directions challenges 
model agent economies real agent economies contain large numbers agents complex details agents behave interact multiple time scales 
order initial progress number simplifying assumptions 
economy restricted competing sellers offering similar identical products large population consumer agents 
sellers compete basis price assumed prices discretized lie maximum price number possible prices 
renders state space small feasible lookup tables represent agents pricing policies expected utilities 
time simulation discretized time step consumers compare current prices sellers instantaneously deterministically choose purchase seller 
time step possible pair seller prices deterministic reward utility seller 
simulation iterate forever may may discounting factor value rewards 
worth noting consumers regarded players model 
consumers strategic role behave extremely simple fixed short term greedy rule buy lowest priced product time step regarded merely providing stationary environment sellers compete player game 
clearly simplifying step study multi agent phenomena models extended include strategic adaptive behavior part consumers 
change notion desirable system behavior 
model desirable behavior resemble collusion sellers charging high prices obtain high profits 
obviously desirable consumers viewpoint 
regarding dynamics seller price adjustments assumed sellers alternately take turns adjusting prices simultaneously setting prices game extensive form normal form 
choice alternating turn dynamics motivated considerations number sellers large model realistic reasonable assume sellers adjust prices different times time probably take turns defined order 
alternating turn dynamics stay normal learning framework function implies deterministic optimal policy known player alternating turn games exists deterministic policy non deterministic policy littman 
contrast games simultaneous moves example rock scissors possible deterministic policy optimal existing qlearning formalism mdps modified extended yield non deterministic optimal policies 
learning studied different economic models described detail sairamesh kephart kephart hanson sairamesh greenwald kephart 
model called price quality model sairamesh kephart models sellers products distinguished different values scalar quality parameter higher quality products perceived valuable consumers 
consumers modeled trying obtain lowest priced product time step subject threshold type constraints quality price consumer maximum allowable price minimum allowable quality 
similarity substitutability seller products leads potential direct price competition vertical differentiation due quality values leads asymmetry sellers utility functions 
believed asymmetry responsible cyclic price wars emerge sellers employ pricing strategies 
second model information filtering model described detail kephart hanson sairamesh 
model competing sellers news articles somewhat overlapping categories 
contrast vertical differentiation model model contains horizontal differentiation differing article categories 
extent categories overlap direct price competition extent differ asymmetries introduced lead potential cyclic price wars 
third model called shopbot model described greenwald kephart intended model situation internet consumers may shopbot compare prices sellers offering product select seller lowest price 
model sellers products exactly identical utility functions symmetric 
pricing leads sellers undercut minimum price point reached 
point new price war cycle launched due buyer asymmetries seller asymmetries 
fact buyers shopbot buyers choose seller random means profitable seller abandon low price competition bargain hunters maximally exploit random buyers charging maximum possible price 
example economic utility function taken price quality model follows represent prices charged seller seller respectively 
represent respective quality parame ters represent cost seller producing item quality assuming particular model consumer behavior described show analytically limit infinitely consumers instantaneous utilities profits consumer obtained seller seller respectively ae gamma gamma gamma gamma ae gamma gamma plot utility landscape seller function prices parameter settings 
specific parameter settings chosen known generate harmful price wars agents myopic optimal pricing 
see myopic optimal price seller function seller price obtained value sweeping values choosing value gives highest utility 
see small values peak utility obtained larger values eventually discontinuous shift peak follows parabolic shaped ridge landscape 
analytic expression myopic optimal price seller function follows defining gamma gamma similarly myopic optimal price seller function price set seller formula assuming prices discrete ffl price discretization interval gamma ffl similar utility landscapes seller information filtering model shopbot model 
models existence multiple disconnected peaks landscapes relative heights change depending seller price leads price wars sellers behave 
models assumed simplicity players essentially perfect information 
model consumer behavior perfectly perfect knowledge costs utility functions 
model essence player perfect information deterministic game similar games sample utility landscape seller model function seller price seller price chess 
main differences utilities strictly zero sum terminating absorbing nodes state space 
payoffs players time step games chess payoffs terminating nodes 
mentioned previously possible seller prices constrained lie range maximum allowable price 
prices discretized create lookup tables seller utility functions 
furthermore optimal pricing policies seller function seller price represented form table lookups 
single agent learning consider ordinary single agent learning seller economic models seller uses qlearning learn function corresponding policy seller maintains fixed pricing policy 
procedure learning follows 
represent discounted long term expected reward agent action state discounting rewards accomplished discount parameter fl value reward expected time steps discounted fl assume function represented lookup table containing value possible state action pair assume table entries initialized arbitrary values 
procedure solving infinitely repeat step loop 
select particular state particular action observe immediate reward state action pair observe resulting state 
adjust equation deltaq ff fl max gamma ff learning rate parameter max operation represents choosing optimal action possible actions taken successor state leading greatest value 
wide variety methods may select state action pairs step provided state action pair visited infinitely 
stationary markov decision problem learning procedure guaranteed converge correct values provided learning rate parameter decreased time appropriate schedule 
simulations described fixed policy seller uses policy represented example price quality model equations 
model distinction states actions somewhat blurred 
assumed state seller sufficiently described seller price action current price decision 
sufficient state description history needed determination immediate reward calculation price fixed strategy player 
definitions immediate reward state modified agent case follows state obtained starting action learner response action fixed strategy opponent 
likewise immediate reward defined sum rewards obtained actions 
modifications introduced state player move state 
possible alternative investigated include side move additional information state space description 
important issue learning exploration state space 
state action pair visited sufficiently learning converge 
necessitates sort randomized policy trajectory generation example boltzmann exploration 
simulations reported sequence stateaction pairs selected table updates generated uniform random selection possible table entries 
correspond actual line training real environment appropriate situations accurate simulation environment envision attempting solve optimal strategies players line training laboratory 
initial values tables generally set immediate reward values 
consequently initial derived policies corresponded policies 
learning rate varied time ff ff fit initial learning rate ff usually set constant fi simulation time measured units size table 
number possible prices selected player 
number different values discount parameter fl studied ranging fl fl 
results single agent learning models indicated learning worked expected case 
model value discount parameter exact convergence table stationary optimal solution 
convergence times ranged sweeps table element smaller values fl updates largest values fl 
addition learning converged expected cumulative profit utility policy derived function measured running policy player myopic policy random starting states time steps averaging resulting cumulative utility player 
case seller achieved greater profit myopic opponent derived policy myopic policy 
true fl due redefinition updates summing time steps case fl effectively corresponds step optimization step optimization myopic policies 
furthermore cumulative utility obtained derived policy monotonically increased increasing fl expected 
interesting note cases expected utility myopic opponent increased playing learner improved monotonically increasing fl 
explanation better exploiting myopic opponent expected zero sum game learner reduced region participate mutually undercutting price war 
typically finds models myopic vs myopic play large amplitude price wars generated start high prices persist way low prices 
learner competes myopic opponent price wars starting high prices learner abandons price war quickly prices decrease 
effect price war regime smaller confined higher average prices leading closer approximation cooperative collusive behavior greater expected players 
interesting come players entirely selfish 
illustrative example results single agent learning shown figures 
plots average utility sellers shopbot model sellers myopic learner 
model symmetric doesn matter seller learner 
myopic vs shopbot model myopic vs myopic average profit plot average utility time step seller seller function discount parameter fl shopbot model seller seller open circles uses myopic policy seller seller filled circles uses learning learn optimal policy myopic player 
dashed line indicates baseline expected utility sellers myopic 
myopic vs cross plot myopic price curve vs derived price curve shopbot model fl 
seller learner seller myopic 
dashed line arrows indicate prices evolve time pricing policies starting particular price pair indicated filled circle 
plots myopic price curve seller derived price curve fl seller 
see curves maximum price minimum price approximately 
portion curves lying diagonal indicates undercutting behavior case seller respond opponent price undercutting ffl price discretization interval 
system dynamics state obtained alternately applying pricing policies 
done simple iterative graphical construction starting point holds constant moves horizontally curve holds constant moves vertically curve 
see iterative graphical construction leads cyclic price war trajectory indicated dashed line 
note price war behavior begins price pair persists price approximately 
point seller abandons price war resets price leading round undercutting 
amplitude price war diminished compared situation players myopic policy 
case seller curve mirror image seller curve price war persist way minimum price point leading lower expected utility sellers 
multi agent learning turn challenging situation simultaneous training functions policies sellers 
procedure studied alternately adjust random entry seller function followed random entry seller function formalism previous section 
seller function evolves seller pricing policy correspondingly updated optimizes agent current function 
modeling step payoff seller equation opponent current policy implied current function 
parameters experiments generally set values previous section 
experiments functions initialized instantaneous payoff values policies corresponded myopic policies initial conditions explored experiments 
simultaneous learning price quality model find robust convergence unique pair pricing policies independent value fl illustrated 
solution corresponds solution generalized minimax generalized dp tesauro kephart 
repeated application pair price curves leads dynamical trajectory eventually converges fixedpoint located 
detailed analysis pricing policies fixed point solution tesauro kephart 
brief sufficiently low prices seller pays seller abandon price war charge high price 
value corresponds highest price seller charge provoking undercut seller step lookahead calculation seller undercuts seller replies undercut 
fixed point differs nash equilibrium game calculated sairamesh kephart 
difference due factors models assume alternating turn dynamics simultaneous move dynamics assumed nash calculation ii game iterated game nash calculation assumes game 
conjectured tesauro kephart solution observed corresponds subgame perfect equilibrium fudenberg tirole nash equilibrium 
vs pq model cross plot optimal price curves seller vs seller price quality model obtained simultaneous learning solution values fl 
dashed line indicates prices evolve time pricing policies starting particular price pair indicated filled circle 
price war eliminated pricing curves dynamics evolves fixed point indicated open circle 
cumulative utilities obtained pair pricing policies plotted 
interesting seller lower quality seller obtains significantly higher profit seller higher quality seller 
contrast myopic vs myopic pricing seller worse seller 
shopbot model exact convergence values fl 
cases exact convergence approximate convergence functions policies converged stationary solutions small random fluctuations 
different vs pq model myopic vs myopic myopic vs myopic average profit plot expected utilities seller solid diamonds seller open diamonds price quality model obtained simultaneous learning 
comparison utilities seller seller myopic vs myopic pricing indicated dashed lines 
seller utility higher seller simultaneous qlearning solution seller lower quality parameter 
solutions obtained value fl 
small fl symmetric solution generally obtained shapes identical broken symmetry solution similar price quality solution obtained large fl 
range fl values symmetric asymmetric solution obtained depending initial conditions 
asymmetric solution counter intuitive expected symmetry sellers utility functions lead symmetric solution 
hindsight apply type reasoning price quality model explain asymmetric solution 
plots symmetric asymmetric solution obtained fl fl respectively shown figures 
plot expected utility sellers function fl shown 
vs cross plot optimal price curves seller vs seller shopbot model obtained simultaneous learning fl 
resulting price dynamics indicated dashed line arrows 
information filtering model simultaneous learning produced exact approximate convergence small values fl fl 
large values fl convergence obtained 
simultaneous learning solutions yielded reduced amplitude price wars increasing profitability sellers function fl fl 
data points examined fl convergence policies yielded greater utility sellers myopic vs myopic case 
plot derived policies system dynamics fl shown 
expected utilities players function fl plotted 
vs cross plot optimal price curves seller vs seller shopbot model obtained simultaneous learning fl 
resulting price dynamics indicated dashed line arrows 
vs shopbot model myopic vs myopic average profit plot expected utilities seller solid diamonds seller open diamonds shopbot model obtained simultaneous learning values fl ranging 
comparison utilities seller seller myopic vs myopic pricing indicated dashed line 
vs model cross plot optimal price curves seller vs seller information filtering model obtained simultaneous learning fl 
vs myopic vs myopic myopic vs myopic average profit plot expected utilities seller solid diamonds seller open diamonds model obtained simultaneous learning values fl ranging 
data points fl represent functions policies 
comparison utilities seller seller myopic vs myopic pricing indicated dashed lines 
learning neural networks lookup tables represent functions described previous sections feasible small scale problems 
situations faced software agents real world large scale complex tackle lookup tables sort function approximation scheme necessary 
section examines multi layer neural networks represent functions economic models studied previously 
initial results case single adaptive learning neural network agent training vs fixed strategy myopic agent described previously section 
neural networks studied multi layer perceptrons mlps back propagation 
learning equation previously quantity deltaq interpreted output error signal backprop style gradient calculation weight changes 
previous sections stateaction pairs chosen uniform random exploration preliminary evidence somewhat better policies obtained training actual trajectories 
experiments fixed learning rate constant ff time varying schedule ff described previously 
appears give significant speed increase cost slight degradation final network performance 
important issues neural networks design input state representation scheme 
schemes incorporate specialized knowledge domain better naive representation schemes 
knowledge included important seller know price greater equal seller price 
suggests coding scheme input units 
units represent seller prices real numbers remaining units binary units representing logical conditions 
experiments networks contained single linear output unit single hidden layer hidden units fully connected input layer 
shopbot model network ability approximate correct function generally poor worst accuracy obtained large values fl 
furthermore improvement approximation error training extremely slow continued decrease extremely slow rate long training continued 
typical training runs lasted tens thousands sweeps possible price pairs 
case fl extremely long training run sweeps performed approximation accuracy quite error decreasing slow rate 
difficulty obtaining accurate function approximation resulted inaccurate targets myopic vs shopbot model myopic vs myopic lookup nn average profit plot expected utility single learning agent training fixed myopic opponent shopbot model function fl 
filled circles represent neural network learner open circles represent lookup table learner 
data point represents best policy obtained training run 
comparison baseline myopic vs myopic expected utility indicated dashed line 
deltaq training due intrinsic limitations function approximator 
separate series experiments neural nets trained exact targets provided lookup tables yielded measurable advantage approximation accuracy suggesting problem due inaccurate heuristic teacher signals equation 
neural net training runs expected performance neural net policy periodically measured vs myopic opponent 
absolute error function improved monotonically policy expected profit reach peak relatively quickly usually sweeps longer large fl level decrease slightly 
plot peak expected utility training run function fl shown 
comparison expected utility exact optimal policy obtained lookup table learning plotted 
encouraging note absolute accuracy neural net function poor improves extremely slowly resulting policies give reasonably decent performance trained relatively quickly 
re emphasizes point successful applications neural nets reinforcement learning neural net approach give surprisingly strong policy absolute accuracy value function poor 
examined single agent multi agent learning models seller economy sellers alternately take turns setting prices instantaneous utilities sellers current price pair 
models fall category player alternating turn markov games rewards state space transitions deterministic 
game markov state space fully observable rewards history dependent 
models price quality shopbot large amplitude cyclic price wars obtained sellers optimize instantaneous utilities regard impact pricing policies 
models learning sellers myopic opponent invariably results exact convergence optimal function optimal policy opponent allowed values discount parameter fl 
derived policy yields greater expected utility learner monotonically increasing utility fl increases 
cases side benefit enhancing social welfare giving greater expected utility myopic opponent 
comes reducing amplitude undercutting price war regime cases eliminating completely 
interesting challenging situation simultaneously training functions sellers studied 
difficult seller function policy change provides nonstationary environment adaptation seller 
convergence proofs exist simultaneous qlearning multiple agents 
despite absence theoretical guarantees generally behavior algorithm 
models shopbot price quality exact approximate convergence obtained simultaneously self consistent functions optimal policies value fl information filtering model simultaneous convergence fl 
information filtering shopbot models monotonically increasing expected utilities sellers small values fl 
price quality model simultaneous learning yields asymmetric solution corresponding solution tesauro kephart highly advantageous lesser quality seller slightly disadvantageous higher quality seller compared myopic vs myopic pricing 
similar asymmetric solution shopbot model large fl utility functions players symmetric 
model exists range discount parameter values solutions obtained simultaneous learning self consistently optimal outperform solutions obtained tesauro kephart 
presumably previously published methods limited lookahead functions principle look ahead infinitely far appropriate discounting 
simultaneous learning works models despite lack theoretical convergence proofs 
sandholm crites simultaneous learning generally converged iterated prisoner dilemma game 
empirical findings suggest deeper theoretical analysis simultaneous learning may worth investigating 
may underlying theoretical principles explain simultaneous learning works certain classes arbitrary sum utility functions 
initial steps taken combining nonlinear function approximation neural nets learning approach 
single neural net learner facing myopic opponent exhibit reasonably pricing policies despite difficulties obtaining accurate approximation 
addition replacing lookup tables function approximators important challenges faced extending approach larger scale realistic simulations 
economic models quite deliberately ignored frictional effects agent search costs 
effects damp price wars lead different system behaviors partial equilibria support stable price differentiation 
eventually frictional effects considered argued prior studies models sairamesh kephart kephart hanson sairamesh greenwald kephart frictional effects web agent economies considerably smaller traditional human economies 
sellers concept sellers turns adjusting prices defined order problematic 
lead additional combinatorial explosion mechanism calculating expected reward anticipate possible orderings opponent responses 
furthermore economic models moderate degree realism utility functions unrealistic assumptions knowledge dynamics 
reported state space fully observable infinitely frequently zero cost zero propagation delays 
expected consumer response price pair instantaneous deterministic fully known players 
players exact utility functions fully known players 
assumed players alternately take turns equally defined order adjusting prices 
assumptions knowledge dynamics hope develop algorithm calculate advance game theoretic optimal pricing algorithm agent 
realistic agent economies agents full knowledge state economy 
agents may know details agents utility functions agent may know utility function extent buyer behavior unpredictable 
dynamics buyers sellers may complex random unpredictable assumed 
may information delays buyers sellers part economic game may involve paying cost order obtain information state economy faster frequently greater detail 
may expect buyer behavior non stationary complex evolution buyer seller strategies 
real world complexities daunting reasons believe learning approaches learning may play role practical solutions 
advantage learning need model instantaneous payoffs statespace transitions environment 
simply observe actual rewards transitions base learning 
theory learning requires exhaustive exploration state space guarantee convergence may necessary function approximators 
case training function approximator relatively small number observed states may generalize unobserved states give decent practical performance 
empirical studies provided evidence tesauro crites barto zhang dietterich 
author jeff kephart amy greenwald helpful discussions 
crites barto improving elevator performance reinforcement learning 
touretzky eds advances neural information processing systems mit press 
fudenberg tirole game theory 
cambridge ma mit press 
greenwald kephart 
appear proceedings ijcai international joint conferences artificial intelligence july august stockholm sweden 
hu wellman self fulfilling bias multiagent learning 
proceedings icmas aaai press 
hu wellman multiagent reinforcement learning theoretical framework algorithm 
proceedings icml 
kephart hanson sairamesh price war dynamics free market economy software agents 
proceedings alife vi los angeles 
kreps course microeconomic theory 
princeton univ press princeton nj 
littman markov games framework multi agent reinforcement learning proceedings eleventh international conference machine learning morgan kaufmann 
sairamesh kephart dynamics price quality differentiation information computational markets 
proceedings international conference information computation economics ice acm press 
sandholm crites multiagent learning semi competitive domain 
th international joint conference artificial intelligence ijcai workshop adaptation learning multiagent systems montreal canada 
tesauro temporal difference learning 
comm 
acm 
tesauro kephart foresight pricing algorithms economy software agents 
proceedings ice 
tesauro kephart foresight pricing algorithms agent economies 
decision support sciences appear 
vidal durfee learning nested agent models information economy experimental theoretical ai appear 
watkins learning delayed rewards 
ph 
thesis cambridge university 
watkins dayan learning 
machine learning 
zhang dietterich high performance job shop scheduling time delay td network 
touretzky eds advances neural information processing systems mit press 
