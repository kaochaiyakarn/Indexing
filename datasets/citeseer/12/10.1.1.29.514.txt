appear proceedings seventeenth international joint conference artificial intelligence ijcai 
foundations cost sensitive learning charles elkan department computer science engineering university california san diego la jolla california elkan cs ucsd edu revisits problem optimal learning decision making different misclassification errors incur different penalties 
characterize precisely intuitively cost matrix reasonable show avoid mistake defining cost matrix economically incoherent 
class case prove theorem shows change proportion negative examples training set order optimal cost sensitive classification decisions classifier learned standard non learning method 
argue changing balance negative positive training examples little effect classifiers produced standard bayesian decision tree learning methods 
accordingly recommended way applying methods domain differing misclassification costs learn classifier training set compute optimal decisions explicitly probability estimates classifier 
making decisions cost matrix specification costs correct incorrect predictions example predicted class leads lowest expected cost expectation computed conditional probability class example 
mathematically entry cost matrix cost predicting class true class prediction correct prediction incorrect 
optimal prediction example class minimizes jjx costs necessarily monetary 
cost waste time severity illness example 
sum alternative possibilities true class framework role learning algorithm produce classifier example estimate probability jjx class true class example making prediction means acting true class essence cost sensitive decision making optimal act class true class probable 
example rational approve large credit card transaction transaction legitimate 
cost matrix properties cost matrix structure classes actual negative actual positive predict negative predict positive papers followed convention cost matrix rows correspond alternative predicted classes columns correspond actual classes row column predicted actual 
notation cost false positive cost false negative conceptually cost labeling example incorrectly greater cost labeling correctly 
mathematically case call conditions reasonableness conditions 
suppose reasonableness condition violated case optimal policy label examples positive 
similarly optimal label examples negative 
leave case reasonableness conditions violated reader analyze 
pointed cost matrices class labels predicted optimal policy equation 
state simple intuitive criterion happens 
say row dominates row cost matrix 
case cost predicting greater cost predicting regardless true class optimal predict special case optimal prediction row dominated rows cost matrix 
reasonableness conditions class cost matrix imply row matrix dominates 
cost matrix decisions optimal unchanged entry matrix multiplied positive constant 
scaling corresponds changing unit account costs 
similarly decisions optimal unchanged constant added entry matrix 
shifting corresponds changing baseline away costs measured 
scaling shifting entries class cost matrix satisfies reasonableness conditions transformed simpler matrix leads decisions 
matrix perspective cost matrix effectively degrees freedom 
costs versus benefits research machine learning terminology costs doing accounting terms benefits generally preferable avoiding mistakes easier natural baseline measure benefits positive negative 
baseline state agent takes decision regarding example 
agent decision better benefit positive 
benefit negative 
thinking terms costs easy posit cost matrix logically contradictory entries matrix measured baseline 
example consider called german credit dataset published part statlog project michie cost matrix dataset follows actual bad actual predict bad predict examples people apply loan bank 
actual means customer loan actual bad means customer default 
action associated predict bad deny loan 
relative baseline associated prediction regardless actual actual bad true 
economically reasonable cost matrix domain entries predict bad row 
costs benefits measured baseline baseline fixed 
opportunity cost benefit missed opportunity actual penalty 
easy mistake measuring different opportunity costs different baselines 
example erroneous cost matrix justified informally follows cost approving customer zero cost rejecting bad customer zero cases correct decision 
customer rejected cost opportunity cost profit 
bad customer approved loan cost lost loan principal 
see concretely reasoning quotes incorrect suppose bank customer types 
clearly cost matrix intended imply net change assets bank 
alternatively suppose customers receive loans 
net change assets 
regardless baseline method accounting give difference scenarios 
erroneous cost matrix scenario gives total cost second scenario gives total cost 
general amount cells cost benefit matrix may constant may different different examples 
example consider credit card transactions domain 
benefit matrix fraudulent legitimate refuse approve size transaction dollars 
approving fraudulent transaction costs amount transaction bank liable expenses fraud 
refusing legitimate transaction non trivial cost customer 
refusing fraudulent transaction nontrivial benefit may prevent fraud lead arrest criminal 
research cost sensitive learning decision making costs may example dependent just zadrozny elkan making optimal decisions class case optimal prediction class expected cost prediction equal expected cost predicting class jx jx jx jx equivalent pc pc jx 
inequality fact equality predicting class optimal 
threshold making optimal decisions assuming reasonableness conditions optimal prediction class rearranging equation leads solution assuming denominator nonzero implied reasonableness conditions 
formula shows cost matrix essentially degree freedom decision making perspective degrees freedom matrix perspective 
cause apparent contradiction optimal decision making policy nonlinear function cost matrix 
achieving cost sensitivity rebalancing section turn question obtain classifier useful cost sensitive decision making 
standard learning algorithms designed yield classifiers maximize accuracy 
class case classifiers implicitly decisions probability threshold 
previous section need classifier example says jx target threshold general different 
standard learning algorithm produce classifier decisions general common method achieving objective rebalance training set learning algorithm change proportion positive negative training examples training set 
rebalancing common idea general formula correctly published 
theorem provides formula 
theorem target probability threshold correspond probability threshold number negative examples training set multiplied formula theorem simple proof correctness 
defer proof section 
special case threshold learning method theorem says number negative training examples multiplied special case breiman directionality theorem important understand 
suppose learning algorithm yields classifiers predictions probability threshold training set desired probability threshold theorem says create training set changing number negative training examples applied gives desired classifier 
theorem say way number negative examples changed 
learning algorithm weights training examples weight negative example set factor theorem 
oversampling undersampling 
oversampling means duplicating examples undersampling means deleting examples 
sampling done randomly deterministically 
deterministic sampling reduce variance risks introducing bias non random choice examples duplicate eliminate correlated property examples 
undersampling deterministic sense fraction examples value certain feature held constant called stratified sampling 
possible change number positive examples changing number negative examples 
domains class rare compared important keep available examples rare class 
cases call rare class positive class theorem says directly change number common examples discarding duplicating rare examples 
new probabilities new base rate section state prove theorem independent interest happens tool needed prove theorem 
new theorem answers question predicted class membership probability example change response change base rates 
suppose jx correct example drawn population base rate positive examples 
suppose fact drawn population base rate jx 
assumption shift base rate change population belongs 
formally assume positive negative subpopulations example probabilities unchanged xjj xjj xjj xjj 
assumptions theorem shows compute function theorem context just described pb pb bb proof bayes rule jx xjj xjj mutually exclusive xjj xjj xjj xjj cb cb similarly solve function pb pe pb pb denominator pb pb pb pb pb pb pb pb pb bb pb pb pb bb important note theorem statement true probabilities different base rates 
proof rely probabilities may estimated learning process 
particular proof assumptions independence conditional independence example naive bayesian classifier 
classifier yields estimated probabilities assume correct base rate theorem lets function 
compute estimated probabilities correct different base rate point view theorem remarkable aspect 
lets classifier learned training set drawn probability distribution test set drawn different probability distribution 
theorem relaxes fundamental assumptions research machine learning training test sets drawn population 
insight proof variable ratio xjj xjj 
try compute actual values probabilities find variables solve simultaneous equations 
fortunately need know particular example ratio special case theorem worked independently weiss provost case interesting 
suppose know base rate positive examples time learn classifier 
reasonable training set 
theorem says compute probabilities correct population test examples base rate specifically function plotted 
theorem lemma prove theorem slight change notation 
theorem target probability threshold correspond probability threshold number negative training examples multiplied proof want compute adjusted base rate classifier trained base rate estimated probability corresponds probability classifier trained base rate need compute adjusted function proof theorem pb bb pb 
collecting terms left pb pb gives adjusted base rate pb pb pb suppose number negative training examples multiplied get adjusted base rate pb bp pb pb note effective cardinality subset negative training examples changed way change distribution examples subset 
effects changing base rates changing training set prevalence positive negative examples common method making learning algorithm cost sensitive 
natural question effect change behavior standard learning algorithms 
separately researchers proposed duplicating discarding examples class examples rare assumption standard learning methods perform better prevalence different classes approximately equal kubat matwin japkowicz purpose section investigate assumption 
changing base rates bayesian learning example bayesian classifier applies bayes rule compute probability class jjx xjj typically xjj computed function learned training set estimated training set frequency class computed indirectly solving equation jjx 
bayesian learning method essentially learns model xjj class separately 
frequency class changed training set change estimated base rate class 
little reason expect accuracy decision making bayesian classifier higher particular base rates 
naive bayesian classifiers important special case bayesian classification 
naive bayesian classifier assumption class values attributes examples independent 
known classifiers tend give inaccurate probability estimates domingos pazzani example suppose naive bayesian classifier computes estimate jx 
usually extreme close jx close jx 
ranking examples naive bayesian classifiers tends correct jx jy 
fact suggests application optimal decision making uses probability threshold empirically determine different threshold equivalent jx procedure improve accuracy decision making changing proportion negative examples theorem order threshold 
decision tree growing turn attention standard decision tree learning methods phases 
phase tree grown top second phase nodes pruned tree 
discuss separately effect phase changing proportion negative positive training examples 
splitting criterion metric applied attribute measures homogeneous induced subsets training set partitioned values attribute 
consider discrete attribute values am 
class case standard splitting criteria form ja probabilities frequencies training set split function measures impurity heterogeneity subset training examples 
functions qualitatively similar unique maximum equal minima 
drummond holte shown attributes impurity function suggested kearns mansour invariant changes proportion different classes training data 
prove general result applies attributes shows related impurity functions including gini index breiman invariant base rate changes 
theorem suppose 
collection discrete valued attributes attribute minimizes regardless changes base rate training set general 
proof attribute definition ja ja am possible values bayes rule jj jj grouping factors gives jj jj base rate factors brought outside sum times sum jj jj constant attributes attribute minimum determined minimum 
depends jj jj depend base rates 
different different base rates jj jj attribute independent class jj jj sum maximum value independent desired sum smaller correlated splitting reasonable 
theorem implies changing proportion positive negative examples training set effect structure tree decision tree growing method uses impurity criterion 
algorithm uses different criterion entropy measure effect usually small impurity criteria similar 
experimental results drummond holte dietterich show criterion normally leads somewhat smaller unpruned decision trees leads accurate trees leads accurate trees 
recommend conclude regardless impurity criterion applying theorem influence growing phase decision tree learning 
decision tree pruning standard methods pruning decision trees highly sensitive prevalence different classes training examples 
classes rare prunes decision tree single node classifies examples members common class 
classifier useless decision making failing recognize example rare class expensive error 
papers examined issue obtain probability estimates decision trees bradford provost domingos zadrozny elkan clear necessary smoothing method adjust probability estimates leaf decision tree 
clear pruning methods best 
experiments bauer kohavi suggest pruning best decision tree probability smoothing 
bradford best pruning pruning call laplace pruning 
idea laplace pruning 
laplace smoothing training examples reach node positive estimate node jx 

compute expected loss node smoothed probability estimates cost matrix training set 

expected loss node sum expected losses children prune children 
show intuitively laplace pruning similar pruning 
absence probability smoothing expected loss node greater equal sum expected losses children 
equality holds optimal predicted class child optimal predicted class parent 
absence smoothing step change meaning decision tree classes predicted tree laplace pruning equivalent pruning 
probability smoothing expected loss node sum expected losses children difference caused smoothing smoothing presumably equality 
pruning children simplification leaves meaning tree unchanged 
note effect laplace smoothing small internal tree nodes nodes typically 
summary growing decision tree done cost insensitive way 
decision tree estimate probabilities preferable pruning 
costs example dependent decisions smoothed probability estimates equation 
costs fixed single defined cost matrix node unpruned decision tree labeled optimal predicted class leaf 
leaves certain node labeled class subtree node eliminated 
simplification tree smaller change predictions 
reviewed basic concepts optimal learning decision making different misclassification errors cause different losses 
class case shown rigorously increase decrease proportion negative examples training set order optimal cost sensitive classification decisions classifier learned standard non cost sensitive learning method 
investigated behavior bayesian decision tree learning methods concluded changing balance negative positive training examples little effect learned classifiers 
accordingly recommended way methods domain differing misclassification costs learn classifier training set equation equation directly smoothing probability estimates adjusting threshold equation empirically necessary 
bauer kohavi eric bauer ron kohavi 
empirical comparison voting classification algorithms bagging boosting variants 
machine learning 
bradford bradford kunz kohavi brunk brodley 
pruning decision trees misclassification costs 
proceedings european conference machine learning pages 
breiman breiman friedman olshen stone 
classification regression trees 
belmont california 
dietterich dietterich kearns mansour 
applying weak learning framework understand improve 
proceedings thirteenth international conference machine learning pages 
morgan kaufmann 
domingos pazzani pedro domingos michael pazzani 
independence conditions optimality simple bayesian classifier 
proceedings thirteenth international conference machine learning pages 
morgan kaufmann 
drummond holte chris drummond robert holte 
exploiting cost sensitivity decision tree splitting criteria 
proceedings seventeenth international conference machine learning pages 
japkowicz japkowicz 
class imbalance problem significance strategies 
proceedings international conference artificial intelligence las vegas june 
kearns mansour kearns mansour 
boosting ability top decision tree learning algorithms 
proceedings annual acm symposium theory computing pages 
acm press 
kubat matwin kubat matwin 
addressing curse imbalanced training sets sided sampling 
proceedings fourteenth international conference machine learning pages 
morgan kaufmann 

class probability estimates cost sensitive evaluation classifiers 
workshop notes workshop cost sensitive learning international conference machine learning june 
michie michie spiegelhalter taylor 
machine learning neural statistical classification 
ellis horwood 
provost domingos foster provost pedro domingos 
trained pets improving probability estimation trees 
technical report stern school business new york university 
weiss provost gary weiss foster provost 
effect class distribution classifier learning 
technical report ml tr department computer science rutgers university 
zadrozny elkan zadrozny charles elkan 
learning making decisions costs probabilities unknown 
technical report cs department computer science engineering university california san diego january 
zadrozny elkan zadrozny charles elkan 
obtaining calibrated probability estimates decision trees naive bayesian classifiers 
proceedings eighteenth international conference machine learning 
appear 
