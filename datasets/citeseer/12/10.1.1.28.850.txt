learning labeled unlabeled data matthias seeger institute adaptive neural computation university edinburgh forrest hill edinburgh eh ql seeger dai ed ac uk february hand aim give review literature dealing problem supervised learning aided additional unlabeled data :10.1.1.114.3629:10.1.1.117.3731
hand part author rst year phd report serves frame bundle related author numerous suggestions potential 
contains speculative partly subjective material reader expect literature review 
give rigorous de nition problem relate supervised unsupervised learning 
crucial role prior knowledge put forward discuss important notion input dependent regularization 
postulate number baseline methods algorithms algorithmic schemes straightforwardly applied problem need genuinely new concepts 
serve basis genuine method 
literature review try cover wide variety classify meaningful categories 
mention done related problems suggest ideas synthesis 
discuss caveats tradeo central importance problem 
contents contents supervised unsupervised learning :10.1.1.117.3731
supervised learning aided additional unlabeled data :10.1.1.117.3731
paradigms supervised classi cation :10.1.1.117.3731
sampling paradigm :10.1.1.117.3731
diagnostic paradigm :10.1.1.117.3731
regularization depending input distribution :10.1.1.117.3731
overview :10.1.1.117.3731
baseline methods unsupervised learning followed assignment clusters classes expectation maximization joint density model :10.1.1.117.3731
general view expectation maximization techniques expectation maximization additional separator variable expectation maximization diagnostic models :10.1.1.117.3731
literature review theoretical analyses early :10.1.1.117.3731
expectation maximization joint density model 
training algorithms :10.1.1.117.3731
adaptive regularization criteria 
fisher kernel 
restricted bayes optimal classi cation 
transduction 
subjective critique slt transductive inference :10.1.1.117.3731
related problems active learning :10.1.1.117.3731
coaching 
learning learn 
transfer knowledge learned related task 
contents caveats tradeo labels missing data :10.1.1.117.3731
diagnostic versus generative methods 
sampling assumption 
bibliography learning data seen rigorous attempt drastically compress data losing inherent information :10.1.1.117.3731
learning strategies belief hidden inherent simplicity relationships occam razor modern natural science 
statistical machine learning tries replicate highly original creative patterns human learning dull computers concepts probability theory 
key ecient compression latent variables associated observables knowledge latent variables reduces complexity describing observables drastically 
combined description latent observable variables costly straightforward description observables 
example invention words language describe objects visual experiences 
link variables order able inference coding decoding machinery need build models model family conditional probability distribution ajb disjoint sets variables may empty latent variable associated model family :10.1.1.117.3731
model families written sets distributions fp ajb elements conditional distributions ajb indexed values called models model families prior distributions variables top hierarchy see certain noise models case get rid variable important note model families inference machinery known coding decoding side ranges variables :10.1.1.117.3731
furthermore require machinery completely de ned sense total models allows compute joint prior distribution variables consistent way 
observing variable conditioned model families determine certain ordering direction data generation gives rise hierarchy 
notion hierarchy important bayesian analysis complex model families describing part hierarchy speci es joint prior distribution subset latent variables referred hierarchical prior 
berger gives bayesian analysis 
important basic mechanisms introducing latent variables achieve better compression 
principle divide conquer states models simple case nonparametric statistical methods :10.1.1.117.3731
give examples 
note treat way latent variable 
framework easier de ned model families simply conditional distributions general convention link model family explicit variable write family set indexed want break 
relationship described easily try separate nite number units accessible ecient description :10.1.1.117.3731
framework done introducing new grouping clustering variable nite range 
model ajb described mixture models kg involves modeling 
second mechanism works imposing functional relationships variables obscured completely unstructured noise 
describe relationship ajb build model family fp ajb model computes xed mapping ajb noise aj 
conditional distribution right side called noise model 
models family simple parametric form gaussian 
note data compressed model family prefer models true distribution aj unstructured possible close noise aj 
models prefer compression aimed separating structure noise central goal learning data 
supervised unsupervised learning statistical machine learning di erent scenarios easily distinguished supervised learning learning teacher unsupervised learning supervised learning scenario aspects unknown probabilistic relationship examples input points targets labels learned labeled data ji ng drawn independently :10.1.1.117.3731
problem class includes pattern recognition classi cation nite regression estimation 
exclusively deal classi cation scenario ideas carry scenarios regression estimation 
respect coding perspective say classi cation case somebody humans earlier generations clever scientists done job identifying grouping variable potentially valuable ecient compression 
world agreed trust decision latent anymore observed 
surely possess prior knowledge relationship data inference prior knowledge come source 
readers object noting simple classi cation learning schemes shown consistent learn relation unlimited data bother compression ideas models altogether 
learning limited data course ill posed problem kind prior knowledge relationship observed data contain information generalize unseen data :10.1.1.117.3731
classi cation schemes grouped major classes see diagnostic sampling paradigm :10.1.1.51.9998
methods diagnostic paradigm referred diagnostic methods discriminative methods schemes sampling paradigm called generative methods designing generative method grouping variable compression sense assume class distributions xjt described eciently 
assumption leads proposing model family class distributions 
diagnostic methods related regression estimation assume tjx described eciently model family impose noisy functional relationship noise model multinomial 
imposed functional relationships simple logistic regression model family parameterized complex way 
example kernel methods model family parameterized latent function mapping represents random processes 
schemes regression estimation usually proceed way diagnostic classi cation methods model tjx 
traditionally diagnostic schemes parameter model family fp tjx input variable priori independent 
leads schemes necessary learn marginal data 
coding terms schemes need describe input points data eciently 
easily modify coding perspective cases allowing input points dataset compressed sent free 
important point see subsections kind independence assumption sensible want learn additional unlabeled data :10.1.1.117.3731
soon drop assumption see ecient description input points issue :10.1.1.117.3731
supervised learning usually follows de ned goal minimizing generalization error classi cation minimizing expected loss regression estimation de nitive criteria unsupervised learning scenarios required nd interesting structures sample fx ji mg independently drawn unknown distribution called source :10.1.1.117.3731
occam razor really looking structures inherently simple obscured random noise 
problem harder term sampling paradigm historical reasons clashes unfortunate way terminology methods employing monte carlo sampling referred sampling methods 
general criterion mentioned 
best solution unsupervised learning problem enables encode data source ecient way 
criterion lead successfully design model family model selection search model family strong absolute criterion practice feasible desirable focus particular aspects attacking problem optimal representation 
supervised learning scenario requires algorithm designer thereof identify latent variables suitable ecient compression source :10.1.1.117.3731
essence unsupervised method performs density estimation successful unsupervised algorithms formulated generative models complexity carefully controlled regularized tted data 
choosing appropriate restrictive model families aim lower targets learning particular kinds structure data intention representing faithfully nal result basic drive optimization data best possible way 
examples unsupervised techniques include latent subspace models principal component analysis pca factor analysis principal curves :10.1.1.32.4821:10.1.1.117.3731
introduce latent compression variable living low dimensional space furthermore impose noisy functional relationship xju 
functional relationships represented linear powerful nonlinear models case model family tightly regularized appropriate prior model parameter 
noise model usually gaussian 
examples mixture models latent variable grouping variable nite set similar class label supervised classi cation conditional models come simple families gaussians structurally restricted covariance matrices 
combinations mixture latent subspace models considered numerous variants :10.1.1.32.4821
note models complexity regulated various levels 
relations latent observable variables kept simple choosing relatively narrow model families regularizing models penalizing complex models family 
naturally achieved placing prior distribution parameter model family judges simple models probable complex ones 
complexity latent variables needs tightly controlled number components mixture number dimensions latent subspaces 
considerable interplay levels hierarchy 
example split heuristics mixture density estimation components split densities grow wide unusually elongated components merged densities overlap strongly 
example prior distributions model families components favour concentrated strongly elongated distributions prior number components favours small numbers 
supervised learning aided additional unlabeled data problems belong principal classes discussed previous subsection immense practical impor tance :10.1.1.117.3731
example face supervised classi cation problem relationship easy obtain large sample unlabeled data process labeling points drawn tjx expensive computationally hard dicult perform reasons 
example labeling require human insight speech recognition object recognition images classifying hypertext pages performance expensive tests experiments medical diagnosis functional proteomics short practical interest methods attack problem supervised learning aided additional unlabeled data short labeled unlabeled problem de ned considerable 
unknown probabilistic relationship input points class labels cg problem predict nd predictor generalization error px small ideally close bayes error minimum generalization errors predictors :10.1.1.117.3731
looking algorithms compute labeled sample ji ng drawn independently unlabeled sample fx ji mg drawn independently marginal input distribution :10.1.1.117.3731
sampled independently prior knowledge assumptions unknown relationship 
empty traditional supervised learning problem 
interesting case practical viewpoint arises jd small jd de ne additional notation 
de ne xn :10.1.1.117.3731
xn xn furthermore denote missing labels points :10.1.1.117.3731
combined evidence complete observed data 
availability prior knowledge relationship form occam razor crucial argued subsection :10.1.1.117.3731
important note prior knowledge assumptions quite di erent degree di erent nal impact compare supervised unsupervised tries deduce function certain proteins cell 
moment ultimate dicult expensive method grow crystal determine dimensional protein structure ray crystallography 
features expression level protein certain type cell certain conditions linear amino acid sequence ow characteristics gel determined cheaper large scale fully 
learning :10.1.1.117.3731
supervised learning prior knowledge merely kind security belt prevent algorithm run ahead fancy model bits pieces dataset 
limit large labeled dataset belt looser looser impact nal prediction vanishes 
unsupervised learning prior assumptions strong impact nal result 
priori interesting structure data kind structure discover data depends view examples features describe distance relate 
having observation algorithm attack general problem supervised learning additional unlabeled data crucial balance impact prior assumptions carefully extremes 
paradigms supervised classi cation basic paradigms supervised classi cation introduced subsection :10.1.1.117.3731
discuss detail describe role unlabeled data plays 
relative merits typical methods paradigm especially possible extensions methods solve labeled unlabeled problem discussed subsection 
sampling paradigm refer architectures sampling paradigm generative methods :10.1.1.117.3731
model class distributions xjt model families fp xjt furthermore class priors tj refer architecture type joint density model architecture modeling full joint density xjt 
xed estimate tjx computed bayes formula tjx xjt xjt alternatively obtain bayesian predictive distribution tjx averaging tjx posterior jd :10.1.1.117.3731
sampling paradigm model marginal emerges naturally xj xjt labeled unlabeled data available natural criterion maximize joint log likelihood log jt log jt alternatively posterior jd :10.1.1.117.3731
essentially issue maximum likelihood presence missing data treating latent variable principle attacked expectation maximization em algorithm see subsection 
implicit representation appealing generative methods exhibit signi cant drawbacks supervised learning tasks expect expressed add unlabeled data learning process described 
discuss issues subsection 
furthermore situations joint density model families appropriate em exhibit severe local maxima problems discussed subsection 
diagnostic paradigm diagnostic methods model conditional distribution tjx directly family fp tjx discussed subsection :10.1.1.117.3731
arrive complete sampling model data model family xj interested updating belief predicting unseen points necessary see 
model priori independent 
likelihood factors jx implies jd jx jd jd posteriori independent 
furthermore jd jd 
means knowledge unlabeled data knowledge changes posterior belief jd labeled sample 
standard data generation model diagnostic methods unlabeled data bayesian inference modelling input distribution necessary 
advantages drawbacks diagnostic methods compared generative ones see subsection discussed subsection :10.1.1.117.3731
order unlabeled data diagnostic methods data generation model discussed modi ed topic subsection 
predict average tjx posterior 
know drawn independent employ posterior jd du 
case test set usually forms part du posteriors 
regularization depending input distribution seen subsection traditional diagnostic methods classi cation additional unlabeled data principal reason model tjx model priori independent :10.1.1.117.3731
words model family fp tjx regularized independently input distribution 
allow prior dependencies shown independence diagram right situation changes 
conditional prior principle allows information transferred 
general dependent labeled data unlabeled data change posterior belief 
conclude additional unlabeled data context diagnostic supervised techniques allow priori dependence latent function representing conditional probability input probability words regularization latent function depends input distribution 
argument explored detail :10.1.1.117.3731
modi cation standard data generation model diagnostic methods suggested straightforward principle choosing appropriate conditional priors hand represent available prior knowledge appropriate way hand render inference machinery tractable approximative sense challenging 
important example training paradigm see subsection 
idea exploiting redundancies views examples regularize hypothesis class information 
idea originates earlier unsupervised learning seen quite general way construct conditional priors task hand reasonably general formulation construction process knowledge 
ideas direction :10.1.1.117.3731
readers feel bit uneasy point 
priori dependent nal predictive distribution depends prior input distribution 
forces model input distribution strong contrast situation traditional diagnostic methods see subsection :10.1.1.117.3731
case method diagnostic 
diagnostic methods clear advantage generative techniques require orders magnitude free parameters adjusted learning data see subsection 
model individual class distribution marginal 
furthermore discussed detail enforce prior assumptions way restrictive introduce bias loose prior expected sucient impact nal prediction :10.1.1.117.3731
input dependent regularization done sensibly see discussion impact model nal prediction severe typical methods belonging sampling paradigm :10.1.1.117.3731
generally assumed unlabeled data abundant theory need restrict simple models 
conclude true diagnostic techniques input dependent regularization share need density modeling generative methods opinion classi ed belonging diagnostic paradigm theoretical studies supervised learning methods probably approximately correct pac framework focus diagnostic schemes consequently ignore input distribution restrict assume uniform question unlabeled examples augment labeled data slippery point view standard pac assumptions citation blum mitchell :10.1.1.114.9164:10.1.1.117.3731
pac bounds analyze deviations training generalization error certain predictors drawn hypothesis set limited complexity 
complexity measures hypothesis sets vapnik chervonenkis vc dimension see usually depend input distribution 
words pac result applies uniformly distribution nice merely bounds probability drawing sample size measuring deviation training error sample generalization error hypotheses restricted class 
hypotheses class unacceptably high training error training sample thing hypothesis class larger complex leading worse large deviation bound 
practical real world applications samples astronomical size best known pac bounds generalization error usually far tight 
principle stops considering pac bounds hold uniformly 
bounds hold value depend characteristics 
bounds interpreted sense :10.1.1.51.9998
bounds tighter uniform ones cases strongly violate prior assumptions 
bounds possibly motivate regularization depending input distribution 
principled frameworks bayesian analysis conditional priors attack labeled unlabeled problem 
discussed review section 
example subsection discusses restricted bayes optimal classi cation relates directly regularization :10.1.1.117.3731
way restricted bayes optimal classi cation considered diagnostic technique see subsection 
baseline methods overview conclude section giving overview remainder :10.1.1.117.3731
essential aim give review literature labeled unlabeled problem parts emphasize re ect author subjective beliefs important original versus criticized 
happy get discussion readers validate views arguments 
done best report argue fair 
claim exhaustive especially respect 
castelli cover collect older :10.1.1.117.3731
important topics outsourced separate papers issue input dependent regularization conditional density models see subsection :10.1.1.117.3731
subject current generalization fisher kernels mentioned brie subsection 
emphasize somewhat biased mentioning done author 
giving comprehensive literature review aim build frame put context basis ort give overview rst year author phd period 
section identify baseline methods problem supervised learning aided additional unlabeled data labeled unlabeled problem 
methods attack problem generic way straightforward transformations existing standard methods supervised unsupervised learning new problem domain 
method claiming solve labeled unlabeled problem ideally compared 
section contains literature review forms main part 
section describes problems think related labeled unlabeled problem selected done problems 
section discuss caveats tradeo linked labeled unlabeled problem 
section presents 
baseline methods section de ne baseline methods principle attack realization labeled unlabeled problem de ned subsection :10.1.1.117.3731
criteria select methods rigid :10.1.1.117.3731
method generic applicable special task 

method relatively straightforward transformation existing standard techniques supervised unsupervised learning 
baseline methods opinion baseline methods straightforward variants thereof considered solutions labeled unlabeled problem certain tasks :10.1.1.117.3731
severe shortcomings discussed opinion addressed probably genuinely new ideas 
baseline methods useful compare genuinely new techniques 
simplest baseline method course discard unlabeled data predict labeled training data available prior knowledge favourite supervised algorithm 
authors importance baseline method 
example jd small supervised methods naturally perform poorly 
labeled unlabeled algorithm outperforms baseline method signi cantly necessarily conclude labeled unlabeled algorithm suitable solve problem problem sensibly attacked sparse data 
possibly subtle issue cross validation set free parameters supervised learning methods 
cross validation small training sets doomed fail due high variance 
opinion sensible way predict small training sets bayesian inference kind available prior information 
certain arti cially created tasks compare labeled unlabeled method non plus ultra method supervised algorithm missing labels course baseline method quite useful case study set limits 
expecting method comes close ideal small naive 
aim supervised method data information closer unsupervised setting 
readers object presenting suggesting methods having tested data 
methods proposed literature special tasks 
clear review section 
second concrete baseline algorithms suggest methods schemes 
concrete realization left want comparative studies 
dicult get working de nition mean small model prior assumptions correct de ne small dataset size bayesian analysis assumptions performs average signi cantly worse optimal bayes classi er achieves bayes error 
practice necessary study learning curves supervised baseline method algorithm test errors averaged trials plotted relative size percentage labeled data 
baseline methods unsupervised learning followed assignment clusters classes assume labeled data sparse unlabeled data abundant :10.1.1.117.3731
case sophisticated unsupervised algorithm generate model posterior distribution models ts unlabeled data 
design unsupervised method course depends prior knowledge 
example assume validity cluster assumption points label path passes regions relatively high 
tipping rattray proposed ways construct sophisticated distance measures aimed nding clusters 
rst gaussian mixture model unlabeled data number mixture components larger number classes jt 
tted model distance points computed 
simple algorithm nearest neighbor see labeled data distance inferred note cluster assumption general weak assumption applicable prior assumption unsupervised tasks 
prior knowledge stronger nature available possible simpler distance measures important distance learned unlabeled data case prior knowledge task allow assume data class faithfully modeled coming underlying low dimensional manifold generally mixture manifolds convolved gaussian noise generative topographic mapping gtm powerful architecture situations obtaining latent manifold smooth nonlinear mapping uniform distribution low dimensional space represented regular grid :10.1.1.130.110:10.1.1.117.3731
try mixture gtm small number components jt keeping component manifolds smooth occam priors easier method constructed imposing existence latent separator variable conditionally independent lives kg typically jt rst mixture model relationship variable selecting component :10.1.1.117.3731
component models xjk train maximizing likelihood labeled data technique discussed subsection giving rise baseline method 
spherical gaussian noise reasonable model gaussians principal axes aligned tangent space manifold center :10.1.1.117.3731
alternatives discussed :10.1.1.130.110:10.1.1.117.3731
tting mixtures gtm proved dicult practice chris williams pointing :10.1.1.117.3731
baseline methods conceptually simple method cluster assumption surprisingly real world problems :10.1.1.117.3731
restricted case computation distances mentioned computationally quite heavy 
fundamental aspect respect labeled unlabeled problem cluster assumption usually true region interest 
labeled dataset small point critical places suggest splitting cluster crossed low density regions 
ideally nal distance depend information 
example start distance carefully inject label information modifying distance places label evidence suggests leaving unchanged 
aware principled done direction 
technique separator variable straightforward run 
expect general standard gaussian mixture models 
strength mixture models density estimation simple component densities connect model quite complicated elongated connected high density regions 
notion connectedness supported method 
general connected high density regions labeled consistently correct labeled data falls components modeling region 
improbable small 
hypothetical mixture gtm method alleviate problem signi cantly centers belonging cluster assigned component gtm constrained lie smooth lowdimensional manifold 
unfortunately inference computationally quite expensive gtm growing exponentially number dimensions latent manifold 
simple extensions gtm possibly employing elaborate noise models powerful component models mixture approach component restricted small latent dimensionality 
nally mention general idea injecting label information having learned probabilistic partitioning kjx idea line method suggested subsection works tting simple local experts tjx logistic regression data di erent clusters parameter vector 
expert trained reweighted version point weighted kjx 
example suppose tjx :10.1.1.117.3731
exp logistic function :10.1.1.117.3731
:10.1.1.117.3731
fitting logistic regression models maximize likelihood weighted data done iteratively reweighted squares irls technique see 
case data sparse employ occam prior 

map approximation full bayesian analysis irls method compute :10.1.1.117.3731
map approximation bayesian analysis brie discussed context subsection :10.1.1.117.3731
details :10.1.1.31.4284
baseline methods advantage method labeling clusters assuming acts separator discussed subsection clusters split classes labeled data suggests :10.1.1.117.3731
note points signi cant weight cluster belong class occam prior map model tjx constant 
expectation maximization joint density model treating genuinely unlabeled data view labels points missing data 
expectation maximization em see general technique maximum likelihood estimation presence latent variables missing data 
idea basic batch version em simple 
distinguish complete likelihood function observed unobserved data marginal likelihood function obtained complete integrating latent variables 
goal maximize marginal likelihood 
done iterating steps 
called step compute conditional distribution latent variables observed data current model estimate 
step compute expectation complete log likelihood function conditional distribution choose new model maximizes criterion 
speci observed hidden variables 
jensen inequality applied concave log log log dz zh zh log distribution :10.1.1.117.3731
xed current model estimate choose step order achieve tightest possible bound 
em seen successive maximization varying lower bounds marginal log likelihood 
crucial fact em criterion lower bound marginal log likelihood equal rst order expanded show local maximum point bound maximizes log locally 
springs mind construct model family joint distribution determine model maximizing joint likelihood em algorithm order attack labeled unlabeled problem 
easily done choosing model families class conditional distributions xjt 
joint log likelihood log jt log jt tj :10.1.1.117.3731
derivation em equations parallels closely case mixture models textbooks :10.1.1.117.3731
baseline methods em ll labels suggested early note little discussion :10.1.1.117.3731
chapter gives idea clear authors suggest approach classi cation merely partially unsupervised learning unsupervised tting mixture model aided labeled points attack labeled unlabeled problem text classi cation :10.1.1.117.3731
usage em context somewhat dangerous argue 
required model class conditional distributions 
terminology subsection operate sampling paradigm robust diagnostic paradigm :10.1.1.117.3731
frequently observed purely supervised setting tting class conditional distributions poor models estimating tjx models bayes formula works surprisingly prediction poor estimate tjx estimates points low entropy prediction con dent points 
suppose choose quite narrow families including simple models classes 
initialize em tting models labeled data rst step missing labels lled expected values current model observed data 
dent nature estimates poor class models pseudo labels quite de nitive points words rst step assign large number points classes quite con dently initial poor model tted subsequent step arti cially labeled points outweigh labeled points leading model exhibit worse predictive performance initial 
case expect em applied way quickly converge poor local maximum joint likelihood largely determined pseudo labels points rst step 
problem alleviated allowing complex class density models 
case clear models initially small 
typical situation straightforward em fails empirically shown 
general view expectation maximization techniques subsection contains advanced material required general understanding remainder :10.1.1.117.3731
think view em techniques useful anybody applies standard em algorithm variants learning problems reader invited jump paragraphs subsection state consequences view relevant 
versions subsection probably outsourced separate comprehensive technical report 
standard batch em algorithm applied straightforwardly labeled unlabeled problem su ers severe robustness problems mentioned baseline methods notions em better suited attack problem :10.1.1.117.3731
key adopt general view em procedures allows modify standard algorithm variety ways losing convergence guarantees 
em special case alternating minimization procedure context information geometry see observed authors :10.1.1.117.3731
important problems information theory computation capacity discrete memoryless channel rate distortion function shown equivalent problem see convex sets distributions minimum divergence qkp minimum distance attained :10.1.1.117.3731
divergence relative entropy kullback leibler divergence eq log useful divergence measure probability distributions clear information theoretic interpretation see strong somewhat deep motivations information geometry :10.1.1.117.3731
convex arguments solution problem unique simple alternating minimization procedure guaranteed nd start alternate steps argmin qkp steps argmin qkp 
minimization procedures outcomes context step called projection projection 
steps iterated improvement qkp observed 
em seen variant algorithm shown 
context family models contains distributions related empirical distribution data determined observed sample 
unfortunately nontrivial applications em turns convex set global solutions algorithm general converge :10.1.1.117.3731
sucient smoothness conditions algorithm nds local solution pair minimizes qkp environment convex manifold distributions visible hidden :10.1.1.117.3731
de ne model submanifold manifold em cases direct product convex sets :10.1.1.117.3731
em regarded alternating minimization procedure convex sets 
aside proposed information bottleneck learning algorithm regarded procedure convex sets theoretical basis em algorithm see 
discuss conditions detail usually ful lled practice :10.1.1.117.3731
able talk smoothness environments rst impose manifold structure context em usually done de ning model family parameterized submanifold thereof :10.1.1.117.3731
shall geometrical properties :10.1.1.117.3731
baseline methods parameterized em algorithm described subsection iterative procedure sample nd local maximum marginal likelihood function :10.1.1.114.3629
dz de ne universal data manifold qu contain marginal dz equal marginal empirical point distribution qu clearly convex :10.1.1.117.3731
seen contain possible beliefs complete data having observed show equivalence em alternating minimization procedure discussed section rst look step 
denote current model looking projection qu minimize 

write de nition data manifold qu easy see log dz log dz constant independent nonnegativity relative entropy minimizer posterior distribution employed em step 
furthermore choice 
log zh zv log right hand side equation usual em criterion maximized step depend 
see em performs projection step 
things note wants new view 
quality nal solution algorithm presents convergence speed depends initial choice model natural idea employ sequence em algorithms having di erent model submanifolds solution computed algorithm initialization sequence 
em algorithm iterative anyway chaining change character 
course sure model family really stands sequence 
advantages principle realized employing suitably chosen sequence monolithic em run model submanifold case em converges satisfactory solution initialized carefully gets stuck poor local solutions 
initialization requires expensive distribution concentrates mass point zv baseline methods search :10.1.1.117.3731
common practice start algorithm lot times di erent randomly chosen initial points 
employing cleverly designed sequence see discussion search done principled ecient 
case em takes unacceptable long time convergence initialized carefully :10.1.1.117.3731
suitably chosen sequence individually quickly converging em runs expected nd initialization 
idea generalized obtain annealed versions em 
second point note necessarily perform complete projections minimizations steps 
equivalently restrict search projections subsets qu respectively 
order get stuck spurious extrema require true reduction qkp achieved step 
means course order assess nal convergence face optimizations full qu especially early stages run get away cheaply 
restrictions course ect quality nal solution number iterations needed convergence cases bene ts drawbacks far 
having argued general terms far show wellknown extensions em arise naturally view 
motivate eventually em better labeled unlabeled problem 
called generalized em variant obtained allowing partial complete minimization steps 
generalized em runs faster due simpler searches steps 
general requires iterations convergence standard em 
variational variant em systematically uses partial minimization steps 
variant useful cases standard em computationally infeasible 
replaces full data manifold qu parameterized convex submanifold chosen minimization 
kp computation criterion optimized step feasible 
note usually contain posterior distributions standard em employs steps variational variant general convergence local maximum marginal likelihood 
reasonably broad submanifold qu nal solution high quality 
example tting gaussian mixtures em initialized rst tting mixture restricted model family family mixtures gaussians identity covariance matrix running means procedure limit case :10.1.1.117.3731
baseline methods sequential versions em step compute posterior distributions subset latent variables posteriors computed earlier steps remaining ones 
view corresponds partial step minimization follows 
current data distribution 
current model 
posteriors computed :10.1.1.117.3731
assume choice model family independent posterior :10.1.1.117.3731
restrict search new data distribution distributions form restriction projection sequential em variants :10.1.1.117.3731
note freedom choose model submanifold universal data manifold qu xed de nition observed data 
tasks useful sensibly restrict qu derive variational variants em discussed 
de ne data submanifold em algorithm submanifold data manifold qu observed data em algorithm formally de ned pair 
de nition allow nonconvex incomplete sense contain potential posteriors 
general convergence guarantee em algorithm holds convex complete 
general em algorithm su ers basic problems 
rst gets stuck shallow local optima nding high quality solutions reasonably high marginal likelihood 
due fact model submanifold usually convex 
second models involving structural choices connectivity network step optimizations intractably hard 
problems especially severe model family exhibits symmetries parameterization di erent levels 
problems principle addressed carefully choosing initial model models dicult nding data rst place 
standard technique attack problems simulated annealing :10.1.1.123.7607
context em basic idea run sequence em algorithms data having model data submanifold 
convergence algorithm solution initialize 
art choose sequence order achieve somewhat continuous transition early stages hardly shallow local optima easy explore large parts model family steps late stages model data manifolds close ones aiming 
successive solutions annealing done carefully better better suited initial models guarantee nal hard em run nd reasonably deep optimum 
example suggested combine standard em deterministic annealing alleviate local optima problem :10.1.1.51.9998:10.1.1.117.3731
framework seen running sequence em algorithms sharing model submanifold employing di erent data submanifolds obtained constraining elements qu particular way 
call step annealing 
model family baseline methods involves structural choices step annealing see involves running sequence em algorithms sharing data manifold qu employing di erent model submanifolds constructed process controlled randomization :10.1.1.51.9998
variant discussed called robust em aimed alleviating robustness problems standard em problem mentioned subsection 
problems identi ed 
need train models class distribution separately 
class distribution complicated broad model class 
complicated models reliably sparse labeled data second early pseudo labeling major parts unlabeled points poor models trained devastating ect nal prediction 
robust variant em start tting simple class models separate sets active inactive 
initially 
sweep extract points con dently labeled classes current model place active set run em data latent variables labels points data looking complex models consider tting reasonable broaden model family slowly injecting points formally incorporated view em follows consists variables labels corresponding input points robust em consists running sequence em algorithms having model submanifold 
example certain stage divided divide accordingly furthermore 
model family general structure :10.1.1.117.3731
denotes uniform distribution 
model really model models remaining variables uninformative uniform distribution 
assumption means associated latent label :10.1.1.117.3731
specify need specify tj 
speci cation broader broader model classes increasing size easy see posterior step criterion maximized step additive constant log 
baseline methods components representing data simply ignored 
variant robust em suggested authors call self training 
conclude new view em allows modify standard version numerous ways unexplored 
context labeled unlabeled problem robust variant em discussed previous paragraphs alleviate shortcomings standard method 
step annealing deterministic annealing useful context 
facing problem posterior distribution latent variables labels corresponding points low entropy components early stages em called dent pseudo labeling step annealing heat posteriors early iterations 
talked speci cally case models incorporate latent variables encoding structural choices 
cases step annealing helpful 
expectation maximization additional separator variable subsection discussed em algorithm joint models attack labeled unlabeled problem 
involves modeling di erent classes separately class conditional models xjt case marginal model xjt see notations 
class conditional models simple poor model family marginal distribution 
idea mentioned subsection introduce latent separator variable model separates sense conditionally independent means model information contains class captured fact illustrated independence model right :10.1.1.117.3731
modeling assumptions joint log likelihood log jk log jk kj :10.1.1.117.3731
straightforward compute em equations model see 
note case need treat labels points latent variables 
miller uyar results model gaussian components xjk 
centers class case seen context 
straightforward weaken assumption separator variable 
lead architecture estimates tjx baseline methods logistic regression take role typically sparseness labeled data sucient train local predictors 
problem possibly alleviated regarding label latent variable applying em terminology section data distribution kept de nitive clamped observed labels :10.1.1.117.3731
resulting predictor tjx kjx kjx xjk similar mixture experts architecture see :10.1.1.136.9119:10.1.1.136.9119
gating models kjx diagnostic generative architecture trained maximize conditional likelihood data joint 
relationship architectures discussed subsection 
separator variable cases outperform straightforward em see subsection comparison xjk xjt come model family respectively simply typically ranges values class label example model family solely contains unimodal distributions version employing principle model multimodal class distributions 
mentioned subsection clear advantage really substantial :10.1.1.117.3731
example range large set values case able identify marginal distribution exactly large 
model encode prior force connect strongly overlapping components cluster assumption mentioned subsection relate components priori way lot labeled data needed associate large number components classes :10.1.1.117.3731
able real unlabeled data supervised tasks able identify marginal required identify certain degree connected components class distributions 
relatively easy large real challenge solved employing prior knowledge nature class distributions 
able identify class distributions hope realize exponential value labeled sample see discussed subsection advantage :10.1.1.117.3731
expectation maximization diagnostic models method discussed subsection di ers aspects methods section 
sure applied reasonably generically 
applied successfully special task giving rise called training paradigm discussed 
second method describe algorithmic scheme algorithm 
scheme applied special task prior knowledge gathered encoded way algorithmic scheme realized exactly approximately feasible algorithm 
need genuinely new ideas particular instances algorithmic scheme training considered baseline algorithms 
baseline methods discussed probabilistic diagnostic methods model tjx directly way class conditional density models :10.1.1.117.3731
traditionally regularization model class fp tjx done independently input distribution case unlabeled data contain additional information latent see subsection :10.1.1.117.3731
setting modeling labels points latent variables sense 
introducing new point game lead new constraints belief information ow 
applying em incorporate latent label new point means rst predict label old belief independent new point 
update belief prediction 
update uses information know new point introduced belief sharper data driven way 
situation change regularization model class depends input distribution discussed subsection :10.1.1.117.3731
clarify claim give example employing terminology subsection :10.1.1.117.3731
model family fp xj occam prior parameters 
input dependent regularization means employ conditional priors case 
exact bayesian solution predictive distribution tjx approximated maximum posteriori map solution tjx map parameters maximize posterior equivalently joint distribution 
recalling notation de ned subsection jx tu jx jx xj exp tu log jx jx xj contrast situation priori independent proposing latent labels running em compute map solution sense case :10.1.1.117.3731
reader object noting simply marginalize joint really reason employ em optimizing lower bounds marginal elaborate sequences alternating steps marginal likelihood maximized directly :10.1.1.117.3731
answer point case tting mixture models 
tting mixture model compute marginal likelihood gradient thereof essentially easily computing statistics needed step em 
standard optimizer maximize marginal likelihood directly 
baseline methods cases optimization em works eciently conceptually simpler 
facts true case diagnostic models regularization 
unlabeled points constrain belief certain way model contain information transferring information easier em way latent labels encodes subtle contraints models tjx enforcing constraints directly optimization force achieve high likelihood small set hard 
robust em variants see subsection candidates attack problem :10.1.1.117.3731
example observe new unlabeled point xn injected case enlarge :10.1.1.117.3731
step latent labels predicted distribution jx represent current map models having seen new point 
xn injected step updated maximize standard map model class fp xj update maximize tu log jx jx :10.1.1.117.3731
involves tting diagnostic model map dataset partly uncertain uncertainty represented data distribution see subsection exist standard methods task example iteratively reweighted squares irls technique see :10.1.1.117.3731
possible extend method variational techniques approximate posterior beliefs 
scheme discussed :10.1.1.117.3731
exploring concrete data model families gaussian process classi cation remains topic research 
reader noticed quite cautious formulation algorithmic scheme 
em diagnostic models quite slippery certain conditions input dependent regularization represented conditional priors successful 
argued priori independent 
hand works extremely certain special tasks strong structural prior knowledge relationship input distribution discriminant function available 
case training method proposed attack problem web page classi cation see subsection :10.1.1.114.9164:10.1.1.117.3731
basic training algorithm seen robust variant em diagnostic models assumptions proposed encoded conditional priors :10.1.1.114.9164:10.1.1.117.3731
detailed derivation discussion view training :10.1.1.117.3731
view model algorithm introduced section seen generalization training 
talk predicting latent variables distribution really mean update data distribution variables coincide see subsection :10.1.1.117.3731
step compute em criterion expectation complete log joint distribution simple important cases resulting em criterion looks log joint distribution complete dataset containing values latent variables values latent variables expectations terminology predicting latent variables understood sense 
literature review nally note design decisions come unlabeled data context 
example split unlabeled dataset part injection robust em variant discussed subsection part select sensible prior model class input distribution 
literature review literature labeled unlabeled problem comes certain variety elds 
attempted unsupervised task classify approaches number clusters aim purely right time position formulate paradigms labeled unlabeled problem :10.1.1.114.3629
class membership fuzzy pointed single 
section discuss solely labeled unlabeled problem directly 
related problem discussed section 
theoretical analyses early idea em joint models train labeled unlabeled data see subsection old seminal general em :10.1.1.117.3731
titterington section review early theoretical problem discriminant analysis presence additional unlabeled data 
authors assume data generated mixture gaussians equal covariance matrices case bayes discriminant linear 
analyze plug method sampling paradigm parameters class distributions estimated maximum likelihood discussed subsection :10.1.1.117.3731
gaussians somewhat separated asymptotic gain unlabeled samples significant 
empirical studies nite samples promising 
details see :10.1.1.32.4821
mclachlan gives practical algorithm case essentially hard version em step unlabeled points allocated populations discriminant derived mixture parameters previous step note general em algorithm proposed time 
proves moderate sized training sets population pool points sampled mixture algorithm initialized ml solution labeled data solutions computed method converge surely true mixture distribution jd :10.1.1.117.3731
papers give positive motivation feasibility labeled unlabeled problem start somewhat unrealistic assumptions 
prior assumption having available limited prior knowledge labeled examples :10.1.1.114.3629
literature review class distributions gaussian equal covariances strong sensibly applied prior knowledge nontrivial real world tasks 
papers discussed far subsection focus generative methods assume parametric forms class conditional distributions 
anderson suggests modi cation logistic regression popular diagnostic methods :10.1.1.32.4821
logistic regression see subsection models log tjx jx jt linear functions augmented dummy attribute dimension value constantly :10.1.1.51.9998:10.1.1.117.3731
true underlying populations normal share covariance linear functions 
supervised ml logistic regression proceeds choosing linear function maximizes conditional likelihood data bayesian approach place prior linear function compute posterior distribution 
map approximation bayesian gaussian process classi cation called generalized penalized maximum likelihood seen logistic regression feature space see :10.1.1.32.4821
purely diagnostic settings unlabeled data help narrowing belief latent function see subsection :10.1.1.117.3731
anderson circumvents problem choosing parameterization mixed diagnostic logistic regression setting sampling paradigm situation :10.1.1.32.4821

assumption log xj xj xj exp xj exp xj :10.1.1.117.3731
chooses parameters xj maximize likelihood unlabeled data subject constraints xj xj distributions sum :10.1.1.117.3731
nite problem transformed unconstrained optimization parameters lagrange multipliers :10.1.1.117.3731
continuous input variable anderson advocates form xj derived nite case smooth function 
algorithm interesting restricted assumption linear logit 
clear generalize powerful case logistic regression feature space 
furthermore small samples continuous form xj obtained non penalized ml inadequate 
idea mixing diagnostic generative techniques probably represented satisfying way scheme suggested subsection extension algorithm miller uyar 
murray titterington see example suggest ad hoc procedure :10.1.1.117.3731
labeled data available class obtain kernel estimates class conditional densities xjt 
estimates em maximize likelihood mixing coecients parameters representing 
procedure robust lot unlabeled data :10.1.1.117.3731
small kernel estimates xjt poor em mixing coecients converges unique global optimum :10.1.1.117.3731
essentially variant blahut algorithm compute rate distortion function important quantization see :10.1.1.117.3731
literature review obtain better values mixing coecients rescue nal discrimination 
method valuable large proportions sample points di erent classes re ect true mixing coecients 
shahshahani landgrebe provide analysis aimed general question unlabeled data help classi cation methods originating asymptotic maximum likelihood theory 
argumentation somewhat unclear criticized various authors :10.1.1.117.3731
de ne model classes confuse asymptotic nite sample terms 
true strong consistency arguments estimators maximum likelihood hold asymptotically independently model class characteristics models crucial nite sample case 
worse want talk labeled unlabeled problem labeled dataset small 
assumption nd unbiased estimators case unrealistic 
clearly come methods reduce variance estimator employing unlabeled data reason believe modi cations introducing new bias 
quite obvious information di erent sources latent model parameter adds 
obvious construct model family learning algorithm information conditioned choice model family non zero algorithm signi cant information introducing new bias 
example pointed standard generative model diagnostic classi cation information unlabeled data latent discriminant zero measured asymptotically nite sample see subsection :10.1.1.117.3731
parametric approach adopted straightforward em algorithm see subsection suggested 
analysis problem employs fisher information techniques :10.1.1.117.3731
models data generation process carefully de ned avoid confusions arise 
diagnostic methods authors show standard generative model unlabeled data help 
commented point subsection :10.1.1.117.3731
generative methods unlabeled data helps 
true assumptions 
analysis draws asymptotic concepts 
fisher information characterizes minimal asymptotic variance unbiased estimator maximum likelihood estimator typically asymptotically unbiased 
applying concepts case small lead strong 
authors interesting empirical evidence concerning performance transduction algorithms discussed subsection text categorization task 
results indicate transduction algorithms su er instability robustness problems similar mentioned context em algorithm see subsection 
deals active learning scenarios scope review 
labeled unlabeled problem analyzed starting strong literature review assumption capable identifying class distributions xjt exactly unlabeled data :10.1.1.117.3731
jd clear achieved unsupervised learning procedure :10.1.1.117.3731
authors mention trivially classi cation unlabeled data possible 
identi ed class regions deduce label information 
continue show identi ed class regions optimal error probability labeled samples converges bayes error exponentially fast authors propose address realistic case nite subsequent aware moment 
motivates approach mentioned subsection strong unsupervised technique identify class regions connected parts class regions :10.1.1.117.3731
authors show task achieved labeling parts fairly easy terms size required 
assumptions strong met practice 
interesting investigate value labeled samples weaker assumption marginal identi able 
expectation maximization joint density model decided classify solutions kind baseline methods see subsections partly lling missing information em standard technique partly running em labeled unlabeled problem instances empirically called solution 
miller uyar discussed subsection 
authors suggest variant treat class label second latent variable alongside separator data distribution de nitive clamped points see subsection apply em :10.1.1.117.3731
ect variation lower bound em criterion marginal log likelihood worse variants come guarantee convergence local maximum marginal likelihood 
architecture proposed miller uyar expect exhibit comparable performance authors nd experiments 
latent label variant advantageous extending architecture suggested subsection 
nigam case study addresses question unlabeled data help improve naive bayes text classi er case small em joint model discussed subsection simple extensions 
suggest weighting sums mentioned section beating purely supervised technique small sucient conclude method succeeds solving problem 
naive bayes assumption proposes models text pages words page conditionally independent class label page 
literature review joint log likelihood corresponding respectively unequally 
reasonable treat points di erently simply run risk informative labeled points dominated sheer amount available unlabeled points introducing weighting factor joint log likelihood straightforward modi cation probabilistic modi ed criterion log likelihood function anymore weighting factor chosen heuristics cross validation purpose unwise small 
extension modeling class center seen special case em technique discussed subsection 
criticized standard em attack labeled unlabeled problem subsection 
critique applies nicely setting discussed naive bayes assumption leads extremely poor models class conditional distributions 
intended provide genuinely new solutions merits considerable clari es joint model em techniques notes problems related techniques provides extensive case study contains detailed section related 
extends case study including robust em variants see subsection 
authors try combine em algorithm joint probability model see subsection active learning strategy committee qbc algorithm see subsection attack instance labeled unlabeled problem text classi cation :10.1.1.20.8521:10.1.1.20.8521:10.1.1.32.4821:10.1.1.13.8629:10.1.1.117.3731:10.1.1.30.3233:10.1.1.30.3233
idea overcome stability problems standard em injecting unlabeled points time 
large pool unlabeled data authors initialize em training labeled data 
criterion derived qbc select informative points unlabeled ones transfer latent labels em dataset rerun em 
iterated convergence criterion met 
combination em active learning original idea particular realization idea somewhat distorted host heuristic intermediates qbc em :10.1.1.13.8629
example criterion value informativeness unlabeled text page obtained multiplying qbc criterion heuristic measure density document 
qbc criterion somewhat distorted sense basic idea qbc select query points maximally reduce variance discriminant ensemble represented posterior distribution data observed far 
xed variance measured sampling xed number discriminants posterior evaluating mccallum nigam produce sample run em convergence starting parameter vector sample replace initial nal converged vector :10.1.1.13.8629
clear committee set parameter vectors ful ls requirements estimate variance done qbc 
experiments authors surprised nd distorted version qbc improve literature review ecient undistorted variant followed single em run 
authors nd proposed method outperforms standard em joint model family see 
interesting compare active selection method robust variants em see subsection self training see subsection :10.1.1.117.3731
training algorithms training learning paradigm proposed address problems strong structural prior knowledge available :10.1.1.114.9164:10.1.1.117.3731
mentioned subsection training seen bayesian inference basic training algorithm robust variant em compute map approximation bayesian inference assumption compatibility target concept input distribution encoded conditional priors attain input dependent regularization see subsection :10.1.1.117.3731
refer details :10.1.1.117.3731
training simple ective idea come surprise related ideas earlier unsupervised learning 
reviewing 
becker hinton propose imax strategy learn coherence structure data 
quoting approach maximize measure agreement outputs groups units receive inputs physically separated space time modality :10.1.1.51.9998
forces units extract features coherent different input sources 
claim reasonable model families carefully regularized aware theoretical backing 
simplicity focus example detecting shift random dot stereograms 
:10.1.1.117.3731
denotes small unknown set point sampled follows rst row drawn product bernoulli variables :10.1.1.117.3731
second row rst shifted positions 
strong sense coherence examples sharing exactly amount shift 
imagine model classes fp tjx :10.1.1.117.3731
ranges nite set size chosen priori possible learn size data sort second level inference skip simplicity 
idea models class get see particular part point de ned window speci models class fed jk jk windows non overlapping usually cover range example drawn detailed particular amount shift coherent views point :10.1.1.117.3731
models classes appropriately regularized occam priors 
goal learn models identify shift particular pattern group examples generally small compared really matter margins say simplicity rst row rotated positions form second free space lled elements pushed :10.1.1.117.3731
literature review cluster exhibit amount shift 
becker hinton showed task solved unsupervised manner maximizing sample mutual information outputs units model classes 
speci de ne random variables joint distribution jx jx expectation empirical distribution dataset :10.1.1.117.3731
marginals jx 
sample mutual information imax criterion de ned log expectation :10.1.1.117.3731
note criterion minimal independent maximal deterministically related identical :10.1.1.117.3731
imax strategy maximize criterion appropriate regularization model classes 
regularization done occam priors example maximize sum imax criterion log priors 
de sa related train families models logistic regression models fed di erent view examples 
views di erent modality example sound lip images order decode speech 
models seen hard discriminants system trained minimize fraction training examples units disagree 
suboptimal logistic regression models con dence information soft estimators neglected 
think thorough theoretical analysis imax related schemes dicult 
give intuitive ideas 
learning regularities class identity coherence restricted view examples conceptually easier complete representation 
partly due limited data spaces lower dimension covered easier partly due inability construct model families high dimensional data partly due simply limited computing resources 
certain coherence exhibited clearly restricted view complete representation coherence pays strongly attempt eciently encode data see section common learning scheme discovers coherence restricted view trained unsupervised manner :10.1.1.117.3731
linking units operating di erent views imax fashion discovered information instantly passed partner model way teacher passes information student supervised learning scheme 
particularly nice example occam razor drive unsupervised learning 
soon units discovers means exploit part coherence data simply potential compress data drastically 
understanding simple families distributions small mixtures gaussians behave high dimensional spaces sense clearly understand combine volumes data 
literature review taken reliable truth passed partners teacher student manner 
note discovered coherence spurious student detect view inability regularized simplicity synchronize teacher turn uence teacher give idea assign lower importance 
discussed far subsection attack problem follows suppose prior knowledge particular coherence hold examples class certain degree class boundaries 
course strongly dependent representation features examples 
simple means nd coherence look di erent views di erent modality examples coming physically di erent information sources 
idea identify groups transformations acting representation examples probably invariant class identity idea important elds dealing object recognition images see subsection 
coherence information try nd di erent views examples ful criteria di erent possible coming di erent physical sources 
ideally conditionally independent class identity example information shared class label 
aimed exhibiting particular coherence clearly isolation 
ideally views describes part information contained example able learn particular coherence view examples essentially easy complete representations 
example shift random dot stereograms discussed represented degree full matrix windowed part 
prior run imax algorithm de sa model classes regularizations choice learn soft partitioning example space unsupervised manner training data 
labeled data assign clusters classes 
related general scheme discussed subsection :10.1.1.117.3731
relations training discussed subsection 
training paradigm introduced blum mitchell see :10.1.1.114.9164:10.1.1.117.3731
idea exploit particularly strong kind coherence sense discussed notion compatibility di erent views example write blum mitchell interested classi cation web pages suggest describe web page di erent views representation words page representation words hyperlinks pointing page :10.1.1.117.3731
note classi cation system sense class coherence examples learnable views isolation 
hypothesis compatible input distribution hypotheses respectively support predict class label respectively :10.1.1.117.3731
compatibility assumption literature review training restricts latent hypothesis compatible latent input distribution 
assumption blum mitchell suggest simple algorithm learn classi cation small labeled dataset large unlabeled works updating set labeled data dw hypotheses respectively :10.1.1.117.3731
tted dw ideally error free set :10.1.1.117.3731
initially dw injects new points dw sequentially labeling retraining augmented dw roles teacher student alternated injection :10.1.1.117.3731
algorithm easily understood instance robust em scheme diagnostic models described subsection 
notion compatibility views encoded conditional prior 
model input distribution estimate support refer 
set compatible support 
detailed derivation training variant diagnostic em :10.1.1.117.3731
blum mitchell give interesting theoretical analysis training 
unfortunately employ strong assumption views conditionally independent class label 
interesting get theoretical insight cases weaker assumptions 
imax schemes training employ feature split coherence di erent views learn unlabeled data coherence kind redundancy unlabeled examples redundancy useful information meaningful grouping examples 
important di erences 
imax related schemes purely unsupervised training leads supervised methods 
imax designed learn grouping corresponds coherence expected prior knowledge hold di erent views examples 
denotes corresponding grouping variable imax requires views selected conditionally independent weakly conditionally dependent hold imax probably learn di erent grouping fail learn meaningful grouping 
example windows random dot stereograms setting discussed overlap imax fail low order conditional dependencies views training feature split chosen coherence views compatible class identity 
split merely sort information bridge unlabeled data belief latent hypothesis essential characteristic grouping induced 
unlabeled data ectively views conditionally independent weakly conditionally dependent expect boost performance compared classi cation somewhat weaker assumptions 
collins singer apply training paradigm problem named entity classi cation :10.1.1.114.3629
interested classifying entities chris williams pointing 
literature review uniquely represented names 
classi cation system example persons cities companies 
correspondence represented small system simple rules 
augments description entity features extracted automatically samples entities occur 
example extract context name text pages names occur 
idea look concrete spelling name 
clear nd di erent views named entities satisfying training requirements 
interesting part development boosting extension powerful adaboost algorithm see supervised classi cation attack labeled unlabeled problem :10.1.1.51.9998:10.1.1.32.4821:10.1.1.133.1040
extension surprisingly simple elegant algorithm prove competitive existing labeled unlabeled algorithms 
authors report su ers principle robustness problems em see subsection existing transduction algorithms see subsection 
algorithm cases fooled apparent simplicity structure unlabeled data exhibited large bias information labeled data detect failure 
nigam ghani case study comparing standard em see subsection basic training suggested robust em variants see subsection partly data natural sources partly arti cially created datasets :10.1.1.114.9164:10.1.1.117.3731
task text classi cation 
valuable comparative study contains interesting ideas increase robustness standard em combine best worlds training em employing class conditional models 
authors confuse points 
realize basic training form diagnostic em see subsection 
compare training versions generative em em employing class conditional models try feature split prior knowledge notion comparing diagnostic versus sampling paradigm labeled unlabeled problem 
second criticize em algorithm strong probabilistic foundation section performs worst experiments 
realize best performer algorithm call self training see subsection just version em strong probabilistic foundation worst performing standard batch em :10.1.1.117.3731
foundation detailed subsection :10.1.1.117.3731
goldman zhou propose algorithm address labeled unlabeled problem related training :10.1.1.32.4821:10.1.1.32.4821
employ feature split di erent views examples incorporates di erent model classes 
time current discriminant represented models class 
models initially chosen training 
algorithm works turn identifying points remaining ones authors call variant em 
literature review models predicts con dently adding predicted pseudo label 
positive side addresses important robustness issues danger accumulating classi cation noise incorporating certain number incorrectly labeled points authors propose safeguards issues conservative 
problem authors apply host tests classical statistics testing hypotheses conditioned completely di erent events di erent partitions input space model classes classi cation trees 
think assumptions classical tests really hold di erent situations tests applied 
concrete weakness frequent fold crossvalidation labeled data jd small cross validation exhibits high variance useful model selection especially frequently address sorts di erent modeling questions :10.1.1.117.3731
short idea employ di erent model classes feature split supported prior knowledge interesting investigated particular algorithm suggested probably bit occam razor :10.1.1.32.4821:10.1.1.32.4821
note seeger proposes generalization training issues goldman zhou attack heuristically dealt principled bayesian way :10.1.1.117.3731
checked approach feasible architecture goldman zhou 
adaptive regularization criteria basic idea adaptive regularization criteria criteria minimized supervised settings generalization error expected loss involve expectation unknown input distribution 
simply stated making mistakes regions large hurts terms criteria mistakes regions low density 
tting problem arise complex model tted sparse data 
complex model data really choose functions relations model able represent compatible data 
model complex usually represent large number functions compatible data show di erent behaviour away data 
criterion gives rules constraints choose random choice generalize badly unseen data 
regularization occam razor gives additional rule prefer simple complex functions 
concrete meanings simple complex course depend task available prior knowledge 
point subsection occam assumption simplicity really enforced regions input points related learning learn multitask learning see subsection 
literature review 
words regularization backed occam razor dependent input distribution see subsection :10.1.1.117.3731
schuurmans captures kind input dependence de ning natural metric hypotheses :10.1.1.51.9998:10.1.1.51.9998
symmetric loss function recall set possible values target expected loss hypothesis tjx tjx 
suggests de nition natural metric hypothesis 
hypothesis space de ne ext fp tjx easy show pseudometric case common loss functions squared error regression estimation zero loss classi cation de ned pseudometric ext particular ful ls triangle inequality 
suppose construct hierarchy increasing complexity select best ts data triangle inequality tjx tjx schuurmans argues estimate left hand side fairly accurately large unlabeled sample expected losses right hand side estimated tting occurs empirical losses grossly di erent true expected losses case strong underestimates :10.1.1.117.3731
inequality violated plugging estimates place true unknown quantities conclude tting occured :10.1.1.117.3731
schuurmans proposes model selection procedure select hypotheses estimated version violated procedure outputs empirical results promising case regression estimation polynomials :10.1.1.117.3731
technique exhibit tting simply triangle inequality usually far tight :10.1.1.117.3731
extension strategy called attempts rst order bias correction pseudometric estimated version say schuurmans get rid priori hierarchy focus criteria additive multiplicative combinations empirical loss tjx penalty :10.1.1.51.9998
propose penalties idea tting detected comparing xed origin function distances estimated reliably plan go discussions foundations occam razor 
people think occam razor backed evolutionary theories complex structures evolve simple initial conditions accumulation small random changes selection processes 
context simple solutions problem complicated ones sequence selection processes active evolution solution unusual 
evolution solutions conditioned particular situation surrounding problem situation creates selection processes give evolution non random direction 
example human cells ectively aspects surprisingly simple temperature lies narrow range 
simplicity graded conditioned situation particular task expected 
literature review estimated input points 
important chosen priori having seen data 
penalties motivated nicely tjx choice course unrealistic 
motivation gets somewhat weaker case chosen arbitrarily 
empirical results polynomial regression estimation task show method competitive results classi cation convincing 
interesting empirically regression estimation quite aggressive multiplicative penalty outperforms additive penalty idea regularization strategies currently including bayesian map estimation employ additive penalties 
note additive criterion applied regression estimation squared error loss suggested :10.1.1.51.9998:10.1.1.117.3731
give convincing theoretical motivation :10.1.1.117.3731
conclude adaptive regularization criteria notion regularization see subsection :10.1.1.117.3731
criteria suggested reported regression estimation tasks reported results classi cation promising :10.1.1.51.9998:10.1.1.51.9998
information labels contains latent function directly accessible regression estimation 
clear inject available prior knowledge procedures suggested interested line research :10.1.1.51.9998
fisher kernel fisher kernel proposed rst general principled attempt exploit information generative model tted input distribution powerful currently available classes discriminative classi ers kernel methods gaussian processes support vector machines :10.1.1.25.8841:10.1.1.117.3731:10.1.1.44.7709
nutshell kernel methods diagnostic schemes see subsection prior distribution latent function gaussian process speci ed positive de nite covariance kernel :10.1.1.117.3731
covariance kernel induces natural distance feature space fisher kernel attempts adapt distance highly genuine interesting way information distribution input points drawn model tted 
main diculties constructing adaptive kernels need ful requirement positive de niteness seen inner products linear space 
finding decent kernels unusual important input spaces spaces variable length sequences discrete structures challenging general solutions available 
case questionable employ families standard kernels highly speci tasks done usually labeled data sparse class case jt latent function represents log ratio classes point called logit 
literature review simply kernel encodes prior knowledge task standard kernels available er vague possibilities doing :10.1.1.117.3731
sense running kernel methods standard kernels ignore probabilistic geometry way employ standard distances squared error impose uncorrelated gaussian noise situations know better see 
naive fisher kernel log log fp xj model family :10.1.1.117.3731
denotes maximum likelihood estimate gradients evaluated denotes fisher information matrix xj log xj log xj gradients evaluated exponential embedding construct family nitely divisible kernels see terms naive fisher kernel :10.1.1.117.3731
motivations fisher kernel see easily accessible scope report :10.1.1.44.7709
probably direct line goes information theoretic perspective authors unpublished workshop talk :10.1.1.44.7709
motivation picked extended ongoing author see :10.1.1.19.8785
number ways suggested improve basic fisher kernel suciently tested empirically 
fisher kernel applied successfully discrimination protein families proteins represented amino acid sequence families tted hidden markov models hmm :10.1.1.44.7709
applied document retrieval 
attempts apply fisher kernel case tted gaussian mixture reported failed far 
problem way fisher kernels generative information nature fisher kernel crude approximation information score sensible mixture models unclear far 
suggestions accurate approximations underlying intractable information score case mixture models :10.1.1.19.8785
conclude fisher kernel covers new ground general technique information generative models diagnostic classi cation schemes opposed straightforward scheme dictated sampling paradigm classes modeled separately see subsection :10.1.1.117.3731
case diagnostic methods outperform generative methods far primary option cases feasibly imbedded consists variable length sequences example speech recognition bioinformatics tasks 
support vector gaussian process classi cation fisher kernel outperform literature review generative scheme comparable modeling orts signi cantly demonstrated 
claim authors fisher kernel consistently outperforms equivalent generative scheme proved certain strong assumptions clear hold practice :10.1.1.44.7709
geometry feature spaces induced kernels imperfectly understood far done direction :10.1.1.117.3731
kernel methods essentially linear machines feature spaces understanding properties geometry valuable encoding available prior knowledge task :10.1.1.51.9998
haussler gives comprehensive problems kernel design suggests general methods construct kernels unusual containing discrete structures di erent fisher kernel 
nally fisher kernel seen instance regularization dependent input distribution see subsection :10.1.1.117.3731
details :10.1.1.117.3731
restricted bayes optimal classi cation tong koller suggest general framework combining generative diagnostic methods classi cation di ers bayesian analysis conditional priors see subsection :10.1.1.117.3731
usual framework regularized discrimination uses loss function hypothesis regularization functional mapping positive real axis 
enforce characteristics hypotheses priori believe penalizing hypotheses violating characteristics larger values 
occam razor see section regularization functionals penalize complicated hypotheses idea select hypothesis minimizes tradeo denotes empirical distribution data tradeo parameter chosen cross validation :10.1.1.117.3731
examples framework include support vector map gaussian process classi cation 
general map approximation bayesian discrimination see subsection falls class 
restricted bayes optimal classi cation tong koller generative method estimate joint data distribution complete observed data call estimate 
minimizing suggest minimize frequently overlooked researchers subjectivity bayesian priors preferring occam regularization notion complexity depends task prior knowledge :10.1.1.117.3731
literature review words replace empirical loss expected loss joint estimate prove interesting theorems giving interpretation maximum margin hyperplanes generalization support vector classi cation 
suppose estimate xjt parzen windows gaussian kernels sum radial gaussians centered points common width 
hyperplane attains lowest error tj 
amounts zero loss fh tg show converges maximum margin hyperplane hyperplane classi es data correctly lies distant convex hulls positive negative points class classi cation 
compare framework directly map approximation bayesian analysis conditional priors 
employ negative log likelihood loss log tjx 
standard supervised map classi cation amounts minimizing log denotes prior distribution assume model input distribution xj employ conditional priors hj see subsection :10.1.1.117.3731
easy show map modi ed data generation model amounts minimizing log hj jd words restricted bayes optimal classi cation modi es empirical loss part generative model tted input data map conditional priors modify regularization functional ective prior :10.1.1.114.3629:10.1.1.117.3731
note labeled dataset small expect changing loss part major ect nal choice re ected experimental results reported :10.1.1.117.3731
transduction transductive inference opposed inductive inference principle introduced learning theory vapnik see 
suppose labeled training set set test points required predict labels test points 
traditional way propose existence latent function linking training test points marginal dependence infer function posterior distribution induction latent labels deduction cases simple evaluation function 
vapnik principle order solve problem come subproblems harder transduction estimating test labels directly original formulation transduction test points additional points 
algorithms consider realistic case test set subset du literature review harder induction infer latent function 
philosophical viewpoint principle leading transductive inference appealing 
vapnik goes try prove pac bounds speci cally tailored transduction case 
technical details messy broad idea view set input points basic pool jd points drawn random replacement 
points labeled tjx 
points remaining pool form employ concentration inequalities multinomial distribution common tools vapnik chervonenkis vc theory derive large deviation bounds empirical error empirical error nd dicult compare bounds tightest vc induction bounds known far 
transduction bounds vapnik derives algorithmic scheme transduction attempt transfer notion large margin discrimination supervised learning transduction case 
case binary classi cation linear discriminants kwk scheme works follows :10.1.1.117.3731
ft denote latent labels points discriminant de ne arti cial empirical margin ae margin art max tu min suppose simplicity exists discriminant ae margin positive necessary condition linearly separable :10.1.1.117.3731
words ae margin largest empirical margin discriminant attain completion data larger empirical margin margin de ned min :10.1.1.117.3731
vapnik transduction scheme case choose discriminant maximizes ae margin 
compared vapnik induction scheme choose maximize margin 
margin intuitive interpretation sort estimator true margin ty discriminant case obviously closely related generalization error know link ae margin generalization error 
bennett suggest variant vapnik scheme case linear discriminants svm 
focus variant svm employs norm kwk jw penalization euclidean norm kwk kwk :10.1.1.117.3731
purely supervised tasks variant perform similarly vapnik original linear svm 
setting optimization discriminants latent labels computed mixed integer programming 
interesting straightforward implementation vapnik transduction scheme exponential jd know ecient realization 
algorithm mixed integer programming scale example datasets tested subsamples size full experiments literature review show signi cant improvements induction third tasks tested 
experimental design somewhat unusual example choose usually important scaling variance parameter heuristic depends adapting data 
authors suggest ecient variant nonconvex quadratic problem local solutions block coordinate descent algorithms :10.1.1.33.4031
substantial drawback scheme algorithm feature space mapping 
joachims presents greedy approximative implementation vapnik transduction scheme case linear discriminants svm 
algorithm guaranteed expected nd true optimum get stuck poor local optima 
runs faster algorithm especially large furthermore nonlinear kernels 
author presents experiments text classi cation tasks 
jaakkola suggest interesting transduction algorithm minimum relative entropy mre discrimination framework 
authors tried relate discriminative classi ers svm diagnostic bayesian prediction gaussian processes 
problematic loss function support vector classi cation svc proper noise model normalized :10.1.1.51.9998
way problem regard svc approximation gaussian process classi cation unusual noise model 
probably satisfying way drop idea svc doing bayesian inference try nd paradigm bayesian svc natural member 
usual derivation svc margin constraints points maximum entropy principle long parallel bayesian paradigm induce distributions way constrained observed data :10.1.1.117.3731
combine principle originating statistical physics bayesian idea prior arrive minimum relative entropy mre principle :10.1.1.117.3731
authors apply mre large margin discrimination show svc arises special case 
advantages view include mre bayesian paradigm natural ways handle missing data 
transduction mre discrimination straightforward exploiting fact latent labels marginalize contrary diagnostic bayesian case regularization independent input distribution see subsection 
words symmetry latent labels broken drive large margin points mre transduction resembles diagnostic em discussed subsection comes similar convergence guarantees local optima 
preliminary experiments task splice sites dna transcription predicted show promising results 
algorithm transduction algorithm aiming maximize arti cial empirical margin builds theoretical foundation vapnik scheme discussed 
interesting compare algorithm diagnostic em bayesian literature review settings employing priors latent functions conditioned input distribution 
mre formulation possible consider hybrids remarked unlabeled dataset split subsets latent labels transduction algorithm narrow prior latent functions 
refer jebara jaakkola conditional em algorithm exciting line research 
subjective critique slt transductive inference discussion theory transduction statistical learning theory slt see scope :10.1.1.117.3731
honest feel somewhat confused vapnik arguments confusion probably due ignorance computational statistical learning theory shared certain degree people working eld labeled unlabeled problem 
put forward aspects slt transduction wrong unfair certain points happy get discussions 
hand quite certain thing realizable slt transduction ers signi cant advantages slt induction pac framework see subsection proven theoretically advantages foundations clear transparent publications transduction probably remain academic regarded doubts majority researchers :10.1.1.117.3731
diagnostic prediction nite data needs regularization favourite way impose existence latent function propose model family slt terms hypothesis class function prior distribution penalizes complex functions giving small volume see subsection :10.1.1.117.3731
fact dealing problem connection training test points see way get assumption latent function modeling thereof prior knowledge slt transduction needs employ step 
assumptions bayesian way inferring information latent function inductively computing posterior process predicting test labels deductively computing predictive distribution exactly called bayesian transduction 
just convenient way write expectation predictive distribution 
information input distribution test points help diagnostic bayesian schemes latent function input distribution seen random variables priori independent generative model discussed :10.1.1.117.3731
assumption bayesian transduction easier way induction 
literature review slt transduction comes frequentist viewpoint induction means picking best function deduction means evaluating test points possible context transduction easier way induction consistently outperform simply due fact invalid inaccurate way inference rst place 
vapnik tries motivate slt transduction presenting bounds speci cally tailored transduction setting 
reading formidable book way vapnik presents results inductive inference 
starts philosophical principles nature learning induction derives pac bounds complexity measures principles sophisticated statistical techniques infers algorithmic schemes guided bounds 
way presentation works transduction case 
dicult compare vapnik transduction bounds best known vc induction bounds see signi cantly tighter especially case jd important practice 
fact argument transduction easier way induction gets weak case distinction transduction induction vanishes :10.1.1.117.3731
information input distribution expected effective labeled data sparse 
case transduction induction bounds usually far tight trivial 
authors criticized connection true margin de ned subsection generalization error discriminant vc theory supervised settings uses link fact margin estimator true margin motivation ae margin :10.1.1.117.3731
especially case run risk discriminant maximizes ae margin smaller margin choices 
worse large discriminant achieves positive ae margin separable function hypothesis class case vapnik suggests soft ae margin variant trading margin violations versus margin size 
evident tradeo believing unlabeled data believing labeled data observed labels noisy quantities faced order solve labeled unlabeled problem convinced publications presently known tradeo reliably interesting note architectures frequently discussed slt publications support vector machine svm understood special case minimum relative entropy mre discrimination see discussion subsection induction means computing mre distribution functions conditioned data somewhat parallel bayesian posterior distribution deduction equivalent bayesian prediction posterior replaced mre distribution 
non experts di erent bounds employ quite similar techniques 
example exists induction bound see proceeds double sample original ghost sample di erent sizes 
close situation transduction bound start 
related problems soft ae margin 
research direction put soft ae margin basis solid soft margin supervised learning 
note vapnik transduction scheme quite similar spirit class estimation methods discussed section 
called clustering methods labels points treated parameters latent variables 
method consists maximizing likelihood model parameters labels quite obvious procedure exhibit bias problem pointed authors see citations cases ml estimator asymptotically consistent 
reported method behaves better context robust estimators speculate suitable large margin discrimination 
citing basic aw clustering method parameter estimation treatment labels parameters treating missing random variables 
sense approach exhibit bias vapnik scheme situations issue clearly needs looked greater detail 
related problems section brie discuss problems related labeled unlabeled task mention done 
review detailed section means exhaustive 
reason discussing related problems course feel ideas successfully applied algorithms labeled unlabeled problem 
trivially related problems supervised unsupervised learning 
discussed large classes subsection :10.1.1.117.3731
prominent project bayesian unsupervised learning autoclass see straightforward implementation baseline methods discussed subsection :10.1.1.32.4821:10.1.1.117.3731
discussion quantization probably important special case unsupervised learning context source compression rate distortion theory helpful :10.1.1.122.8863
active learning pool active learning classi cation pool input points sampled :10.1.1.117.3731
access oracle producing tjx input labels produced di erent calls oracle conditionally independent 
goal supervised classi cation see subsection :10.1.1.117.3731
course algorithm problem active learning simply pick points random label oracle form collect remaining related problems points form declared goal active learning outperform schemes 
freedom call oracle selectively points focusing early informative points narrow belief latent relationship fast small number calls oracle 
authors considered non pool active learning access sample 
focus pool active learning called query ltering reasons 
non pool active learning obvious connections labeled unlabeled problem 
pac framework shown ability actively query labels signi cant advantage access sample 
furthermore practice unexpectedly dicult produce sample tjx small points discussed detail section :10.1.1.20.8521:10.1.1.20.8521:10.1.1.32.4821:10.1.1.117.3731
mackay discusses bayesian active learning multi layer perceptrons :10.1.1.31.4284
cohn introduce general problem focus joint density models kind discussed see subsection :10.1.1.18.5535:10.1.1.32.4821:10.1.1.117.3731
general query ltering algorithm query committee qbc see mentioned subsection :10.1.1.20.8521:10.1.1.20.8521:10.1.1.32.4821:10.1.1.117.3731:10.1.1.30.3233:10.1.1.30.3233
qbc sequential nature pool character stream 
incoming decide label place discard 
case may time 
similar sequential learning ltering algorithms qbc maintains updates belief distribution latent function conditioned data seen far belief seen approximation optimal belief bayesian posterior distribution 
qbc judges informativeness example expected amount variance removed belief conditioned label 
estimated sampling committee discriminants current belief evaluating predictions committee members compute measure disagreement predictions 
example committee consists discriminants employ symmetrized variant relative entropy predictive distributions discriminants 
matter algorithmic design taste derive criterion discard measure threshold criterion threshold annealed time 
algorithm stopped discards certain large number points row small threshold 
speculate active learning ideas labeled unlabeled problem context 
example subsection suggested starting unsupervised learning technique inject labeled points :10.1.1.117.3731
advantageous inject points ordering suggested active learning informativeness criteria 
suppose human expert label images supposed represent hand written digits 
small true tjx low entropy bitmap probably mess pixels expert able associate digits 
related problems coaching 
learning learn coaching problem analyzed 
goal infer probabilistic relationship 
example estimate regression tjx 
suppose third variable variables distributed unknown law 
complete sample law examples assumed dicult expensive collect forced predict 
trivial approach coaching discard examples entirely employ standard supervised algorithm 
authors ask better knowledge contained examples 
call coaching variable potential ability coach estimation 
clear sample help conditionally independent contains information nature observation learned common sample 
tibshirani hinton propose di erent coaching schemes 
mixture coaching builds representation tjx tjx jx dz dependent tjx quite di erent di erent values reasonable try learn partitioning space experts locally regression partition 
furthermore need learn conditional distributions partition algorithm suggested authors essentially mixture experts see choice expert observed training set :10.1.1.136.9119:10.1.1.136.9119
scheme response coaching builds tjx jx dz idea train model jointly predict way achieve class factorized models jx tjx jx :10.1.1.117.3731
conditional dependence represented shared parameter vector example regression trees share partition space 
art choose common separate parameters parameter priors regularization guided available prior knowledge assumptions 
response coaching seen special case problem learning learn multitask learning :10.1.1.51.9998:10.1.1.117.3731
relationship 
second task learned primary attempt employ information ow latent shared variables 
general approach problem suggested author refers problem family discovery 
model family fp xj assumed smooth low dimensional manifold embedded view model families comes information geometry see :10.1.1.117.3731
task learn prior enforces assumption multiple tasks 
manifold modeled connecting locally linear patches kernel smoothing 
alternatively generative topographic mapping gtm see related problems subsection probabilistic generative modeling approach manifold learning considered context :10.1.1.130.110:10.1.1.117.3731
promising approach suggested multiple task data learn covariance kernel gaussian process support vector classi cation see subsection :10.1.1.117.3731
relation labeled unlabeled problem coaching learning learn problem strong 
multitask learning coaching special case try grasp information shared low level tasks seen prior learning hierarchical bayesian setting :10.1.1.117.3731
pointed think prior knowledge available human insight learned sources crucial solutions labeled unlabeled problem learning priors combining related tasks useful context 
come back learning learn subsection relate method prior learning 
transfer knowledge learned related task learning learn discussed subsection largely data driven way learn prior distributions task 
assumption related tasks share common low level basis 
related tasks really interested gain information common basis learning solve models share parameters low level chosen regularization forces learning algorithm shared parameters ectively 
employ information prior knowledge primary task 
approach prior learning employs lot human insight nature primary task 
insight choose convenient representation encode prior knowledge select related tasks believe share nature primary task representation prior knowledge applies 
structure usually easy learn free parameters representation related tasks simply plug architecture primary 
concrete example handwritten signs recognition task 
generative assumption observation created mapping latent image latent transformation class small number images 
large number observations class possible learn images map fashion authors simple hillclimbing approach multiple alignment call 
possible learn posterior distribution transformations conditioned class applying techniques kernel smoothing 
authors distribution prior transformations dealing class examples available 
example example new class available take example image class create arti cial dataset sampling transformations caveats tradeoffs learned prior applying image 
motivation learning priors context labeled unlabeled problem subsection 
methods discussed subsection need large labeled dataset related task 
reason goal learn priors strong sense 
example transformations deform image signi cantly chance get high priori weight supported data 
learning representation class single example possible strong invariance knowledge available 
caveats tradeo section discuss issues think important addressed dealing labeled unlabeled problem 
popped frequently text section collect relate 
include obvious caveats tradeo faced 
think insight issues need gained probably model tasks 
insight may big payo algorithms dealing problem 
labels missing data formally treat labels points labeled unlabeled problem missing data :10.1.1.117.3731
compare kinds missing data missing uncertain attribute values nd important di erences 
missing attribute values nuisance marginalized lack information data labels essential target 
job predict labels nuisance attributes target prediction 
second talk missing attribute usually mean attribute missing uncertain small fraction examples dataset 
attribute missing large majority examples really consider include representation examples making model building task easier learning process easier control see discussion imax subsection 
interesting instances labeled unlabeled problem lot labels missing 
special character label latent variable raises question value information provided labels compared information provided input point 
sensible valuation course depend current belief unknown relationship 
example castelli cover see subsection show know class distributions xjt labels exponential value reducing generalization error bayes error concrete belief usually caveats tradeoffs gained practice :10.1.1.117.3731
suppose goal learn soft partitioning reasonably small number clusters high probability clusters cut class boundary 
clearly slightly easier solving labeled unlabeled problem 
apply unsupervised technique problem training argue context role labels point possible aws soft partitioning 
context label valued conjunction labels unlabeled labeled data suggest di erent partitionings local area nd way 
question value labeled data abundant unlabeled data far understood 
example derive estimate tjx capture label information treat estimate belief basic unit information injected unsupervised clustering 
hand value element conditioned belief gained far 
approach clearly exible slippery 
associate high value elements change belief relationship drastically run risk robust outliers classi cation noise 
prefer elements compatible belief far may able correct major inaccuracies belief simply trust label information help 
tradeo robustness information gain 
face similar tradeo start training labeled data inject points pseudo labels 
inject point label con dently predicted current belief injection provide new information lead minor change belief 
operation may considered robust 
hand injecting point label quite uncertain completely random current belief leads phase learning process quite di erent alternatives tested phase potential change belief signi cantly risk wrong update higher 
operation result higher information gain 
diagnostic versus generative methods suppose model classes fp xjt faithful problem hand class conditional distributions members classes latent furthermore assume limit jd class conditional distributions identi ed assumption castelli cover :10.1.1.117.3731
case employ generative architecture faithful model classes standard em variants see subsection circumvent poor local maxima likelihood :10.1.1.117.3731
approach exhibit severe drawbacks caveats tradeoffs applied non toy problems 
faithful model classes regularization broad posterior belief narrow suciently large large available unlabeled data abundant cost dealing large searching broad model classes map solution higher tolerate 
order able method eciently narrow model classes tighten regularization case risk run severe robustness problems 
true relation far apriori believe possible method simple structure supported priors abundant unlabeled data quite di erent 
diagnostic methods model tjx directly wasting resources modeling class distributions 
generative methods induced model tjx derived class models bayes formula model comes heavy requirements resources training data computing time resources wasted training aspects class models ect induced model signi cantly 
diagnostic methods neglect information focus maximizing conditional likelihood labeled data harmful small 
distinction generative diagnostic methods clear comparing diagnostic mixture experts architecture see extension generative method proposed discussed subsection :10.1.1.136.9119:10.1.1.136.9119
terminology de ned 
methods divide conquer principle construct soft partitioning input space experts estimate tjx locally 
diagnostic architecture achieves soft partitioning gating network diagnostic model kjx architecture experts gating network trained maximize conditional likelihood data 
architecture divide resources simple experts logistic regression models tjx achieve discrimination data 
example expect gating network position experts true decision boundaries classes experts locally boundary total mixture gives decent estimate tjx globally 
hand generative architecture partitions largely tting mixture 
component models xjk unimodal gaussian clusters means architecture tends position experts middle clusters generally far decision boundaries classes 
argued positioning divide conquer strategy improved discrimination 
regularization depending input distribution see subsection principle unlabeled data diagnostic architec hierarchy case powerful hierarchical mixture experts see :10.1.1.117.3731:10.1.1.136.9119:10.1.1.136.9119
tures 
invariably requires model input distribution apart dicult task construct sensible conditional priors notation subsection prior knowledge face tradeo distribute resources training diagnostic models models :10.1.1.117.3731
furthermore constructing conditional priors enforce prior assumptions various degrees strength induces weighting information nal prediction 
face tradeo robustness weak priori uence belief information gain strong priori uence belief 
sampling assumption generative assumption detailed subsection :10.1.1.117.3731
equivalent context transduction see subsection 
sample points 
pick points pool random replacement label points put remaining pool assumption ful lled reasonable real world tasks note violated certain settings 
example situations labeled data sparse process labeling input points expensive reasonable assume input points labeled form picked random large pool 
selected somewhat representative representativeness judged de ned measures insight problem 
cases may possible incorporate selection process generative model 
situations stick standard generative assumption risk ignoring bias sample try valuation rules see subsection order alleviate bias :10.1.1.117.3731
example knew characteristics selection process apply resample inject representative points earlier 
respect key aspects labeled unlabeled problem lies middle founded areas 
problem situated unsupervised supervised learning 
opinion prior knowledge crucial labeled unlabeled method roles prior knowledge unsupervised supervised learning di erent discussed subsection :10.1.1.117.3731
supervised learning quite robust false inaccurate prior knowledge solutions delivered unsupervised methods depend encoded prior knowledge 
labeled unlabeled problem expect robustness certain degree sparseness abundance unlabeled data usage unsupervised techniques obligatory 
basic paradigms supervised learning diagnostic sampling see subsection :10.1.1.117.3731
signi paradigms labeled unlabeled problem discussed subsection 
order unlabeled data diagnostic methods input dependent regularization certain degree modeling aspects input distribution necessary see subsection :10.1.1.117.3731
think promising combine methods diagnostic sampling paradigm employ diagnostic generative model families architecture order attack labeled unlabeled problem 
think role class label latent variable special intricate see subsection general treat labeled unlabeled problem way common problems missing uncertain data :10.1.1.117.3731
believe prior knowledge central importance role plays method strongly vary solutions tasks 
powerful ways encode prior knowledge proposed context unsupervised learning genuinely new interfaces supervised methods inject label information unsupervised setting adapt distance supervised method information input distribution 
methods learning priors see subsections ideas redundant representations examples kind redundancy strongly linked class identity see subsection may applicable successfully 
acknowledgments chris williams amos ralf herbrich hugo zaragoza neil lawrence tong zhang kristin bennett helpful discussions 
author gratefully acknowledges support research studentship microsoft research shun ichi amari :10.1.1.117.3731
di erential geometrical methods statistics 
number lecture notes statistics 
springer st edition :10.1.1.117.3731
shun ichi amari 
information geometry em em algorithms neural networks 
neural networks :10.1.1.117.3731
anderson :10.1.1.32.4821
multivariate logistic compounds 
biometrika :10.1.1.117.3731
jonathan baxter 
bayesian information theoretic model bias learning 
proceedings colt pages :10.1.1.117.3731
beal ghahramani 
variational inference bayesian mixtures factor analyzers 
advances neural information processing systems :10.1.1.117.3731
mit press :10.1.1.117.3731
becker hinton 
self organizing neural network discovers surfaces random dot stereograms 
nature :10.1.1.117.3731
becker :10.1.1.51.9998
learning recognize moving objects model tting problem 
advances nips pages :10.1.1.117.3731
kristin bennett 
semi supervised support vector machines 
kearns solla cohn editors advances neural information processing systems pages :10.1.1.117.3731
mit press :10.1.1.117.3731
james berger 
statistical decision theory bayesian analysis 
springer nd edition :10.1.1.117.3731
bishop svens en williams :10.1.1.130.110:10.1.1.117.3731
gtm generative topographic mapping 
neural computation :10.1.1.117.3731
christopher bishop :10.1.1.117.3731
neural networks pattern recognition 
clarendon press oxford :10.1.1.117.3731
avrim blum tom mitchell :10.1.1.117.3731
combining labeled unlabeled data training 
proceedings colt :10.1.1.117.3731
christopher burges :10.1.1.117.3731
geometry invariance kernel methods 
advances kernel methods support vector learning 
mit press cambridge :10.1.1.117.3731
christopher burges :10.1.1.117.3731
tutorial support vector machines pattern recognition 
data mining knowledge discovery :10.1.1.117.3731
rich caruana :10.1.1.117.3731
learning related tasks time backpropagation 
advances neural information processing systems 
mit press :10.1.1.117.3731
castelli thomas cover :10.1.1.117.3731
exponential value labeled samples 
pattern recognition letters :10.1.1.117.3731
malik ismail :10.1.1.117.3731
incorporating test inputs learning 
advances neural information processing systems :10.1.1.117.3731
mit press :10.1.1.117.3731
peter cheeseman john stutz :10.1.1.117.3731
bayesian classi cation autoclass theory results 
fayyad piatetsky shapiro smyth uthurusamy editors advances knowledge discovery data mining 
mit press :10.1.1.117.3731
david cohn zoubin ghahramani michael jordan :10.1.1.18.5535:10.1.1.117.3731
active learning statistical models 
journal arti cial intelligence research :10.1.1.117.3731
michael collins yoram singer :10.1.1.114.3629
unsupervised models named entity classi cation 
proceedings emnlp :10.1.1.117.3731
thomas cover joy thomas :10.1.1.117.3731
elements information theory 
wiley series telecommunications 
wiley new york st edition :10.1.1.117.3731
csisz ar 
information geometry alternating minimization procedures 
editor statistics decisions pages :10.1.1.114.3629
oldenburg verlag munich :10.1.1.117.3731
dawid 
properties diagnostic data distributions 
biometrics :10.1.1.117.3731
virginia de sa 
learning classi cation unlabeled data 
cowan tesauro alspector editors advances neural information processing systems 
morgan kaufmann :10.1.1.117.3731
kristin bennett :10.1.1.33.4031
optimization approaches semisupervised learning 
ferris mangasarian pang editors applications algorithms complementarity 
kluwer academic publishers boston :10.1.1.114.3629
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society :10.1.1.117.3731
devroye gy lugosi 
probabilistic theory pattern recognition 
applications mathematics stochastic modelling applied probability 
springer st edition :10.1.1.117.3731
duda hart 
pattern recognition scene analysis 
wiley :10.1.1.117.3731
everitt 
latent variable models 
chapman hall :10.1.1.117.3731
freund schapire :10.1.1.32.4821:10.1.1.133.1040
experiments new boosting algorithm 
machine learning proceedings th international conference :10.1.1.117.3731
yoav freund seung eli shamir naftali tishby :10.1.1.20.8521:10.1.1.20.8521:10.1.1.32.4821:10.1.1.117.3731
selective sampling query committee algorithm 
machine learning :10.1.1.117.3731
mclachlan :10.1.1.32.4821
eciency linear discriminant function unclassi ed initial samples 
biometrika :10.1.1.117.3731
mclachlan :10.1.1.32.4821
small sample results linear discriminant function estimated mixture normal populations 
journal statistical computation simulation :10.1.1.117.3731
zoubin ghahramani geo rey hinton :10.1.1.32.4821
em algorithm mixtures factor analyzers 
technical report crg tr university toronto :10.1.1.117.3731
zoubin ghahramani michael jordan :10.1.1.32.4821
supervised learning incomplete data em approach 
cowan tesauro alspector editors advances neural information processing systems 
morgan kaufmann :10.1.1.117.3731
sally goldman yan zhou :10.1.1.32.4821:10.1.1.32.4821
enhancing supervised learning unlabeled data 
international joint conference machine learning :10.1.1.114.3629
green bernhard silverman :10.1.1.32.4821
nonparametric regression generalized linear models 
monographs statistics probability 
chapman hall :10.1.1.117.3731
robin hanson john stutz peter cheeseman :10.1.1.32.4821
bayesian classi cation theory 
technical report fia nasa ames research center :10.1.1.117.3731
hastie stuetzle :10.1.1.32.4821
principal curves 
journal american statistical association :10.1.1.117.3731
david haussler 
convolution kernels discrete structures 
technical report ucsc crl university california santa cruz july :10.1.1.117.3731
available www cse ucsc edu haussler pubs html 
geo rey hinton radford neal :10.1.1.117.3731
new view em algorithm justi es incremental variants 
jordan editor learning graphical models 
kluwer :10.1.1.117.3731
thomas hofmann 
learning similarity documents approach document retrieval categorization 
solla leen uller editors advances neural information processing systems pages :10.1.1.114.3629:10.1.1.117.3731
mit press :10.1.1.117.3731
tommi jaakkola david haussler 
fisher kernel method detect remote protein homologies 
proceedings ismb :10.1.1.117.3731
tommi jaakkola marina meila tony jebara 
maximum entropy discrimination 
advances neural information processing systems :10.1.1.117.3731
mit press :10.1.1.117.3731
tommi jaakkola marina meila tony jebara 
maximum entropy discrimination 
technical report mit massachusetts institute technology cambridge august :10.1.1.117.3731
available www ai mit edu tommi papers html 
tommi jaakkola david haussler :10.1.1.44.7709
exploiting generative models discriminative classi ers 
advances neural information processing systems :10.1.1.117.3731
toni jebara 
reversing jensen inequality 
advances neural information processing systems :10.1.1.114.3629:10.1.1.117.3731
toni jebara tommi jaakkola 
feature selection dualities maximum entropy discrimination 
proceedings uai :10.1.1.114.3629
thorsten joachims 
making large scale svm learning practical 
advances kernel methods 
mit press :10.1.1.117.3731
jordan jacobs :10.1.1.136.9119:10.1.1.136.9119
hierarchical mixtures experts em algorithm 
neural computation :10.1.1.117.3731
kearns vazirani :10.1.1.117.3731
computational learning theory 
mit press cambridge :10.1.1.117.3731
kirkpatrick gelatt :10.1.1.123.7607
optimization simulated annealing 
science :10.1.1.114.3629:10.1.1.117.3731
david mackay :10.1.1.31.4284
bayesian methods adaptive models 
phd thesis california institute technology :10.1.1.117.3731
david mackay 
gaussian processes 
technical report cavendish laboratories cambridge university :10.1.1.117.3731
available wol ra phy cam ac uk mackay readme html 
andrew mccallum kamal nigam :10.1.1.13.8629
employing em pool active learning text classi cation 
international joint conference machine learning :10.1.1.117.3731
nelder 
generalized linear models 
monographs statistics applied probability 
chapman hall st edition :10.1.1.117.3731
mclachlan 
iterative cation procedure constructing asymptotically optimal rule allocation discriminant analysis 
journal american statistical association :10.1.1.117.3731
mclachlan basford 
mixture models 
marcel dekker new york :10.1.1.117.3731
david miller hasan uyar 
mixture experts classi er learning labelled unlabelled data 
advances neural information processing systems pages :10.1.1.117.3731
mit press :10.1.1.117.3731
erik miller nicholas paul viola 
learning example shared densities transforms 
proceedings ieee conference computer vision pattern recognition cvpr :10.1.1.114.3629
thomas minka picard :10.1.1.117.3731
learning learn learning point sets 
unpublished manuscript 
available media mit edu papers learning html :10.1.1.117.3731
tom mitchell 
role unlabeled data supervised learning 
proceedings th international colloquium cognitive science :10.1.1.117.3731
michael murray john rice 
di erential geometry statistics 
number monographs statistics applied probability 
chapman hall st edition :10.1.1.117.3731
kamal nigam ghani 
understanding behaviour training 
submitted kdd workshop text mining :10.1.1.114.3629
kamal nigam andrew mccallum sebastian thrun tom mitchell 
text classi cation labeled unlabeled documents em 
proceedings national conference arti cial intelligence aaai :10.1.1.117.3731
omohundro 
family discovery 
advances neural information processing systems 
mit press :10.1.1.117.3731
neill 
normal discrimination unclassi ed observations 
journal american statistical association :10.1.1.117.3731
magnus rattray 
model distance clustering 
proceedings ijcnn :10.1.1.114.3629
redner walker 
mixture densities maximum likelihood em algorithm 
siam review :10.1.1.117.3731
brian ripley :10.1.1.51.9998
pattern recognition neural networks 
cambridge university press :10.1.1.117.3731
:10.1.1.51.9998:10.1.1.117.3731
latent variable models neural data analysis 
phd thesis california institute technology :10.1.1.117.3731
schapire yoram singer :10.1.1.51.9998
improved boosting algorithms con predictions 
proceedings th annual conference computational learning theory :10.1.1.117.3731
schmidhuber :10.1.1.51.9998:10.1.1.117.3731
learning learn learning strategies 
technical report technische universit unchen :10.1.1.117.3731
sch olkopf shawe taylor smola williamson :10.1.1.51.9998
kernel dependent support vector error bounds 
proceedings icann 
iee conference publications :10.1.1.117.3731
bernhard sch olkopf simard alexander smola vladimir vapnik :10.1.1.51.9998
prior knowledge support vector kernels 
advances neural information processing systems :10.1.1.117.3731
dale schuurmans :10.1.1.51.9998:10.1.1.51.9998
new metric approach model selection 
proceedings aaai :10.1.1.117.3731
dale schuurmans :10.1.1.51.9998
adaptive regularization criterion supervised learning 
international joint conference machine learning :10.1.1.114.3629
matthias seeger :10.1.1.51.9998
bayesian model selection support vector machines gaussian processes kernel classi ers 
advances neural information processing systems :10.1.1.117.3731
mit press :10.1.1.117.3731
matthias seeger :10.1.1.51.9998
annealed expectation maximization entropy projection 
available www dai ed ac uk seeger papers html :10.1.1.114.3629
matthias seeger :10.1.1.19.8785
covariance kernels bayesian generative models 
preparation 
check www dai ed ac uk seeger papers html :10.1.1.114.3629
matthias seeger :10.1.1.117.3731
input dependent regularization conditional density models 
submitted icml :10.1.1.114.3629:10.1.1.117.3731
available www dai ed ac uk seeger papers html :10.1.1.114.3629
seung opper sompolinsky :10.1.1.30.3233:10.1.1.30.3233
query committee 
proceedings th workshop colt pages 
morgan kaufmann :10.1.1.117.3731
shahshahani landgrebe 
ect unlabeled samples reducing small sample size problem mitigating hughes phenomenon 
ieee transactions geoscience remote sensing :10.1.1.117.3731
noam slonim naftali tishby :10.1.1.122.8863
agglomerative information bottleneck 
advances neural information processing systems :10.1.1.117.3731
mit press :10.1.1.117.3731
sebastian thrun 
learning th thing easier learning rst 
advances neural information processing systems 
mit press :10.1.1.117.3731
robert tibshirani geo rey hinton 
coaching variables regression classi cation 
technical report university toronto september :10.1.1.117.3731
tipping 
deriving cluster analytic distance functions gaussian mixture models 
proceedings th international conference ann 
iee london :10.1.1.117.3731
tipping bishop 
mixtures probabilistic principal component analyzers 
neural computation :10.1.1.117.3731
naftali tishby fernando pereira william bialek 
information bottleneck method 
proceedings th annual allerton conference communication control computing :10.1.1.117.3731
titterington smith makov 
statistical analysis finite mixture distributions 
wiley series probability mathematical statistics 
wiley st edition :10.1.1.117.3731
simon tong daphne koller :10.1.1.117.3731
restricted bayes optimal classi ers 
proceedings aaai pages :10.1.1.114.3629
ueda nakano 
deterministic annealing em algorithm 
neural networks :10.1.1.117.3731
ueda nakano ghahramani hinton 
algorithm mixture models 
advances neural information processing systems :10.1.1.117.3731
mit press :10.1.1.117.3731
vladimir vapnik 
estimation dependences empirical data 
springer series statistics 
springer st edition :10.1.1.117.3731
vladimir vapnik 
nature statistical learning theory 
springer :10.1.1.117.3731
vladimir vapnik 
statistical learning theory 
wiley :10.1.1.117.3731
grace wahba 
spline models observational data 
cbms nsf regional conference series 
siam :10.1.1.117.3731
waterhouse robinson 
classi cation hierarchical mixtures experts 
proceedings th ieee workshop neural networks signal processing pages :10.1.1.117.3731
christopher williams 
prediction gaussian processes linear regression linear prediction 
jordan editor learning graphical models 
kluwer :10.1.1.117.3731
christopher williams carl rasmussen :10.1.1.25.8841:10.1.1.117.3731
gaussian processes regression 
advances neural information processing systems 
mit press :10.1.1.117.3731
yuille stolorz :10.1.1.117.3731
statistical physics mixtures distributions em algorithm 
neural computation :10.1.1.117.3731
tong zhang frank oles :10.1.1.117.3731
probability analysis value unlabeled data classi cation problems 
international joint conference machine learning :10.1.1.114.3629
