continuous time hierarchical reinforcement learning mohammad cse msu edu sridhar mahadevan cse msu edu department computer science michigan state university east lansing mi usa hierarchical reinforcement learning rl general framework studies exploit structure actions tasks accelerate policy learning large domains 
prior hierarchical rl maxq method limited discrete time discounted reward decision process smdp model 
generalizes maxq method continuous time discounted average reward smdp models 
describe hierarchical reinforcement learning algorithms continuous time discounted reward maxq continuous time average reward maxq 
apply algorithms complex multiagent agv scheduling problem compare performance speed known agv scheduling heuristics 

hierarchical methods provide general framework scaling reinforcement learning problems large state spaces task action structure restrict space policies 
prior hierarchical rl including parr options sutton maxq dietterich limited discrete time discounted reward smdp model 
extends maxq hierarchical rl framework continuous time smdp model introduces new versions maxq continuous time discounted model continuous time average reward model 
average reward rl extensively studied discrete time mdp model schwartz mahadevan tadepalli ok continuous time smdp model mahadevan wang mahadevan prior limited algorithms 
proposed algorithms tested complex multiagent agv scheduling task 
rest organized follows 
section brie introduces continuous time smdp framework discounted average reward paradigms 
section describes maxq method automated guided vehicle agv task 
section illustrate continuous time discounted reward maxq continuous time average reward maxq algorithms respectively 
section presents experimental results proposed algorithms multiagent agv scheduling problem 
section summarizes discusses directions 

semi markov decision processes semi markov decision processes smdps useful modeling temporally extended actions 
extend discrete time mdp model aspects 
time modeled continuous entity decisions discrete points time events 
state system may change continually decisions mdps state changes due actions 
smdp de ned tuple nite set states set actions set state action dependent transition probabilities reward function function giving probability transition times state action pair 
js denotes probability action cause system transition state state transition decision epochs 
basically smdp represents snapshots system decision points called natural process describes evolution system times 
tjs probability decision epoch occurs time units agent chooses action state decision epoch 
compute js js tjs denotes probability system state decision epoch time units choosing action state decision epoch 
reward function smdps complex mdp model 
addition xed reward action state additional reward may accumulated rate time natural process remains state decision epochs 
formally expected reward decision epochs system state chooses action rst decision epoch expressed transition time second decision epoch denotes state natural process transition 
discounted models short overview nite horizon discounted semi markov decision processes puterman bradtke du 
assume continuoustime discounting rate means value reward unit received time units equals model policy denotes expected nite horizon discounted reward process occupies state rst decision epoch de ned dt expression represents times successive decision epochs transforms reward values rst decision epoch 
model expected discounted reward decision epochs de ned js dt equation value function equation dt js action value function represents discounted cumulative reward doing action state policy subsequently 
js average reward models theory nite horizon semi markov decision processes average reward criterion complex discounted models 
puterman mahadevan 
simplify exposition assume stationary policy embedded markov chain transition probability matrix 
assumption expected average reward stationary policy vary initial state 
policy state time denotes expected total reward generated process time system occupies state time de ned vu vu dug number decisions time model expected total reward decision epochs de ned js dt average expected reward gain policy state de ned limit inferior ratio expected total reward nth decision epoch expected total time nth epoch 
gain policy expressed ratio lim dt mdps gain policy state independent write transition expected transition time de ned dt js average reward smdps expected average adjusted sum rewards stationary policy de ned time decision epoch occurs 
bellman equation average reward smdps de ned function equation written js action value function represents average adjusted value doing action state policy subsequently 
js 
maxq framework continuous time hierarchical reinforcement learning algorithms introduced extensions maxq method discrete time hierarchical reinforcement learning dietterich 
approach involves graph store distributed value function 
task rst decomposed subtasks desired level detail task graph constructed 
illustrate idea agv scheduling task experimental testbed 
automated guided vehicles agvs exible manufacturing systems fms material handling 
fms system agvs faces problem optimally scheduling paths agvs system 
vehicle available multiple move requests queued decision needs request serviced vehicle 
agv scheduling requires dynamic dispatching rules dependent state system number parts bu er state agv processing going workstations 
system performance usually measured terms throughput number nished assemblies deposited unloading deck unit time 
shows layout factory environment 
parts type carried drop station machine assembled parts brought back warehouse 
task parallelized agv working 
note agents need learn skills 
subtask deliver material stations navigation perform ickup load action 
second agents need learn order subtasks go load station load part heading drop station machine 
agvs need learn coordinate agvs agv deliver part machine agv deliver nished assembly machine 
strength maxq framework serve substrate learning types skills 
unload parts warehouse load machine drop station pick station assemblies 
agv optimization task agv agents shown carry raw materials nished parts machines warehouse 
agv scheduling task decomposed subtasks task graph built 
task graph converted maxq graph shown gure 
maxq graph types nodes max nodes triangles nodes rectangles represent di erent actions done parents 
formally maxq method decomposes mdp set subtasks mn subtask tuple de ned termination predicate partitions state space set active states set terminal states policy subtask executed current state set actions performed achieve subtask actions primitive actions set primitive actions mdp subtasks 
pseudo reward function speci es pseudo reward transition terminal state pseudo reward tells desirable terminal states particular subtask 
primitive action primitive subtask maxq decomposition executable terminates immediately execution root da da dm dm dm dm load put nav load put forward left right left forward right navigate dropoff station navigate loading deck dai deliver assembly station max node node dmi deliver material station 
maxq graph agv scheduling task 
pseudo reward function uniformly zero 
projected value function value executing hierarchical policy starting state root hierarchy 
completion function expected cumulative discounted reward completing subtask invoking subroutine subtask state value function maxq method calculated decomposing parts value subtask independent parent task value completion task course depends parent task 
max composite js js primitive values values learned standard temporal di erence learning method sample trajectories see dietterich details important point note subtasks temporally extended time qlearning update rule smdp model puterman 
assume agent state doing task chooses subtask execute 
subtask terminate steps result state smdp learning rule update completion function max hierarchical policy set containing policy subtasks problem projected value function hierarchical case denoted value executing hierarchical policy starting state starting root task hierarchy 
recursively optimal policy mdp maxq decomposition fm mn hierarchical policy subtask corresponding policy optimal smdp de ned set states set actions state transition probability function js reward function sum original reward function js pseudo reward function 
maxq learning algorithm proven converge unique recursively optimal policy mdp maxq graph discounted nite horizon mdp discount factor maxq graph de ned subtasks fm mn 
continuous time discounted reward maxq algorithm center maxq method hierarchical reinforcement learning maxq value function decomposition 
show value function policy decomposed collection value functions individual subtasks continuoustime discounted reward model 
projected value function hierarchical policy subtask denoted expected cumulative discounted reward executing policies descendents starting state terminates 
value form continuous time discounted reward framework de ned equation 
suppose rst action chosen invoked executes number steps terminates state js 
rewrite equation rst summation right hand side equation discounted sum rewards executing subroutine starting state terminates words projected value function child task second term right hand side equation value current task discounted current state subroutine terminates sample transition time state state write equation form bellman equation js equation restated action value function decomposition follows js right term equation expected discounted cumulative reward completing task executing action state term called completion function denoted 
de nition express function recursively de nition composite js dt primitive formulas obtain update equations value function outside completion function inside completion function continuoustime discounted reward model 
pseudo code resulting algorithm shown algorithm notation algorithm algorithm abbreviation stochastic approximation update rule algorithm continuous time discounted reward maxq algorithm 
function maxq state seq fg sequence states visited transition times executing primitive execute action state observe state time units receive lump portion reward continuous portion reward rate push state transition time seq terminated choose action current exploration policy maxq sequence states visited transition times executing action observe result state argmax append front seq return seq maxq 
continuous time average reward maxq algorithm describe new average reward hierarchical reinforcement learning algorithm maxq framework 
simplify exposition assume possible stationary policy subtask hierarchy embedded markov chain transition probability matrix 
assumption subtask hierarchy smdp 
means expected average reward stationary policy subtask hierarchy vary initial state 
mentioned earlier value function decomposition heart maxq method 
show function policy decomposed collection functions individual subtasks continuoustime average reward maxq method 
projected function hierarchical policy subtask denoted average adjusted sum rewards earned policy policies descendents starting state terminates lim length decision epochs gain subtask respectively 
suppose rst action chosen invoked executes number steps terminates state js 
write equation form bellman equation js expected total reward decision epochs subtask system occupies state rst decision epoch decision maker chooses action expected length time decision epoch replacing expression equation written js restate equation action value function decomposition follows js equation term js denotes average adjusted reward completing task executing action state term called completion function denoted 
de nition express function recursively de nition composite js dt js tf primitive formulas obtain update equations function outside completion function inside completion function continuous time average reward model 
pseudo code resulting algorithm shown algorithm 
mentioned subtasks hierarchy primitive actions modeled smdp 

experimental results apply proposed continuous time algorithms agv scheduling task described section compare performance speed known agv scheduling heuristics 
experimental results generated model parameters 
agvs environment inter arrival time parts warehouse uniformly distributed mean sec variance sec 
percentage part part part part part arrival process algorithm continuous time average reward maxq algorithm 
function maxq state seq fg sequence states visited transition times reward executing primitive execute action state observe state time units receive lump portion reward continuous portion reward rate non random action update average reward gain subtask push state transition time reward seq terminated choose action current exploration policy maxq sequence states visited transition times executing action observe result state argmax non random action update average reward gain subtask append front seq return seq maxq respectively 
time required assembling various parts normally distributed means sec part part part part respectively variance sec 
time required primitive actions normally distributed 
experiment conducted times results averaged 
multiagent task extend proposed algorithms multiagent case approach introduced 
approach call cooperative maxq agent uses maxq hierarchy decompose task subtasks 
learning decentralized coordination skills agents learned joint actions highest level hierarchy 
nodes highest level hierarchy con gured represent joint action space multiple agents 
approach agent knows agents doing level high level subtasks unaware lower level actions 
idea allows agents learn coordination faster sharing information level subtasks attempting learn coordination account primitive joint state action values 
compares proposed maxq algorithms known agv scheduling rules showing clearly improved performance reinforcement learning methods 
seen gure agents learn little faster initially discounted maxq framework nal system throughput achieved average reward algorithm higher discounted reward case 
result consistent assumption undiscounted optimality framework appropriate cyclical tasks agv scheduling discounted framework 

describes new continuous time hierarchical rl algorithms maxq framework 
ectiveness algorithms demonstrated applying large scale multiagent agv scheduling problem 
rst algorithm extends original discrete time maxq algorithm continuous time discounted smdp model second algorithm extends maxq continuoustime average reward smdp model 
second algorithm rst hierarchical algorithm proposed knowledge deserves discussion 
particular maxq approach assumes subtasks terminate 
time start simulation sec highest queue heuristic nearest station heuristic come serve continuous time average reward maxq continuous time discounted reward maxq 
plot shows continuous time average reward discounted reward multiagent maxq algorithms outperform known widely industrial heuristics agv scheduling 
complete generality need consider case subtasks cyclical nonterminating just task agv problem 
way handle non terminating subtasks interruptions implemented options framework sutton 
alternatively imagine mixed mode framework subtasks optimized undiscounted total reward criterion parent task formulated average reward criterion 
practical theoretical issues remain unexplored research 
demonstrated proof convergence algorithms 
analyzing proof average reward extension maxq particularly interesting 
obvious manufacturing robotics problems bene maxq approach particularly multiagent case task level coordination greatly accelerate learning 
supported defense advanced research projects agency darpa contract 

bradtke du 

reinforcement learning methods continuous time markov decision problems 
tesauro touretzky leen eds advances neural information processing systems vol 

cambridge ma mit press 
dietterich 

hierarchical reinforcement learning maxq value function decomposition 
journal arti cial intelligence research 
mahadevan 

average reward reinforcement learning foundations algorithms empirical results 
machine learning 
mahadevan das 

self improving factory simulation continuous time average reward reinforcement learning 
proceedings fourteenth international conference machine learning pp 

mahadevan 

hierarchical multiagent reinforcement learning 
proceedings fifth international conference autonomous agents 
parr 

hierarchical control learning markov decision processes 
doctoral dissertation department computer science university california berkeley 
puterman 

markov decision processes 
new york usa wiley interscience 
schwartz 

reinforcement learning method maximizing undiscounted rewards 
proceedings tenth international conference machine learning pp 

sutton precup singh 

mdps semi mdps framework temporal abstraction reinforcement learning 
journal arti cial intelligence 
tadepalli ok 

auto exploratory average reward reinforcement learning 
proceedings thirteenth aaai pp 

wang mahadevan 

hierarchical optimization policy coupled semi markov decision processes 
proceedings sixteenth international conference machine learning pp 

