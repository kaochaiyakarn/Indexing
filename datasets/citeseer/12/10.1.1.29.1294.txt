estimating kernel fisher discriminant presence label noise neil lawrence neil net bernhard sch olkopf bs conclu de microsoft research st george house street cambridge cb nh data noise machine learning problems domains studied received attention 
propose algorithm constructing kernel fisher discriminant kfd training examples noisy labels 
approach allows associate example probability label flipped 
utilise expectation maximization em algorithm updating probabilities 
step uses class conditional probabilities estimated product kfd algorithm 
step updates flip probabilities determines parameters discriminant 
demonstrate feasibility approach real world data sets 

presence noise data common problem 
context supervised learning task noise may input data observed target data may take form regression target class label 
widely studied noise model independent gaussian noise regression target 
wish concentrate noisy class labels noise type may data mis labellings 
knowledge form model widely studied may norton hirsh guyon graepel herbrich 
section describe general form probabilistic model allows learning presence label noise 
purposes illustration consider specific fairly restrictive functional forms component distributions model 
section describes distributions parameters may optimised expectation maximisation em algorithm demonstrated toy data section 
extend representational power model section fitting probabilistic models high dimensional feature space kernel trick 
section section approach evaluated ocr data photographic image data set 

graphical representation underlying probabilistic model 
representation signifies joint distribution variables may factorised follows xjy 
section discuss limitations approach hope overcome works 
notationally choose represent discrete probability 
probability density function 

probabilistic model general approach propose model noise process probabilistically particular approach entire classification problem generative model model noise process component part model 
whilst thought traditional classification task purely discriminative approach neural network support vector machine usually effective casting problem generative model allows incorporate model label noise straightforward manner 
specifically assume joint distribution variables 
model may fully specified describing functional form probabilistic relationships 
firstly class conditional densities xjy simply modelled gaussian mean covariance xjy xjm secondly probabilistic relationship noisy observed class label actual class label conditional distribution specified probability table 
class classification table parameterised follows 
graphical representation underlying probabilistic model 
representation signifies joint distribution variables may factorised follows yj xjy 
note difference plot direction arrow reversed 

table implies data point xn yn probability true class true class 
final distribution need consider prior probability class definitions equations provide details require perform inference learning classification problem noisy labels 
convenient consider slightly different factorisation joint distribution implement algorithm 
whilst may intuitive sense model addition label noise described practical setting prior distribution parameter estimated 
typically priors estimated considering proportion data set class data set points belong class estimated labels noisy estimate sense 
easier directly estimate parameters 
making definitions obtain prior associated observed labels def fact rewrite factorisation joint distribution utilising affecting way model representative power 
conditional distribution yj parameterised 
shown summary model simple probabilistic model class conditional densities noise generating process 
consider learning occur model 

optimising model parameters process optimising generative model classification normally involves fitting class conditional densities xjy maximisation likelihood log likelihood function 
typically data set containing points fxn yn log likelihood objective function take form ln xn jy parameters class conditional density 
noisy label case reasonable fit perceived class conditional density xj similar manner 
ln xn determine xn marginalise associated latent variable yn marginalisation performed combination optimisation em algorithm dempster 
performing marginalisation explicitly optimise modified form log likelihood yjx ln yj yjx def 
entropy 
dropped index clarity notation 
bound equality yjx yjx 
maximisation note equation implies label noise independent different data points 
complex relationships conceivable situations warranted scope 
functional consists steps 
expectation step consists simply computation posterior distribution latent variable yjx yj xj setting yjx yjx bound equality 
maximisation step step optimisation function respect parameters 
model achieve differentiation rearranging form fixed point update equations yjx yn xn yjx yn yjx yn yjx yn yn yjx yn expected number points class update equations em algorithm similar em mixtures gaussians 
implement algorithm initialise parameters assuming labels noiseless computing covariances means gaussian distributions 
posterior probability data point may computed step update equations obtain new parameter values optimisation fixed point equations steps alternated convergence achieved 
section demonstrate approach simple toy data set 

toy problem create toy problem sampled gaussian distributions contains strong correlations contains weaker correlations 
data point times sampled weakly correlated gaussian strongly correlated gaussian 
total data points sampled 
sampled data shown dots weakly correlated gaussian crosses strongly correlated 
labels data set flipped probability form observed labels observed labels shown 
models trained data 
involved fitting gaussian class conditional distributions data modelling label noise 
gaussian distributions learnt model represented solid lines 
shown true underlying gaussians generated data dotted lines 
second model model label noise 
initialisation training model label noise initialised 
parameters gaussians initialised label noise 
training em algorithm considered converged log likelihood parameters changed convergence occurred iterations em algorithm 
results statistics learnt algorithm summarised table statistics data sets data points 
note increased statistics improve 
table 
examples different statistic estimates varying numbers data points true values statistics 
true discussion whilst results encouraging stress real world problems easily modelled toy problem proposed 
true class conditional densities captured gaussian model label noise estimated 
crucial approach complexity approximating distributions matched true underlying distributions 
gaussians clearly flexible fulfill criterion 
real world problems provide inaccurate estimates posterior yjx 
approximating distributions overly complex values estimated 
converse true values underestimated 
resolve issue utilise flexible model class conditional densities 
section achieve aim seeking model class conditional densities input data mapped high dimensional feature space input values directly 

data points sampled strongly weakly correlated gaussian 
observed labels associated datapoints depicted showing point dot cross 
show true labellings data 
noisy labels shown 

data models feature space section going kernel trick model data high dimensional feature space 
idea positive definite kernel corresponds mapping data feature space map dot product space large class kernels leading large class available feature spaces 
allows consider range complex models carries problems 
seek fit gaussian distributions mapped data general unable calculate likelihood data distributions 
reason kernel allows data implicitly terms dot products 
operations reduced dot products feasible scholkopf 
case exploit fact gaussians subjected certain constraints terms likelihoods cancel consider ratios likelihoods 
ratios type require perform em updates 
consider constraint imposed gaussian class conditional densities 
constrain covariance gaussians equal call class covariance matrix 
constraint update equation jx yn sight appears foolish thing 
constrained complexity model 
advantage model may easily 
absence label noise model recognised fisher discriminant fd fisher fukunaga 
authors mika roth steinhage shown fisher discriminant may 
main trick centres rayleigh coefficient bw known class covariance matrix performed feature space advantageous add small regularisation term denominator eqn equal multiple identity matrix kernel gram matrix mika 
fisher discriminant may computed maximisation rayleigh coefficient respect naturally standard version fisher discriminant means covariances posterior weighted versions shown step em version fisher discriminant merely involves substituting posterior weighted versions parameters 
optimum value shown proportional 
vector defines discriminating hyper plane 
fisher discriminant relies 
gaussians simply fitted data assuming label noise 
approach utilised learn label noise 
covariance gaussians generated data marked dotted ellipses 
lines ellipses represent eigenvalues eigenvectors covariance matrices 
lines cross mean generating gaussian 
learnt covariances represented solid ellipses 
note learnt covariances correspond true covariances better 
expanding direction discrimination nxn substituting rayleigh coefficient 
resulting matrices dot products replaced called gram matrix ij order perform algorithm feature space 
model introduces flexibility required model large variety class conditional densities 
complexity class conditional densities controlled varying parameters kernel matrix squared exponential kernel may vary width parameter 
preceding update equations proceeds straightforward manner similar standard kernel fisher discriminant kfd 
whilst likelihoods class conditional distribution xj may computable ratios likelihoods may computed determinant cancel 
may obtain necessary posterior probabilities eqn 
reason constrain class conditional variances equal 
problem algorithm increases complexity number input features 
shall see applications envisage may serious handicap 
moment seek fully resolve referred gaussian kernel 
workarounds possible considered see tipping 
issue 
simple sub sample matrix random manner reduce computational complexity 

method evaluation entire algorithm described turn evaluations approach 
artificial ocr problem purely illustrative 
second image understanding problem shows algorithm may utilised practice 
artificial digits problem understand better characteristics approach took data postal service cedar cd rom digits 
data pre processed digits gray scale images 
added artificial label noise data trained kfd squared exponential kernel exp kx decrease computation time constrained lie spaced spanned possible feature vectors define set sub sampled data points nxn reducing computational complexity 
implemented algorithm initialising kernel width proved effective previous works scholkopf data dimension 
way looking priori constrain zero 
standard kfd 
different models trained different sub samples kernel matrix 
label noise modelling results utilised maximum iterations em algorithm 
predictions test set label noise added 
results depicted region interest label noise shown larger scale 
label noise 
plot test set error vs induced label noise digit classification problem 
dotted line standard kfd solid line kfd label noise 
label noise 
detail test set error vs induced label noise plot noise 
dotted line standard kfd solid line kfd label noise 
whilst label noise modelling kfd perform better regions standard approach standard kfd slightly better label noise 
performance standard kfd drop fairly rapidly point whilst label noise modelling approach unaffected label noise 
real world image problem whilst digit problem improvement seen performance model large levels label noise performance standard kfd low levels label noise 
label noise product occasional mis labellings human expert operating central regions merit approach 
formulate classification task merits new algorithm mind 
consider image problem task label pixels sky sky 
standard machine learning approach problem create training set containing labelled examples sky pixels sky pixels 
creation training set human expert task 
task simplified expert merely say picture contains sky 
data labelled manner image image basis apply labels data pixel pixel basis constructed problem noisy labelling 
noise sky class 
noise sky class large 
proportion sky pictures containing sky typically 
whilst noise level greater learning possible class noiseless 
tested approach data set construction images taken corel gallery collection corel 
training set images containing sky images sky 
input data taken block pixels 
aim predict central pixel label true label considered sky central pixel thought sky portion picture 
pixels block utilised subsampled 
seventeen sub samples channels red green blue led input values data point 
full training data set 
arrangement pixels block 
black pixels utilised 
uniformly sub sampled obtain actual training data 
images pixels size led data points image 
uniformly spaced samples image leading total training set points 
sub sampling leads greater independence data points 
sub sampled kernels computational reasons 
computational reasons assumed priori solution lay space spanned possible data points 
squared exponential kernel width chosen labelled validation set images sampled manner training set 
performance algorithms evaluated labelled test set images pre processed similar manner validation training sets 
compared fd kfd label noise standard kfd algorithm 
trained standard fd kfd labelled version data set 
results summarised table 
table 
summary results image understanding problem 
table labelled data refers case training set labelled pixel wise human observer 
method test accuracy standard kfd fd label noise kfd label noise fd labelled data kfd labelled data fraction sky show original test images alongside corresponding images sky removed 
note linear approaches obtained respectively accuracy corresponding fully labelled data approaches effort human expert 

discussion summarise proposed algorithm performing classification presence noisy data labels 
initial results promising 
whilst appears little benefit utilising approach label noise small shown modelling noise allows perform sloppy labellings sky problem 
alternative approach type problem see keeler maron lozano 
issues remain approach 
standard cross validation techniques really sense model selection algorithm 
sky example validation set determine correct model complexity 
basic approach refined incorporating proportion accurately labelled data labelled data 
algorithm may utilised perform active learning cohn freund 
active learning aim select data labelling provide information classifier 
data algorithm uncertain posterior true class label close passed human expert accurate labelling 
approach unlabelled data case label noise initialised accordance class prior probabilities see roth steinhage 
mentioned earlier computational problems associated large data sets kernel methods 
focus resolving issues see example smola scholkopf 
authors mike tipping ralf herbrich helpful discussions 


generalized discriminant analysis kernel approach 
neural computation 
cohn ghahramani jordan 

active learning statistical models 
journal artificial intelligence research 
corel 
corel gallery magic 
www corel com 
corel gallery 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society 
fisher 

multiple measurements taxonomic problems 
annals 
reprinted contributions mathematical statistics john wiley new york 
freund seung shamir tishby 

selective sampling query committee algorithm 
machine learning 
fukunaga 

statistical pattern recognition 
san diego ca academic press 
nd edition 
graepel herbrich 

kernel gibbs sampler 
leen 
press 
guyon matic vapnik 

discovering informative patterns data cleaning 
fayyad piatetsky shapiro smyth uthurusamy eds advances knowledge discovery data mining 
cambridge ma mit press 

test image sky removed number corel gallery collection 

test image sky removed number corel gallery collection 
keeler rumelhart 

integrated segmentation recognition hand printed numerals 
advances neural information processing systems pp 

san mateo ca morgan kauffman 
leen dietterich tresp 
eds 

advances neural information processing systems vol 

cambridge ma mit press 
press 
maron lozano 

framework multiple instance learning 
advances neural information processing systems pp 

cambridge ma mit press 
mika ratsch weston scholkopf muller 

fisher discriminant analysis kernels 
neural networks signal processing ix pp 

ieee 
norton hirsh 

learning dnf probabilistic evidence combination 
proceedings international conference machine learning pp 

san fransisco ca morgan kauffman 
roth steinhage 

nonlinear discriminant analysis kernel functions 
advances neural information processing systems pp 

cambridge ma mit press 
scholkopf smola muller 

nonlinear component analysis kernel eigenvalue problem 
neural computation 
scholkopf sung burges girosi niyogi poggio vapnik 

comparing support vector machines gaussian kernels radial basis function classifiers 
ieee transactions signal processing 
smola scholkopf 

sparse greedy matrix approximation machine learning 
proceedings international conference machine learning pp 

san francisco ca morgan kauffman 
tipping 

sparse kernel principal component analysis 
leen 
press 
