drama connectionist architecture control learning autonomous robots billard hayes dept artificial intelligence university edinburgh forrest hill edinburgh eh ql dai ed ac uk contact tel fax published adaptive behavior journal vol 
jan 
drama connectionist architecture learning autonomous robots proposes connectionist architecture drama dynamic control learning autonomous robots 
drama stands dynamical recurrent associative memory architecture 
time delay recurrent neural network hebbian update rules 
allows learning spatio temporal regularities time series discrete sequences inputs face important amount noise 
part gives mathematical description architecture analyses theoretically numerical simulations performance 
second part reports implementation drama simulated physical robotic experiments 
training rehearsal drama architecture computationally fast inexpensive model particularly suitable controlling computationally challenged robots 
experiments basic hardware system limited computational capability show robot carry real time computation line learning relatively complex cognitive tasks 
experiments autonomous robots wander randomly fixed environment collecting information elements 
mutually associating information sensors actuators learn physical regularities underlying experience varying stimuli 
agents learn mutual interactions 
teacher learner scenario mutual agents enable transmission vocabulary robot 
keywords time delay recurrent neural network hebbian learning spatio temporal associations unsupervised dynamical learning autonomous robots 
adaptation considered fundamental capability survival living agents robotics research investigates artificial agents may benefit 
continuous learning important factor adaptation individual agent varying environment 
adaptation continuous learning opposed evolutionary adaptation particularly interesting autonomous artificial agents especially learning capabilities teaching agents new skills increasing priori knowledge 
proposes general framework control architecture autonomous agents combines continuous learning predefined abilities 
robotics studies tend developing learning model address particular problem sensor actuator coordination maze travelling spatial navigation exploration object manipulation direction control sensor actuator considered 
contrast approach tries develop single control architecture enables robot learn act independently specific task environment robot implementation 
look common constraints prerequisites learning dynamic noisy environment propose artificial neural network architecture learning spatio temporal regularities time series discrete sequences inputs 
learning hebbian associations robot sensor actuator modalities 
model differentiate actuator sensor information treats similarly associations 
correlations performed sensor sensor actuator sensor sensor actuator actuator actuator 
correlations determine robot actions sensor actuator order predict effect robot actions environment actuator sensor predict new sensor measurements current ones sensor sensor 
architecture time delay recurrent neural network hebbian update rules winner take neural activation function 
call drama dynamical recurrent associative memory architecture 
different algorithms enable learning autonomous mobile agents reinforcement learning genetic algorithms artificial neural network ann architectures 
neural network architectures interesting rl ga techniques require little knowledge task relying design evaluation function robot performance 
general problem ann learning methods developed previously computationally heavy enable line computation 
line computation fundamental requirement creating really autonomous agents 
case requirement quite difficult satisfy lego robots poor computational capacities 
robots provided micro controller byte space ks byte static ram 
cpu central processing unit phillips series compatible running mhz 
facilities floating points calculus done integers 
disadvantage learning techniques require numerous examples long training phase performing 
training robot testing performances separate phases 
undesirable considering continuous adaptation lifelong learning necessary qualities autonomous agents interact fast changing environments case environments occupied humans 
important skill robots possess expected interact agents human robotic ability communicate symbolically :10.1.1.45.8898
desiderata design control architecture drama architecture allows continuous learning control autonomous robots 
training rehearsal drama architecture require inexpensive fast computation allows computation carried line board robot 
report model implementation physical robotic experiments line learning spatial regularities time series robot perceptions 
experiments robots teacher robot learner robot interact dynamically environment 
hand learner robot learns spatio temporal regularities perceptions recognising landmarks learning locations objects recording time delays observation object 
hand learner robot taught teacher robot vocabulary label landmarks 
learner robot grounds teacher words sensor perceptions 
experiment agents share common vocabulary describe environment words grounded agent distinct set perceptions 
learning rehearsal training retrieval drama network performed continuously experiments order record robot observations direct robot actions respectively 
rest organised follows 
section gives mathematical description drama architecture section evaluates theoretically numerical simulations architecture performance 
section describes experimental set experiments carried test model section reports results 
section discusses performance drama architecture demonstrated experiments sections 
section concludes short summary main results 
learning model development drama architecture inspired model associative memory proposed willshaw model hippocampus 
development driven wish build control architecture enable real time control learning physical autonomous agent 
particular choice connectionist model especially hebbian associative memory driven considerations pertaining implementation real robot limited computational power 
require fast computation system react real time robustness adaptability face varying environmental constraints little built knowledge possible keep system unspecific particular type implementation task agent environment 
willshaw net original version willshaw net developed model biological associative memory 
thought fully connected network symmetrical connections weights updated basic hebbian rule weights connections active nodes reinforced 
patterns consist pairs input output bit strings 
patterns arrays binary inputs 
learning stage begins weights equal zero 
input output pair binary encoded input connection weight intersection node activated units input output node updated 
weight updated return zero 
recall memorised pattern done counting positive connection weights leading output unit 
input pattern net 
output column number positive variation environmental constraints experiments changes spatial distribution objects variation lighting electro magnetic field changes timing sequence measurements due variable speed travel robots 
connection weights corresponding input line counted 
output nodes number positive connection weights greater equal number active inputs activated 
willshaw net works badly noisy data net way distinguish nodes activated erroneously noisy data correct ones 
graham willshaw investigate different alternatives original retrieval function improve model capacity face noisy data sparse connectivity 
show knowing exact value unit usage frequency activation connection unit node pattern storage greatly improve robustness architecture 
model functioning human neural system assume biological network information 
contrast concern define artificial architecture associative memory robotic applications necessarily biological plausibility 
robustness important criterion improve defined update rule connection parameter exact record connection usage frequency correlated activation units kept 
call parameter confidence factor 
model similar willshaw kept basic principle training retrieval algorithms original model 
resulted statistical type network functioning mixture classical hebbian network willshaw network 
note willshaw network originally derived hebbian network 
full discussion functioning implementation reader may refer 
new extension describe adds recurrent connections nodes network order correlations delayed simultaneous occurrences different input patterns 
uniform structure original network changed fully recurrent non symmetrical network connections associated weight parameters recording separately spatial temporal features training patterns 
willshaw model time step training algorithm updating parameters hebbian rules winner take algorithm retrieval unit activity 
resulting model simple version recurrent neural network rnn compared rnn back propagation hidden layers satisfies basic requirements fast computation real time functioning temporal associative learning capabilities 
section discuss detail differences model rnn models 
section presents complete extended version developed original willshaw model 
organised follows give brief overview model functioning controlling learning behaviour robotic agents section give detailed description implementation architecture experiments report 
give mathematics drama architecture 
control learning process drama architecture provides general control architecture framework autonomous agents 
model implementation grounded robotics experiments description contain notions actuators sensor systems 
actuators experiments motors radio emitter 
sensors proprioceptive inclination sensor energy level checker compass light infra red detectors whiskers bumpers 
extension term sensor system sensor actuator systems differentiating unnecessary processing information independent type system provided 
shows schematic representation model sensory information drama architecture associative module sensor input compass sensory system event detector radio receiver emitter sensory system detector event motor left motor right sensor input actuator output output actuator schema robot control system drama architecture 
processed drama architecture learning activating actuators outputs 
structure system composed parts preprocessing module data event recognition drama architecture 
processing cycle vector state measured information processed event detector modules associated sensory system 
sensor actuator inputs arrays binary data bit strings different length system 
information sensor system treated separately event detector module event determined differently system 
sensor represented box input units number units associated sensor vary sensor 
variation sensor actuator input measured event novel information forwarded associative architecture drama correlated simultaneous previously recorded events systems 
drama fully recurrent neural network 
recurrent structure provides short term memory measurements 
long term memory obtained updating internal connections hebbian rules 
sensory systems interpreted motor compass radio systems robots 
experiments reported section radio signals associated different compass states providing agent vocabulary different words defined different radio encodings express direction movement 
robot actions determined retrieving activity network connections actuators particular sensor actuator state inverting motor speed 
note bidirectional associative memory drama notions input output interchangeable 
refer direction retrieval associations 
sensor actuator information input output depending information trigger result retrieval 
instance actuator state drama output determined retrieval sensor actuator association control robot actions drama input information calculate sensor state prediction robot perceptions 
data encoding mentioned previously sensor actuator state encoded bit string composed set smaller bit strings different length bit string sensory system 
discuss section model capacity decreases importantly pattern encodings overlap 
experiments tried encode sensory information orthogonal patterns possible 
example information provided compass encoded bit string length bit correspond quadrants 
bit corresponds angle degrees bit angle compass measurement represented pattern single bit activated 
representation data serves classification sensory information subclasses 
event recognition output drama architecture sensor binary input input units memory units threshold unit output units event detector module event detector module sensor 
module receives input units outputs associated units drama architecture number units particular sensor see 
neuronal representation internal structure module 
input unit connected memory unit output unit threshold unit 
output memory unit time simply value input unit time gamma gamma 
output th threshold unit result function applied difference input units memory units outputs th jx gamma function threshold function outputs state output unit calculated follows th jx gamma gamma threshold fixes minimal number unit inversions input activation 
example threshold unit fires soon input unit changed output unit outputs receives input value equal 
output vector equal input vector threshold unit fires 
short output vector event detector equal input vector sufficiently different previous input relative threshold minimal variation vector zero 
result event units detect changes unit activation reverse 
note orthogonal encoding sensor information event detector non zero 
associative module drama sensory system sensory system sensory system sensor information actuators state binary encoding drama architecture 
drama architecture consists network composed units number sensors system number input units associated sensor different sensor 
fully connected recurrent network non symmetrical connections unit connected units network self recurrent connections 
unit receives input output connection event recognition module 
hidden units 
note robotic experiments network fully connected level sensory systems units sensor connected units sensor units inside sensory system interconnected see 
done order save computation cost running system line reducing size weight matrixes number operations training retrieval network connections units inside sensor system improved learning performance experiments patterns inside system orthogonal inside connections updated 
equations network general case units interconnected 
section analyse network performance numerical simulations fully connected network 
tp ij cf ij yi xi sensor input tp cf ji ji input unit unit sensor xj yj left bidirectional connectivity network units 
right unit connectivity 
similarly time delay neural networks connection drama network parameters associated time parameter tp confidence factor cf see left 
time parameters confidence factors positive numbers real numbers simulation integers physical implementations 
record respectively time delay frequency units activation 
unit activation function output unit time function input time output gamma time gamma outputs gamma time gamma units see right 
real number value comprised 
equation 
output unit equal normalised sum input activation previous output activation decreased factor tp ii sum activation winning units units passed conditions encapsulated function tp ii delta gamma tp ji cf ji gamma transfer function identity function input value saturates value greater retrieving function equation equation explained paragraph 
indices notation equations interpreted follows cf ji confidence factor connection leading unit unit retrieval input unit activated activation propagated internal connections network units network 
unit active effect tp tp tp tp confidence factor winner take time delay cf propagation unit activity network connections 
activation unit function applied output unit equation value unit retrieving function depends value connections parameters tp ji cf ji output unit defined follows tp ji cf ji gamma tp ji delta cf ji tp ji gamma jy gamma gamma tp ji cf ji cf ji max cf ji max cf ji maximal value confidence factor connections activated units unit satisfy temporal condition encoded tp ji 
function threshold function outputs output function equal terms equal zero 
temporal spatial conditions represented terms paraphrased follows tp ji time delay activation unit memorised correlated activation unit time delay encoded value decreases linearly time new activation occurs see short term memory paragraph equal time encoded time parameter tp ji interval error cf ji confidence factors cf ji associated connection activated unit unit satisfies condition tp ji greater equal times maximum confidence factor activated connections max cf ji 
effect terms tp ji cf ji particular threshold memory capacity discussed sections algorithm calculating parameters line section 
shows schematic representation propagation unit activity network connection 
unit activity passes filter time represented factor equation activated time tp ji activates unit time tp passes threshold represented term winner take mechanism cf connection parameters 
short term memory self connections units network provide short term memory unit activation 
unit receives external activation input units outputs output activity equal tp ii delta gamma decreases ratio proportional temporal parameter tp ii value returns maximal decimal capacity system reached limit number processing cycles set keeping record unit activity fixing duration short term memory 
information sensor actuator triggered event detector memorised period fixed decrease activation self connections associated incoming event sensor system 
results system capable associating events delayed time maximal time delay equal length short term memory stm effect value success learning robotic experiments discussed earlier 
algorithm update line evaluated section 
sensor input event detector output drama units output time time transcription sensor input event detector output drama units output 
summary output unit network takes values event just detected ii sum activation provided units sufficient pass thresholds time confidence factor represented function 
value inferior represents memory past full activation value 
example tp ii gammat delta gamma delta means unit activated time steps decrease rate activation recurrent connections equal 
note experiments tp ii set equal value providing memory duration units 
coming back complete control architecture composed event detector module drama module section show transcription time steps sequence bits sensor input corresponding event detector output drama units output 
choose ratio decrease activation drama unit self connection equal 
diagrams right show shape unit activity sensor event detector drama units straight line activity unit dotted line activity unit 
training patterns associative memory connections active units updated hand confidence factors incremented represent structural correlation input patterns hand time parameters updated record temporal delay patterns occurrences 
connection parameters asymmetric associations directional 
training dynamic occurs time output event detector module activated 
input pattern memorised period cycles self connections see explanation section correlated patterns appearing period 
time parameters confidence factors updated hebbian rules unit activated output maximal recall unit output takes values afferent connections unit previously simultaneously activated units updated 
connections directed newly activated unit updated 
connections leading activated unit updated cycle unit fully activated 
update rules parameter equations 
tp ji tp ji gamma delta cf ji cf ji cf ji cf ji gamma time parameter tp records time delay activation units linked connections short term memory mechanism causes values decrease cycle factor explained short term memory paragraph ratio gives notion relative delay activation 
time parameter value calculated arithmetic mean value time delay training data value tp ii closer events bigger time parameter tp ij events simultaneous 
confidence factor keeps memory frequency pattern occurrence 
value incremented updating step fixed quantity experiment increase confidence factor linear fixed slope value experiment values confidence factors time parameters set connections apart self recurrent ones predefined values parameters determining duration short term memory unit activation 
order prevent confidence factor values large experiments values rescaled dividing factor reach value increase factor rescaled factor keep proportional increase time parameters 
interesting option slope proportional value cf ji confident greater increase 
speed learning may increase robustness model noisy data giving greater influence nodes activated see section general discussion robustness model 
unit unit unit unit time time time time retrieval training association retrieved activation cf cf tp tp example association unit unit 
left show example association unit 
unit activated time unit activated time 
association done time 
parameters connection unit activated unit activated cf tp updated 
association value cf tp zero correlation 
association cf increase factor equation tp activity level unit output decrease steps ratio decrease self connection equal 
right hand side show retrieval activity unit activating unit time 
condition factor equation unit reactivated time minus error time delay unit activation 
model capacity performances drama architecture functions associative memory associates pairs inputoutput patterns delayed time occurrence leading learning time series 
fully recurrent structure provides short term memory unit activation 
network input patterns recorded short delay associated new input pattern incoming delay 
time delay pattern occurrence structure unit activity pattern learned separately parameters attached network connection time parameter confidence factor 
inputoutput pair learned presentation input net retrieves output recorded time delay 
retrieval output units activity results winner take mechanism applied spatial temporal structure pattern input units activity 
capacity storage binary patterns time time delay input output patterns constant term equation equal retrieval function depends term applies spatial structure input units activity 
case model similar willshaw network 
correlated occurrences input patterns distinguished randomly generated ones keeping record frequency correlated activation patterns units increment confidence factor cf parameter connection 
pair input output patterns said correlated patterns associated minimal noise threshold corresponding threshold equation case 
correct retrieval pattern pair second depends proportion relative threshold correct noisy associations pattern patterns 
graham willshaw define network capacity number patterns stored bit error recall pattern output 
data binary inputs case model proportion active units overlapping training patterns important factor limits capacity associative memory discussed willshaw network 
evaluated drama maximal capacity equal number units network order network size 
result followed observation network trained input output pairs long pairs overlap input output patterns 
words unit activated pattern inputs outputs 
follows possible input output pairs patterns 
pointed capacity binary hebbian network winner take retrieval mechanism drama depends choosing correctly threshold activation case thresholds equation 
sufficiently high discard correlations connections update due spurious unit activity sufficiently low allow retrieval patterns different frequency activation different values confidence factor connections patterns activated units 
sufficiently large include maximal variation time delay units activation sufficiently small allow precise prediction time unit activation 
estimating correctly percentage noise spurious unit activity system correct values learning possible especially unsupervised learning robotic experiments 
algorithm calculate values thresholds simultaneously training network 
idea information ratio spurious relevant units activation reflected current values network connection parameters 
calculation assumption incorrectly updated connections lowest confidence factor values frequently updated important variation time parameter values regularity time delay units activation 
thresholds values calculated gaussian estimation distribution parameter values correct incorrect connections equations 
max fy cf ij delta mean fy cf ij gamma max fy cf ij min fy cf ij mean ij ij ij ij gamma delta cf ij ij gamma cf ij max fy cf ij mean fy cf ij min fy cf ij maximum mean minimum values confidence factor time parameter mean ij activated units time retrieval 
ij equation represents maximal variation factor tp ij activated units recorded training 
noise ta ta tp recall performances units network different values noise different threshold strategies 
shows result numerical simulations evaluate network recall performance overlap recalled trained patterns mean value patterns network units trained maximum capacity varying percentage noise frequency random activation unit training 
overlap means patterns perfectly retrieved 
compare recall performance threshold strategies threshold time parameter tp factor equation fixed threshold cf threshold time time variant threshold cf equation time variant thresholds cf tp tp equation 
results show pattern recall perfect proportion noise performance decreases minimal proportion percent overlap retrieved training data third strategy 
performance average better time variant strategies allows better recall bigger proportion noise 
especially better introducing threshold time 
capacity storage binary patterns time note time parameter improves greatly capacity network compared case uses confidence factor parameter see discussion previous paragraph 
determined network capacity equal delta gamma number network units 
maximal capacity network complete retrieval function see equation order network size 
reasoning observation supplementary information time parameter allowed distinguish patterns involving unit pairing 
words unit network paired units apart previous case paired unit 
space time efficiency important characteristic drama architecture computationally fast inexpensive 
evaluate formally mean 
training network requires number time steps computation equal times number connections update connection parameters cf tp 
faster usual backpropagation algorithm requires times number connections number steps needed reduce error minimum bigger 
retrieval unit activity equation relatively fast require calculation derivatives needs time steps computation 
retrieval requires summing twice units order calculate mean maximal minimal values cf tp parameters determine values threshold factors sum vote activation unit 
time efficiency neural network model counterbalanced poor space efficiency 
decrease model capacity front overlapping pattern encoding leads poor space efficiency see section 
maximal number patterns network units trained equal number inferior maximal number combinations formed units equal 
space efficiency determined number global variables network 
drama architecture requires space times number connections store parameters associated connections network 
higher number global variables required nn parameter connection 
sequence learning previous section discussed model performance associating pairs input output patterns 
consider model performance learning sequences patterns 
discussion consider variation drama retrieval algorithm described section 
addition conditions confidence factor time parameters encapsulated function equation add third condition requires units activated time retrieval agree activation output unit activated 
condition similar winner take algorithm original version willshaw network see section 
version allows point facts duration sequence learned network restricted short term memory transitivity associations sequence steps derived association shorter consecutive sequences sequence restricted include strictly different patterns composed occurrences pattern sequence repetition subpattern cd 
second result due third condition requires agreement voting units activation output pattern 
consider sequence example timing short term memory duration equal steps associations activation patterns activation subgroup cd determined vote patterns respectively addition votes correct retrieval sequences loop patterns subgroup patterns creates specific conditions patterns structure timing sequence 
training sequence form learning successful 
memory duration long allow association patterns preceding subsequence sequence loop example 

number occurrences subpattern sequence inferior threshold confidence factors activated example ncd cf cf cd 
pattern subgroup patterns sequence loops occur sequence pattern previous loop necessary determine activation correct subsequent pattern 
note learning sequence internal loop equivalent learning sequences common subpattern 
previous results imply number sequences learned restricted long structure temporal pattern occurrence satisfy previously mentioned conditions learning algorithm short term memory parameter previous sections pointed importance correctly choosing values learning parameters system determined bounds values relative structure training patterns percentage noise system 
mentioned earlier seldom case access information learning 
desirable define learning algorithm tuning parameters line associative learning process 
section gave possible example calculate line values thresholds updated line algorithm delta gamma predicted activation unit updated error time settled small value 
increased decreased depending unpredicted unit activations zero activity prediction measuring unit activation incorrectly predicted unit activations non zero activity prediction measuring unit activation 
performance learning algorithm evaluated simulation learning types sequences abcde different noise proportions variation time delay activation pattern different starting value memory duration times lower bigger correct 
results showed algorithm converged trials respect sequence noise 
statistical fluctuations correct value observed learning sequence noise 
top shows variation parameters error number incorrect prediction learning sequence noise variation timing pattern occurrence input 
convergence parameters values achieved presentations sequence error equal zero 
time threshold settled minimal value equal maximal variation timing pattern occurrence noise value short term memory settled value comprised minimal maximal value required conditions section 
bottom shows superposed plots retrieved straight line training dotted line pattern activation learning sequence noise 
min max time threshold noise teaching number training cycles nm processing cycles patterns training cycle nm processing cycles patterns top variation short term memory error time threshold training sequence 
bottom superposed plots retrieved straight line training dotted line pattern activation training shows snapshots left cycles training right th training cycle convergence 
shows snapshots left cycles training convergence right th training cycle convergence 
retrieved pattern activity begins second cycle pattern incorrect third cycle patterns activated twice 
retrieved training pattern activity match patterns apart th cycle 
pattern activation pattern retrieval series pattern retrieved correlated pattern time lag series pattern new pattern long associated 
retrieved activations patterns occur slightly earlier training ones margin noise variation time delay activation pattern 
summary principal properties drama architecture summarised follows model structure functioning consists fully connected network self connections unit hidden units 
connection network associated parameters time parameter confidence factor 
structure network dynamically updated time unit activated external input see table complete description learning algorithm 
time parameters confidence factors updated hebbian rules providing associative type learning time parameters record time delay units activation confidence factors keep memory frequency units activation 
self connections units provide short term memory activation unit duration memory fixed ratio decrease activation recurrent connection 
short term memory unit activation enables associations patterns unit activation delayed time leads transitivity associations learning sequences patterns unit activation 
data retrieval depends value time parameters confidence factors associated connections act separate filters spatial temporal features input output units activated conditions satisfied time delay input time occurrence equal memorised temporal correlation ii confidence factor values active input units greater fixed percentage maximal value confidence factor active units network time retrieval 
model performances capacity network decreases importantly overlapping encoding data orthogonal encoding preferable possible leading poor space efficiency 
maximal capacity network order size storing pairs input output patterns fixed delay order size storing pairs variable time delay 
full capacity network retrieval performance perfect proportion noisy data performance decreases minimal proportion overlap retrieved training data 
model learn sequences pattern activation types abcdef steps sequence duration longer short term memory derived transitivity associations association shorter consecutive sequences ii sequence composed occurrences pattern repetition subpattern cd occurrences pattern 
retrieval depends correctly choosing values learning parameters short term memory duration threshold time parameter threshold confidence table training algorithm input system 
compute output corresponding event detector equation 
compute output units drama network equation 
update connection parameters drama network units drama network update parameters cf ji tp ji connection unit equations 
update learning parameters equations 
factor parameter values depends proportion noise imprecise sequence timing spurious unit activation sequence type theoretical boundaries determined parameters 
algorithm tuning learning parameters simultaneously updating network connection parameters defined validated numerical simulations 
training uses time step algorithm model computationally fast inexpensive allows implementation real time computation line learning basic hardware system 
describe implementation sections 
model application robotics experiments drama architecture provides general framework control architecture autonomous robot 
allows line learning spatio temporal regularities multiple sensor actuator modalities robot learning time series sensor actuator actuator sensor actuator actuator inputs 
provides dynamic control robot behaviour retrieval learned predefined sensor actuator sequences 
basic behaviours determined fixing connections drama network specific sensor actuator systems robot 
drama architecture general sense structure functioning prerequisites type robots robot sensors actuators body structure 
report sets experiments study aspect architecture capacity spatial association multiple sensor modalities experiment capacity learning time series sensor stimuli second experiment 
experiments carried autonomous mobile robots teacher robot learner robot simulated physical environments 
robots controlled drama architecture 
simulation studies carried order determine feasibility experiments studying successes failures reliable environment 
results physical experiments compared simulations 
experimental procedure describe experimental procedures experiment separately 
labelling landmarks experiment learner robot learns distinguish different objects attaching different labels names 
robot taught teacher robot 
teaching occurs part teacher learner scenario imitative strategy mutual agents agents wander randomly environment teacher sends signals words describe novel perceptions recognition objects 
learner attaches meaning teacher signals terms perception object features 
teaching provided robot simulations human instructor holds lamp robot follows physical experiment 
top show graphical representation simulated environment picture physical experiment 
experimental procedure experiment consists letting robots wander environment time period 
exploration learner robot learns map environment set landmarks associating set features representing object particular location 
simulated environment composed hills boxes 
object defined unique set features hills inclination different colours third hill different inclination colour second 
boxes different colours shape 
simulated robots perceive objects features light detectors colours infra red detectors shape inclination sensors 
boxes distinguished walls arena shape upper infra red detector response signals box walls produce response lower 
robots locate objects environment relative polar coordinates 
processing cycle calculate position relative middle arena terms distance measure internal energy sensor angle measure compass 
physical environment composed boxes robots perceive side whiskers foil lying ground robots perceive light detectors fixed chassis 
learning time series perceptions second experiment learner robot learns time series sensor measurements travelling highly regular environment 
environment consists series inter connected corridors right angles corridors delimited walls side 
middle second corridor lies plate robot detect light detectors 
bottom show graphical representation simulated environment picture physical experiment 
experiment consists letting robots travel times corridors 
robots follow closely teacher front 
run robots travel times series corridors 
travelling corridors robots perceive different light compass measurements crossing plate robots perceive increase light intensity measurement light detectors carry underneath body 
corridors placed right angles travelling corridor corresponds measuring compass value refers different quadrant call south west north 
scenario results follower agent implicitly imitating replicating followed agent movement plane 
note robots measure distorted component earth magnetic field due noisy magnetic addition learner robot perceives different radio signals travelling corridors 
signals sent teacher robot represent labels compass measurements third corridors signals south north increase light measured second corridor crossing plate signal object 
assuming robots travel corridors average speed circling expect learner robot perceive series sensor stimuli travelling corridors radio signal south compass measurement south radio signal object compass measurement west measurement light increase radio signal north compass measurement north 
unsupervised learning strategy experiments learner robot learning 
teacher robot role consists directing learner robot learner teaching learner robot vocabulary label objects teacher sending radio signals time encounters object learner robot attaching teacher signals perception objects 
teacher robot knowledge vocabulary predefined correlations radio signals corresponding objects features set start teacher drama network see explanations section 
robots mutual results phototaxis behaviour robot carries bright light light detectors 
robots sufficiently close detect random wandering align teacher front 
go wandering environment 
stops robot distracted avoiding complicated set obstacles attracted bright point environment windows 
time teacher robot perceives objects hill box plate emits corresponding radio signal 
learner robot grounds gives meaning teacher signals associating observations sensor measurements 
experiment teacher robot teaches vocabulary words simulation physical experiment respectively word object environment 
word different radio signal associated particular sensor combination describes features corresponding objects box colour shape hill inclination colour 
second experiment learner robot learns words vocabulary words different compass measurements north south word label plate second corridor 
bounded process learner teacher agents set position share common set perceptions 
share similar view environment face direction similar identical due different sensor sensitivity set external perceptions 
share similar internal perceptions travel path similar energy consumption inclination 
implicit similarity agents perceptions enables learner sense teacher words teacher talks senses aware learner perceptions 
unsupervised teaching strategy 
spatial displacement agents due teacher learner observations landmarks delayed relative 
follower perceptions similar leader follower travelled distance body length reaches previously leader position 
learning successful drama architecture associates simultaneous emissions laboratory machines labels south west north correspond usual meaning 
simulated left physical right environments top second bottom experiments 
table table robots sensors sensitivity information encoding physical simulated experiments 
physical exp simulations sensor type sensitivity encoding sensitivity encoding bumpers cm touch contact active inactive bit whiskers cm lateral active inactive bit infra red cm levels bits cm quadrants bits light front gamma lux levels sensor bits cm quadrants bits colours types bits light incl 
gamma lux levels bits levels bits radio mhz arena bits arena bits delayed successive sensory stimuli 
observations landmark features extracted continuous flow sensor measurements event detector modules see section produce change robot measurements inclination light infra red compass energy level sensors 
new incoming sensor information memorised fixed period time corresponds travelling twice robot body length experiment half corridors length second associated events measured sensor system 
experiments learner agent teacher radio signals perceived novel radio stimulus receives earlier corresponding sensor observation associates sensor actuator events measures period time signals occurrence equal memory duration 
set autonomous lego robots teacher learner real experiments 
robot equipped frontal infra red sensor bumper avoid obstacles see pictures right 
sets light detectors set front learner back teacher follow set underneath chassis detect covered region arena 
addition carry compass measures bearings degrees 
range sensitivity sensors table 
radio transceiver means transmission communication signals 
signal encoded byte bit activated north south 
arena consists rectangular cage robots continuously similarly system bumper cars game 
roof bottom arena creating potential difference 
robots carry long stick touching ends cage receive current power battery light bulb 
picture top right see learner robot front covered area box cardboard 
picture bottom right see teacher robot followed learner robot managing corner second corridor 
estimate sensor measurements noisy radio transmissions correctly received signal received perfect noise corresponds case emitted signal received 
quadrants compass correctly detected cases observe influence magnetic fields produced motors powering cage 
foil boxes correctly detected cases limited sensitivity light detectors whiskers 
simulation studies carried simulator graphical representation matlab environment 
simulated environment consists experiment rectangular arena measuring units see top left second experiment series corridors bottom left 
graphical representation simulator figures left robots represented rectangles units triangle indicating front hills boxes represented big rectangles sides arena small squares middle respectively 
top left see teacher robot followed learner robot moving hill 
simulated robots provided colour vision colours infra red vision see walls detect boxes inclination sensor detect hilly region equivalent light detectors underneath robots body physical implementation 
carry radio transceiver communicate compass measures bearing degrees energy sensor gives relative measure travelled distance 
energy sensor value incremented cycle factor proportional robot speed note robot speed varies depending robot crosses plane hill speed slowed accelerated climbing hill 
infra red light detectors associated cone vision degrees segmented quadrants 
measurements sensors bit string bit corresponds values measured quadrants infra red stands infra red activation quadrants 
range sensitivity sensors table 
robots behaviours calculated simulation routines physical robots network drama architecture robot retrieving updating functions equations applied sequentially determining behaviour learning robots independently 
code written processed serially order produce realistic simulation behaviour imperfect 
mutual agent aligns basis light measurement 
similarly happens reality agent able determine position agent respect precision degrees 
alignment robots imprecise results differences agents perceptions 
accounts noise incorrect teaching observation associations occur physical experiments 
happens physical reality simulate imprecision sensor measurements vary external conditions intensity light infra red emissions course experiments effect observed real world 
randomness introduced calculation robot movements order represent imprecision measured real robot movements 
addition experiment robots direction movement reset random value cycles complete cycle arena order cover space homogeneously approach object different directions avoid cyclic behaviour 
real world occurs naturally effect light variation resulting robots suddenly attracted room corners loose sight wander randomly directions 
internal processing gives schematic representation processing sensor actuator information drama architecture 
restrict schema sensor systems radio motors compass infra red sensor reasons clarity picture 
simulations sensor systems radio motors compass inclination ir light colours energy 
processing programs simulations run ultra model 
motor left motor right active forward reverse compass 
radio transducer obj obj object detection predefined connections connections learned external teaching connections learned self organisation object wall detection full half speed infra red sensors drama connections state experiments 
cycle drama network output robot motors calculated order determine motor activity defined activity drama motor units 
motor activity encoded bit string 
bit determines state activity motor active bit encodes direction forward reverse third bit determines speed full half 
basic behaviours obstacle avoidance mutual robots phototaxis light detectors predefined setting connection parameters confidence factors time parameters infra red ir light detector systems motor system 
order perform purely reactive behaviour thresholds event detector modules ir light sensor systems set zero 
motor activity results winner take retrieval mechanism applied inputs sensors 
shows variation activity units corresponding left right motors compass light infra red radio sensors processing cycles sensory system fact represented unit represent maximal activation units corresponding system 
observe activation infra red detector unit times produce immediate deactivation right motor 
robot turns left faces obstacle predefined setting values connection parameters 
result robot rotation new value compass measured time 
light detection correspond detect second robot time produce deactivation left right motors alternatively 
result robot aligns robot 
teacher learner behaviours controlled set predefined basic behaviours 
learning mechanism teacher knowledge vocabulary defined setting connections radio sensor words radio signals sensors correspond sensor measurement words describe 
learner robot uses input output radio module receive teacher signals emit answer corresponds retrieval learned radio signal current sensor measurements recorded experimenter order evaluate progress robot learning experiment 
teacher robot uses output radio emitter send signals learner robot 
learner robot answer teacher robot check efficiency teaching 
reason teaching experiments completely unsupervised 
similarly done motor activation teacher robot ability emit radio signals speaking teaching results retrieving output radio sensor system robot current sensor motor state 
teacher speaks sees learner 
inhibition activation radio output achieved giving high value confidence factor connections light detector units learner recognition radio output units 
input connection long learner view zero 
learner view activation light units wins competition activation high confidence factor value inhibiting activation sensor units 
result radio units activated definition produce output 
radio compass compass ir light left right number cycles variation activity units corresponding left right motor compass light inclination sensors processing cycles 
learning occurs unit sensor system newly activated output event detector associated sensor system activated 
time parameters confidence factors network connections linking previously simultaneously activated units newly activated unit updated equations section 
time parameters give measure mean time delay consecutive activation units confidence factors record frequency activation units 
table comparison results simulations physical experiments 
experiment experiment simulations 
exp simulations 
exp mean std mean std mean std mean std sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma drama network keeps memory unit activation fixed time delay determined rate activation decrease units self connection 
example unit activation conserved cycles effect recurrent connections similarly done simulation experiment 
level activity decreases ratio cycle 
see decrease activation radio compass units 
motor infra red units decrease constantly maximally activated new input event detection sensor systems 
radio unit activated cycles associated activation compass unit just deactivation 
bidirectional associations simultaneous sequential sensory activations lead learning vocabulary radio bit strings associated particular sensor activities 
simultaneously associations occur sensory stimuli showing physical regularities environment associating objects features location terms compass energy measurements 
results section report results experiments simultaneously comparing results simulation studies physical experiments 
part evaluate speed stability learning variation learning parameter values experiments 
second part show success sequence learning second experiment running line rehearsal sequence 
third part study influence winner take threshold confidence factors determination learning success experiment 
speed stability learning hebbian type associative memory success learning measured time step ratio connection parameters values 
presentation new example new teaching connection parameters confidence factor time parameter updated 
study parameter values variation experiment inform variation percentage noise consecutively stability learning 
noise case incorrect matching sensor information due noisy sensor measurements hardware imperfection learner teacher agents making different observations observing different events 
agents imperfect bad alignment respective measures compass direction instance differ 
happen object teacher field view cone learner 
sets simulated runs carried second experiment respectively 
experiment run simulated processing cycles robots hours real experiment 
run robots started different position randomly generated close aligned far apart 
prevent bias experiments allowed homogeneous covering space adding randomness robots travelling word taught number times object approached different directions 
second experiment run started robots aligned placed bottom left entrance series corridors 
run consisted robots travelling times corridors time direction 
sets physical experiments carried reproduce simulated experiments runs set 
table gives set experiments mean value standard deviation ratio confidence factor cf values associated correct word maximal value cf attached words sensor measurement cf cf max cf number words learned run number run 
observe experiments confidence factor ratio greater means correct correlations average incorrect ones learning successful 
addition learning stable standard deviation ratio small keeping value threshold 
learning complete vocabulary successful second experiment 
due fact experiment robots object run making 
especially case simulation covering space depends randomness robots behaviour physical experiments human observer force robots cover space attracting external light 
note fact vocabulary completely learned due unsuccessful learning absence teaching particular objects update connection parameters corresponding radio unit 
second experiment happen robots path constrained corridors forces perceive time expected stimuli 
shows variation time parameters values correct connection radio units signal corresponding sensor measurement object run 
data represent mean value runs 
left right figures show result simulation studies physical experiments 
expected observe values time parameter experiment top stabilise significant fluctuations times bigger experiment outside values graphic measured experiment 
vocabulary learned regularity time occurrence consecutive sensor perception 
opposition second experiment bottom values stabilise quickly half run 
experimental results right similar simulations showing quick stabilization values experiment continuous fluctuations experiment 
sequence rehearsal quantitative qualitative comparison connection parameter values run able assess previous section success learning second experiment 
observed robot expected correlations different sensor measurements correct correlations radio signals different compass light measurements demonstrated ratio confidence factors remaining threshold see table correctly recorded number object object hill hill hill number obj obj obj nm south object north nm south object north variation time parameter values correct connection signal object 
temporal correlation specific sensor measurements compass light sensors demonstrated stabilization time parameter values 
remains demonstrate robot learned correct sequence stimuli learned correct timing sensor measurement occurrence 
signal signal signal object compass south west north east number cycles signal signal signal object compass south west north east number cycles retrieval sequence sensor measurement radio signal south signal start activation simulated left physical right experiments 
demonstrate running line rehearsal sequence measurements radio signal south starting activation signal south supposed sensor measurement robot perceives entering corridor 
rehearsal consists retrieving network units outputs cycles corresponds time needed robot circle corridors minutes starting units input output set zero apart input third radio sensor unit set unit corresponds signal south 
rehearsal network done values connection parameters obtained run highest ratio word learning success 
results show starting activation radio unit signal south followed sequence unit output activations radio compass light sensors 
shows sequence sensor measurements results rehearsal process applied network parameters obtained simulation left physical experiment right 
observe cases radio signal south retrieves successively compass value south st corridor compass value west nd corridor radio signal object signal patch plate physical experiment lying middle second corridor radio signal north signal corresponding value compass 
demonstrates expected stimuli sequences see chapter correctly learned simulations physical experiments 
exact time delay terms processing cycles retrieval stimulus differs simulation physical experiments 
simulated set quite similar experimental proportion corridor length robots size position dimension patch environments differ 
robots wandering corridors real environment chaotic varies significantly run 
addition radio reception perfect reality results significant variation time delay reception signal measure corresponding sensor stimuli facts account observed differences time delay sequence rehearsal simulated physical experiments 
retrieval threshold discussing model capacity section pointed importance threshold parameter determining success learning 
value determines minimal ratio values confidence factors correctly incorrectly updated connections 
learning unsuccessful percentage noisy examples incorrect update connections exceeds times total number examples case function equation output applied correctly updated connections preventing correct association retrieved 
value needs carefully chosen account estimation percentage experimental noise 
table left show confidence factor values run mean values runs simulation studies experiment 
observe associations radio units unit defines different word boxes hills features spurious 
instance radio unit standing hill correlated combination features box colour shape hill colour inclination different values confidence factor 
lowering threshold confidence factor effect allowing retrieval combinations features just hill presenting radio signal hill 
table right show effect varying value determination correlation success results 
features color color shape shape incl incl box box hill hill hill 



left confidence factor values connections radio units unit word words hills boxes colours shape inclination sensors units hills boxes features 
right success signal object correlation different values threshold stand correct incorrect correlation radio sensor box box hill hill hill 
columns show results learning direction association radio sensor object description vice versa 
mentioned previously estimate proportion experimental noise due hardware imperfection add noise due imprecise agents resulting incorrect matchings sensor perceptions 
threshold allows correct retrieval data face maximum noisy data 
expected value threshold correct correlations object corresponding word vice versa correctly retrieved 
lowering threshold spurious correlation retrieved 
threshold radio signal hill retrieves sensor features box hill threshold teacher sends signal times stimulus order compensate loss reception variation time delay arises learner catching latest signals 
combination features hill retrieves signals hill box 
hand restrictive threshold high means correlations longer retrieved radio signals box hill hill longer retrieve full set features corresponding elements 
discussion important part sections describe drama dynamical recurrent associative memory architecture architecture developed allow learning spatio temporal regularities autonomous robot 
model consists fully recurrent neural network hidden units uses hebbian update rules 
similarly time delay networks uses weight parameters connection record separately time delay frequency input patterns occurrence 
drama network differs structurally functionally similar ann models main aspects opposition recurrent neural networks unsupervised learning algorithm uses hebbian rules 
contrast associative memory models hebbian networks connections network associated parameters order keep separate record spatial temporal structure input patterns 
particular temporal parameter allows recording real time occurrence pattern 
section analysed theoretically numerical simulations properties model 
model shown cope capacity remains maximal noise noise corresponded likelihood spurious unit activation presenting training patterns variation time delay consecutive activation input output units pattern 
model shown able learn time series inputs series overlap inputs learn time series series similar inputs different ordering occurrence 
training retrieval algorithms time step model computationally fast inexpensive run allows implementation line learning computationally limited robot 
advantages model terms quick easy computation discussed section 
discuss general properties model comparison associative memory models recurrent neural networks similar properties 
model associative memory drama architecture characteristics common associative memory models hebbian networks uses similar training algorithm hebbian rules similar retrieval algorithm winner take 
similarly hebbian networks binary encoding patterns capacity drama model decreases trained patterns encoding overlaps patterns common units active 
section compared drama model willshaw network special case hebbian neural network inspired 
addition simulation studies showed decrease drama network capacity face noisy data graceful remains maximal capacity noise improvement compared willshaw network decreases maximal capacity face minimum noise 
drama architecture differs hebbian networks mainly recurrent structure self connections units bidirectional asymmetric connections units models unidirectional symmetric connections 
recurrent connections introduce short term memory units activity allows association temporally delayed unit activations time delay units activation unspecified remains margin short term memory duration 
transitivity associations time series unit activations learned 
retrieval associations unit activates correlated unit correct time delay passed 
property drama introduce explicitly time connection parameters time parameter distinguishes significantly models associative memory 
associative memory models learn sequences patterns exist 
models time delay pattern occurrence fixed equal processing cycle intrinsic relationship real time pattern occurrence 
patterns series sequentially net delay retrieved similarly 
model recurrent neural network structure drama network fully recurrent network hidden units 
similarly recurrent neural networks allows learning time series inputs 
drama differs rnn models fact uses unsupervised training algorithm hebbian rules rnns supervised training algorithm backpropagation algorithm derivatives 
advantage hebbian rules training network requires time step processing cycle allows process information sensor information robot real time backpropagation needs time steps computation usually order information processing cycle 
drawback hebbian learning algorithm train network hidden units intermediary units input output units values unknown 
algorithms developed train rnns hidden units boltzmann machine learning procedure backpropagation similar procedures require time steps computation information processing cycle prevents line learning computationally limited robotic system 
interesting investigate hidden units improve network performance particular capacity 
hidden units possible network discover exploit regularities task hand symmetries replicated structures 
hidden units improve network ability discriminating redundant overlapping patterns consequently increase network capacity number patterns network store allowing overlap patterns network fails distinguish 
drama performance robotic experiments experiments reported section demonstrated possible implementation model controlling behaviour learning autonomous robotic agents 
learning task involved experiments relatively complex 
multiple associations stimuli variable time lags occurrence 
particular patterns composed set features colour inclination compass measurement different combinations distinguished 
learning successful data corrupted hardware noise imprecision teaching method 
experiments showed drama architecture enables real time computation line control learning autonomous mobile robots performs static sequential associations leading robot learning spatial regularities sensor actuator space time series consecutive sensor measurements 
simulation versus physical experiments simulation studies carried physical experiments order demonstrate test stability success learning proposed experimental set 
main advantage simulations physical experiments repeatable faster simulating hour experiment takes minutes suffer unexpected hardware breakdown 
disadvantages terms model faithfulness course known complete discussion see 
experiments physical simulated worlds differ aspects 
instance simulation poor account physics sensors world perceived robot simple field view defined light dispersion intensity invariant time inclination hills perfect points 
addition simulated real objects described number features set experiment 
results simulations physical experiments compared qualitatively 
claims results see qualitative characteristics drama architecture demonstrated simulations physical experiments 
evaluation learning parameters success learning depends correctly choosing values learning parameters duration short term memory values threshold parameters appear neuronal activation function 
reported experiments relate cases success failure learning particular choices environmental constraints objects relative dispersion featural descriptions values duration short term memory events determine bounds parameters inside learning successful 
threshold factors appear units activation function see equation determine success learning fixing tradeoff considering units activation spurious relevant 
threshold discards correlations due spurious unit activity evaluating mean time delay units activation considering irrelevant association units delay activation varies importantly training relative maximal variation fixed threshold determines tradeoff spurious relevant associations comparing frequency activation correlated units confidence factor parameter connection linking units 
units activated activation hazardous 
section discussed influence threshold parameter determination success experiment 
section algorithm determine line values thresholds calculate values time step 
section algorithm line tuning short term memory value tested simulation learning sequence patterns 
algorithm shown successfully converge proportion noise input 
time needed convergence relatively long algorithm slow reliably physical robotic experiments 
note factors influence success experiment agent sensor capabilities range sensitivity behaviour control non homogeneous travelling 
implementation model different robotic set ups particular robots degrees freedom finer sensor sensitivity allow determine real influence hardware characteristics success learning 
note fact implemented architecture different robotic set ups vehicles lego vehicles experiment doll robot different sensors applied different environments showed success learning dependent particular type hardware 
improved finer sensor capabilities give information distinguish objects teaching lowering overlap teaching patterns improving network capacity better actuator capacities robot robot smoother prone incorrect measurements 
robot grounding perceptions actions requirements starting system learn quickly require long series examples performing adequately 
experiment showed correct associations radio signals object features learned correspond minutes physical experiment 
similar experiments grounding radio signals robots sensor capabilities carried previously yanco stein steels vogt respectively reinforcement learning evolutionary techniques 
experiments showed vocabulary words learned training examples respectively 
method faster learning larger similar vocabulary 
addition general mentioned methods restricted sensor stimuli robots talk 
yanco stein vocabulary consisted robot actions learning algorithm action selection mechanisms 
steels vogt vocabulary concerned robots external perceptions perceptions share 
contrast mutual strategy allows agents share common context external face direction internal perceptions perform movement travel distance ground 
addition learning mechanism simply mutual associations inputs sensor actuator systems agent vocabulary potentially concern perceptions agent 
reported experiments agents talked external perceptions objects internal perceptions inclination direction 
reported experiments learner agent taught vocabulary words describing movements terms motor states move turn right turn left direction north south west east relative compass 
robot learning sequences perceptions second experiment robot learned timing ordering sequence compass radio light measurements resulting regular travelling series corridors regular reception teacher robot signals 
experiment demonstrated drama architecture capacity learning spatio temporal regularities time series sequences robot perceptions 
number studies robotics robot learned spatio temporal regularities sensor inputs 
works instance layer topographical map store separately spatial temporal regularities layer robot visual information ccd camera infra red sensors recurrent neural network predict sequences robot perception action travelling corridor 
experiment drama architecture advantages compared works learning retrieval performed line contrast works done line learning concerns sensor actuator modalities opposed consider association robot sensor perception robot actions 
ann architecture experiments part learning task experiments learn topography environment relative landmarks wonder self organising map kohonen nets models developed previously robotic tasks 
reason simply associations want sensor actuator state point topographical relationships sensor actuator vector space demonstrated left show dispersion associations learning 
may question current recurrent neural network models elman net jordan net dynamical recurrent net :10.1.1.117.1928
reasons 
hardware limitations restricted defining system integers floating points computationally fast limited board processing power 
recurrent neural networks backpropagation algorithm developed extension eliminated long time computation multiple training steps complex computation calculating derivatives 
associative hebbian networks attractive simplicity 
may argue favour buying powerful hardware system 
apart financial aspect motivation try best disposal 
expensive robots expensive sensors camera laser developed system capable complex cognition simple behaviours obstacle avoidance wall note drama architecture poor space efficiency capacity order disadvantage particular robotic experiments 
important time efficiency capacity learning complex time series 
disadvantage application sensors high sensitivity camera require important number units represent sensor information unit pixel set pixels 
case relevant pre process data ann architecture topographical map feed forward nn preliminary classification data reduced amount information drama network higher level classification 
approach followed tani uses combination hopfield associative memory network rnn 
robot camera information processed hopfield net determines categories visual inputs 
recurrent neural network trained output hopfield net simultaneous motor state robot 
net learns sequences visual perception action robot robot travels circling corridor 
tani backpropagation algorithm train rnn process information line 
interesting carry drama architecture rnn backpropagation determine allow carry successfully line computation 
experiments reported learning done line reported line learning retrieval different sequences actions perceptions robot 
note topographical maps particularly relevant sensors high resolution classification relies finding topological invariance input 
described novel connectionist architecture drama dynamic control learning autonomous robots 
drama stands dynamical recurrent associative memory architecture 
time delay recurrent neural network hebbian update rules 
part mathematical description drama analysed theoretically numerical simulations architecture performance 
model shown allow learning spatiotemporal regularities time series sequences inputs face important amount noise 
training rehearsal drama architecture computationally fast inexpensive model particularly suitable controlling computationally challenged robots 
second part reported implementation drama simulated physical robotic experiments line learning control autonomous robot 
experiment robot extracted spatial regularities perceptions resulted recognition labelling objects environment 
objects labels taught second autonomous robot 
second experiment robot learned time series perceptions travelling series corridors 
results simulated physical experiments consistent showing successful learning demonstrating robustness learning architecture face significant amount experimental noise 
grounding objects names shown faster similar robotic experiments general task related learning mechanisms drama architecture 
addition single architecture drama enabling learning directing robot behaviour learning mechanisms restricted particular direction association sensor actuator states case robotics learning experiments 
complexity experiments limited poor sensor capabilities computational power robots 
particular restricted number things robot learn perceive features record important amount data 
interesting implement model powerful robots complex sensor modalities 
billard supported swiss national science foundation 
facilities provided university edinburgh 
big technicians ai dept precious support 
bruce graham maja mataric anonymous reviewers useful comments earlier versions 
asada hosoda cooperative behavior acquisition mobile robots dynamically changing real worlds vision reinforcement learning development proc 
international workshop cooperative distributed vision pp 
billard dautenhahn 
grounding communication autonomous robots experimental study robotics autonomous systems special issue scientific methods mobile robotics recce nehmzow eds vols 

billard dautenhahn hayes experiments human robot communication imitative learning communicative doll robot proceedings socially situated intelligence workshop zurich ch part fifth conference society adaptive behaviour edmond dautenhahn eds centre policy modelling technical report series 
cpm 
billard drama connectionist model robot learning experiments grounding communication imitation autonomous robots phd thesis oct 
technical report dept ai university edinburgh 
billard hayes 
learning communicate imitation autonomous robots proceedings icann th international conference artificial neural networks gerstner 
eds springer verlag pp 
billard dautenhahn 
grounding communication situated social robots proceedings intelligent mobile robots conference manchester 
tech 
rep series dept computer science manchester university issn 
report number umcs 
billard allo follow learning speak imitation social robots 
msc thesis oct 
technical report dept ai university edinburgh 
buckingham willshaw 
performance characteristics associative net 
network pp 
chauvin rumelhart back propagation theory architecture application lawrence erlbaum associates publisher 
day davenport continuous time temporal back propagation adaptive time delay ieee transaction neural networks vol 
pp 

dautenhahn getting know artificial social intelligence autonomous robots robotics autonomous systems pp :10.1.1.45.8898
elman :10.1.1.117.1928
finding structure time 
cognitive science pp 
floreano mondada 
evolution homing navigation real mobile robot ieee transactions systems man cybernetics part cybernetics pp 
gaussier moga perception action loop imitation processes bottom approach learning imitation applied artificial intelligence vol 
giles kuhn williams 
dynamic recurrent neural networks theory applications 
ieee transactions neural networks 
special issue 
graham willshaw capacity information efficiency brain associative net 
advances neural information processing systems pp 
graham willshaw improving recall sparse autoassociative memory 
biol cyber pp 

episodic associative memories neurocomputing vol pp 
hebb 
organisation behaviour 
wiley new york 
partially reprinted anderson rosenfeld editors neurocomputing foundation research mit press cambridge 
hinton sejnowski 
learning relearning boltzmann machines rumelhart mcclelland editors parallel distributed processing vol chapt pp 
mit press cambridge 
hopfield tank 
computing neural circuits model science pp 
demiris kaiser 
human robot communication machine learning applied artificial intelligence journal pp 
kohonen 
self organisation associative memory springer verlag berlin rd edition 
kolen pollack memory proceedings thirteenth annual conference cognitive science society august pp 
kuipers 
qualitative approach robot exploration map learning aaai workshop spatial reasoning multi sensor fusion chicago pp 

kurz constructing maps mobile robot navigation ultrasonic range data ieee trans 
systems man cybernetics vol 
lin learning spatio temporal topology adaptive time delay neural network proceedings world congress neural networks portland vol 
pp 

jordan 
attractor dynamics parallelism connectionist sequential machine 
proceedings annual conference cognitive science society pp amherst 
lawrence erlbaum hillsdale 
mataric 
reinforcement learning multi robot domain autonomous robots vol 
january 
owen nehmzow route learning mobile robots self organisation proc 
st euromicro workshop advanced mobile robotics kaiserslautern 
ieee computer society 
nolfi 
emergent modularity develop control system mobile robots adaptive behaviour vol 
pp 

nordin banzhaf online method evolve behavior control miniature robot real time genetic programming adaptive behaviour vol fall pp 
pearlmutter 
gradient calculations dynamic recurrent neural networks survey ieee transactions neural networks pp 
pfeifer scheier 
sensory motor coordination metaphor 
pfeifer brooks eds robotics autonomous systems special issue practice autonomous agents 
nos 

pineda 
generalisation backpropagation recurrent neural networks phys 
rev lett pp 

combinatorial neural network exhibiting episodic semantic memory properties spatio temporal patterns ph thesis dept cognitive neural systems boston university boston ma 
sommer palm 
iterative retrieval sparsely coded associative memory patterns neural networks pp 
steels vogt grounding adaptive language games robotic agents proceedings fourth european conference artificial life pp brighton mit press bradford books 
tani yamamoto dynamical interactions visual attention learning behavior experiment vision mobile robot proceedings fourth european conf 
artificial life pp 
mit press 
thrun 
explanation neural network learning lifelong learning approach 
kluwer academic publishers boston ma 
torrance 
case realistic mobile robot simulator working notes aaai fall symposium applications artificial intelligence real world autonomous mobile robots cambridge ma october 
willshaw buneman longuet higgins non holographic associative memory nature vol 
pp 

wyatt hayes 
design analysis comparison robot learners robotics autonomous systems special issue scientific methods mobile robotics recce nehmzow eds vols 

yanco stein adaptive communication protocol cooperating mobile robots animals animats proceedings second international conference simulation adaptive behaviour edited 
meyer roitblat wilson 
mit press bradford books pp 

zimmer robust world modelling navigation real world neurocomputing vol 
nos 
pp 

zrehen 
elements brain design autonomous agents phd thesis swiss federal institute technology lausanne computer science department june 
