discretizing continuous attributes learning bayesian networks nir friedman stanford university dept computer science gates building stanford ca nir cs stanford edu moises goldszmidt rockwell science center high st suite palo alto ca moises rockwell com introduce method learning bayesian networks handles discretization continuous variables integral part learning process 
main ingredient method new metric minimal description length principle choosing threshold values discretization learning bayesian network structure 
score balances complexity learned discretization learned network structure model training data 
ensures discretization variable introduces just intervals capture interaction adjacent variables network 
formally derive new metric study main properties propose iterative algorithm learning discretization policy 
illustrate behavior applications supervised learning 
bayesian networks provide efficient effective representation joint probability distribution set random variables 
successfully applied expert systems diagnostic engines optimal decision making systems classifiers 
building bayesian networks laborious expensive process large applications 
learning bayesian networks data rapidly growing field research seen great deal activity years 
objective induce network set networks best describes probability distribution training data 
optimization process implemented practice heuristic search techniques find best candidate space possible networks 
search process relies scoring metric asses merits candidate network 
domains interest medicine industrial control variables training data continuous values 
basic approaches deal continuous variables restrict specific families parametric distributions methods described discretize variables learn network discretized domain 
tradeoff options 
model conditional density variable network assumptions regarding family distributions current stage efficient reasoning machinery models 
second captures rough characteristics distribution continuous variables 
induces models efficiently probabilistic inference optimal decision making 
interesting note current applications bayesian networks medical diagnostic involve discretization supplied human experts 
principled approach second option 
problem variable discretization essentially finding continuous variable set threshold values partition real line finite number intervals 
intervals values discretized counterpart approach problem minimal description length mdl principle 
mdl score network composed parts 
part measures complexity network second part measures network model data 
optimal network data set network minimizes mdl score 
minimization involves tradeoff parts score 
practice mdl score regulates number parameters learned helps avoid overfitting 
derive augmented version mdl take account discretization continuous variables data 
approach embodies new tradeoff complexity learned discretization learned network model training data 
tradeoff guarantees discretization variable introduces just intervals capture interaction adjacent variables network 
new metric provides principled approach selecting threshold values discretization process 
proposal regarded generalization method proposed fayyad irani 
roughly speaking approach applies supervised learning discretizes variables increase mutual information respect class variable 
fact show experimentally approaches identical restrict attention suitable network structures variables restricted interact solely class variable 
general particular unsupervised learning discretization variable maximize mutual information respect directly related variables 
relationships explicitly represented bayesian network structure arc denotes direct dependence variables 
local variable network structure contains variables directly interact 
having discretization integral part learning bayesian network able maximize mutual information related variables 
organized follows section review mdl metric process learning bayesian network 
section augment metric account discretizations 
section examine computational issues identify properties scoring metric exploited regard 
section experimental results evaluate method classification tasks compare method proposed fayyad irani 
section summarizes contributions discusses 
learning bayesian networks consider finite set fx xng discrete random variables variable may take values finite domain 
capital letters variable names lowercase letters denote specific values taken variables 
set values attain denoted val cardinality set denoted jjxjj sets variables denoted boldface capital letters assignments values variables sets denoted boldface lowercase letters val jjxjj obvious way 
joint probability distribution variables subsets conditionally independent val val val bayesian network annotated directed acyclic graph encodes joint probability distribution domain composed set random variables 
formally bayesian network pair hg thetai 
directed acyclic graph nodes correspond random variables xn edges represent direct dependencies variables 
graph structure encodes set independence assumptions node independent non descendants parents 
second component pair theta represents set parameters quantifies network 
contains parameter pi pi possible value pi pi set parents 
defines unique joint probability distribution pb xn pb pi pi problem learning bayesian network stated follows 
training set fu un instances value assignment variables find network best matches formalize notion goodness fit network respect data normally introduce scoring function solve optimization problem usually rely heuristic search techniques space possible networks :10.1.1.52.2692
different scoring functions proposed literature :10.1.1.156.9918
focus attention mdl score 
score simple intuitive proven quite effective practice 
idea mdl principle follows 
suppose set instances store keep records 
naturally conserve space save compressed version need find suitable model encoder take model produce compact image want able recover store version model encoder compress description length data model particular encoder length compressed data plus representation size model 
case length measured number bits needed storage 
mdl principle dictates optimal model particular class interest minimizes total description length 
mdl principle applied learning bayesian networks network model data encoder produced compressed version idea follows network assigns probability instance probabilities construct efficient code 
particular huffman code assigns shorter codes frequent instances 
benefit mdl scoring metric best network optimally balances complexity formally notion minimality associated definition ignore 
see details 
network degree accuracy network represents frequencies describe detail representation length required storage network coded data 
mdl score candidate network defined total description length 
store network hg thetai need describe theta describe store number variables cardinality variable information stored log log jjx jj bits 
describe dag sufficient store variable description pi parents 
description consists number parents followed list parents 
encode log bits graph structure encoded pi log bits 
describe parameters theta consider parameters conditional probability table 
table associated need store jj pi jj jjx jj gamma parameters 
representation length parameters depends number bits numeric parameter 
usual choice literature logn 
encoding length theta log jj pi jj jjx jj gamma description length function depend actual values parameters theta 
combining components dl net gamma log jjx jj pi logn delta log jj pi jj jjx jj gamma turn attention description length data 
probability measure defined construct huffman code instances code exact length codeword depends probability assigned particular instance 
closed form description length 
known choose longer coding blocks approximate optimal encoding length encoding gamma log pb bits 
description length data simply gamma log pb rewrite expression convenient form 
pd probability measure corresponds frequency instances data 
algebraic manipulations rewrite representation number slightly imprecise know advance length encoding 
length data gamma log pb gamman pi pd pi log pi standard arguments show term minimized set pi pd pi 
assume choose values theta manner 
write description length data network structure dl data pi xjy gamma pd log pd xjy conditional entropy xjy unconditional entropy 
information theoretic interpretation rewrite dl data slightly 
mutual information sets variables defined pd log pd pd pd 
define 
easy verify xjy gamma 
rewrite dl data gamma pi depend structure term constant description length possible structures 
ignore compare candidate network structures 
information theoretic interpretation shows minimize description length need find structure maximizes mutual information variable parents 
mdl score candidate network structure defined total description length dl dl net dl data mdl principle strive find network structure minimizes description length 
practice usually done searching space possible networks 
space large search non trivial problem 
details search process 
mdl score discretization assumed variables universe discrete nominal 
consider case variables take numerical values 
assume cont set continuous variables discretization sequence ht increasing sequence real values 
sequence defines function ir 
kg follows discretization policy collection cont policy defines new collection variables fx ug cont 
discrete random variable associated discretization policy omit subscript understood context 
target jointly learn discretization policy bayesian network discretized data 
augment mdl score include description length needed storing information able recover original data discretized data 
lets assume fixed 
collection instances define discretization fu hx hf output encoder consists parts 
description network 

description 
description encoding information necessary recover discrete dataset description length part derived section dl bits long eq 

introduce useful notation 
val set values appear dataset number values clearly 
need recover record set values val continuous random variable cont representation set depends exact precision choose encode real numbers 
encoding depends length possible instances ignore factor total score 
turn description discretization policy description stored part description records number values recorded number threshold points need encode threshold values need distinguish values val limit attention threshold values mid points successive values set 
thresholds chosen gamma mid point values val 
need store index discretization policy enumeration gamma gamma gamma delta different discretization policies cardinality jjx jj 
index described log gamma gamma gamma delta bits 
sterling approximation see get log gamma gamma gamma delta gamma gamma gamma gammap log gamma gamma log gamma 
description length dl cont gamma gamma gamma need describe reconstruct continuous value discretized instance specifies interval 
need encode value interval appears note limited number values val 
huffman code encode appears specific instance optimal huffman code constructed frequencies values val 
need store frequencies 
frequencies depend data particular discretization policy network structure ignore description length 
recorded frequencies reconstruct code value encoding particular value approximately gamma log pd jx bits long 
summing length encoding instances variables cont get dld gamma log pd jx combining parts conclude representation length dl dl dld rewrite term convenient expression 
basic properties entropy mutual information measures see easily prove proposition pi jx gamma pi note function data depend ignore 
simplification take score dl dl net dl gamman pi bound tight sense gamma gamma gamma gamma log log gamma gamma gamma delta difference previous version score represented eq 
middle term encodes discretization policy 
previous case score weights addition new arc increase complexity network increase mutual information nodes extremes edge 
additional term dl takes account tradeoff involving number intervals discretization mutual information score 
learning discretizations dispense assumption previous section set examine learn discretization policy network structure practice 
start assuming fixed network structure examine find discretization policy minimizes score dl 
consider simplest case exactly continuous variable examine number properties exploited computation dl 
general case variable discretized describe iterative approach takes advantage results 
section discuss learn network structure discretization policy 
discretizing variable continuous variable discretized 
examine dl changes change show terms score change change discretization fact exploited reduce computations discretization process 
define local score consist terms dl involve discretization dl local log delta jj pi jj jjx jj gamma log delta pi jj pi jj jjx jj gamma log gamma gamma gamma gamma delta pi pi pi immediately get proposition discretization policies differ discretization dl gamma dl dl local gamma dl local changing discretization suffices minimize dl local 
important aspect result dl local involves variables markov blanket parents children parents children term attempt minimize weights cost describing partitions mutual information gained parents children having number partitions 
problem search discretization minimizes dl local noted points consider thresholds gamma midpoints successive values space possible discretizations consists subsets gamma points 
practice search space starting trivial discretization discretization empty threshold list search possible refinements 
approach usually called top common supervised learning literature 
search strategy known ones greedy search hill climb search beam search carrying search expensive 
simple greedy search strategy need perform gamma threshold evaluations find discretization thresholds 
show properties dl local necessary recompute change dl local possible splits step 
ht define delta refinement discretization sequence contains suppose current candidate discretizing possible successors form delta midpoints val 
evaluate successor examining change term dl local described 
notice term dl local depends particular threshold choose add measures mutual information markov blanket 
find best successor discretization sequence need compute information gain possible successor gain deltat pi piy pi deltat gamma pi gamma piy pi term property speed search proposition gain delta gain 
proposition conclude choose add threshold need recompute gain thresholds split interval splits 
implementation uses greedy search routine 
routine maintains list possible splits associated gain 
iteration best split added current candidate 
change negative routine terminates 
recomputes gain relevant thresholds proposition reiterates 
discretizing variables turn general problem continuous variables computationally hard problem 
need search possible discretizations variables cont take advantage structure independences encodes find non interacting discretizations 
roughly speaking non interacting discretize separately 
discretization policy cont discretization policy define discretization policy results replacing discretization discretization definition disjoint subsets cont non interacting respect dag set instances discretization policies discretization policies dl dl dl gamma dl 
non interacting difference total score change discretization sum differences score change individually 
particular implies optimize discretization independently 
hand interacting case discretizing set reconsider discretization set 
read non interaction relationships structure moral graph undirected graph adjacent cases hold easy verify markov blanket exactly set adjacent nodes 
discretization depends markov blanket show theorem disjoint subsets cont adjacent respect example fa cg variables attributes class variable 
naive bayes model bayesian network structure pi class variable root pi fcg parent attribute class variable 
attribute markov blanket consists class variable attributes non interacting 
consequently discretize isolation techniques introduced previous section account class variable 
assume adjacent interact 
say set cont interacting component ae gamma interacting 
intuitively interacting component subdivided non interacting subsets 
finding interacting components done efficiently formally equivalent finding connected components graph 
definition non interaction guarantees discretize variables interacting component independently discretization interacting components 
interacting components singletons problem reduces discretizing single variables discussed previous section 
question deal non trivial interacting components 
suppose interact 
changing discretization changes local environment lead reconsider discretization doing need reconsider discretization forth 
main approaches deal problem 
search space possible discretizations variables interacting component 
approach ensures consider interactions component discretize variables 
problem large components procedure impractical 
second approach discretize variable time treating variables discrete fixed 
roughly speaking procedure goes follows find initial discretization variable 
step select variable search discretization treating variables discrete discretization policy held fixed 
done method described previous section 
repeat step improve discretization variables component 
easy see approach converge 
step improving score current discretization policy 
score function bounded stage 
procedure essentially hill climbing procedure guarantees find local minima score necessarily optimal discretization 
remaining question order variables discretized 
experiments algorithm outlined perform reasonably 
algorithm desirable property discretization passes discretizations variables markov blanket reconsidered 
guarantees discretize propagate changes previous discretization neighbors 
procedure initialized discretization 
current implementation square quantization method 
method attempts find best partitioning random variable essentially input initial discretization output locally optimal discretization push continuous variables queue empty remove element compute new discretization dl dl replace interacting push return algorithm discretizing variables iterative fashion 
matching gaussians distribution values 
experiments initialized discretization minimum logn upper bound 
choice quite arbitrary 
suspect finding initial discretization greatly improve quality discretization 
currently exploring issue detail 
learning network structure situations need learn structure network optimization problem want find network structure discretization policy maximize dl 
possible ways going 
similar current approaches learning bayesian networks 
approach search space possible candidates evaluate candidate search discretization maximizes score candidate 
search procedure computationally costly local properties discretization avoid variables candidate network 
second approach alternates learning discretization policy learning network structure iterative fashion 
idea follows start discretization learn network structure fixed discretization 
learned network 
cycle repeated improvement 
procedure improves score iteration easy see procedure terminate 
experiments process terminated iterations 
procedure guarantees find local minima necessarily optimal solution 
show choice initial discretization affect outcome procedure 
currently exploring issues detail 
table experimental results comparing naive bayes models method fayyad irani 
dataset prediction accuracy naive fi naive australian breast cleve crx diabetes glass glass iris lymphography pima shuttle small vehicle waveform preliminary experimental results section describes preliminary experiments designed test soundness method proposed 
experiments run datasets irvine repository 
estimated accuracy learned classifiers fold cross validation shuttle small waveform datasets hold method 
report mean prediction accuracies cross validation folds 
report standard deviation accuracies fold 
computations done mlc library 
see details 
experiment concerned application supervised learning comparison discretization method fayyad irani fi 
method considered state art supervised learning 
approach fi attempt maximize variable class variable 
method developed context bayesian networks applicable particular network structure naive bayes classifier 
example shows naive bayes classifier markov blanket attribute consists class node 
network approach discretize variable maximize mutual information class variable 
case particular fixed structure network reasonable compare method theirs 
table shows results comparison naive denotes naive bayes classifier learned original data discretized approach fi naive data set method fi training data manner described naive bayes classifier learned discretized data 
table experimental results comparing unsupervised learning network structure 
dataset prediction accuracy unsup ls unsup naive fi unsup australian breast cleve crx diabetes glass glass iris lymphography pima shuttle small vehicle waveform results show fi method performed slightly better surprising specially designed classification problems 
method comparable discretizations coincided 
experiment fixed network structure 
second learn discretization structure network examine effects interaction 
learned structure searching greedy fashion space bayesian networks 
procedure unsupervised distinguish class variable variables domain 
table contains results experiment unsup ls denotes unsupervised learning method described section initial discretization performed square quantization described section unsup naive denotes unsupervised learning method initial discretization naive procedure fi unsup unsupervised learning performed training data fi method 
results inconclusive 
expected unsup naive better biased better classification 
somewhat surprisingly clear dominance unsup ls unsup naive 
evident fi method informed classification task optimal discretization networks learned 
due fact take account interactions variables discretization process 
order compare discretization policies computed experiments discretizations learned naive unsup ls unsup naive fi method input classifier 
note unsupervised bayesian network classifiers better naive bayes classifier 
datasets multiple attributes performance canbe poor 
explain phenomena detail 
table lists prediction accuracies performance data 
results show procedures unsupervised ones comparative fi method internal discretization 
emphasize results compare supervised approach informed goal discretization process unsupervised approach give class variable special status 
results show unsupervised method competitive state art supervised method 
task examined narrow 
measures limited fashion quality learned model 
currently devising experimental setup compare approach parametric method dealing continuous variables 
discussion presents innovative approach discretization continuous variable supervised unsupervised learning mdl principle 
discretization handled integral part learning bayesian network 
number advantages 
discretization variable introduces just intervals capture interactions adjacent variables network 
second non interacting variables discretized isolation 
validated approach state art discretization procedure context supervised learning currently performing experiments validate approach applications learning 
problem facing approach discretizing learning bayesian networks lack methodological established validation standards 
currently process defining 
addition research opened number issues currently investigating 
understanding iterative procedures depend initial conditions choices initial conditions 
second scale process large databases 
currently discretization procedure tests mid point continuous values 
data contains large number values continuous variable procedure extremely expensive 
considering approaches including subsampling techniques simple quantization methods finding reasonably sized candidate pool threshold values 
open question development similar scoring metric discretization bayesian concepts opposed mdl principle 
natural path augment bayesian scoring metric introduced :10.1.1.156.9918
main obstacle specify compact way prior parameters theta possible discretization data 
non trivial problem table learned discretizations 
dataset prediction accuracy naive unsup ls unsup naive fi australian breast cleve crx diabetes glass glass iris lymphography pima shuttle small vehicle waveform methods immediately apply 
acknowledgments authors grateful denise draper usama fayyad ronny kohavi sahami comments previous draft useful discussions relating 
parts done author rockwell science center 
author supported part ibm graduate fellowship nsf iri 
bouckaert 
properties bayesian network learning algorithms 
uai pp 


buntine 
operations learning graphical models 
artificial intelligence research 
cheeseman kelly self stutz taylor freeman 
autoclass bayesian classification system 
ml 

cooper herskovits 
bayesian method induction probabilistic networks data 
machine learning 
cover thomas 
elements information theory 
wiley 
dougherty kohavi sahami 
supervised unsupervised discretization continuous features 
ml 

fayyad irani 
multi interval discretization continuous valued attributes classification learning 
ijcai pp 

friedman goldszmidt 
building classifiers bayesian networks 
aaai 

heckerman :10.1.1.52.2692
tutorial learning bayesian networks 
technical report msr tr microsoft research 
heckerman geiger chickering 
learning bayesian networks combination statistical data 
machine learning 
geiger 
learning bayesian networks unification discrete gaussian domains 
uai pp 


john langley 
estimating continuous distributions bayesian classifiers 
uai pp 


kohavi john long manley pfleger 
mlc machine learning library 
tools artificial intelligence pp 

ieee computer society press 
lam bacchus 
learning bayesian belief networks 
approach mdl principle 
computational intelligence 
murphy aha 
uci repository machine learning databases 
www ics 
uci edu mlearn mlrepository html 
neal 
belief networks 
artificial intelligence 
pearl 
probabilistic reasoning intelligent systems 
morgan kaufmann 
quinlan 
programs machine learning 
morgan kaufmann 
