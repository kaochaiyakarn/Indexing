long short term memory learns context free context sensitive languages felix gers urgen schmidhuber idsia switzerland www idsia ch previous learning regular languages exemplary training sequences showed long shortterm memory lstm outperforms traditional recurrent neural networks rnns 
demonstrate lstm superior performance context free language cfl benchmarks show works better previous hardwired highly specialized architectures 
best knowledge lstm variants rst rnns learn context sensitive language csl standard recurrent neural networks rnns see survey pearlmutter plagued major practical problem gradient total output error respect previous inputs quickly vanishes time lags relevant inputs errors increase 
standard rnns fail learn presence time lags exceeding discrete time steps relevant input events target signals 
long short term memory lstm method ected problem :10.1.1.13.634
lstm learn bridge minimal time lags excess discrete time steps enforcing constant error ow constant error cecs special units loss short time lag capabilities 
multiplicative gate units learn open close access constant error ow 
lstm learning algorithm cient previous rnn algorithms real time recurrent learning rtrl back propagation time bptt local space time computational complexity time step weight 
previous showed lstm outperforms traditional rnn algorithms numerous tasks involving real valued discrete inputs targets including tasks require learn rules regular languages rls describable deterministic nite state automata dfa :10.1.1.13.634
remained unclear lstm superiority carries tasks involving context free languages discussed rnn literature 
recognition requires functional equivalent stack 
conceivable lstm just right bias rls fail 
focus common cfl benchmarks apply lstm context sensitive language csl 
include include rls 
focus classic example csl cfl 
general csl recognition requires linear bounded automaton special turing machine tape length linear input size 
knowledge rnn able learn csl 
lstm lstm forget gates introduced peephole connections 
basic unit lstm network memory block containing memory cells adaptive multiplicative gating units shared cells block 
memory cell core recurrently self connected linear unit called constant error carousel cec activation called cell state 
cecs enforce constant error ow overcome fundamental problem previous rnns prevent error signals decaying quickly back time 
adaptive gates control input output cells put output gate learn reset cell state contents date forget gate 
peephole connections connect cec gates 
errors cut leak memory cell gate serve change incoming weights 
ect cecs part system errors ow back forever gates learn nonlinear aspects sequence processing 
lstm updates ecient signi cantly affecting learning power lstm learning algorithm local space time computational complexity time step weight 
cecs permit lstm bridge huge time lags discrete time steps relevant events traditional rnns fail learn presence step time lags despite requiring complex update algorithms 
forward pass 
see gers schmidhuber detailed description lstm forward pass forget gates peephole connections 
essentially cell output calculated current cell state sources input cell input gate forget gate input output gate 
gates sigmoid squashing functions range 
state memory cell calculated adding squashed squashing function sigmoid range gated input state previous time step multiplied forget gate activation 
cell output calculated multiplying gating output gate activation 
gradient backward pass 
essentially lstm backward pass details see hochreiter schmidhuber gers 
ecient fusion slightly modi ed truncated bptt customized version rtrl 
iterative gradient descent minimizing objective function usual mean squared error function 
bptt rtrl lstm learning algorithm local space time 
experiments network sequentially observes exemplary symbol strings language input symbol time 
traditional approach rnn literature formulate task prediction task 
time step target predict possible symbols including terminal symbol symbol occur step possible symbols predicted 
input sequence begins start symbol empty string consisting st considered part language 
string accepted predictions correct 
rejected 
prediction task equivalent classi cation task classes accept reject system prediction errors strings outside language 
system learned language string size able correctly predict strings size symbols encoded locally dimensional vectors equal number symbols language plus start symbol input terminal symbol output input units output units standing symbols 
signi es symbol set set decision boundary network output 
cfl 
strings input sequences form input output vectors dimensional 
rst occurrence sequence possible step 
input target cfl 
second half string palindrome mirror language completely predictable rst half 
task involves intermediate time lag length 
input output vectors dimensional 
rst occurrence symbols possible step 
input target csl input output vectors dimensional 
rst occurrence symbols possible step 
input target training testing learning testing alternate training sequences freeze weights run test 
training test sets incorporate legal strings length positive exemplars 
training stopped training sequences accepted training sequences 
generalization set largest accepted test set 
weight changes sequence 
apply momentum algorithm learning rate momentum parameter 
results averages independently trained networks di erent weight initializations experiment 
cfl study training sets ng 
test sets mg fn sequences length 
cfl training sets set rodriguez wiles sequences length 
set sequences length 
test sets mg mg sequences length 
csl study training sets parameters cfl test sets mg fn sequences length 
topology experimental parameters input units fully connected hidden layer consisting memory blocks cell 
cell outputs fully connected cell inputs gates output units direct shortcut connections input units 
gates cell output unit biased 
bias weights input gate forget gate output gate initialized respectively standard values experiments precise initialization critical 
weights initialized randomly range 
cell input squashing function identity function 
squashing function output units sigmoid function range 
cfl memory block cell 
peephole connections adjustable weights unit unit bias connections 
cfl blocks cell resulting adjustable weights unit bias connections 
csl topology language input output units giving adjustable weights unit unit bias connections 
previous results cfl published results language summarized table 
rnns trained plain bptt tend learn just reproduce input 
sun 
highly specialized architecture neural pushdown automaton generalize 
cfl rodriguez wiles bptt rnns hidden nodes 
training sequences length best network generalized sequences length 
networks learn complete training set generalization restricted strings 
csl knowledge previous rnn learned csl 
lstm results cfl solved training sets lhs table 
small training sets sucient perfect generalization tested maximum 
note long sequences kind require stable nely tuned control network internal counters 
performance better previous approaches largest set learned specially designed neural push automaton 
required training sequences length test sequences 
training set lstm generalized best previous result see table generalized slightly larger training set 
contrast wiles observe networks forgetting solutions training progresses 
previous approaches lstm reliably nds solutions generalize 
cfl training set solved 
training sequences best network generalized strings length symbols processed correctly average generalization set strings length symbols processed correctly learned 
training sequences average 
training set solved 
training sequences best network generalized strings length symbols processed correctly 
average generalization set strings length symbols processed correctly learned 
training sequences average 
previous approach rodriguez wiles lstm easily learns complete training set reliably nds solutions generalize 
csl lstm learns training sets trials training set generalizes rhs table 
small training sets sucient perfect generalization tested maximum sequences length 
analysis solutions discovered lstm 
cfl cell state increases symbols fed network decreases step size symbols fed 
sequence beginnings rst symbols observed step size smaller due closed input gate triggered 
results overshooting initial value sequence leads prediction sequence termination 
cfl network learned establish control counters symbol pairs treated separately di erent cells respectively 
cell tracks di erence number observed symbols 
opens string table 
previous results cfl showing left right number hidden units state units values training number training sequences number solutions trials largest accepted test set 
hidden units train 
set train 
str 
sol tri 
best test table 
results cfl lhs csl rhs showing left right values training average number training sequences best generalization achieved average networks parenthesis percentage correct solutions best generalization average networks parenthesis 
cfl csl train 
set train 
str 
generalization set train 
str 
generalization set predicts nal cell treats embedded substring similar way 
values stored manipulated cell output gate remains closed 
prevents cell disturbing rest network protects cec incoming errors 
csl network solutions combination counters instantiated separately memory blocks 
second cell counts input symbol 
counts second memory block respectively 
long short term memory lstm clearly outperforms previous rnns regular language benchmarks previous research context free language cfl benchmarks 
learns faster generalizes better 
lstm rst rnn learn context sensitive language 
acknowledgment 
supported snf long short term memory 
gers schmidhuber 
recurrent nets time count 
proc 
ijcnn int 
joint sun training set augmented stepwise sequences misclassi ed testing nal accepted set random sequences length exact generalization performance unclear 
applying brute force search weights best network rodriguez 
improves performance acceptance 
conf 
neural networks como italy 
gers schmidhuber cummins 
learning forget continual prediction lstm 
neural computation 
hochreiter schmidhuber 
long short term memory 
neural computation 
pearlmutter 
gradient calculations dynamic recurrent neural networks survey 
ieee transactions neural networks 
rodriguez wiles elman 
recurrent neural network learns count 
connection science 
paul rodriguez janet wiles 
recurrent neural networks learn implement symbol sensitive counting 
advances neural information processing systems volume pages 
mit press 
sun lee giles chen lee 
neural network pushdown automaton model stack learning simulations 
technical report cs tr university maryland college park august 
wiles 
learning context free task recurrent neural network analysis stability 
proceedings fourth biennial conference australasian cognitive science society 
wiles elman 
learning count counter case study dynamics activation landscapes recurrent networks 
proceedings seventeenth annual conference cognitive science society pages pages cambridge ma 
mit press 
