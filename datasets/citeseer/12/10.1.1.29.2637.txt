mining needles haystack classifying rare classes phase rule induction mahesh joshi ibm watson research center university minnesota minneapolis department computer science ibm com ramesh agarwal ibm watson research center box yorktown heights ny ibm com vipin kumar university minnesota department computer science minneapolis mn kumar cs umn edu learning models classify rarely occurring target classes important problem applications network intrusion detection fraud detection deviation detection general 
analyze previously proposed phase rule induction method context learning complete precise signatures rare classes 
key feature method separately objectives achieving high recall high precision target class 
phase method aims high recall inducing rules high support reasonable level accuracy 
second phase tries improve precision learning rules remove false positives collection records covered phase rules 
existing sequential covering techniques try achieve high precision individual disjunct learned 
claim approach inadequate rare classes problems false positives error prone small disjuncts 
motivated strengths phase design design various synthetic data models identify analyze situations state art methods ripper rules fail learn model learn poor model 
situations phase approach learns model significantly better recall precision levels 
comparison methods challenging real life network intrusion detection dataset 
method significantly better comparable best competitor terms achieving better balance recall precision 

motivation contact author permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
acm sigmod may santa barbara california usa copyright acm 
important problems data mining commonly known deviation detection modeling rarely occurring phenomena data 
situations deviation detection rare deviant events occurred past identified leading labeled data 
example network intrusion detection consider attacks type remote local computer remotely guessing password opening ftp data connection 
provided various data network activity collected successful incidents attack rare identified analyzed 
postulate proposed rare events unique signatures learned data 
focus supervised learning signatures 
classification powerful supervised technique data mining learns models labeled data 
input classification problem set observations real world recorded set records characterized multiple attributes 
associated record categorical attribute called class 
training set records known class labels problem learn model class terms attributes 
goal model predict class set records certain objective function predicted actual classes optimized 
concentrate general specific rule induction techniques building classifier easily interpretable humans computationally tractable practical scenarios exhibited competitive performance application domains 
general specific techniques start building dnf model general rule empty rule progressively add specific conditions 
commonly technique sequential covering iteratively discovers multiple rules having high accuracy respect target class predicts 
time rule learned examples supported removed iteration 
discovered rule expected cover disjoint signature target class 
signature target class pure rule corresponding covers negative examples approach works fine 
describe possible scenarios se covering approach discovering high accuracy rules may fail 
target class signature composed components presence target class absence non target class component correctly completely learned 
happen especially rare classes signature presence class inherently impure 
example intrusion detection rare attack type signature presence connection type ftp 
cover ftp connections flood computer denial service dos attack 
rule refined detecting signatures absence dos attacks 
existing techniques tight accuracy constraints cause rule refined immediately adding conjunctive conditions detect second component 
rule may cover sufficient number negative examples needed learn correct signatures 
refer problem false positives 
problem existing sequential covering methods may face small disjuncts rules cover small number target class examples prone generalization error rules covering larger number examples 
prevalent classes rules usually arise iterations algorithm remainder data set consists small number target class examples learn 
believe rare classes rules start appearing early total number target class examples small start 
believe problem small disjuncts manifests greatly existing techniques tight constraint accuracy rule 
constraint translates low support rule statistical point view decisions small evidential support unreliable 
critical observation problems leads focal point analyze effectiveness proposed phase rule induction approach especially context building models rare classes 
assessment existing techniques may inadequate try achieve objectives high recall high precision simultaneously may prevalent classes prevalent signatures 
phase approach called pnrule separately objectives phases 
existing techniques phase technique starts tight accuracy constraints 
high accuracy rule sacrificing support favor rule higher support lower accuracy 
effect technique seeks high recall objective phase 
refer rules discovered phase rules detect presence target class 
second phase seek precision 
key point starting second phase collect examples covered rules positive negative learn rules remove false positives collection 
contrasted way existing techniques refine rule covers individually 
call rules discovered second phase rules detect absence target class 
hypothesize strengths twophase method especially rare classes presence support total number examples rule covers positive negative second phase allows phase pnrule sensitive problem small disjuncts second phase pnrule better ability learn signatures absence target class combines false positives 
experimentally validate hypotheses designing variety synthetic datasets comparing technique ability model rare classes datasets existing stateof art core methods rule induction ripper rules identify analyze situations existing methods rule induction fail method yields significantly better performance 
focus binary classification problem goal learn signatures distinguish rest traditional evaluation metric accuracy adequate target class rare 
class rare say predicting non target class achieve high accuracy level 
believe rare classes classifier evaluated performs recall precision 
metric called measure widely information retrieval community 
start comparative study simple model generating synthetic datasets progressively model complex 
observe superiority method models 
results indicate technique performs significantly better rare classes 
proportion target class increases training set performance methods catches method 
improvements phase algorithm proposed 
illustrate effectiveness improvements real life dataset validates better suitability twophase approach rare classes 
dataset domain network intrusion detection supplied part kddcup classifier learning contest 
unique features data set interesting application 
results rare classes dataset 
improved method better suited achieve higher performance levels previous version significantly outperforms ripper rules data set 
rest organized follows 
start overview key features improved twophase section 
section various synthetic dataset models designed comparative experimental results analyzed 
section results real life dataset 
section concludes summarizing remarks 
related various rule classification algorithms proposed literature far cn family aq algorithms ripper oth refer methods method core methods rule induction methods proposed lri slipper concepts boosting bagging learning process 
believe methods meta techniques phase induction core techniques just way ripper core slipper 
framework applicability multi class problem different costs misclassification illustrated 
ers 
give overview ripper rules explain face problems false positives small disjuncts 
believe problem small disjuncts caused low statistically insignificant support small coverage rules 
having small coverage avoided rare subclasses sequential covering technique 
causes small coverage disjuncts small support relatively tight accuracy constraints existing algorithms 
technique relaxes accuracy constraints phase required 
ripper rules try avoid overfitting disjunct small large pruning rules 
existing pruning procedures rules ripper learning set specific rules generalizing individual rule removing conjunctive conditions 
iteration ripper splits remaining data set random similar parts 
grow specific rule generalize rule immediately 
remainder dataset size reduces approach may face problems 
amount training data reduces support rule decreases 
second estimates error obtained small pruning set may reliable 
ripper uses principle minimizing description length mdl avoid small disjuncts 
experience looking formula computing mdl small disjuncts tend longer lengths small support 
higher chance getting removed final rule set possibly losing rare signatures 
strategy rules starts rules obtained overfitted decision tree uses entire training set generalization rule 
generalization guided pessimistic error rate estimates 
estimate small disjunct may reliable low support 
decision comparing estimate estimate generalized version may unreliable 
problem false positives ripper may face learning method described earlier 
decision tree induction technique may run problem 
greedy approach splits data set learning decision node 
decisions correspond signatures presence target classes examples required learn absence non target class get split multiple disjoint paths tree 
rules starts initial rule set supplied decision tree 
procedure usually improves generalization ability tree model mechanism gather false positives re learn signatures absence non target class 
believe rules may face problem 
contributions key contributions ffl focus complete precise modeling rare classes goal shifted achieving high recall high precision rare class achieving high accuracy 
ffl improved version phase rule induction algorithm allows better implicit control recall precision 
ffl illustrate strengths phase rule induction algorithm designing various synthetic data models range relatively simple fairly generic complex represent real life classification scenarios 
ffl empirical illustration synthetic models situations existing core rule induction algorithms rules ripper learn poor unacceptable model 
phase rule induction algorithm performs situations 
ffl comparative results challenging real life data set illustrate effectiveness new improved version phase algorithm yielding better performance previous version rules ripper 

phase rule induction pnrule algorithm proposed classification framework phase rule induction approach 
give brief overview key stages phase algorithm context learning binary classifier model specified target class 
primarily describe new parameters methods incorporated pnrule framework previously proposed version 
particular process adding growing rule phases new parameters give user control recall precision 
new efficient algorithm evaluate conditions numerical attributes 
start overview main learning algorithm 
main learning algorithm model format training data set target class algorithm learns binary phase model target class 
model represented kinds rules rules rules 
rules predict presence target class rules predict absence target class 
pand rules learned sequential covering stages 
true false positives covered high support possibly low accuracy rules collected starting phase 
phases followed step constructs scoring mechanism rule combinations 
overview rule building scoring mechanism subsections 
points note regarding algorithm ffl phase algorithm phase strives rules high support reasonable accuracy 
mechanism build rules includes evaluation metric criterion stopping growth rule user specified minimum support ensure rule support low 
ffl stage stage stage algorithm distinct stopping criterion decides adding rules rule set phase 
current implementation rules added minimum user specified fraction target class covered new rule added satisfies minimum accuracy threshold 
desired minimum coverage high low accuracy low support rules may get added model 
coverage low important rarer subclasses missed 
proper value parameter needs experimentation 
currently rules added new rule increases description length limit minimum value obtained far 
ffl final step building scoring mechanism crucial 
model learned pnrule framework simply mean rule applies rule applies record record belongs target class formally means pn gamma nnn gamma equivalently dnf model form nnn gamma nnn gamma pn gamma nnn gamma 
seen model restrictive sense conjunctions conditions common 
restrict kinds functions learned phase model 
scoring mechanism allows relax restriction selectively deciding ignore effects certain rule 
building evaluating rules rule built discovering conjunctive condition time 
candidate conditions categorical attributes currently single value attribute 
numerical attribute kinds conditions evaluated 
sided conditions evaluated single scan data set sorted values 
existing techniques rule induction explicitly evaluate range condition vl vr 
values vl vr computed doing extra scan sorted data set 
condition vr higher value condition vl fix vr scan best value vl left vr 
condition vl higher value condition vr fix vl scan best value vr right vl 
experimentation verified ability simple efficient method find close best conditions 
range decision emulated single sided decisions believe finding range conditions step advantage methods single sided decisions 
discovering limits range condition limit may discovered may discovered attribute condition prevalent 
features rule building procedure need explanation 
evaluation metric 
metric crucial deciding rules added 
metric expected yield high value rule having high support high accuracy respect target class 
metric experiments number metrics 
possible choices gini index information gain gain ratio chi squared statistics metrics 
final important aspect rule building criterion stopping refinement growth rule 
current rule new refined rule 
need decide accept specific version accuracy better equal support equal support accuracy find values evaluation metric rules respect distribution target class data set remains removing data supported earlier rules 
phase accept evaluation metric value support higher minimum specified fraction target class population 
phase select evaluation metric higher value phase guided lower limit recall original target class user specified 
lower value choosing cause recall go lower limit rule refined 
lower limit high lot highly refined low support rules discovered leading overfitting phase loss precision 
low rules may remain general introducing lot false negatives losing recall 
note minimum coverage requirement phase section acts upper limit recall 
parameters give user control classifier recall precision performance 
pnrule classification strategy scoring algorithm describe rules rules classify unseen record 
rules rules arranged decreasing order significance order discovery 
record consisting attributevalue pairs rules applied ranked order 
rule applies prediction false 
rule applies accepted rules applied ranked order 
rule applies accepted 
default rule applies discovered rules apply 
classifier simple true false decision predict record true rule applies rule applies 
desirable assign score decision classifier interpreted probability record belonging target class 
depending rule rule combination applies predict record true certain score interval 
binary classification declare record true score greater threshold usually 
motivation assigning probabilistic score individual rule rule combination weigh effect rule rule 
remember learned set records collectively supported rules 
rule significant removing collective false positives 
rule may effective removing false positives subset rules 
low accuracy rule may introducing excessive false negatives rules possibly primary contribution remove false positives lower accuracy rules 
excessive false negatives recovered assigning correspondingly low score 
need properly judge significance rule rule 
scoring mechanism precisely 
detailed scoring algorithm 
effect scoring mechanism selectively ignore effects certain rules rule 
reflects approximate probability record belongs target class particular rule rule combination applied 

analysis synthetic models hypothesis pnrule suitable rare classes empirically evaluated section designing synthetic datasets 
compare pnrule performance core state art rule induction methods viz 
rules ripper 
comparison strategy compare binary classification performance methods 
goal learn rule model distinguish class target class rest 
comparative study comparison metric 
widely information retrieval community 
target class examples data set 
classifier predict examples correctly 
classifier predicts examples class belong class false positives 
recall defined precision defined 
comparison metric measure defined rp 
metric range 
special case general metric derived recall precision equal weights 
higher values indicate classifier performing better recall precision 
variations method tried dataset yields best results test data chosen 
pnrule combinations values upper limit recall lower limit recall 
rest input parameters pnrule fixed conservative values 
available versions ripper version rules release comparison 
default recommended settings input parameters 
variations methods obtained different training sets 
variation refers set results record training set unit weight 
second variation denoted henceforth extension refers set results stratified training set stratified set target class record identical weight sum weights equal number non target class records unit weight 
stratification process converts originally rare class class equal strength 
designing datasets analysis performance purpose designing synthetic datasets identify nature datasets pnrule advantage competitors 
goal find situations general resemble real life scenarios pnrule approach performs better necessity 
rules stratified set build overfitted decision tree tree construct rules unit weight training set 
design process guided strengths pnrule ffl presence second phase pnrule requires go second phase find set highly accurate high support rules cover entire target class 
words second phase required high support rule inevitably captures chunk negative examples positive examples 
ffl collective removal false positives second phase pnrule collects false positives covered union rules learns rules collection remove false positives 
believe mechanism pnrule sensitive problem false positives competitors may face indicated section 
note key difference phase pnrule needs learn signatures presence non target class phase ripper rules need learn signatures absence target class attributes distinguish non target class 
strengths pnrule mind designed synthetic datasets 
dataset built fairly simple model attributes 
varying parameters model show performance rules ripper starts deteriorate 
extend results build model categorical valued attributes 
final set models designed mix continuous categorical attributes reasonably complex resemble real life situations 
set general datasets show effect varying proportion target class see pnrule required rare classes 
datasets continuous valued attributes simple model simple model target class non multiple subclasses 
subclass distinguished signatures single attribute 
signatures appear disjoint uniformly spaced identical peaks subclass distribution values distinguishing attribute 
records subclasses randomly uniformly distributed attribute 
training examples subclass equally divided disjoint signatures 
pictorial description datasets shows class distributed attribute 
attribute distinguishing signature peaks target class second third attributes similar signatures distinguish nc nc subclasses non target class nc respectively 
difficulty problem seen tiny deeply embedded signatures attribute distribution 
full coverage inherently captures large number false positives 
classifier able learn model nc 
desired classifier model target class dataset follows true attribute left value range peaks attributes values fall range signature peaks nc nc 
words ideal pnrule model peaks attribute form rules peaks nc nc attributes form rules 
model parameters ffl number target subclasses tc 
ffl number disjoint signatures target subclass 
ffl total width signature peaks subclass target class tr 
increasing value tr increases number false positives need removed order learn high precision model 
ffl number non target subclasses ntc 
ffl number disjoint signatures non target subclass 
ffl total width signature peaks subclass non target class nr 
ffl shape signature distribution gamma shape 
flat rectangular signifying uniform distribution triangular gaussian 
table describes various parameters experiments conducted model gives comparative results techniques 
dataset training set target class population total training set records 
testing techniques rare class 
training set test set generated identical models 
results reported test set total records target class 
effect varying ntc seen table effect variations tr nr seen table 
performance results table indicate number signatures number subclasses nontarget class increase performance rules ripper starts deteriorate 
observing model learned rules indicated learns rules capture regions values non target class attributes 
example dataset learns strong rules target class 
subclasses non target class subclass signatures making non signature regions 
disjunction possible combinations regions forms rule set target class 
works long sufficient number examples region 
number combinations increases example dataset rules performance 
fact dataset unable learn acceptable model poor recall precision levels 
rules rules learns non target class precisely non signature regions target class attribute 
detailed look decision tree model rules gets initial rule set showed tree model facing precisely problem examples pointed section 
due lack ability collectively remove false positives tries remove false positives peak target class individually 
process learns incomplete description signatures non target class 
see ripper performance degrades 
observation learned model indicated faces adverse effect 
learns correct signatures target class conditions rules 
signatures impure dominated negative examples tries remove negative examples immediately refining rule 
rules ripper add conditions learn non signature regions non target class attributes 
ripper rules specific version rules learned rules ripper rules target class signature 
combinations non signature regions non target class increases ripper performance starts degrade earlier rules 
looking pnrule results table seen pnrule able learn models situations 
fact observation rules rules learned pnrule shows learning close precise signatures target class able remove large number false positives phase learning close precise signatures non target class 
strategy stopping refinement rule global value improve working 
effect stratified training set stratified set target class record high weight 
non signature regions non target class attributes purer target class compared unit weight training set 
conjunction dominating signature peaks target class attribute ripper satisfied learning regions piece meal fashion 
effect learning incomplete description remove false positives 
results high recall poor precision observed dataset 
note reporting results tree model rule model 
usually models large trees took unacceptable amount learning time build rule model 
effect varying tr nr analyze effect variation tr nr values 
results dataset table dataset illustration 
increase value tr causes signature peak widen 
correct signature target class learned causes false positives covered non target class uniformly distributed attributes 
expected effect techniques suffer problem capturing single target class signature negative examples available learner increasing chance learning correct combinations non signature regions non target class 
effect observed rules ripper especially values nr nr 
value nr increases difficult remove false positives removing true positives target class 
seen degradation performance methods unit weight training sets 
course degrading effect significant rules ripper pnrule 
attributed effect observed rules increase nr causes decrease number examples available learn combinations nc nc nc nc nc nc nc nc nc nc nc nc dataset nr nr nr rec prec rec prec rec prec tr cte cte cte re re re tr cte cte cte re re re tr cte cte cte re re re effect varying tr nr dataset 
top part gives pictorial description distributions target class subclass nc nc non target class values numerical attributes 
axes values attributes axes show number examples class 
attribute leftmost graph distinguishing signatures peaks distribution nc nc uniformly randomly distributed 
similarly second third attributes signatures nc nc 
combination tr nr best classifiers indicated bold faced values 
classifier notations rules cte tree model ripper re ripper pnrule numerical datasets target class non target class dataset tc tr ntc nr gamma shape triangular triangular triangular triangular triangular triangular dataset ripper ripper pnrule rec prec rec prec rec prec rec prec rec prec table comparative results numerical datasets 
table gives description data set 
see text explanation column names 
rec recall prec precision rp 
bold faced values indicate classifiers perform best dataset 
non signature regions non target class 
pnrule able learn models reasonably acceptable levels recall precision 
effect increasing tr ripper incur loss precision 
due wider highly weighted target class peaks 
increased width causes false positives covered target class signature increased weight true positives signature techniques ignore job removing false positives 
performance ripper unaffected value nr fact may increase slightly increase nr 
explained follows 
nr increases non signature regions non target class shrink 
target class examples higher weights ripper regions purer 
learned bit accurately 
summary summary datasets generated simple disjunctive model rules ripper learn models pnrule learn equally models 
rules ripper start fall apart learning difficulty increases pnrule holds performance acceptably high level 
datasets categorical valued attributes extending idea designing numerical datasets create model generate datasets categorical attributes 
representing signatures peaks distribution continuous valued attribute form signature conjuction set words 
gives description categorical datasets generated 
role tc ntc played distinct values na target non target class 
role played roles tr nr played values target nontarget class 
results categorical datasets tabulated table similar seen numerical datasets sense pnrule outperforms ripper rules 
results datasets categories indicate number signatures nontarget class increases na theta performance pnrule competitors degrades 
observation similar numerical datasets 
effect variation probability signature seen datasets category probability increases performance classifiers decreases 
decrease pnrule performance acceptable level 
general datasets generic dataset generated 
dataset categorical numerical attributes attribute distinguishing subclass target non target class 
description dataset 
signature subclass target class formed disjunction conjunctions peaks marked attributes left graphs 
note attributes signatures subclass nc non target class 
signatures subclasses nc target non target class disjunctions respective peaks graphs 
subclasses nc distinguished categorical attributes having signature parameters shown 
believe model fairly general complex represent reallife situations 
results generic model table 
show pnrule outperforms best competitor techniques combinations tr nr illustrating pnrule strength complex dataset 
pnrule especially better rare classes demonstrating pnrule superiority different situations including complex close reality scenarios interesting see claim pnrule especially suitable rare classes 
datasets described far target class size set training dataset 
take general dataset designed 
keeping data generation model vary percentage target class examples available learner 
done datasets training test target dataset nr nr rec prec rec prec tr cte cte re re tr cte cte re re table comparative results dataset 
cte re ripper na class na number subclasses 
subclass distinguished number disjoint signatures distinct pair attributes 
distinct pair attributes total combinations words identify corresponding signature 
subclass target class values different set distinguishing words 
subclass non target class values different set distinguishing words 
example target class na subclass csig csig csig ac ac ac ac ac ac ac ac csig ac ac ac ac ac ac ac ac description categorical datasets categorical datasets dataset target class non target class na na coa coa coa coa coa coa coad coad coad coad dataset rules ripper pnrule rec prec rec prec rec prec coa coa coa coa coa coa coad coad coad coad table comparative results categorical datasets 
table gives description data set 
see text explanation column names 
nc nc nc nc nc nc nc nc nc nc nc subclasses target non target respectively distinguished categorical attributes na nc na nc nc nc nc description dataset 
total attributes categorical attributes distinguish subclasses nc target non target class respectively 
numerical attributes class distributions shown distinguish subclasses 
distribution subclass attributes random uniform 
dataset nr nr rec prec rec prec tr re re tr re re table comparative results dataset dataset tr nr ntc frac tc rules ripper pnrule dataset tr nr ntc frac tc rules ripper pnrule table comparing effect variation target class size dataset 
ntc frac fraction non target class examples randomly sampled dataset target class proportion 
class rules ripper old pnrule rec prec rec prec rec prec probe table results rules ripper older version pnrule binary classification probe kddcup test data set 
class proportion retaining target class examples randomly sampling varying fraction nontarget class examples 
results table 
target class proportion increases difference performances techniques lesser lesser 
simpler dataset tr nr pnrule certainly definitive edge competitors target class proportions upto pnrule performs competitively target class rare 
tougher datasets tr nr shown table pnrule edge extend larger target class proportions 
pnrule clearly best choice target class rare 
final set results synthetic datasets corroborates claim classes necessary pnrule capability attacking recall precision goals separately learning high support rules get recall collectively removing false positives get inevitably covered 

pnrule real life dataset illustrated synthetic datasets pnrule phase design certainly potential perform rare classes 
section test technique actual real life dataset 
dataset domain network intrusion detection 
provided part kdd cup classifier learning contest 
data set collected monitoring real life military computer network intentionally various attacks hackers break 
original training set close records belonging attack classes attack normal class 
sample original set supplied part contest 
sample experiment 
reported results rare classes data set previous version pnrule 
interest study effect new parameters pnrule especially minimum coverage target class phase lower recall limit phase improving pnrule performance 
results rare classes probe respective populations training data set 
data set preserves examples rare classes original training set 
test data test data supplied part contest 
note test data different distribution classes probe new subclasses training data inherent limitation core data mining techniques pnrule rules ripper perform 
synthetic datasets default settings ripper tested training sets stratified 
best results classifiers test dataset table 
results obtained previous version pnrule shown table 
results certainly better rules ripper 
want see obtain improvement improved pnrule version 
start varying control parameters minimum target class coverage phase denoted rp lower limit recall control growth rule phase denoted rn 
ripper evaluation metric information gain experiments 
results follows denote recall precision measure respectively rn rp effect increasing rn negligible value rp 
rp covers entire target class phase covering relatively false positives variation rn role play controlling refined rules 
large values rn forced refine rules point cover small number false positives phase original target class 
seen precision recall get affected 
best result just similar ripper 
observed rules covering relatively small number false positives 
usually thing skewed different distribution test data decided try making rules general 
effect variation rp rn model learned rule length kept rn rp observation results restricting rule length allows rules general giving pnrule ability collectively remove false positives second phase 
furthermore varying rp rn control recall precision obtainable classifier class 
improved substantially ripper rules learning signatures 
see pnrule performs probe doing similar experiments 
results length controlled growth criterion section artificially restricted probe rn rp see similar effect varying rn seen affecting performance 
obtain results close ripper results 
observed phase covering relatively small number false positives 
decided rule general 
results rule length probe rn rp performance gets sudden boost general rules 
pnrule margin performance probe stronger rules ripper compared previous pnrule version 
pnrule performance probe attributed things ability collectively remove false positives second phase second ability control recall precision varying rn rp 
result probe show helps generate false positives phase 
results indicate choosing correct values parameters rp rn important 
rn set high rn probe classifier gain somewhat recall may lose precision significantly overfitting rules able remove false positives 
course rn low rn total recall suffer measure lower 
rp set high rp rn rules phase may run overfitting probably due problem small disjuncts reducing recall 
rp set low rp table rule length restricted artificially recall may suffer low target class coverage 
general combination rp rn determines performance individual values 
observations results illustrate twophase approach control parameters improved version pnrule give extra tools ability outperform existing state art techniques class efficient modeling rare classes challenging real life situations 

empirically demonstrate necessity phase rule induction framework learning effective rule models especially target class modeled small sufficient number examples training data 
experiments specially designed synthetic datasets real life data set show strength ability technique perform rare target classes 
time data sets identify analyze situations existing state art classifiers fail build acceptable model 
primary contribution show method attacking recall precision separately phases potential solve problem false positives problem small disjuncts existing techniques may face especially modeling rare classes 
proposed framework opens avenues research 
possibilities evaluating various stopping criteria growing rules automating guiding selection recall limits stage adding pruning mechanisms protect stage running overfitting improving scoring mechanism reflect close true probabilities extending phase approach multi phase approach 

agarwal joshi 
pnrule new framework learning classifier models data mining case study network intrusion detection 
proceedings siam conference data mining chicago april 
expanded version available ibm research division report rc april 
breiman 
bagging predictors 
machine learning 
clark niblett 
cn induction algorithm 
machine learning 
cohen singer 
simple fast effective rule learner 
proc 
annual conference american association artificial intelligence pages 
cohen 
fast effective rule induction 
proc 
twelfth international conference machine learning lake tahoe california 
provost 
small disjuncts action learning diagnose errors local loop telephone network 
proc 
tenth international conference machine learning pages 
morgan kaufmann 
elkan 
results kdd classifier learning contest 
www cse ucsd edu elkan html september 
holte acker porter 
concept learning problem small disjuncts 
proc 
eleventh international joint conference artificial intelligence ijcai pages 
michalski hong lavrac 
multi purpose incremental learning system aq testing application medical domains 
proc 
fifth national conference ai aaai pages philadelphia 
mitchell 
machine learning 
mcgraw hill 
quinlan 
programs machine learning 
morgan kaufmann 
schapire singer 
improved boosting algorithms confidence rated predictions 
machine learning 

shih 
family splitting criteria trees 
statistics computing 
van rijsbergen 
information retrieval 
butterworths london 
weiss 
learning rare cases small disjuncts 
proc 
twelfth international conference machine learning pages lake tahoe california 
weiss indurkhya 
lightweight rule induction 
proc 
seventh international conference machine learning icml 
