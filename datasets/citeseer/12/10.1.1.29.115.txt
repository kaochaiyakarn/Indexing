convergence rates voting gibbs classifier application bayesian feature selection andrew ng ang cs berkeley edu computer science division university california berkeley ca michael jordan jordan cs berkeley edu computer science division department statistics university california berkeley ca gibbs classifier simple approximation bayesian optimal classifier samples posterior parameter classifies single classifier indexed parameter vector 
study voting gibbs classifier extension scheme full monte carlo setting samples drawn posterior new inputs classified voting resulting classifiers 
show error voting gibbs converges rapidly bayes optimal rate particular relative error decays rapid rate 
discuss feature selection problem voting gibbs context 
show choice prior voting gibbs algorithm high tolerance presence irrelevant features 
particular algorithm sample complexity logarithmic number irrelevant features 

bayesian methods reasoning uncertainty natural appeal increasing availability approximation algorithms played important role making methods practical 
approximation methods poorly understood 
consider elementary foundational question regarding performance sampling approximation methods setting bayesian classification 
consider setting family discriminative classifiers parameterized 
observing number training examples obtain posterior distribution 
asked classify new example exact bayesian inference demands integrate determine posterior distribution class label integral difficult perform 
simple approximation provided gibbs classifier draws single sample posterior distribution class label uses prediction 
known gibbs classifier error twice bayesian optimal classifier 
consider generalization gibbs classifier full monte carlo setting draw samples posterior distribution take majority vote samples obtain final prediction 
refer voting gibbs algorithm cf 
green denison mallick 
ask elementary important question performs relative bayesian optimal classifier 
show mild assumptions relative error voting gibbs compared bayesian optimal classifier decays rapid rate 
address case learning algorithm may prior different true prior 
reasons believe case misspecified priors important aspect analysis costly elicit prior experts simplified textbook priors substituted realistic prior available computationally intractable implement prior simplified priors easier understand 
interesting somewhat surprising application results misspecified priors problem feature selection 
particular show voting gibbs algorithm uses particular misspecified prior sample complexity logarithmic number irrelevant features result matches best known results feature selection problems frequentist relevant feature number features 
plot error vs total number features optimal classifier knows feature relevant classification decision solid voting gibbs algorithms different priors dash dash dot 
details provided section 
setting ng littlestone kivinen warmuth :10.1.1.30.7849:10.1.1.130.9013
result merely theoretical interest demonstrated empirical results shown 
results described detail section show classification error rates experiment feature relevant classification decision 
solid curve plots error rate algorithm told advance feature relevant 
curves show error rates voting gibbs algorithms told feature relevant different priors 
high degree insensitivity irrelevant features exhibited lower dashed curve surprising noteworthy 
remainder structured follows 
section provides formal problem algorithms 
section presents main results quality voting gibbs classifier case correctly specified prior section goes discuss voting gibbs context misspecified priors application feature selection 
lastly section presents experimental results section closes 

problem definition notation bayesian classification concerned problem bayesian classification discriminative setting input wish predict corresponding label 
formally assume family probabilistic binary predictors ff 
thetag parameterized theta interpreted probability example bayesian logistic regression exp gammaw gamma fi parameterized fi 
bayesian setting prior distribution theta delta 
assume fixed distribution training examples drawn iid 
training set examples generated sampling prior theta sampling iid setting independently probabilities pr jx 
theta js denote posterior distribution dataset explicitly allow case training examples case posterior reduces prior 
classifier possibly stochastic map 

example familiar bayesian optimal classifier hb obtained calculating pr jx theta js predicting hb xjs hb pr jx predicting 
voting gibbs classifier theta denote prior learning procedure 
theta theta say theta misspecified prior 
note refer bayesian optimal classifier mean classifier uses theta gibbs classifier hg theta required classify samples possibly misspecified posterior distribution theta samples probability probability gamma 
prediction hg xjs hg interested performance extension gibbs classifiers full monte carlo setting multiple samples taken posterior 
call voting gibbs vg classifier 
asked predict label input voting gibbs classifier vg draws parameter iid samples posterior distribution theta 
samples independently setting probability probability gamma 
picks output majority vote predicting 
alternatively may skip second stage sampling predict vg xjs vg 
algorithm skips step randomization probably appealing experiments 
analyses results apply 
voting gibbs uses monte carlo approximation bayesian optimal classifier vg gibbs classifier 
voting gibbs thought samples obtain monte carlo estimate pr js thresholding estimate prediction 
folk wisdom suggests small number monte carlo samples needed order bayesian classification setting 
seek investigate degree true 
important feature voting gibbs classifier draw samples line new input vector 
expensive methods markov chain monte carlo gilks rejection sampling ripley required draw samples enables perform expensive sampling offline making algorithm subsequently able classify individual inputs quickly 
error metrics training set expected generalization error hypothesis particular input pr yjx pr yjx theta js probability randomization uncertainty note bayesian expected error averaged differs pac notion error misclassification measured respect single true valiant 
cases may appropriate view test set generated distribution different training distribution 
assume testing distribution input space define generalization error classifier xd pr yjx subscript means expectation respect distributed bayesian notion error 
concerned voting gibbs classifier approximates bayesian optimal classifier hb standard ways quantify 
classifier define additional absolute error compared bayesian optimal classifier gamma define additional relative error gamma hb note measures error closely related 
example upper bound additional relative error immediately implies bound additional absolute error upper bound additional absolute error lower bound bayes error similarly implies bound additional relative error 
case correct priors performance vg improve large approaching bayes error limit 
running time vg linear important practical applications quantify exactly quickly happens 
section study rate performance vg approaches bayes error learning non learning scenarios 

voting gibbs correct priors section presents results rate error voting gibbs approaches bayesian error 
treat case correct priors theta theta case misspecified priors left section 
theta theta training play significant role identical priors give identical posteriors prove bound arbitrary prior training data may define prior posterior theta obtain bound case learning data 
turn complicated consider misspecified priors section 
case correctly specified priors theorem parts 
part states worst case additional relative error voting gibbs 
full ng jordan shows tight additional assumptions 
intuition result vg averaging random samples estimate pr jx standard deviation estimates additional error 
second part theorem shows additional fairly mild technical assumption additional relative error voting gibbs shown decay faster rate 
see note gamma hb gamma hb hb hb 
theorem fixed suppose theta theta additional relative error voting gibbs compared bayes optimal classifier vg gamma hb big hide terms depend theta suppose assume theta random variable pr jx distribution induced density small interval gamma ffi ffi vary constant sup gammaffi ffi inf gammaffi ffi 
additional relative error voting gibbs vg gamma hb hb big notation hides constants depending ffi 
proof sketch 
due space constraints prove additional absolute relative error vg gamma hb bounded quantities 
proofs additional relative error full version ng jordan 
prove bound show additional absolute error particular input bounded vg gamma hb en relabeling outputs necessary may assume loss generality pr jx fixed assume hb 
note bayesian expected error just hb hb predicts chance label 
expected error vg vg gamma pr vg gamma pr vg gamma pr vg gamma pr average samples drawn vg 
note expectation works 
vg gamma hb gamma pr gamma gamma pr gamma gamma exp gamma gamma gamma exp gamma gamma sup fl fl exp gammafl gamma second inequality hoeffding inequality referred additive form chernoff bound see kearns vazirani bounds chance mean iid random variables far expected value 
proves equation 
expectations sides respect gives vg gamma hb en completes part proof 
bound assume pr jx assume loss generality ffi 
showing additional absolute relative error bound requires weaker assumptions density stated theorem require exists constant sup gammaffi ffi 
easily verified satisfied picking ffi density integrate greater contradiction 
write additional absolute error pr vg gamma dy gamma pr jy gamma dy gamma pr jy dy gammaffi pr jy dy gammaffi gamma pr jy dy show integrals done 
easy 
gamma ffi pr jy exp gamma ffi hoeffding inequality gammaffi pr jy dy exp gamma ffi 
second integral apply hoeffding inequality get gammaffi gamma pr jy dy gammaffi gamma exp gamma gamma dy gamma gamma exp gamma gamma dy exp gamma dt completes proof 
non triviality conditions relative error 
additional relative error just additional absolute error divided 
theta fixed delta statement additional relative error interesting additional absolute error 
notes theorem big notation clear showing stronger particular absorbing term big notation sense honest bounds relative just absolute error 
note bounds number samples needed dependence quantities dimension parameter vector input space 
learning misspecified priors application feature model selection section study case misspecified priors theta theta motivating example results give theorem terms result feature selection 
classification problem inputs features unknown subset relevant 
specifically random variable string bits indicates features relevant 
prior theta assumes subset relevant features picked randomly procedure 
number relevant features chosen uniformly fg 

second gamma delta subsets features chosen randomly 
note particular feature subset size chance gamma delta chosen 
assume conditioned prior theta jr theta jr assigns positive probability classifier delta examines th feature inputs 
instance logistic regression fw fi exp gammaw gamma fi may drawing normal oe distribution 
interested evaluating vg algorithm perform feature selection 
want compare performance bayes optimal classifier knows advance exactly features relevant 
true set relevant features theta theta jr 
voting gibbs misspecified prior 
theorem ffl fixed assume training testing distributions 
fixed number relevant features 
training set size distributed uniformly fd gamma ffl gamma ffl vg log expectations random training set 
corollary ensure vg constant ffl suffices choose omega gamma log 
random training set size 
theorem contains minor technical assumption small amount randomization condition treats training set size random usually unrealistic assumption needed proof theorem 
see full ng jordan details 
note state simplest possible result theorem terms convergence rate 
note letting result gives bound setting exact bayesian inference misspecified priors 
corollary re states error bound theorem terms sample complexity result shows approximate training set size omega gamma log unreasonably small nearly known exactly features relevant 
sample complexity bayesian feature selection logarithmic total number features means bayesian feature selection particular prior described earlier insensitive presence irrelevant features 
result recovers best known rates littlestone kivinen warmuth ng sample complexity beats common wrapper model kohavi john feature selection algorithm see analysis ng :10.1.1.30.7849:10.1.1.130.9013
logarithmic dependence suggests instance square total number features need twice training data result 
alternatively view saying bayesian feature selection handle exponentially irrelevant features training examples 
believe result important implications feature design practical supervised learning tasks 
theorem proved showing general result proof deferred full ng jordan terms kl theta jj theta 
specifically theta correct prior hb theta misspecified prior vg conditions theorem vg delta kl theta jj theta results stated terms worstcase error bounds online learning bounds inspiration theorem 
closely related result worst case setting exact bayesian inference corresponding see barron 
proof theorem 
result easily shown equation observing theta theta jr theta jr gamma delta theta jr gamma delta theta implies kl theta jj theta theta log theta theta theta log gamma gamma delta delta log gamma gamma delta delta log substituted back equation gives theorem 
interesting note naive choice prior instance prior posits feature independently fixed probability relevant sequence independent coin tosses argument similar give kl theta jj theta 
gives upper bound sample complexity feature selection vastly inferior log experiments section empirically compare types priors feature selection 

experiments case correct priors experiment compares vg bayes optimal classifier simple setting chosen exact bayesian inference feasible allows repeated comparison methods 
consider parameter uniformly distributed target output input uniformly distributed 
correct priors theta theta classifier bayes optimal vg trained training examples noisy labels corrupted known noise rate probability probability 
trial classifiers observed exactly data sample 
presents plot generalization errors bayes optimal classifier voting gibbs plotted function training set size 
vg error somewhat larger bayes optimal classifier vg appears tracking quite vg performance virtually indistinguishable bayes optimal classifier 
bounds additional relative error plot additional error function training set size 
see 
expected additional relative error decrease quite rapidly additional relative error decays expect log log scale plot see line slope approximately 
ignoring single point corresponding see ex number training examples generalization error comparison voting gibbs bayes optimal classifier additional relative error voting gibbs experiment training examples experiment training examples 
plot error vs number training examples bayes optimal classifier solid vg vg vg dash dot higher corresponding lower curves 
curve vg completely overlaps bayes optimal classifier 
results reported averages trials 
plot additional relative error vg function odd 
dotted part line corresponds point graph 
ignore small sample case slope rest line approximately 
previous 
match asymptotic slope predicted theory 
repeating training set size shows nearly identical behavior additional relative error decays 
feature selection case misspecified priors second set experiments studied feature selection 
learning problem bayesian logistic regression described section bayes optimal classifier serves baseline knows exactly features relevant 
tested vg prior naive prior posits sequence independent coin tosses described previous section 
experiments training test examples reversible jump markov chain monte carlo green draw classifiers vg 
total number features vary just single feature relevant 
results shown 
results shown averages independent trials 
solid line near bottom shows error hb knows exactly feature relevant 
dashed line shows vg prior dash dot line vg naive prior 
results dramatically experimental details inputs drawn multivariate standard normal distribution 
relevant feature priors oe fi normal oe oe 
oe defined section 
relevant features oe rescaled 
naive prior feature assumed equally relevant irrelevant 
exact bayesian inference tractable long mcmc se different predicted theory prior insensitive presence large numbers irrelevant features slightly worse told exactly features relevant 
contrast number irrelevant features large error naive prior approaches random guessing 
note scale axis learning training examples features irrelevant algorithm performs 
presents results extended experiment errors vg assessed priors 
cases lower lines correspond larger values see smaller values performance quite reasonable 
shows results relevant features 
see prior exhibits high tolerance presence irrelevant features 

summary shown mild assumptions relative error voting gibbs converges bayes optimal performance rate 
tractable sample posterior distribution parameters indicates voting gibbs quence run correct prior approximate hb ground truth posterior distributions 
lastly experiments run alternative version vg described section skips second stage sampling involving drawing bernoulli predicts 
relevant feature number features relevant features number features 
plot errors vg vg vg vg dash naive dash dot priors 
higher lines correspond lower values 
relevant features 
provide practical way approximate optimal bayesian classification 
context feature selection showed voting gibbs high tolerance presence irrelevant features bounds comparable best known feature selection algorithms 
de freitas vassilis hanna pasula helpful conversations 
supported onr muri nsf iis 
barron clarke haussler 

information bounds risk bayesian predictions redundancy universal codes 
proceedings international symposium information theory 
denison mallick 

classification trees 
dey ghosh mallick eds generalized linear models bayesian perspective 
marcel dekker 
gilks richardson spiegelhalter 
eds 

markov chain monte carlo practice 
chapman hall 
green 

reversible jump markov chain monte carlo computation bayesian model determination 
biometrika 
kearns vazirani 

computational learning theory 
mit press 
kivinen warmuth 

exponentiated gradient versus gradient descent linear predictors technical report ucsc crl 
univ california santa cruz computer research laboratory 
kohavi john 

wrappers feature subset selection 
artificial intelligence 
littlestone 

learning quickly irrelevant attributes abound new linear threshold algorithm 
machine learning 
ng 

feature selection learning exponentially irrelevant features training examples 
proceedings fifteenth international conference machine learning pp 

morgan kaufmann 
ng jordan 

convergence rates voting gibbs classifier application bayesian feature selection 
www cs berkeley edu ang papers icml vg long ps 
ripley 

stochastic simulation 
john wiley 


input selection reversible jump markov chain monte carlo sampling 
advances neural information processing systems pp 

valiant 

theory learnable 
communications acm 
