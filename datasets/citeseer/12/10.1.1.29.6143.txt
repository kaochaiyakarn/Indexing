fast exact multiplication hessian barak pearlmutter dept computer sci 
eng 
oregon graduate institute nw von neumann drive beaverton bap cse ogi edu just storing hessian matrix second derivatives error respect pair weights large neural network difficult 
common large matrix compute product various vectors derive technique directly calculates hv arbitrary vector 
allows treated generalized sparse matrix 
calculate hv define differential operator rff rv note rfr hv apply rf deltag equations compute result exact numerically stable procedure computing hv takes computation local gradient evaluation 
apply technique backpropagation networks recurrent backpropagation stochastic boltzmann machines 
show technique heart iterative techniques computing various properties obviating need direct methods 
efficiently extracting second order information large neural networks important problem properties hessian appear frequently 
instance analysis convergence learning algorithms widrow le cun pearlmutter techniques estimating generalization neural networks mackay moody techniques enhancing generalization pruning weights le cun hassibi stork full second order numerical optimization methods watrous :10.1.1.27.5965
exist algorithms calculating full hessian matrix second derivative terms error respect weights backpropagation network bishop buntine weigend reasonable estimates thereof mackay just storing full hessian impractical large networks :10.1.1.27.5965
algorithm efficiently computing just diagonal hessian becker le cun le cun 
useful trace hessian needed diagonal approximation reason believe diagonal approximation general reasonable suppose system grows diagonal elements hessian dominant 
inverse diagonal approximation hessian known poor approximation diagonal inverse hessian 
derive efficient technique calculating product arbitrary vector hessian allows information pulled hessian calculating storing hessian 
common estimate hessian take product various vectors 
takes time parameters 
technique derive finds product time space approximations 
operate general framework develop basic technique proceed apply series complicated systems starting simple backwards propagation error calculate gradients backpropagation network proceeding toa deterministic relaxation network stochastic boltzmann machine 
relation gradient hessian basic technique note hessian matrix appears expansion gradient point parameter space deltaw deltaw jj error point parameter space deltaw perturbation gradient vector partial derivatives hessian matrix second derivatives respect pair elements equation analyze convergence properties variants gradient descent pearlmutter approximate effect deleting weight network le cun hassibi stork 
choosing deltaw rv vector small number 
wish compute hv 
note rv rv gamma dividing hv rv gamma provides simple approximation algorithm finding hv system gradient efficiently computed time double required compute gradient assuming gradient computed 
applying technique requires minimal programming effort 
approximation effect le cun 
unfortunately formula numeric roundoff problems 
constant small term insignificant 
small large numbers added tiny ones rv causing loss precision similar loss precision occurs subtraction original gradient perturbed nearly identical vectors subtracted obtain tiny difference 
rf deltag technique fortunately way algorithm exactly computes hv just approximating simultaneously rid numeric difficulties 
pn time typical supervised neural networks full gradient sum gradients single exemplar 
take limit equation 
left hand side stays hv right hand side matches definition derivative hv lim rv gamma rv fi fi fi fi shall see simple transformation convert algorithm computes gradient system computes new quantity 
key transformation define operator rff rv fi fi fi fi take equations procedure calculating gradient backpropagation procedure deterministic boltzmann machine gradient calculation gradient calculation involves apply rf deltag operator equation 
rf deltag differential operator obeys usual rules differential operators rff rff rff rff gg rff ae df dt oe dt note rules sufficient derive equations normally compute gradient new set equations new set variables 
new equations variables original gradient calculation right hand sides 
thought adjoint system gradient calculation just gradient calculation backpropagation thought adjoint system forward calculation error measure 
new adjoint system computes vector rfr precisely vector hv desire 
application rf deltag technique various networks utilize new technique transforming equations compute gradient equations compute hv product vector hessian mechanically derive appropriate algorithms standard sorts neural networks 
course process mechanically derive similar algorithms neural network gradient calculation procedures covered usually error sum errors patterns sums patterns hv usual clarity outer sum patterns shown necessary gradient hv procedures shown single exemplar 
simple backpropagation networks apply procedure simple backpropagation network derive algorithm set equations efficiently calculate hv backpropagation network 
algorithm independently discovered martin mller unpublished manuscript different derivation 
convenience change notation indexing weights weights doubly indexed source destination units indices ij weight unit unit dimension elements identically indexed 
sums indices limited weights exist network topology 
usual quantities occur left sides equations treated computationally variables calculated topological order assumed exist matrix ij zero diagonal put triangular form werbos 
forward computation network ji oe oe delta nonlinearity th unit total input th unit output th unit external input outside network th unit 
error measure simple direct derivative respect de dy assume depends true common error measures sum square error cross entropy 
write simple function 
backward pass ij oe ij applying rf deltag equations gives rfx gamma ji rfy ji delta rfy rfx oe forward pass backward pass ae oe rfy ij ij compact form backpropagation equations due fernando pineda unifies special cases input units hidden units output units 
case unit incoming weights input unit simplifies oe allowing value set entirely externally 
hidden unit output term 
corresponding equations backward pass output units nonzero direct error terms output units outgoing weights situation output unit simplifies 
assumption violated equation rfy term generalizes rfy ae oe rfx oe oe ae oe ij rfy vector elements terms rf ij just rfr hv quantity wish compute 
simple squared error gamma desired output unit simplifies simple output units rf rfy note equations topology neural network results variables guaranteed zero sparse particular delta delta delta delta delta delta compute single desired column hessian 
fact sequence vectors pull hessian column column efficient known technique compute hessian networks sort 
recurrent backpropagation networks recurrent backpropagation algorithm almeida pineda consists set forward equations relax solution gradient ji dy dt gammay oe dz dt gammaz oe gamma ij delta ij adjoint equations calculation hv obtained applying rf deltag operator yielding rfx gamma ji rfy ji delta dt oe rfx dt oe gamma ij ij delta rfy ij rfy equations specify relaxation process computing hv 
just relaxation equations computing linear computing new relaxation equations linear 
stochastic boltzmann machines ask technique derive hessian multiplication algorithm classic boltzmann machine ackley discrete stochastic continuous deterministic cousin application rf deltag simple 
classic bm operates stochastically binary unit states random values probability oe ji equilibrium probability state ff units related energy ff ff ff ij ff gamma exp gammae ff partition function defined ff exp gammae ff system equilibrium statistics sampled equilibrium ij ij gamma gamma ij ij hs information theoretic error term asymmetric divergence temperature gamma superscripts indicate varying environmental distributions 
applying rf deltag operator obtain ij ij gamma gamma ij oj shall soon find useful define ff rfe ff ff ff ij ij hs di letter chosen relation note hdi ij ij little calculus see gammae ff tg gammap ff zd ff gammaz hdi relation probability state energy find rfp ff ff hdi gamma ff simplify rfp ij ff rfp ff ff ff ff ff hdi gamma ff ff ff hs hdi gamma hs di gamma ij hdi gamma ij delta beautiful formula gives efficient way compute hv bm efficient way compute gradient simply sampling estimate ij hdi 
requires additional calculation broadcast single global quantity local 
collection statistics gradient accelerated equation ij hs hs js hp hp js analogous identity accelerating computation ij ij hp hs djs ij hp hp gamma deltad js deltad ji defined analogy deltae ej gamma ej ji derivation simplest sort boltzmann machine binary units pairwise connections units 
technique immediately applicable higher order boltzmann machines hinton boltzmann machines non binary units movellan mcclelland 
weight perturbation weight perturbation jabri flower alspector flower jabri kirk gradient approximated globally broadcast result computation 
done adding random zero mean perturbation vector deltaw repeatedly resulting change error deltaw deltae delta deltaw viewpoint individual weight deltae deltaw noise 
central limit theorem reasonable squares estimate deltaw omega deltaw ff numerator estimated corresponding samples deltaw deltae denominator prior knowledge distribution deltaw 
requires global broadcast deltae deltaw generated locally 
hard see way mechanically apply rf deltag procedure derive suitable procedure estimating hv 
note better approximation change error deltaw delta deltaw deltaw deltaw estimate hessian wish include properties 
define hv 
small squares sense hv best choice zv symmetric equation similar form gradient entropy geoff hinton personal communication 
error surface defined 
adding symmetry requirement squares zv vz gamma delta vv substituting rearranging terms find perspective weight deltae deltaw deltaw delta deltaw gamma deltaw delta noise 
allows estimated squares fashion locally available values deltaw globally broadcast deltae plus new quantity computed globally broadcast deltaw delta technique applies equally procedures unit perturbation similar derivation find diagonal elements need additional globally broadcast values 
applications rf deltag technique possible calculate hv efficiently 
center different iterative algorithms order extract particular properties essence allows treated generalized sparse matrix 
finding eigenvalues eigenvectors standard variants power method allow ffl calculate largest eigenvalues eigenvectors 
ffl calculate smallest eigenvalues eigenvectors 
ffl sample eigenvalue spectrum 
clever algorithm skilling choosing random vector calculating dot products delta estimate moments eigenvalue spectrum moments recover shape eigenvalue spectrum applicable hessian rf deltag technique deterministic minor modifications stochastic settings 
inverse hessian frequently necessary compute gamma key calculation newton method secondorder numerical optimization techniques optimal brain surgeon technique techniques predicting generalization rate 
rf deltag technique directly solve problem solve hx minimizing gamma conjugate gradient method exactly computing gamma iterations calculating storing gamma squares condition number known positive definite minimize hx delta square condition number 
see press page details 
step size line search optimization techniques repeatedly choose direction proceed direction distance takes system constrained minimum 
finding value minimizes called line search searches line techniques performing line search 
approximate attempt find exact constrained minimum value error gradient 
particular line search scaled conjugate gradient scg optimization procedure deterministic mller stochastic mller incarnations second order information determine far move 
order information simply second order information precisely hv calculated sided finite difference approximation equation 
benefit immediately exact calculation hv 
fact procedure independently discovered precisely application martin mller personal communication 
scg line search proceeds follows 
assuming error approximated quadratic product hv gradient allows predict gradient point line hv disregarding term wish choose minimize error take dot product set equal zero gradient constrained minimum perpendicular space consideration 
gives delta hv gamma delta hv equation gives prediction gradient access accuracy quadratic approximation wish compare gradient measurement taken point preemptively take step direction 
divorced scg algorithm application way calculating eliminate step size conventional gradient descent uses gamma jr gradually minimize gradient descent suffers poor convergence rate need constantly tune rapid convergence minimization proceeds 
simple line search suggests gamma step gamma jjr jj hr necessary modifications gradient descent momentum trivial appropriate modifications stochastic gradient setting 
course simple procedure needs augmented mechanisms check hv quadratic assumption inaccurate cause failure reduce step 
eigenvalue learning rate optimization stochastic gradient descent technique previous section stated suitable deterministic gradient descent 
typically large systems deterministic gradient descent impractical noisy estimates gradient available 
joint colleagues bell labs le cun approximation technique equation enabled treated generalized sparse matrix properties extracted order accelerate convergence stochastic gradient descent 
information accumulated online particular eigenvalues eigenvectors principle eigenspace linearly transform weight space way ill conditioned axis long narrow valleys weight space slow gradient descent circular 
exact value hv stochastic unbiased estimate hessian just single exemplar time 
computations form replaced relaxations form gamma ff gamma ff ff chosen trade steady state noise speed convergence 
summary second order information error great practical theoretical importance 
allows sophisticated optimization techniques applied appears theories generalization sophisticated weight pruning procedures 
unfortunately hessian matrix elements second derivative terms unwieldy 
derived rf deltag technique directly computes hv product hessian vector 
technique ffl exact approximations 
ffl numerically accurate drastic loss precision 
ffl efficient takes amount computation gradient calculation 
ffl flexible applies current gradient calculation procedures 
ffl robust gradient calculation gives unbiased estimate procedure gives analogous unbiased estimate hv 
procedures result applications rf deltag technique local parallel efficient original untransformed gradient calculation 
technique applies naturally backpropagation networks recurrent networks relaxation networks stochastic boltzmann machines 
give idea efficiency compute column time cycling unit basis vectors 
efficient best algorithms calculate full hessian backpropagation network studied problem 
hopefully application technique facilitate construction efficient algorithms exact second order information calculating storing full hessian 
acknowledgments yann le cun patrice simard encouragement generosity 
enthusiasm derived rf deltag technique recognized importance 
go john moody tang steve andreas weigend helpful comments careful readings 
partially supported nsf ecs onr john moody 
ackley hinton sejnowski 

learning algorithm boltzmann machines 
cognitive science 
almeida 

learning rule asynchronous perceptrons feedback combinatorial environment 
butler pages 
alspector meir 

parallel gradient descent method learning analog vlsi neural networks 
cowan giles 
press 
becker le cun 

improving convergence back propagation learning second order methods 
touretzky hinton sejnowski editors proceedings connectionist models summer school 
morgan kaufmann 
published technical report crg tr department computer science university toronto 
bishop 

exact calculation hessian matrix multilayer perceptron 
neural computation 
buntine weigend 

calculating second derivatives feedforward networks 
ieee transactions neural networks 
submission 
butler editors 
ieee international conference neural networks san diego ca 


fast stochastic error descent algorithm supervised learning optimization 
cowan giles 
press 
cowan giles editors 
advances neural information processing systems 
morgan kaufmann 
press 
flower jabri 

summed weight neuron perturbation improvement weight perturbation 
cowan giles 
press 
hassibi stork 

second order derivatives network pruning optimal brain surgeon 
cowan giles 
press 
hinton 

connectionist learning procedures 
technical report cmu cs carnegie mellon university pittsburgh pa 
jabri flower 

weight perturbation optimal architecture learning technique analog vlsi feedforward recurrent multilayer networks 
neural computation 
kirk fleischer barr 

analog vlsi implementation gradient descent 
cowan giles 
press 
le cun denker solla howard jackel 

optimal brain damage 
touretzky editor advances neural information processing systems 
morgan kaufmann 
le cun solla 

second order properties error surfaces learning time generalization 
lippmann moody touretzky editors advances neural information processing systems pages 
morgan kaufmann 
le cun simard pearlmutter 

automatic learning rate maximization line estimation hessian eigenvectors 
cowan giles 
press 
mackay 

practical bayesian framework back prop networks 
neural computation 
mller 

scaled conjugate gradient algorithm fast supervised learning 
neural networks 
press 
mller 

supervised learning large redundant training sets 
international journal neural systems 
press 
moody 

effective number parameters analysis generalization regularization nonlinear learning systems 
moody pages 
moody hanson lippmann editors 
advances neural information processing systems 
morgan kaufmann 
movellan mcclelland 

learning continuous probability distributions contrastive hebbian algorithm 
technical report pdp cns carnegie mellon university dept psychology pittsburgh pa pearlmutter 

gradient descent second order momentum saturating error 
moody pages 
pineda 

generalization back propagation recurrent neural networks 
physical review letters 
press flannery teukolsky 

numerical recipes cambridge university press 
skilling 

eigenvalues mega dimensional matrices 
skilling editor maximum entropy bayesian methods pages 
academic publishers 
watrous 

learning algorithms connectionist networks applied gradient methods nonlinear optimization 
butler pages 
werbos 

regression new tools prediction analysis behavioral sciences 
phd thesis harvard university 
widrow mccool johnson jr 

stationary nonstationary learning characteristics lms adaptive filter 
proceedings ieee 

