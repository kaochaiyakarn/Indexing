learning belief networks presence missing values hidden variables nir friedman computer science division soda hall university california berkeley ca nir cs berkeley edu years flurry works learning probabilistic belief networks 
current state art methods shown successful learning scenarios learning network structure parameters complete data learning parameters fixed network incomplete data presence missing values hidden variables 
method demonstrated effectively learn network structure incomplete data 
propose new method learning network structure incomplete data 
method extension expectation maximization em algorithm model selection problems performs search best structure inside em procedure 
prove convergence algorithm adapt learning belief networks 
describe learn networks scenarios data contains missing values presence hidden variables 
provide experimental results show effectiveness procedure scenarios 
belief networks bn known bayesian networks directed probabilistic networks graphical representation probability distributions 
arguably representation choice uncertainty artificial intelligence 
networks provide compact natural representation effective inference efficient learning 
successfully applied expert systems diagnostic engines optimal decision making systems heckerman 
belief network consists components 
directed acyclic graph vertex corresponds random variable 
graph represents set conditional independence properties represented distribution 
component captures structure probability distribution exploited efficient inference decision making 
belief networks represent arbitrary probability distributions provide computational advantage distributions represented simple structure 
second component collection local interaction models describe conditional probability variable parents graph 
components represent unique probability distribution pearl 
eliciting belief networks experts laborious expensive process large applications 
years growing interest learning belief networks data cooper herskovits lam bacchus heckerman 
current methods successful learning structure parameters complete data data record describes values variables network 
unfortunately things different data incomplete 
current learning methods essentially limited learning parameters fixed network structure 
significant problem reasons 
real life data contains missing values non synthetic datasets uc irvine repository murphy aha 
cited advantages belief networks heckerman allow principled methods reasoning incomplete data :10.1.1.52.2692
unreasonable time require complete data training 
second learning concise structure crucial avoiding overfitting efficient inference learned model 
introducing hidden variables appear explicitly model learn simpler models 
simple example originally binder shown 
current methods learning hidden refer interested reader tutorial heckerman overviews current state field 
example network hidden variable 
simplest network capture distribution hidden variable 
variables require human experts choose fixed network structure small set possible structures 
reasonable domains clearly infeasible general 
motivation learning models place avoid strong dependence expert 
propose new method learning structure incomplete data uses variant expectation maximization em dempster algorithm facilitate efficient search large number candidate structures 
roughly speaking reduce search problem complete data case solved efficiently 
experimentally show method capable learning structure non trivial datasets 
example procedure able learn structures domain dozen variables missing values learn structure presence hidden variables 
note experiments rely prior knowledge reduce number candidate structures 
believe crucial step making belief network induction applicable real life problems 
convey main idea approach review source difficulties learning belief networks incomplete data 
common approach learning belief networks introduce scoring metric evaluates network respect training data search best network metric 
current metrics extent likelihood function probability data candidate network 
data complete independencies encoded network structure decompose likelihood function score metric product terms term depends choice parents particular variable relevant statistics data counts number common occurrences possible assignment values variable parents 
allows modular evaluation candidate network local changes 
additionally evaluation particular change adding arc remains changing different part network removing arc 
making change need reevaluate score possible neighbors search space 
properties allow efficient learning procedures 
data incomplete longer decompose likelihood function perform inference evaluate 
evaluate optimal choice parameters candidate network structure perform non linear optimization em lauritzen gradient descent binder 
particular em procedure iteratively improves current choice parameters theta steps 
step current parameters computing expected value statistics needed evaluate current structure 
second step replace theta parameters maximize complete data score expected statistics 
second step essentially equivalent learning complete data done efficiently 
step requires compute probabilities events instance training data 
learning parameters em procedure significantly slower learning parameters complete data 
incomplete data case local change part network affect evaluation change part network 
current proposed methods evaluate neighbors networks different local changes candidate visit 
requires calls em procedure making single change current candidate 
best knowledge methods successfully applied problems choices clustering methods cheeseman chickering heckerman select number values single hidden variable networks fixed structure heckerman describes experiment single missing value observable variables 
novel idea approach perform search best structure inside em procedure 
procedure maintains current network candidate iteration attempts find better network structure computing expected statistics needed evaluate alternative structures 
search done complete data setting exploit properties scoring metric effective search 
fact exactly search procedures complete data case 
contrast current practice procedure allows significant progress search em iteration 
show experimental validation procedure requires relatively em iterations learn non trivial networks 
rest organized follows 
start section review learning belief networks 
section describe new variant em model selection em ms em algorithm show choosing model network structure maximizes com plete data score ms em step improve objective score 
section discuss details applying ms em algorithm learning belief networks minimal description length scoring metric 
section experimental evaluation procedure handling missing values learning structure hidden nodes 
section discuss implications results possible extensions 
review learning belief networks consider finite set fx xn discrete random variables variable may take values finite set 
capital letters variable names lowercase letters denote specific values taken variables 
sets variables denoted boldface capital letters assignments values variables sets denoted boldface lowercase letters belief network annotated directed acyclic graph encodes joint probability distribution formally belief network pair hg thetai 
component directed acyclic graph vertices correspond random variables xn encodes set conditional independence assumptions variable independent parents second component pair theta represents set parameters quantifies network 
contains parameter pi pi possible value pi pi pi denotes set parents belief network defines unique joint probability distribution pb xn pb pi problem learning belief network stated follows 
training set fx instances find network best matches common approach problem introduce scoring function evaluates network respect training data search best network metric 
main scoring functions commonly learn belief networks belief scoring function cooper herskovits heckerman principle minimal description length mdl lam bacchus equivalent schwarz bayesian information criterion bic schwarz 
extended concentrate mdl bic metric 
defer treatment bayesian metric full version 
hg thetai belief network fx training set assigns value variables mdl score network training data set written score equation score gamma log number parameters network 
term log likelihood log log likelihood statistical interpretation higher log likelihood closer modeling probability distribution data second term penalty term biases score metric prefer simpler networks 
refer interested reader heckerman lam bacchus detailed description score :10.1.1.52.2692
instances complete assign values variables loglikelihood term decomposes 
nx statistics number instances 
note nx delta defined complete datasets 
omit subscript nx clear context 
applying definition pb log likelihood changing order summation yields known decomposition log likelihood structure pi pi log pi easy show expression maximized pi pi pi network structure closed form solution parameters maximize log likelihood score 
second term depend choice parameters solution maximizes mdl score 
decomposition network structure score decomposes score score pi theta score pi theta pi pi log pi gamma log pi pi number parameters need represent pi 
explained decomposition crucial learning structure 
local search procedure changes arc move efficiently evaluate gains adding removing arc procedure reuse computations previous stages evaluate changes parents variables changed move 
particular search procedure exploits decomposition greedy hill climbing procedure step mdl scoring metric defined negative inverse learning attempts minimize score maximize 
performs local change results maximal gain reaches local maxima 
procedure necessarily find global maxima perform practice see heckerman 
training data incomplete assign values variables situation quite different 
case log likelihood consequently mdl score decompose 
choose parameters local interaction model independently 
issues drastic effect learning procedure 
evaluate score structure find optimal parameter setting see chickering heckerman 
usually involves form parametric search gradient descent binder em lauritzen 
candidate structure consider reevaluate choice parameters general adopt parameters computed previous candidates 
theoretical foundations standard em algorithm method parametric estimation problems missing data dempster mclachlan krishnan tanner 
extension em call model selection em ms em algorithm deals model selection parameter estimation 
start notation 
assume input dataset records 
define random variables fx ng describes assignment th input record 
set observed variables values specified gamma hidden variables 
assume class models model parameterized vector theta legal choice values theta defines probability distribution delta theta theta shorthand theta model clear context 
assume want find choice theta maximizes scoring metric form theta log theta gamma pen theta term log theta log likelihood data choice model term pen theta penalty function depend values observable variables training data 
assume observed complete data able maximize score 
unfortunately values 
examine expected score expectation possible values take 
need estimate probability assignments 
particular estimate theta expected score theta theta log theta gamma pen theta expectation value theta 
ms em algorithm stated concisely procedure ms em choose theta randomly 
loop convergence find model maximizes delta theta theta arg max theta theta theta stage choose model parameters highest expected score previous assessment 
particular instantiation algorithm show find pair theta maximizes expected score 
fact necessary maximize expected score suffices iteration choose theta theta theta theta theta 
process converges improvement objective score 
theta theta 
practice procedure change objective score negligible smaller percent 
show algorithm useful need show improves objective score iteration 
theorem theta theta theta theta theta theta iteration choose theta higher expected score previous candidate bound improve objective score 
proof theorem relatively simple extension corresponding proof parametric em mclachlan krishnan 
theorem shows ms em progress step converges 
know point convergence algorithm 
results standard em theta theta arg max theta theta theta theta stationary point theta theta gradient point zero mclachlan krishnan 
means choice parameters point convergence local maxima local minima saddle point say choice model convergence point 
formal notion stationarity apply discrete space possible models 
define class stationary points algorithm points algorithm converge 
note set points subset set stationary points spaces parameterization candidate models run standard parametric em potentially tered number stationary points 
discussion suggests modification learning algorithm 
shall see maximizing choice parameters theta fixed model computationally cheaper searching better model 
modify algorithm alternates iterations optimize parameters current model candidate iterations search different model 
call procedure alternating ms em ams em procedure ams em choose theta randomly 
loop convergence loop convergence max theta arg max theta theta theta find model maximizes delta theta theta arg max theta theta theta variant attempts progress parametric em steps specified number steps convergence 
considers making additional progress changing choice model 
lead computational advantages shall see section situations avoid early convergence undesirable local maxima 
ms em learning belief networks apply ms em algorithm learning belief networks mdl score need show choose model theta increases expected score 
usual models exponential family get expected score form complete data score 
definition mdl score linearity expectation get theta theta pi theta theta pi theta pi pi log pi gamma log pi take expectation pb get analogue decomposition mdl score complete data case 
difference expected statistics model actual statistics 
complete data case maximize expected score particular network structure setting pi pi jo pi jo consequence search strategies exist complete data case 
ms em algorithm belief networks implemented architecture described 
search engine module responsible choosing candidate networks evaluate 
candidate calls score data training current model expected statistics score search engine candidate networks architecture implementation ms em algorithm 
module implements particulars scoring metric mdl bayesian scoring 
evaluate score score module requires corresponding statistics supplied statistics module 
complete data case statistics module answer queries counting instances training data 
ms em module computes expected statistics network previous iteration 
implementation previous implementation complete data learning software 
ms em involved minor changes statistics module implementation inference algorithm 
running time execution procedure spent computations expected statistics 
procedure differs parametric em 
parametric em know advance expected statistics required 
statistics events form pi pi parents fixed network structure considering 
variables parents current candidate employ efficient inference algorithms compute required statistics pass training data clique tree algorithm lauritzen spiegelhalter 
procedure determine advance statistics required 
handle query separately 
consider different structures bound evaluate larger number queries parametric em structural iteration search better network structures expensive parametric iteration update parameters 
usually need iterations converge best parameters suggests ams em algorithm save computational resources 
additionally implemented straightforward mechanisms reduce overhead computing statistics 
iteration clique tree algorithm compute statistics current candidate 
computation needed parametric optimization steps ams em statistics initial phases search ms em 
second log loss percent missing values insurance ms em samples ams em samples ms em samples ams em samples ms em samples ams em samples log loss percent missing values alarm ms em samples ams em samples ms em samples ams em samples plots showing degradation learning performance function percentage missing values 
horizontal axis shows percentage missing values vertical show log loss higher better 
statistics module caches queries computed current network 
allow avoid computing query marginals iteration 
crucial point choice initial model ms em 
clearly 
choice determines convergence point algorithm algorithm deterministic general want choose simple initial structure structure embodies independencies biases expected statistics indicate variables independent 
hand want choose complex initial structure structure hard perform inference 
sections discuss particular choices initial structure scenarios consider 
usual em procedures choose initial parameters chosen structure randomly 
order avoid unfortunate early convergence usually run procedure times starting different initial points 
choose network highest score runs 
experimental results section report initial experimental results scenarios incomplete data missing values hidden variables 
discuss aspects procedure relevant type problem address 
settings evaluate procedure density estimator learns distribution training data 
interesting aspect measure belief networks model structure domain number correct arcs 
defer analysis full version 
missing values real life data sets contain missing values 
order evaluate procedure performed experiment examines degradation performance learning procedure function percentage missing values 
experiment generated artificial training data networks insurance network classifying car insurance applications binder variables alarm network intensive care patient monitoring beinlich variables 
network randomly sampled training sets different sizes 
randomly removed values training sets get training sets varying percentage missing values 
data point generated independent training sets 
training set started procedure random initial points 
choose initial structure random chain network connected variables 
suspect missing value problems relatively low number missing values gamma topology initial network crucial 
hope verify experimentally 
experiments tried ms em procedure ams em procedure 
evaluated performance learned networks measuring model target distribution 
sampled test set instances evaluated average log loss learned network test set log pb 
test set evaluating learned networks 
results summarized 
expected degradation performance large number missing standard method evaluating density estimates 
network experiments 
shaded nodes correspond hidden variables 
networks learned samples hidden variable 
variables 
reasonable percentage missing values degradation moderate non existent 
results encouraging show values missing procedure usually performs better receives half number complete instances 
note missing values virtually instances dataset incomplete 
results show ams em algorithm roughly performs ms em algorithm 
note experiment values missing random 
real life domains case 
domains pattern missing values provide additional information patient ill take certain test 
information easily modeled introducing new variable labeled seen denotes observed 
datasets annotated information handled procedure 
allows learning procedure learn correlations observations various variables value variables 
approaches utilizes belief network describe domain process observation domain 
hope return issue 
hidden variables domains observable variables describe relevant aspects world 
adverse effect learning procedure 
consider example medical domain training data consists observed symptoms fever headache blood pressure medication prescribed doctor 
hidden quantity observe disease patient 
knowledge patient disease suspect slight improvement score due randomness procedure 
apparently allows search escape local maxima missing values 
treatment independent symptoms 
hand observe disease observables related 
hope introducing hidden variables able learn simpler models prone overfitting efficient inference 
growing interest learning networks include hidden variables 
unfortunately methods learn hidden variables restrict small number structure candidates done cheeseman chickering heckerman cost running em find parameter setting candidate structure high 
procedure allows learn network structures hidden variables 
experiments attempt show effectiveness procedure 
describe experiments explain choose initial network procedure 
start simple observations 
hidden variable isolated network computing expected statistics expected statistics show independent variables 
ms em algorithm add arcs second suppose learn network leaf arcs leading root child 
case marginalizing learned distribution effect distribution observable variables 
contribute representation domain 
observations suggest hidden variable beneficial connected variables networks 
lose property effectively ignoring hidden variable 
want ensure choice initial structure connects hidden variables observable variables 
structure ensures case bipartite graph hidden variables parents observable vari log loss hidden network inst 
inst 
inst 
inst 
log loss hidden network inst 
inst 
inst 
inst 
plots showing learning performance learning models hidden variables function number hidden variables 
results shown average loss different experiments sample size 
able 
course hidden variables network require parameters 
cases randomly choose edges bipartite graph ensuring observable variable certain fixed number parents 
usual randomly choose parameters network 
random choices create weak dependencies observable variables hidden variables 
immediately apply ms em initial guess choosing structure hidden variables independent rest networks 
avoid problem run iterations standard em procedure initial network 
iterations change parameters stronger dependencies hidden variables observables 
preprocessing stage continue ms em ams em 
experiments significant difference ms em ams em focus reminder section 
experiment created networks topology shown variables binary topology shown network second network hidden variables groups observed variables 
includes variables problem harder 
quantified networks randomly chosen parameters 
sampled network training sets sizes instances observable variables learned networks presence hidden binary variables ams em algorithm 
tested average log loss procedure separate test set 
results summarized 
experiments ms em algorithm led similar results omit space limitations 
results show introducing hidden variables results improved density estimation 
number instances grows larger able afford learn complex structures impact adding hidden variables diminishes 
note small samples additional hidden variables lead worst performance 
suspect due overfitting currently exploring phenomena 
discussion introduced new method learning belief networks incomplete data 
preliminary experimental results show method allows learn non trivial domains 
believe results impact domains 
particular planning explore application method learning temporal models involve hidden entities 
models crucial applications speech recognition computational biology reinforcement learning 
additionally exploring method classification tasks 
main computational efforts procedure computation expected statistics structural iterations procedure performs search 
current inference procedure unoptimized restricted scope reported experiments 
currently replacing module optimized inference engine 
additionally examining stochastic variant ms em analogous stochastic em mclachlan krishnan tanner 
variant sampling procedures complete training data 
partial instance sample complete instances current estimate model 
learn completed data complete data methods 
approach appealing exact inference methods running time sampling sensitive complexity network 
examining alternative search procedures attempt escape local maxima various local perturbations 
additional topic deserves attention learning models hidden variables 
current procedure starts set hidden variables attempts find model includes 
sophisticated approach create hidden variables needed basis learning process 
requires making decision add new variable insert current model 
currently exploring methods recognizing insert hidden variable 
current presentation focused mdl learning score 
analogous development carried bayesian learning score cooper herskovits heckerman relatively straightforward manner 
due space restrictions defer presentation issues full version 
acknowledgments am grateful stuart russell geoff zweig particularly moises goldszmidt daphne koller comments earlier version useful discussions relating topic 
research supported aro muri program integrated approach intelligent systems number daah 
beinlich suermondt chavez cooper 
alarm monitoring system case study probabilistic inference techniques belief networks 
proc 
nd european conf 
ai medicine 
berlin springer verlag 
binder koller russell kanazawa 
adaptive probabilistic networks hidden variables 
machine learning volume 
cheeseman kelly self stutz taylor freeman 
autoclass bayesian classification system 
ml 
chickering heckerman 
efficient approximations marginal likelihood incomplete data bayesian network 
proc 
twelfth conference uncertainty artificial intelligence uai pp 

cooper herskovits 
bayesian method induction probabilistic networks data 
machine learning 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
heckerman 

tutorial learning bayesian networks 
technical report msr tr microsoft research 
heckerman geiger chickering 
learning bayesian networks combination knowledge statistical data 
machine learning 
heckerman mamdani wellman 
real bayesian networks 
communications acm 
lam bacchus 
learning bayesian belief networks 
approach mdl principle 
computational intelligence 
lauritzen 

em algorithm graphical association models missing data 
computational statistics data analysis 
lauritzen spiegelhalter 
local computations probabilities graphical structures application expert systems 
journal royal statistical society 
mclachlan krishnan 
em algorithm extensions 
wiley interscience 
murphy aha 
uci repository machine learning databases 
www ics 
uci edu mlearn mlrepository html 
pearl 

probabilistic reasoning intelligent systems 
san francisco calif morgan kaufmann 
schwarz 

estimating dimension model 
annals statistics 
tanner 

tools statistical inference 
new york springer verlag 
