evaluation metrics language models widely evaluation metric language models speech recognition perplexity test data 
perplexities calculated efficiently access speech recognizer correlate speech recognition word error rates 
research attempt find measure perplexity easily calculated better predicts speech recognition performance 
investigate approaches attempt extend perplexity similar measures utilize information language models perplexity ignores 
second attempt imitate word error calculation speech recognizer artificially generating speech recognition lattices 
test new metrics built varied language models 
find perplexity correlates word error rate remarkably considering gram models trained domain data 
considering types models novel metrics superior perplexity predicting speech recognition performance 
conclude measures predict word error rate sufficiently accurately effective tools language model evaluation speech recognition 

literature primary metrics estimate performance language models speech recognition systems 
evaluated word error rate wer yielded placed speech recognition system 
second commonly evaluated perplexity test data informationtheoretic assessment predictive power 
word error rate currently popular method rating speech recognition performance computationally expensive calculate 
furthermore calculation generally requires access speech recognition system publically available 
word error rate speech difficult different research sites compare language models measure 
perplexity hand computed trivially isolation perplexity pp language model supported national security agency mda mda darpa award daah 
views contained document authors interpreted representing official policies expressed implied government 
stanley chen douglas beeferman ronald rosenfeld school computer science carnegie mellon university pittsburgh pa sfc roni cs cmu edu word history test set pp just inverse geometric average probability assigned word test set model 
perplexity theoretically elegant logarithm upper bound number bits word expected compressing domain text employing measured model 
unfortunately language models lower perplexities tend lower word error rates numerous examples literature language models providing large improvement perplexity baseline model yielded little improvement word error rate 
addition perplexity inapplicable unnormalized language models models true probability distributions sum perplexity comparable language models different vocabularies 
research attempt find measure evaluating language models applicable unnormalized models predicts word error rate accurately perplexity perplexity computationally inexpensive computed separately speech recognition system 
consider different approaches task 
approach involves extending perplexity utilize information previously ignores 
seen equation perplexity depends probabilities assigned actual text 
word error rate depends probabilities assigned transcriptions hypothesized speech recognizer errors occur incorrect hypothesis higher score correct hypothesis 
consider metrics harness information 
second approach involves attempt mimic process calculating word lattice rescoring speech recognition system construct lattices 
artificially generate lattices evaluate language models word error rates artificial lattices 
evaluate novel language model measures constructed language models varying types including class gram trigger cache language models 
find perplexity correlates word error rate remarkably considering gram models trained domain data 
considering types models novel metrics superior perplexity predicting speech recognition performance 
conclude measures predict word error rate sufficiently accurately effective tools language model evaluation speech recognition 

previous iyer investigate prediction speech recognition performance language models switchboard domain trigram models built differing amounts domain domain training data 
models constructed find perplexity predicts word error rate domain training data poorly domain text added 
find trigram coverage fraction trigrams test data training data better predictor word error rate perplexity 
unclear extend gram coverage comparing types models class models gram models different order 
addition measure distinguish different models trained data 
techniques building decision tree predicts relative performance models word test set 
decision tree able predict high accuracy relative performance pairs trigram models 
technique promising features build tree include lexical information part speech information phonetic lengths words 
investigate possible measures perplexity ignore detailed lexical information 

methodology research investigate speech recognition performance broadcast news domain 
generated narrow beam lattices sphinx iii recognition system trigram model trained words broadcast news text trigrams occurring excluded model 
word error rates reported calculated rescoring lattices language model 
created language models divided sets 
set contains gram models built broadcast news training data 
training set size smoothing gram order gram cutoffs varied 
set contains various kinds models including gram class models trigram models enhanced cache triggers gram models built domain data models interpolation gram models built domain domain data 
table list language models set 
held test sets consist words respectively broadcast news data 

perplexity word error rate display graph word error rate versus log perplexity models sets linear correlation word error rate log perplexity remarkably strong models set consists gram models built domain data models set disparate collection models 
indicates log perplexity may predictor speech recognition performance considering particular types models 
somewhat surprising log perplexity measured bits recall information theoretic interpretation perplexity mentioned section correlated different unit word errors 
attempt shed light apparently unrelated quantities related graph relation set data smooth alg katz poor poor poor ii set description class gram model class gram model class gram model trigram model cache trigram model cache trigram model katz katz model triggers katz model triggers ap news training data ap news training data ap news training data switchboard swb data switchboard data switchboard data ap bn models mixed ap bn models mixed swb bn models mixed swb bn models mixed table language models sets column describes order gram model unigram bigram 
data column describes size training set 
set model labeled excludes bigrams trigrams count model labeled ii excludes bigrams trigrams fewer counts 
abbreviation stands kneser ney 
smoothing method poor algorithm specially designed perform poorly 
set models trained words data gram cutoffs smoothed kneser ney smoothing specified 
ship language model probability assigned word test set chance word transcribed correctly speech recognition 
dotted lines represent curves individual models sets generate curve calculated probability assigned model word held set placed words logarithmically spaced buckets probabilities 
corresponding speech recognition run nist software mark word held set correct incorrect 
calculated fraction words bucket correct incorrect 
relate log perplexity word error rate consider approximating curves straight line correct log models constants denotes language model probability assigned word model history test set expected word accuracy correct log log pp expected word accuracy linear function perplexity 
approximation word error rate linear function word error rate word error rate set log perplexity set log perplexity word error rate vs log perplexity word accuracy word error rate linear function perplexity 
analysis rough lend insight perplexity word error rate related suggests perplexity improved perplexity wer relationship break 
example clear linear approximation poor low probabilities probability correctness predicted zero 

extending perplexity 
modeling relation language model probability word accuracy natural technique try analysis section functions displayed estimate word error rate 
log perplexity predict word error rate viewed hypothesis functions linear better empirically estimated function 
implement technique model calculated probability assigned word test set placed words log spaced buckets probabilities 
calculated average curves estimate fraction words correct bucket collated results buckets get final estimate word accuracy 
subtract produce estimate word error rate call measure ref 
graph value versus real word error rate set probability correct language model probability word probability word correct speech recognition language model probability 
line represents language models sets quantify correlation different metrics word error rate calculate linear correlation coefficient pearson measuring degree linear correlation spearman rank order correlation coefficient measuring ranks models linearly correlate kendall measuring relative performance pairs models predicted 
table display correlations perplexity ref versus word error rate 
set perplexity correlates word error rate better measure ref measures set measure ref marginally better 

additional information perplexity ref depend probabilities words test set speech recognition simply transcript 
word error rate depends probabilities assigned incorrect hypotheses particular errors occur incorrect hypothesis correct hypothesis 
example intuitive errors occur incorrect words assigned large language model probabilities 
word error rate measure ref word error rate vs measure ref set set set linear rank pair linear rank pair pp ref table correlations perplexity measure ref rate considered methods estimating effect language model probabilities word error rate examined relationship absolute language model probability assigned word frequency word occurs error speech recognition secondly examine relationship relative language model probability word compared probability assigned correct word 
say word occurs error mean word occurred transcription hypothesized speech recognizer marked incorrect word error rate scoring 
absolute relative probabilities relevant determining frequently word occurs error correct hypothesis high score relative probability probably important absolute probability may play larger role 
estimate relation absolute probability error frequency calculated language model probability assigned word hypothesis utterance held set 
placed word deemed incorrect buckets language model probability find frequency errors bucket 
estimate frequency words occurring bucket language model evaluated language model words vocabulary held set held data evaluated probabilities form words dividing errors bucket total number words bucket yields estimate probability word occurring error language model probability probability occurring error language model probability word relation language model probability word frequency word occurs error 
line represents language models sets probability occurring error language model probability word relative correct word relation language model probability word relative correct word frequency word occurs error quantity graphed 
different lines correspond individual model 
interesting note small variation curves model linearity curves plotted log log scale 
estimate relation relative probability error frequency similar procedure absolute probability step bucketing absolute probability bucket ratio probability word correct word 
order determine correct word consider substitution errors analysis 
calculating language model probability correct word history calculate language model probability word 
similar procedure described produce graph displayed 
curves quite linear log log space tightly packed tightly previous graph 
graphs create new metrics approximate word error rate 
information largely orthogonal perplexity may possible combine achieve stronger metric 
explore avenue 

artificial lattices predicting speech recognition performance examining basic features language model perplexity approach attempt mimic process calculating word error rate speech recognizer 
section discuss methods artificially generating speech recognition lattices 
word error rates calculated artificial lattices evaluate language models describe method constructing lattices artificial word error rates correlate word error rates calculated genuine lattices 
addition lattices constructed narrow artificial unclear count word occurs bucket speech recognition language model probabilities word may estimated multiple times position utterance different histories 
purposes calculation pretend total words occur word position utterance vocabulary normalize accordingly 
pick yo yo yo example artificial lattice utterance yo yo yo word error rates calculated quickly 
generating lattices simplifying assumptions method works 
assume correct hypothesis lattice 
secondly assume words lattice perfectly time aligned correct hypothesis words lattice times word correct hypothesis substitution errors considered 
advantage assumption hypotheses length words insertion penalty effect ignored 
thirdly assume words acoustically confusable word correct hypothesis words acoustic score correct word 
equivalent including acoustically confusable words position lattice setting acoustic scores zero 
assumption language weight irrelevant hypotheses acoustic score 
algorithm generating lattice test set utterance follows 
lattice just contains correct path 
start frames frames word unimportant words lattice time aligned 
word utterance randomly generate distribution specified words occur position times 
typically taken 
acoustic scores set zero 
show artificial lattice utterance yo yo yo 
generate words acoustically confusable word utterance possibility determine words acoustically nearby 
assumption choose random words genuinely acoustically confusable words affect word error rate single probability distribution generate alternatives words 
distribution reasonable unigram distribution just reflects frequency words training text 
empirically distributions form produce lattices predicting actual word error rate value worked broadcast news switchboard experiments 
value generated artificial lattices entire test set 
calculated word error rates artificial lattices models sets display graph artificial word error rate vs actual word error rate models 
table display correlation artificial word error rate actual word error rate 
perplexity marginally better set artificial word error rate substantially superior set mix models 
performed experiments switchboard task lattices generated janus speech recognition system 
real word error rate real word error rate set artificial word error rate set artificial word error rate actual word error rate vs artificial word error rate models sets generating artificial lattices values compared correlation perplexity artificial rate actual word error rate gram models 
gram models built varying training data sizes count cutoffs smoothing gram order 
table display correlations perplexity artificial word error rate artificial word error rate superior data set 
terms computation compare different metrics language model probability evaluations required word test set 
perplexity requires language model evaluation broadcast news set set linear rank pair linear rank pair pp switchboard linear rank pair pp table correlations perplexity artificial word error rate actual word error rate word far efficient 
trigram model artificial word error rate requires language model evaluations word practice actual value 
time required rescore artificial lattices word held set mhz pentium ii machine ranged minutes trigram model minutes trigram model triggers 
rescoring actual lattices trigram model required language model evaluations word 
computation time required varied hours trigram model hours trigram model triggers 
calculating artificial word error rate significantly expensive calculating perplexity expensive rescoring genuine lattices absolute times involved quite reasonable 

discussion shown perplexity predict word error rate quite conventional gram models trained domain data 
models disparate nature perplexity poorer predictor 
developed measure ref extends perplexity better predicts word error rate complex language models 
described technique generating artificial lattices word calculated lattices correlate actual error rates better perplexity 
error rate calculation lattices quite inexpensive 
despite unclear perplexity novel evaluation metrics effective tools language modeling researchers 
perplexity popular comparison measure historically allows language model research develop isolation speech recognizers theoretically elegant properties 
unfortunately modularization language modeling justified isolated measures predict application performance accurately 
perplexity indication performance application text compression shown inadequate predicting speech recognition performance 
example basic criterion language model evaluation metric distinguish language models application performances significantly different 
rate difference absolute considered significant refer find models essentially perplexity differ error rate 
property true novel evaluation metrics described 
practice language model development hub evaluations discontinued calculating perplexities calculate word error rates directly decide changes useful 
experience dictated effective course action 
consider accurate measure developed perplexity language model features 
great factors affect speech recognition performance values language weight insertion penalty search algorithm search algorithms long distance models tend effective stage language model applied decoding lattice rescoring best list rescoring language models stages interaction language model acoustic model 
factors significantly impact recognition performance unclear metric blind factors compensate effects 
measures imitate speech recognition process issues 
example artificial lattice genera tion search algorithm issue assume different search algorithms artificial lattices cause variation performance real lattices 
acoustic scores artificial lattices optimize language weights artificial lattices just real lattices 
measures complex expensive compute calculating word error rates directly attractive alternative 
existing measures perplexity novel measures accurate effective tools language model development speech recognition unclear useful continue compare language models speech recognition perplexity 
leaves researchers unpleasant requirement compare language models respect speech recognizer reasonable alternative effective measures developed 
techniques making word error rate computation expensive best list rescoring lattice rescoring narrow beam lattices techniques common practice 
move solely word error rate reporting just mirrors decision long ago acoustic modeling acoustic models accurately judged context speech recognition system 

martin ney 
adaptive language modelling word 
proceedings eurospeech 

iyer ostendorf meteer 
analyzing predicting language model improvements 
proceedings ieee workshop automatic speech recognition understanding 

peter brown vincent della pietra peter desouza jennifer lai robert mercer 
class gram models natural language 
computational linguistics december 

hermann ney ute essen reinhard kneser 
structuring probabilistic stochastic language modeling 
computer speech language 

beeferman berger lafferty 
model lexical attraction repulsion 
proceedings acl madrid spain 

kuhn de mori 
cache natural language model speech reproduction 
ieee transactions pattern analysis machine intelligence 

chen jain parikh raj ravishankar rosenfeld seymore siegler stern thayer 
hub sphinx system 
proceedings darpa speech recognition workshop february 

reinhard kneser hermann ney 
improved backing gram language modeling 
proceedings ieee international conference acoustics speech signal processing volume pages 

slava katz 
estimation probabilities sparse data language model component speech recognizer 
ieee transactions acoustics speech signal processing assp march 

alex waibel 
janus speech recognizer 
arpa slt workshop 
