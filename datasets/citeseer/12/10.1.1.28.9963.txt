optimal active learning sampling estimation error reduction nicholas roy nicholas roy ri cmu edu robotics institute carnegie mellon university pittsburgh pa usa andrew mccallum mccallum whizbang com whizbang 
labs research henry street pittsburgh pa usa presents active learning method directly optimizes expected error 
contrast popular techniques aim reduce version space size 
methods popular learning models closed form calculation expected error intractable 
approach feasible sampling approach estimating expected reduction error due labeling query 
experimental results real world data sets reach high accuracy quickly times fewer labeled examples competing methods 

traditional supervised learning methods set parameters training data 
contrast active learning framework learner freedom select data points added training set 
active learner may small number labeled examples carefully select additional examples requests labels learn result request newly gained knowledge carefully choose examples request 
way active learner aims reach high performance labeled examples possible 
active learning invaluable common case limited resources labeling data obtaining labels time consuming difficult 
cohn 
describe statistically optimal solution problem 
method selects training example labeled added training data expected result lowest error test examples 
develop method simple regression problems question answered closed form 
unfortunately tasks models optimal selection efficiently closed form 
widely active learning methods attain practicality optimizing different non optimal criterion 
example uncertainty sampling lewis gale selects example current learner lowest certainty query committee seung freund selects examples reduce size version space mitchell size subset parameter space correctly classifies labeled examples 
tong koller support vector machine method reducing version space size 
methods directly optimize metric learner ultimately evaluated learner expected error test examples 
uncertainty sampling fails selecting examples outliers high uncertainty getting labels doesn help learner bulk test distribution 
version space reducing methods query fail spending effort eliminating areas parameter space effect error rate 
methods immune selecting outliers see mccallum nigam examples 
presents active learning method combines best worlds 
method selects example optimal criterion reduced error rate test examples solves practicality problem sampling estimation 
describe method framework document classification pool sampling apply forms classification regression generative sampling 
describe implementation terms naive bayes technique apply learning method incremental training efficient example support vector machines svms poggio 
method estimates error rate log loss entropy posterior class distribution sample unlabeled examples loss posterior probability probable class sampled unlabeled examples 
round active learning select example labeling sampling unlabeled examples adding training set sample possible labels estimating resulting error rate just described 
seemingly daunting sampling re training efficient number rearrangements computation careful sampling choices efficient incremental training procedures underlying learner 
show experimental results real world document classification tasks comparison query committee reach full performance quarter number training examples 

optimal active learning sampling estimation optimal active learner asks labels examples incorporated training result lowest expected error test set 
yjx unknown conditional distribution inputs output classes fy yn marginal input distribution 
learner labeled training set consisting iid input output pairs drawn yjx estimates classification function input produces estimated output distribution pd yjx 
write expected error learner follows pd yjx pd yjx loss function measures degree disappointment differences true distribution yjx learner prediction pd yjx 
common loss functions log loss yjx log pd yjx loss yjx arg max pd jx 
order markov active learning aims select query query label added training set learner trained resulting set lower error pd pd concern pool active learning learner available large pool unlabeled examples sampled queries may chosen pool 
pool provides finite set queries estimate 
takes sampling approach error estimation choice query 
estimating expected error full distribution measure sample pool 
furthermore true output distribution yjx unknown sample estimate current learner 
log loss results estimating error entropy learner posterior distribution 
writing labeled documents log loss pd jp pd yjx log pd yjx loss pd jp max pd yjx course query true label unknown 
current learned classifier gives estimate distribution true label chosen pd yjx expectation calculation calculating estimated error possible label fy yn average weighted current classifier posterior pd yjx pd 
formulation current learner estimate true label probabilities may counter intuitive 
loss functions cause learner select examples maximizes sharpness learner posterior belief unlabeled examples 
example selected dramatically reinforces learner existing belief unlabeled examples currently unsure 
practice selecting instances labeling reasonable useful informative labelings usually consistent learner prior belief majority unlabeled examples 
algorithm consists steps 
train classifier current labeled examples consider unlabeled example pool candidate labeling request consider possible label add pair training set ii 
re train classifier enlarged training set iii 
estimate resulting expected loss equation equation 
assign average expected losses possible labeling weighted current classifier posterior pd yjx 
select labeling unlabeled example generated lowest expected error examples 
implemented naively algorithm hopelessly inefficient 
thought order reduce variance estimate create training sets sampling replacement labeled set bagging averaging resulting posterior class distribution 
see section details 
rearrangements computation number optimizations approximations algorithm efficient tractable importantly learning algorithms algorithms efficient incremental training 
cost re training adding example training set far re training entire set new 
example naive bayes classifier event counts need incremented 
svms efficient re training procedures poggio 
furthermore learners efficient algorithms incremental re classification examples pool 
incremental re classification parts computation need redone changed result additional training instance 
naive bayes svms examples algorithms permit 
adding candidate query training set need re estimate error associated examples pool effected inclusion candidate training set 
cases means simply skipping examples neighborhood candidate skipping examples features overlap features candidate 
inverted indices examples containing particular features listed extremely efficient 
pool candidate queries reduced random sub sampling pre filtering remove outliers criteria 
fact suboptimal active learning methods pre filters 
expected error estimated subsample pool 
especially pool large need examples estimate may formed examples 
remainder describe naive bayes implementation method discuss related experimental results real world data sets showing method significantly outperforms methods optimize indirect criteria query uncertainty 
outline 

naive bayes text classification text classification task tremendous practical significance interesting problem machine learning involves unusually large number features requires estimating unusually large number parameters 
domain obtaining labeled data expensive human effort reading assigning documents categories required 
large number parameters estimated small amount labeled data 
little training data estimate parameters large number features best simple learning model 
cases data support estimations feature correlations complex interactions 
classification method performs surprisingly simplicity naive bayes 
naive bayes best performing classification algorithm text nigam joachims continues widely purpose efficient simple implement significantly complex methods rarely trails far accuracy 
sampling approach active learning applied different learners 
apply naive bayes sake simplicity explanation implementation 
experiments learners item 
naive bayes bayesian classifier generative model data produced selecting class generating features instance independently class 
text classification common variant naive bayes unordered word counts features uses class multinomial generate words mccallum nigam 
tth word dictionary words jy parameters model prior probability class written jy probability generating word multinomial associated class written jy jy probability generating ith instance jyj jx ik jy ik kth word document bayes rule probability document generated class jx jx ik jy jyj jx ik jy maximum posteriori parameter estimation performed ratios counts jy jdj jx jv jvj jdj jx jdj jx jy jdj number times word occurs document jx indicator variable document label 
fast naive bayes updates equations show pool unlabeled documents estimate change classifier error label document 
order choose best candidate pool unlabeled documents train classifier jp times time classify jp documents 
performing jp classifications query computationally infeasible large document pools 
reduce total number classifications query take advantage certain data structures naive bayes classifier allow efficient retraining classifier relabeling unlabeled document 
recall equation class probability unlabeled document product word probabilities label 
compute class probabilities unlabeled document new classifier approximation modifying word probabilities product equation 
propagating changes word probabilities words labeled document gain substantial computational savings compared retraining document 
classifier learned training data add new document label training data update class probabilities class unlabeled document jx jx xp xp new word probabilities old word probabilities denominator divides old multinomials previous classifier 
product right hand side numerator multiplies new word probabilities result adding labeled document old multinomials divided equation 
new multinomials numerator obtained rapidly incrementally adding word counts terms numerator denominator need added pre existing counts rest numerator denominator jy jdj jx jx jv jvj jdj jx word count word labeled document label probabilities putative label label probabilities remain unchanged 
obtaining smoother posteriors naive bayes active learning method relies obtaining reasonably accurate class posteriors classification procedure 
known naive bayes violated independence assumption gives overly sharp posteriors probability winning class tends close losing classes probabilities close 
address problem sampling approach variance reduction known bagging breiman 
original labeled training set size different training set created sampling times replacement original 
learner creates new classifier sample procedure repeated times final class posterior instance taken unweighted average class posteriors classifiers 
round new query chosen training set bags resampled labeled document temporarily added bag turn 
regions uncertain classification case classifiers different samples give different answers 
posteriors individual classifier completely extreme bagged posterior smooth reflective true uncertainty 
approach shown necessarily reduce overfitting domingos certainly give better posterior probabilities 
interesting aspect approach applied classifier ones don give class posterior probabilities distribution classifier parameters unclear 
bagging approach sampling distribution classifiers previous related qbc abe see related section details 

related cohn 
propose statistical analyses active learning demonstrating construct queries maximize error reduction minimizing learner variance 
take advantage fact unbiased learner minimizes expected error expected sum squared error equivalent unbiased learner minimizes variance 
learner estimated distribution estimate expected variance learner querying closed form solution expected variance text classifier difficult compute 
furthermore construct exactly query maximizes reduction choosing pool possible queries 
cohn constructive query generation approach contrasted query filtering seung 
selective sampling unlabeled data learner distribution learner chooses queries sample pool stream 
data oriented perspective lewis gale uncertainty sampling algorithm choosing example greatest uncertainty predicted label 
freund 
showed uncertainty sampling converge optimal classifier quickly query committee algorithm seung 
query committee qbc approach method reduce error learner choosing instance labeled minimize size version space mitchell consistent labeled examples 
explicitly determining size version space predicted labels unlabeled example generated drawing hypotheses probabilistically version space distribution concepts version space 
hypotheses predict example label 
examples arrive stream labeled committee hypotheses disagree predicted label 
approach chooses examples split version space parts comparable size degree probability guarantees data efficiency logarithmic desired probability error 
number qbc style algorithms particular liere tadepalli committees winnow learners text classification argamon engelson dagan qbc natural language processing 
algorithm differs theirs estimating error reduction argamon simply estimating example disagreement 
point committee selection viewed monte carlo method estimating label distributions possible models labeled data 
abe bagging boosting approach maximizing classifier accuracy test data 
approach suggests maximizing margin training data accuracy test data improved approach successful grove schuurmans 
furthermore qbc algorithms qbc boosting approach fails maximize margin unlabeled data choosing query single instance smallest margin 
mccallum nigam extend earlier qbc approach pool qbc novel disagreement metric 
approaches classify level disagreement possibly occurs pool qbc best unlabeled example chosen 
argamon engelson dagan suggest probabilistic measure vote entropy committee mccallum nigam explicitly measure disagreement divergence lin pereira 
recognize error metric measure impact labeled document classifier uncertainty unlabeled documents 
factored document density error metric decrease likelihood uncertain documents outliers 
document density rough heuristic specific text classification directly measure impact document label predicted labelings 
tong koller active learning support vector machines text classification 
svm approach reduces classifier uncertainty estimating reduction version space size function querying instances 
qbc explicitly reduce version space size implicitly reducing expected error 
active learning technique propose strong assumptions linear separability data 
similar approach lindenbaum 
examine active learning minimizing expected error nearest neighbor classifiers 
approach similar respect loss function maximization expected utility exactly equivalent minimization error loss function 
smooth label distributions bagging 
tong koller describe method active learning learning parameters bayes nets 
expected posterior risk similar expected error equation 
slightly different loss function average loss possible models opposed estimating loss maximum posteriori distribution 
method emphasizes learning joint distribution instance space advantage creating better generative models may necessarily lead useful queries discriminative model 

experimental results newsgroup domain set experiments ken lang newsgroups containing articles evenly divided usenet discussion groups mccallum nigam 
performed experiments perform binary classification 
experiment classes comp graphics comp windows data pre processed remove usenet headers binary data 
words formed contiguous sequences alphabetic characters 
additionally words removed stoplist common words appear fewer documents 
mccallum nigam feature selection stemming performed 
resulting vocabulary words 
results reported average trials 
data set documents split training set documents test documents 
tested different active learning algorithms random choosing query document random 
uncertainty sampling choosing document largest label uncertainty lewis gale 
density weighted qbc choosing document greatest committee disagreement predicted label measured jensen shannon divergence weighted document density mccallum nigam 
number committees 
error reduction sampling method introduced choosing document maximizes reduction total predicted label entropy equation error equation 
number bags 
algorithms initially labeled examples class 
iteration documents unlabeled documents randomly sampled larger pool unlabeled documents candidates labeling 
error metric computed putative labeling remaining unlabeled documents just sampled pool 
shows active learning process 
vertical axis show classification accuracy heldout test set queries 
results reported average trials 
solid gray line shows maximum possible accuracy unlabeled data labeled 
queries error reduction sampling algorithm reached maximum possible accuracy 
density weighted qbc took queries reach point times slowly maintained lower accuracy remainder queries 
interesting compare documents chosen algorithms initial labeling 
looking documents chosen queries trials documents chosen error reduction sampling algorithm faq tutorial times 
comparison documents chosen density weighted qbc algorithm faq times 
high incidence highly informative documents initial phases quantitatively meaningful suggest learner behavior somewhat intuitive 
tried error reduction sampling loss performance essentially random 
explanation 
sub sampling performed interests experimental results 
real active learning setting algorithms run unlabeled data computationally feasible setting 
number added labeled examples accuracy comp graphics vs comp windows error reduction sampling density weighted qbc uncertainty sampling random 
average test set accuracy comp graphics vs comp windows error reduction sampling algorithm reaches maximum documents compared documents disagreed algorithm 
error bars placed local maximum reduce clutter 
particular newsgroups preceding experiment chosen relatively easy distinguish 
difficult text categorization problem classifying newsgroups comp sys ibm pc hardware comp os ms windows misc 
documents pre processed resulting vocabulary size 
data set documents split training set documents test documents 
unlabeled data sampled randomly documents candidate labelings iteration sampling error measured unlabeled documents 
examine documents chosen different algorithms initial phases 
error reduction sampling average incidence faqs documents compared density weighted qbc 
experiment see intuitive behavior sufficient algorithm clearly perform learners required documents achieve reasonable accuracy 
solid gray line shows maximum possible accuracy unlabeled data labeled 
queries error reduction sampling algorithm reached maximum possible accuracy 
density weighted qbc algorithm reached accuracy queries times slowly 
job category domain third set experiments data set collected whizbang 
labs 
job category data set contained documents containing job descriptions different categories clerical educational engineer 
different categories bro number added labeled examples accuracy comp sys ibm pc hardware vs comp os ms windows misc error reduction sampling density weighted qbc uncertainty sampling random 
average test set accuracy comp sys ibm pc hardware vs comp os ms windows misc 
error reduction sampling algorithm reaches maximum documents compared documents qbc algorithm 
error bars placed local maximum 
ken subcategories 
data set collected automatic spidering job openings web labeled hand 
selected engineer job category took articles engineer categories chemical civil industrial electrical mechanical operations 
documents pre processed remove job title rare stoplist words 
experiment trained naive bayes classifier distinguish job category remaining 
data point average trials job category averaged job categories 
example error reduction sampling algorithm reached accuracy maximum accuracy documents 
job category data set easily distinguishable similar accuracy achieved choosing documents random 
region interest evaluating domain initial stages shown 
algorithms catch error reduction sampling algorithm reached high accuracy quickly 

summary earlier version space reduction approach aims maximize expected error reduction directly 
pool unlabeled data estimate expected error current learner determine impact potential labeling request expected error 
reduce variance error estimate averaging learners created sampling bagging labeled data 
approach compared existing number added labeled examples accuracy job categorization error reduction sampling density weighted qbc uncertainty sampling random 
average test set accuracy job category domain distinguishing job category 
sampling algorithm reaches accuracy documents compared documents density weighted qbc random algorithms 
statistical techniques cohn compute reduction error equivalent quantity closed form approximate reduction error repeated sampling 
respect attempted bridge gap closed form statistical active learning query committee freund mccallum nigam 
results domains newsgroups domain job category domain 
results show error reduction sampling algorithm outperforms existing algorithm substantially achieving high level accuracy fewer labeling queries required density weighted qbc 
naive implementation approach computationally complex compared existing qbc algorithms 
shown number optimizations approximations algorithm efficient tractable 
ultimately trade computational complexity number queries decided favor fewer queries requires humans loop 
human labeler typically requires seconds label document time computer active learner select example large pool documents 
results typically required second computation time query 
furthermore algorithm uses sub sampling unlabeled data generate pool candidates iteration 
initially fairly restrictive pool candidates labeling increasing pool time permits algorithm considered anytime algorithm 
technique perform strongly number added labeled examples accuracy job categorization error reduction sampling density weighted qbc 
magnified view average test set accuracy job category domain distinguishing job category 
error reduction sampling algorithm clearly reaches high level accuracy density weighted qbc algorithm 
models complex complex parameter spaces regions relevant performance particular data set 
situations active learning methods version space reduction expected expend effort excluding portions version space impact expected error 
plan extend active learning technique classifiers support vector machines 
poggio describes techniques efficient incremental updates svms apply approach 
authors supported whizbang 
labs 
authors andrew ng advice suggestions 
fernando pereira kamal nigam simon tong pedro domingos provided valuable feedback suggestions earlier versions anonymous reviewers 
abe 

query learning strategies boosting bagging 
international conference machine learning icml 
argamon engelson dagan 

committee sample selection probabilistic classifiers 
journal artificial intelligence research 
breiman 

bagging predictors 
machine learning 
poggio 

incremental decremental support vector machine learning 
advances neural information processing nips 
denver cohn ghahramani jordan 

active learning statistical models 
journal artificial intelligence research 
domingos 

bayesian averaging classifiers overfitting problem 
proceedings international conference machine learning icml pp 

freund seung shamir tishby 

selective sampling query committee algorithm 
machine learning 
grove schuurmans 

boosting limit maximizing margin learned ensembles 
proc 
fifteenth national conference artificial intelligence aaai 
joachims 

text categorization support vector machines learning relevant features 
proceedings european conference machine learning 
lewis gale 

sequential algorithm training text classifiers 
proceedings international acm sigir conference research development information retrieval pp 

liere tadepalli 

active learning committees text categorization 
proceedings national conference artificial intelligence aaai 
providence ri 
lin 

divergence measures shannon entropy 
ieee transactions information theory 
lindenbaum markovitch 

selective sampling nearest neighbor classifiers 
proceedings sixteenth national conference artificial intelligence aaai pp 

mccallum nigam 

comparison event models naive bayes text classification 
aaai workshop learning text categorization 
mccallum nigam 

employing em active learning text classification 
proc 
fifteenth inter 
conference machine learning pp 

mitchell 

generalization search 
artificial intelligence 
nigam lafferty mccallum 

maximum entropy text classification 
proceedings ijcai workshop information filtering 
pereira tishby lee 

distributional clustering english words 
proceedings st acl 
seung opper sompolinsky 

query committee 
proceedings fifth annual acm workshop computational learning theory pp 

tong koller 

support vector machine active learning applications text classification 
proc 
seventeenth international conference machine learning 
tong koller 

active learning parameter estimation bayesian networks 
advances neural information processing nips 
