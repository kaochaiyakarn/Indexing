learning local structure nir friedman computer science division soda hall university california berkeley ca 
nir cs berkeley edu moises goldszmidt sri international ravenswood avenue ek menlo park ca 
moises erg sri com 
examine novel addition known methods learning bayesian networks data improves quality learned networks 
approach explicitly represents learns local structure conditional probability distributions cpds quantify networks 
increases space possible models enabling representation cpds variable number parameters 
resulting learning procedure induces models better emulate interactions data 
describe theoretical foundations practical aspects learning local structures provide empirical evaluation proposed learning procedure 
evaluation indicates learning curves characterizing procedure converge faster number training instances standard procedure ignores local structure cpds 
results show networks learned local structures tend complex terms arcs require fewer parameters 

bayesian networks graphical representations probability distributions arguably representation choice uncertainty artificial intelligence 
networks provide compact natural representation effective inference efficient learning 
suc nir friedman moises goldszmidt omega omega omega omega pr 
simple network structure associated cpd variable showing probability values 
applied expert systems diagnostic engines optimal decision making systems 
bayesian network consists components 
directed acyclic graph dag vertex corresponds random variable 
graph describes conditional independence properties represented distribution 
captures structure probability distribution exploited efficient inference decision making 
bayesian networks represent arbitrary probability distributions provide computational advantage distributions represented sparse dag 
second component collection conditional probability distributions cpds describe conditional probability variable parents graph 
components represent unique probability distribution pearl 
years growing interest learning bayesian networks data see example cooper herskovits buntine heckerman lam bacchus 
research focused learning global structure network edges dag 
structure fixed parameters cpds quantifying network learned estimating locally exponential number parameters data 
article introduce methods algorithms learning local structures represent cpds part process learning network 
structures model various degrees complexity cpd representations 
show approach considerably improves quality learned networks 
naive form cpd encoded means tabular representation locally exponential number parents variable possible assignment values parents need specify distribution values take 
example con learning bayesian networks local structure sider simple network variables correspond events alarm armed burglary earthquake loud alarm sound respectively 
assuming variables binary tabular representation cpd requires parameters possible state parents 
possible quantification cpd 
note alarm armed probability zero regardless values interaction parents simpler way situation assumed tabular representation cpd 
locally exponential size tabular representation cpds major problem learning bayesian networks 
general rule learning parameters liability large number parameters requires large training set assessed reliably 
learning procedures generally encode bias structures involve parameters 
example training set instances sampled network learning procedure choose simpler network structure original network 
tabular representation cpd requires parameters 
network parents say require parameters 
small training set network may preferred ignores effect example illustrates account number parameters learning procedure may penalize large cpd interactions variable parents relatively benign 
strategy address problem explicitly representing local structure cpds 
representation requires fewer parameters encode cpds 
enables learning procedure weight cpd number parameters requires capture interaction variable parents maximal number required tabular representation 
words explicit representation local structure network cpd allows adjust penalty incurred network reflect real complexity interactions described network 
different types local structures cpds prominent example noisy gate generalizations heckerman breese pearl srinivas 
article focus learning local structures motivated properties context specific independence csi boutilier 
independence statements imply contexts defined assignment variables network conditional probability variable independent parents 
example network alarm set nir friedman moises goldszmidt pr delta delta omega omega 
representations local cpd structure default table decision tree 
context defined conditional probability depend value values see csi properties induce equality constraints conditional probabilities cpds 
article concentrate different representations capturing local structure follows equality constraints 
representations shown general require fewer parameters tabular representation 
describes default table similar usual tabular representation list possible values parents 
table provides default probability assignment values parents explicitly listed 
example default table requires parameters parameters required tabular representation 
describes possible representation decision trees quinlan rivest 
leaf decision tree describes probability internal nodes arcs encode necessary information decide choose leaves values parents 
example tree probability regardless state probability regardless state example decision tree requires parameters 
main hypothesis incorporating local structure representations learning procedure leads important improvements quality induced models 
induced parameters reliable 
representations usually require parameters frequency estimation parameter takes average larger number samples account robust 
second global learning bayesian networks local structure structure induced network better approximation real dependencies underlying distribution 
local structure enables learning procedure explore networks incurred exponential penalty terms number parameters required taken consideration 
stress importance point 
finding better estimates parameters global structure unrealistic independence assumptions overcome deficiencies model 
crucial obtain approximation global structure 
experiments described section confirm main hypothesis 
results section show local representations cpds significantly affects learning process learning procedures require fewer data samples order induce network better approximates target distribution 
main contributions article derivation scoring functions algorithms learning local representations formulation hypothesis introduced uncovers benefits having explicit local representation cpds empirical investigation validates hypothesis 
cpds local structure exploited tasks knowledge acquisition experts mentioned noisy gate generalizations known examples heckerman breese pearl srinivas 
context learning authors noted cpds represented logistic regression noisy neural networks buntine neal spiegelhalter lauritzen 
exception buntine authors focused case network structure fixed advance motivate local structure learning reliable parameters 
method proposed buntine limited case fixed structure points decision trees representing cpds 
provide empirical theoretical evidence benefits local structured representations regards accurate induction global structure network 
best knowledge benefits relate convergence speed learning procedure terms number training instances unknown literature prior 
reminder article organized follows section review definition bayesian networks scores learning networks 
section describe forms local structured cpds consider article 
section formally derive score learning networks cpds represented default tables decision nir friedman moises goldszmidt trees describe procedures learning structures 
section describe experimental results 
section 
learning bayesian networks consider finite set fx discrete random variables variable may take values finite domain 
capital letters denote variable names lowercase letters denote specific values taken variables 
set values attain denoted val cardinality set denoted jjx jj sets variables denoted boldface capital letters assignments values variables sets denoted boldface lowercase letters val jjxjj obvious way 
joint probability distribution variables subsets conditionally independent val val val bayesian network annotated dag encodes joint probability distribution domain composed set random variables 
formally bayesian network pair hg li 
dag nodes correspond random variables edges represent direct dependencies variables 
graph structure encodes set independence statements variable independent parents set composed variable parents usually referred family standard arguments pearl show distribution satisfies independence statements encoded graph factored pa pa denote parents note completely specify distribution form need provide conditional probabilities right hand side 
precisely second component bayesian network set cpds specify conditional probability pa variables immediately follows exactly distribution form equation conditional probabilities specified deal discrete variables usually represent cpds conditional probability tables 
tables learning bayesian networks local structure contain parameter jpa value val pa val pa 
problem learning bayesian network stated follows 
training set fu un instances find network hg li best matches formalize notion goodness fit network respect data normally introduce scoring function solve optimization problem usually rely heuristic search techniques space possible networks heckerman 
different scoring functions proposed literature 
article focus attention ones frequently minimal description length mdl score lam bacchus bde score heckerman 

mdl score mdl principle rissanen motivated universal coding suppose set instances store records 
naturally conserve space save compressed version way compressing data find suitable model encoder produce compact version want able recover store model encoder compress total description length defined sum length compressed version length description model compression 
mdl principle dictates optimal model particular class interest minimizes total description length 
context learning bayesian networks model network 
network describes probability distribution pb instances appearing data 
distribution build encoding scheme assigns shorter code words probable instances shannon encoding huffman code see cover thomas 
mdl principle choose network combined length network description encoded data respect pb minimized 
implies learning procedure balances complexity induced network degree accuracy network represents frequencies describe detail representation length required storage network coded data 
mdl score candidate network defined total description length 
store article assume training data complete assigns values variables existing solutions problem missing values apply approaches discuss see heckerman 
nir friedman moises goldszmidt network hg li need describe describe store number variables cardinality variable candidate networks ignore description length comparisons networks 
describe dag sufficient store variable description pa parents 
description consists number parents followed index set pa agreed enumeration gamma delta sets cardinality 
encode number log bits encode index log gamma delta bits description length graph structure dl graph log log jpa describe cpds store parameters conditional probability table 
table associated need store jj jjx jj gamma parameters 
representation length parameters depends number bits numeric parameter 
usual choice literature log see friedman yakhini thorough discussion point 
encoding length cpd dl tab pa jj jjx jj gamma log encode training data probability measure defined network construct huffman code instances code exact length codeword depends probability assigned particular instance 
closed form description length 
known cover thomas approximate optimal encoding length gamma log pb encoding length instance description length data approximated dl data gamma log pb rewrite expression convenient form 
start introducing notation 
pd empirical probability measure description lengths measured terms bits logarithms base article 
learning bayesian networks local structure induced data set precisely define pd ae events interest val 
number instances omit subscript pd superscript subscript clear context 
clearly delta 
equation rewrite representation length data dl data gamma log pb gamman log pa gamma pa pa log pa encoding data decomposed sum terms local cpd terms depend counts pa 
standard arguments show 
proposition pa represented table parameter values minimize dl data jpa pa fixed network structure learning parameters minimize description length straightforward simply compute appropriate long run fractions data 
assuming assign parameters manner prescribed proposition rewrite dl data convenient way terms conditional entropy pa gamma log conditional entropy formula provides information theoretic interpretation representation data measures bits necessary encode value know value pa mdl score candidate network structure assuming choose parameters prescribed defined total description length dl dl graph dl tab pa pa nir friedman moises goldszmidt mdl principle strive find network structure minimizes description length 
practice usually done searching space possible networks 

bde score scores learning bayesian networks derived methods bayesian statistics 
prime example scores bde score proposed heckerman 

score earlier cooper herskovits buntine 
bde score proportional posterior probability network structure data 
learning amounts searching network maximize probability 
denote hypothesis underlying distribution satisfies independencies encoded see heckerman 
elaborate discussion hypothesis 
structure theta represent vector parameters cpds quantifying posterior probability interested pr 
bayes rule write term pr ff pr pr ff normalization constant depend choice term pr prior probability network structure term pr probability data network structure ways choosing prior network structures 
heckerman suggest choosing prior pr delta delta difference edges prior network structure penalty edge 
article prior mdl encoding pr dl graph evaluate pr consider possible parameter assignments pr pr theta pr theta theta pr theta defined equation pr theta prior density parameter assignments heckerman 
cooper herskovits identify set assumptions justify decomposing integral 
roughly speaking assume distribution pa learned independently distributions 
learning bayesian networks local structure assumption rewrite pr pr pa pa jpa pr theta jpa theta jpa decomposition analogous decomposition equation 
prior multinomial distribution theta jpa dirichlet prior integrals equation closed form solution heckerman 
briefly review properties dirichlet priors 
detailed description refer reader degroot 
dirichlet prior multinomial distribution variable specified set hyperparameters fn val say pr theta dirichlet fn val pr theta ff ff normalization constant 
prior dirichlet prior probability observing sequence values counts pr theta theta gamma gamma gamma gamma gamma gamma gammat dt gamma function satisfies properties gamma gamma gamma 
returning bde score assign theta jpa dirichlet prior hyperparameters jpa pr pa gamma jpa gamma jpa pa gamma jpa pa gamma jpa remains problem direct application method 
possible network structure assign priors parameter values 
clearly infeasible number possible structures extremely large 
heckerman propose set assumptions justify method prior network equivalent sample size assign prior probabilities parameters possible network structure 
prior assigned theta jpa structure computed prior distribution represented method assign jpa delta pb pa 
note pa parents nir friedman moises goldszmidt necessarily proposal essentially uses conditional probability pa prior network expected probability 
similarly confidence prior magnitude hyperparameters proportional expected number occurrences values pa exposition shows score network structure order predictions probability distribution set variables structure need compute set parameters quantify bayesian methodology average possible assignments theta pr pr theta pr theta theta decompose term structure assumptions stated completely observable data parameter independence get pr pr pa pr pa jpa pr jpa pa jpa dirichlet priors integrals closed form solution pr pa jpa pa jpa pa consider large data sets mdl score bde score tend score candidate structures similarly 
precisely scores asymptotically equivalent 
equivalence derived asymptotic approximations gamma delta function equation done bouckaert general result schwarz 
schwarz shows regularity constraints prior log pr log pr theta gamma log theta maximum likelihood parameters dimension setting number free parameters note term right hand side equation attempts maximize negative mdl score equation attempts minimize ignore description corresponds logarithm prior pr 
note term negligible asymptotic analysis depend learning bayesian networks local structure 
local structure discussion assumed standard tabular representation cpds quantifying networks 
representation requires variable encode locally exponential number jj jjx jj gamma parameters 
practice interaction parents pa benign regularities exploited represent information fewer parameters 
example cpd encoded parameters means decision tree contrast parameters required tabular representation 
formal foundation representing reasoning regularities provided notion context specific independence csi boutilier 
formally say contextually independent context val csi statements specific conditional independence statements captured bayesian network structure 
csi implies independence specific value context variable conditional independence applies value assignments conditioning variable 
shown boutilier 
representation csi leads benefits knowledge elicitation compact representation computational efficiency 
show csi beneficial learning models quantified fewer parameters 
see equation csi statements force equivalence relations certain conditional probabilities 
contextually independent val val 
parent set pa equal csi statements induce equality constraints conditional probability parents 
observation suggests alternative way thinking local structure terms partitions induce possible values parents variable note csi properties imply partitions partitions characterized csi properties 
partitions impose structure cpds article interested representations explicitly capture structure reduces number parameters estimated learning procedure 
focus representations relatively straightforward learn 
called default tables represent set singleton partitions additional partition contain values pa savings introduce depends values nir friedman moises goldszmidt val pa grouped 
second representation decision trees represent complex partitions 
consequently reduce number parameters 
induction algorithm decision trees somewhat complex default tables 
introduce notation simplifies presentation 
representation cpd capture partition structure represented characteristic random variable upsilon random variable maps value pa partition contains 
formally upsilon pa upsilon pa values pa pa pa values partition easy see definition upsilon get upsilon pa pa pa partition pa pa 
means describe parameterization structure terms characteristic random variable upsilon follows theta jae val ae val upsilon example consider tabular cpd representation csi properties taken consideration 
implies partitions contain exactly value pa case val upsilon isomorphic val pa 
cpd representations specify csi relations fewer partitions require fewer parameters 
sections formally describe default tables decision trees partition structures represent 

default tables default table similar standard tabular representation cpd subset possible values parents variable explicitly represented rows table 
values parents explicitly represented individual rows mapped special row called default row 
underlying idea probability node values parents mapped default row need represent values separately entries 
consequently number parameters explicitly represented default table smaller number parameters tabular representation cpd 
example showing values parents alarm armed mapped default row table probability situations regardless values formally default table object theta 
sd describes structure table rows represented explicitly represented default row 
define rows val pa learning bayesian networks local structure set rows represented explicitly 
structure defines characteristic random variable 
pa rows value upsilon pa partition contains pa pa rows value upsilon pa default partition contains values explicitly represented val pa gamma rows 
partitions defined default table correspond rows explicit representation table 
set parameters theta constitutes parameterization contains parameters jae value val upsilon 
determine pa representation need consider cases 
pa rows pa upsilon pa rows pa upsilon val pa rows partition corresponds default row 

decision trees decision tree variable tree internal node annotated parent variable outgoing edges particular node annotated values variable represented node take leaves annotated probability distribution process retrieving probability value parents follows 
start root node traverse tree reach leaf 
internal node choose subtree traverse testing value parent annotates node outgoing edge corresponds value 
suppose know pr tree shown 
follow edge right subtree edge annotated value similarly follow left edge annotated right edge till reach appropriate leaf 
formally denote tree object theta 
component represents structure tree defined recursively 
tree leaf composite tree 
leaf represented structure equal special constant composite tree represented structure form hy fs ty val gi test variable root tree ty tree structure value denote label variable tested root sub subtree associated value label 
need describe partitions induced representation 
path set arcs lying root leaf 
path consistent pa labeling path consistent assignment values pa easy verify pa val pa unique consistent path tree 
partitions induced nir friedman moises goldszmidt decision tree correspond set paths tree partition correspond particular path consists value assignments consistent 
define set parameters theta contain parameters jae value val upsilon 
associate realizable path tree distribution determine pa representation simply choose jae ae pa consistent pg unique path consistent pa 
learning local structure start section deriving mdl bde scoring functions default table decision tree representations 
describe procedures searching high scoring networks 
note material section easily generalized derive score produce learning procedure structured representation cpds represents partition values parent variables 
see boutilier 
discussion representations 

scoring functions introduce notations necessary derivations 
denote local representation pa default table tree complete table 
denote sl structure local representation theta parameterization assume theta jae val ae val upsilon 
mdl score local structure hg fl gi bayesian network local representation pa 
mdl encoding dag remains section 
changes occur encoding encode structure sl parameters theta additionally choice optimal parameters data depends choice local structure 
describe encoding sl default table tree representations 
default table need describe set rows represented explicitly table rows 
start encoding number describe rows encoding index agreed enumeration gamma jj delta sets cardinality 
learning bayesian networks local structure description length structure dl local struct log jj log jj tree need encode structure tree labeling internal nodes tree 
encoding proposed quinlan rivest 
tree encoded recursively follows leaf encoded single bit value equal 
encoding composite tree starts bit set value differentiate leaf followed description associated test variable description immediate subtrees 
encoding test variable depends position node tree 
root test variable parents 
contrast subtree choice test variable restricted single path test variable 
general variables tested path root current node tree need store log bits describe test variable 
total description length tree structure described recurring formula dl leaf log dl gamma composite tree subtrees tm formula define dl local struct dl jpa 
encode jjx jj gamma jj upsilon jj parameters description length dl param jjx jj gamma jj upsilon jj log section describe encoding data model equation 
generalize proposition describe optimal choice parameters network cpds represented local structure 
proposition pa represented local representation rewrite dl data dl data gamman ae val upsilon upsilon ae log jae wallace patrick note encoding inefficient sense number legal tree structures described bit strings significantly smaller encoding efficient easily incorporated mdl encoding 
clarity presentation quinlan rivest encoding article 
nir friedman moises goldszmidt parameter values minimize dl data upsilon ae upsilon ae case tabular cpd representation dl data minimized parameters correspond appropriate frequencies training data 
consequence result find fixed local structure minimal representation length data simply delta upsilon 
derive information theoretic interpretation dl data theta 
interpretation shows encoding depends values upsilon data processing inequality cover thomas follows upsilon pa 
implies local structure fit data better tabular cpd 
experiments confirm reduction number parameters compensate potential loss information 
summarize mdl score graph structure augmented local structure dl dl graph dl local struct dl param upsilon 
bde score local structure describe extend bde score learning local structure 
hypothesis denote hypothesis underlying distribution satisfies constraints set local structures fl ng local structure cpd bayes rule follows pr pr pr pr specification priors local structures presents additional complications specification priors structure network buntine example suggests possible priors decision trees 
natural prior local structures defined mdl description length setting pr gamma dl local struct term pr assumption parameter independence similar heckerman 
buntine parameter values possible value characteristic variable upsilon independent 
multinomial learning bayesian networks local structure sample independent derive analogue equation pr ae val upsilon ae jae pr theta jae theta jae decomposition analogous described proposition assume priors pr theta jae dirichlet get closed form solution equation pr ae val upsilon gamma jae gamma jae ae gamma jae ae gamma jae faced problem specifying multitude priors specifying pr theta jae possible combination global local structures 
objective case tabular cpds set priors prior distribution represented specific network recall values characteristic random variable partitions imposed local structure val pa 
assumptions regarding priors groupings generated partition 
assume prior value characteristic variable depend local structure 
depends set instances parents grouped particular value characteristic random variable 
example consider possible trees cpd tests tests assumption requires leaves correspond assigned prior trees 
second assume vector dirichlet hyperparameters assigned element partition corresponds union smaller partitions local structure simply sum vectors dirichlet hyperparameters assigned smaller partitions 
consider trees consists single leaf test root 
assumption requires val dirichlet hyperparameter jae root tree sum jae leaves second tree 
straightforward show prior distribution structures local structures parameters satisfies assumptions assumptions heckerman 
distribution positive real structure choice nir friedman moises goldszmidt local structure lg pr theta jae dirichlet fn delta upsilon ae val result allows represent prior information bayesian network specifies prior distribution positive real compute dirichlet hyperparameters hypothesis need evaluate learning 
note schwarz result show mdl bde scores local structure asymptotically equivalent 

learning procedures define appropriate score learning task reduces finding network maximizes score data 
unfortunately intractable problem 
chickering shows finding network quantified tabular cpds maximizes bde score np hard 
similar arguments apply learning mdl score 
indications finding optimal decision tree family np hard problem see quinlan rivest 
suspect finding graph set local structures fl jointly maximize mdl bde score intractable problem 
standard approach dealing hard optimization problems heuristic search 
search strategies applied 
clarity focus simplest greedy hillclimbing strategy initialize search network empty network repeatedly apply current candidate local change adding removing edges leads largest improvement score 
upward step repeated local maxima reached modification current candidate improves score 
heckerman 
compare greedy procedure sophisticated search procedures 
results indicate greedy hillclimbing quite effective learning bayesian networks practice 
greedy hillclimbing procedure learning network structure summarized follows 
procedure current generate successors fg current deltas core maxg score gamma score current deltas core current arg maxg score learning bayesian networks local structure deltas core return current successors current structure generated adding arc removing arc reversing direction arc 
consider legal successors involve cycle 
greedy procedure particularly efficient learning bayesian networks scores decompose 
mdl score logarithm bde score form score pa 
successors considered search modify parent sets need recompute terms evaluate successor 
cache computations get additional savings see bouckaert buntine 
allowing local structured representations modify loop adding learning operation scoring successor modification invokes local search procedure attempts find approximation best local structure cpd 
parent sets modified successor invoke procedure cpds 
specific procedures learning default tables decision trees described 
procedures applied independently cpd fix choice parents pa discussion 
procedures rely additional decomposability properties score functions terms underlying partitions defined characteristic random variable 
precisely score data local structure dl data mdl log pr bde written sum ae val upsilon score ae score ae function counts possible values takes instances upsilon ae 
decomposition implies consider refining local structure replacing partition corresponds value upsilon union partitions need reevaluate terms correspond new 
greedy strategy inducing default tables 
procedure starts trivial default table containing default row 
iteratively refines default row finding single row assignment values parents represented explicitly leads biggest improvement score 
refinement done efficiently need replace term corresponded previous default row sum terms correspond new row nir friedman moises goldszmidt new default row 
greedy expansion repeated improvement score gained adding row 
procedure summarized follows 
procedure rows arg max val pa score rows frg score rows frg score rows return rows rows rows frg inducing decision trees adopt approach outlined quinlan rivest 
common wisdom decision tree learning literature quinlan greedy search decision trees tends stuck bad local minima 
approach quinlan rivest attempts circumvent problem phased approach 
phase grow tree top fashion 
start trivial tree consisting leaf add branches greedy fashion maximal tree learned 
note stages growing phase adding branches lower score rationale continue grow branches improve score 
second phase remove harmful branches trimming tree bottom fashion 
describe phases details 
phase grow tree top fashion 
repeatedly replace leaf subtree root parent say children leaves value order decide parent split tree compute score tree associated parent select parent induces best scoring tree 
scores decomposable compute split local fashion evaluating instances respect training data compatible path root tree node split 
recursive growing tree stops node training instances associated value constant associated training set parents tested path leading node 
second phase trim tree bottom manner 
node consider score subtree rooted node better equal score replacing subtree leaf 
case subtree trimmed replaced leaf 
phases implemented simple recursive procedure receives set instances returns best tree learning bayesian networks local structure set instances 
procedure val leaf return hy fl val gi procedure homogeneous return growing phase split arg max pa score val split fu split hy split ft val gi trimming phase score score return return 
experimental results main purpose experiments confirm quantify hypothesis stated learning procedure learns local structures cpds induce accurate models reasons fewer parameters lead reliable estimation flexible penalty larger families result network structures better approximations real dependencies underlying distribution 
experiments compared networks induced table treebased default procedures procedure learns networks representation cpds 
ran experiments mdl score bde score 
bde score needed provide prior distribution equivalent sample size 
experiments uniform prior distribution examined settings equivalent sample size learning procedures search method discussed section 
ran experiments variations including different settings bde prior equivalent size different initialization points search procedures 
experiments involved learning approximately networks 
results summarized 
nir friedman moises goldszmidt table 
description networks experiments 
name description jjujj thetaj alarm network medical experts monitoring patients intensive care beinlich 
network modeling summer hail northeastern colorado www lis pitt edu dsl 
insurance network classifying insurance applications russell 

methodology data sets experiments sampled bayesian networks main characteristics described table 
networks sampled training sets sizes instances ran learning procedures 
learning procedures received data sets access generating network 
order increase accuracy results repeated experiment independently sampled sets training data 
experiments methods compared received input training data 
virtue having golden model experiment represented original networks precisely quantify error induced models original model 
able quantify effect local structures parameter estimation structure selection 

measures error main measurement error entropy distance known leibler divergence relative entropy generating distribution induced distribution 
entropy distance distribution approximation defined jjq log quantity measure inefficiency incurred assuming distribution real distribution note entropy distance symmetric jjq equal general qjjp 
important property entropy distance function jjq equality holds learning bayesian networks local structure possible justifications entropy distance 
axiomatic side shore johnson suggest desirable properties approximation measures show entropy distance function satisfies 
motivating examples data compression gambling 
examples entropy distance measures loss incurred distribution true distribution additional bits needed expected monetary losses 
refer reader cover thomas discussion detailed analysis examples 
measuring entropy distance induced networks allows compare generalization error different procedures 
interested assessing separate influences parameter estimation induced network structure error 
network structure 
define inherent error respect target distribution struct jjg min jj inherent error smallest error achievable possible choice cpds find best possible parameters hope get smaller error struct jjg 
turns measure error evaluated means closed form equation 
expect best cpds conditional distribution pa identical pa 
proposition network structure distribution 
struct jjg jj pa pa alternative way thinking inherent error network structure measure reasonable independence assumptions encoded attempt measure error network structure estimating degree independencies violated way measuring strength dependency variables measure conditional mutual information 
sets variables conditional mutual information defined gamma intuitively term measures knowledge helps compress know known independent cover thomas 
nir friedman moises goldszmidt mutual information quantitative measure strength dependencies measure extent independence assumptions represented violated real distribution 
suggests evaluate measure conditional independencies represented independence assumptions overlap sense imply 
need find minimal set independencies imply independencies represented pearl shows construct minimal sets independence 
assume variable ordering consistent arc direction parent 
independent fx gamma gamma pa pa chain rule find factored equation 
consequence find set independence assumptions implies independence assumptions represented starting different consistent orderings get different minimal sets assumptions 
proposition shows evaluating error model respect sets leads answer 
proposition network structure variable ordering consistent arc direction distribution 
struct jjg fx gamma gamma pa pa proposition shows struct jjg imap independence statements encoded true small values struct jjg indicate map dependencies captured weak 
note struct jjg sided error measure sense penalizes structures representing wrong independence statements penalize structures representing redundant dependence statements 
particular complete network structures ones add edges introducing cycles inherent error represent conditional independencies 
postulate difference error measured entropy distance inherent error due errors introduced estimation cpds 
note learn local structure additional error may due induction inappropriate local structure local structure assumptions context specific independencies hold target distribution 
global structure measure inherent error local structure learned 
network structure sl structures cpds inherent local error learning bayesian networks local structure sl local jjg fsl min theta theta ln jj theta ln theta ln get expected generalization proposition 
proposition network structure sl local structure cpds distribution 
local jjg fsl jj upsilon upsilon definitions inherent error follows network jjp local jjg fsl struct jjg measures evaluation experiments measure quality global independence assumptions network structure struct quality local global independence assumptions network structure local structure local total error includes quality parameters 

results want characterize error induced models function number samples different learning algorithms induction 
plot learning curves axis displays number training instances axis displays error learned model 
general curves exhibit exponential decrease error 
visual comparisons different learning procedures hard differences large sample range obscured logarithmic scale axis differences small sample range hard visualize 
see example 
address problem propose normalization curves motivated theoretical results friedman yakhini 
show learning curves bayesian networks generally behave linear function log plot error scaled log shows result application normalization curves 
observe resulting curves roughly constant 
thin dotted diagonal lines correspond lines constant error 
plot lines entropy distances 
nir friedman moises goldszmidt entropy distance number instances table tree default entropy distance number instances table tree default 
error curves showing entropy distance achieved procedures mdl score alarm domain 
axis displays number training instances axis displays entropy distances induced network 
curves logarithmic axis 
point average learning independent data sets 
figures display entropy distance networks learned bde mdl scores table summarizes values 
experiments learning curves appear converge target distribution eventually intersect dotted line ffl entropy distance ffl 
appear roughly conform behavior specified results friedman yakhini 
respect entropy distance tree procedures performed better experiments table procedures 
exceptions default procedures performed better procedures alarm insurance domains 
default methods performed poorly domain 
general rule see constant gap curves corresponding learning bayesian networks local structure normalized entropy distance number instances table tree default normalized entropy distance number instances table tree default normalized entropy distance number instances table tree default 
normalized error curves showing entropy distance achieved procedures bde score alarm domain domain insurance domain 
axis displays number training instances axis displays normalized entropy distances induced network see section 
nir friedman moises goldszmidt normalized entropy distance number instances table tree default normalized entropy distance number instances table tree default normalized entropy distance number instances table tree default 
normalized error curves showing entropy distance achieved procedures mdl score alarm domain domain insurance domain 
learning bayesian networks local structure different representations 
fixed error procedure representing local structure constant fraction error corresponding procedure represent local structure learns tabular cpds 
example see large sample region errors procedures trees default tables approximately respectively error table procedures 
corresponding ratios 
way interpreting results obtained looking number instances needed reach particular error rate 
example tree procedure reaches error level approximately instances 
hand table procedure barely reaches error level instances 
want ensure level performance need supply table procedure additional instances 
number instances unavailable practice 
continued investigation examining network structures learned different procedures 
evaluated inherent error struct structures learned different procedures 
experiments inherent error network structures learned tree default procedures smaller inherent error networks learned corresponding table procedure 
example examine struct column tables 
results conclude network structures learned procedures local representations fewer mistaken assumptions global independence predicted main hypothesis 
hypothesis predicts procedures learn local representation able assess fewer parameters making local assumptions independence cpds 
illustrate measured inherent local error local number parameters needed quantify networks 
see tables networks learned procedures exhibit smaller inherent error struct require fewer parameters inherent local error local roughly networks learned table procedures 
making global assumptions independence local representation procedures local assumptions independence better capture regularities target distribution require fewer parameters 
consequence parameter estimation procedures accurate 
investigated depend particular choices experiments 
see local structure leads improvements regardless choices 
examined aspects nir friedman moises goldszmidt learning process choice parameters priors search procedure 
start looking effect changing equivalent sample size heckerman 
show choice drastic effects quality learned networks 
basis experiments alarm domain heckerman report achieves best results 
table shows effect changing experiments 
see choice influences magnitude errors learned networks sizes error gaps different methods 
influences suggest changes benefits local structures 
bde score mdl score involve explicit choice priors 
bayesian averaging select parameters structures learned mdl score opposed maximum likelihood estimates 
table compare error maximum likelihood estimates bayesian averaging 
expected averaging leads smaller errors parameter estimation especially small sample sizes 
exception alarm domain bayesian averaging improve score large samples 
conclude changing parameter estimation technique may improve score instances change basic 
aspect learning process needs investigation heuristic search procedure 
better search technique lead better induced models illustrated experiments heckerman 

experiments modified search initializing greedy search procedure informed starting point 
heckerman 
maximal branching starting state search 
maximal branching network network jpa maximal branching efficient manner low order polynomial time heckerman 
table reports results experiment 
alarm domain maximal branching initial point led improvements learning procedures 
hand insurance domain choice starting point led worse error 
observe described regarding local structure held runs 

main contribution article structured representations cpds learning process identification learning bayesian networks local structure benefits representations empirical validation hypothesis 
mentioned section consider efficient representations cpds context learning 
best knowledge consider demonstrate effects representations may learning global structure network 
focused investigation fairly simple structured representations cpds trees default tables 
certainly possible representation cpds example decision graphs rules cnf formulas see boutilier 

choice mainly due availability efficient computational tools learning representations 
refinement methods studied incorporate representations deserves attention 
machine learning literature various approaches learning trees easily incorporated learning procedures bayesian networks 
addition certain interactions search procedures global local structures exploited reduce computational cost learning process 
leave issues research 
important distinguish local representations examine noisy logistic regression models examined literature 
noisy logistic regression applied bayesian network literature attempt estimate cpd fixed number parameters 
number usually linear number parents cpd 
cases target distribution satisfy assumptions embodied models estimates cpds produced methods arbitrarily diverge target distribution 
hand local representations involve learning structure cpd range lean structure parameters complex structure exponential number parameters 
representations scale accommodate complexity training data 
ensures theory asymptotically correct samples construct close approximation target distribution 
shown induction local structured representation cpds significantly improves performance procedures learning bayesian networks 
essence improvement due fact changed bias learning procedure reflect nature distribution data accurately 
nir friedman moises goldszmidt acknowledgments authors grateful anonymous reviewer wray buntine david heckerman comments previous versions useful discussions relating 
part research done authors rockwell science center palo alto laboratory 
nir friedman stanford university time 
support provided rockwell stanford university gratefully acknowledged 
addition nir friedman supported part ibm graduate fellowship nsf iri 
preliminary version article appeared proceedings th conference uncertainty artificial intelligence 
beinlich suermondt chavez cooper 
alarm monitoring system case study probabilistic inference techniques belief networks 
proc 
nd european conf 
ai medicine 
springer verlag berlin 
bouckaert 
properties bayesian network learning algorithms 
opez de poole editors proc 
tenth conference uncertainty artificial intelligence uai pages 
morgan kaufmann san francisco ca 
boutilier friedman goldszmidt koller 
context specific independence bayesian networks 
horvitz jensen editors proc 
twelfth conference uncertainty artificial intelligence uai pages 
morgan kaufmann san francisco ca 
buntine 
theory learning classification rules 
phd thesis university technology sydney australia 
buntine 
theory refinement bayesian networks 
ambrosio smets bonissone editors proc 
seventh annual conference uncertainty artificial intelligence uai pages 
morgan kaufmann san francisco ca 
buntine 
learning classification trees 
hand editor artificial intelligence frontiers statistics number iii ai statistics 
chapman hall london 
chickering 
learning bayesian networks np complete 
fisher 
lenz editors learning data artificial intelligence statistics springer verlag 
cooper herskovits 
bayesian method induction probabilistic networks data 
machine learning 
cover thomas 
elements information theory 
john wiley sons new york 
degroot 
optimal statistical decisions 
mcgraw hill new york 

parameter adjustment bayes networks generalized noisy gate 
heckerman mamdani editors proc 
ninth conference uncertainty artificial intelligence uai pages 
morgan kaufmann san francisco ca 
friedman yakhini 
sample complexity learning bayesian networks 
horvitz jensen editors proc 
twelfth conference uncertainty products names mentioned article trademarks respective holders 
learning bayesian networks local structure artificial intelligence uai 
morgan kaufmann san francisco ca 
heckerman breese 
new look causal independence 
opez de poole editors proc 
tenth conference uncertainty artificial intelligence uai pages 
morgan kaufmann san francisco ca 
heckerman geiger chickering 
learning bayesian networks combination knowledge statistical data 
machine learning 
heckerman 
tutorial learning bayesian networks 
technical report msr tr microsoft research 
lam bacchus 
learning bayesian belief networks approach mdl principle 
computational intelligence 

belief network induction 
phd thesis university california berkeley ca 
neal 
connectionist learning belief networks 
artificial intelligence 
pearl 
probabilistic reasoning intelligent systems 
morgan kaufmann san francisco ca 
quinlan rivest 
inferring decision trees minimum description length principle 
information computation 
quinlan 
programs machine learning 
morgan kaufmann san francisco ca 
rissanen 
stochastic complexity statistical inquiry 
world scientific river edge nj 
russell binder koller kanazawa 
local learning probabilistic networks hidden variables 
proc 
fourteenth international joint conference artificial intelligence ijcai pages 
morgan kaufmann san francisco ca 
schwarz 
estimating dimension model 
annals statistics 
shore johnson 
axiomatic derivation principle maximum entropy principle minimum cross entropy 
ieee transactions information theory 
spiegelhalter lauritzen 
sequential updating conditional probabilities directed graphical structures 
networks 
srinivas 
generalization noisy model 
heckerman mamdani editors proc 
ninth conference uncertainty artificial intelligence uai pages 
morgan kaufmann san francisco ca 
wallace patrick 
coding decision trees 
machine learning 
nir friedman moises goldszmidt table 
summary entropy distance networks learned procedure mdl score bde score 
mdl score bde score domain size table tree table tree default theta alarm insurance learning bayesian networks local structure table 
summary inherent error inherent local error number parameters networks learned table tree procedures bde score 
table tree domain size struct local param local struct param theta alarm insurance table 
summary inherent error inherent local error number parameters networks learned table tree procedures mdl score 
table tree domain size struct local param local struct param theta alarm insurance nir friedman moises goldszmidt table 
summary entropy distance procedures bde score 
domain size table tree default table tree default theta alarm insurance table 
summary entropy distance procedures mdl score learning structure local structure combined methods parameter estimation 
maximum likelihood bayesian domain size table tree default table tree default theta alarm insurance learning bayesian networks local structure table 
summary entropy distance methods initializing search bde score 
empty network maximal branching network domain size table tree default table tree default theta alarm insurance 
