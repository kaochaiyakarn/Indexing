algorithmic stability generalization performance olivier bousquet cmap ecole polytechnique palaiseau cedex france bousquet polytechnique fr andr technologies waters avenue ga usa andre com novel way obtaining pac style bounds generalization error learning algorithms explicitly stability properties 
stable learner learned solution change small changes training set 
bounds obtain depend measure complexity hypothesis space vc dimension depend learning algorithm searches space applied vc dimension nite 
demonstrate regularization networks possess required stability property apply method obtain new bounds generalization performance 
key issue computational learning theory bound generalization error learning algorithms 
research area focused uniform priori bounds giving guarantee di erence training error test error uniformly small hypothesis class 
bounds usually expressed terms combinatorial quantities 
years researchers tried re ned quantities estimate complexity search space covering numbers posteriori information solution algorithm margin :10.1.1.21.996
exist approaches observed vc dimension concerned structural properties learning systems 
novel way obtaining pac bounds speci algorithms explicitly stability properties 
notion stability introduced devroye wagner context classi cation analysis leave oneout error re ned kearns ron context regression order get bounds empirical error error 
method nice advantage providing bounds done author laboratoire eric universit lumi ere lyon avenue pierre es france cedex france depend complexity measure search space vc dimension covering numbers way algorithm searches space 
respect approach related freund estimated size subset hypothesis space searched algorithm bound generalization error 
freund result depends complexity term looking separately hypotheses considered algorithm risk 
structured follows section introduces notations de nition stability 
section presents main result pac theorem 
section prove regularization networks stable apply main result obtain bounds generalization ability 
discussion results section 
notations de nitions respectively input output space consider learning set fz ym size drawn unknown distribution learning algorithm function mapping learning set function fs avoid complex notations consider deterministic algorithms 
assumed algorithm symmetric respect permutation elements fs yields result 
furthermore assume functions measurable sets countable limit interest results 
empirical error function measured training set rm cost function 
risk generalization error written study describe intends bound di erence empirical generalization error speci algorithms 
precisely goal bound term ps jr fs usually learning algorithms output just function pick function fs set representing structure architecture model 
classical vc theory deals structural properties aims bounding quantity ps sup applies algorithm hypothesis space bound quantity directly implies similar bound 
classical bounds require vc dimension nite information algorithmic properties 
set exists ways search may yield di erent performance 
instance multilayer perceptrons learned simple backpropagation algorithm combined weight decay procedure 
outcome algorithm belongs cases set functions performance di erent 
vc theory initially motivated empirical risk minimization erm case uniform bounds quantity give tight error bounds 
intuitively empirical risk minimization principle relies uniform law large numbers 
known advance minimum empirical risk necessary study di erence empirical generalization error possible functions consider minimum focus outcome learning algorithm may know little bit kind functions obtained 
limits possibilities restricts supremum functions possible outcomes algorithm 
algorithm outputs null function need studied uniform law large numbers 
introduce notation modi ed training sets denotes initial training set fz mg denotes training set replaced di erent training example fz mg 
de ne notion stability regression 
de nition uniform stability fz training set training set instance removed symmetric algorithm 
say stable holds jc condition expresses possible training set replacement example di erence cost measured instance incurred learning algorithm training smaller constant main result stable algorithm stable small property replacing element learning set change outcome 
consequence empirical error thought random variable small variance 
stable algorithms candidates empirical error close generalization error 
assertion formulated theorem theorem stable algorithm learning set ps jr fs mm ps jr fs exp notice theorem gives tight bounds stability order proved section regularization networks satisfy requirement 
order prove theorem study random variable fs rm done di erent approaches 
rst corresponding exponential inequality uses classical martingale inequality detailed 
second bit technical requires standard proof techniques symmetrization 
brie sketch proof refer reader details 
proof inequality theorem theorem mcdiarmid 
yn random variables values set assume satis es sup yn jf yn yn jf yn yn order apply theorem bound expectation useful lemma lemma symmetric learning algorithm es fs rm proof notice es rm es es mg symmetry assumption 
simple renaming get es rm observation es fs concludes proof 
lemma fact stable easily get es fs rm compute constants appearing theorem 
jr fs jc rm jc jc theorem applied fs rm gives inequality 
sketch proof inequality recall chebyshev inequality jx random variable order apply inequality bound 
done similar reasoning expectation 
calculations complex describe 
details see 
result gives inequality concludes proof 
stability regularization networks de nitions regularization networks introduced machine learning poggio girosi 
relationship networks support vector machines bayesian interpretation attractive 
consider training set ym regression setting 
regularization network technique consists nding function space minimizes functional kfk kfk denotes norm space framework chosen reproducing kernel hilbert space rkhs basically functional space endowed dot product rkhs de ned kernel function symmetric function assume bounded constant follows particular property hold jf stability study section show regularization networks furthermore stable soon small 
theorem regularization networks fs rm ln fs rm proof denote fs minimizer de ne kfk minimizer denote di erence fs simple algebra fs fs tg fs detail properties space refer reader additional details :10.1.1.21.996
give full detail de nition appropriate kernel functions refer reader 
explicitly written factor similarly tg optimality fs fs tg tg summing inequalities dividing making get kgk gives kgk 
obtain kf fs kh jf proved minimization stable procedure allows apply theorem 
discussion inequalities interest range tight di erent 
poor dependence deteriorate high con dence sought 
give high con dence bounds looser small 
results exposed indicate optimal dependence obtained ln ln 
plug bounds notice converge 
may conjectured poor estimation variance coming martingale method mcdiarmid inequality responsible ect ner analysis required fully understand phenomenon 
interests results provide mean choosing parameter minimizing right hand side inequality 
usually determined validation set data learning chosen error fs validation set minimized 
drawback approach reduce amount data available learning 
new approach get bounds generalization performance learning algorithms speci properties algorithms 
bounds obtain depend complexity hypothesis class measure stable algorithm output respect changes training set 
focused regression believe extended classi cation particular making stability requirement demanding stability average uniform stability 
aim nding algorithms stable appropriately modi ed exhibit stability property 
promising application model selection problem tune parameters algorithms parameters kernel regularization networks 
cross validation measure stability uenced various parameters interest plug measures theorem derive bounds generalization error 
acknowledgments lugosi chapelle interesting discussions stability concentration inequalities 
smola anonymous reviewers helped improve readability 
alon ben david cesa bianchi haussler :10.1.1.21.996
scale sensitive dimensions uniform convergence learnability 
journal acm 

theory reproducing kernels 
trans 
amer 
math 
soc 

kernels splines functions 
studies computational mathematics 
north holland 
devroye wagner 
distribution free performance bounds potential function rules 
ieee trans 
information theory 
study algorithmic stability relation generalization performances 
technical report laboratoire eric univ lyon 
pontil poggio 
uni ed framework regularization networks support vector machines 
technical memo aim massachusetts institute technology arti cial intelligence laboratory december 
freund 
self bounding learning algorithms 
proceedings th annual conference computational learning theory colt pages new york july 
acm press 
kearns ron 
algorithmic stability sanity check bounds leave oneout cross validation 
neural computation 
mcdiarmid 
method bounded di erences 
surveys combinatorics pages 
cambridge university press cambridge 
poggio girosi 
regularization algorithms learning equivalent multilayer networks 
science 
shawe taylor bartlett williamson anthony :10.1.1.21.996
framework structural risk minimization 
proc 
th annu 
conf 
comput 
learning theory pages 
acm press new york ny 
shawe taylor williamson 
generalization performance classi ers terms observed covering numbers 
paul fischer hans ulrich simon editors proceedings th european conference computational learning theory eurocolt volume lnai pages berlin march 
springer 
