complexity loss functions rissanen ibm research division almaden research center dpe san jose ca rissanen almaden ibm com loss complexity loss functions defined analogously stochastic complexity logarithmic loss functions mean provides achievable lower bound estimation mean taken respect worst case data generating distribution need lie set defined loss function 
loss complexity provides lower bound worst case mean prediction error predictors 
important ff functions jy gamma yj ff gamma denotes prediction fitting error ff interval accurate asymptotic formula loss complexity 
index terms ff functions complexity maximum entropy minmax bounds prediction bound simple loss functions yamanishi defined extended stochastic complexity variety loss functions follows gamma ln gamma ffi parametric estimate prediction ffi measures distance estimate prior density function parameters positive parameter 
main justification definition predictive estimation shown give effective learning algorithms aggregating strategy computational learning theory introduced vovk pioneered mixture type 
laplace method integration yamanishi derived asymptotic expansion gives upper bound extended stochastic complexity batch mode loss 
yamanishi showed extended stochastic complexity attains minmax cumulative prediction loss specific restricted loss functions maximum taken sequences 
inspired works define extension stochastic complexity call loss complexity way analogous stochastic complexity mean provides achievable lower bound mean accumulated loss 
mean taken respect worst case data generating distribution class need coincide class models defined loss function 
loss complexity gives lower bound worst case mean prediction error resulting predictor 
bounds appear new 
order able calculate loss complexity consider mainly called simple loss functions normalizing coefficient gamma gammaffi dy depend 
show class includes important ff functions ffi jy gamma yj ff positive ff 
ff range ff derive accurate asymptotic formula loss complexity 
determine optimal parameter function data case lower bound worst case mean loss loss complexity mean 
formula loss complexity provides convenient criterion selection model classes particular absolute value error function lack differentiability obstacle past 
results allow generalize earlier prediction bound gaussian arma processes shows lower bound prediction estimation stochastic complexity restricted single worst case data generating distribution holds essence wide class distributions 
loss complexity simple loss functions turns consist minimized loss term viewed ideal code length optimal parameters suitably weighted 
extended stochastic complexity shown admit similar asymptotic upper bound second term weighted real code length optimally quantized parameters 
show section loss functions representation holds restrictions effect state estimator functions loss complexity reachable lower bound power obtained class estimators 
distributions induced simple loss functions consider sequence observed data vectors real valued components real numbers 
interested modeling data generating machinery parametric function capture statistical relationship data sequences parameters ranging subset omega euclidean space 
measure inevitable deviations observed values predicted fitted values loss function ffi needed gives accumulated loss data jx ffi loss function defines probability model conditioned follows yjx gamma gammaffi positive real valued parameter normalizing constant gamma gammaffi dy assumed exist 
extending model sequences independence obtain class fp jx omega ae probability models jx gamma gammal jx particular interest loss functions called simple depend mean 
estimate minimizes loss maximum likelihood estimate minimizes loss function class ideal code length ln jx 
class simple loss functions includes loss binary data prediction error unity 
importantly show family includes loss functions form ffi jy gamma yj ff ff called ff functions 
notice important quadratic loss function special case giving rise normal distribution absolute value loss function ff case gives laplace distribution 
integral omega gammal jx dy finite define nml normalized maximum likelihood model jx gammal jx jx mention passing ideal code length powerful loss function sense nml model captures regular features data expressed terms model class 
derive important properties models class simple loss function shared exponential family densities 
differentiating integral respect get type positive ffi gamma dz denotes expectation respect yjx 
minimize ideal code length gamma ln jx jx ln suppose values derivative respect vanishes jx jx jx expectation taken fixed value 
differentiate integral respect result jx gamma db expectation respect jx 
denote value minimizes jx ln assume jx gamma values omega gamma jx jx gamma conclude section show distributions yjx maximum entropy distributions 
consider problem max ln maximization ffi ffi 
restriction density functions ln jx ffi ln denotes entropy yjx 
shannon inequality entropy satisfies ln jx right hand side 
equality reached result generalizes familiar fact normal distribution variance oe maximum entropy distributions variance exceed oe loss complexity showed nml density function solves minmax problem min max ln jx range wide class distributions minmax value ln reached jx 
consider analogous minmax problem min max jx gamma jx jx ffi obtained predictor gamma general estimator function jx 
specifically consider estimator functions form parameter components ranging subset psi denoting estimate parameter write denote set estimator functions jx gamma jx gkp gamma gkp ln deltak delta denotes kullback leibler distance predictor estimator gamman psi gammal jx dy assumed finite 
jx gammal jx role data generating distributions model statistical restrictions data may captured models class restrict distributions set obtain stronger inequality bounds just distributions light restrict set fg jx gamma right hand side inequality equals jx 
theorem theorem positive predictors gamma estimators max jx jx gamma ln predictors estimators equality reached proof max jx jx pkp jx gamma jx ln gives jx jx ln claims follow 
claim follows 
lower bound worst case mean accumulated loss provided theorem depends parameter see large value bulk probability mass lies strings loss small vice versa 
serves measure achievable loss 
indication loss data hand jx picking view get lower bound relevant observed data 
state result corollary corollary jx positive 
predictors gamma estimators max jx jx gamma ln predictors estimators set defined fg jx jx view theorem corollary define jx jx ln loss complexity data relative model class similarly relative model class fp jx omega loss complexity defined jx jx ln remarks term ln loss complexity interpreted logarithm number distinguishable models models distinguished data manner probability error goes zero see discussion 
may viewed code length parameter effect optimally quantized needed implement optimal model 
parameter provides weight code length model converted loss optimal weight 
extended stochastic complexity shown admit asymptotic expansion sum minimized accumulated loss term upper bounded explicitly calculated code length parameters written optimal precision weighted 
extended stochastic complexity jx close long data strings 
explicitly optimized value determined 
case logarithmic loss interpretation stochastic complexity sum negative logarithm maximized likelihood ideal code length parameters natural code lengths expressed units 
code length parameters satisfy kraft inequality part code length provides natural requirement estimator functions admissible providing fair comparison losses density function define integrate unity 
case loss function obvious normalization requirement estimator function may wonder permit perfect estimator defined gives zero loss 
intuitive grounds reasonable demand fair loss comparison estimator function described decodable manner adding code length parameters non codelength loss appears arbitrary 
inequality rewritten form max jx gamma ln jx gamma ln provides required normalization call estimator realizable accumulated mean loss left hand side achievable satisfies inequality 
show section loss functions intuitive interpretation realizable estimator achievable loss sum minimized loss minimized code length estimator fails restrictions imposed estimator function restrictions difficult say estimator functions allowed allowed 
ff functions showing ff functions simple 
fact change variables ff formula gammat ff gamma dt gamma ff gamma gamma function gives normalizing constant ff gamma ff du gamma gammaj ff dy ff ff gamma ff models write mean function vanishing 
order simplify matters take function inner product generalization functions possible results require assumptions behave inner product 
extend defined density function ff gamma ff gamma xj ff sequences independence result jx gamman ff gamma jy gamma ff denote family models ff fp jx omega omega closed bounded subset ff functions mean loss evaluated jy gamma ff ff expectation respect ff 
denoting normalizing constant ff restriction ff accurate asymptotic formula permits calculation loss complexity 
result theorem proved appendix theorem model class ff omega closed bounded subset interior omega gamma sigma 
positive ff interval ff ln ff ln ln sigmaj omega ff ln nff ff gamma ff gamma gamma ff sigmaj omega gamma ff ff omega denotes volume omega order calculate right hand side need derivative ln ff obtained theorem remainder term depends term shown equation result convergence ff central limit theorem 
ff normalizing coefficient theorem remainder term jx dy hypercube side length centered 
centers taken hypercubes partition omega boundary may cut portions hypercubes 
need calculate explicitly see differentiable function theorem ln ff ff get jx gamma ff solution jx jx gamma ff jx 
corollary theorem give main theorem theorem jx positive 
ff interval ff predictors max jx jx ff gamma ln ff ln denotes fixed parameter defined fg jx jx estimators max jx gamma ln ff jx ff gamma ln ff ln ff gamman psi gamma jx dy finite 
equality reached 
proof predictors ff gamma jx dy ff omega gamma jx dy ff get max jx jx ln ff substitute max jx jx ff gamma ln ff gamma ff gamma ln ff theta term order ln theorem ln ff ln claim predictors follows 
claim estimator functions follows way corollary 
providing reachable lower bound estimation loss complexity theorem provides criterion selection model classes min fl jx ff gamma ln ff obtained 
show worst case bound predictors theorem isolated case bound effect holds mean taken respect data generating distributions jx 
easy generalization inequality mean quadratic prediction error gaussian processes theorem gamma predictor 
ff interval positive ffl inequality jx ff gamma ffl ff ln holds large omega set volume goes zero grows infinity expectation respect proof consider ln jx gamma jx condition required theorem hold satisfied right hand side exceeds gammaffl ln quantifications 
get 
rewrite form max jx ff kff ln see comparing worst case bound isolated case 
question remains tight lower bound theorem prediction 
tantamount question mean stochastic complexity reached predictively asymptotic sense 
lower bound shown reached asymptotically ff case data generating model class gamma gamma general problem appears difficult settle example 
reachability lower bound sure sense gaussian ar processes shown survey predictive coding number loss functions ff types 
example ff take predictor arithmetic mean past data write gamma gamma identity gamma gamma gamma gamma gamma gamma oe fe gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma toe gamma gamma gamma oe gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma oe gamma gamma oe oe gamma oe ln gamma oe putting oe gamma see lower bound theorem reached asymptotically 
lower bound loss functions possible generalize definition loss complexity simple loss functions evaluation problem general 
denote parameter values minimize accumulated losses jx ffi jx ffi respectively estimator 
time universal density functions equivalent 
consider type assume normalizing integrals omega jx dy psi jx dy finite 
define jx gammal jx jx gammal jx defined analogously replaced 
fg jx jx max jx jx jx gamma jx pk ln ln ln ln 
estimators ln clearly implied inequality max jx gamma ln jx gamma ln estimators define jx gamma ln loss complexity relative model class loss functions 
see intuitive break loss complexity minimized loss weighted ideal code length parameter estimate hold general condition stated 
small loss implies large normalizing coefficient condition may interpreted statement estimator function powerful family 
appendix evaluation ff conditions quite accurate asymptotic formula ff holds ff ln ln omega ji jd ji fisher information 
validity formula requires conditions density functions family considered involving times differentiability respect parameters 
ff loss functions differentiable origin values ff interest ff 
overcome obstacle considering modified loss functions follows ffl ff ff ffl ffl ffl parameter small values limit zero 
coefficients fifth degree polynomial determined ffl pieces function ffl ff equal values including derivatives 
gives scaled coefficients ffl gammaff ffl gammaff ffl gammaff ffl gammaff equations ff ff ff gamma ff ff gamma ff gamma solution gamma ff gamma ff gamma ff gamma gamma ff gamma ff gamma ff gamma ff gamma ff gamma ff gamma gamma ff gamma ff gamma ff polynomial part ffl ff ff ffl ff ffl cffl ff ffl dffl ff ffl direct evaluation ffl ff number different values ff interval ff verified nonnegative zero fact required analysis 
consider class density functions ff ffl fp ffl jx omega ffl jx gamman ff ffl gamma ffl ff gamma omega gamma closed bounded subset omega interior 
notice restrict predictor function linear unreasonable 
conditions expansion holds model class ff ffl follows conditions 
elements ffl ff gamma defining matrix ffl continuous omega ffl ei ffl fe ffl ff gamma ffl expectation respect ffl jx 
limit satisfies ji ffl omega omega ji ffl jd 
maximum likelihood estimator satisfies central limit theorem distribution gamma converges normal distribution mean zero covariance gamma ffl omega 
compactness omega requirement convergence uniform needed proof 

ffl ffl ff gamma positive definite matrix maximum likelihood estimate omega addition family elements ij ffl ffl ff gamma function normalized variable 
verify members model family ff ffl satisfy conditions ff prove theorem original family ff proof repeatedly formulas fl ff gammat ff gamma dt ff ff gamma ff ff ff ff gamma gammat ff gamma dt gamma ff gamma fl ff evaluate normalizing constant ff ffl ffl gammaf dy ffl gammay ff dy polynomial ffl ff dropped 
expanding gammaf gamma gamma get ffl gammaf dy ffl ffl ff second integral put ff apply get ffl gammay ff dy ff gamma ff ff fl ff ffl ff ff gamma ffl ffl ff ff ffl ff ffl ff ff ff gamma ff ffl ff order verify conditions need evaluate derivatives ffl ff gamma 
gamma ffl follows ffl ff gamma gammaff gamma ff gamma ffl ff gamma ff ff gamma gamma ff gamma xx ffl ff gamma gammaff ff gamma ff gamma gamma ff gamma gamma ffl ffl ff gamma ffl ff gamma xx ffl ff gamma gammad de ff gamma ff gamma cffl ff gamma dffl ff gamma ff gamma ff gamma cffl ff gamma dffl ff gamma de ff gamma cffl ff gamma dffl ff gamma elements matrix ffl condition clearly continuous omega denote expected value needed ffl ffl gamma ff ffl ffl gammaf dy ff ff gamma ffl gammay ff ff gamma dy introduce multiply result second derivative integrate term term get integral ffl gamma gamma dy ffl ff gamma ffl ff gamma ff gamma ffl ff gamma ff put ff second integral apply fi gamma ff gives ffl gammay ff ff gamma dy ff fi gamma fi gamma fl fi ffl ff ff gamma ff gamma gamma ff gamma ff ff gamma ffl ff gamma ffl ff combining integrals substituting expression ff ffl get ffl ff ff ff gamma ff ff gamma gamma gamma ff gamma ff gamma ff ffl ff gamma ffl ff ff 
convergence condition satisfied assumption ffl ffl ffl sigma ffl 
see ffl ff ff gamma ff gamma gamma ff gamma ff ffl limit may taken fisher information matrix yjx double derivative exist 
ffl depend omega bounded rest condition satisfied 
verify condition 
maximum likelihood estimates scalar valued parameter satisfy central limit theorem provided cramer conditions differentiability likelihood function satisfied 
proof extends vector valued parameters provided conditions hold componentwise 
conditions require case ff ffl times differentiable interior omega gamma secondly need show absolute values derivatives likelihood function integrable omega ffl yjx omega ffl yjx inequality equivalent omega ffl yjx ff ffl gamma follows equations 
verify second inequality note ffl yjx ffl yjx fj ln ffl yjx ln ffl yjx jj ln ffl yjx jg integral term finite finiteness integral second terms follows 
cramer condition ffl yjx ff ffl gamma needs verified 
ffl gammay ff ff gamma dy ffl ff gamma ff ffl get ffl gammaf ff gamma cffl ff gamma ffl ff gamma dy ffl ff gamma ffl ff gamma inequality holds 
fisher information matrix ff ffl clearly bounded positive definite cramer theorem central limit theorem holds family fp ffl yjx verify condition get gamma ffl ffl ff gamma ffl ff gamma gamma ffl ffl ff gamma ff ff gamma gamma ff gamma ffl ff gamma inequality holds ff 
condition holds ff 
complete proof theorem notice ffl yjx yjx ffl 
theorem equation holds family ff ffl replaced ffl letting ffl get formula theorem 
balasubramanian 
statistical inference occam razor statistical mechanics space probability distributions neural computation arxiv org list barron rissanen yu 
mdl principle modeling coding special issue ieee trans 
information theory years information theory vol 
october pp 
probability theory mathematical statistics john wiley sons new york pages table integrals series products academic press new york pages grunwald 
minimum description length principle reasoning uncertainty phd thesis institute logic language computation universiteit van amsterdam pages hannan 
recursive estimation royal statistical society series vol 
issue pp merhav feder 
universal prediction ieee trans 
information theory special issue ieee trans 
information theory years information theory vol 
october pp rissanen 
universal coding information prediction estimation ieee trans 
information theory vol 
nr 
rissanen 
fisher information stochastic complexity ieee trans 
information theory vol 
pp rissanen 
strong optimality normalized ml models universal codes information data ieee trans 
information theory appear vovk 
aggregating strategies proceedings rd annual workshop computational learning theory morgan kauffman pp 
yamanishi kenji decision theoretic extension stochastic complexity application learning ieee trans 
information theory vol 
july 
yamanishi kenji minimax relative loss analysis sequential prediction algorithms parametric hypotheses proceedings eleventh annual conference computational learning theory acm press pp 
yamanishi kenji extended stochastic complexity minimax relative loss analysis lecture notes artificial intelligence algorithmic learning theory th international conference alt springer verlag 
pp 

