query committee seung racah institute physics center neural computation hebrew university jerusalem israel seung mars huji ac il opper institut fur theoretische physik universitat germany manfred opper physik uni dbp de sompolinsky racah institute physics center neural computation hebrew university jerusalem israel haim galaxy huji ac il propose algorithm called query committee committee students trained data set 
query chosen principle maximal disagreement 
algorithm studied toy models high low game perceptron learning perceptron 
number queries goes infinity committee algorithm yields asymptotically finite information gain 
leads generalization error decreases exponentially number examples 
marked contrast learning randomly chosen inputs information gain approaches zero generalization error decreases relatively slow inverse power law 
suggest asymptotically finite information gain may important characteristic query algorithms 
query algorithms proposed variety learning problems bau little gone understanding general principles algorithms constructed 
argue shannon information query suitable guide fed 
show degree disagreement committee learners serve estimate information value 
address bell laboratories mountain ave murray hill nj seung physics att com address institut fur theoretische physik julius maximilians universitat am germany opper vax rz uni 
dbp de gorithms considered focus minimizing number queries required relevant situations queries carry heaviest computational cost 
consider paradigm incremental query learning training set built example time 
restrict scope parametric learning models continuously varying weights learning perfectly realizable boolean valued target rules 
prior distribution weight space assumed flat 
incremental learning procedure consists components training algorithm query algorithm 
set examples training algorithm produces set weights satisfying training set 
query algorithm select example 
training algorithm run newly incremented training set 
training algorithm consider zero temperature gibbs algorithm selects weight vector random version space set weight vectors consistent training set 
enable techniques statistical mechanics sst 
training students training set query committee algorithm selects input classified positive half committee negative half 
maximizing disagreement committee information gain query high 
limit query bisects version space information gain saturates bound bit query 
query committee algorithm illustrated simple model high low game 
move complicated model perceptron learning perceptron gd 
models information gain approaches finite value number queries goes infinity 
asymptotically finite information gain leads generalization error decreases exponentially number queries 
marked contrast case learning random inputs information gain approaches zero number examples increases 
random input case generalization error decreases relatively slowly inverse power law number examples 
information content query denote target function teacher oe parametric learning model student oe 
teacher student boolean valued functions maps set sigma 
assume target function perfectly realizable student means exists weight vector oe oe input vector called positive negative example depending sign teacher output oe oe 
training set input output pairs oe determines version space wp fw oe oe pg set consistent training set 
prior distribution assumed flat posterior distribution uniform version space vanishes outside tls 
written wj ae gamma wp vp volume wp consider gibbs training algorithm weight vector drawn random posterior distribution 
query algorithm specifies way choosing input query teacher 
general written conditional probability leave unspecified 
input chosen distribution receives label oe teacher 
results query added training set 
entropy posterior distribution weight space log vp entropy quantifies uncertainty information gained query defined reduction entropy gamma deltas gamma log defined volume ratio vp vp information gain depends query sequence dependence explicit notation 
dependence eliminated considering averaged quantities 
example average information gain quantities interest query sequences prior distribution purposes partial average suffice 
holding query sequence constant average input respect teacher weight vector respect posterior distribution 
words average teacher vectors consistent examples inputs xp query algorithm 
average information gain hi xp similarly calculate complete probability distribution volume ratio ffi gamma vp vp ae xp note quantities contain dependence query sequence performing average leads bayesian interpretation formula 
input divides version space wp parts fw wp oe gamma fw wp oe gamma averaging posterior distribution find average information gain hi gamma vp log vp gamma gamma vp log gamma vp ae sigma volumes sigma depend implicitly 
teacher answers query oe known certainty 
answer arrives value oe uncertain bayesian probability vp gamma probability gamma vp entropy distribution precisely information value query expression inside average 
average information gain maximized gamma queries divide version space half 
case exact bisection bit exactly 
unfortunately nontrivial learning models geometry version space complex practically calculate volumes sigma input find input gamma training algorithms typically yield single points version space global information version space 
committee students obtain global information 
train committee weight vectors gibbs algorithm 
find input vector classified positive example members committee classified negative query teacher input vector 
train committee new enlarged training set repeat 
algorithm approaches bisection algorithm 
algorithm spirit oh consensus improve generalization performance 
lack consensus choose query principle maximal disagreement 
define generalization function ffl theta gammaoe oe average taken prior distribution inputs 
measures probability error student input output pairs drawn teacher query sequence generalization error defined ffl hffl averages taken posterior distribution 
average generalization error ffl defined average query sequence 
high low game high low simplest model query learning formalized parametric learning framework 
teacher student oe sgn gamma oe sgn gamma input weight spaces unit interval prior distributions spaces flat query teacher respond gamma depending higher lower suppose exists training set examples 
xl largest negative example xr smallest positive example 
version space wp xl xr volume vp xr gamma xl posterior weight distribution pp ae gamma xl xr entropy log vp gibbs training algorithm simply picks random version space interval xl xr 
consider case randomly chosen inputs 
probability gamma vp input fall outside version space volume ratio 
falls inside version space probability distribution volume ratio result obtained averaging xl xr 
combining alternatives find probability distribution volume ratio jv gamma vp ffi gamma vp expected information gain average gamma log respect distribution vp nat number examples increases volume vp shrinks information gain tends zero 
generalization error linear volume version space ffl vp averaging possible training sets show average generalization error satisfies ffl inverse power law number examples 
maximal information gain attained bisection algorithm xp chosen halfway xl xr case probability distribution volume ratio ffi gamma information gain bit query 
volume decreases exponentially vp gammap knowledge binary representation increases bit query 
committee members probability distribution volume ratio dy dz gamma gamma gamma gamma omitted normalization constant 
written distribution independent query history random input case 
shows various values note approaches infinity curves approach delta function bisection algorithm 
average information gain nats log gamma gamma gamma gamma gamma euler function 
saturates bound bit query 
ffl vp product volume ratios log ffl log const probability distribution volume ratio member committee playing high low game 
volume ratios independent identically distributed random variables drawn 
central limit theorem ffl approaches log normal random variable 
probable ffl scales ffl gammap 
query learning high low simple exponential relationship information gain generalization error 
section see similar relationship written perceptrons 
perceptron learning consider perceptron learning perceptron teacher student oe sgn delta oe sgn delta vectors components 
weight space taken hypersphere fw delta ng input distribution gaussian gamman exp gamma deltax examples posterior distribution uniform version space wp fw delta delta pg calculate average ffl log ffl coefficient resulting exponential hlog random inputs inputs chosen random distribution replica method calculate entropy posterior distribution gt 
calculation exact thermodynamic limit ff constant 
entropy weight ff log gamma ff flx log flx fl gamma dx dx gammax dx entropy respect order parameter average generalization error obtained relation ffl gamma cos gamma large ff limit leads inverse power law behavior ffl ff ff large ff asymptotics obtained examining scaling entropy generalization error similar arguments sst high limit general classification learning curves 
term dominates entropy simple logarithmic dependence generalization error log ffl asymptotic result ffl gamma gamma 
information gain ff gamma ff gamma flx log flx gamma gamma log ffl condition combined satisfied inverse power law ffl ff 
performed monte carlo simulation algorithm 
deterministic perceptron algorithm mp obtain weight vector version space 
zero temperature monte carlo random walk inside version space 
maintain acceptance rates approximately size monte carlo step scaled downward increasing ff 
resulting learning curve shown fig 
fits analytic results obtained replica method 
query committee query depends previous history queries committee algorithm dynamical process number examples plays role time 
replica calculations algorithm know overlaps weight vectors different times 
calculations difficult case random inputs 
replica calculation appendix simplifying assumption typical overlap weight vectors different times equal typical overlap vectors earlier time 
words assume overlaps satisfy delta minft self averaging quantities 
thermodynamic limit turns discrete time dynamics continuous time dynamics ff entropy weight space time ff calculated treating ff previous times ff ff external field 
resulting saddle point equation ff integral equation solved numerically 
replica results member committee shown fig 
monte carlo simulations 
simulations zero temperature gibbs training implemented training perceptrons standard perceptron algorithm zero temperature monte carlo 
input vectors selected random prior distribution input produced disagreement 
teacher queried input nat bit table information gain query committee algorithm perceptron learning input teacher output added training set 
note generalization error member committee close random input results ff order 
relatively large ff different committee sizes shown perform identically 
algorithms quite different large ff asymptotic properties 
derivative committee entropy respect ff find information gain approaches limit ff gamma gammax log gammax ff 
limiting values small table 
information gain saturates bound bit query 
information gain affect generalization performance 
committee algorithm information gain approaches finite constant value ds dff gammai assume entropy ff log ffl 
assumption shown consistent appendix leads asymptotic result ffl gammaffi generalization error asymptotically exponential ff decay constant determined information gain 
markedly faster inverse power law random inputs 
implementation committee algorithms source random inputs screened committee provokes disagreement 
practical drawback scheme time find query diverges inverse generalization error 
respect algorithms construct queries directly may superior 
example query algorithm proposed kr wr perceptron learning constructs input vectors perpendicular student weight vector 
conjunction gibbs training algorithm method yields generalization performance slightly worse committee algorithms moderate ff faster query times 
committee algorithms appear possess generality query algorithms 
ffl ff committee random inputs replica replica generalization curves perceptron learning random input committee minimal training set algorithms 
monte carlo simulations done averaged samples 
query perceptron steps 
standard error measurement smaller size symbol 
solid lines analytic results replica theory 
compared query committee random input training case perceptron 
random inputs information gain approaches zero generalization error ff gamma ds dff ffl committee algorithm information gain asymptotically finite ff 
perceptron learning entropy behaves asymptotically log ffl random input query algorithms 
ds dff terms ffl derive generalization curve function ff 
random inputs imply ffl ff committee algorithms imply log ffl gammai ff high low game perceptron provided simple illustrations query committee algorithm relationship information gain generalization error 
addressed issue behaviors exhibited toy models general 
architectures query committee lead asymptotically finite information gain 
conditions asymptotically finite information gain lead exponential generalization curves 
questions currently investigation 
acknowledge helpful discussions freund hansel tishby 
bau baum 
neural net algorithms learn polynomial time examples queries 
ieee trans 
neural networks 
fed fedorov 
theory optimal experiments 
academic press new york 
gd gardner 
unfinished works optimal storage capacity networks 
phys 
gt tishby 
statistical theory learning rule 
editors neural networks spin glasses pages singapore 
world scientific 
haussler kearns schapire 
bounds sample complexity bayesian learning information theory vc dimension 
warmuth valiant editors proceedings fourth annual workshop computational learning theory pages san mateo ca 
morgan kaufmann 
kr 
improving network generalization ability selecting examples 
europhys 
lett 
mp minsky papert 
perceptrons 
mit press cambridge expanded edition 
oh opper haussler 
generalization performance bayes optimal classification algorithm learning perceptron 
phys 
rev lett 
sst seung sompolinsky tishby 
statistical mechanics learning examples 
phys 
rev 
tls tishby levin solla 
consistent inference probabilities layered networks predictions generalization 
proc 
int 
joint conf 
neural networks volume pages washington dc 
ieee 
wr rau 
selecting examples perceptrons 
phys 
appendix replica calculations query committee volume vp version space determines entropy ln vp fixed teacher average entropy possible query sequences consistent teacher denoted av quantity shall find overlap random weight vectors inside version space wp take target vector turn give generalization error new random input 
definition av hln vp fx lim ln fx second line introduced replica trick average prior distribution teachers added oh 
symmetry teacher students explicit teacher vector treated replica av lim ln tr foe dw theta theta oe delta fx note integer replicas 
recall committee st input selected cause disagreement committee members gamma drawn random version space probability density jw gamma theta oe sigma theta delta theta gamma delta added achieve correct normalization dx jw gamma 
prior distribution patterns gaussian gamma deltax consider average committee weights gamma fixed time 
theta oe delta dx theta oe delta jw gamma dx theta oe delta theta oe sigma theta delta theta gamma delta theta oe sigma dx theta delta theta gamma delta gamma average respect involves quantities delta symbols gamma 
quantities gaussian random variables zero mean covariance delta delta ae delta ab average depends vectors overlaps ab accord ansatz ansatz replica symmetry assume ffi gamma ffi sigma sigma sigma sigma upsilon replacements sigma delta sigma gamma delta gamma gamma sigma uncorrelated gaussian random variables unit variance 
change variables yields proper covariances 
average done yields entropy depends order parameters gamma overlap time weight vectors treat external fields determine 
valid provided self averaging quantities fluctuations committee members times affect calculation entropy time take continuum limit define ffn ff replace product exp ff ff ln 
integrals done usual introducing entropic terms containing generalizing committee members find entropy weight ln gamma ff dff dx fl gammafl ln fl gammafl fl ff gamma ff ff gamma gamma ff gamma value ff determined 
information gain calculated differentiating respect ff gamma flx flx log flx flx fl defined 
yields asymptotic result 
assuming generalization error asymptotically exponential ff show disorder term integral ff approaches constant 
ln gamma term dominates log ffl derivation text asymptotically exponential ffl 
