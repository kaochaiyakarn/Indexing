additive logistic regression statistical view boosting jerome friedman trevor hastie robert tibshirani august boosting freund schapire schapire singer important developments classification methodology 
performance classification algorithms dramatically improved sequentially applying reweighted versions input data weighted majority vote sequence classifiers produced 
show seemingly mysterious phenomenon understood terms known statistical principles additive modeling maximum likelihood 
class problem boosting viewed approximation additive modeling logistic scale maximum bernoulli likelihood criterion 
develop direct approximations show exhibit nearly identical results boosting 
direct multi class generalizations multinomial likelihood derived exhibit performance comparable proposed multi class generalizations boosting situations far superior 
suggest minor modification boosting reduce computation factors 
apply insights produce alternative formulation boosting decision trees 
approach best truncated tree induction leads better performance provide interpretable descriptions aggregate decision rule 
faster computationally making suitable large scale data mining applications 
department statistics sequoia hall stanford university stanford california stat stanford edu department public health sciences department statistics university toronto toronto edu starting point interesting procedure called boosting way combining boosting performance weak classifiers produce powerful committee 
boosting proposed machine learning literature freund schapire received attention 
boosting evolved somewhat years describe commonly version procedure freund schapire call discrete adaboost concise description adaboost class classification setting 
training data vector valued feature gamma 
define classifier producing values sigma constants corresponding prediction sign 
adaboost procedure trains classifiers weighted versions training sample giving higher weight cases currently misclassified 
done sequence weighted samples final classifier defined linear combination classifiers stage 
detailed description discrete adaboost boxed display titled algorithm discrete adaboost freund schapire 
start weights 
repeat fit classifier weights training data 
compute ew fm log gamma 
set exp delta fm renormalize 
output classifier sign algorithm ew represents expectation training data weights wn 
iteration adaboost increases weights observations misclassified fm factor depends weighted training error 
written success adaboost producing essentially adaboost binary data freund schapire accurate classifiers 
authors explored tree classifier demonstrated consistently produces significantly lower error rates single decision tree 
fact breiman nips workshop called adaboost trees best shelf classifier world see breiman 
interestingly test error consistently decrease level classifiers added ultimately increase 
reason adaboost immune overfitting 
shows performance discrete adaboost synthetic classification task adaptation cart tm breiman friedman olshen stone base classifier 
adaptation grows fixed size trees best manner see section page 
included bagged tree breiman averages trees grown bootstrap resampled versions training data :10.1.1.30.8572:10.1.1.32.9399
bagging purely variance reduction technique trees tend high variance bagging produces results 
early versions adaboost resampling scheme implement step algorithm weighted importance sampling training data 
suggested connection bagging major component success boosting variance reduction 
boosting performs comparably ffl weighted tree growing algorithm step weighted resampling training observation assigned weight removes randomization component essential bagging 
ffl stumps weak learners 
stumps single split trees terminal nodes 
typically low variance high bias 
bagging performs poorly stumps fig 
top right panel 
observations suggest boosting capable bias variance reduction differs fundamentally bagging 
base classifier discrete adaboost produces classification rule 
gamma domain predictive features implementation base classifier deal observation weights weighted resampling 
freund schapire breiman schapire singer suggested various modifications improve boosting algorithms 
focus version due schapire singer call real adaboost uses number terms test error bagging discrete adaboost real adaboost node trees number terms stumps number terms test error node trees test error bagging discrete adaboost real adaboost simulated class nested spheres problem see section page 
training data points dimensions bayes error rate zero 
trees grown best pruning 
left iteration corresponds single tree 
real valued confidence rated predictions gamma discrete adaboost 
weak learner generalized boosting produces mapping 
sign gives classification jf measure confidence prediction 
real valued boosting tends perform best simulated examples fig 
especially stumps see node trees discrete adaboost real adaboost iterations 
real adaboost schapire singer 
start weights 
repeat estimate confidence rated classifier 
constant training data weights 
set exp gammac delta renormalize 
output classifier sign algorithm real adaboost algorithm allows estimator fm range special case fm gamma reduces adaboost fm correct gamma incorrect classification 
general case constant absorbed fm 
describe schapire singer estimate fm section 
freund schapire schapire singer provide theory support algorithms form upper bounds generalization error 
theory schapire evolved machine learning community initially concepts pac learning kearns vazirani game theory freund breiman 
early versions boosting weak learners schapire far simpler described theory precise 
bounds theory associated adaboost algorithms interesting tend loose practical importance 
practice boosting achieves results far impressive bounds imply 
analyze adaboost procedures statistical perspective 
show fitting additive logistic regression model 
adaboost algorithms newton methods optimizing particular exponential loss function criterion behaves log likelihood logistic scale 
derive new boosting procedures classification 
section briefly review additive modeling 
section shows boosting viewed additive model estimator proposes new boosting methods class case 
multiclass problem studied section 
simulated real data experiments discussed sections 
tree growing implementation truncated best trees described section 
weight trimming speed computation discussed section discussion section 
additive models adaboost produces additive model weighted committee ensemble sound 
additive models long history statistics give examples 
additive regression models initially focus regression problem response quantitative interested modeling mean jx 
additive model form separate function input variables generally component function small pre specified subset input variables 
backfitting algorithm friedman stuetzle buja hastie tibshirani convenient modular algorithm fitting additive models 
backfitting update gamma jx method algorithm estimating function obtain estimate conditional expectation 
particular include nonparametric smoothing algorithms local regression smoothing splines 
right hand side latest versions functions forming partial residuals 
backfitting cycles repeated convergence 
fairly general conditions backfitting shown converge minimizer gammaf buja 
extended additive models generally consider additive models elements ff functions potentially input features usually context taken simple functions characterized set parameters fl multiplier fi fi fl additive model fm fi fl example single hidden layer neural networks fl oe fl oe delta sigmoid function fl parameterizes linear combination input features 
signal processing wavelets popular choice fl parameterizing location scale mother wavelet fl 
applications fb fl generally called basis functions span function subspace 
squares fitting criterion solve optimal set parameters generalized back fitting algorithm updates ffi fl arg min fi fl gamma fi fl gamma fib fl 
alternatively greedy forward stepwise approach ffi fl arg min fi fl gamma fm gamma gamma fib fl ffi fl gamma fixed corresponding solution values earlier iterations 
approach mallat zhang matching pursuit fl selected complete dictionary wavelet bases 
language boosting fib fl called weak learner fm committee 
decision trees weak learner parameters fl represent splitting variables split points constants terminal node number terminal nodes tree 
note back fitting procedure greedy cousin require algorithm fitting single weak learner data 
base algorithm simply applied repeatedly modified versions original data gamma forward stepwise procedure modified output mth iteration depends value gamma solution gamma previous iteration gamma gamma gamma step previous output values gamma modified previous model gamma explanatory power new outputs view procedure boosting weak learner fib fl form powerful committee fm 
classification problems classification problem learn bayes theorem need jjx posterior conditional class probabilities 
transfer regression machinery classification domain simply noting jx jjx indicator variable representing class works fairly general problems noted hastie tibshirani buja constrained regression methods 
estimates typically confined severe masking problems occur classes notable exception trees regression method fact approach breiman 

logistic regression popular approach statistics overcoming problems 
class problem model log jx jx monotone logit transformation left guarantees values probability estimates lie inverting get jx general additive form special cases exist known statistics 
particular linear logistic regression mccullagh nelder example additive logistic regression hastie tibshirani popular 
models usually fit maximizing binomial log likelihood enjoy associated asymptotic optimality features maximum likelihood estimation 
generalized version backfitting called local scoring hastie tibshirani fit additive logistic model 
starting guesses defined form working response gamma gamma apply backfitting response observation weights gamma obtain new 
process repeated convergence 
forward stage wise version procedure bears close similarity logitboost algorithm described 
boosting additive logistic regression model section show boosting algorithms stage wise estimation procedures fitting additive logistic regression model 
optimize exponential criterion second order equivalent binomial log likelihood criterion 
propose standard likelihood boosting procedure 
exponential criterion consider minimizing criterion gammayf estimation 
lemma shows function minimizes symmetric logistic transform jx lemma gammayf minimized log jx gamma jx represents expectation depending context may population expectation sample average 
ew indicates weighted expectation 
jx gammaf gamma jx gammaf gammaf proof entails expectation joint distribution sufficient minimize criterion conditional gammayf jx jx gammaf gamma jx gammayf jx gammap jx gammaf gamma jx setting derivative zero result follows 
usual logistic transform factor multiplying numerator denominator get usual logistic model models equivalent factor 
corollary replaced averages regions constant terminal node decision tree result applies sample proportions gamma 
proposition show discrete adaboost increments algorithm newton style updates minimizing exponential criterion 
terms typically required stage crude approximation place conditional expectation 
lemma resulting algorithm interpreted stage wise estimation procedure fitting additive logistic regression model 
proposition discrete adaboost algorithm fits additive logistic regression model adaptive newton updates minimizing gammayf 
proof gammayf 
suppose current estimate seek improved estimate cf 
fixed expand cf second order cf gammay cf gammayf gamma minimizing pointwise respect gamma find arg min ew gamma jx arg min ew gamma cf jx arg min ew gamma jx yjx exp gammayf exp gammayf follows considering possible choices 
minimizing quadratic approximation criterion leads weighted squares choice gamma constitutes newton step 
gamma directly minimize determine arg min ew log gamma ew 
note negative weak learner worse case automatically reverses polarity 
combining steps get update log gamma iteration new contribution augments weights yjx yjx delta gammac followed normalization 
theta gamma see update equivalent yjx yjx delta exp log gamma function weight updates identical discrete adaboost 
parts derivation adaboost breiman schapire singer additive logistic regression models 
version adaboost translates naturally data version trees 
weighted squares criterion grow tree classifier constant weighted training error 
note newton step weights change tree configuration change 
adds adaptive twist newton algorithm 
corollary update weights weighted misclassification error weak learner 
proof follows noting minimizes cf satisfies cf gammae gammay cf yf result follows yf correct gamma incorrect classification 
schapire singer give interpretation weights updated new weighted problem maximally difficult weak learner 
discrete adaboost algorithm expects tree weak learner deliver classifier gamma 
show real adaboost algorithm uses weighted probability estimates terminal nodes tree update additive logistic model classifications 
derive population algorithm apply data 
proposition real adaboost algorithm fits additive logistic regression model stage wise optimization gammayf proof suppose current estimate seek improved estimate minimizing gammayf gammayf jx gammaf gammayf jx gammayf gamma jx dividing gammayf jx setting derivative wrt zero get log ew jx ew gamma jx log pw jx pw gamma jx yjx exp gammayf exp gammayf jx 
careful examination schapire singer shows update matches theirs algorithm redundant 
weights get updated yjx yjx delta gammay algorithm iteration 
practice crude approximations conditional expectation steps required 
corollary optimal weighted conditional mean 
proof optimal gammayf think weights providing alternative residuals binary classification problem 
optimal function information weighted conditional distribution update iteration discrete real adaboost algorithms composed additive function form components greedy forward stage wise fashion fixing earlier components 
term stage wise refers similar approach statistics ffl variables included sequentially stepwise regression 
ffl coefficients variables included receive adjustment 
ee gammayf far justification exponential criterion sensible population minimizer algorithm described performs real data 
addition ffl schapire singer motivate gammayf differentiable upperbound misclassification error yf see fig 
ffl adaboost algorithm generates extremely modular requiring iteration retraining classifier weighted training database 
values parametrize binomial probabilities gammaf expected binomial log likelihood log gamma log gamma gammae log gamma yf ffl population minimizers gammae ee gammayf coincide 
easily seen expected log likelihood maximized true probabilities jx define logit 
lemma see exactly minimizer ee gammayf yf misclassification shapire singer log likelihood squared error squared error losses approximations misclassification error variety loss functions estimating function classification 
horizontal axis yf negative errors positive correct classifications 
loss functions monotone yf curve labeled squared error gamma gives uniformly better approximation misclassification loss exponential criterion schapire singer 
curve labeled squared error gamma increases yf exceeds increasingly penalizing classifications correct 
fact exponential criterion negative log likelihood equivalent second order taylor series gamma exp gammayf log graphs exp gammayf log gammayf suitably scaled shown fig 
function yf positive values yf imply correct classification 
note gamma exp gammayf proper log likelihood equal log probability mass function sigma 
ffl way view criterion 
easy show gammayf jy gamma gamma log gamma right hand side known chi statistic statistical literature 
feature exponential log likelihood criteria monotone smooth 
training error zero criteria drive estimates purer solutions terms probability estimates 
estimate minimizing squared error gamma fm gamma gamma current prediction leads forward stage wise procedure unweighted fit response gammaf gamma step 
empirically approach works quite dominated monotone loss criteria 
believe non monotonicity squared error loss fig 
reason 
correct classifications yf incur increasing loss increasing values jf squared error loss especially poor approximation misclassification error rate 
classifications correct penalized misclassification errors 
log likelihood criterion section explore algorithms fitting additive logistic regression models stage wise optimization bernoulli log likelihood 
focus class case response represent outcome 
represent probability gammaf algorithm gives details 
logitboost classes 
start weights probability estimates 
repeat compute working response weights gamma gamma gamma estimate weighted squares fitting 
update 

output classifier sign sign algorithm adaptive newton algorithm fitting additive logistic regression model 
proposition logitboost algorithm classes uses adaptive newton steps fitting additive symmetric logistic model maximum likelihood 
proof consider update expected log likelihood gamma log conditioning compute second derivative gamma jx gamma gamma jx defined terms 
newton update gamma gamma gamma jx ep gamma jx ew gamma gamma jx gamma 
equivalently newton update solves weighted squares criterion min gamma gamma gamma population algorithm described translates immediately implementation data replaced regression method regression trees breiman 
role weights somewhat artificial case implementation constant conditioned terminal node tree example depend current values typically constant 
get small regions perceived pure close 
cause numerical problems construction led crucial implementation protections ffl compute gammap gammap number get large small threshold ratio zmax 
particular value chosen zmax crucial empirically zmax works 
likewise compute gamma gammap lower threshold 
ffl enforce lower threshold weights max zero 
optimizing ee gammayf newton stepping real adaboost procedure algorithm optimizes ee gammay exactly respect iteration 
explore version takes adaptive newton steps logitboost algorithm just described 
proposition gentle adaboost algorithm uses adaptive newton steps minimizing ee gammayf gentle adaboost 
start weights 
repeat estimate weighted squares fitting 
update update gammay fm renormalize 

output classifier sign sign algorithm modified version real adaboost algorithm newton stepping exact optimization step proof gammae gammayf yjx gammayf jx newton update gammayf yjx gammayf jx ew yjx yjx gammayf gammayf jx main difference real adaboost algorithm uses estimates weighted class probabilities update functions 
update pw jx gamma pw gamma jx half log ratio log pw jx pw gamma jx log ratios numerically unstable leading large updates pure regions update lies range gamma 
empirical evidence suggests see section conservative algorithm similar performance real adaboost logitboost algorithms outperforms especially stability issue 
strong similarity updates gentle adaboost algorithm logitboost algorithm 
jx gammaf gammayf yjx gammayf jx gammaf gamma gamma gammaf gamma gamma gamma gamma analogous expression logitboost gamma gamma nearly differ extreme 
example blows falls gamma 
multiclass procedures explore extensions boosting classification multiple classes 
start proposing natural generalization class symmetric logistic transformation consider specific algorithms 
context schapire singer define responses class problem values gamma 
similarly indicator response vector elements standard statistics literature 
assume classes mutually exclusive 
definition class problem jx 
define symmetric multiple logistic transformation log gamma log equivalently centering condition numerical stability simply pins add arbitrary constant probabilities remain 
equivalence definitions easily established equivalence class case 
schapire singer provide generalizations adaboost multiclass case describe adaboost mh algorithm see boxed algorithm dominate empirical studies 
connect models 
refer adaboost mh schapire singer original observations expanded theta pairs ij ij response class jth observation 
start weights ij nj 
repeat estimate confidence rated classifier theta 
training data weights ij 
set ij ij exp gammay ij renormalize ij 
output classifier argmax 
algorithm adaboost mh algorithm converts class problem estimating class classifier training set times large additional feature defined set class labels 
augmented variable algorithm class variable observations ffl version algorithm minimizes ee gammay equivalent running separate boosting algorithms problems size obtained partitioning theta samples obvious fashion 
seen trivially conditioning xjc computing conditional expectations 
ffl true tree algorithm 
see 
split nary split permitted gamma binary splits sub trees identical separate trees grown groups 
case tree 

tree split path terminal node node returns function contributes classification decision 
long tree includes split path terminal node contribution classifier input feature values 
advantage disadvantage building large tree class label additional input feature clear 
motivation provided 
implement adaboost mh traditional direct approach building separate trees minimize ee gammay shown proposition adaboost mh algorithm class problem fits uncoupled additive logistic models log gamma class rest 
principal parametrization fine monotone 
estimating uncoupled fashion guarantee implied probabilities sum 
give examples difference adaboost mh performs poorly alternative coupled likelihood procedure 
schapire singer adaboost mh intended cover situations observations belong class 
mh represents multi label hamming hamming loss measure errors space possible class labels 
context fitting separate classifier label reasonable strategy 
schapire singer propose adaboost mh class labels mutually exclusive focus 
algorithm natural generalization algorithm fitting class logistic regression model 
proposition logitboost algorithm classes uses adaptive steps fitting additive symmetric logistic model maximumlikelihood sketch informal proof 
proof ffl give score hessian newton algorithm corresponding standard multi logit parametrization log jx jx logitboost classes 
start weights ij 

repeat repeat compute working responses weights jth class ij ij gamma gamma ij gamma ii 
estimate mj weighted squares fit ij set mj gamma mj gamma mk mj update 

output classifier argmax algorithm adaptive newton algorithm fitting additive multiple logistic regression model 
choice base class arbitrary 
expected conditional log likelihood gamma jx gamma log gamma gamma jx gamma gammap ffi jk gamma gamma ffl quasi newton update amounts diagonal approximation hessian producing updates gamma jx gamma gamma ffl convert symmetric parametrization note set gamma 
procedure applied class base just jth 
averaging choices base class get update gamma gamma jx gamma gamma gamma jx gamma rigid parametric models full newton stepping symmetrization redundant 
quasi newton steps adaptive tree models symmetrization removes dependence choice base class 
simulation studies section flavors boosting outlined applied artificially constructed problems 
comparisons real data section 
advantage comparisons simulation setting aspects example known including bayes error rate complexity decision boundary 
addition population expected error rates achieved respective methods estimated arbitrary accuracy averaging large number different training test data sets drawn population 
boosting methods compared dab discrete adaboost algorithm 
rab real adaboost algorithm 
lb logitboost algorithms 
gab gentle adaboost algorithm 
dab rab gab handle multiple classes adaboost mh approach 
attempt differentiate performance simulated examples involve fairly complex decision boundaries 
input features examples randomly drawn dimensional standard normal distribution 
examples decision boundaries separating successive classes nested concentric dimensional spheres constructed thresholding squared radius origin class defined subset observations fx gamma 
ft gamma example chosen put approximately equal numbers observations class 
training sample size delta approximately training observations class 
independently drawn test set observations estimate error rates training set 
averaged results independently drawn training test set combinations final error rate estimates 
corresponding statistical uncertainties standard errors final estimates averages approximately line width plot 
top left compares algorithms class case terminal node decision tree stump base classifier 
shown error rate function number boosting iterations 
upper black line represents dab nearly coincident lines methods dotted red rab short dashed green lb long dashed blue gab note somewhat erratic behavior dab colors visible online version stat stanford edu trevor papers boost ps number terms test error discrete adaboost real adaboost logitboost gentle adaboost stumps classes number terms node trees classes number terms test error stumps classes number terms stumps classes additive decision boundary additive decision boundary 
panels top right solid curve representing discrete adaboost lies curves 
especially iterations due statistical uncertainty 
iterations lb minuscule edge dead heat rab gab 
dab shows substantially inferior performance roughly twice error rate iterations 
lower left shows corresponding results classes terminal node trees 
problem difficult represented increased error rates methods relationship roughly upper black line represents dab nearly coincident lines methods 
situation somewhat different larger number classes 
shows results typical dab incurs higher error rates rab gab nearly identical performance 
performance lb relative rab gab changed 
iterations error rate 
iterations lb error rates slightly higher 
iterations error rate lb continues improve rab gab level decreasing slowly 
iterations error rate lb rab gab 
speculation reason lb performance gain situations 
examples stump base classifier 
expect larger trees better complex problems 
top right shows results class problem boosting trees terminal nodes 
results compared stumps fig 
top left 
initially error rates boosting node trees decrease rapidly stumps successive iteration methods 
error rates quickly level improvement slow iterations 
performance dab improved bigger trees coming close methods 
rab gab lb exhibit nearly identical performance 
note iteration node tree model consists times number additive terms corresponding stump model 
error rates decrease rapidly early iterations 
terms model complexity training time iteration model terminal node trees equivalent iteration stump model comparing top panels fig 
sees rab gab lb error rate bigger trees fact higher stumps iterations times complex 
seemingly mysterious behavior easily understood examining nature decision boundary separating classes 
bayes decision boundary classes set ae log jx gamma jx oe simply fx 
approximate set sufficient estimate logit monotone transformation closely possible 
discussed boosting produces additive logistic model component functions represented base classifier 
stumps base classifier component function form tm tm mth stump chose split coordinate split point weighted means response left right terminal nodes 
model produced boosting stumps additive original features adds stumps involving exist 
examination reveals optimal decision boundary examples additive original features const context decision trees stumps ideally matched problems larger trees needed 
boosting larger trees need counter productive case splits individual tree predictor variable 
produce additive model original features 
due forward greedy stage wise strategy boosting happen decision boundary function involves predictor individual tree try best involve important predictors 
owing nature decision trees produce models interaction effects terms model involve products variable 
non additive models suited approximating truly additive decision boundaries 
reflected increased error rate observed fig 

discussion suggests decision boundary separating pairs classes inherently non additive predictors number terms test error dab rab lb gab stumps classes number terms node trees classes number terms test error node trees classes non additive decision boundary interactive decision boundary boosting stumps advantageous larger trees 
tree terminal nodes produce basis functions maximum interaction order min gamma number predictor features 
higher order basis functions provide possibility accurately estimate high order interactions 
purpose example verify intuition 
classes training observations fx drawn dimensional normal distribution previous examples 
class labels randomly assigned observation log odds log pr pr gamma gamma approximately equal numbers observations assigned classes bayes error rate 
decision boundary problem complicated function predictor variables involving second order interactions equal strength 
examples test sets observations estimate error rates training set final estimates averages replications 
top left shows test error rate function iteration number boosting methods stumps 
previous examples rab gab track closely 
dab begins slowly dominated iterations passes rab gab 
lb dominates having lowest error rate iterations 
point dab catches iterations may slight edge 
boosting methods perform stumps problem best error rate 
top right shows corresponding plot terminal node trees boosted 
dramatic improvement methods 
time small differentiation rab gab 
nearly iterations performance ranking lb best followed gab rab dab order 
iterations lb achieves error rate 
lower left shows results terminal node trees boosted 
error rates generally reduced lb improving dominating 
performance ranking methods changes increasing iterations dab rab iterations gab fairly close lb iterations error rate 
limited scope simulation studies suggest trends 
explain boosting stumps superior larger trees suggest situations case decision boundaries closely approximated functions additive original predictor features 
higher order interactions required stumps exhibit poor performance 
examples illustrate close similarity rab gab 
cases difference performance dab decreases larger trees iterations overtaking 
generally relative performance methods depends problem hand terms nature decision boundaries complexity base classifier number boosting iterations 
superior performance lb fig 
lower right appears consequence multi class logistic model algorithm 
methods asymmetric adaboost mh strategy algorithm building separate class models individual class pooled complement classes 
decision boundaries separating class pairs relatively simple pooling classes produce complex decision boundaries difficult approximate friedman 
considering classes simultaneously symmetric multi class model better able take advantage simple pairwise boundaries exist hastie tibshirani 
noted pairwise boundaries induced simple viewed context additive modeling pooled boundaries complex approximated functions additive original predictor variables 
decision boundaries associated examples deliberately chosen geometrically complex attempt illicit performance differences methods tested 
complicated boundaries occur practice 
practical problems involve comparatively simple boundaries holte cases performance differences situation dependent correspondingly pronounced 
experiments data section show results running fitting methods logitboost discrete adaboost real adaboost gentle adaboost collection datasets uc irvine machine learning archive plus popular simulated dataset 
base learner tree case terminal nodes 
comparison single cart decision tree fit tree size determined fold cross validation 
datasets summarized table 
test error rates shown table smaller datasets table larger ones 
vowel sonar satimage letter datasets come pre specified test set 
waveform data simulated described breiman 
fold cross validation estimate test error 
difficult discern trends small data sets table quite large observed differences performance attributed sampling fluctuations 
vowel breast cancer ionosphere sonar waveform data purely additive stump models perform comparably larger node trees 
glass data benefit little larger trees 
clear differentiation performance boosting methods 
larger data sets table clearer trends discernible 
satimage data node tree models slightly significantly accurate purely additive models 
letter data contest 
boosting stumps clearly inadequate 
clear differentiation boosting methods node trees 
stumps logitboost real adaboost gentle adaboost comparable performance distinctly superior discrete adaboost 
consistent results simulation study section 
discrete adaboost real data examples fail demonstrate performance differences various boosting methods 
contrast simulated data sets section 
logitboost generally dominated small margin 
inability real data examples discriminate may reflect statistical difficulties estimating subtle differences small samples 
alternatively may underlying decision boundaries relatively simple holte reasonable methods exhibit similar performance 
additive logistic trees applications boosting base classifier considered primitive repeatedly called boosting procedure iterations proceed 
operations performed base classifier context data weights 
fact final model going linear combination large number classifiers table datasets experiments data set train test inputs classes vowel breast cancer fold cv ionosphere fold cv glass fold cv sonar fold cv waveform satimage letter taken account 
particular decision trees tree growing pruning algorithms generally employed 
alterations pruning programming convenience speed 
boosting viewed light additive modeling greedy approach seen far optimal situations 
discussed section goal final classifier produce accurate approximation decision boundary function 
context boosting goal applies final additive model individual terms base classifiers time constructed 
example seen section close additive original predictive features boosting stumps optimal produced approximation structure 
building larger trees increased error rate final model resulting approximation involved high order interactions features 
larger trees optimized error rates individual base classifiers weights step produced lower unweighted error rates early stages 
sufficient number boosts stump model achieved superior performance 
generally consider expansion decision boundary function functional anova decomposition friedman jk jkl sum represents closest function additive original features represent closest approximation involving feature interactions represent table test error rates small real examples method terminal nodes terminal nodes iterations vowel cart error logitboost real adaboost gentle adaboost discrete adaboost breast cart error logitboost real adaboost gentle adaboost discrete adaboost ion cart error logitboost real adaboost gentle adaboost discrete adaboost glass cart error logitboost real adaboost gentle adaboost discrete adaboost sonar cart error logitboost real adaboost gentle adaboost discrete adaboost waveform cart error logitboost real adaboost gentle adaboost discrete adaboost table test error rates larger data examples 
method terminal iterations fraction nodes satimage cart error logitboost real adaboost gentle adaboost discrete adaboost logitboost real adaboost gentle adaboost discrete adaboost letter cart error logitboost real adaboost gentle adaboost discrete adaboost logitboost real adaboost gentle adaboost discrete adaboost feature interactions 
accurately approximated expansion truncated low interaction order allowing base classifier produce higher order interactions reduce accuracy final boosted model 
context decision trees higher order interactions produced deeper trees 
situations true underlying decision boundary function admits low order anova decomposition take advantage structure improve accuracy restricting depth base decision trees larger actual interaction order 
known advance particular problem maximum depth meta parameter procedure estimated model selection technique cross validation 
restrict depth induced decision tree standard pruning procedure starting largest possible tree requiring delete splits achieve desired maximum depth 
computationally wasteful depth small 
time required build tree proportional depth largest possible tree pruning 
dramatic computational savings achieved simply stopping growing process maximum depth alternatively maximum number terminal nodes 
standard heuristic arguments favor growing large trees pruning apply context boosting 
shortcomings individual tree compensated trees grown boosting sequence 
truncation strategy number terminal nodes employed necessary define order splitting takes place 
adopt best strategy 
optimal split computed currently terminal node 
node split achieve greatest reduction tree building criterion split 
increases number terminal nodes 
continues maximum number terminal notes induced 
standard computational tricks employed inducing trees order requires computation orderings commonly decision tree induction 
truncation limit applied trees boosting sequence 
meta parameter entire boosting procedure 
optimal value estimated standard model selection techniques minimizing cross validated error rate final boosted model 
refer combination truncated best trees boosting additive logistic trees alt 
procedure simulated real examples 
compare results tables corresponding results reported dietterich table common coordinate functions additive logistic trees coordinate functions additive logistic tree obtained boosting stumps class nested sphere example section 
data sets 
error rates achieved alt small truncation values seen compare quite favorably committee approaches larger trees boosting step 
error rates computational savings associated alt quite important data mining contexts large data sets cause computation time issue 
advantage low order approximations model visualization 
particular models additive input features contribution feature viewed graph plotted shows plots features class nested spheres example fig 

functions shown class concentrated near origin corresponding functions class negatives functions 
plots fig 
clearly show contribution log odds individual feature approximately quadratic matches generating model 
classes plots similar fig 
class analogously interpreted 
higher order interactions models difficult visualize 
feature interactions variable contributions visualized contour perspective mesh plots 
feature interactions visualization techniques effective 
non interaction stump models achieve highest accuracy useful descriptive statistics owing interpretability resulting model 
weight trimming section propose simple idea show dramatically reduce computation boosted models sacrificing accuracy 
despite apparent simplicity approach appear common 
boosting iteration distribution weights training sample 
iterations proceed distribution tends highly skewed smaller weight values 
larger fraction training sample correctly classified increasing confidence receiving smaller weights 
observations low relative weight little impact training base classifier carry dominant proportion weight mass influential 
fraction high weight observations small iterations 
suggests iteration simply delete training sample large fraction observations low weight having effect resulting induced classifier 
computation reduced tends proportional size training sample regardless weights 
boosting iteration training observations weight threshold fi train classifier 
take value fi fith quantile weight distribution training data corresponding iteration 
observations carry fraction gamma fi total weight mass training 
typically fi data training carries percent total weight mass note weights training observations recomputed iteration 
observations deleted particular iteration may re enter iterations weights subsequently increase relative observations 
left panel shows test error rate function iteration number letter recognition problem described section gentle adaboost node trees base classifier 
error rate curves shown 
black solid represents full training sample iteration fi blue dashed curve represents corresponding error rate fi curves track closely especially iterations 
right panel shows corresponding fraction observations train base classifier function iteration number 
curves similar 
fi number observations training drops rapidly reaching roughly total iterations 
iterations stays rest boosting procedure 
number terms test error number terms left panel shows test error letter recognition problem function iteration number 
black solid curve uses training data blue dashed curve uses subset weight thresholding 
right panel shows percent training data approaches 
upper curve steps training entire class fit sufficiently see text 
computation reduced factor apparent loss classification accuracy 
reason sample size case decreases fi iterations observations particular class classified correctly high confidence log training class stops continues remaining classes 
iterations classes remained original classes 
column labeled fraction table shows average fraction observations training base classifiers iterations boosting methods tree sizes 
node trees methods behave shown fig 

stumps logitboost uses considerably data correspondingly faster 
genuine property logitboost gives advantage weight trimming 
methods logitboost weights gamma way involve class outputs simply measure nearness currently estimated decision boundary fm 
discarding small weights retains training observations estimated close boundary 
procedures weight monotone gammay fm 
gives highest weight currently misclassified training observations especially far boundary 
trimming fraction observations remaining error rate subsample passed base learner highly unbalanced containing correctly classified observations 
imbalance inhibit learning 
imbalance occurs logitboost near decision boundary correctly misclassified observations appear roughly equal numbers 
example illustrates large reductions computation boosting achieved simple trick 
variety examples shown exhibit similar behavior boosting methods 
note committee approaches classification bagging breiman randomized trees dietterich admitting parallel implementations take advantage approach reduce computation :10.1.1.32.9399:10.1.1.131.1931
concluding remarks order understand learning procedure statistically necessary identify important aspects structural model error model 
important determines function space approximator characterizing class functions accurately approximated 
error model specifies distribution random departures sampled data structural model 
defines criterion optimized estimation structural model 
shown structural model boosting additive logistic scale base learner providing additive components 
understanding explains properties boosting 
surprise large number jointly optimized components defines richer class learners 
reveals context boosting base learners equivalent universally best choice situations 
illustrated section base learners need chosen resulting additive expansion matches particular decision boundary encountered 
limited context boosting decision trees interaction order characterized number terminal nodes needs chosen care 
purely additive models induced decision stumps best 
conjecture boundaries involving high order interactions rarely encountered practice 
motivates additive logistic trees alt procedure described section 
error model class boosting obvious binary variables bernoulli distribution 
show adaboost procedures maximize criterion closely related expected log bernoulli likelihood having identical solution distributional limit infinite data 
derived direct procedure maximizing log likelihood logitboost show exhibits properties nearly identical real adaboost 
multi class case adaboost procedures maximize separate bernoulli likelihood class versus 
natural choice especially appropriate observations belong class schapire singer 
usual setting unique class label observation symmetric multinomial distribution appropriate error model 
develop multi class logitboost procedure maximizes corresponding log likelihood quasi newton stepping 
show simulated examples exist settings approach leads superior performance situations encountered set real data examples illustration performance approaches quite similar performance examples 
concepts developed suggest little connection deterministic weighted boosting randomized ensemble methods bagging breiman random ized trees dietterich :10.1.1.32.9399:10.1.1.131.1931
language squares regression purely variance reducing procedures intended mitigate instability especially associated decision trees 
boosting hand fundamentally different 
appears purely bias reducing procedure intended increase flexibility stable highly biased weak learners incorporating jointly fitted additive expansion 
distinction clear boosting implemented finite random importance sampling weights 
advantages disadvantages introducing randomization boosting drawing finite samples clear 
turns advantage randomization situations degree randomization reflected sample size open question 
obvious common choice size original training sample optimal situations 
fascinating issue covered fact boosting flavor seldom overfit matter terms included additive expansion 
possible explanations ffl logitboost iterations proceed impact changes introduced reduces 
observations appreciable weight determine new functions near decision boundary 
definition observations near zero affected changes pure regions large values jf modified 
ffl stage wise nature boosting algorithms allow full collection parameters jointly fit far lower variance full parameterization suggest 
machinelearning literature explained terms vc dimension ensemble compared weak learner 
ffl classifiers hurt overfitting function estimators famous risk bound nearest neighbor classifier cover hart 
explanation empirical evidence strong boosting schapire freund colleagues brought exciting important set new ideas table 
andreas buja alerting text classification laboratories bogdan popescu illuminating discussions pac learning theory 
jerome friedman partially supported department energy contract number de ac sf dms national science foundation 
trevor hastie partially supported dms dms national science foundation roi ca national institutes health 
robert tibshirani supported natural sciences engineering research council canada 
breiman 
bagging predictors machine learning breiman 
prediction games arcing algorithms technical report technical report statistics department university california berkeley 
submitted neural computing 
breiman 
combining predictors technical report statistics department university california berkeley 
breiman friedman olshen stone 
classification regression trees wadsworth belmont california 
buja hastie tibshirani 
linear smoothers additive models discussion annals statistics 
cover hart 
nearest neighbor pattern classification proc 
ieee trans 
inform 
theory pp 

dietterich 
experimental comparison methods constructing ensembles decision trees bagging boosting randomization machine learning 
freund 
boosting weak learning algorithm majority information computation 
freund schapire 
experiments new boosting algorithm machine learning proceedings thirteenth international conference pp 

friedman 
multivariate adaptive regression splines discussion annals statistics 
friedman 
approach classification technical report stanford university 
friedman stuetzle 
projection pursuit regression journal american statistical association 
hastie tibshirani 
generalized additive models chapman hall 
hastie tibshirani 
classification pairwise coupling annals statistics 
appear 
hastie tibshirani buja 
flexible discriminant analysis optimal scoring journal american statistical association 
holte 
simple classification rules perform commonly datasets machine learning 
kearns vazirani 
computational learning theory mit press 
mallat zhang 
matching pursuits time frequency dictionaries ieee transactions signal processing 
mccullagh nelder 
generalized linear models chapman hall 
schapire 
strength weak learnability machine learning 
schapire singer 
improved boosting algorithms confidence rated predictions proceedings eleventh annual conference computational learning theory 

