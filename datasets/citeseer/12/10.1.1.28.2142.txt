imitation reinforcement learning agents heterogeneous actions bob price price cs ubc ca department computer science university british columbia vancouver canada craig boutilier cs toronto edu department computer science university toronto toronto canada study problem accelerating reinforcement learning rl observation implicit imitation expert agents mentors acting domain 
consider problems arise learner mentor heterogeneous actions 
extend earlier implicit imitation model allow feasibility testing determining specific mentor action duplicated repair discovering plan simulates mentor trajectory demonstrate empirically components allow agents learn readily standard rl agents implicit imitation agents capabilities 

cooperative multiagent systems rely shared models communication coordinate actions common environment 
researchers examined explicit communication argued implicit communication techniques imitation increase range applications multi agent systems pose interesting cognitive models interaction agent societies dautenhahn price boutilier 
imitation model implicit communication agents learn communicating explicit context applicability behaviour bakker kuniyoshi need pre existing communication protocol competitive situations agents unwilling share information agents unwilling fulfill teacher role 
ability imitation effect skill transfer agents demonstrated range domains atkeson schaal billard hayes hayes demiris kuniyoshi mataric mitchell utgoff clouse 
domains dealt agents imitating agents essentially action set 
goal extend benefits imitation situations capabilities agents environment differ 
previous showed implicit imitation improve reinforcement learning rl agent effectiveness allowing take advantage knowledge implicit observations skilled agents price boutilier 
assume learner shared objectives mentors rely crucially fact actions homogeneous action taken mentor corresponded action available learner 
relax assumption introduce mechanisms allow acceleration rl presence heterogeneous actions 
specifically introduce notions action feasibility testing allows learner determine specific mentor action duplicated step repair learner attempts determine approximate mentor trajectory 
concepts modify influence mentor observations learner estimate value function 
viewed loosely falling formal imitation framework proposed nehaniv dautenhahn propose viewing imitation model process constructing mappings states actions goals different agents see abstraction model kuniyoshi 

key differences include fact assume state space mappings mentor actions directly observable objectives goals mentor learner may different environments stochastic 
furthermore require learner explicitly try duplicate behavior mentor 
way model atkeson schaal differs demonstration models robotics hayes demiris mataric 
repair strategies invoke bear relation models 
similar spirit research behavioral cloning objectives generally different 
specifically aim cloning exactly reproduce observed behavior simple classification learning policy sammut inducing objective function observed behavior optimal control policy computed suc bratko 
number domains implicit imitation play important role accelerating reinforcement learning rl 
include robot motion planning computer animation adaptation new controllers environments existing controllers successfully employed acceleration learning schedule repair strategies rl zhang dietterich observation human expert 
elaborate final section 

imitation homogeneous actions implicit imitation price boutilier assume agents mentor observer acting fixed environment 
assume observer learner learning control markov decision process mdp states actions reward function pr tjs denote probability transition state action taken 
mentor controlling mdp underlying state space am rm prm denote mdp 
assumptions mentor implements deterministic stationary policy induces markov chain prm tjs prm tjs action taken mentor exists action distributions pr prm 
assumption homogeneous action assumption implies learner duplicate mentor policy 
assume learner knows priori identity action state learner wants duplicate policy agents may different reward functions 
learner observe mentor transitions actions directly form estimates mentor markov chain estimates mdp transition probabilities reward function 
define augmented bellman equation follows fl max max ao pr tjs prm tjs usual bellman extra term added second summation denoting expected value duplicating mentor action 
unknown action identical observer actions term redundant augmented value equation valid 
furthermore certain standard assumptions show estimates model quantities converge true values implicit imitation learner acting accordance value estimates converge optimally standard rl assumptions 
interesting fact acting accordance extension multiple mentors straightforward price boutilier 
generalization stochastic policies easily handled 
assume appropriate exploration strategy value estimates produced augmented bellman backups observer generally converges quickly learner guidance mentor 
demonstrated price boutilier implicit typically accumulate reward higher rate earlier standard model rl agents mentor reward function identical observer 
states mentor visits infrequently traversed optimal policy learner estimates mentor markov chain may poor compared learner estimated action models 
cases suppress mentor influence 
model confidence augmented backups 
mentor markov chain observer action transitions assume dirichlet prior parameters multinomial distributions 
sample counts mentor observer transitions learner updates distributions 
technique inspired kaelbling interval estimation method variance estimated dirichlet distributions model parameters construct lower bounds augmented value function incorporating mentor model value function strictly observer experience 
lower bound augmented value function lower bound value function augmented value fact lower highly variable 
circumstance suppression mentor influence appropriate bellman backup 

imitation heterogeneous actions homogeneity assumption violated implicit imitation framework described cause learner perform poorly 
particular learner unable state transition transition probability mentor state may drastically overestimate value state 
inflated value estimate causes learner return repeatedly state exploration produce feasible action obtaining inflated estimated value 
mechanism removing influence mentor markov chain value estimates observer extremely correctly confident mentor model 
problem lies fact augmented bellman backup justified assumption observer duplicate mentor action 
overcome difficulty propose techniques allow observers retain guidance mentors suppress guidance apparent misleading 
fundamental sense straightforward action feasibility testing intuitively learner sure duplicate mentor action state suppresses effect augmented backups state reverting stan influenced estimated value learner choose actions higher estimated value 
dard bellman backups 
technique simple eliminates lockup effect observed basic implicit imitation framework agents differing capabilities 
unfortunately cause useful guidance form higher value estimates cut certain cases guidance useful 
specifically learner repair mentor trajectory finding short sequence actions leads state infeasible action value guidance appropriate 
reason introduce notion step repair method deciding allow mentor guidance persist state despite infeasibility mentor action observer 
action feasibility testing dirichlet distributions model find variance associated transition probability estimate 
variance test feasibility mentor action 
examine simple case suppose successor states specific action taken estimate probability pr tjs 
suppose mentor action am similarly restricted mentor markov chain state modeled prm tjs 
test statistically actions am performing difference means test hypothesis mean probability getting state actions 
hypothesis pooled variance statistics computed weighting variances number samples statistic pr gamma pr ar nm ar nm ff dirichlet distribution highly non normal small sample sizes construct test criterion ff inequality 
value left side equation greater right conclude actions different point having observer attempt duplicate mentor 
generally number possible outcomes action just perform multivariate difference means test 
behaved distributions normal exist multi variate difference means tests 
specific multivariate testing dirichlet generalized beta distributions assumes sufficient number samples bounds computed reasonably tight goodman 
second method applicable multivariate dirichlet distributions bonferroni test allows construct multivariate test univariate components 
assumptions normality independence shown give results practice mi sampson decision binary envision smoother decision criterion measures extent mentor action duplicated 
table 
action feasibility testing function feasible boolean ao true successors delta pro tjs gamma prm tjs delta delta tjs tjs delta ff false return true return false 
computational ease lead employ bonferroni method implementation 
idea bonferroni test perform multivariate hypothesis test conjoining single variable tests 
generally set specific hypotheses wish test simultaneously 
complementary hypothesis bonferroni inequality tells pr gamma pr theta obtain probability ff joint hypothesis testing complementary hypotheses ff individual hypotheses independent 
testing action equivalence individual hypotheses correspond tests see transition probability particular successor state actions joint hypothesis successor state transition probabilities actions number successor states 
summarize test distribution successor states mentor unknown action distribution successor states observer actions bonferroni test 
observer experience action models rejected concludes mentor action infeasible influence model derived mentor observations suppressed 
algorithm summarized table 
step repair observer duplicate mentor action particular state guidance mentor may useful trajectory mentor state space broadly similar feasible trajectory observer 
capture notion similarity augmenting feasibility testing device encourages learner find similar trajectories 
example suppose observer state mentor observed transition state state state times observer estimates prm tjs prm confident see 
suppose state highly rewarding state 
prior guidance agents 
basis observations observer assigns high value encouraged move states exploration 
suppose time mentor action state judged infeasible obstacle navigable mentor learner 
observer embarked sufficient exploration area discover alternate path judgment value state plunge immediately 
turn eliminates observer motivation move state explore local alternatives 
observer assumes default roughly similar trajectory mentor may persist backing value belief able discover local path bridge intuitively bridge short feasible path bridges gap value function due infeasible action 
starts mentor trajectory state observer duplicate mentor action navigates infeasible transition mentor trajectory downstream infeasible transition 
bridges provide important guidance value defined augmented bellman backup determined mentor action learner actions 
states value estimates drop drastically soon mentor action discovered infeasible bridge discovered 
note bridges formed naturally imitation model formulated far 
frequently random exploration agent attempts duplicate mentor path cause sample states actions general vicinity mentor path 
action mentor path judged infeasible alternative paths partially explored unattractive vicinity mentor worth checking form bridges 
difficult problems little exploration significantly mentor trajectory background agent insufficient provide bridges 
second source bridges comes grid world domain 
uniform prior possible action effects state judged initially reachable nonnegligible probability states neighborhood 
situation occurs described mentor action deemed infeasible learner value estimate drops 
drop mitigated exploit local topology grid world experiments state connected action priori neighbours 
step paths 
reachability flow value obstacle neighboring states 
uniform priors help process 
encourage learner persist exploring neighborhood feasible bridge exists fairly early 
prior guidance reliable means discovering bridges 
combined effect discounting small prior probability state transitions cause magnitude value decrease rapidly length trajectory backed 
negative rewards easily small values 
states significantly distant value gradient point significant way 
consider explicit means encouraging exploration area 
step repair strategy initiates explicit searches bridges specifies criteria detecting formation caches existence bridge order eliminate need check 
step repair uses reachability analysis learner current domain model test existence bridge 
consider situation 
learner discovers mentor action state infeasible undertakes search existing bridge 
bridge termination state state mentor trajectory steps algorithm steps mentor transitions greater prior probability 
observer searches step bridge starts follows observed feasible transitions ends bridge termination state 
observed transitions considered misleading priors undue influence 
bridge mentor influence ignored state value flowing back existing bridge 
flag state bridged preclude bridge tests 
bridge immediately suppress mentor influence state 
intuitively keep value flowing back encourage observer come state infeasible action explore local neighbourhood discounting mentor influence 
imitation sensible domain expect reasonable assume path repaired short search steps 
search performed step random walk grid worlds average explores locations steps starting point locations steps away 
walk observer encounters bridge termination state set bridge flag originating state suppress value backup sible transition 
attempts discover bridges long bridge remains undiscovered performed times visits state 
mentor action infeasible mentor influence retained agent believes bridge possible order guide agent starting point potential bridge 
random walks attempts mentor influence state suppressed 
note step developed measure similarity agents 
measure decide worthwhile attempt repairs required imitate mentor 
feasibility step repair easily integrated existing imitation framework 
complete decision procedure appears 
original model check see observer experience calculation value state better lower bound mentor calculation observer uses experience calculation 
check see observer sufficient number samples behaviour perform action feasibility test 
assume default action taken mentor feasible observer 
assumption cause permanent harm error increase value state turn cause observer explore state increase number experience samples state 
threshold samples 
sufficient data perform action feasibility test 
mentor action feasible accept value calculated mentor observations value function 
action infeasible check see possible bridging 
test checks qualities state bridge built bridging unnecessary 
exhausted threshold bridging attempts say impossible 
case bridging actions necessary dispense mentor guidance observer calculations 
bridging possible delay suppression mentor influence augmented value function guide agent bridge building states repair potentially 

empirical demonstrations section empirically demonstrate utility feasibility testing step repair show techniques differences actions agents small local differences state space topology 
problems chosen specifically guarantee executing stochastic action required form bridge form bridge trial 
bridge discovered guarantee optimal problems bridge increase attractiveness state determine suitable values assumptions state space structure noise level actions 
gamma suitable connected grid world low noise 
note large values reduce performance non imitating agents 
sufficient data feasibility test possible 
step repair accept mentor influence test lower bound reject mentor influence infeasible feasible observer better mentor better 
implicit imitation feasibility tests demonstrate necessity utility feasibility testing step repair 
space limited refer price boutilier discussion gains due imitation increase problem size qualitative difficulty 
experiment shows necessity feasibility testing implicit imitation agents heterogeneous actions 
scenario agents navigate obstacle free gridworld upper left corner goal location lower right 
agent reset upper left corner 
agent mentor news action set north south east west movement actions 
mentor optimal stationary policy problem 
study performance learners skew action set ne sw unable duplicate mentor exactly duplicating mentor move requires learner move ne followed 
learner employs implicit imitation feasibility testing second uses imitation feasibility testing third control agent uses imitation standard rl agent 
agents experience limited stochasticity form chance action randomly perturbed 
price boutilier agents model reinforcement learning prioritized sweeping moore atkeson 

effectiveness feasibility testing implicit imitation seen 
horizontal axis represents time simulation steps vertical axis represents average number goals achieved time steps averaged runs 
see imitation agent feasibility testing converges quickly optimal goal attainment rate agents 
agent feasibility testing achieves sporadic success early frequently locks due repeated attempts duplicate infeasible mentor actions 
agent manages reach goal time time stochastic actions permit agent permanently stuck feas ctrl fs series simulation steps average reward steps 
utility feasibility testing 
obstacle map mentor path obstacle free scenario 
control agent form significant delay convergence relative due lack form guidance easily surpasses agent feasibility testing long run 
gradual slope control agent due higher variance control agent discovery time optimal path imitator control converge optimal solutions eventually 
shown comparison imitation agents feasibility testing necessary adapt implicit imitation contexts involving heterogeneous actions 
developed feasibility testing bridging primarily deal problem adapting agents heterogeneous actions 
techniques applied agents differences state space connectivity equivalent notions ultimately 
test constructed domain agents news action set alter environment learners introducing obstacles aren mentor 
learners find mentor path obstructed obstacles 
movement obstacle causes learner remain current state 
sense action different effect mentor 
see results qualitatively similar previous experiment 
top goal rate achieved observer feasibility testing control agent higher action set mentor mimic path directly 
observer feasibility testing difficulty maze obstacles value estimates augmented backups lead states feas ctrl fo series simulation steps average reward steps 
interpolating obstacles 
parallel generalization path goal directly blocked 
essentially local differences state handled feasibility testing 
demonstrate feasibility testing completely generalize mentor trajectory 
mentor follows path completely infeasible imitating agent 
fix mentor path runs give imitating agent maze shown states mentor visits blocked obstacle 
imitating agent able mentor trajectory guidance builds parallel trajectory completely disjoint mentor 
results show gain imitator feasibility testing control agent diminish marginally exist imitator forced generalize completely infeasible mentor trajectory 
agent feasibility testing poorly compared control agent 
gets stuck doorway 
high value gradient backed mentor path accessible agents doorway 
imitation agent feasibility conclude proceed south doorway wall try different strategy 
imitator feasibility testing explores far away doorway setup independent value gradient guide goal 
slower decay schedule exploration imitator feasibility testing find goal reduce performance imitator feasibility testing 
imitator feasibility testing prior beliefs follow mentor backup value perpendicular feas ctrl fp series simulation steps average reward steps 
parallel generalization results mentor path 
value gradient form parallel infeasible mentor path imitator follow side infeasible path doorway necessary feasibility test proceeds goal 
explained earlier simple problems chance informal effects prior value leakage stochastic exploration may form bridges feasibility testing cuts value propagation guides exploration 
difficult problems agent spends lot time exploring accumulate sufficient samples conclude mentor actions infeasible long agent constructed bridge 
imitator performance drop reinforcement learner 
demonstrate bridging devised domain agents navigate upper left corner river bottom right corner 
river runs vertically steps wide penalty step 
goal state worth 
long exploration phase agents generally discover negative states river curtail exploration direction making 
examine value function estimate steps imitator feasibility testing repair capabilities see due suppression feasibility testing dark high value states backed goal terminate abruptly infeasible transition making river see 
fact dominated lighter grey circles showing negative values 
barrier forms agent optimistic exploration policy get goal considerable exploration 
experiment apply step repair agent problem 
examining graph see imitation agents experience early negative dip guided deep river mentor influence 
agent repair eventually decides mentor action infeasible avoids river possibility finding goal 
imitator repair discovers mentor action infeasible immediately dispense mentor guidance 
keeps 
river scenario repair repair ctrl ctrl fb series simulation steps average reward steps 
utility bridging exploring area mentor trajectory random walk accumulating negative reward suddenly finds bridge rapidly converges optimal solution 
control agent discovers goal runs 

seen feasibility testing extends implicit imitation principled manner deal situations homogeneous actions assumption invalid 
adding bridging capabilities preserves extends mentor guidance presence infeasible actions due differences action capabilities local differences state spaces 
new approach model compute actions imitator take requiring observer duplicate mentor actions exactly 
approach relates idea sense imitator uses local search model repair discontinuities augmented value function acting world 
point investigations exploratory suggest implicit imitation effectively speed rl observer abilities differ mentor 
implicit imitation concrete applications necessary better gauge effectiveness 
expect combining enhanced algorithm advanced exploration techniques gen repair steps take place area negative reward scenario need case 
repair doesn imply shortterm negative return 
capabilities open broad range tasks mobile robot navigation process control language learning 
applications mobile robotics example exploit techniques directly 
optimal learned controller robot learning process second robot different constraints dynamics accelerated implicit imitation sample trajectories robot 
different holonomic contraints robot bodies viewed directly inducing heterogeneous actions type described 
practical experience necessary determine effectiveness method exploratory analysis encouraging 
currently investigating important extension model continuous action state spaces 
part motivated application exploring apply imitation learning controllers realistic expressive character animation 
optimal control techniques plan trajectories animated characters subject constraints dynamics van de panne optimization problems difficult solve suggests rl methods 
imagine provided motion capture data specific task walking empty room trajectories generated controller task 
suppose changes environment room crowded reward function controller penalized bumping people constraints motion character limp new controller constructed 
existing controller circumstances simply seeding new rl problem old controller value function may lead excessive exploration irrelevant parts state space 
expect implicit imitation ability take new constraints reward function consideration bootstrap process learning new controller existing trajectories 
important research direction involves extending model deal partially observable environments explicit abstraction generalization techniques order tackle wider range problems 
reviewers helpful suggestions michael neff discussions animation 
research supported iris phase project bac 
atkeson schaal 

robot learning demonstration 
proceedings fourteenth international conference machine learning pp 

nashville tn 
bakker kuniyoshi 

robot see robot overview robot imitation 
aisb workshop learning robots animals pp 

brighton uk 
billard hayes 

learning communicate imitation autonomous robots 
seventh international conference artificial neural networks pp 

lausanne switzerland 
dautenhahn 

getting know artificial social intelligence autonomous robots 
robotics autonomous systems 
goodman 

simultaneous multinomial proportions 
technometrics 
hayes demiris 

robot controller learning imitation technical report dai 
university edinburgh 
dept artificial intelligence 
kaelbling 

learning embedded systems 
cambridge ma mit press 
kuniyoshi inaba inoue 

learning watching extracting reusable task knowledge visual observation human performance 
ieee transactions robotics automation 
mataric 

communication reduce locality distributed multi agent learning 
journal experimental theoretical artificial intelligence 
mataric williamson demiris mohan 

behaviour primitives articulated control 
proceedings animals animats fifth international conference simulation adaptive behavior pp 

zurich 
mi sampson 

comparison bonferroni bounds 
journal statistical planning inference 
mitchell mahadevan steinberg 

leap learning apprentice vlsi design 
proceedings ninth international joint conference artificial intelligence pp 

los altos california morgan kaufmann publishers moore atkeson 

prioritized sweeping reinforcement learning data real time 
machine learning 
nehaniv dautenhahn 

mapping dissimilar bodies affordances algebraic foundations imitation 
proceedingsof learning robots pp 

edinburgh 
price boutilier 

implicit imitation multiagent reinforcement learning 
proceedings sixteenth international conference machine learning pp 

bled si 
sammut hurst michie 

learning fly 
proceedings ninth international conference machine learning pp 

aberdeen uk 


analysis variance 
new york wiley 


multivariate observations 
new york wiley 
suc bratko 

skill reconstruction induction lq controllers subgoals 
proceedings fifteenth international joint conference artificial intelligence pp 

nagoya 
utgoff clouse 

kinds training information evaluation function learning 
proceedings ninth national conference artificial intelligence pp 

anaheim ca aaai press 
van de panne fiume 

reusable motion synthesis state 
computer graphics pp 

dallas tx 


crc concise encyclopedia mathematics 
boca raton crc press 
extended version wolfram com 
zhang dietterich 

reinforcement learning approach job shop scheduling 
proceedings fourteenth international joint conference artificial intelligence pp 

montreal 
