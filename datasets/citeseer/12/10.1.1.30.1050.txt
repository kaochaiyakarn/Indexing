wrapper approach ron kohavi george john data mining visualization data mining silicon graphics marketing software shoreline boulevard drive mountain view ca mountain view ca ronnyk sgi com cs stanford edu robotics stanford edu ronnyk robotics stanford edu feature subset selection problem learning algorithm faced problem selecting relevant subset features focus attention ignoring rest 
achieve best possible performance particular learning algorithm particular training set feature subset selection method consider algorithm training set interact 
explore relation optimal feature subset selection relevance 
wrapper method searches optimal feature subset tailored particular algorithm domain 
compare wrapper approach induction feature subset selection relief filter approach feature subset selection 
improvement accuracy achieved datasets families induction algorithms decision trees naive bayes 
addition feature subsets selected wrapper significantly smaller original subsets learning algorithms producing comprehensible models 
supervised machine learning induction algorithm typically set training instances instance described vector feature attribute values class label 
example medical diagnosis problems features include age weight blood pressure patient class label indicate physician parts chapter reprinted artificial intelligence journal vol 
nos 
pp 
kind permission elsevier science nl sara kv amsterdam netherlands 
feature selection search estimation accuracy estimated final evaluation test set algorithm feature set hypothesis performance training set training set feature evaluation induction algorithm feature set feature set induction wrapper approach feature subset selection 
induction algorithm black box subset selection algorithm 
patient suffering heart disease 
task induction algorithm inducer induce training data classifier useful classifying cases 
classifier mapping space feature values set class values 
feature subset selection problem learning algorithm faced problem selecting subset features focus attention ignoring rest 
idea wrapper approach john kohavi pfleger kohavi john shown simple induction algorithm black box 
repeatedly run dataset various feature subsets 
method evaluate performance subset feature subset highest evaluation chosen final set run induction algorithm 
resulting classifier evaluated independent test set search 
typical goal supervised learning algorithms maximize classification accuracy unseen test set accuracy metric guiding feature subset selection 
practical machine learning algorithms decision tree algorithms quinlan cart breiman instance algorithms ibl aha known degrade prediction accuracy trained data containing superfluous features 
algorithms naive bayes duda hart domingos pazzani robust respect irrelevant features performance degrades correlated relevant features added 
john shows examples adding single irrelevant feature credit approval diabetes dataset degrades performance 
problem feature subset selection finding subset original features dataset induction algorithm run data containing features generates classifier highest possible accuracy 
purely theoretical standpoint question features may interest 
bayes rule predicts probable class instance full joint probability distribution wrappers features class 
bayes rule monotonic adding features decrease accuracy subset selection useless 
practical learning scenarios faced problems 
learning algorithms access underlying distribution usually relatively small training set 
second quite similar algorithms may incorporate different heuristics aid quickly building models training data finding smallest model consistent data np hard cases rivest blum rivest 
define optimal feature subset respect particular induction algorithm dataset account algorithm biases interaction training sample definition inducer training dataset features xn optimal feature subset opt subset features maximizes accuracy induced classifier 
definition get highest possible accuracy best subset feature subset selection algorithm select optimal feature subset 
chapter organized follows 
section definitions relevance distinguish relevance optimality 
section describe filter methods feature subset selection 
section describes algorithm wrapper approach comparing algorithm empirically filter algorithm 
related discussed sections section 
relevance features section define degrees relevance weak strong 
earlier definitions relevance reviewed kohavi john 
define relevance terms bayes rule 
feature strongly relevant removal result performance deterioration optimal bayes rule 
feature weakly relevant strongly relevant contexts may contribute prediction accuracy optimal bayes rule 
feature relevant weakly relevant strongly relevant irrelevant 
definition strong relevance set features gamma fx feature strongly relevant iff exist values definition weak relevance feature weakly relevant iff strongly relevant exists subset features exists subset selection feature input features algorithm induction feature filter approach features filtered independently induction algorithm 
bayes rule optimal set features include strongly relevant features possibly weakly relevant features 
classifiers induced real data nice theoretical properties 
relevance feature imply optimal feature subset particular induction algorithm somewhat surprisingly irrelevance imply optimal feature subset kohavi john 
example optimality imply relevance feature takes value irrelevant definitions 
consider limited perceptron classifier rosenblatt 
weight associated feature classifies instances linear combination weights feature values greater zero 
adding feature set limited perceptron equivalent representational power regular perceptron additional parameter allowing arbitrary threshold just zero 
removal irrelevant features remove crucial feature 
relevance optimality equivalent concepts idea related empirically motivates set feature selection methods measure relevance feature data 
filter approach wrapper approach attempts identify best feature subset particular algorithm filter approach common statistics attempts assess merits features data 
filter approach shown selects features preprocessing step training data 
section describe algorithms machine learning literature focus relief tree filters 
focus almuallim dietterich almuallim dietterich feature selection algorithm noise free boolean domains 
exhaustively examines subsets features selecting minimal subset features sufficient determine label value instances training set 
relief algorithm kira rendell kononenko assigns relevance weight feature meant denote relevance feature target concept 
relief algorithm attempts find relevant features 
real domains features high correlations label weakly relevant removed relief 
kohavi john discuss early experiments relief wrappers relief focus totally irrelevant features strongly relevant features weakly relevant features view feature relevance 
variant relieved 
relief searches relevant features weak strong 
tree filters cardie decision tree algorithm select subset features typically nearest neighbor algorithm 
datasets may select bad feature subsets features decision trees necessarily useful nearest neighbor 
due fragmentation tree may fail include relevant features 
filter approaches feature subset selection take account biases induction algorithms select feature subsets independent induction algorithms 
cases measures devised algorithm specific may computed efficiently 
example measures mallows prediction sum squares devised specifically linear regression 
tailored measures algorithms naive bayes 
filter approaches fail corral artificial dataset john example 
instances boolean features plus boolean class 
target concept 
feature named irrelevant uniformly random feature correlated matches class label time 
greedy strategies building decision trees pick correlated feature best known selection criteria 
wrong root split instances fragmented instances subtree describe correct concept 
correlated feature weakly relevant harmful decision trees 
feature removed optimal tree 
filter perfectly selected strongly weakly relevant features naive bayes example cases performance naive bayes improves removal relevant features 
examples discussion relevance versus optimality suggest feature selection method take induction algorithm account 
wrapper approach wrapper approach shown feature subset selection done induction algorithm black box knowledge algorithm needed just interface 
feature subset selection algorithm conducts search subset induction algorithm part evaluation function 
accuracy induced classifiers estimated accuracy estimation techniques kohavi 
problem investigating state space search different search engines investigated sections 
wrapper approach conducts search space possible parameters 
search requires state space initial state termination condition search engine ginsberg russell norvig 
search space organization chose state represents feature subset 
features bits state bit indicates feature absent 
operators determine connectivity states chosen operators add delete single feature state corresponding search space commonly stepwise methods statistics 
shows state space operators feature problem 
size search space features impractical search space exhaustively small 
goal search find state highest evaluation heuristic function guide 
know actual accuracy induced classifier accuracy estimation heuristic function evaluation function 
evaluation function fold crossvalidation repeated multiple times 
number repetitions determined fly looking standard deviation accuracy estimate assuming independent 
standard deviation accuracy estimate cross validations executed execute cross validation run 
heuristic practice avoids multiple cross validation runs large datasets 
best search engine best search russell norvig ginsberg robust search method 
idea select promising node generated far expanded 
algorithm varies slightly standard version explicit goal condition problem 
best search usually terminates reaching goal 
problem optimization problem search stopped point best solution far returned theoretically improving time making anytime algorithm boddy dean 
practice wrappers state space search feature subset selection 
node connected nodes feature deleted added 
dotted arrows enhancement explained section 
run stage call stale search improved node expansions terminate search 
improved node defined node accuracy estimation ffl higher best far 
experiments set epsilon 
best search thorough search technique obvious better say hill feature subset selection 
bias variance tradeoff geman kohavi wolpert possible thorough search increase variance reduce accuracy 
state space compound operators previous section looked search engine 
section look topology state space dynamically modify accuracy estimation results 
state space commonly organized node represents feature subset operator represents addition deletion feature 
main problem organization search expand generate successors node path initial feature subset best feature subset 
section introduces new way change search space topology creating dynamic operators directly connect node nodes considered promising evaluation children 
motivation compound operators comes feature subsets partitioned strongly relevant weakly relevant irrelevant features 
practice optimal feature subset contain relevant features strongly weakly relevant features 
backward elimination search starting full set features removes feature time expanding children reachable operator expand children node removing single feature 
irrelevant features features delta nodes evaluated 
similar reasoning applies forward selection search starting empty set features 
domains feature subset selection useful features search may prohibitively expensive 
compound operators operators dynamically created standard set children created add delete operators evaluated 
single node expansion discarded 
intuitively information evaluation children just identification node maximum evaluation 
compound operators combine operators led best children single dynamic operator 
depicts possible set compound operators forward selection 
formally rank operators estimated accuracy children define compound operator combination best operators 
example compound operator combine best operators 
best operators added feature compound operator add operator added operator deleted try operation 
compound operators applied parent creating children nodes farther away state space 
compound node evaluated generation compound operators continues long estimated accuracy compound nodes improves 
compound operators improve search finding nodes higher accuracy faster 
main advantage compound operators backward feature subset selection computationally feasible 
compound operators number features decrease increase node expansion 
example dna dataset nodes evaluated best backward feature subset selection subset features selected 
compound operators algorithm expand gamma delta nodes just get feature subset 
backward fss slow 
compared original algorithm wrapper runs orders magnitude slower 
example running dna dataset takes minutes 
wrapper model run times node evaluated state space dna hundreds nodes 
wrappers table comparison feature selection relieved filter rlf wrapper backward best search compound operators bfs 
val columns indicates probability top algorithm improving lower algorithm 
rlf bfs bfs dataset rlf bfs vs vs vs rlf breast cancer sigma sigma sigma cleve sigma sigma sigma crx sigma sigma sigma dna sigma sigma sigma horse colic sigma sigma sigma pima sigma sigma sigma sick euthyroid sigma sigma sigma soybean large sigma sigma sigma corral sigma sigma sigma sigma sigma sigma monk sigma sigma sigma monk local sigma sigma sigma monk sigma sigma sigma monk sigma sigma sigma average real average artif 
experimental comparison arguments favor wrapper approach 
section show experimental comparison 
modern algorithm performs variety real databases expect difficult improve performance feature selection 
table shows case accuracy real datasets decreased relieved accuracy slightly increased wrapper relative reduction error 
note relieved perform artificial databases corral contain strongly relevant totally irrelevant attributes 
artificial datasets relieved significantly better plain confidence level 
real datasets relevance ill determined relieved worse plain dataset performance significantly worse confidence level case performance better confidence level 
wrapper algorithm significantly better plain real table comparison naive bayes nb feature selection filter rlf wrapper backward best search compound operators bfs 
val columns indicates probability top algorithm improving lower algorithm 
nb rlf nb bfs nb bfs dataset nb nb rlf nb bfs vs nb vs nb vs nb rlf breast cancer sigma sigma sigma cleve sigma sigma sigma crx sigma sigma sigma dna sigma sigma sigma horse colic sigma sigma sigma pima sigma sigma sigma sick euthyroid sigma sigma sigma soybean large sigma sigma sigma corral sigma sigma sigma sigma sigma sigma monk sigma sigma sigma monk local sigma sigma sigma monk sigma sigma sigma monk sigma sigma sigma average real average artif 
databases artificial databases significantly worse 
note significant improvement real database real dataset features dna 
relieved outperformed wrapper significantly real datasets outperformed wrapper dataset 
corral dataset wrapper selected correct features fa best node early search settled features gave better cross validation accuracy 
training set small instances problem wrapper gave ideal feature set built correct tree accurate pruned back pruning criterion training set data insufficient warrant large tree 
surprisingly naive bayes algorithm turned difficult improve feature selection table 
filter wrapper approaches significantly degraded performance breast cancer crx databases 
cases wrapper approach chose feature subsets high estimated accuracy turned poor performers real test data 
filter caused significantly worse performance dataset pima wrappers diabetes significantly improved plain naive bayes artificial datasets 
partly due fact severely restricted hypothesis space naive bayes prevents doing artificial problems monk partly naive bayes accuracy hurt conditional dependence features presence irrelevant features 
contrast wrapper approach significantly improved performance databases plain naive bayes accuracy 
monk dataset discarding features 
conditional independence assumption violated obtains better performance naive bayes throwing features marginal probability distribution classes predict majority class 
wrapper approach significantly improved filter cases significantly outperformed filter approach 
results id detailed kohavi john 
filter approach significantly degraded performance real dataset significantly improved artificial datasets monk wrapper approach 
focused accuracy criteria merit consideration 
wrapper method extends directly minimizing misclassification cost 
irvine datasets include cost information accuracy natural performance metric trivially cost function accuracy evaluation function wrapper 
filter approaches adapting misclassification costs research topic 
second compare number features selected filter wrapper 
table shows number features dataset number selected relieved filter note filter independent induction algorithm prescribes set features id naive bayes number selected plain versions algorithms wrapper enhanced versions 
plain naive bayes uses features column 
average reduction column shows wrapper reduces number features significantly relieved 
summary feature subset selection wrapper approach significantly improves id naive bayes datasets tested 
real datasets wrapper approach clearly superior filter method 
surprising result naive bayes performs real datasets discretization feature subset selection performed 
explanations apparently high accuracy naive bayes independence assumptions violated explained domingos pazzani 
see real world domains dna feature selection step important improve performance 
table number features dataset number selected relieved number plain versions algorithms number wrapped versions backward best search compound operators bfs 
dataset rlf bfs nb bfs id id bfs breast cancer cleve crx dna horse colic pima sick euthyroid soybean large corral monk monk local monk monk average reduction vs vs vs rlf vs rlf vs vs id overfitting induction algorithm overfits dataset models training data predictions poor 
example specialized hypothesis classifier lookup table features 
overfitting closely related bias variance tradeoff kohavi wolpert geman breiman algorithm fits data variance term large error increased 
accuracy estimation methods including cross validation evaluate predictive power hypothesis feature subset setting aside instances holdout sets shown induction algorithm assess predictive ability induced hypothesis 
search algorithm explores large portion space guided accuracy estimates choose bad feature subset subset high accuracy estimate poor predictive power 
overuse accuracy estimates feature subset selection may cause overfitting feature subset space 
feature subsets leads hypothesis high predictive wrappers accuracy holdout sets 
example overfitting shown information dataset rand features label completely random 
small sample instances estimated accuracy rose optimistic node evaluations indicative overfitting 
theoretical problem exists experiments wrapper approach indicate overfitting mainly problem number instances small kohavi sommerfield 
estimates biased algorithm may choose correct feature subsets relative accuracy matters 
related pattern recognition statistics literature offers filter approaches feature subset selection devijver kittler 
sequential backward elimination introduced marill green 
machine learning induction algorithms obey monotonic restrictions underlie early statistics pattern recognition applied databases large number features require special heuristic methods 
feature selection machine learning community includes langley reviews feature subset selection methods machine learning contrasted wrapper filter approaches 
new filter approach koller sahami cross entropy practice 
turney defines primary contextual features related different ideas strong weak relevance 
idea wrapping induction algorithms appeared times literature explicit name wrapper approach 
closest formulation search bias space approach described provost buchanan 
moore lee describe algorithm feature subset selection uses search method motivated genetic algorithms leave cross validation 
clever trick fully evaluating node search space perform partial evaluations frontier nodes parallel race node clear winner 
wrapper approach john authors experimented various contexts 
langley sage wrapper approach select features naive bayes 
pazzani wrapper approach select features join features create compound naive bayes showed finds correct combinations features interact 
singh provan wrapper approach select features bayesian networks showed significant improvements original algorithm 
kohavi john describe wrapper search methods search probabilistic estimates 
wrapper idea contexts addition feature selection 
applied wrapper approach parameter tuning specifically setting parameters maximal perfor mance kohavi john 
brunk kelly kohavi describe commercial data mining system includes wrapper algorithm feature subset selection parameter tuning 
atkeson cross validation search multidimensional real valued space includes feature weights addition parameters local learning similar generalized memory learning approach moore 
skalak uses wrapper approach selecting prototype subset nearest neighbor addition feature selection interesting example choosing training instances opposed features 
consider wrapper methods larger class methods involve running induction algorithm multiple times get better results 
class include ensemble methods bagging boosting stacking cf 
dietterich 
question arises amount computation better ensemble method wrapper feature subset selection 
generally ensembles produce high accuracy models complex single model produced run inducer 
contrast model produced running inducer feature subset selected wrapper usually simpler model produced inducer subset selection 
interpretability final model important wrapper subset selection better choice 
variations extensions current possible 
previous papers investigated hill climbing best search engines starting full empty subset 
search methods initial states lead better candidate subsets 
algorithm described explores general area search space heavily 
worthwhile introduce diversity search restarting random points starting exploring search space fully magnify problems overfitting discussed 
strategies evaluating promising states fully doing extra cross validation runs possibly accuracy estimation method improve results significantly 
extensions considering nodes search space evaluating candidate nodes fully obviously increase running time wrapper slow area runtime performance enhancements 
method similar moore lee races allow wrapper waste time evaluating unpromising nodes 
larger datasets possible cheaper accuracy estimation methods holdout decrease number folds 
continue full cross validation candidate subset inducers allow incremental addition deletion instances leading possibility doing incremental cross validation suggested kohavi drastically reducing running time 
wrapper approach easy par wrappers 
node expansion children evaluated parallel cut running time factor bounded number attributes assuming processors available 
described feature subset selection problem supervised learning involves identifying relevant useful features dataset giving subset learning algorithm 
investigated relevance irrelevance features defined degrees relevance weak strong 
shown definitions mainly useful respect bayes optimal rule practice look optimal features respect specific learning algorithm training set hand 
optimal features necessarily correspond relevant features weak strong 
optimal features depend specific biases heuristics learning algorithm wrapper approach naturally fits definition 
feature relevance helped motivate compound operators practice currently practical way conduct backward searches feature subsets wrapper approach datasets features 
compared relieved filter algorithm wrapper different families induction algorithms decision trees naive bayes 
significant performance improvement achieved datasets 
dna dataset wrapper approach naive bayes reduced error rate relative error reduction making best induction algorithm problem methods statlog experiments taylor 
surprising results naive bayes performed naive bayes outperforms feature selection real datasets 
average performance feature subset selection improved algorithms 
naive bayes relieved degraded average accuracy real datasets wrapper improved accuracy 
feature selection algorithms improved average accuracy artificial datasets 
wrapper algorithm accuracy significantly higher relieved cases significantly worse 
relieved significantly improved plain inducers cases significantly worse 
wrapper significantly better plain inducers cases worse 
wrapper reduced number features significantly filter 
results support contention subset selection improve accuracy wrapper method preferable filter 
shown problems wrapper approach overfitting large amounts cpu time required 
acknowledgments karl pfleger contributed wrapper 
chapter incorporates number corrections improvements suggested readers earlier publications wrapper method especially pat langley nick littlestone nils nilsson peter turney 
dan sommerfield implemented large parts wrapper mlc kohavi experiments 
george john supported national science foundation graduate research fellowship 
research completed authors stanford university 
aha kibler albert 

instance learning algorithms 
machine learning 
almuallim dietterich 

learning irrelevant features 
ninth national conference artificial intelligence pages 
mit press 
almuallim dietterich 

learning boolean concepts presence irrelevant features 
artificial intelligence 
atkeson 

locally weighted regression robot learning 
proceedings ieee international conference robotics automation pages 
blum rivest 

training node neural network np complete 
neural networks 
boddy dean 

solving time dependent planning problems 
sridharan editor proceedings eleventh international joint conference artificial intelligence volume pages 
morgan kaufmann publishers breiman friedman olshen stone 

classification regression trees 
wadsworth international group 
brunk kelly kohavi 

integrated system data mining 
heckerman mannila pregibon uthurusamy editors proceedings third international conference knowledge discovery data mining pages 
aaai press 
www sgi com products software 
cardie 

decision trees improve case learning 
proceedings tenth international conference machine learning pages 
morgan kaufmann publishers devijver kittler 

pattern recognition statistical approach 
prentice hall international 
domingos pazzani 

independence conditions optimality simple bayesian classifier 
machine learning 
duda hart 

pattern classification scene analysis 
wiley 
wrappers geman bienenstock doursat 

neural networks bias variance dilemma 
neural computation 
ginsberg 

essentials artificial intelligence 
morgan kaufmann 


estimation probabilities essay modern bayesian methods 
press 
rivest 

constructing optimal binary decision trees np complete 
information processing letters 
john kohavi pfleger 

irrelevant features subset selection problem 
machine learning proceedings eleventh international conference pages 
morgan kaufmann 
john 

enhancements data mining process 
phd thesis stanford university computer science department 
kira rendell 

practical approach feature selection 
proceedings ninth international conference machine learning 
morgan kaufmann 
kohavi 

power decision tables 
lavrac wrobel editors proceedings european conference machine learning lecture notes artificial intelligence pages berlin heidelberg new york 
springer verlag 
kohavi 

study cross validation bootstrap accuracy estimation model selection 
mellish editor proceedings th international joint conference artificial intelligence pages 
morgan kaufmann 
kohavi john 

automatic parameter selection minimizing estimated error 
prieditis russell editors machine learning proceedings twelfth international conference pages 
morgan kaufmann 
kohavi john 

wrappers feature subset selection 
artificial intelligence 
kohavi sommerfield 

feature subset selection wrapper model overfitting dynamic search space topology 
international conference knowledge discovery data mining pages 
kohavi sommerfield dougherty 

data mining mlc machine learning library tools artificial intelligence pages 
ieee computer society press 
received best award 
www sgi com technology mlc 
kohavi wolpert 

bias plus variance decomposition zero loss functions 
saitta editor machine learning proceedings thirteenth international conference pages 
morgan kaufmann 
available robotics stanford edu users ronnyk 
koller sahami 

optimal feature selection 
proceedings thirteenth international conference machine learning pages 
morgan kaufmann publishers kononenko 

estimating attributes analysis extensions relief 
bergadano raedt editors proceedings european conference machine learning 
langley 

selection relevant features machine learning 
aaai fall symposium relevance pages 
langley sage 

induction selective bayesian classifiers 
proceedings tenth conference uncertainty artificial intelligence pages seattle wa 
morgan kaufmann 
mallows 

comments technometrics 
marill green 

effectiveness receptors recognition systems 
ieee transactions information theory 
moore hill johnson 

empirical investigation brute force choose features smoothers function approximators 
computational learning theory natural learning systems conference 
moore lee 

efficient algorithms minimizing cross validation error 
cohen hirsh editors machine learning proceedings eleventh international conference 
morgan kaufmann publishers wasserman 

applied linear statistical models 
irwin il rd edition 
pazzani 

searching dependencies bayesian classifiers 
fisher lenz editors proceedings fifth international workshop artificial intelligence statistics ft lauderdale fl 
provost buchanan 

inductive policy pragmatics bias selection 
machine learning 
quinlan 

programs machine learning 
morgan kaufmann san mateo california 
rosenblatt 

perceptron probabilistic model information storage organization brain 
psychological review 
russell norvig 

artificial intelligence modern approach 
prentice hall englewood cliffs new jersey 
singh provan 

comparison induction algorithms selective non selective bayesian classifiers 
machine learning proceedings twelfth international conference pages 
skalak 

prototype feature selection sampling random mutation hill climbing algorithms 
cohen hirsh editors machine learning proceedings eleventh international conference 
morgan kaufmann publishers taylor michie 

machine learning neural statistical classification 
paramount publishing international 
turney 

identification context sensitive features formal definition context concept learning 
kubat widmer editors proceedings workshop learning context sensitive domains pages 
available national research council canada technical report 
contributing authors george john data mining guru marketing software developing third generation data mining technology applications 
earned ph distinction computer science department stanford university research supported national science foundation fellowship 
dissertation enhancements data mining process available xenon stanford edu 
