modelling sub phone insertions deletions continuous speech recognition hain woodland cambridge university engineering department street cambridge cb pz uk 
fth eng cam ac uk extension standard hidden markov models speech recognition called hidden model sequence hms modelling introduced 
approach relationship phones pronunciation dictionary hmms model context assumed stochastic 
important feature hms framework ability handle arbitrary model phone sequence alignments 
try exploit capability di erent methods model sub phone insertions deletions 
experiments resource management rm corpus subset switchboard corpus show relative standard hmm baseline reduction word error rate wer relative obtained rm absolute switchboard 

dominant techniques acoustic modelling speech recognition hidden markov models hmms 
number advances hmms years simple left right chain topology models recognition systems 
pearls string modelling approach widely believed suboptimal complex schemes rarely state art systems 
standard hmm recognition system phone context uniquely associated particular hmm 
commonly associated hmm phonetic decision trees local phonetic context 
decision trees usually built approximation models single gaussian distribution state targeted mixture gaussian distributions 
decision tree technique allows particular model states complete models variety phone contexts sets shared states context ll predetermined model topology 
common topology state left right model self transitions transitions state skip transitions 
pronunciation variants pronunciation networks additional methods model selection 
pronunciation modelling considered particularly important spontaneous speech variability pronunciation stress speaking rate considerably greater read speech 
phone level transcriptions spontaneous speech show signi cant di erences canonical pronunciations dictionary actual realisation 
di erences described substitutions deletions insertions appear play fairly minor role 
explicit modelling variations pronunciation networks brought moderate improvements 
furthermore analysis spontaneous speech data 
showed changes canonical dictionary forms surface forms occur sub phone level 
previously introduced hidden model sequence hms modelling framework 
unique mapping dictionary phones hmms replaced stochastic mapping 
stochastic mapping uses additional model sequence model msm approach model sequence particular pronunciation sequence hidden 
appropriate hmm dependent particular acoustics determined priori decision trees 
straight forward hms implementation 
constrained exactly hmm dictionary phone aimed better handling phone variability 
hms hmms allow insertions deletions hmm states variation usual gram approach general multigram method 
map dictionary phone sequence arbitrary length hmm sequence 
rest organised follows 
section brie explains framework theory implementational issues 
section discusses approaches modelling sub phone insertions deletions explains strategies investigated 
followed experimental evaluation techniques resource management switchboard corpora 

hidden model sequence modelling standard training procedure hmms speech recognition maximises likelihood acoustic feature sequence transcriptions training data 
cases dictionary translate word sequence state level hms hmms hmms dictionary phone 
phone transcription sequence turn modelled deterministic sequence context dependent hmms 
knowledge equivalent knowing hmm sequence 
contrast hms hmms assume stochastic mapping jr jr additional stochastic layer jr model sequence model msm sum taken possible hmm sequences particular phone sequence 

assumed probability stochastic sequence depends sequence level sequences higher hierarchy 
note assumption length alignment sequences 
value obtained standard hmm set 
shown model jr de ned algorithm locally maximise likelihood 
viterbi approximation model sequence level allows independent optimisation hmm msm parts greatly simpli es implementation 
experiments 
sequences assumed aligned obvious realisation msm gram model 
triphone model equivalent formulation dependence immediate left right phonetic context jr jr denotes number symbol pairs denote individual models phones respectively 
phone set potential hmms exists 
models parallel constitute new phone model model distribution gives probability model particular phone context 
scheme allows modelling phone sub phone substitutions 
deal unseen events training standard discounting schemes katz backo interpolated left right context independent phone distribution 
witten bell discounting 
give superior performance schemes hms hmm experiments reported 
hms hmms implemented phone state level 
model distributions denoted 
dependent position phone 
practice large number parallel models computationally expensive 
distributions models certain phone context pruned retain probability mass similar way language models recognition msm scores scaled log domain experimentally scale factor 
hms hmms insertion deletion modelling initialised standard phonetic decision tree clustered hmms set possible models particular phone derived stripping triphone context standard hmms 
skip hms hmm topology state level state insertions deletions 

modelling sub phone insertions deletions hms framework possibilities available model insertions deletions sub phone level 
option assume xed phone context model alignment 
refer hmms purely symbolic way underlying topology hmm arbitrary 
model may multiple states may contain emitting state case model referred skip model 

shows example hms phone model insertions deletions 
objects boxes standard hmms links carry context dependent model distribution 
alignment remains xed training model construction schemes may kept identical ones described section 
models drawn existing hmm set done substitution modelling alternative methods initialisation required 

multigrams arbitrary phone model mappings implemented multigrams 
theory multigrams allows optimisation joint probability sequences 
theoretically arbitrary length subsequence string phone sequence produce arbitrary length string model sequence 
empty string permitted practically string lengths constrained 
set possible model strings particular phone string automatically training scheme segmentation sequences hidden parameter 
original multigram 
joint probability served objective function optimisation 
require estimate jr allows arbitrary length model strings represents case mappings 
order enable certain degree variation sequence alignment longest strings contain models 
multigrams ected data sparsity problems additional multiple phone strings infeasible 
multigram training requires optimisation jr hidden parameter represents particular segmentation model sequence string boundaries exactly length em algorithm implemented eciently forward backward scheme jr nm jr nr time indices notation denotes subsequence including boundary elements forward backward variables computed recursions jr jr denotes maximum number models joined stands phone plus context position triphone equivalent case triple 
reestimation probability particular model string length particular phone context indicator functions 
implementation practice phone model mapping relaxed gradually rst introducing skips insertions second stage 
rst set experiments scenario aimed multigrams straight forward fashion 
phone context large set model strings 
model strings consist single state models 
stage modelling skips implemented adding possible alternative models single state deleted 
steps standard gram msm second stage gram model replaced multigram model 
sharing states adjacent phone contexts allowed 
initial model strings scenario obtained state level hms hmm system 
state model triplets triphone context collected added new hms hmm set 
hms hmm operates phone level 
number triplets large sparsity problems require considerable amount pruning model distributions 
sparsity distributions major disadvantage stages scenario required hard limits placed number allowable alternative models training 
overcome diculties second approach scenario tried estimates model probabilities robust state level 
explicit skip hmm output distribution added list possible models phone context 
common initial value probability skip models contexts chosen 
iterations estimate probability skips phone context obtained 
di erent second stages tested 
case pairs models phone position added model distribution 
suciently modelled grams 
second case models adjacent phone positions particular phone added 
reestimation steps grams multigram msm maximum elements model string 

experiments insertion deletion experiments hms hmm model set initialisation initialised baseline hmm model set see 
details 
model sets particular corpus exactly number states mixture components state 
experiments conducted resource management rm switchboard 
rm chosen small amount training data single pronunciation dictionary standard hmm setup 
main focus lies modelling spontaneous speech 
experiments switchboard corpus hour training set switchboard data tests conducted minute switchboard test sets ws 
hmm baseline system states mixture components uses conversation side cepstral mean variance normalisation 
ws baseline hmm baseline hms hms model level max model deletions multigram table scenario wer switchboard various model level hms hmms 
results obtained rescoring trigram lattices 
due problems data sparsity experiments scenario conducted switchboard 
table 
shows word error rates test sets baseline hmm hms hmm systems various stages scenario seen step phone unit modelling brings performance degradation partly recovered setting hard limit maximum number models phone context max model 
addition deletion modelling brings minor improvements poorer wer hms hmm baseline 
multigrams nally brings slightly better performance baseline hms hmm system 
scenario tested rm switchboard 
speaker independent resource management corpus consists sentences training data sentences test data split feb oct feb sep evaluation sets 
standard rm model sets states mixture components 
dictionary contains words single pronunciations 
word pair grammar recognition 
table 
shows word error rates test sets 
baseline hms hmm system gave considerable improvement standard hmm models 
slight improvement added skip models 
data gain added insertions larger 
modelling sub phone insertions deletions phone position gave relative reduction word error rate hms baseline relative reduction hmm baseline 
skips brought absolute reduction word deletions substitutions increase insertions 
modelling insertions reduced number deletions substitution insertion errors remained virtually unchanged 
models adjacent phone positions gave word error rate poorer results extended multigrams 
potential reason overestimated skip probabilities 
system feb oct feb sep hmm hms hmm skips insertions table scenario wer rm state level hms hmms 
results obtained viterbi decoding single pronunciation dictionary grammar 
table 
shows scenario results switchboard 
di erence hms baseline compared table 
due silence modelling identical hmm baseline 
largest improvement wer absolute comes straight forward hms modelling 
contrast results rm wer reduction achieved skip models 
number word deletions remained approximately improvement stems reduced number word substitutions 
contrast rm insertions brought signi cant reduction word error rate considerable increase acoustic log likelihood observed 
probable reason msm distributions controlled stricter pruning 
improvement wer absolute hmm constitutes best result far 
similarly situation rm models adjacent positions multigrams gave considerably poorer performance 
system ws hmm hms hmm skips insertions table scenario wer switchboard obtained rescoring trigram lattices 

sub phone insertions deletions modelled hms framework 
substantial improvements gained hms hmms straightforward fashion context speci deletion modelling give consistent performance improvements 
major reasons broad model distributions obtained insertions especially switchboard data add confusion system 
area concentrate mechanisms enforce compact distributions 

part supported 


byrne finke khudanpur riley sara wooters 
pronunciation modelling hand labelled corpus conversational speech recognition 
proc 
icassp pp 
seattle 

deligne bimbot 
inference variablelength linguistic acoustic units multigrams 
speech communication vol 
pp 


greenberg switchboard transcription project 
lvcsr summer workshop technical reports 
www icsi berkeley edu real stp 

hain woodland 
dynamic hmm selection continuous speech recognition 
proc 
eurospeech pp 
budapest 

sara khudanpur 
pronunciation ambiguity vs pronunciation variability speech recognition 
proc 
icassp pp 
istanbul 

witten bell 
zero frequency problem estimating probabilities novel events adaptive text compression 
ieee trans 
information theory vol 
pp 


young odell woodland 
tree state tying high accuracy acoustic modelling 
proc 
arpa human language technology workshop pp 

morgan kaufmann 
