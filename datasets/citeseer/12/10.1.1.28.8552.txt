journal artificial intelligence research submitted published induction order decision lists results learning past tense english verbs raymond mooney mooney cs utexas edu mary elaine califf cs utexas edu department computer sciences university texas austin tx presents method inducing logic programs examples learns new class concepts called order decision lists defined ordered lists clauses cut 
method called foidl foil quinlan employs intensional background knowledge avoids need explicit negative examples 
particularly useful problems involve rules specific exceptions learning past tense english verbs task widely studied context symbolic connectionist debate 
foidl able learn concise accurate programs problem significantly fewer examples previous methods connectionist symbolic 

inductive logic programming ilp growing subtopic machine learning studies induction prolog programs examples presence background knowledge muggleton lavrac dzeroski :10.1.1.35.951
due expressiveness order logic ilp methods learn relational recursive concepts represented attribute value representations assumed machine learning algorithms 
ilp methods successfully induced small programs sorting list manipulation shapiro sammut banerji muggleton buntine quinlan cameron jones produced encouraging results important applications predicting protein secondary structure muggleton king sternberg automating construction natural language parsers zelle mooney 
current ilp techniques important assumptions restrict application 
common assumptions 
background knowledge provided extensional form set ground literals 

explicit negative examples target predicate available 

target program expressed pure prolog clause order irrelevant procedural operators cut 
disallowed 
currently known successful ilp systems golem muggleton feng foil quinlan assumptions 
assumptions brings significant limitations 
adequate extensional representation background knowledge frequently infinite intractably large 
fl ai access foundation morgan kaufmann publishers 
rights reserved 
mooney califf 
explicit negative examples frequently unavailable adequate set negative examples computed closed world assumption infinite intractably large 

concise representation concepts requires clause ordering cuts bergadano 
presents new ilp method called foidl order induction decision lists helps overcome limitations incorporating properties 
background knowledge represented intensionally logic program 

explicit negative examples need supplied constructed 
assumption output completeness implicitly determine hypothesized clause overly general quantify degree generality simply estimating number negative examples covered 

learned program represented order decision list ordered set clauses cut 
representation useful problems best represented general rules specific exceptions 
name implies foidl closely related foil follows similar top greedy specialization guided information gain heuristic 
algorithm substantially modified address advantages listed 
intensional background knowledge fairly straightforward incorporated previous foil derivatives lavrac dzeroski pazzani kibler zelle mooney development foidl motivated failure observed applying existing ilp methods particular problem learning past tense english verbs 
problem studied fairly extensively connectionist symbolic methods rumelhart mcclelland macwhinney ling previous efforts specially designed feature encodings impose fixed limit length words fail capture position independence underlying transformation 
believed representing problem constructing logic program predicate past words represented lists letters past past past produce better results 
due limitations mentioned unable get reasonable results foil golem 
overcoming limitations foidl able learn highly accurate programs past tense problem fewer examples required previous methods 
remainder organized follows 
section provides important background material foil past tense learning problem 
section presents foidl algorithm details incorporates advantages discussed 
section presents results learning past tense english verbs demonstrating foidl performs previous methods problem 
section reviews related section discusses limitations directions section summarizes presents 
induction order decision lists learning english past tense 
background foidl foil section presents brief review important ilp system quinlan quinlan cameron jones cameron jones quinlan provide complete description 
section presents brief review previous english past tense problem 
foil foil learns function free order horn clause definition target predicate terms background predicates 
input consists extensional definitions predicates tuples constants specified types 
example input appropriate learning definition list membership member elt lst components lst elt lst 
elt type denoting possible elements includes lst type defined consisting flat lists containing elements components background predicate true iff list element rest list provided place function list construction 
foil requires negative examples target concept supplied directly computed closed world assumption 
example closed world assumption produce pairs form elt lst 
explicitly provided positive examples 
input foil learns program clause time greedy covering algorithm summarized follows positives cover positive examples 
positives cover empty find clause covers preferably large subset positives cover covers negative examples 
add developing definition 
remove examples covered positives cover 
example clause learned member iteration loop member components 
covers positive examples element list cover negatives 
clause learned cover remaining examples member components member 
clauses constitute correct program member 
find clause step implemented general specific hill climbing search adds antecedents developing clause time 
step evaluates possible literals added selects maximizes information gain heuristic 
algorithm maintains set tuples satisfy current clause includes bindings new variables introduced body 
pseudocode summarizes procedure mooney califf initialize 
target predicate arity initialize contain positive tuples positives cover negative tuples 
contains negative tuples find best literal add clause 
form new training set containing tuple satisfies tuples form delta concatenated set bindings new variables introduced literal satisfied matches tuple extensional definition predicate 
replace foil considers adding literals possible predicate long type restrictions satisfied arguments existing variable bound head previous literal body 
literals evaluated number positive negative tuples covered preferring literals cover positives negatives 
denote number positive tuples set define gamma log jt chosen literal maximizes gain delta gamma number tuples extensions number current positive tuples covered 
foil includes additional features heuristics pruning space literals searched methods including equality negation failure useful literals immediately provide gain determinate literals pre pruning post pruning clauses prevent fitting methods ensuring induced programs terminate 
papers referenced consulted details features 
learning past tense english verbs rumelhart mcclelland build computational model learning classic perceptron algorithm special phonemic encoding words employing called 
general goal show connectionist models account interesting language learning behavior previously thought require explicit rules 
model heavily criticized opponents connectionist approach language acquisition relatively poor results achieved heavily engineered representations training techniques employed pinker prince 
macwhinney attempted address criticisms standard multi layer backpropagation learning algorithm simpler encoding phonemes phonemes encoded single ascii character 
ling marinov ling criticize current connectionist models past tense acquisition heavily engineered representations poor experimental methodology 
systematic results system called spa symbolic pattern associator uses slightly modified version quinlan build induction order decision lists learning english past tense forest decision trees maps fixed length input pattern fixed length output pattern :10.1.1.34.5768
ling head head results show spa generalizes significantly better backpropagation number variations problem employing different phonemic encodings vs training examples 
previous encodes problem fixed length pattern association fails capture generativity position independence true transformation 
example letter patterns 
phonemic encoding 
separate decision tree output unit predict character output pattern input characters 
learning general rules add ed repeated position word words longer characters handled 
best results spa exploit highly engineered feature template modified version default leaf labeling strategy tailor string transformation problems 
ilp methods appropriate problem initial attempts apply foil golem past tense learning gave disappointing results califf 
discuss problems listed contribute difficulty applying current ilp methods problem 
principle background predicate append sufficient constructing accurate past tense programs incorporated ability include constants arguments equivalently ability add literals bind variables specific constants called theory constants foil 
background predicate allow appending empty list appropriate 
predicate called split splits list non empty sublists intensional definition split split 
split split 
split add ed rule represented past split 
foil learned form past split 
providing extensional definition split includes possible strings fewer characters strings clearly intractable 
providing partial definition includes possible splits strings appear training corpus possible generally sufficient 
providing adequate extensional background knowledge cumbersome requires careful engineering major problem 
supplying appropriate set negative examples problematic 
assumption produce pairs words training set second past tense feasible useful 
case clause mooney califf past split 
learned covers positives negatives word prefix word past tense 
clause useless producing past tense novel verbs domain accuracy measured ability generate correct output novel inputs ability classify pre supplied tuples arguments positive negative 
obvious solution supplying strings characters negative examples past tense word clearly intractable 
providing specially constructed near negative examples past helpful requires careful engineering exploits detailed prior knowledge problem 
order address problem negative examples quinlan applied foil problem employed different target predicate representing transformation 
place predicate past true iff input word transformed past tense form removing current substituting example past past 
simple preprocessor map data place predicate form 
sample verb pairs contains different fragments results manageable number closed world negatives approximately positive example training set 
approach phonemic encodings quinlan obtained slightly better results ling best spa results exploited highly engineered feature template vs training examples significantly better spa normal results 
place target predicate incorporates knowledge desired transformation arguably requires representation engineering previous methods 
quinlan notes results hampered foil inability exploit clause order 
example normal alphabetic encoding foil quickly learns clause sufficient regular verbs past 
clause covers fair number negative examples due irregular verbs continues add literals 
result foil creates number specialized versions clause fail capture generality underlying default rule 
problem compounded foil inability add constraints foil separates addition literals containing variables binding variables constants literals form learn clauses past split 
word split ways clearly equivalent learnable clause past split 

quinlan problem motivated early attempts foil 
induction order decision lists learning english past tense consequently approximate true rule learning clauses form past split 
past split 
result foil generated overly complex programs containing clauses phonemic alphabetic versions problem 
experienced prolog programmer exploit clause order cuts write concise program handles specific exceptions falls general default rules exceptions fail apply 
example program past split split 
past split split 
past split split 
past split 
summarized word ends eep replace eep sleep word ends replace ied word ends add add ed 
foidl directly learn programs form ordered sets clauses cut 
call programs order decision lists due similarity propositional decision lists introduced rivest 
foidl uses normal binary target predicate requires explicit negative examples 
believe requires significantly representation engineering previous area 

foidl induction algorithm stated foidl adds major features foil intensional specification background knowledge output completeness substitute explicit negative examples support learning order decision lists 
subsections describe modifications incorporate features 
intensional background described foil assumes background predicates provided extensional definitions burdensome frequently intractable 
providing intensional definition form general prolog clauses generally preferable 
example providing numerous tuples components predicates easier give intensional definition components 
intentional background definitions restricted function free pure prolog exploit features language 
mooney califf modifying foil intensional background straightforward 
matching literal set tuples determine covers example prolog interpreter attempt prove literal satisfied intensional definitions 
foil expanded tuples maintained positive negative examples target concept alternative specialization developing clause 
pseudocode learning clause simply initialize 
target predicate arity initialize contain examples positives cover negative examples 
contains negative tuples find best literal add clause 
subset examples proved instances target concept specialized clause 
replace expanded tuples produced information gain heuristic picking best literal simply gain jt delta gamma output completeness implicit negatives order overcome need explicit negative examples mode declaration target concept provided specification argument input output 
assumption output completeness indicating unique input pattern training set training set includes correct output patterns 
output program produces input assumed represent negative example 
require positive examples part training set unique input pattern training set positive examples input pattern training set 
assumption trivially met predicate represents function single unique output input 
example assumption output completeness mode declaration past indicates correct past tense forms included input word training set 
predicates representing functions past implies output example unique outputs implicitly represent negative examples 
output completeness applied non functional cases append indicating possible pairs lists appended produce list included training set append append append 
output completeness assumption determining clause overly general straightforward 
positive example output query determine outputs input past 
outputs generated positive examples clause covers negative examples requires specialization 
note intensional interpretation learned clauses required order answer output queries 
addition order compute gain alternative literals specialization negative coverage clause needs quantified 
incorrect answer output induction order decision lists learning english past tense query ground contains variables clearly counts single negative example past 
output queries frequently produce answers universally quantified variables 
example overly general clause past split query past generates answer past 
implicitly represents coverage infinite number negative examples 
order quantify negative coverage foidl uses parameter represent bound number possible terms 
set possible terms herbrand universe background knowledge examples generally infinite meant represent heuristic estimate finite number terms occur practice number distinct words english 
negative coverage represented non ground answer output query estimated gamma number variable arguments answer number positive examples answer unifies 
term stands number unique ground outputs represented answer answer append stands different ground outputs term stands number represent positive examples 
allows foidl quantify coverage large numbers implicit negative examples explicitly constructing 
generally sufficient estimate fairly large constant empirically method sensitive exact value long significantly greater number ground outputs generated clause 
unfortunately estimate sensitive 
example clauses past split 
past split 
cover implicit negative examples output query past produces answer past second produces answer past 
second clause clearly better requires output input suffix added 
presumably words words start assuming total number words finite clause considered cover negative examples 
arguments partially instantiated counted fraction variable calculating specifically partially instantiated output argument scored fraction subterms variables counts variable argument 
clause scored covering implicit negatives second covering reasonable values number positives covered clause literal split preferred 
revised specialization algorithm incorporates implicit negatives initialize 
target predicate arity initialize contain examples positives cover output queries positive examples 
contains output queries find best literal add clause 
subset positive examples proved instances target concept specialized clause plus output queries mooney califf produce incorrect answers 
replace literals scored described previous section jt computed number positive examples plus sum number implicit negatives covered output query order decision lists described order decision lists ordered sets clauses cut 
answering output query cuts simply eliminate answer produced trying clauses order 
representation similar propositional decision lists rivest ordered lists pairs rules form test conjunction features category label example assigned category pair test satisfies 
original algorithm rivest cn clark niblett rules learned order appear final decision list new rules appended list learned 
webb argue learning decision lists reverse order preference functions tend learn general rules best positioned default cases 
introduce algorithm prepend learns decision lists reverse order results indicating cases learns simpler decision lists superior predictive accuracy 
foidl seen generalizing prepend order case target predicates representing functions 
learns ordered sequence clauses reverse order resulting program produces output generated satisfied clause 
basic operation algorithm best illustrated concrete example 
alphabetic past tense current algorithm easily learns partial clause past split 
discussed section clause covers negative examples due irregular verbs 
produces correct ground output subset examples regular verbs 
indication best terminate clause handle examples add earlier clauses decision list handle remaining examples 
fact produces incorrect answers output queries safely ignored decision list framework handled earlier clauses 
examples correctly covered clause removed positives cover new clause begun 
literals provide best gain past split 
just add 
clause produces correct ground output subset examples complete produces incorrect output examples correctly covered previously learned clause past 
specialization continues cases eliminated 
results clause 
note untrue literals added initially empty clause 
induction order decision lists learning english past tense past split split 
added front decision list examples covers removed positives cover 
approach ensures new clause produces correct outputs new subset examples doesn result incorrect output examples correctly covered previously learned clauses 
process continues adding clauses front decision list exceptions handled positives cover empty 
resulting clause specialization algorithm summarized follows initialize 
target predicate arity initialize contain examples positives cover output queries positive examples 
contains output queries find best literal add clause 
subset positive examples output query produces answer unifies correct answer plus output queries produce non ground answer unifies correct answer produce incorrect answer produce correct answer previously learned clause 
replace cases algorithm able learn accurate compact order decision lists past tense expert program shown section 
due highly irregular verbs algorithm encounter local minima unable find literals provide positive gain covering required minimum number examples 
originally handled terminating search memorizing remaining uncovered examples specific exceptions top decision list past 
result premature termination prevents algorithm finding low frequency regularities 
example alphabetic version system get stuck trying learn complex rule double final consonant grab grabbed fail learn rule changing ied frequent 
current version foil tests learned clause meets minimum accuracy threshold foil counting errors incorrect outputs queries correctly answered previously learned clauses 
meet threshold clause thrown positive examples covers memorized top decision list 
algorithm continues learn clauses remaining positive examples 
allows foidl just memorize difficult irregularities consonant doubling continue learn rules changing ied 
minimum accuracy threshold met decision list property exploited final attempt learn completely accurate program 
negatives covered clause examples correctly covered previously learned clauses foidl 
foil foidl includes parameter minimum number examples clause cover normally set 
mooney califf treats exceptions exception rule returns positives covered correctly subsequently learned clauses 
example foidl frequently learns clause past split split 
changing ied 
clause incorrectly covers examples correctly covered previously learned add ed rule bay delay delayed 
exceptions ied rule small percentage words system keeps rule returns examples just add ed positives cover 
subsequently rules past split split 
learned recover examples resulting program completely consistent training data 
setting minimum clause accuracy threshold foidl applies uncovering technique results covering examples uncovers guaranteeing progress fitting training examples 
algorithmic implementation details section briefly discusses additional details foidl algorithm implementation 
includes discussion modes types weak literals theory constants 
current version foil includes features basically form 
foidl types modes limit space literals searched 
argument predicate typed literals previously bound arguments correct type tested specializing clause 
example split types split word prefix suffix preventing system splitting prefixes suffixes exploring arbitrary substrings word regularities 
predicate mode declaration literals input arguments variables tested 
example split mode split preventing clause creating new strings appending previously generated prefixes suffixes 
case literal provides positive information gain foidl gives small bonus literals introduce new variables 
number weak literals added row limited user parameter normally set 
example allows system split word possible prefixes suffixes may provide gain substrings constrained subsequent literals 
theory constants provided type literals tested binding existing variable constant appropriate type 
example literal generated type suffix 
runs past tense theory constants included prefix suffix occurs words training data 
helps control training time limiting number literals searched affect literals chosen minimum clause coverage test prevents foidl choosing literals don cover examples anyway 
induction order decision lists learning english past tense foidl currently implemented common lisp quintus prolog 
current prolog version common lisp version supports learning recursive clauses output completeness non functional target predicates 
common lisp version significantly slower relies un optimized prolog interpreter compiler written lisp norvig 
consequently results prolog version running sun sparcstation 

experimental results test foidl performance english past tense task ran experiments data ling available appendix 
experimental design data consist english verb forms normal alphabetic form phoneme representation label indicating verb form base past tense past participle label indicating form regular irregular francis kucera frequency verb 
data include distinct pairs base past tense verb forms 
ran different experiments 
phonetic forms verbs 
second phonetic forms regular verbs easiest form task problem ling provides learning curves 
ran trials alphabetic forms verbs 
training testing followed standard paradigm splitting data testing training sets training progressively larger samples training set 
results averaged trials testing set trial contained verbs 
order better separate contribution implicit negatives contribution decision list representation ran experiments variant system uses intensional background output completeness assumption build decision lists 
ran experiments foil foidl compared results ling 
foil experiments run quinlan representation described section 
quinlan negative examples provided generated closed world assumption 
experiments foidl standard default values various numeric parameters term universe size minimum clause coverage weak literal limit 
differences foil foidl tested significance paired test 

handling intensional interpretation recursive clauses target predicate requires additional complexities discussed relevant decision lists generally recursive 

versions available anonymous ftp net cs utexas edu directory pub mooney foidl 

replicated quinlan approach memory limitations prevented generated negatives larger training sets 
mooney califf training examples foidl foil spa neural network accuracy phonetic past tense task verbs results results phonetic task regular irregular verbs 
graph shows results foil foidl best results ling provide learning curve task 
expected foidl performed systems task surpassing ling best results examples examples 
performed quite poorly barely beating neural network results despite effectively having negatives opposed foil 
poor performance due part overfitting training data lacks noise handling techniques foil 
foil advantage place predicate gives bias learning suffixes 
poor performance task shows implicit negatives sufficient bias decision lists place predicate noise handling needed 
differences foil foidl significant level 
foidl significant level 
differences foil significant training examples significant level examples 
presents accuracy results phonetic task 
curves spa neural net results reported ling 
foidl outperformed systems 
particular task demonstrated problems closed world negatives 
regular past tense task second argument quinlan place predicate empty list 
constants generated positive examples foil produce rules ground second argument create negative examples constants second argument 
prevents system learning rule generate past tense 
order induction order decision lists learning english past tense training examples foidl foil spa neural network accuracy phonetic past tense task obtain results reported introduced extra constants second argument specifically constants third argument enabling closed world assumption generate appropriate negatives 
task gain advantage foil able effectively negatives 
regularity data allows foil achieve accuracy examples 
differences foil foidl significant level foidl 
differences foil significant examples significant level examples significant level training examples 
results alphabetic version appear 
task typically considered literature interest concerned incorporating morphology natural language understanding systems deal text 
difficult task primarily consonant doubling 
results foidl foil 
alphabetic task irregular full phonetic task overfits data performs quite poorly 
differences foil foidl significant level examples level examples 
differences foidl significant level 
foil significant training examples significant level training examples significant level examples 
tasks foidl clearly outperforms systems demonstrating order decision list bias learning task 
sufficient set negatives necessary systems provide way neural network spa learn multiple class classification tasks phoneme belongs position foil uses place predicate closed world negatives mooney califf training examples foidl foil accuracy alphabetic past tense task foidl course output completeness assumption 
primary importance implicit negatives provide advantage propositional neural network systems enable order systems perform task 
knowledge task required 
foidl decision lists give significant added advantage advantage apparent regular phonetic task exceptions 
clearly foidl produces accurate rules systems consideration complexity rule sets 
ilp systems measures complexity number rules number literals generated 
shows number rules generated foil foidl phonetic task verbs 
number literals generated appears 
interested generalization foil attempt fit training data results include rules foidl add order memorize individual exceptions 
numbers comparable examples increasing numbers examples programs foil generate grow faster foidl programs 
large number rules literals learned show tendency overfit data 
foidl generates comprehensible programs 
example program generated alphabetic version task examples excluding memorized examples 
past split split 
past split split split 
past split split split 

large number irregular english foidl memorizes average verbs trial examples 
induction order decision lists learning english past tense training examples foidl foil number rules created phonetic past tense task training examples foidl foil number literals created phonetic past tense task mooney califf past split split split 
past split split 
past split split 
past split 
training times various systems considered research difficult compare 
ling provide timing results probably assume research comparing symbolic neural learning algorithms shavlik mooney towell spa runs fairly quickly backpropagation took considerably longer 
tests foil foidl directly comparable run different architectures 
foil runs done sparc 
examples foil averaged minutes phonetic task verbs 
foidl experiments ran sparc averaged minutes task 
allowing differences speed machines factor foidl quite bit slower probably due largely cost intentional background part implementation prolog opposed 
related related ilp features mentioned distinguishes foidl inductive logic programming number related pieces research mentioned 
intensional background knowledge distinguishing feature number ilp systems incorporate aspect 
focl pazzani kibler lavrac dzeroski cohen forte richards mooney zelle mooney intensional background degree context foil algorithm 
ilp systems employ intensional background include early ones shapiro sammut banerji ones bergadano 
stahl wirth 
implicit negatives significantly novel 
described section approach considerably different explicit construction closed world assumption employed explicit construction sufficient negative examples intractable 
bergadano 
allows user supply intensional definition negative examples covers large set ground instances past equal equivalent output completeness user explicitly provide separate intensional negative definition positive example 
non monotonic semantics eliminate need negative examples claudien de raedt bruynooghe effect output completeness assumption case arguments target relation outputs 
output completeness permits flexibility allowing arguments specified inputs counting negative examples extra outputs generated specific inputs training set 
flip bergadano provides method learning functional programs negative examples making assumption equivalent output completeness functional case 
output completeness general permits learning non functional programs 
foidl previous induction order decision lists learning english past tense methods provide way quantifying implicit negative coverage context heuristic top specialization algorithm 
notion order decision list unique foidl 
ilp system attempts learn programs exploit clause order cuts bergadano 

discusses problems learning arbitrary programs cuts brute force search approach intractable realistic problems 
addressing general problem learning arbitrary programs cuts foidl tailored specific problem learning order decision lists cuts stylized manner particularly useful functional problems involve rules exceptions 
bain muggleton bain discuss technique uses negation failure handle exceptions 
negation failure significantly different decision lists simply prevents clause covering exceptions learning additional clause rides existing clause specifies correct output set exceptions 
related past tense learning shortcomings previous past tense learning reviewed section results section clearly demonstrate generalization advantage foidl exhibits problem 
couple issues deserve additional discussion 
previous problem concerned modelling various psychological phenomenon shaped learning curve children exhibit irregular verbs acquiring language 
addressed issue psychological validity focused performance accuracy exposure fixed number training examples 
specific psychological claims current results 
humans obviously produce correct past tense arbitrarily long novel words foidl easily model fixed length feature representations clearly 
ling developed version spa eliminates position dependence fixed word length ling sliding window nettalk sejnowski rosenberg 
large window includes letters side current position padded blanks necessary order include entire word examples corpus 
results approach significantly better normal spa inferior foidl results 
approach requires fixed sized input window prevents handling arbitrary length irregular verbs 
recurrent neural networks avoid word length restrictions plunkett appears applied standard tense past tense mapping problem 
believe difficulty training recurrent networks relatively poor ability maintain state information arbitrarily long limit performance task 
issue comprehensibility transparency learned result 
foidl programs past tense short concise readable complicated networks decision forests pure logic programs generated previous approaches 
ling marinov discusses possibility transforming spa decision forest mooney califf comprehensible order rules approach directly learning order rules data clearly preferable 

obvious topic research foidl cognitive modelling abilities context past tense task 
incorporating fitting avoidance methods may allow system model shaped learning curve manner analogous demonstrated ling marinov 
ability model human results generating past tense novel verbs examined compared spa ling marinov connectionist methods 
order decision lists represent fairly general class programs currently convincing experimental results past tense problem 
realistic problems consist rules exceptions experimental results additional applications needed support general utility representation 
despite advantages intensional background knowledge ilp incurs significant performance cost examples continually testing alternative literals specialization 
computation accounts training time foidl 
approach improving computational efficiency maintain partial proofs examples incrementally update proofs additional literals added clause 
approach foil approach maintaining tuples require meta interpreter prolog incurs significant overhead 
efficient intensional knowledge ilp greatly benefit rapid incremental compilation logic programs incrementally updating compiled code account small changes definition predicate 
foidl potentially benefit methods handling noisy data preventing fitting 
pruning methods employed foil related systems quinlan lavrac dzeroski easily incorporated 
decision list framework alternative simply ignoring incorrectly covered examples noise treat exceptions handled subsequently learned clauses uncovering technique discussed section 
theoretical results learnability restricted classes order decision lists interesting area research 
results pac learnability propositional decision lists rivest restricted classes ilp problems dzeroski muggleton russell cohen appropriately restricted class order decision lists pac learnable 

addressed main issues appropriateness order learner popular past tense problem problems previous ilp systems handling functional tasks best representation rules exceptions 
results clearly demonstrate ilp system outperforms decision tree neural network systems previously applied past tense task 
important results showing order learner performs significantly better apply induction order decision lists learning english past tense ing propositional learners best feature encoding problem 
research demonstrates efficient effective algorithm learning concise comprehensible symbolic programs small interesting subproblem language acquisition 
shows possible efficiently learn logic programs involve cuts exploit clause order particular class problems demonstrates usefulness intensional background implicit negatives 
solutions practical problems require general default rules characterizable exceptions may best learned order decision lists 
basic research conducted author leave university sydney supported prof quinlan australian research council 
ross quinlan providing enjoyable productive opportunity ross mike cameron jones important discussions pointers greatly aided development foidl 
ross aiding running foil experiments 
discussions john zelle thompson university texas influenced 
partial support provided iri national science foundation mcd fellowship university texas awarded second author 
bain 

experiments non monotonic order induction 
muggleton 
ed inductive logic programming pp 

academic press new york ny 
bain muggleton 

non monotonic learning 
muggleton 
ed inductive logic programming pp 

academic press new york ny 
bergadano 

interactive system learn functional logic programs 
proceedings thirteenth international joint conference artificial intelligence pp 
chambery france 
bergadano 

difficulties learning logic programs cut 
journal artificial intelligence research 
califf 

learning past tense english verbs inductive logic programming approach 
unpublished project report 
cameron jones quinlan 

efficient top induction logic programs 
sigart bulletin 
clark niblett 

cn induction algorithm 
machine learning 
cohen 

pac learning nondeterminate clauses 
proceedings twelfth national conference artificial intelligence pp 
seattle wa 
mooney califf cohen 

compiling prior knowledge explicit bias 
proceedings ninth international conference machine learning pp 
aberdeen scotland 
plunkett 

learning past tense recurrent network acquiring mapping meaning sounds 
proceedings thirteenth annual conference cognitive science society pp 
chicago il 
de raedt bruynooghe 

theory clausal discovery 
proceedings thirteenth international joint conference artificial intelligence pp 
chambery france 
dzeroski muggleton russell 

pac learnability determinate logic programs 
proceedings workshop computational learning theory pittsburgh pa 

relation linguistic structure associative theories language learning constructive critique connectionist learning models 
pinker mehler 
eds connections symbols pp 

mit press cambridge ma 
lavrac dzeroski 
eds 

inductive logic programming techniques applications 
ellis horwood 
ling 

learning past tense english verbs symbolic pattern associator vs connectionist models 
journal artificial intelligence research 
ling 

personal communication 
ling marinov 

answering connectionist challenge symbolic model learning past tense english verbs 
cognition 
macwhinney 

implementations conceptualizations revising verb model 
cognition 
muggleton buntine 

machine invention order predicates inverting resolution 
proceedings fifth international conference machine learning pp 
ann arbor mi 
muggleton feng 

efficient induction logic programs 
proceedings conference algorithmic learning theory tokyo japan 
ohmsha 
muggleton king sternberg 

protein secondary structure prediction logic machine learning 
protein engineering 
muggleton 
ed 

inductive logic programming 
academic press new york ny 
norvig 

paradigms artificial intelligence programming case studies common lisp 
morgan kaufmann san mateo ca 
induction order decision lists learning english past tense pazzani kibler 

utility background knowledge inductive learning 
machine learning 
pinker prince 

language connectionism analysis parallel distributed model language acquisition 
pinker mehler 
eds connections symbols pp 

mit press cambridge ma 
quinlan 

programs machine learning 
morgan kaufmann san mateo ca 
quinlan 

past tenses verbs order learning 
zhang lukose 
eds proceedings seventh australian joint conference artificial intelligence pp 
singapore 
world scientific 
quinlan cameron jones 

foil midterm report 
proceedings european conference machine learning pp 
vienna 
quinlan 

learning logical definitions relations 
machine learning 
richards mooney 

automated refinement order horn clause domain theories 
machine learning press 
rivest 

learning decision lists 
machine learning 
rumelhart mcclelland 

learning past tense english verbs 
rumelhart mcclelland 
eds parallel distributed processing vol 
ii pp 

mit press cambridge ma 
sammut banerji 

learning concepts asking questions 
michalski carbonell mitchell 
eds machine learning ai approach vol 
ii pp 

morgan kaufman 
sejnowski rosenberg 

parallel networks learn pronounce english text 
complex systems 
shapiro 

algorithmic program debugging 
mit press cambridge ma 
shavlik mooney towell 

symbolic neural learning algorithms experimental comparison 
machine learning 
stahl wirth 

methods improving inductive logic programming systems 
machine learning ecml pp 
vienna 
webb 

learning decision lists prepending inferred rules 
proceedings australian workshop machine learning hybrid systems pp 
melbourne australia 
zelle mooney 

combining top bottom methods inductive logic programming 
proceedings eleventh international conference machine learning new brunswick nj 
mooney califf zelle mooney 

inducing deterministic prolog parsers treebanks machine learning approach 
proceedings twelfth national conference artificial intelligence pp 
seattle wa 

