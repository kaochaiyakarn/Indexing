generalizing generalization generalization similarity bayesian inference joshua tenenbaum thomas griffiths department psychology jordan hall building stanford university stanford ca psych stanford edu running head generalizing generalization generalizing generalization short shepard theoretical analysis generalization originally formulated ideal case encountering single consequential stimulus represented point continuous metric space recast general bayesian framework 
formulation naturally extends realistic situation generalizing multiple consequential stimuli arbitrary representational structure 
framework subsumes version tversky set theoretic models similarity conventionally thought primary alternative shepard approach 
unification allows draw deep parallels set theoretic spatial approaches significantly advance explanatory power set theoretic models 
long shepard argued universal law govern generalization different domains perception cognition organisms different species different planets 
starting basic assumptions natural kinds derived exponential decay function form universal generalization gradient accords strikingly wide range empirical data 
original formulation applied ideal case generalization single encountered stimulus single novel stimulus stimuli represented points continuous metric psychological space 
recast shepard theory general bayesian framework show naturally extends approach realistic situation generalizing multiple consequential stimuli arbitrary representational structure 
framework subsumes version tversky set theoretic models similarity conventionally thought primary alternative shepard continuous metric space model similarity generalization 
unification allows draw deep parallels set theoretic spatial approaches significantly advance explanatory power set theoretic models 
generalizing generalization consider hypothetical case doctor trying determine particular hormone naturally produced human body affects health patients 
patients little hormone blood suffer negative effects patients hormone 
assume possible concentration levels hormone represented real numbers arbitrary measuring scale healthy patient examined hormone level 
hormone levels doctor consider healthy 
imagine baby robin mother just worm eat 
worms robin environment vary level skin worms intermediate density eat dark light worms unhealthy 
suppose simplicity robins capable detecting shades worm coloration arbitrary scale worm baby robin scores skin level 
assuming mother chosen worm eat levels baby robin consider eat 
scenarios cases shepard ideal generalization problem encounter single stimulus patient worm represented point psychological space hormone level level particular consequence healthy eat stimuli space expected consequence 
shepard observes wide variety experimental situations including human animal subjects generalization gradients tend fall approximately exponentially distance appropriately scaled psychological space obtained multidimensional scaling mds 
gives rational probabilistic argument origin universal law starting basic assumptions geometry natural kinds psychological spaces expected apply equally doctors robins galaxy 
argument distinction principle conscious deliberate cognitive inferences healthy hormone levels scenario unconscious automatic perceptual inferences eat worms scenario long satisfy conditions ideal generalization problem 
generalizing generalization opening sentences universal law generalization shepard invokes newton universal law gravitation standard theoretical scope significance 
analogy holds strongly anticipated 
newton law gravitation expressed terms attraction point masses object universe attracts object force directed line connecting centers mass proportional product masses inversely proportional square separation 
interesting gravitational problems encountered universe involve point masses 
order model real world gravitational phenomena physicists newton developed rich theory classical mechanics extends law gravitation address interactions multiple arbitrarily extended bodies 
likewise shepard formulated universal law respect generalization single encountered stimulus single novel stimulus assumed stimuli represented points continuous metric psychological space 
interesting problems generalization psychological science fit mold 
involve inferences multiple examples stimuli easily represented strictly spatial terms 
example doctor observes hormone levels healthy patients 
change generalization gradient 
numbers observed different context examples certain mathematical concept teacher student 
certain features numbers salient hormone context multiples important mathematical context 
consequently simple dimensional metric space representation may longer appropriate may instance mathematical concept exemplified examples hormone context may healthy level 
just physicists see newton original point mass formulation special case general classical theory gravitation general theory generalization reduces shepard original points psychological space formulation appropriate special cases extends approach handle generalization multiple arbitrarily structured examples 
generalizing generalization article outline foundations theory working general framework bayesian inference 
proposal extending shepard theory cases multiple examples arbitrary stimulus structures introduced papers griffiths tenenbaum tenenbaum tenenbaum xu 
goal explicit link shepard framework connections models learning feldman gluck haussler kearns schapire kruschke mitchell generalization nosofsky heit similarity chater hahn medin goldstone gentner tversky 
particular lot say generalization shepard theory relates tversky known set theoretic models similarity 
tversky set theoretic approach shepard metric space approach considered classic classically opposed theories similarity generalization 
demonstrating close parallels tversky approach bayesian generalization shepard approach hope go way unifying theoretical approaches advancing explanatory power 
plan article follows 
section recast shepard analysis generalization general bayesian framework preserving basic principles approach form allows apply theory situations multiple examples arbitrary non spatially represented stimulus structures 
sections describe extensions section concludes discussing implications theory internalization perceptual cognitive universals 
bayesian framework generalization shepard formulates problem generalization follows 
example consequence healthy person eat worm 
assume represented point continuous metric psychological space dimensional space hormone levels corresponds region consequential region space 
task infer probability newly encountered object instance fall consequential generalizing generalization region formalizing induction problem probabilistic terms asking cjx conditional probability falls observation example theory generalization shepard develops extend best understood considering addresses crucial questions learning chomsky 

constitutes learner knowledge consequential region 

learner knowledge decide generalize 

learner acquire knowledge example encountered 
commitment paradigm bayesian probabilistic inference leads directly rational answers questions 
rest section presents answers illustrates concretely hormone levels tasks introduced 
main advance shepard original analysis comes introducing size principle tenenbaum scoring hypotheses true consequential region size specificity 
little difference simplest case generalization studied shepard size principle provide major explanatory force turn realistic cases generalizing multiple examples section arbitrary structure section 
constitutes learner knowledge consequential region 
learner knowledge consequential region represented probability distribution hjx priori specified hypothesis space possible consequential regions forms set exhaustive mutually exclusive possibilities element assumed true consequential region different candidate regions represented may overlap arbitrarily stimuli include 
learner background knowledge may include domain specific domain general components translate constraints subsets objects belong shepard suggests general constraint consequential regions basic natural kinds generalizing generalization correspond connected subsets psychological space 
applying connectedness constraint domains hormone levels worm levels relevant stimulus spaces dimensional continua hypothesis spaces consist intervals ranges stimuli minimum maximum consequential levels 
shows number intervals consistent single example 
simplicity assumed integer stimulus values possible cases stimulus hypothesis spaces form true continua 
times learner knowledge consequential region consists probability distribution prior observing distribution prior probability observing posterior probability hjx 
probabilities hjx numbers reflecting learner degree belief fact true consequential region corresponding hjx indicated thickness height corresponding bar 
probability contain zero true consequential region contain observed example 
shows hypotheses consistent 
insert 
learner knowledge decide generalize 
generalization function cjx computed summing probabilities hjx hypothesized consequential regions contain cjx hjx refer computation hypothesis averaging thought averaging predictions hypothesis membership weighted hypothesis posterior probability 
hjx probability distribution normalized sum structure equation ensures cjx lie 
general hypothesis space need finite countable 
case continuum generalizing generalization hypotheses space intervals real numbers probability distributions probability densities sums equations integrals 
top panel shows generalization gradient results averaging predictions integer valued hypotheses shown weighted probabilities 
note probability generalization equals hypothesis containing contains moves away number hypotheses containing contain decreases probability generalization correspondingly decreases 
shows characteristic profile shepard universal generalization function concave negatively accelerated 
replace integer valued interval hypotheses full continuum real valued intervals sum equation integral piecewise linear gradient shown smooth function similar concave profile depicted top panels figures 
demonstrates shepard approximately exponential generalization gradient emerges particular assignment hjx reasonable ask sensitive result choice hjx 
shepard showed shape gradient remarkably insensitive probabilities assumed 
long probability distribution hjx isotropic independent location generalization function concave profile 
condition isotropy equivalent saying hjx depends jhj size volumetric measure notice constraint satisfied 
learner acquire knowledge example encountered 
observing example consequence learner updates beliefs consequential region prior posterior hjx 
consider rational learner arrives hjx bayes rule 
say origins section shepard tenenbaum discuss reasonable alternatives scenarios isotropic assume little knowledge true consequential region 
generalizing generalization bayes rule couples posterior prior likelihood xjh probability observing example true consequential region follows hjx xjh xjh xjh likelihood function determined think example generating process relates true consequential region shepard argues default assumption example consequential region sampled independently just happens land inside assumption standard machine learning literature haussler kearns schapire mitchell maps heit bayesian analysis inductive reasoning 
tenenbaum argues conditions natural treat random positive example involves stronger assumption explicitly sampled refer models weak sampling strong sampling respectively 
weak sampling likelihood just measures binary fashion hypothesis consistent observed example xjh weak sampling strong sampling likelihood informative 
assuming sampled uniform distribution objects xjh jhj strong sampling jhj indicates size region discrete stimulus spaces jhj simply cardinality subset corresponding continuous spaces hormone levels likelihood probability density jhj measure hypothesis dimension just length interval 
equation implies smaller specific hypotheses tend receive higher probabilities larger general hypotheses equally consistent observed consequential stimulus 
call tendency size principle 
closely related principles genericity generalizing generalization proposed models visual perception categorization knill richards feldman 
depicts application size principle graphically 
note equations isotropic choice strong sampling weak sampling effect shepard main result generalization gradients universally concave 
turn look phenomena generalization multiple stimuli arbitrary non spatially represented structures see size principle implied strong sampling carries great deal explanatory power shepard original analysis 
multiple examples section extend bayesian analysis situations multiple consequential examples 
situations arise quite naturally generalization scenarios discussed 
instance doctor generalize observing hormone levels healthy patients 
discuss basic phenomena arise multiple examples turn extension theory 
compare approach alternative ways shepard theory adapted apply multiple examples 
phenomena generalization multiple examples focus classes phenomena effects example variability number examples 
example variability 
things equal smaller variability set observed examples lower probability generalization outside range 
probability healthy hormone level greater examples examples greater 
effects exemplar variability generalization documented categorization inductive inference tasks fried holyoak osherson smith wilkie lopez rips 
generalizing generalization number examples 
things equal examples observed range lower probability generalization outside range 
probability healthy hormone level greater examples examples greater 
effect dramatic little variability observed examples 
consider sets examples 
just examples probability generalizing lower probability close zero 
extending theory fx xng denote sequence examples consequence denote novel object want compute probability generalizing cjx 
theory section applicable replace appears adopt assumption strong sampling shepard original proposal weak sampling 
rest formalism unchanged 
complication introduces comes computing likelihood xjh 
simplifying assumption examples sampled independently standard assumption bayesian analysis equation xjh jh jhj xn size principle equation generalized include influence smaller hypotheses receive higher likelihoods larger hypotheses factor increases exponentially number examples observed 
figures depict bayesian gradients generalization result different numbers ranges examples assuming xjh strong sampling erlang distribution shepard 
addition showing universal concave profile gradients display appropriate sensitivity number variability examples 
generalizing generalization understand size principle generates effects consider equation weights representative hypotheses smallest interval containing examples broader interval centered extending units side jh jh observing examples relative probabilities proportional likelihood ratio xjh xjh jh jh positive 
jh increases quantities remain fixed increases 
see relative probability extends distance examples increases range spanned examples increases 
increases quantities remain fixed quickly approaches 
see probability extends distance examples rapidly decreases number examples increases fixed range 
tighter examples smaller jh faster decreases increasing accounting interaction factors pointed earlier 
see shepard original assumption weak sampling generate phenomena 
weak sampling likelihoods consistent hypotheses 
range number examples effects hypotheses weighted 
general expect strong sampling weak sampling models uses 
real world learning situations may require combination examples generated mere observation consequential stimuli strong sampling trial error exploration weak sampling 
insert 
insert 
illustrates extension generalizing separable dimensions inferring healthy levels independent hormones details see tenenbaum 
shepard assume consequential regions correspond axis aligned rectangles dimensional space independent priors dimension 
generalizing generalization shown size principle acts favor generalization dimensions examples high variability restrict generalization dimensions low variability 
tenenbaum reports data human subjects consistent predictions task estimating healthy levels independent biochemical compounds 
studies need done test predictions multidimensional perceptual spaces sort shepard concerned 
insert 
alternative approaches number computational models may seen alternative methods extending shepard approach case multiple examples framework describe preserves take central features shepard original analysis hypothesis space possible consequential regions bayesian inference procedure updating beliefs true consequential region 
standard exemplar models classification nosofsky take shepard exponential law generalization primitive justify assumption exemplar activation functions decay exponentially distance psychological space 
different approach connectionist networks gluck gluck shepard shepard tenenbaum input hidden units represent consequential regions error driven learning bayesian inference adjust weights consequential region inputs response outputs 
third class models kruschke love medin combines aspects embedding shepard exponential law activation functions hidden units connectionist network classification learning 
space permit full comparison various alternative models proposals 
important point difference models generalization gradients produced multiple examples consequence essentially just superpositions exponential decay gradients produced individual example 
consequently models easily explain phenomena discussed encountering additional consequential stimuli causes probability generalizing new stimulus decrease generalizing generalization additional examples similar new stimulus original example 
exemplar exemplar connectionist hybrid models frequently equipped variable attentional weights scale distances input dimension greater lesser amount order produce variations contours generalization 
models account phenomena postulating dimension length scale initially large decreases number examples increases variability examples decreases formal structure models necessarily implies mechanism 
bayesian analysis contrast necessarily predicts effects rational consequences size principle 
arbitrary stimulus structure shepard assumed objects represented points continuous metric psychological space consequential subsets correspond regions space convenient properties connectedness central symmetry 
general need assume hypothesized consequential subsets correspond regions continuous metric space notion consequential subset sufficient defining bayesian account generalization 
section examine arbitrary non spatially represented structures modeled bayesian framework 
authors including shepard described extensions original theory generalization conjunctive feature structures objects represented terms presence absence primitive binary features possible consequential subsets consist objects sharing different conjunctions features 
cases generalization gradients shown follow exponential decay function appropriately defined distance measure gluck russell shepard 
bayesian analysis generalization widely applicable 
show analysis applies independent notion distance stimuli exponential gradient emerges sum consequential regions 
motivate analysis consider new generalization scenario 
computer programmed variety simple mathematical concepts defined integers generalizing generalization subsets numbers share common mathematically consequential property number power square number 
computer select subsets random choose numbers random subset show examples quiz asking certain numbers belong concept 
suppose number offered example concept computer chosen 
probability computer accept 

syntactically task identical hormone levels scenario 
generalization monotonic function proximity numerical magnitude follow measure mathematical similarity 
instance number shares mathematical properties making better bet accepted example closer magnitude better bet doctor trying determine healthy hormone levels 
bayesian framework difference scenarios stems different consequential subsets elements considered 
doctor knowing healthy levels hormones general quite natural assume true consequential subset corresponds unknown interval gives rise generalization function monotonically related proximity magnitude 
model number game identify mathematical property learner knows possible consequential subset shows generalization function results set simple hypotheses calculated size principle equation hypothesis averaging equation 
generalization function appears jagged figures mathematical hypothesis space respect proximity dimension numerical magnitude corresponding abscissa figures 
insert 
generally numerical cognition may incorporate spatial magnitude properties non spatial mathematical properties numbers 
investigate nature mental representations numbers shepard cunningham collected human similarity judgments pairs integers range different contexts 
submitting data additive clustering analysis shepard arabie generalizing generalization tenenbaum construct hypothesis space consequential subsets best accounts people similarity judgments 
table shows kinds subsets occur best fitting additive clustering solution tenenbaum numbers sharing common mathematical property consecutive numbers similar magnitude 
tenenbaum studied people generalized concepts version number game mathematical magnitude properties salient 
bayesian model hypothesis space inspired additive clustering results defined integers yielded excellent fit people generalization judgments 
flexibility hypothesis space structure allowed bayesian framework model spatial hormone level scenario non spatial number game scenario allows model generalization generic context hypothesizing mixture consequential subsets spatial magnitude properties nonspatial mathematical properties 
fact define bayesian generalization function just spatial featural simple hybrids representations collection hypothesis subsets whatsoever 
restriction able define prior probability measure discrete continuous measure space objects required strong sampling sense 
measure space objects bayesian analysis weak sampling possible 
insert table 
relations generalization set theoretic models similarity classically mathematical models similarity generalization fall poles continuous metric space models shepard theory set theoretic matching models tversky contrast model 
strictly include special case commonly applied domains set discrete conceptual features opposed low dimensional continuous space provide natural stimulus representation shepard 
domain number game generalize shepard generalizing generalization bayesian analysis consequential regions continuous metric spaces apply arbitrary consequential subsets model comes look version tversky set theoretic models 
making connection explicit allows unify classically opposing approaches similarity generalization explain significant aspects similarity tversky original treatment attempt explain 
tversky contrast model expresses similarity gamma fff gamma gamma fif gamma feature sets representing respectively denotes measure feature sets ff fi free parameters model 
similarity involves contrast common features distinctive features possessed gamma possessed gamma tversky suggested alternative form matching function ratio model written fff gamma fif gamma ratio model remarkably similar bayesian model generalization particularly apparent bayesian model expressed form mathematically equivalent equation cjx xjh represents weight assigned hypothesis light example depends prior likelihood 
bottom sum ranges hypotheses include top sum ranges hypotheses include include identify feature tversky framework hypothesized subset object belongs possesses feature standard assumption measure additive bayesian model expressed equation corresponds precisely ratio model ff fi 
monotonically related contrast model parameter settings 
interpreting formal correspondence bayesian model generalization tversky set theoretic models complicated fact general similarity generalization understood 
number authors proposed generalizing generalization similarity primitive cognitive process forms part basis capacity generalize inductively quine rips osherson 
standpoint reverse engineering mind explaining similarity generalization computations take satisfying theory similarity depend theory generalization vice 
problem generalization stated objectively principled rational analysis question similar objects notoriously slippery underdetermined goodman 
expect depending context judgment similarity may correspond probability generalizing combination 
may depend factors altogether 
qualifications aside interesting consequences follow just hypothesis similarity depends generalization specifying exact nature dependence 
syntax similarity fundamentally bayesian analysis provides rational basis qualitative form set theoretic models 
instance explains similarity principle depend common distinctive features objects 
tversky asserted axiom similarity function common distinctive features empirical evidence consistent assumption attempt explain hold general 
exist empirical models shepard rational models chater hahn successfully employed common distinctive features 
rational analysis equation contrast explains kinds features matter general assumption similarity depends generalization consequential subsets contain relative number contain higher probability known contain contain similar lines hypothesis similarity depends part generalization explains similarity may principle asymmetric relationship similarity may differ similarity tversky compelling demonstrations asymmetries showed modeled set theoretic framework subsets distinctive features gamma gamma different measures generalizing generalization different weights equations 
tversky formal theory explain different weights merely allows possibility 
contrast probability generalizing intrinsically asymmetric function depending distinctive features extent similarity depends inherits intrinsic asymmetry 
note generalization symmetric distinctive features equal number weight 
condition holds spatial scenarios considered shepard coincidentally domains similarity nearly symmetric tversky 
shepard analysis generalization tversky contrast model originally defined comparison individual objects 
bayesian framework justifies natural extension problem computing similarity object set objects fx xng just shepard theory section 
heit proposed intuitive grounds contrast model apply situation feature set examples intersection feature sets individual examples 
bayesian analysis replacing equation explains intersection opposed combination mechanism union appropriate 
hypotheses consistent examples corresponding features belonging intersection feature sets receive non zero likelihood equation 
semantics similarity persistent criticisms contrast model relatives focus semantic questions qualifies feature 
determines feature weights 
weights change judgment contexts 
contrast model broad explanatory scope allows kind features feature weights whatsoever lack constraint prevents model explaining origins features weights 
bayesian model likewise offers constraints qualifies feature explain aspects origins dynamics feature weights 
bayesian feature weight xjh decomposes prior likelihood terms 
prior constrained analysis accommodate arbitrary flexibility contexts explains flexibility 
generalizing generalization contrast likelihood xjh constrained assumption strong sampling follow size principle 
obvious implication constraint context features belonging fewer objects corresponding hypotheses smaller sizes assigned higher weights 
tested hypothesis survey additive clustering analyses distinct data sets papers describing leading algorithms arabie carroll carroll lee submitted tenenbaum 
cases observed strong negative correlations size feature extension magnitude weight mean gamma standard deviation 
tversky diagnosticity principle similar spirit relationship explicitly propose correlation feature specificity feature weight formal model designed predict effects 
second implication size principle certain kinds features tend receive higher weights similarity comparisons systematically belong fewer objects 
medin goldstone argued primitive features important relational features higher order features defined relations primitives 
cases relation appears important primitive feature 
consider bottom stimulus similar top stimulus panel inspired medin comparisons 
left panel top stimulus shares primitive feature triangle top relational feature different shapes 
informal survey observers chose primitive feature match similar glance 
right panel different relation shape dominates primitive feature different observers chose similar 
goldstone gentner medin report cases relations weighted highly different relations similarity comparisons 
similarity depends part bayesian generalization size principle explain relative salience features 
number distinct shapes square triangle appear positions stimulus pattern 
consequential subset shape contains exactly distinct stimuli subset triangle top contains stimuli subset different shapes contains gamma gamma stimuli 
feature saliency inversely related subset size generalizing generalization just expect size principle 
careful empirical tests hypothesis required conjecture general relative importance relational features versus primitive features explained large part differing 
insert 
final implication arises interaction size principle multiple examples 
recall generalizing multiple examples likelihood preference smaller hypotheses increases exponentially number examples equation 
effect observed weights features similarity judgments 
instance assessing similarity number feature multiple may may receive slightly greater weight feature number 
assessing similarity set numbers features equally consistent full set examples specific feature multiple appears salient 
learning evolution origins hypothesis spaces described bayesian framework learning generalization significantly extends shepard theory principal ways 
addressing generalization multiple examples analysis fairly direct extension shepard original ideas making substantive additional assumptions strong sampling 
contrast analysis generalization arbitrarily structured stimuli represents radical broadening shepard approach giving notion generalization constrained metric properties evolutionarily internalized psychological space 
positive side step allows draw tversky set theoretic models similarity shepard continuous metric space models generalization single rational framework advance explanatory power tversky set theoretic models tools chiefly size principle advance shepard analysis generalization 
opens door large unanswered questions close article pointing 
generalizing generalization discussing similarity generalization arbitrarily structured stimuli bayesian analysis explains piece puzzle features hypotheses weighted 
weights product size likelihoods priors size principle follows rationally assumption strong sampling assignment prior probabilities lies outside scope basic bayesian analysis 
say certain relative weights features hypotheses merely relative sizes size difference overruled greater difference prior probability 
ability prior probability differences overrule opposing size likelihood difference hardly pathological contrary essential successful inductive generalization 
consider hypothesis number game computer accepts multiples 
multiples slightly specific multiples receive higher probability size principle set examples consistent hypotheses 
obviously happen people minds 
bayesian framework accommodate phenomenon stipulating hypothesis receives somewhat higher likelihood receives lower prior probability significantly lower posterior probability prior likelihood combined 
reasonable priori constraints hypotheses learners consider innumerable bizarre hypotheses multiples stand way reasonable inductive generalizations goodman mitchell 
trying determine nature origin constraints major goals current research medin goldstone gentner schyns goldstone 
shepard original analysis generalization compelling part proposed answers questions sufficient constraints form generalization provided merely representation stimuli points continuous metric psychological space assumption hypotheses correspond suitable family regions space psychological spaces products evolutionary process shaped optimally reflect structure environment 
proposing theory generalization allows arbitrarily structured hypothesis spaces owe account hypothesis spaces priors come generalizing generalization 
surely evolution sufficient account hypotheses multiples considered natural hypotheses multiples 
major alternative evolution source hypothesis space structure kind prior learning 
directly prior experience objects belonging particular subset tend possess number important consequences may lead learners increase new consequences sort 
unsupervised learning observation properties objects consequential input may extremely useful forming hypothesis space supervised consequential learning 
noting subset objects tend cluster similar objects primitive features may increase learner prior probability subset share important consequence 
machine learning community intensely interested improving inductive generalizations supervised learning agent draw labeled examples building unsupervised inferences agent draw large body unlabeled examples mitchell poggio shelton 
expect critical issue near cognitive science 
proposal building blocks shepard perceptual cognitive universals come heads learning just evolution contribution issue barlow 
fundamentally agree earlier statement shepard learning alternative evolution depends evolution 
learning absence principles learning principles unlearned shaped evolution shepard 
ultimately believe may difficult impossible separate contributions learning evolution internalization world structure crucial role process plays making ecologically viable means adaptation 
think may worthwhile look productive synergies processes tools evolution efficiently learning hypothesis spaces lead successful bayesian generalizations 
tools include appropriately tuned stimulus metrics topologies shepard proposes unsupervised clustering algorithms exploit size principle defined metrics vocabulary templates kinds hypothesis spaces generalizing generalization continuous spaces taxonomic trees conjunctive feature structures recur basis mental representations domains ability recursively compose hypothesis spaces order build structures increasing complexity 
believe search universal principles learning generalization just begun shepard 
universality invariance elegance shepard exponential law quote article reprinted volume impressive ultimately significance spirit rational analysis pioneered general avenue discovery perceptual cognitive universals 
shown line analysis extended yield may prove universal size principle governs generalization examples arbitrary structure 
speculate universal principles result turning attention interface learning evolution 
generalizing generalization acknowledgments writing articles supported part nsf dbs gift mitsubishi electric research labs 
second author supported studentship 
generalizing generalization arabie carroll 

mathematical programming approach fitting model 
psychometrika 
ashby alfonso reese 

categorization probability density estimation 
journal mathematical psychology 
chater hahn 

representational distortion similarity universal law generalization 
hahn pain eds proceedings interdisciplinary workshop similarity categorisation pp 

edinburgh department artificial intelligence edinburgh university 
carroll 

alternating combinatorial optimization approach fitting generalized models 
journal classification 
chomsky 

language problems knowledge lectures 
cambridge ma mit press 
clark 

principle contrast constraint language acquisition 
macwhinney ed th annual carnegie symposium cognition 
hillsdale nj erlbaum 
feldman 

structure perceptual categories 
journal mathematical psychology 
fried holyoak 

induction category distributions framework classification learning 
journal experimental psychology learning memory cognition 
gluck 

stimulus generalization representation adaptive network models category learning 
psychological science 
goldstone 

role similarity categorization providing groundwork 
cognition 
goldstone gentner medin 

relations relating relations 
proceedings th annual meeting cognitive science society 
hillsdale nj erlbaum 
goodman 

similarity 
goodman ed problems projects 
new york merrill generalizing generalization griffiths tenenbaum 
press 
bayesian approach predicting 
proceedings nd annual conference cognitive science society 
hillsdale nj erlbaum 
hahn chater 

concepts similarity 
eds knowledge concepts categories pp 

cambridge ma mit press 
haussler kearns schapire 

bounds sample complexity bayesian learning information theory vc dimension 
machine learning 
heit 

features similarity category induction 
hahn pain eds proceedings interdisciplinary workshop similarity categorisation pp 

edinburgh department artificial intelligence edinburgh university 
heit 

bayesian analysis forms inductive reasoning 
chater eds rational models cognition pp 

oxford oxford university press 
kahneman tversky 

subjective probability judgment representativeness 
cognitive psychology 
knill richards 

perception bayesian inference 
cambridge university press 
kruschke 

exemplar connectionist model category learning 
psychological review 
lee 
submitted 
simple method generating additive clustering models limited complexity 
complex psych adelaide edu au members post grads pdf love medin 

sustain model human category learning 
proceedings fifteenth national conference artificial intelligence aaai pp 

marr 

vision 
san francisco freeman 
medin goldstone gentner 

respects similarity 
psychological review 
mitchell 

role unlabeled data supervised learning 
proceedings sixth international colloquium cognitive science 
generalizing generalization mitchell 

machine learning 
new york mcgraw hill 
muggleton 
review 
learning positive data 
machine learning 
nosofsky 

attention similarity identification categorization relationship 
journal experimental psychology general 
nosofsky 

optimal performance exemplar models classification 
chater eds rational models cognition pp 

oxford oxford university press 
osherson smith wilkie lopez 

category induction 
psychological review 
poggio shelton 

machine learning machine vision brain 
ai magazine 
quine 

ontological relativity essays 
new york columbia university press 
rips 

inductive judgments natural categories 
journal verbal learning verbal behavior 
rips 

similarity typicality categorization 
ortony eds similarity analogical reasoning pp 

cambridge cambridge university press 
russell 

analogy similarity 
helman ed analogical reasoning pp 

new york kluwer academic publishers 
schyns goldstone 

development features object concepts 
behavioral brain sciences 
gluck 

tests adaptive network model identification categorization continuous dimension stimuli 
connection science 
shepard 

multidimensional scaling tree fitting clustering 
science 
shepard 

universal law generalization psychological science 
science 
shepard 
august 
law generalization connectionist learning 
address cognitive science society ann arbor mi shepard 

perceptual cognitive universals reflections world 
psychonomic bulletin review 
reprinted issue generalizing generalization shepard 

mental universals century science mind 
massaro eds science mind pp 

new york oxford university press 
shepard arabie 

additive clustering representation combinations discrete overlapping properties 
psychological review 
shepard 

connectionist implementation theory generalization 
lippmann moody touretzky hanson eds advances neural information processing systems vol 
pp 

san francisco ca morgan kaufmann 
shepard cunningham 

internal representation numbers 
cognitive psychology 
shepard tenenbaum 
november 
connectionist modeling multidimensional generalization 
meeting psychonomic society san francisco smith 

concepts induction 
posner ed foundations cognitive science pp 

cambridge ma mit press 
tenenbaum 

learning structure similarity 
touretzky mozer hasselmo eds advances neural information processing systems vol 
pp 

cambridge ma mit press 
tenenbaum 

bayesian framework concept learning 
hahn pain eds proceedings interdisciplinary workshop similarity categorisation pp 

edinburgh department artificial intelligence edinburgh university 
tenenbaum 

bayesian framework concept learning 
unpublished doctoral dissertation massachussets institute technology cambridge ma 
tenenbaum 

bayesian modeling human concept learning 
kearns solla cohn eds advances neural information processing systems pp 

cambridge ma mit press 
generalizing generalization tenenbaum 

rules similarity concept learning 
solla leen 
muller eds advances neural information processing systems 
cambridge ma mit press 
tenenbaum xu 
press 
word learning bayesian inference 
proceedings nd annual conference cognitive science society 
hillsdale nj erlbaum 
tversky 

features similarity 
psychological review 
generalizing generalization footnotes derive equation follows 
denotes exhaustive mutually exclusive set possibilities expand generalization function cjx hjx hjx note fact independent simply 
rewrite equation form equation 
note continuous space jhj xjh greater 
occurs xjh probability density probability distribution probability density functions may take values greater long integrate generalizing generalization table additive clustering similarity judgments integers tenenbaum rank weight stimuli class interpretation powers small numbers multiples large numbers middle numbers odd numbers numbers numbers generalizing generalization captions 
illustration bayesian approach generalization dimensional psychological space inspired shepard august 
sake simplicity intervals integer valued endpoints shown 
hypotheses size grouped bracket 
thickness height bar illustrating hypothesis represents hjx learner degree belief true consequential region observation curve top illustrates gradient generalization obtained integrating just consequential regions 
profile generalization concave regardless values hjx takes long hypotheses size bracket take probability 

effect example variability bayesian generalization assumptions strong sampling erlang prior 
filled circles indicate examples 
curve gradient generalization single example purpose comparison 
remaining graphs show range generalization increases function range examples 

effect number examples bayesian generalization assumptions strong sampling erlang prior 
filled circles indicate examples 
curve gradient generalization single example purpose comparison 
remaining graphs show range generalization decreases function number examples 

bayesian generalization multiple examples separable dimensions 
examples indicated filled circles 
contours show posterior probability increments 
black contours illustrate points cjx 
range generalization affected number examples variability dimension 

bayesian generalization number game example 
hypothesis space includes mathematically consequential subsets equal prior probabilities numbers odd numbers primes perfect squares perfect cubes multiples small number generalizing generalization powers small number numbers digit numbers digits equal numbers 

relative weight relations primitive features depends size set objects identify 
observers choose primitive feature match similar top stimulus left panel choose relational match right panel part relation shape identifies smaller subset objects relation different shapes 

