draft uc berkeley variational mcmc de freitas pedro jen rensen michael jordan stuart russell june uc berkeley computer science division soda hall berkeley ca usa jordan cs berkeley edu department mathematical modelling technical university denmark dk lyngby denmark phs imm dtu dk propose new class learning algorithms combines variational approximation markov chain monte carlo mcmc simulation 
naive algorithms variational approximation proposal distribution perform poorly approximation tends underestimate true variance features data 
solve problem introducing sophisticated mcmc algorithms 
algorithms mixture mcmc kernels random walk metropolis kernel block metropolis hastings mh kernel variational approximation proposal distribution 
mh kernel allows locate regions high probability eciently 
metropolis kernel allows explore vicinity regions 
algorithm outperforms variational approximations yields slightly better estimates mean considerably better estimates higher moments covariances 
outperforms standard mcmc algorithms locates regions high probability quickly speeding convergence 
adaptive mcmc algorithm iterates improving variational approximation improving mcmc approximation 
demonstrate algorithms problem bayesian parameter estimation logistic sigmoid belief networks 
mcmc simulation powerful accurate strategy inference learning gilks richardson spiegelhalter robert casella 
requires design complex proposal distributions applied new tasks 
algorithms take long converge mix poorly 
hand variational methods shown provide fast approximate estimates scenarios jaakkola jordan jordan ghahramani jaakkola saul 
rely simpli cations original problem order ensure mathematical tractability 
results algorithms yield poor estimates high order moments covariances kurtosis 
introduce class markov chain monte carlo mcmc algorithms variational approximations proposal distributions 
show naive algorithms exploiting property mix poorly solve problem introducing sophisticated mcmc kernels block sampling mixtures mcmc kernels 
particular mixtures variational kernels allow algorithm detect regions high probability quickly metropolis kernels enable explore neighbourhood regions 
resulting algorithm converges quickly regions high probability yields reasonable approximations entire distribution interest 
approach possible combine variational mcmc algorithms rigorous probabilistic setting exploit bene ts approaches simultaneously 
introduce adaptive variational mcmc scheme mcmc simulation improve variational approximation turn proposal distribution 
algorithm assists adaptively 
attempts combining speci approximation techniques simulation methods researchers statistics community combine laplace approximation simulation methods gilks 
laplace method approximate integration method truncated taylor expansions 
high dimensions require expensive computation cross derivative terms 
provide poor approximation integrand approximately log quadratic 
ghahramani beal showed variational approximation mixtures factor analyzers proposal importance sampler lead improvement accuracy results 
approach take general problems encountered importance sampling approach 
demonstrate approach task bayesian parameter estimation logistic sigmoidal belief networks latent variables 
problem interest reasons 
exhibits nonlinearity non gaussianity 
second includes problems logistic regression classi cation missing observations sub case 
third noise uninformative consequently careful applying model testing techniques cross validation 
motivates bayesian paradigm particular gaussian prior regularisation mechanism 
lastly type network important connections research carried area neural computation 
remainder organised follows 
probabilistic models estimation goals outlined section 
section variational approximations original models expectation maximisation em algorithm perform necessary computations 
novel strategy combines mcmc variational methods proposed section 
experimental results obtained method logistic bns section 
recommendations drawn section 
notation appears appendix 
model speci cation section probabilistic model parameter estimation belief networks bns 
networks provide convenient pictorial representation probability distributions factorised follows nx nx jx nx fx nx represents stacked set nodes denotes variable associated node denotes parent nodes node unknown parameters associated node shows simple bn nodes observed bn value nodes unknown 
cases show possible design algorithms estimate parameters 
formally consider countable set random variables partition set visible part hidden part fx shall assume sets measurements observed variables distribution random variable parameterised simplicity denote random variable realisation 
consequently express continuous probability distributions dx pr dx discrete distributions pr 
distributions admit densities respect underlying measure usually counting lebesgue denote densities 
example considering space lebesgue measure dx dx dx shall allow slight abuse terminology referring distribution 
fully observed belief network 
belief network hidden node right 
parameters treated hidden units bayesian framework 
dashed box represents markov blanket node continuous box template indicating copies number variables depends number parent nodes case belief network 
general cardinality parameters bayesian setting regarded hidden variables notational distinction hidden states distributional parameters hidden states 
shall restrict parameterisation conditional probability distributions bernoulli family logistic mapping jx exp exp exp assumed xed 
note assumption presentation purposes 
introduce extra node xed treat extra parameter 
complete speci cation bayesian model assume gaussian prior parameters prior independence 
goal analysis compute posterior distribution jx 
distribution easily derive quantities interest predictive distributions marginal distributions 
illustrated need distinguish scenarios fully observed networks networks hidden nodes 
fully observed bns shown left plot markov blanket nodes inside dashed box include parameters 
result problem parameter estimation bns simpli es logistic regression sub problems node parents 
posterior nodes computed bayes rule jx jx jx xc denotes number nodes parent 
ii bns hidden nodes hidden nodes introduce dependencies parameters model 
example right plot parameters depend parameters unknown 
compute posterior need marginalise hidden variables jx jx jx posterior distributions cases calculated analytically large integrals sums appearing denominators 
circumvent problem section introduce variational methods obtain approximate solutions 
methods require map original model simpli ed model amenable analytical computational treatment 
shall correct change model markov chain monte carlo simulation section 
variational approximation section presenting general variational framework parameter estimation 
enforce belief network topological constraints nally derive approximations parameter estimation logistic belief networks 
resulting approximations similar jaakkola jordan exception introduce extra parameter treat multimodality 
variational methods parameter estimation aim variational methods convert complex problem simpler tractable problem see example jordan 
simpler problem generally characterised decoupling degrees freedom original problem 
decoupling achieved introducing extra set parameters called variational parameters 
variational parameters optimised solution simpler problem resembles solution complex problem 
bounds convexity play important role variational paradigm 
situations including bns likelihood data easily evaluated 
know lower bound likelihood maximise bound obtain approximate solution 
lower bounds likelihood easily obtained jensen inequality log log xj log xj log arbitrary density hidden states respect lebesgue counting measure 
right hand side negative kullback leibler distance kl qkp term known entropy log distribution clear maximising lower bound equivalent minimising kullback leibler distance 
distribution yields tightest bound free form maximisation typically leads bounds evaluated chandler 
alternative approach choose parametric form right hand side equation easy evaluate 
variational parameters optimised get bound tight possible 
approach similar done statistical mechanics uses tractable energy function gibbs inequality calculate partition function normalising density bayes rule system intractable energy function zhang 
may impossible general choose speci functional form evaluation log xj tractable 
additional exibility introduced lower bounding xj chosen function xj denotes additional set variational parameters 
summarise variational approach involves steps 
introduce variational parameters conditional joint distribution hidden visible variables xj tractable 

introduce variational distribution parameters conditional marginal distribution visible variables tractable 
steps obtain unnormalised lower bound likelihood exp log xj log io bayes rule easily obtain lower bound posterior distribution jx exp log xj obtain lower bound evidence standard marginalisation bp implicitly replacing integrand normalising expression posterior distribution tractable lower bound integrated easily 
maximise resulting lower bound integral approximate true integral 
words replaced integration problem easier optimisation problem 
alternative approach obtain lower bound likelihood proposed jaakkola jordan 
method convexity jensen inequality 
particular fact geometric average probability distribution equal arithmetic average result likelihood lower bounded follows xj xj xj bp xj log entropy random variable distribution 
lower bound likelihood written follows bp xj dependencies variables resulted performing exact marginalisation replaced dependencies shared variational distribution 
shall bound equation general tractable 
compute parameters maximise lower bound evidence 
step carried coordinate ascent maximum likelihood 
expectation step compute expectation complete log likelihood old values variational parameters jx old old log 
maximisation step maximise respect variational parameters new new arg max 
go maximum number iterations required error tolerance reached 
em algorithm variational approximation 
algorithm shown dempster laird rubin 
algorithm guaranteed maximise lower bound evidence guaranteed maximise actual evidence 
monitoring convergence misleading 
bounds likelihood observed complete data chosen carefully existing empirical results suggest framework perform complex scenarios de freitas niranjan gee jaakkola jordan jaakkola jordan 
bns introduced previous section expectation complete log likelihood de ned jx old old log jx jx old old bq log exp log jx denotes hidden nodes fx bq bq 
sections show compute quantity case logistic belief networks 
variational approximation fully observed logistic bns analysing logistic bns lower bound likelihood data gaussian approximation jaakkola jordan follows jx exp tanh trivial apply bayes rule compute lower bound posterior distribution parameters jx jx jx corresponds right hand side equation 
conjugate analysis completing squares obtain recursive expressions mean variance gaussian posterior distribution instance equation compute variational parameters maximising lower bound evidence new arg max jx old log jx distributions gaussian take derivatives equate zero obtain recursive formula variational parameters jx old tr em algorithm computing variational approximation fully observed logistic bns shown 
variational approximations logistic bns hidden nodes obtain em update equations logistic networks hidden nodes rst calculate lower bound posterior distribution jx jx exp log jx exp child node initialise initialise iterations counter error tolerance tol tr compute tolerance em fully observed logistic bns 
proceeding previous section easily obtain recursive formulas tr obtain update equation variational distribution introduce parametric mean eld approximation fj hidden node represented independent bernoulli distribution 
nd optimal parameters need compute arg max jx old old bq log exp log jx bq jx old old log jx log bq jx old old log accomplish computing derivative equating zero 
doing rst notice bq log consequently exp exp jx old old jx old old em algorithm logistic bns hidden nodes analogous fully observed bns exception compute expectations respect include equation 
example observed node hidden parent second term right hand side equation equal zero yielding variational mcmc previous section showed variational methods map complex problem simpler problem apply methods exploit analytical properties functions consideration 
strategy course result biased estimates 
correct error resort mcmc simulation 
particular shall variational posterior distribution jx proposal distribution various mcmc samplers 
explain done need introduce basic notions mcmc simulation 
mcmc techniques set powerful simulation methods may applied solve integration optimisation problems large dimensional spaces gilks robert casella tierney 
types problem play fundamental role elds machine learning physics econometrics statistics decision analysis 
context maximum likelihood estimation mcmc techniques carrying necessary geyer thompson 
bayesian framework unknown variables data mcmc simulation adopted solve integration problems brooks gilks thomas spiegelhalter normalisation obtain posterior distribution jy prior likelihood yj normalising factor bayes theorem needs computed jy yj yj marginalisation joint posterior may interested marginal posterior jy expectation objective analysis obtain summary statistics form jy jy function interest integrable respect 
examples appropriate functions include conditional mean case conditional covariance jy jy 
emphasize dicult problem computing integrals restricted bayesian learning 
example statistical mechanics needs compute partition function system states hamiltonian potential kinetic energy exp kt boltzmann constant denotes temperature system 
turns basic problem equilibrium statistical mechanics compute sum integral continuum systems trace quantum mechanical systems baxter 
idea perfect monte carlo integration methods draw set samples ng target distribution posterior jx bayesian analysis obtain empirical distribution pn denotes delta dirac mass located consequently approximate integrals discrete sums follows estimate unbiased strong law large numbers surely converge 
lim variance satis es variance equal var central limit theorem yields convergence distribution error denotes convergence distribution robert casella section 
advantage monte carlo integration deterministic integration arises fact positions integration grid samples regions high probability 
hand main disadvantage simple monte carlo methods possible draw samples directly 
problem circumvented mcmc algorithms 
assuming draw samples proposal distribution key idea mcmc simulation design markov chain mechanisms cause proposed samples migrate empirical distribution approximates 
popular example class algorithms metropolis hastings mh algorithm hastings metropolis rosenbluth rosenbluth teller teller 
metropolis hastings step invariant distribution say proposal distribution say involves sampling candidate value current value 
markov chain moves acceptance probability minf remains 
pseudo code shown 
pseudo code assume proposal target distributions admit densities respect lebesgue counting measures 
transition kernel associated 
initialise set 
iteration sample sample 



min 




go 
metropolis hastings algorithm 
mh algorithm assuming lebesgue measure generality 



acceptance term 
min 


probability associated candidate accepted probability staying point 
rejection term 

fairly easy prove samples generated mh algorithm mimic samples drawn target distribution property known ergodicity 
construction 

satis es detailed balance condition reversibility 






follows measurable set 





construction mh algorithm admits invariant distribution 
show mh algorithm converges need ensure cycles state positive probability reached nite number steps irreducibility 
algorithm allows rejection follows aperiodic 
ensure irreducibility simply need sure 
entire state space 
conditions obtain convergence result equation tierney theorem page 
space small example bounded possible conditions prove uniform geometric ergodicity tweedie 
possible prove geometric ergodicity foster lyapunov drift conditions tweedie roberts tweedie 
properties mh algorithm worth mentioning 
firstly normalising constants target distribution required 
need know target distribution constant proportionality 
secondly pseudo code single chain easy simulate chains parallel 
success failure algorithm hinges choice proposal distribution 
demonstrated 
proposal simple random walk 

proposal narrow mode visited 
hand wide rejection rate high 
modes visited acceptance probability high chain said mix 
subsections show variational approximation proposal distribution improve mixing chains scenarios 
naive variational mcmc approach obvious immediate way improving variational approximation mcmc sample new candidates variational distribution 


jx target distribution mcmc approximation markov chain approximations obtained metropolis algorithm gaussian proposal distributions di erent variances 
case acceptance probability mh algorithm simpli es 
min 
jx bp jx jx bp 
jx min 


bp 
denotes importance weights 
type algorithm known independent mh algorithm closely related standard importance sampler geweke 
previous section pointed algorithm converge posterior distribution mild conditions 
state encouraging results metrics commonly variational literature jx unique invariant distribution markov chain follows relative entropy kullback leibler distance true posterior mcmc approximation converges zero number iterations increases cover thomas 
importance sampler independent mh algorithm known perform poorly high dimensions proposal distribution close target distribution geweke mengersen tweedie 
practice acceptance ratio usually tends zero approximately dimensions 
fact result proposition mengersen tweedie theorem independent mh algorithm converges uniform geometric rate exists constant jx jx supp case kk pk tv 
tv denotes total variation norm 
conversely exists set positive measure bound importance weights hold algorithm geometrically ergodic 
negative result proposition interesting 
bound importance weights regions high probability tails approach bound fail 
apply result proposition obtain corollary corollary uniform ergodicity naive variational mcmc independent mh algorithm logistic bns variational approximation proposal distribution converges uniformly geometric rate exp case kk pk tv converse result proposition applies 
proof 
target distribution variational approximation proper likelihood bounded possible values require ratio prior distribution proposal distribution bounded 
trivial see case condition satis ed dimensional case bound previous corollary satis ed variance prior distribution equal variance proposal distribution 
block mcmc approach previous section argued acceptance rate independent mh sampler low high dimensions 
problem certain extent exploit nature variational approximation propose update parameters blocks 
modi ed algorithm denote size th block denote number blocks shown 
uses notation 
initialise set 
iteration sample block mh step proposal distribution invariant distribution 
sample block mh step proposal distribution invariant distribution 
sample block bn bn mh step proposal distribution pn bn bn bn bn bn invariant distribution bn bn bn 

go 
block variational mh algorithm 
bn bn 
algorithm includes gibbs sampler special case proposals correspond full conditionals acceptance equal geman geman 
proposal distribution corresponds gaussian distribution mean subset elements mean original variational distribution covariance corresponding component original covariance 
transition kernel algorithm expression 

denotes th mh algorithm cycle 
kernel allows visit sets positive measure aperiodic algorithm simple convergence holds true number samples large 
obviously choosing size blocks poses trade samples components multi dimensional vector time chain may take long time explore target distribution 
problem gets worse correlation components increases 
alternatively samples components probability accepting large move tends low 
mixtures mcmc steps powerful property mcmc possible combine samplers mixtures cycles individual samplers tierney 
way global proposals explore vast regions parameter space local proposals discover ner details target distribution andrieu de freitas doucet andrieu doucet 
transition kernels invariant distribution 
cycle hybrid kernel mixture hybrid kernel transition kernels invariant distribution 
combine variational mcmc algorithm discussed section random walk metropolis blocks 
useful example target distribution narrow peaks 
variational proposal locks particular peak random walk allows explore space peak 
pseudo code mixture shown 
initialise set 
iteration sample perform block mh algorithm variational proposal 
perform block metropolis algorithm random walk proposal 

go 
mixture mcmc algorithm 
adaptive variational mcmc look chain top right notice chain stays state long time 
tells reduce variance proposal 
ideally automate process choosing proposal distribution possible 
shows results algorithm introducing soon illustrates point start variational distribution underestimates variance mcmc samples update proposal distribution 
obtain better variational approximation terms sparse sucient statistics monte carlo approximation 
iterations iterations iterations iterations adaptive mcmc 
variational approximation 
underestimates variance true posterior 
exhibits slightly di erent mean 
samples generated markov chain update variational proposal 
iterations new variational approximation provides better estimate mean 
eventually variational approximation closer target distribution mcmc algorithm converges 
warning allow adaptation take place nitely disturb stationary distribution consistency equation 
problem arises past information nitely violate markov prop erty transition kernel 
pr longer simplify pr 
gelfand sahu pathological example stationary distribution disturbed despite fact participating kernel stationary distribution 
avoid problem carry adaptation initial xed number steps standard mcmc simulation ensure convergence right distribution 
methods doing gelfand sahu 
rst idea running chains parallel sampling importance resampling rubin multiply kernels doing suppress 
approach uses approximation marginal density chain proposal 
second method simply involves monitoring transition kernel changing components example proposal distribution improve mixing 
subsection sophisticated elegant alternative 
adaptation regeneration denote irreducible markov chain space invariant distribution 
transition kernel 
key idea idea regeneration nd set state independent past history 
break markov chain independent markov chains called tours 
set satisfying condition called proper atom 
idea obtain independent segments prove convergence strong law large numbers 
applied design empirical convergence tests mykland tierney yu robert casella 
see possible perform adaptation time visit atom violating markov property chain gilks roberts sahu 
discrete state spaces state proper atom 
general state spaces set proper atom exists measure usually possible extend probabilistic structure chain construct atoms critical analysis markov chains general state spaces follows discrete counterpart tweedie 
signi cant contribution theory due athreya ney 
presenting strategy construction atoms termed splitting need assumptions 
shall able justify better presenting algorithm 
particular need assume possible nd function probability measure 

equation known condition 
construct atoms rst generating bernoulli variable success probability pr pr simulating follows modi cation ect transition kernel time chain regenerated transition mechanism independent current state split chain proper atom 
pair atom transition kernel note need condition ensure numerator second expression equation positive 
expected tour length 
ensure atom visited nitely nite time positive recurrence need assumption 
previous technique requires simulate simulating possible way avoid having know normalisation constant 
sample 
sample bernoulli distribution success probability pr modi cation ect transition kernel note construction split chain depends atom product eliminating need know normalising constant 
denote regeneration times adaptive mcmc algorithm gilks shown 
subsection deal issue choosing construct practical adaptive mcmc algorithms 

initialisation set sample 

iteration assuming current chain transition kernel atom proceed follows sample 

sample bernoulli distribution success probability 

set discard 
update past sample path sample probability measure proportional set 

go 
generic adaptive mcmc algorithm regeneration 
regeneration markov chain samplers proposed mykland regeneration applied speci variations metropolis hastings algorithm hastings metropolis gibbs sampler geman geman 
importantly applied hybrid samplers mixtures cycles transition kernels situation regeneration needs applied individual samplers mykland tierney 
focus application regeneration independent sampler 
dicult show see theorem mykland choices min 

min 

initialisation set sample 
rejection sampling 

iteration sample sample 


min 

sample compute 
equation 

update regeneration time adapt proposal 

adapt set mode discovered far 
discard 
sample 

instance rejection sampling 
repeat sample sample 

min 



go 
adaptive independent mh algorithm 
ensure condition holds true 
choices acceptance term transition kernel regeneration probability 
max 

max 


regeneration algorithm independent mh shown 
simulations performed experiments fully partially observed logistic bns 
nodes observed posterior unimodal symmetric 
allows compare algorithms evaluating distance estimates mean optimal mean 
likelihood fairly prior higher estimates close optimal mean 
notice optimal mean di erent generating mean 
illustrate model single parameter set generated observations 
repeated times time evaluated likelihood distribution discrete grid 
shown generating mean necessarily equal optimal mean 
non informative noise model amenable model testing techniques cross validation 
performed experiments multimodal distributions show performance algorithm terms approximating mean terms approximating entire posterior distribution 
likelihood data observations generated bernoulli logistic node single parameter set 
clearly observations recover true value parameter 
dealing uninformative noise model consequently standard cross validation tests expected perform 
unimodal models logistic model consisting child varying number parents generate sets data samples 
computed posterior approximations variational em algorithm block sampler variational proposal distribution random walk metropolis rw mcmc mixture variational kernel metropolis kernel 
repeated experiment times obtain estimates performance terms means error bars 
set random walk variance bias parameter bernoulli mean generating parameters uniformly random values 
chose fairly prior results samples shown figures respectively 
dimension variational mean mean mean gure shows relative log likelihood variational methods respect log likelihood random walk metropolis samples 
curves positive methods outperform metropolis algorithm 
addition mcmc mixture variational metropolis kernels provides better estimates mean di erent numbers parents 
clear algorithm outperforms algorithm turn outperforms standard variational algorithm 
performance rw relative log likelihood dimension variational mean mean mean mcmc mixture variational metropolis kernels provides better estimates mean samples 
note performance metropolis algorithm improved perform better 
recall random walk component worst perform similarly standard metropolis 
depends initialisation data set realisation 
perform depending initialised regions high probability 
course number samples goes nity rw algorithm approximate mean central limit theorem 
practical scenarios need reliable faster options 
lastly computational time em mcmc algorithms shown table 
experiment discussed performance methods terms approximating mean posterior 
want compute characteristics distribution variances kurtosis 
section show algorithm suited dicult problem 
samples samples dimension em mcmc em mcmc table computational time giga ops 
true posterior variational approximation mcmc rw approximation approximation random walk metropolis algorithm iterations bivariate model 
contour plot histogram mcmc samples indicates random walk spend considerable time regions low probability 
multimodal models experiment considered network parents hidden observed 
posterior bivariate modes 
modes need true posterior variational approximation mcmc var approximation approximation variational mcmc algorithm iterations bivariate model 
variational approximation allows locate region high probability 
symmetrical 
demonstration set generating parameters hidden observed nodes respective bernoulli means hidden variables 
set bias parameter number data prior 
posterior case evaluated numerically dimensional grid 
show contour curves 
gure shows contour plot rw mcmc histogram iterations variational approximation 
notice variational approximation ts closely modes 
notice random walk starts region low probability take long locate modes 
performance poor dealing posteriors elongated contours 
illustrates point naive variational mcmc algorithm locates modes fails explore support posterior 
mixture mcmc algorithm shown solves problem provides best solution methods 
true posterior variational approximation mcmc mix approximation approximation mixture mcmc algorithm iterations bivariate model 
variational component allows locate region high probability random walk allows explore neighbourhood region 
showed advantageous combine variational mcmc methods 
variational methods allow map problem consideration subset simpler problems 
solving subproblems obtain suboptimal distributions turn proposals complex sampling schemes 
pointed naive algorithms principle perform poorly variational approximation tends underestimate variance posterior distribution 
proposed sophisticated mcmc algorithms clearly able bene variational approximation outperform standard metropolis algorithms 
multimodal scenario worried approximating modes 
models multimodality arises result label permutation ability mode correct statistical solution 
case mixture models 
recognise complex situations sources multimodality need extend algorithms 
simple strategy compute variational approximations di erent initial conditions 
approximations parallel multiple mcmc mixture visit modes quickly 
tempering method described neal serve purpose jumping modes 
adaptive variational mcmc scheme works low dimensions scale dimensions approximately greater 
feel essential carry research direction ultimately represent high dimensional distributions mixture adapted better variational approximations 
large dimensional mixtures document retrieval may require megabytes store single sample hofmann puzicha 
storage requirements decrease considerably able store sucient statistics 
needless say better proposal distributions lead faster convergence improved results 
interesting research directions 
need consider algorithms exploit lower upper variational bounds 
believe allow locate modes jump eciently 
second need variational approximation approximate marginals 
possible apply idea implementing complex hierarchical bayesian schemes 
christophe andrieu beal arnaud doucet zoubin ghahramani tommi jaakkola kevin murphy 
notation symbols stacked vector vector th component missing entry matrix th row th column 
dimensional matrix size identity matrix dimension euclidean dimensional space 
dz distribution conditional distribution dz dy joint distribution dz distributed dz 
zj dz conditional distribution 
sigma eld subsets space 
operators functions transpose matrix inverse matrix tr trace matrix jaj determinant matrix indicator function set 
dz dirac delta function impulse function 
expectation random variable var variance random variable exp 
exponential function 
log 
logarithmic function base ln 
tv total variation norm tv sup inf 
standard probability distributions bernoulli br gaussian exp uniform ua dz andrieu doucet 

joint bayesian detection estimation noisy sinusoids reversible jump mcmc ieee transactions signal processing 
andrieu de freitas doucet 

robust full bayesian methods neural networks solla leen 
uller eds advances neural information processing systems mit press pp 

athreya ney 

new approach limit theory recurrent markov chains transactions american mathematical society 
baxter 

exactly solved models statistical mechanics academic press 
brooks 

markov chain monte carlo method application statistician 
chandler 

modern statistical mechanics oxford university press oxford uk 
cover thomas 

elements information theory wiley series telecommunications new york 
de freitas niranjan gee 

dynamic learning em algorithm neural networks journal vlsi signal processing systems 
dempster laird rubin 

maximum likelihood incomplete data em algorithm journal royal statistical society series 
gelfand sahu 

markov chain monte carlo acceleration journal computational graphical statistics 
geman geman 

stochastic relaxation gibbs distributions bayesian restoration images ieee transactions pattern analysis machine intelligence 
geweke 

bayesian inference econometric models monte carlo integration econometrica 
geyer thompson 

constrained monte carlo maximum likelihood estimation dependent data discussion journal royal statistical society 
ghahramani beal 

variational inference bayesian mixtures factor analysers solla leen 
uller eds advances neural information processing systems mit press pp 

gilks richardson spiegelhalter 
eds 
markov chain monte carlo practice chapman hall su 
gilks roberts sahu 

adaptive markov chain monte carlo regeneration journal american statistical association 
gilks thomas spiegelhalter 

language program complex bayesian modelling statistician 
hastings 

monte carlo sampling methods markov chains applications biometrika 
hofmann puzicha 

unsupervised learning dyadic data technical report tr international computer science institute 
jaakkola jordan 

variational methods qmr dt database journal arti cial intelligence 
jaakkola jordan 

bayesian parameter estimation variational methods statistics computing 
jordan ghahramani jaakkola saul 

variational methods graphical models machine learning 
mengersen tweedie 

rates convergence hastings metropolis algorithms annals statistics 
metropolis rosenbluth rosenbluth teller teller 

equations state calculations fast computing machines journal chemical physics 
tweedie 

markov chains stochastic stability springerverlag new york 
mykland tierney yu 

regeneration markov chain samplers journal american statistical association 
neal 

sampling multimodal distributions tempered transitions statistics computing 


irreducible markov chains non negative operators cambridge university press cambridge 
robert casella 

monte carlo statistical methods springer verlag new york 
roberts tweedie 

geometric convergence central limit theorems multidimensional hastings metropolis algorithms biometrika 
rubin 

sir algorithm simulate posterior distributions bernardo degroot lindley smith eds bayesian statistics oxford university press cambridge ma pp 

tierney 

markov chains exploring posterior distributions annals statistics 
zhang 

application gibbs feynman inequality mean eld calculations markov random elds ieee transactions image processing 

