bayesian parameter estimation variational methods tommi jaakkola michael jordan massachusetts institute technology university california cambridge ma berkeley ca november consider logistic regression model gaussian prior distribution parameters 
show accurate variational transformation obtain closed form approximation posterior distribution parameters yielding approximate posterior predictive model 
approach readily extended binary graphical model complete observations 
graphical models incomplete observations utilize additional variational transformation obtain closed form approximation posterior 
show dual regression problem gives latent variable density model variational formulation leads exactly solvable em updates 
bayesian methods number virtues particularly uniform treatment uncertainty levels modeling process 
formalism allows ready incorporation prior knowledge seamless combination knowledge observed data bernardo smith gelman heckerman 
elegant semantics comes sizable computational cost posterior distributions resulting incorporation observed data represented updated generally involves high dimensional integration 
computational cost involved carrying operations call question viability bayesian methods relatively simple settings generalized linear models mccullagh nelder 
concern particular generalized linear model logistic regression focus bayesian calculations computationally tractable 
particular describe exible deterministic approximation procedure allows posterior distribution logistic regression represented updated eciently 
show methods permit bayesian treatment complex model directed graphical model belief network node logistic regression model 
deterministic approximation methods develop known generically variational methods 
variational techniques extensively physics literature see parisi sakurai applications statistics 
roughly speaking objective methods transform problem interest optimization problem extra degrees freedom known variational parameters 
xed values variational parameters transformed problem closed form solution providing approximate solution original problem 
variational parameters adjusted optimization algorithm yield improving sequence approximations 
variational methods context graphical models see jordan 

brie sketch variational method develop 
study logistic regression model gaussian prior parameter vector 
variational transformation replaces logistic function adjustable lower bound gaussian form exponential quadratic function parameters 
product prior transformed likelihood yields gaussian expression posterior conjugacy optimize 
procedure iterated successive data point 
methods compared laplace approximation logistic regression cf 
spiegelhalter lauritzen closely related method utilizes gaussian approximation posterior 
anticipate discussion sections see variational approach advantage laplace approximation particular variational parameters gives variational approach greater exibility 
show exibility translates improved accuracy approximation 
variational methods contrasted sampling techniques method choice bayesian statistics thomas neal gilks 
sampling techniques enjoy wide applicability powerful evaluating multi dimensional integrals representing posterior distributions 
yield closed form solutions guarantee monotonically improving approximations 
precisely features characterize variational methods 
organized follows 
describe detail variational approximation method bayesian logistic regression 
followed evaluation accuracy method comparison laplace approximation 
extend framework belief networks considering complete data incomplete data 
consider dual regression problem show techniques lead exactly solvable em updates 
bayesian logistic regression logistic regression model jx logistic function binary response variable fx set explanatory variables 
represent uncertainty parameter values prior distribution assume gaussian possibly full covariance structure 
predictive distribution sjx sjx order utilize distribution need able compute posterior parameter distribution jd assume fs complete observation 
calculation intractable large consider variational approximation 
approach involves nding variational transformation logistic function transformed function approximate likelihood 
particular wish consider transformations combine readily gaussian prior sense gaussian prior conjugate prior transformed likelihood 
introducing type variational transformations purpose 
brief variational methods consider continuously di erentiable convex function 
provides example convex function 
convexity function guarantees de nition tangent line remains function 
may interpret collection tangent lines parameterized family lower bounds convex function cf 
convex duality rockafellar 
tangents family naturally parameterized locations 
point view approximating convex non linear function natural simpler tangent lines lower bound 
formulate little precisely tangent line jz follows 
terminology variational methods variational lower bound parameter known variational parameter 
lower bound considerably simpler linear case nonlinear function may attractive substitute lower bound note free adjust variational parameter location tangent accurate approximation possible point interest quality approximation degrades rate happens depends curvature 
function relatively low curvature case adjustable linear approximation quite attractive 
variational methods bayesian logistic regression illustrate variational methods type described transform logistic likelihood function form convex function tangent lines 
locations tangents indicated short vertical line segments 
readily combines gaussian prior conjugacy 
precisely transformed logistic function depend parameters quadratically exponent 
log logistic function log log log noting log convex function variable 
readily veri ed second derivatives behavior function shown 
discussed tangent surface convex function global lower bound function bound globally rst order taylor expansion variable log tanh note lower bound exact combining result eq 
exponentiating yields desired variational transformation logistic function sjx exp tanh 
introduce notation sjx exp sjx denotes variational lower bound logistic function 
lower bound longer normalized 
refer eq 
transformation conditional probability 
xed value fact recover exact value logistic function particular choice variational parameter 
maximizing lower bound respect yields substituting value back lower bound recovers original conditional probability 
values obtain lower bound 
true posterior jd computed normalizing sjx 
calculation feasible general form bound sjx sjx normalize variational approximation sjx 
gaussian choice gaussian variational form sjx normalized variational distribution gaussian 
note sjx lower bound true conditional probability variational posterior approximation proper density longer bound 
approximate bayesian update amounts updating prior mean prior covariance matrix posterior mean posterior covariance matrix 
omitting algebra nd updates take form pos xx pos pos single observation successive observations incorporated posterior applying updates recursively 
nished posterior covariance matrix depends variational parameter specify 
choose optimization procedure particular nd value yields tight lower bound eq 

fact variational expression eq 
lower bound important allows em algorithm perform optimization 
derive em algorithm appendix result closed form update equation post post expectation taken respect jd old variational posterior distribution previous value 
owing em formulation update corresponds monotone improvement posterior approximation 
empirically nd procedure converges rapidly iterations needed 
accuracy approximation considered sections 
summarize variational approach allows obtain closed form expression posterior predictive distribution logistic regression sjx sjx jd posterior distribution jd comes making single pass data set fd applying updates eq 
eq 
optimizing associated variational parameters step 
predictive lower bound jx takes form log jx log log complete observation signify parameters jd subscript refers posterior jd augmenting data set include point note nally variational bayesian calculations need carried sequentially 
compute variational approximation posterior probability jd introducing separate transformations logistic functions dj jx resulting variational parameters optimized jointly time 
believe sequential approach provides cleaner solution 
accuracy variational method logistic function shown variational approximation 
noted value variational parameter particular point approximation exact remaining values approximation lower bound 
logistic function solid line variational form dashed line 
di erence predictive likelihood variational approximation function described text 
integrating eq 
parameters obtain lower bound predictive probability observation 
tightness lower bound measure accuracy approximation 
assess variational approximation measure compared lower bound true predictive likelihood evaluated numerically 
note single observation evaluation predictive likelihood reduced dimensional integration problem sjx ective prior gaussian mean variance actual prior distribution mean covariance 
reduction ect accuracy variational transformation evaluating accuracy 
shows di erence true predictive probability variational lower bound various settings ective mean variance optimized separately di erent values fact variational approximation lower bound means di erence predictive likelihood positive 
emphasize tightness lower bound relevant measure accuracy 
tight lower bound predictive probability assures associated posterior distribution highly accurate converse true general 
words poor lower bound necessarily imply poor approximation posterior distribution point interest longer guarantees accuracy 
practice expect accuracy posterior important predictive probability errors posterior run risk accumulating course sequential estimation procedure 
defer evaluation posterior accuracy section comparisons related methods 
comparison methods sequential approximation methods yield closed form posterior parameter distributions logistic regression models 
method closely related spiegelhalter lauritzen refer approximation 
method laplace approximation utilize local quadratic approximation complete log likelihood centered prior mean 
parameter updates implement approximation similar spirit variational updates eq 
eq 
post xx post post 
additional adjustable parameters approximation simpler variational method expect lack exibility translate accurate posterior estimates 
compared accuracy posterior estimates methods context single observation 
simplify comparison utilized reduction described previous section 
accuracy method ected reduction suces purposes carry comparison simpler setting 
posterior probability interest jd computed various choices values prior mean prior standard deviation 
correct posterior mean standard deviations obtained numerically 
figures results 
plot signed di erences comparing obtained posterior means correct ones relative errors posterior standard deviations 
error measures left signed reveal systematic biases 
note posterior mean variational method guaranteed lower bound true mean 
guarantees predictive likelihood 
seen figures variational method yields signi cantly accurate estimates posterior means values prior variance 
posterior variance estimate variational estimate appear yield roughly comparable accuracy small value prior variance larger prior variance variational approximation superior 
note variational method consistently underestimates true posterior variance fact re ne approximation 
terms kl divergences approximate true posteriors variational method approximation roughly equivalent small prior variance variational method superior larger value prior variance 
shown 
extension belief networks belief network probabilistic model set variables fs identi ed nodes acyclic directed graph 
letting denote set parents node graph de ne joint distribution associated belief network product js refer conditional probabilities js local probabilities associated belief network 
note true posterior distribution recovered posterior computed dimensional reduced parameter error mean approximation variational relative error approximation variational errors posterior means function prior mean 
prior 
relative errors posterior standard deviations function 
prior distribution 
section extend earlier consider belief networks logistic regression de ne local probabilities models studied non bayesian setting neal saul jaakkola jordan 
introduce parameter vectors binary variable consider models local probability js logistic regression node parents simplify arguments sections consider augmented belief networks parameters treated nodes belief network see 
standard device belief network literature course natural bayesian formalism 
complete cases complete case refers data point variables fs observed 
data points complete cases methods developed previous section apply immediately belief networks 
seen follows 
consider markov associated parameters 
complete cases nodes markov blanket parameters observed shaded error mean approximation variational relative error approximation variational plots prior distribution 
diagram 
independence semantics belief networks implies posterior distributions parameters independent conditioned observed data 
problem estimating posterior distributions parameters reduces set independent subproblems bayesian logistic regression problem 
apply methods developed previous sections directly 
incomplete cases situation substantially complex incomplete cases data set 
incomplete cases imply longer markov parameters network 
dependencies arise parameter distributions di erent conditional models 
consider situation detail 
missing value implies observations arise marginal distribution obtained summing missing values unobserved variables 
marginal distribution mixture distribution mixture component corresponds particular con guration missing variables 
weight assigned component essentially posterior probability associated con guration spiegelhalter lauritzen 
note dependencies arising missing values observations network quite densely connected missing value node ectively connects neighboring nodes graph 
dense connectivity leaves little kl divergence approximation variational kl divergence approximation variational kl divergences approximate true posterior distribution function 
prior 

approximation methods visually identical curves 
structure exploited exact probabilistic computations networks tends exact probabilistic calculations intractable 
approach developing bayesian methods belief networks missing variables combines variational techniques 
particular augment transformation introduced earlier second variational transformation refer transformation 
purpose transformation convert local conditional probability form integrated analytically purpose transformation approximate ect marginalizing missing values associated parents 
intuitively transformation lls missing values allowing variational transformation complete data invoked 
result closed form approximation marginal posterior 
correct marginalization missing variables global operation ects conditional models depend variables marginalized 
variational approximation describe marginalization local operation acts individually relevant conditional models 
treating parameter parent node helps emphasize similarity variational transformations 
principal di erence parameter node single child general parents multiple children 
complete observation shaded variables markov blanket dashed line associated parameters observation value missing unshaded gure 
approximate marginalization consider problem marginalizing set variables joint distribution js performed marginalization exactly resulting distribution retain factorization original joint assuming involved conditionals seen js js js partitioned product set factors depend indexed indexed 
marginalization generally local operation individual node probabilities js 
maintaining locality desirable goal computational reasons achieved forgo exact marginalization consider approximations 
particular describe variational approximation preserves locality expense providing lower bound marginal probability exact result 
obtain desired variational transformation exploit convexity property 
particular sequence ng consider geometric average probability distribution 
known geometric average equal arithmetic average 
easily established invocation jensen inequality 
exploit fact follows 
consider arbitrary distribution rewrite marginalization operation way js inequality comes transforming average bracketed term respect distribution geometric average 
third line follows plugging form joint distribution exchanging order products 
logarithm multiplicative constant entropy variational distribution log log observations result eq 

note lower bound equation factored form original joint probability 
particular de ne transformation ith local conditional probability follows js js lower bound eq 
product transformations 
second note conditionals transformed distribution change ect transformed conditionals 
means dependencies variables resulted exact marginalization replaced ective dependencies shared variational distribution bound eq 
holds arbitrary variational distribution obtain tight bound need optimize 
practice involves choosing constrained class distributions optimizing class 
simplest form variational distribution completely factorized distribution yields variational bound traditionally referred mean eld approximation 
simpli ed approximation appropriate dense models relatively large number missing values 
generally consider structured variational distributions involving partial factorizations correspond tractable substructures graphical model cf 
saul jordan 
consider topic sections 
main constraint choice computational associated evaluation optimization additional constraint borne mind 
particular transformed conditional probabilities form subsequent transformation invoked yielding result tractable bayesian integral 
simple way meet constraint require variational distribution depend parameters 
discuss section case transformations simply involve products logistic functions behave transformation 
bayesian parameter updates derivation previous section shows approximate variational marginalization set variables viewed geometric average local conditional probabilities sjs sjs variational distribution missing values 
note transformations carried separately relevant conditional model variational distribution associated missing values transformations 
transformation eq 
approximate bayesian updates obtained readily 
particular conditioning data point missing components rst apply transformation 
ectively lls missing values resulting transformed joint distribution factorizes case complete observations 
posterior parameter distributions obtained independently parameters associated transformed local probabilities 
issues need considered 
transformed conditional probabilities cf 
eq 
products logistic functions complicated 
transformation method transforms logistic function exponential quadratic dependence parameters 
products transforms exponential quadratic dependence parameters 
approximate likelihood gaussian prior multivariate gaussian approximate posterior gaussian 
second issue dependence posterior parameter distributions variational distribution optimize variational parameters distribution case bounds tight possible particular set distribution maximizes lower bound 
optimization carried conjunction optimization parameters transformations logistic functions lower bounds 
show appendix fact approximations lower bounds implies devise em algorithm perform maximization 
updates derived appendix follows pos pos pos vector parents expectations taken respect variational distribution numerical evaluation section provide numerical evaluation proposed combination transformation transformation 
study simple graph consists single node parents contrast simple logistic regression case analyzed earlier parents observed distributed distribution 
distribution manipulate directly experiments essentially provides surrogate ects pattern evidence ancestral graph associated node cf 
spiegelhalter lauritzen 
interest posterior probability parameters associated conditional probability sjs 
suppose observe 
exact posterior probability parameters case jd js variational method focuses lower bounding evidence term brackets 
natural evaluate accuracy approximation evaluating accuracy marginal data likelihood js consider di erent variational approximations 
rst approximation variational distribution left unconstrained second approximation factorizes parents mean eld approximation 
emphasize cases variational posterior approximation parameters single gaussian 
results experiment shown figures 
gure displays curves corresponding exact evaluation data likelihood variational lower bounds 
number parents prior distribution taken zero mean gaussian variable covariance matrix 
symmetry gaussian distribution sigmoid function exact value cases 
considered choices 
rst case assumed factorize parents leaving single parameter speci es stochasticity 
similar setting arise applying mean eld approximation context general graph 
shows accuracy variational lower bounds function covariance matrix diagonal diagonal components set sample covariance matrix gaussian random vectors distributed 
results averaged independent runs 
choice scaling insure 
gures indicate variational approximations reasonably accurate little di erence methods 
see mean eld approximation unimodal deteriorates distribution changes factorized distribution mixture distribution 
speci cally jp uniform factorized distribution discussed parameter pm pure mixture distribution assigns probability mass di erent randomly chosen con gurations parents jp mpm parameter controls extent resembles pure mixture distribution 
illustrates accuracy variational methods function 
expected mean eld approximation deteriorates increasing rst variational approximation remains accurate 
dual problem logistic regression formulation eq 
parameters explanatory variables play dual symmetric role cf 
nadal parga 
bayesian logistic regression setting symmetry broken associating parameter vector multiple occurences explanatory variables shown 
alternatively may break symmetry associating single instance explanatory variable multiple realizations 
sense explanatory variables play role parameters functions continuous latent variable 
dual bayesian regression model latent variable density model binary response variable graphically dual interpretation single parameter node separate nodes required di erent realizations illustrated gure explain successive likelihood likelihood exact data likelihood solid line variational lower bound dashed line variational lower dotted line function stochasticity parameter 
sample covariance random vectors distributed 
observations latent variable density model single binary variable particularly interesting generalize response variable vector binary variables component distinct set parameters im associated 
latent variables remain dual interpretation note strictly speaking dual interpretation require assign prior distribution new parameters vectors simplicity omit consideration treat simply adjustable parameters 
resulting latent variable density model binary vectors akin standard factor analysis model see everitt 
model facilitate visualization high dimensional binary vectors tipping 
turn technical treatment latent variable model 
joint distribution jx jx conditional probabilities binary observables logistic regression models jx ij likelihood exact data likelihood solid line variational lower bounds dashed dotted lines respectively function mixture parameter 

bayesian regression problem 
dual problem 
em algorithm parameter estimation 
achieve exploit variational transformations 
transformations introduced conditional probability joint distribution optimized separately observation fs database consisting values binary output variables 
logistic regression case transformations change unwieldy conditional models simpler ones depend parameters quadratically exponent 
variational evidence product transformed conditional probabilities retains property 
consequently variational approximation compute posterior distribution latent variables closed form 
mean covariance posterior obtained analogously regression case giving variational parameters associated observation conditional model updated eq 
replaced vector parameters associated th conditional model 
solve step em algorithm accumulating sucient statistics parameters closed form posterior distributions corresponding observations data set 
omitting algebra obtain explicit updates parameters subscript denotes quantities pertaining observation note variational transformations arrive updates lower bounds step necessarily results monotonically increasing lower bound log probability observations 
desirable monotonicity property arise types approximation methods laplace approximation 
discussion exempli ed variational techniques setting bayesian parameter estimation 
variational methods exploited yield closed form expressions approximate posterior distributions parameters logistic regression 
methods apply immediately bayesian treatment logistic belief networks complete data 
showed combine mean eld theory variational transformation treat belief networks missing data 
variational techniques lead exactly solvable em algorithm latent variable density model dual logistic regression problem 
interest note variational method provides alternative standard iterative newton raphson method maximum likelihood estimation logistic regression algorithm known iterative reweighted squares irls 
advantage variational approach guarantees monotone improvement likelihood 
derivation algorithm appendix alternative perspective application variational methods bayesian inference see hinton van camp mackay 
authors developed variational method known ensemble learning viewed mean eld approximation marginal likelihood 
bernardo smith 
bayesian theory 
new york wiley 
heckerman geiger chickering 
learning bayesian networks combination knowledge statistical data 
machine learning 
everitt 
latent variable models 
cambridge university press 
gelman 
bayesian data analysis 
boca raton fl crc press 
gilks richardson spiegelhalter 
markov chain monte carlo practice 
london chapman hall 
hinton van camp 
keeping neural networks simple minimizing description length weights 
proceedings th annual workshop computational learning theory 
new york acm press 
jordan ghahramani jaakkola saul 
variational methods graphical models 
jordan ed learning graphical models cambridge ma mit press 
mccullagh nelder 
generalized linear models 
london chapman hall 
mackay 
ensemble learning hidden markov models 
unpublished manuscript 
department physics university cambridge 
available web wol ra phy cam ac uk mackay 

nadal parga 
duality learning machines bridge supervised unsupervised learning 
neural computation neal 
connectionist learning belief networks 
arti cial intelligence 
neal 
probabilistic inference markov chain monte carlo methods 
technical report crg tr university toronto 
parisi 
statistical eld theory 
redwood city ca addison wesley 
rockafellar 
convex analysis 
princeton university press 

variational methods statistics 
new york academic press 
saul jordan 
exploiting tractable substructures intractable networks 
touretzky mozer hasselmo eds advances neural information processing systems 
cambridge ma mit press 
saul jaakkola jordan 
mean eld theory sigmoid belief networks 
journal arti cial intelligence research 
spiegelhalter lauritzen 
sequential updating conditional probabilities directed graphical structures 
networks 
tipping 
probabilistic visualisation high dimensional binary data 
appear advances neural information processing systems 
thomas spiegelhalter gilks 
bugs program perform bayesian inference gibbs sampling 
bayesian statistics 
clarendon press 
optimization variational parameters optimize variational approximation eq 
context observation fs formulate em algorithm maximize predictive likelihood observation respect 
words nd maximizes right hand side sjx sjx em formalism achieved iteratively maximizing expected complete log likelihood old flog sjx expectation jd old 
derivative respect setting zero leads old monotonically decreasing function maximum obtained substituting old procedure repeated 
iteration yields better approximation sense eq 

holds 
sjx symmetric function assuming ect quality approximation 
parameter posteriors ll possible missing values observation fs employ transformations described text 
result joint distribution approximate marginalization factorizes complete observations 
posterior distributions parameters remain independent di erent conditional models computed separately 
jd js form posterior remains unwieldy bayesian logistic regression problem considered earlier 
proceeding analogously transform logistic functions eq 
corresponding conditional probabilities product obtain jd js hs hs efh js vector parents expectations respect variational distribution simplicity variational parameter vary independently con gurations missing values assumed con gurations 
choice naturally suboptimal primarily notational simplicity choice may necessary cases number missing values large 
linear parameters exponent eq 
consisting averages square respect variational distribution stays quadratic parameters multivariate gaussian prior conjugate likelihood posterior gaussian 
mean pos covariance pos posterior omit algebra pos pos pos note posterior depends distribution parameters 
optimization parameters shown appendix 
optimization variational parameters introduced variational parameters distribution missing values parameters corresponding logistic transformations 
metric optimizing parameters comes fact transformations associated parameters introduce lower bound probability observations 
maximizing lower bound nd parameter values yield accurate approximations 
attempt maximize right hand side log log log log js log contains observed settings variables 
fact joint distribution approximations factorizes complete cases 
similarly case simple bayesian logistic regression considered previously see appendix devise em algorithm maximize variational lower bound respect parameters parameters considered latent variables formulation 
step em algorithm nding posterior distribution latent variables described appendix consider detail step 
simplicity solve step phases rst variational distribution kept xed maximization second roles reversed 
start rst phase 
variational joint distribution factorizes problem nding optimal parameters separates independent problems concerning transformed conditionals 
optimization analogous simple bayesian logistic regression considered earlier 
di erences exist rst posterior obtained eq 
second additional expectation respect variational distribution di erences optimization analogous appendix won repeat 
part stage step new consider detail 
objective optimize keeping parameters xed previously obtained values 
similarly case construct em algorithm perform inner loop optimization old flog log js log rst expectation respect old factorizes conditional probabilities explained previously expectations component distributions old obtained directly eq 

insert form transformed conditional probabilities js de nition function 
clarity omit terms dependence variational distribution obtain old fh fh log fh fh refers expectation respect variational distribution second equation follows exchanging order mutually independent expectations fact log entropy see text 
recall notation binary vector parents proceeding maximize function respect explicate averages formula fh fh mean covariance respectively posterior old associated th conditional model 
simply inserting back expression function get old binary variables value assignment observation remaining variables averaged variational distribution assuming priori constraints form distribution maximizing boltzmann distribution see parisi exp variational distribution constrained case completely factorized distribution may longer expect nd maximizes eq 

locally optimal solution example sequentially solving old respect components factorized distribution 
technical note ml estimation standard maximum likelihood procedure estimating parameters logistic regression uses iterative newton raphson method nd parameter values 
method fast monotonic probability observations guaranteed increase iteration 
show derive monotonic fast estimation procedure logistic regression making variational transformation eq 

denote write log probability observations log jx log log variational lower bound exact parameters solved easily allows closed form solution xed variational log probability quadratic function 
parameters maximize successively solving updating yields chain inequalities prime signi es update assumed initially 
combined update leads monotonically increasing log probability 
addition closed form updates procedure comparable speed standard newton raphson alternative 

