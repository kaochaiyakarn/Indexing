models languages parallel computation david skillicorn queen university domenico talia universita della survey parallel programming models languages criteria assess suitability realistic portable parallel programming 
argue ideal model easy program software development methodology architecture independent easy understand guarantee performance provide accurate information cost programs 
criteria reflect belief developments parallelism driven parallel software industry portability efficiency 
consider programming models categories depending level abstraction provide 
conceal presence parallelism software level 
models software easy build port efficient predictable performance usually hard achieve 
spectrum low level models messy issues parallel programming explicit threads place express communication schedule communication software hard build portable usually efficient 
models near center spectrum exploring best tradeoffs expressiveness performance 
models achieved abstractness efficiency 
kinds models raise possibility parallelism part mainstream computing 
categories subject descriptors performance systems programming techniques programming languages language classifications general terms languages performance theory additional key words phrases general purpose parallel computation logic programming languages object oriented languages parallel programming languages parallel programming models software development methods taxonomy parallel computing years old roots traced back cdc ibm 
years parallel computing permitted complex problems solved high performance applications implemented traditional areas science engineering new application areas authors addresses skillicorn computing information science queen university kingston canada email skill ca talia isi cnr deis universita della cs italy email talia si deis 
permission digital hard copy part personal classroom granted fee provided copies distributed profit commercial advantage copyright notice title publication date appear notice copying permission acm copy republish post servers redistribute lists requires prior specific permission fee 
acm acm computing surveys vol 
june artificial intelligence finance 
despite successes promising parallel computing major methodology computer science parallel computers represent small percentage computers sold years 
parallel computing creates radical shift perspective surprising central part practical applications computing 
opinion past years wild optimism question parallelism answer extreme pessimism parallelism declining niche market time examine state parallel computing 
chosen examination parallel programming models 
doing addresses software development issues hardware performance issues 
discussing reasons parallel computing idea suggest failed important central 
section review basic aspects parallel computers software 
section discuss concept programming model list properties believe models parallel programming ought useful software development effective implementation 
section assess wide spectrum existing parallel programming models classifying meet requirements suggested 
reasons parallelism topic interest 
real world inherently parallel natural straightforward express computations real world parallel way way preclude parallelism 
writing sequential program involves imposing order actions independent executed concurrently 
particular order placed arbitrary barrier understanding program places order significant obscured 
arbitrary sequencing compiling difficult harder compiler infer code movements safe 
nature real world suggests right level abstraction design computation 
parallelism available computational performance available single processor getting performance parallel computers straightforward 
applications computationally bounded science grand challenge problems engineering weather forecasting 
new application areas large amounts computation put profitable data mining extracting consumer spending patterns credit card data optimization just time retail delivery 
limits sequential computing performance arise fundamental physical limits speed light 
hard tell close limits 
cost developing faster silicon gallium processors growing faster performance time performance increases obtained internal parallelism superscalar processors small scale 
tempting predict performance limits single processors near 
optical processors provide large jump computational performance decades applications quantum effects processors may provide large jump longer time period 
single processor speed improvements continue historical trend parallel computation skillicorn talia acm computing surveys vol 
june cost effective applications leading edge uniprocessors 
largely costs designing new generation uniprocessors drop newer technologies optical computation mature 
release new faster uniprocessor drives price previous generations putting ensemble older processors provides cost effective computation cost hardware required connect kept reasonable limits 
new generation processors provides decimal order magnitude increase performance modestly sized ensembles older processors competitive terms performance 
economics processor design production favor replication clever design 
effect part responsible popularity networks workstations low cost supercomputers 
reasons parallelism expect moved rapidly mainstream computing 
clearly case 
parts world parallel computing regarded marginal 
turn examining problems difficulties parallelism explain advantages led widespread 
conscious human thinking appears sequential appealing software considered sequential way program plot novel designing understanding debugging way 
property parallelism difficult course human cognition take place parallel way 
theory required parallel computation immature developed technology 
theory suggest directions limits technology 
result know representations parallel computations logics reasoning parallel algorithms effective real architectures 
long time understand balance necessary performance different parts parallel computer balance affects performance 
careful control relationship processor speed communication interconnect performance necessary performance balanced memory hierarchy performance 
historically parallel computers failed deliver small fraction apparently achievable performance taken generations particular architecture learn lessons balance 
parallel computer manufacturers designated high performance scientific numerical computing market larger high effectiveness commercial market 
high performance market small tended oriented military applications 
world events seen market predictable consequences profitability parallel computer manufacturers 
small market parallel computing meant parallel computers expensive sold increased risk manufacturers users dampening enthusiasm parallelism 
execution time sequential program changes constant factor moved uniprocessor 
true parallel program execution time change order magnitude moved parallel computation acm computing surveys vol 
june architecture families 
fundamental nonlocal nature parallel program requires interact communication structure cost communication depends heavily program interconnect arranged technology implement interconnect 
portability serious issue parallel programming sequential 
transferring software system parallel architecture may require amount including rebuilding software completely 
fundamental reasons best architecture family independent technological changes 
parallel software users expect continual changes computing platforms moment implies continual redesign rebuilding software 
lack long term growth path parallel software systems major reason failure parallel computation mainstream 
approaches parallelism driven bottom technological possibilities top theoretical elegance 
argue progress far best hope lies driving developments middle attacking problem level model acts interface software hardware issues 
section review basic concepts parallel computing 
section define concept model construct checklist properties model provide appropriate interface software architectures 
section assess large number existing models properties working concrete 
show models raise possibility long term portability performance 
suggests way provide missing growth path parallel software development mainstream parallel computing industry 

basic concepts parallelism section briefly review essential concepts parallel computers parallel software 
considering components parallel computers 
parallel computers consist building blocks processors memory modules interconnection network 
steady development sophistication building blocks arrangement differentiates parallel computer 
processors parallel computers increasingly exactly processors single processor systems 
technology possible fit chip just single processor considerable investigation components give greatest added value included chip processor 
communication interfaces relevant parallel computing 
interconnection network connects processors memory modules 
major distinction variants multiple instruction multiple data mimd architectures processor local memory accesses values memories network interconnection network connects processors memory 
alternatives called distributed memory mimd shared memory mimd respectively illustrated 
distributed memory mimd architectures differentiated total capacity interconnection networks total volume data transit network time 
example architecture processor mem skillicorn talia acm computing surveys vol 
june ory pairs called processing elements connected mesh requires number connections network processor matter large parallel computer member 
total capacity network grows linearly number processors computer 
hand architecture interconnection network hypercube requires number connections processor logarithmic function total size computer 
total network capacity grows faster linearly number processors 
important style parallel computer single instruction multiple data simd class 
single processor executes single instruction stream broadcasts instruction executed number data processors 
data processors interpret instruction addresses local addresses local memories global addresses modified adding local base address 
turn terminology parallel software 
code executing single processor parallel computer environment quite similar processor running multiprogrammed single processor system 
speak processes tasks describe code executing inside operating system protected region memory 
actions parallel program involve communicating remote processors memory locations takes time processors execute process time 
standard techniques multiprogramming apply processes involving remote communication ready execution suitable response received 
useful distinction virtual parallelism program number logically independent processes contains physical parallelism number processes active simultaneously course equal number processors executing parallel computer 
number communication actions occur typical parallel program processes interrupted sequential environment 
process manipulation expensive multiprogrammed environment increasingly parallel computers threads processes 
threads operating system protected memory region 
result context save context switch occurs 
threads safe making compiler responsible enforcing interaction possible threads come single parallel program 
processes communicate number different ways constrained course possible executing architecture 
main ways follows 
fig 

distributed memory mimd shared memory mimd architectures 
parallel computation acm computing surveys vol 
june message passing 
sending process packages message header indicating processor process data routed inserts interconnection network 
message passed network sending process continue 
kind send called nonblocking send 
receiving process aware expecting data 
indicates readiness receive message executing receive operation 
expected data arrived receiving process suspends blocks 
transfers shared memory 
shared memory architectures processes communicate having sending process place values designated locations receiving process read 
actual process communication straightforward 
difficult detecting safe put value location remove 
standard operating system techniques semaphores locks may purpose 
expensive complicates programming 
architectures provide full empty bits associated word shared memory provide lightweight high performance way synchronizing senders receivers 
direct remote memory access 
early distributed memory architectures required processor interrupted time request received network 
poor processor increasingly distributed memory architectures pair processors processing element 
application processor program computation messaging processor handles traffic network 
taken limit possible treat message passing direct remote memory access memories processors 
hybrid form communication applies distributed memory architectures properties shared memory 
communication mechanisms need correspond directly architecture provides 
straightforward simulate message passing shared memory possible simulate shared memory message passing approach known virtual shared memory 
interconnection network parallel computer mechanism allows processors communicate memory modules 
topology network complete arrangement individual links processing elements 
naturally represented graph 
diameter interconnection network topology maximum number links traversed pair processing elements 
forms lower bound worst case latency network longest time required pair processing elements communicate 
general observed latency greater implied topology congestion network 
performance parallel program usually expressed terms execution time 
depends speed individual processors arrangement communication ability interconnection network deliver 
speak cost program usually mean execution time cost 
context software development cost may include resources necessary develop maintain program 

models properties model parallel computation interface separating high level properties low level ones 
concretely model machine providing certain operations pro skillicorn talia acm computing surveys vol 
june gramming level requiring implementations operations architectures 
designed separate software development concerns effective parallel execution concerns provides abstraction stability 
abstraction arises operations model provides higher level underlying architectures simplifying structure software reducing difficulty construction 
stability arises software construction assume standard interface long time frames regardless developments parallel computer architecture 
time model forms fixed starting point implementation effort transformation system compiler run time system directed parallel computer 
model insulates issues concern software developers concern implementers 
furthermore implementation decisions require target program 
model just machine models exist different levels abstraction 
example programming language model sense provides simplified view underlying hardware 
hard compare models neatly range levels abstraction involved high level models emulated lower level models 
connection models low level model naturally emulate different higher level ones high level model naturally emulated different low level ones 
explicitly distinguish programming languages models asynchronous order preserving message passing follows 
executing parallel program extremely complex object 
consider program running processor system large unusual today 
active threads moment 
conceal latency communication memory access processor probably multiplexing threads number active threads times larger say 
thread may communicate threads communication may asynchronous may require synchronization destination thread 
possible interactions progress instant 
state program large 
program gives rise executing entity significantly description entity manageable humans 
put way great deal actual arrangement executing computation ought implicit capable inferred static description program having stated explicitly 
implies models parallel computation require high levels abstraction higher sequential programming 
just conceivable construct modestly sized sequential programs assembly code newest sequential architectures increasingly difficult 
probably impossible write modestly sized mimd parallel program processors assembly code cost effective way 
furthermore detailed execution behavior particular program architecture style different detailed execution 
abstractions conceal differences architecture families necessary 
hand model great practical interest efficient method executing programs written 
models intellectually computationally expensive find way execute reasonable efficiency large number parallel architec parallel computation acm computing surveys vol 
june tures 
model useful address issues abstraction effectiveness summarized set requirements skillicorn 
model parallel computation properties 
easy program 
executing program complex object model hide details programmers able manage intellectually creation software 
possible exact structure executing program inserted translation mechanism compiler run time system programmer 
implies model conceal 
decomposition program parallel threads 
program divided pieces execute distinct processors 
requires separating code data structures potentially large number pieces 
mapping threads processors 
program divided pieces choice piece placed processor 
placement decision influenced amount communication place pair pieces ensuring pieces communicate lot placed near interconnection network 
may necessary ensure particular pieces mapped processors special hardware capability example high performance floating point functional unit 
communication threads 
nonlocal data required communication action kind generated move data 
exact form depends heavily designated architecture processes ends arrange treat consistently process wait data come 
synchronization threads 
times computation pair threads larger group know jointly reached common state 
exact mechanism target dependent 
enormous potential deadlock interaction communication synchronization 
optimal decomposition mapping known exponentially expensive compute 
nature communication introduces complexity 
communication actions involve dependencies receiving prepared block sender 
means chain communications processes converted deadlocked cycle actions single additional process containing say receiving action process chain sending action 
correctly placing communication actions requires awareness arbitrarily large fraction global state computation 
experience shown deadlock parallel programs easy create difficult remove 
requiring humans understand programs level detail effectively rules scalable parallel programming 
models ought simple possible 
close fit possible natural way express program demanded programming language 
programs may mean parallelism explicit program text 
applications naturally expressed concurrent way implies apparent parallel structure program need related actual way parallelism exploited execution 
software development methodology 
previous requirement implies large gap information pro skillicorn talia acm computing surveys vol 
june vided programmer semantic structure program detailed structure required execute 
bridging requires firm semantic foundation transformation techniques built 
ad hoc compilation techniques expected problems complexity 
large gap specifications programs addressed firm semantic foundations 
existing sequential software exceptions built standard building blocks algorithms 
correctness programs properly established subjected various test regimes designed increase confidence absence disastrous failure modes 
methodology testing debugging extend portable parallel programming major reasons new degree freedom created partitioning mapping increases state space tested debugging requires interacting state space simple checkpoints difficult construct programmer access designated architectures program eventually execute test software architectures 
number mundane reasons example reproducibility systems communication networks effectively nondeterministic sheer volume data testing generates 
verification program properties construction unwieldy practical 
process aiming build software correct construction long term 
calculational approaches advocated sequential programming workable large 
essential parallel programming remain goals practice medium term 
great deal parallel software produced far numerical scientific kind development methodology issues priority 
reasons 
numerical computing essentially linear algebra program structures regular naturally related mathematics derive 
software relatively simpler newer commercial applications 
second longterm maintainability emphasized research nature software 
programs intended short term generating results obsolete suggesting new problems attacked new techniques 
contrast commercial software development costs business edge creates 
architecture independent 
model architecture independent programs migrated parallel computer parallel computer having modified nontrivial way 
requirement essential permit widespread software industry parallel computers 
computer architectures comparatively short life spans speed processor interconnection technology developing 
users parallel computing prepared see computers replaced years 
furthermore new parallel computer resemble replaces 
software scratch happens cost effective usually happens today 
parallel computation useful possible insulate software changes underlying parallel computer changes substantial 
requirement means model features parallel computation acm computing surveys vol 
june particular style parallel computer 
requirement easy satisfy isolation sufficiently model satisfies difficult requirements 
easy understand 
model easy understand teach impossible educate existing software developers 
parallelism mainstream part computing large numbers people proficient 
parallel programming models able hide complexities offer easy interface greater chance accepted 
generally easy tools clear goals minimal preferable complex ones difficult 
properties ensure model forms effective target software development 
useful time model implemented effectively range parallel architectures 
requirements 
guaranteed performance 
model guaranteed performance useful variety parallel architectures 
note mean implementations extract performance designated architecture 
problems level performance high possible architecture unnecessary especially obtained expense higher development maintenance costs 
implementations aim preserve order apparent software complexity keep constants small 
fundamental constraints architectures communication properties understood 
architectures categorized power sense architecture powerful execute arbitrary computation expected performance text program basic properties designated architecture 
reason reduced performance general congestion arises individual actions program collective behavior actions 
powerful architectures resources principle handle congestion having impact program performance powerful ones 
powerful architecture class consists shared memory mimd computers distributed memory mimd computers total interconnection network capacity grows faster number processors fast log number processors 
computers arbitrary computation parallelism time executed way product pt called preserved valiant 
apparent time computation preserved real implementation communication memory access imposes latencies typically proportional diameter interconnection network 
time dilation causes compensated fewer processors multiplexing threads original program preserving product time processors 
cost implementation indirect parallelism program designated architecture property known parallel slackness 
architectures richly connected class powerful scale increasing proportion resources devoted interconnection network hardware 
worse interconnection network typically customized part architecture far expensive part 
second class architectures distributed memory mimd computers total interconnection network capacity grows linearly number processors 
computers scalable require constant number communication skillicorn talia acm computing surveys vol 
june links processor local neighborhoods processors unaffected scaling constant proportion resources devoted interconnection network hardware 
implementing arbitrary computations machines achieved loss performance proportional diameter interconnection network 
computations time processors actual cost diameter interconnection network goes wrong emulating arbitrary computations architectures step processors generate communication action 
total capacity proportional interconnection network communications entire capacity steps worst case 
communication actions attempted window steps avoided entire program slowed factor compensate 
class necessarily introduces reduced performance executing computations communicate heavily 
architectures class scalable powerful previous class 
third class architectures simd machines scalable emulate arbitrary computations poorly 
inability small constant number different actions step skillicorn 
scalable architectures powerful powerful architectures scalable 
guarantee performance architectures results imply amount communication allowed programs reduced factor proportional diameter realistic parallel computers factor log computations regular processors fewer different operations moment simd architectures considered viable designated architectures 
amount communication program carries reduced ways reducing number simultaneous communication actions reducing distance travels 
attractive think distance reduced clever mapping threads processors arbitrary programs 
heuristic algorithms goal placement maximum locality expensive execute guarantee results 
models limit frequency communication restricted local placement easy compute guarantee performance full range designated parallel computers 
cost measures 
program design driven explicitly performance concerns 
execution time important processor utilization cost development important 
describe collectively cost program 
interaction cost measures design process sequential software construction relatively simple 
sequential machine executes speed proportional design decisions change asymptotic complexity program consideration computer eventually run 
target decision changes nature tuning algorithm choice 
words construction process phases decisions algorithms asymptotic costs program affected decisions arrangements program text constants front asymptotic costs affected knuth 
neat division parallel software development parallel computation acm computing surveys vol 
june small changes program text choice designated computer capable affecting asymptotic cost program 
real design decisions model cost operations visible stages software development exact arrangement program designated computer decided 
intelligent design decisions rely ability decide algorithm better algorithm particular problem 
difficult requirement model violate notion abstraction 
hope determine cost program information computer execute insist information required minimal actual computation cost tedious practical 
say model cost measures possible determine cost program text minimal designated computer properties number processors information size values input 
essentially view cost theoretical models parallel complexity pram karp ramachandran 
requirement contentious 
requires models provide predictable costs compilers optimize programs 
way parallel software regarded today reiterate design possible 
ability design parallel software construction remain black art engineering discipline 
requirement cost measures behaved respect modularity 
modern software developed pieces separate teams important team need know details interface pieces 
means possible give team resource budget cost goal met team meets individual cost allocation 
implies cost measures compositional cost easily computable cost parts convex possible reduce cost increasing cost part 
naive parallel cost measures fail meet requirements 
implications requirements model quite demanding subsets strongly tension 
example models easy build programs hard compile efficient code low level models hard build software easy implement efficiently 
requirements metric classify assess models 
level abstraction models provide primary basis categorizing 
acts surrogate simplicity model model needs said details thread structure points communication synchronization take place 
level abstraction correlates quality software development methodology calculations programs powerful semantics clean 
extent structure program implementations constrained structure program text closely related properties guaranteed performance cost measures 
levels constraint program structure 
models allow dynamic process thread structure restrict communication 
new thread generate communication models restrict communication particular syntactic block limit program 
models guarantee communication gener skillicorn talia acm computing surveys vol 
june ated program overrun total communication capacity designated parallel computer 
dynamic process creation involves decisions run time system impossible define cost measures design 
mean course impossible write efficient programs models dynamic process structure impossible improve efficiency design clever compilation techniques 
means programs written model perform poorly straightforward detect ones 
programs potentially avoided programming discipline ruled model terms 
models static process thread structure impose syntactic limits communication permit programs written perform badly communication overruns 
hand usually possible define cost measures overruns statically predicted program structure 
models static process thread structure guarantee performance suitably limiting frequency communication actions written program 
follows possible define cost measures 
summary structure program static amount communication bounded model guarantee performance allow performance predicted cost measures 
control structure communication secondary basis categorizing models 
choice priorities classification reflects view parallel programming aim mainstream part computing 
specialized areas requirements may important 
example domain high performance numerical computing program execution times quadratic worse size problem 
setting performance penalty introduced execution distributed memory mimd computer mesh topology say may insignificant compared flexibility unrestricted communication model 
probably model satisfies potential users parallelism 
models satisfy preceding requirements candidates general purpose parallelism application parallelism wide problem domains mccoll 

overview models turn assessing existing models criteria outlined previous section 
models developed ambitious goal general purpose parallelism criticism say fail meet requirements 
goal provide picture state parallel programming today perspective seeing far general purpose parallelism reasonable get 
covered models parallel computation tried include introduce significant ideas sense history models 
give complete description model concentrate important features provide comprehensive 
important papers programming models languages reprinted skillicorn talia 
models decreasing order abstraction categories 
models parallelism completely 
models describe purpose program achieve parallel computation acm computing surveys vol 
june purpose 
software developers need know program build execute parallel 
models necessarily relatively simple programs need complex sequential ones 
models parallelism explicit decomposition programs threads implicit mapping communication synchronization 
models software developers aware parallelism expressed potential programs know parallelism applied run time 
models require programs express maximal parallelism algorithm implementation reduces degree parallelism fit designated architecture time working implications mapping communication synchronization 
models parallelism decomposition explicit mapping communication synchronization implicit 
models require decisions breaking available pieces relieve software developer implications decisions 
models parallelism decomposition mapping explicit communication synchronization implicit 
software developer break pieces consider best place pieces designated processor 
locality marked effect communication performance inevitably requires awareness designated processor interconnection network 
hard software portable different architectures 
models parallelism decomposition mapping communication explicit synchronization implicit 
software developer making implementation decisions fine scale timing decisions avoided having system deal synchronization 
models explicit 
software developers specify detail implementation 
noted earlier extremely difficult build software models correctness performance achieved attention vast numbers details 
categories models degree control structure communication categories models thread structure dynamic models thread structure static communication limited models thread structure static communication limited 
categories models common paradigms 
figures show classification models parallel computation way 
explicit best models parallel computation programmers need aware parallelism 
hiding activities required execute parallel computation means software developers carry existing skills techniques sequential software development 
course models necessarily implementer job difficult transformation compilation run time systems infer structure eventual program 
means deciding skillicorn talia acm computing surveys vol 
june specified computation achieved dividing appropriately sized pieces execution mapping pieces scheduling communication synchronization 
time widely believed automatic translation program implementation effective starting ordinary sequential imperative language 
great deal invested parallelizing compilers approach defeated complexity determining aspect program essential simply artifact sequential expression 
acknowledged highly automated translation process practical begins carefully chosen model expressive 
inferring details required efficient architecture independent implementation possible difficult models guarantee performance 
consider models high level abstraction subcategories permit dynamic process thread structure communication static process thread struc explicit parallelism implicit dynamic structure higher order functional haskell concurrent rewriting obj maude interleaving unity implicit logic languages ppp reduce opera palm concurrent constraint languages static structure algorithmic skeletons cole darlington static communication limited structure homomorphic skeletons bird meertens formalism cellular processing languages carpet cdl crystal parallelism explicit decomposition implicit dynamic structure dataflow sisal id explicit logic languages concurrent prolog parlog ghc delta prolog strand multilisp static structure data parallelism loops fortran variants modula data parallelism types parallel sets match move gamma pei apl moa static communication limited structure data specific skeletons scan dataparallel nesl decomposition explicit mapping implicit dynamic structure static structure bsp logp static communication limited structure 
mapping explicit communication implicit dynamic structure coordination languages linda sdl non message communication languages pcn compositional virtual shared memory annotated functional languages rpc dp cedar concurrent clu dp static structure graphical languages enterprise parsec code contextual coordination languages ease linda opus static communication limited structure communication skeletons communication explicit synchronization implicit dynamic structure process networks actors concurrent aggregates darwin external oo abcl abcl pool emerald concurrent smalltalk objects processes argus presto nexus active messages movie static structure process networks static dataflow internal oo mentat static communication limited structure systolic arrays alpha explicit dynamic structure message passing pvm mpi shared memory fork java thread packages rendezvous ada sr concurrent static structure occam pram 
classification models parallel computation 
parallel computation acm computing surveys vol 
june ture unlimited communication limit amount communication progress moment 
dynamic structure 
popular approach describing computations declarative way desired result specified saying result computed set functions equations 
result computation solution usually fixed point equations 
attractive framework develop software programs amenable formal reasoning equational substitution 
implementation problem find mechanism solve equations 
higher order functional programming treats functions terms computes values reduction calculus allowing stored data structures passed arguments returned results 
example language allows higherorder functions haskell hudak fasel 
haskell includes typical features functional programming user defined types lazy evaluation pattern matching list comprehensions 
furthermore haskell parallel functional system provides module facility 
actual technique higherorder functional languages computing function values called graph reduction peyton jones lester 
functions expressed trees common subtrees shared subfunctions graphs 
computation rules select graph substructures reduce simpler forms replace larger graph structure 
computation rules applied graph remains result computation 
easy see graph reduction approach parallelized principle rules applied nonoverlapping sections graph independently simultaneously 
multiple processors search reducible parts graph independently way depends structure graph inferred compiler 
example expression exp exp exp exp arbitrary expressions evaluated threads may independently evaluate exp exp values computed simultaneously 
unfortunately simple idea turns quite difficult effectively 
computations contribute final result executed doing wastes resources alters semantics program nonessential piece fails terminate 
example functional languages form conditional clearly exactly values needed known value known 
evaluating prevents redundant hand critical path computation compared evaluating speculatively 
things worse say fails terminate values false 
evaluating speculatively causes program terminate evaluation order 
difficult find independent program pieces known required compute final result quite sophisticated analysis program 
actual structure graph changes dramatically evaluation difficult load balancing handle spawning new subtasks communication effectively 
parallel graph reduction limited success shared memory distributed skillicorn talia acm computing surveys vol 
june computers effectiveness distributed memory computers unknown 
models simple allow software development transformation guarantee performance 
happens execution determined dynamically run time system cost measures sense practically provided 
concurrent rewriting closely related approach rules rewriting parts programs chosen way 
programs terms describing desired result 
rewritten applying set rules subterms repeatedly rules applied 
resulting term result computation 
rule set usually chosen terminating infinite sequence rewrites confluent applying rules overlapping subterms gets result order position rules applied difference final result 
examples models obj goguen winkler goguen functional language semantics equational logic maude 
example lincoln gives flavor approach 
functional module polynomial differentiation assuming existence module represents polynomials usual actions 
lines eq rewrite rules familiar elementary calculus 
line ceq conditional rewrite rule 
fmod poly der protecting polynomial op der var poly poly op der var mon poly var int var nznat poly mon 
eq der der der eq der der der eq der der ceq der 
eq der 
eq der 
endfm expression der computed parallel soon multiple places rewrite rule applied 
simple idea emulate parallel computation models 
models kind simple allow software development transformation guarantee performance dynamic allow useful cost measures 
interleaving third approach derives multiprogramming ideas operating systems models concurrency transition systems 
computation expressed set subcomputations commute evaluated order repeatedly considerable freedom implementing system decide actual structure executing computation 
quite hard express computation form considerably easier allowing piece computation protected guard boolean valued expression 
informally speaking semantics program form guards evaluated subprograms guards true evaluated 
completed process begins 
guards determine sequence computation having guards form step intent model weakest guards see darlington hudak fasel kelly peyton jones thompson 
see meseguer meseguer winkler winkler 
parallel computation acm computing surveys vol 
june say pieces fit 
idea lies unity alternative considers independence statements action systems 
unity unbounded nondeterministic iterative transformations computational model proof system 
unity program consists declaration variables specification initial values set multiple assignment statements 
step execution assignment statement selected nondeterministically executed 
example program program initially assign consists assignments selected nondeterministically executed 
selection procedure obeys fairness rule assignment executed infinitely 
rewriting approaches interleaving models simple guaranteed performance cost measures possible 
implicit logic languages exploit fact resolution process logic query contains activities performed parallel de 
main types inherent parallelism logic programs parallelism parallelism 
parallelism exploited unifying subgoal head clauses parallel 
instance subgoal solved matching clauses 

parallelism exploited unifying parallel subgoal head clauses 
logic programming see lloyd 
parallelism divides computation goal threads solves single subgoal parallel 
instance goal solved 
subgoals solved parallel 
minor forms parallelism search parallelism unification parallelism parallelism exploited respectively searching clause database unification procedure 
implicit parallel logic languages provide automatic decomposition execution tree logic program network parallel threads 
done language support static analysis compile time run time 
explicit annotations program needed 
implicit logic models include ppp fagin despain process model reduce model opera palm 
models differ view parallelism designated architectures varied mainly designed implemented mimd machines talia 
implement parallelism models thread strategies 
thread models single goal solved starting thread 
subtree models search tree divided subtrees thread associated subtree 
different approaches correspond different grain sizes thread models grain size fine subtree models parallelism grain size medium coarse 
approaches discussed section implicit parallel logic languages highly 
hard guarantee performance performance may achieved 
cost measures see chandy misra gerth radha muthukrishnan 
see back back 
skillicorn talia acm computing surveys vol 
june provided implicit logic languages highly dynamic 
constraint logic programming important generalization logic programming aimed replacing pattern matching mechanism unification general operation called constraint satisfaction saraswat 
environment constraint subset space possible values variable interest take 
programmer explicitly parallel constructs program defines set constraints variables 
approach offers framework dealing domains herbrand terms integers booleans lloyd 
concurrent constraint logic programming computation progresses executing threads concurrently communicate placing constraints global store synchronizes checking constraint entailed store 
communication patterns dynamic predetermined set threads thread interacts 
threads correspond goal atoms activated dynamically program execution 
concurrent constraint logic programming models include cc saraswat chip clp language van clp jaffar lassez 
parallel logic models concurrent constraint languages dynamic allow practical cost measures 
static structure 
way infer structure compute program insist program fundamental units components implementations predefined 
words programs built connecting ready building blocks 
approach natural advantages 
building blocks raise level abstraction fundamental units programmers 
hide arbitrary amount internal complexity 
building blocks internally parallel composable sequentially case programmers need aware programming parallel 
implementation building block needs done architecture 
implementation done specialists time energy devoted making efficient 
context parallel programming building blocks come called skeletons cole underlie number important models 
example common parallel programming operation sum elements list 
arrangement control communication exactly computing maximum element list similar operations 
observing special cases reduction provides new abstraction programmer implementer alike 
furthermore computing maximum element array tree different computing list concept reduction carries potential applications 
observing classifying regularities important area research parallel programming today 
overview classification skeletons basel algorithm classification scheme burkhart 
time restrict attention algorithmic skeletons encapsulate control structures 
idea skeleton corresponds standard algorithm algorithm fragment skeletons composed sequentially 
software developers select skeletons want put 
compiler library writer chooses encapsulated algorithm implemented intra parallelism exploited possible target architecture 
parallel computation acm computing surveys vol 
june briefly mention important algorithmic skeleton approaches 
pisa parallel programming language uses set algorithmic skeletons capture common parallel programming paradigms pipelines worker farms reductions 
example worker farms modeled means farm constructor follows 
farm int data int result data result result data farm skeleton executed number workers executed parallel processes emitter collector 
worker executes function data partition 
similar skeletons developed cole computed cost measures parallel architecture 
similar sort skeletons reduce map pairs pipelines farms done darlington group imperial college darlington 
algorithmic skeletons simple 
programs expressed compositions skeletons provided expressiveness programming language open question 
approaches previously described addresses explicitly natural way develop algorithmic skeleton programs higher level abstraction directly skeleton level 
hand guaranteed performance skeletons possible chosen care cost measures provided 
static communication limited structure 
skeleton approaches bound amount communication takes place usually incorporate awareness geometric information 
model homomorphic skeletons data types approach developed bird meertens formalism skillicorn 
skeletons model particular data types set lists set arrays set trees 
homomorphisms data type expressed instance single recursive highly parallel computation pattern arrangement computation steps implementation needs done datatype 
consider pattern computation communication shown 
list homomorphism computed appropriately substituting associative 
see 

skeleton computing arbitrary list homomorphisms 
skillicorn talia acm computing surveys vol 
june example sum maximum length sort id id binary max function returns id merge template scheduling individual computations communications reused compute different list homomorphisms replacing operations done part template 
furthermore template compute homomorphisms bags multisets slightly weaker conditions operations slots commutative associative 
communication required skeletons deducible structure data type implementation needs construct embedding communication pattern interconnection topology designated computer 
communication requirements mild example easy see list homomorphisms require existence logarithmic depth binary tree designated architecture interconnection network 
communication take place nearest neighbors constant time 
homomorphic skeletons built standard types sets bags skillicorn lists bird malcolm spivey trees gibbons arrays molecules skillicorn graphs singh 
homomorphic skeleton approach simple method construction data type homomorphisms automatically generates rich environment equational transformation 
communication pattern required type known standard topology type 
implementations guaranteed performance built designated computers interconnection topologies standard topology embedded 
complete schedule computation communication determined advance implementer cost measures provided skillicorn cai 
cellular processing languages execution model cellular automata 
cellular automaton consists possibly infinite dimensional lattice cells 
cell connected limited set adjacent cells 
cell state chosen finite alphabet 
state cellular automaton completely specified values variables cell 
state single cell simple structured variable takes values finite set 
states cells lattice updated simultaneously discrete time steps 
cells update values transition function takes input current state local cell limited collection nearby cells lie bounded distance known neighborhood 
simple neighborhoods cell lattice cellular processing languages eckart carpet talia cdl allow cellular algorithms described defining state cells typed variable record typed variables transition function containing evolution rules automaton 
furthermore provide constructs definition pattern cell neighborhood 
languages implement cellular automaton simd spmd program depending designated architecture 
spmd single program multiple data approach cellular algorithms implemented collection medium grain processes mapped different processing elements 
parallel computation acm computing surveys vol 
june process executes program transition function different data state cells 
processes obey parallel local rule results global transformation automaton 
communication occurs neighboring cells communication pattern known statically 
allows scalable implementations guaranteed performance mimd simd parallel computers 
cost measures provided 
model takes geometric arrangement explicitly account crystal 
crystal functional language added data types called index domains represent geometry locality arrangement 
feature distinguishing crystal languages geometric annotations index domains transformed transformations reflected computational part programs 
crystal simple possesses transformation system functional semantics transformations index domains 
index domains flexible way incorporating target interconnection network topology derivations crystal provides set cost measures guide derivations 
formal approach lead interesting developments area jay shapely types 
key model details implementation implicit text program 
seen approaches 
relies run time system find exploit parallelism second relies predefined building blocks knowledge built compiler replace program operations implementations piece piece 
parallelism explicit second major class models parallelism explicit programs software developers need explicit computations divided pieces pieces mapped processors communicate 
main strategies implementing decomposition depend making decomposition mapping computationally possible effective 
temporal spatial locality assume low cost context switch particular decomposition affect performance 
decomposition effective simple algorithm compute 
second skeletons natural mapping designate processor topologies skeletons structure data program uses 
dynamic structure 
dataflow expresses computations operations may principle size usually small explicit inputs results 
execution operations depends solely data dependencies operation computed inputs computed moment determined runtime 
operations mutual data dependency may computed concurrently 
operations dataflow program considered connected paths expressing data dependencies data values flow 
considered collections order functions 
decomposition implicit compiler divide graph representing computation way 
cut edges places data move processor 
processors execute operations order depends solely ready moment 
temporal context execution single operation see yang choo 
skillicorn talia acm computing surveys vol 
june advantage temporal locality 
operations direct dependence executed widely different times possibly different processors advantage spatial locality 
result decomposition little direct effect performance caveats apply 
decomposition done automatically decomposing programs smallest operations clustering get pieces appropriate size designated architecture processors 
random allocation operations processors performs dataflow systems 
communication explicit programs 
occurrence name result operation associated compiler places name input operation 
really threads threads contain single instruction communication effectively unsynchronized 
dataflow languages taken different approaches expressing repetitive operations 
languages id sisal mcgraw mcgraw order functional single assignment languages 
syntactic structures looking loops create new context execution loop body imperative languages variable name may assigned context 
example sisal loop single assignment semantics written follows 
returns value sum sisal dataflow languages parallelism visible sense loops expected opportunities parallelism 
example loop bodies scheduled simultaneously results collected 
dataflow languages simple natural software development methodology 
provide guaranteed performance sisal performs competitively best fortran compilers shared memory architectures mcgraw 
performance distributed memory architectures competitive 
scheduling done dynamically runtime cost measures possible 
explicit logic languages programmers specify parallelism explicitly shapiro 
called concurrent logic languages 
examples languages class parlog gregory delta prolog pereira nasr concurrent prolog shapiro ghc ueda strand foster taylor 
concurrent logic languages viewed new interpretation horn clauses process interpretation 
interpretation atomic goal viewed process conjunctive goal cn process network logic variable shared subgoals viewed communication channel processes 
exploitation parallelism achieved enrichment logic language prolog set mechanisms annotation programs 
mechanisms instance annotation shared logical variables ensure instantiated subgoal 
example model parallelism utilized parlog concurrent prolog languages csp communicating sequential processes model 
particular communication channels implemented parlog concurrent prolog means logical variables shared subgoals 
languages guard concept handle nondeterminism way csp delay communication parallel processes commitment reached 
parallel computation acm computing surveys vol 
june program concurrent logic language finite set guarded clauses gn bm 
clause head set gi guard bi body clause 
operationally guard test successfully evaluated head unification clause selected 
symbol commit operator conjunction guard body 
guard empty commit operator omitted 
declarative reading guarded clause true gi bi true 
process interpretation solve necessary solve guard gi resolution successful bm solved parallel 
languages require programmers explicitly specify annotations clauses solved parallel talia 
example parlog clause separators control search candidate clause 
group separated clauses tried parallel 
clauses tried clauses precede clauses 
instance suppose relation defined sequence clauses 

clauses tested parallel clause tested clauses 
concurrent logic languages extend application areas logic programming artificial intelligence system level applications program annotations require different style programming 
weaken declarative nature logic programming making exploitation parallelism responsibility programmer 
point view classification concurrent logic programs parlog programs define parallel execution explicitly means annotations 
furthermore parlog allows dynamic creation threads solve subgoals irregular structure programs limit communication threads 
symbolic programming language parallelism explicit programmer multilisp 
multilisp halstead language extension lisp opportunities parallelism created futures 
language implementation correspondence threads futures 
expression returns suspension value immediately creates new process evaluate allowing parallelism process computing value process value 
value computed value replaces 
futures give model represents partially computed values especially significant symbolic processing operations structured data occur 
attempt result suspends value computed 
futures class objects passed regardless internal status 
construct creates computation style dataflow model 
fact futures allow eager evaluation controlled way fits fine grained eager evaluation dataflow laziness higher order functional languages 
futures dynamic thread creation programs dynamic structure communication values expressions limited 
static structure 
turning models static structure skeleton concept appears time single data structures 
glance monolithic operations objects data type doing item list array programming skillicorn talia acm computing surveys vol 
june model limited expressiveness 
turns powerful way describing interesting algorithms 
data parallelism arose historically attempt computational pipelines 
algorithms analyzed situations operation applied repeatedly different data separate applications interact 
situations exploit vector processors dramatically reduce control overhead repetition pipeline stalls guaranteed occur independence steps 
development simd computers quickly realized vectorizable code simd code independent computations proceed simultaneously sequentially 
simd code efficiently executed mimd computers vectorizable code situations usefully exploited wide range parallel computers 
code involves arrays seen abstractly instances maps application function element data structure 
having abstraction interesting ask operations useful consider applied data structure 
data parallelism general approach programs compositions monolithic operations applied objects data type producing results type 
distinguish approaches describing parallelism parallel loops monolithic operations data types 
consider fortran addition forall loop iterations loop body conceptually independent executed concurrently 
example forall statement forall parallel computer executed parallel 
care taken ensure loops locations example indexing element array different index expression 
checked automatically general fortran dialects kind place responsibility programmer check 
loops maps single data object 
fortran dialects fortran tseng high performance fortran hpf high performance fortran language specification steele start kind parallelism add direct data parallelism including constructs specifying data structures allocated processors operations carry data parallel operations reductions 
example hpf parallel language fortran fortran simd fortran includes align directive specify certain data distributed way certain data 
instance hpf align aligns ensures elements indices placed processor 
furthermore distribute directive specifies mapping data processors example hpf distribute block block specifies processors considered dimensional array points associate processors array blocked fashion 
hpf offers directive inform compiler operations loop executed independently parallel 
example code asserts share memory space 
hpf independent parallel computation acm computing surveys vol 
june related languages ii andre andre thomas larus 
converge skeleton approaches example darlington group developed fortran extension uses skeletons darlington 
similar approach latest language modula family modula heinz 
modula supports forall style loops data types loop body executes independently loop ends barrier synchronization 
compiled intermediate language similar functionality hpf 
data parallel languages data types arrays developed 
examples parallel setl flynn hummel kelly hummel parallel sets kilian match move gamma banatre pei 
parallel setl imperative looking language data parallel operations bags 
example inner statement matrix multiplication looks 
outer set braces bag comprehension generating bag containing values expression colon values implied text colon 
bag reduction fold summing values 
gamma language dataparallel operations finite sets 
example code find maximum element set specifies pair elements may replaced set element provided value larger value models arrays derive apl fortran 
include mathematics arrays moa mullin array theory 
data parallel languages simplify programming operations require loops lower level parallel languages written single operations revealing compiler try infer pattern intended programmer 
sufficiently careful choice data parallel operations program transformation capability achieved 
natural mapping data parallel operations architectures simple types guaranteed performance cost measures possible 
static communication limited structure 
data parallel languages previous section developed primarily program construction mind 
set similar languages inspiration primarily architectural features 
origins typically pay attention amount communication takes place computing operation 
thread structures fixed communication takes place defined points size data involved communications small fixed 
wide variety languages developed basic operations data parallel list operations inspired architecture connection machine 
included map operation form reduction fixed set operators scans parallel prefixes permutation operations 
approximately chronological order models scan blelloch goldman data parallel language hatcher quinn quinn hatcher scan vector model nesl blelloch blelloch blelloch greiner 
skillicorn talia acm computing surveys vol 
june data parallel languages models simple fairly 
instance extension language incorporates features simd parallel model 
data parallelism implemented defining data parallel kind 
programs map variables particular data type defined parallel keyword poly separate processing elements 
way processing element executes parallel statement instance specified data type 
architectures data parallel languages provide implementations guaranteed performance accurate cost measures 
weakness choice operations basis efficiently implemented basis formal software development methodology 
decomposition explicit models kind require programs specify pieces divided placement pieces processors way communicate need described explicitly 
static structure 
examples class locality ensures placement matter performance 
bulk synchronous parallelism bsp model interconnection network properties captured architectural parameters 
bsp machine consists collection processors local memory connected interconnection network properties interest time barrier synchronization rate continuous randomly addressed data delivered 
bsp parameters determined experimentally parallel computer 
bsp program consists threads divided supersteps 
superstep consists computation processor locally held values global message transmission processor set barrier synchronization 
superstep results global communications visible processor local environment 
superstep shown 
maximum local computation step takes time maximum number values sent received processor total time superstep hg see mccoll skillicorn valiant 

bsp superstep 
parallel computation acm computing surveys vol 
june network parameters easy determine cost program 
time bound depends randomizing placement threads randomized adaptive routing bound communication time 
bsp programs decomposed threads placement threads done automatically 
communication implied placement threads synchronization takes place program 
model simple fairly lacks software construction methodology 
cost measures give real cost program architecture known 
current implementation bsp uses spmd library called fortran 
library provides operations put data local memory remote process get data remote process synchronize 
illustrate small program compute prefix sums int int int left right bsp left sizeof int bsp sync right bsp bsp pid bsp bsp put bsp pid right left sizeof int bsp sync bsp pid right left right bsp left return right bsp bsp calls needed process refer variables remote processes name allocated heap stack storage 
related approach logp culler uses similar threads local contexts updated global communications 
logp barrier synchronization 
logp model intended model capture technological reality parallel computation 
logp models parallel computations parameters latency overhead bandwidth communication number processors 
set programming examples designed logp model implemented cm parallel machine evaluate model usefulness 
logp model powerful bsp bilardi bsp simpler style preferred 
mapping explicit models class require programs specify programs decomposed pieces pieces placed provide abstraction communication actions pieces 
hardest part describing communication necessity labeling ends communication action say belong ensuring communication actions properly matched 
number communications large parallel program tedious burden software developers 
models class try reduce burden decoupling ends communication providing higher level abstractions patterns communication providing better ways specifying communication 
dynamic structure 
coordination languages simplify communication separating computation aspects programs communication aspects providing separate language specify communication 
separation computation communication orthogonal particular coordination style applied sequential language 
best known example linda ahuja carriero car skillicorn talia acm computing surveys vol 
june gelernter replaces point point communication large shared pool data values placed processes retrieved associatively 
shared pool known tuple space 
linda communication model contains communication operations removes tuple tuple space arity values fields filling remaining fields retrieved tuple read rd copies tuple tuple space places tuple tuple space 
example read operation rd canada usa searches tuple space tuples elements element canada element usa middle element type variable basic operations linda provides eval operation implicitly creates new process evaluate tuple insert tuple space 
linda operations decouple send receive parts communication sending thread know receiving thread exists 
model finding tuples associative matching implementations typically compile away patterns visible compile time 
linda model requires programmers manage threads program reduces burden imposed managing communication 
unfortunately tuple space necessarily implemented guaranteed performance model provide cost measures worse linda programs deadlock 
important issue software development methodology 
address issue high level programming environment called linda program builder implemented support design development linda programs ahmed 
environment guides user program design coding monitoring execution linda software 
communication languages reduce overheads managing communication communication ways fit naturally threads 
example arbeit campbell treats message passing communication channels memory mapped 
certain message variables different threads behaves message transfer 
pcn foster foster tuecke compositional hide communication single variables 
attempt read variables blocks thread value placed thread 
approaches similar full empty bits variables old idea coming back prominence multithreaded architectures 
particular pcn program composition notation language simple concepts concurrent composition single assignment variables 
pcn single assignment variables called definitional variables 
concurrent composition allows parallel execution statement blocks specified specifying concurrent composition pieces composition mapped processors 
processes share definitional variable communicate 
instance parallel composition producer consumer processes producer consumer communicate regardless location parallel computer 
pcn mapping processes processors specified programmer annotating programs location functions defining mappings virtual physical topologies 
logical extension mapping communication memory virtual shared memory abstraction pro parallel computation acm computing surveys vol 
june vided program single shared address space regardless real arrangement memory 
requires remote memory compiled messages effected messages run time 
far results suggested approach scalable ongoing research area 
annotated functional languages compiler job easier allowing programmers provide extra information suitable ways partition computation pieces place kelly 
reduction rules apply communication synchronization induced placement follow way pure graph reduction 
example kind language hudak 
functional language lazy evaluation expression evaluated demand 
allows user control evaluation order explicit annotations 
communication synchronization implicit provides mapping notation specify expressions evaluated processor 
expression followed annotation proc evaluated processor identified proc 
example expression self self denotes computation subexpression neighbor processor parallel execution 
remote procedure call rpc mechanism extension traditional procedure call 
rpc procedure call different processes caller receiver 
process calls remote procedure process receiver executes code procedure passes back caller output parameters 
rendezvous rpc synchronous cooperation form 
execution procedure caller blocked reactivated arrival output parameters 
full synchronization rpc limit exploitation high degree parallelism processes compose concurrent program 
fact process calls remote procedure process caller process remains idle execution terminates execute operation execution partially limit effect new rpc systems lightweight threads 
languages remote procedure call mechanism dp hansen cedar swinehart concurrent clu cooper hamilton 
static structure 
graphical languages simplify description communication allowing inserted graphically higher structured level 
example language enterprise lobe szafron classifies program units type generates communication structure automatically type 
metaphor office program units communicating secretary example 
parsec wagner allows program units connected set predefined connection patterns 
code newton browne high level dataflow language computations connected graphically firing rule result passing rule associated computation 
decomposition models explicit communication visible simpler describe 
particular communication patterns available chosen applicability reasons efficiency performance guaranteed cost measures 
coordination languages contexts extend linda idea 
weaknesses linda provides single global tuple space pre see chin mccoll li hudak wilkinson 
skillicorn talia acm computing surveys vol 
june modular development software 
model extends linda including ideas occam language ease zenith 
ease programs multiple tuple spaces called contexts may visible threads 
threads access particular context known contexts take properties occam channels 
threads read write data contexts linda tuple spaces associative matching reads inputs 
second set primitives move data context relinquish ownership data retrieve data context remove context 
operations pass guarantee data referenced thread time 
ease properties linda easier build implementations guaranteed performance 
ease helps decomposition allowing process structuring style occam 
related language douglas extension setl paradigm computing sets aggregates 
adds linda style tuple spaces data type treats class objects 
put way linda resembles data parallel language bags data type associative matching selection operation bags 
linda seen extending setl languages new data type extending languages skeletons 
language kind derived fortran opus mehrotra haines task data parallelism communication mediated shared data abstractions 
autonomous objects visible subset tasks internally sequential method object active time 
kind generalization monitors 
static communication limited structure 
communication skeletons extend idea building blocks communication skillicorn 
communication skeleton interleaving computation steps consist independent local computations communication steps consist fixed patterns communication topology 
patterns collections paths topology functions broadcast channel 
shows communication skeleton computation steps interleaved different communication patterns 
model blend ideas bsp algorithmic skeletons concepts adaptive routing broadcast supported new architectural designs 
model 
communication skeleton 
parallel computation acm computing surveys vol 
june moderately architecture independent communication skeletons built assuming weak topology target embedding results build implementations targets richer interconnection topologies 
provide guaranteed performance cost measures 
communication explicit models class require communication explicit reduce burden synchronization associated 
usually done having asynchronous semantics messages delivered sender depend time delivery delivery multiple messages may order 
dynamic structure 
process nets resemble dataflow sense operations independent entities respond arrival data computing possibly sending data 
primary differences operations individually decide response data arrival individually decide change behavior 
lack global state exists implicitly dataflow computations 
important model class actors agha vidal 
actor systems consist collections objects called actors incoming message queue 
actor repeatedly executes sequence read incoming message send messages actors identity knows define new behavior governs response message 
names actors class objects may passed messages 
messages delivered asynchronously unordered 
guaranteed performance cost measures actors possible total communication actor program may bounded 
actor model highly distributed compilers serialize execution achieve execution efficiency conventional processors 
effective compiler transformations eliminate creation types actors change messages sent actors processor function calls kim agha 
actual cost executing actor program indeterminate specification actors mapped processors threads 
different kind process net provided language darwin eisenbach patterson eisenbach calculus 
language provides semantically founded configuration subset specifying ordinary processes connected communicate 
process net configuration language binding semantics communication connections dynamic 
weaknesses actor model actor processes message queue sequentially lead bottlenecks 
extensions model address issue proposed concurrent aggregates chien chien dally agha 
concurrent aggregates ca object oriented language suited exploit parallelism finegrain massively parallel computers 
unnecessary sources serialization avoided 
aggregate ca homogeneous collection objects called representatives grouped referenced single aggregate name 
aggregate multiaccess may receive messages simultaneously object oriented languages actor model abcl 
concurrent aggregates incorporates innovative features delegation intra aggregate addressing class messages user continuations 
delegation allows behavior aggregate constructed incrementally aggregates 
skillicorn talia acm computing surveys vol 
june tra aggregate addressing cooperation parts aggregate possible 
model extends actor model avoid unnecessary serializations 
actor space computationally passive container actors acts context matching patterns 
fact model uses communication model destination patterns 
patterns matched listed attributes actors actor spaces visible actor space 
messages sent arbitrary member group broadcast members group defined pattern 
turn external oo models 
actors regarded existing regardless communication status 
superficially similar approach quite different underneath extend sequential object oriented languages thread active time 
way called external object orientation allow multiple threads control highest level language 
objects retain traditional role collecting code logically belongs 
object state act communication mechanism altered method executed thread observed method executed part thread 
second approach call internal object orientation encapsulates parallelism methods object top level language appears sequential 
closely related data parallelism 
return second case concentrate external oo models languages 
interesting external objectbased models abcl yonezawa abcl watanabe pool america black emerald black concurrent smalltalk 
languages parallelism assigning thread object asynchronous message passing reduce blocking 
object language influenced design emerald 
emerald entities objects passive data active 
object consists parts name representation data set operations optional process run parallel invocations object operations 
active objects emerald moved processor 
move initiated compiler programmer simple language constructs 
primary design principles abcl object concurrent language practicality clear semantics message passing 
types message passing defined past 
mode operates synchronously past modes operate asynchronously 
message passing mechanisms abcl provides distinct modes ordinary express correspond different message queues 
give example past type message passing ordinary express modes respectively obj msg obj msg obj receiver object msg sent message 
abcl independent objects execute parallel actor model messages processed serially object 
message passing abcl programs may take place concurrently message arrive object simultaneously 
limits parallelism objects 
extension abcl abcl reflection introduced 
objects processes exploit parallelism external object oriented languages principal ways objects unit parallelism assigning processes object defining processes components language 
approach languages active objects process bound parallel computation acm computing surveys vol 
june particular object created 
approach different kinds entities defined objects processes 
process bound single object perform operations required satisfy action 
process execute objects changing address space invocation object 
object oriented models discussed approach systems argus liskov presto bershad second approach 
case languages provide mechanisms creating controlling multiple processes external object structure 
argus supports coarse grain medium grain objects dynamic process creation 
argus guardians contain data objects procedures 
guardian instance created dynamically call creator procedure explicitly mapped processor creator parameters processor expense dynamic process creation reduced maintaining pool unused processes 
new group processes created pool emptied 
models parallelism implemented top object organization explicit constructs defined ensure object integrity 
worth noticing models developed programming coarse grain programs distributed systems tightly coupled fine grain parallel machines 
active messages approach decouples communication synchronization treating messages active objects passive data 
essentially message consists parts data part code part executes receiving processor message transmitted 
message changes process arrives destination 
synchronization process receiving message send corresponding receive 
approach movie system faigle language environments machine chien dally dally dally 
static structure 
internal object oriented languages parallelism occurs single methods 
mentat programming language mpl parallel objectoriented system designed developing architecture independent parallel applications 
mentat system integrates data driven computation model object oriented paradigm 
datadriven model supports high degree parallelism object oriented paradigm hides parallel environment user 
mpl extension supports intra parallelism 
compiler run time support language designed achieve high performance 
language constructs mapped model computation model underlying mentat medium grain data driven model programs represented directed graphs 
vertices program graphs computation elements perform function 
edges model data dependencies computation elements 
compiler generates code construct execute data dependency graphs 
parallelism mentat largely transparent programmer 
example suppose vectors consider statements vect op add vect op add vect op add mentat compiler run time system detect additions data dependent executed parallel 
result automati skillicorn talia acm computing surveys vol 
june cally forwarded final addition 
result forwarded caller associated approach programmer granularity partitioning decisions mentat class definition constructs compiler run time support manage communication synchronization grimshaw grimshaw 
static structure 
systolic arrays architectures processing elements cells process data dimensional pipelined fashion 
analogy systolic dynamics heart systolic computers perform operations rhythmic incremental repetitive manner kung pass data neighbor cells directions 
particular computing element computes incremental result systolic computer derives final result interpreting incremental results entire array 
parallel program systolic array specify data mapped systolic elements data flow elements 
high level programmable arrays allow development systolic algorithms definition inter parallelism cell cell data communication 
clearly principle rhythmic communication distinguishes systolic arrays parallel computers 
high level programmability systolic arrays creates flexible systolic architecture penalties occur complexity possible slowing execution due problem data availability 
high level programming models necessary promoting widespread programmable systolic arrays 
example language alpha de programs expressed recurrence equations 
transformed systolic form regarding data dependencies defining affine vector space geometrically transformed 
explicit category models hide detail decomposition communication 
generation models parallel computation level designed single architecture style explicitly managed 
dynamic structure 
models provide particular paradigm handling partitioning mapping communication 
models tried general provide multiple paradigms example pi dally wills wills providing sets primitives style communication 
models guaranteed performance cost measures task software construction difficult amount detail computation 
set models general kind programming languages orca bal sr andrews olsson andrews 
orca object language uses shared data objects interprocess communication 
orca system hierarchically structured set abstractions 
lowest level reliable broadcast basic primitive writes replicated structure take effect rapidly system 
level abstraction shared data encapsulated passive objects replicated system 
parallelism orca expressed explicit process creation 
new process created fork statement fork proc name params cpu number 
part optionally specifies processor run child process 
parameters specify shared data objects communication parent child processes 
synchronizing resources sr resource concept 
re parallel computation acm computing surveys vol 
june source module contain processes 
resource dynamically created create command processes communicate semaphores 
processes belonging different resources communicate restricted set operations explicitly defined program procedures 
larger set models programming languages single communication paradigm 
consider paradigms message passing shared memory rendezvous 
message passing basic communication technology provided distributed memory mimd architectures message passing systems available machines 
interfaces low level sends receives specify message exchanged process identifier address 
quickly realized messagepassing systems look distributed memory architecture natural build standard interfaces improve portability message passing programs 
example mpi message passing interface dongarra message passing interface forum provides rich set messaging primitives including point point communication broadcasting ability collect processes groups communicate group 
mpi aims standard message passing interface parallel applications libraries dongarra 
point point communications send receive primitives mpi send buf datatype dest 
mpi recv buf datatype source 
mpi provides primitives collective communication synchronization mpi barrier mpi bcast mpi gather 
version mpi provision process creation mpi version additional features active messages process startup dynamic process creation provided 
architecture independent message passing models developed allow transparent networks workstations 
principle networks unused compute power exploited 
practice large latencies involved communicating workstations low performance parallel computers 
models workstation messagepassing include systems pvm butler lusk 
models exactly inter multiprocessor message passing systems typically larger grain processes help conceal latency address heterogeneity processors 
example pvm parallel virtual machine gained widespread acceptance programming toolkit heterogeneous distributed computing 
provides set primitives process creation communication incorporated existing procedural languages order implement parallel programs 
pvm process created pvm spawn call 
instance statement proc num pvm spawn progr null proc spawns proc copies program progr 
actual number processes started returned proc num 
communication processes implemented primitives pvm send proc id msg pvm rec proc id msg 
group communication synchronization functions pvm bcast see beguelin geist sunderam 
skillicorn talia acm computing surveys vol 
june pvm mcast pvm barrier 
pvm similar models programmers decomposition placement communication explicitly 
complicated need deal different operating systems communicate information messaging software 
models may useful increasing optical interconnection high performance networks connecting workstations 
shared memory communication natural extension techniques operating systems multiprogramming replaced true multiprocessing 
models paradigm understood 
aspects change parallel setting 
single processor sensible busy wait message denies processor processes best strategy parallel computer avoids overhead context switches 
shared memory parallel computers typically provide communication standard paradigms shared variables semaphores 
model computation attractive issues decomposition mapping important 
closely linked single style architecture shared memory programs portable 
important shared memory programming language java lea popular connection platform independent software delivery web 
java thread allows threads communicate synchronize condition variables 
shared variables accessed synchronized methods 
critical section enclosing text methods automatically generated 
critical sections misleadingly called monitors 
notify wait operations explicitly invoked sections automatically associated entry exit 
thread packages available providing lightweight processes shared memory communication 
rendezvous programming models distributed memory paradigms particular cooperation mechanism 
rendezvous communication model interaction processes takes place calls entry executes accept entry 
entry call similar procedure call accept statement entry contains list statements executed entry called 
best known parallel programming languages rendezvous cooperation ada fisher concurrent gehani 
ada designed behalf department defense mainly program real time applications sequential parallel distributed computers 
parallelism ada language processes called tasks 
task created explicitly statically declared 
case task activated block containing declaration entered 
tasks composed specification part body 
discussed mechanism entry declarations entry calls accept statements 
entry declarations allowed specification part task 
accept statements entries appear body task 
example accept statement executes operation entry square called 
accept square integer integer important features ada parallel programming select statement similar see buhr buhr faust levy mukherjee 
parallel computation acm computing surveys vol 
june csp alt command expressing nondeterminism exception handling mechanism dealing software failures 
hand ada address problem mapping tasks multiple processors provide conditions associated entry declarations special cases protected objects 
surveys models bal gottlieb 
static structure 
lowlevel models allow dynamic process creation communication 
exception occam jones goldsmith process structure fixed communication takes place synchronous channels 
occam programs constructed small number primitive constructs assignment input output 
design complex parallel processes primitive constructs combined parallel constructor par proc proc processes executed parallel par constructor terminates components terminated 
alternative constructor alt implements nondeterminism 
waits input number channels executes corresponding component process 
example code alt request data exec oper waits get data request operation request 
process corresponding selected guard executed 
occam strong semantic foundation csp hoare software development transformation possible 
low level development process practical small critical applications 
pram final model considered pram model karp ramachandran basic model theoretical analysis parallel computation 
pram machine consists set processors capable executing independent programs doing synchronously connected shared memory 
processors access location unit time forbidden access location step 
pram model requires detailed descriptions computations giving code processor ensuring memory conflict avoided 
unit time memory access part cost model satisfied scalable real machine cost measures pram model accurate 
accurate uniform way real cost accessing memory algorithm depends total number accesses pattern occur 
attempt provide abstraction pram language fork kessler seidl 
overview models aimed particular architectures mccoll 

summary overview parallel programming models languages set criteria ideal model satisfy 
criteria relate need model target software development 
ease programming existence methodology constructing software handles issues correctness independence particular architectures simplicity abstractness 
remaining criteria address need execution model real parallel machines skillicorn talia acm computing surveys vol 
june guaranteed performance existence costs inferred program 
ensure predictable performance programs 
assessed models satisfy criteria dividing classes ranging generally satisfy software development criteria predictable performance criteria concrete models provide predictable performance hard construct software 
models described represent extremely wide variety approaches different levels 
interesting trends visible 
low level models description computations completely explicit diminished significantly 
regard thing shows awareness importance abstraction spread research community 
concentration models middle range abstraction great deal ingenuity applied concealing aspects parallel computations struggling retain maximum expressiveness 
thing tradeoffs expressiveness software development complexity run time efficiency subtle 
presumably blend theoretical analysis practical experimentation road success strategy applied 
models provide predictable useful performance range parallel architectures 
existence raises hope models satisfying properties began eventually constructed 
trends show parallel programming models leaving low level approaches moving approaches languages tools simplify task designers programmers 
time trends provide robust parallel software predictable performance 
scenario brings benefits parallel software development 
models languages tools represent intermediate level users parallel architectures allow simple effective utilization parallel computation application areas 
availability models languages architecture complexity significant impact parallel software development process widespread parallel computing systems 
hope years models easy program providing moderate abstraction wide range parallel computers making portability standard feature parallel programming easy understand executed predictably performance 
take longer software development methods come general surprise struggling software development sequential programming 
computing costs programs possible model predictable performance integrating costs software development useful way difficult 
acknowledgments grateful dave dove luigi comments draft article anonymous referees helpful comments 
agha 
actors model concurrent computation distributed systems 
mit press cambridge ma 
agha 
open distributed programming paradigm 
proceedings fourth acm sigplan symposium principles practice parallel programming may 
ahmed carriero gelernter parallel computation acm computing surveys vol 
june 
program building tool parallel applications 
dimacs workshop specification parallel algorithms princeton university may 
ahuja carriero gelernter krishnaswamy 
matching languages hardware parallel computation linda machine 
ieee trans 
comput 
august 
america 
pool parallel objectoriented language 
object oriented concurrent programming yonezawa eds mit press cambridge ma 
andre thomas 
system 
ifip working conference decentralized systems dec poster presentation 
andre le fur 

compiler overview experimental results 
tech 
rep pi irisa october 
andrews olsson 
sr programming language 
benjamin cummings redwood city ca 
andrews olsson nilsen townsend 
overview sr language implementation 
acm trans 
program 
lang 
syst 
jan 
arbeit campbell 
system implementation performance 
national laboratory internal report 
back 
method refining atomicity parallel algorithms 
parle parallel architectures languages europe june lncs springer verlag 
back 
refinement calculus part ii parallel reactive programs 
tech 
rep bo departments computer science mathematics sf bo finland 
back 
stepwise refinement action systems 
mathematics program construction lncs june springer verlag 
back 
deriving occam implementation action systems 
tech 
rep bo departments computer science mathematics sf bo finland 
jazayeri pel 
architectural models design methodologies general purpose highly parallel computers 
ieee advanced computer technology reliable systems applications may 
bal steiner tanenbaum 
programming languages distributed computing systems 
comput 
surv 
sept 
bal tanenbaum kaashoek 
orca language distributed processing 
acm sigplan 
may 
bana tre le tayer 
gamma 
research directions high level parallel programming languages banatre le metayer eds lncs june springer verlag 

arrays categorical type constructors 
proceedings workshop arrays june 

construction multidimensional arrays categorical data types 
ph thesis queen university kingston canada 

utilisation du paradigme pour le calcul 
ph thesis universite de paris sud 
vidal 
actors parallel programming model 
proceedings eighth symposium theoretical aspects computer science 
lncs springer verlag 
beguelin dongarra geist manchek moore sunderam 
pvm tools heterogeneous network computing 
software parallel computation nato asi series vol 
eds 
springer verlag 
beguelin dongarra geist manchek sunderam 
enhancements pvm 
int 

appl 
high perform 
comput 
beguelin dongarra geist manchek sunderam pvm software system documentation 
email netlib ornl gov beguelin dongarra geist manchek otto walpole 
pvm experiences current status direction 
tech 
rep cs oregon graduate institute cs 
bershad lazowska levy 
presto system object oriented parallel programming 
softw 
pract 
exper 
august 

composable specifications asynchronous systems unity 
proceedings international symposium advanced research asynchronous circuits systems nov 
bilardi spirakis 
bsp vs logp 
proceedings eighth annual symposium parallel algorithms architectures june 
skillicorn talia acm computing surveys vol 
june bird 
theory lists 
logic programming calculi discrete design broy ed springerverlag 
black hutchinson jul levy carter 
distribution types emerald 
ieee trans 
softw 
eng 
jan 
black 
programmers guide 
university washington seattle june 
blelloch 
scans primitive parallel operations 
proceedings international conference parallel processing august 
blelloch 
vector models dataparallel computing 
mit press cambridge ma 
blelloch 
nesl nested dataparallel language 
tech 
rep cmu cs carnegie mellon university april 
blelloch 
programming parallel algorithms 
commun 
acm march 
blelloch greiner 
parallel complexity model functional languages 
tech 
rep cmu cs school computer science carnegie mellon university october 
blelloch 
compiling collection oriented languages massively parallel computers 
proceedings second symposium frontiers massively parallel computation 
blelloch 
compiling collection oriented languages massively parallel computers 
parallel distrib 
comput 
favre geyer de 
scheduling prolog scalable reconfigurable distributed memory multiprocessor 
proceedings parle springer lecture notes computer science springer verlag 
buhr 
system providing light weight concurrency shared memory multiprocessor computers running unix 
softw 
pract 
exper 
sept 
buhr macdonald cher 
system manual version 
tech 
rep department computer science university waterloo waterloo ontario canada march 
burkhart korn 
basel algorithm classification scheme 
tech 
rep institut fu informatik der universita basel march 
butler lusk 
user guide programming system 
tech 
rep anl argonne national laboratory mathematics computer science division october 
di spa talia 
parallel cellular automata environment multicomputers computational science 
parallel comput 

talia 
parallel logic system multicomputer architecture 
fut 
gen comput 
syst 

carriero 
implementation tuple space machines 
tech 
rep yaleu dcs rr department computer science yale university december 
carriero gelernter 
application experience linda 
acm sigplan symposium parallel programming july vol 

carriero gelernter 
learning success 
software parallel computation nato asi series vol 
eds 
springer verlag 
chandy misra 
parallel program design foundation 
addison wesley reading ma 
de 
parallel logic programming systems 
acm comput 
surv 

chen choo li 
crystal theory pragmatics generating efficient parallel code 
parallel functional languages compilers szymanski ed acm press frontier series new york 
chien 
concurrent aggregates multiple access data abstractions manage complexity concurrent programs 
oops messenger april 
chien dally 
concurrent aggregates 
second sigplan symposium principles practice parallel programming feb 
chin mccoll 
virtual shared memory algorithms complexity 
inf 
comput 
sept 
cole 
algorithmic skeletons structured management parallel computation 
research monographs parallel distributed computing pitman london 
cole 
fully local multicomputer implementations functional programs 
tech 
rep cs department computing science university glasgow january 
cole 
writing parallel programs non parallel languages 
software parallel computers exploiting parallelism software environments tools algorithms parallel computation acm computing surveys vol 
june application libraries ed chapman hall london 
cole 
parallel programming list homomorphisms maximum segment sum problem 
parallel computing trends applications ed northholland 

parallel execution logic programs 
kluwer academic 
cooper hamilton 
preserving abstraction concurrent programming 
ieee trans 
softw 
eng 
se 

implementation gamma connection machine 
research directions high level parallel programming languages banatre le metayer eds lncs june springer verlag 
culler karp patterson schauser santos von eicken 
logp realistic model parallel computation 
acm sigplan symposium principles practice parallel programming may 
dally wills 
universal mechanisms concurrency 
parle parallel architectures languages europe june lncs springer verlag 
dally keen davison 
processor 
ieee micro april 
di orlando pel 
methodology development support massively parallel programs 
fut 
gen comput 
syst 
appears language hewlett packard rep hpl psc december 
di 
high level language constructs massively parallel computing 
tech 
rep hewlett packard pisa science center hpl psc 

high level languages easy massively parallel computing 
tech 
rep hewlett packard pisa science center 
darlington field harrison reeve 
design implementation alice parallel graph reduction machine 
selected reprints dataflow reduction architectures ed ieee computer society press los alamitos ca 
darlington field harrison kelly wu 
parallel programming skeleton functions 
parle parallel architectures languages europe june 
darlington guo yang 
parallel fortran family new perspective 
massively parallel programming models berlin oct ieee computer society press los alamitos ca 
de 
structuration alpha language 
massively parallel programming models berlin oct ieee computer society press los alamitos ca 
dongarra otto snir walker 
mpi standard 
tech 
rep cs university tennessee 
available www netlib org tennessee ut cs ps january 
dongarra otto snir walker 
message passing standard mpp workstations 
commun 
acm 
douglas rowstron wood 
linda parallel programming bags 
tech 
rep department computer science university york uk september 
eckart 
manual 
sigplan 

eisenbach patterson 
calcu lus semantics concurrent configuration language darwin 
proceedings th annual hawaii international conference system science jan vol 
ii ieee computer society press los alamitos ca 

perspective id parallel functional languages compilers szymanski ed acm press new york 
fagin despain 
performance parallel prolog programs 
ieee trans 
comput 

faigle haupt simoni 
movie model open systems high performance distributed computing 
concurrency pract 
exper 
june 
faust levy 
performance object oriented threads package 
oopsla ecoop conference objectoriented programming systems languages applications ottawa canada oct published sigplan 
oct 
wagner 
parsec software development environment performance oriented parallel programming 
transputer research applications atkins wagner eds may ios press amsterdam 
skillicorn talia acm computing surveys vol 
june flynn hummel kelly 
rationale parallel programming sets 
program 
lang 

foster taylor 
strand new concepts parallel programming 
prenticehall englewood cliffs nj 
foster tuecke 
parallel programming pcn 
tech 
rep anl rev mathematics computer science division argonne national laboratory december 
foster olson tuecke 
productive parallel programming pcn approach 
sci 
program 

foster tuecke taylor 
portable run time system pcn 
tech 
rep mcs tm 
argonne national laboratory caltech december 
available ftp info mcs anl gov pub tech reports 
gehani 
concurrent softw 
pract 
exper 

geist 
pvm network computing 
parallel computation ed lncs springer verlag 
gerth pnueli 
unity 
proceedings fifth international workshop software specification design pittsburgh pa may 
gibbons 
algebras tree algorithms 
phil 
thesis programming research group university oxford 
goguen winkler 
introducing obj 
tech 
rep sri csl sri international menlo park ca august 
goguen kirchner kirchner gre lis meseguer winkler 
obj 
lecture notes computer science vol 
springer verlag 
goguen winkler meseguer jouannaud 

introducing obj 
applications algebraic specification obj 
ed cambridge 
goldman 
views abstractions efficient scientific computing connection machine 
tech 
rep mit lcs tm mit laboratory computer science 
gottlieb 
nyu designing mimd shared memory parallel computer 
ieee trans 
comput 
feb 
gottlieb rudolph 
basic techniques efficient coordination large numbers cooperating sequential processes 
acm trans 
program 
lang 
syst 
april 
gregory 
parallel logic programming parlog 
addison wesley reading ma 
grimshaw 
parallel object oriented programming mentat 
tech 
rep computer science department university virginia april 
grimshaw 
easy object oriented parallel processing mentat 
ieee comput 
may 
grimshaw 
mentat computation model data driven support objectoriented parallel processing 
tech 
rep computer science department university virginia may grimshaw weissman 
mentat programming language 
mpl manual university virginia november 
grimshaw weissman 
mentat user manual 
tech 
rep university virginia november 

data parallel categorical machine 
parle parallel architectures languages europe june lncs springer verlag 
hansen 
distributed processes concurrent programming concept 
commun 
acm 
hatcher quinn 
dataparallel programming mimd computers 
mit press cambridge ma 
heinz 
modula efficiently compilable extension modula explicitly parallel programming 
proceedings joint symposium parallel processing waseda university tokyo may 

anl gmd macros fortran portable parallel programming message passing programming model users guide manual 
tech 
rep gmd postfach sankt augustin germany november 
hoppe 
library interface specification 
tech 
rep gmd postfach sankt augustin germany december 
saito 
dataflow computing 
parallel algorithms architectures may lncs 
high performance fortran language specification 
available ftp titan rice cs edu january 
hoare 
communicating sequential processes 
international series computer science prentice hall englewood cliffs nj 
hudak 
para functional programming 
ieee comput 

parallel computation acm computing surveys vol 
june hudak fasel 
gentle haskell 
sigplan 
may 
hummel kelly flynn hummel 
set language prototyping parallel algorithms 
proceedings computer architecture machine perception conference dec 
jaffar lassez 

constraint logic programming 
acm symposium principles programming languages acm press new york 
jay 
semantics shape 
sci 
comput 
program 
dec 
andre ernst 
hpf translator 
tech 
rep irisa institut de recherche en informatique systemes may jones goldsmith 
programming occam 
prentice hall englewood cliffs nj 
halstead jr 
parallel symbolic computing 
ieee comput 
aug 

reduce process model parallel evaluation logic programs 
proceedings fourth international conference logic programming melbourne australia 
karp ramachandran 
parallel algorithms shared memory machines 
handbook theoretical computer science vol 
van leeuwen ed elsevier science publishers mit press 
kelly 
functional programming loosely coupled multiprocessors 
pitman london 
seidl 
integrating synchronous asynchronous paradigms fork parallel programming language 
massively parallel programming models berlin oct ieee computer society press los alamitos ca 
kilian 
aid massively parallel programming 
proceedings dartmouth institute advanced graduate study parallel computation symposium johnson metaxas eds 
june 
kilian 
parallel sets objectoriented methodology massively parallel programming 
ph thesis harvard university 
kim 
agha 
efficient support location transparency concurrent object oriented programming languages 
supercomputing 
knuth 
big big omega big theta 
sigact 
kung 
systolic architectures 
ieee comput 

larus richards viswanathan 
large grain object oriented data parallel programming language 
tech 
rep tr university november 
lea 
concurrent programming java design principles patterns 
addison wesley reading ma 
lengauer wirsing 
object oriented airport specification refinement maude 
trends data type specifications astesiano reggio tarlecki eds lncs springer verlag berlin 
li hudak 
memory coherence shared virtual memory systems 
acm trans 
comput 
syst 
nov 
lincoln mart meseguer 
specification transformation programming concurrent systems rewriting logic 
tech 
rep sri csl sri may liskov 
implementation argus 
proceedings eleventh symposium operating systems principles acm press new york 
lloyd 
foundations logic programming 
springer verlag 
lobe lu parsons fer smith szafron 
enterprise model developing distributed applications 
tech 
rep department computing science university alberta november 
malcolm 
algebraic data types program transformation 
ph thesis rijksuniversiteit groningen september 
mccoll 
general purpose parallel computing 
lectures parallel computation gibbons spirakis eds cambridge international series parallel computation cambridge university press cambridge 
mccoll 
special purpose parallel computing 
lectures parallel computation gibbons spirakis eds cambridge international series parallel computation cambridge university press cambridge 
mccoll 
bulk synchronous parallel computing 
second workshop models parallel computation oxford university press new york 
mccoll 
architecture independent programming model scalable parallel computing 
portability performance parallel processors ferrante hey eds wiley new york 
mcgraw allan thomas 
sisal streams skillicorn talia acm computing surveys vol 
june iteration single assignment language manual 
tech 
rep rev lawrence livermore national laboratory march 
mcgraw 
parallel functional programming sisal facts 
advanced workshop programming tools parallel machines june 
available laurence livermore national laboratories tech 
rep jc 
mehrotra haines 
overview opus language runtime system 
tech 
rep nasa icase may mentat 
mentat tutorial 
available ftp cs virginia edu pub mentat tutorial 
ps meseguer 
logical theory concurrent objects realization maude language 
tech 
rep sri csl sri international july 
meseguer winkler 
parallel programming maude 
research directions high level parallel programming languages banatre le metayer eds lncs june springer verlag 
message passing interface forum 
mpi message passing interface 
proceedings supercomputing ieee computer society washington dc 

development array theory 
tech 
rep ibm cambridge scientific center 
mukherjee ghosh 
machine independent interface lightweight threads 
acm oper 
syst 
rev jan 
mullin 
mathematics arrays 
ph dissertation syracuse university syracuse ny december 
fisher 
parallel processing ada 
ieee comput 


parallel programming bags 
research directions high level parallel programming languages bana tre le metayer eds lncs june springer verlag 
newton browne 
code graphical parallel programming language 
proceedings acm international conference supercomputing july 
dally 
system design machine 
proceedings sixth mit conference advanced research vlsi mit press cambridge ma 
campbell 
programming tools coupling application codes network environment 
proceedings heterogeneous network concurrent computing workshop fl oct 
supercomputing computations research institute florida state university 
proceedings available anonymous ftp ftp edu directory pub parallel workshop 

pereira nasr 
delta prolog distributed logic programming language 
proceedings international conference fifth generation computer systems tokyo nov 
peyton jones clack harris 
grip parallel graph reduction machine 
tech 
rep department computer science university london 
peyton jones lester 
implementing functional programming languages 
international series computer science prentice hall englewood cliffs nj 
quinn hatcher 
dataparallel programming multicomputers 
ieee softw 
sept 

experiments transputer parallel graph reduction machine 
concurrency pract 
exper 
august 
eisenbach 
get calculus semantics 
parle parallel architectures languages europe theodoridis eds lncs springer verlag 
radha muthukrishnan 
portable implementation unity von neumann machines 
comput 
lang 


software controlled shared virtual memory management transputer multiprocessor 
transputer research applications fielding ed ios press amsterdam 

fluent parallel computation 
ph thesis yale university 
sa bot 
model architecture independent parallel programming 
mit press cambridge ma 
saraswat rinard panangaden 
semantic foundations concurrent constraint programming 
proceedings popl conference acm press new york 

cellular programming language 
parallel comput 

shapiro 
concurrent prolog progress report 
ieee comput 
august 
shapiro 
family concurrent logic programming languages 
acm comput 
surv 


match move approach data parallel computing 
ph parallel computation acm computing surveys vol 
june sis carnegie mellon october 
appears rep cmu cs 
singh 
graphs categorical data type 
master thesis computing information science queen university kingston canada 

sisal 
parallel functional languages compilers szymanski ed acm press frontier series new york 
skillicorn 
architecture independent parallel computation 
ieee comput 
dec 
skillicorn 
categorical data types 
second workshop models parallel computation oxford university press new york 
skillicorn 
foundations parallel programming 
number cambridge series parallel computation cambridge university press new york 
skillicorn 
communication skeletons 
machine models parallel distributed computing kara davy nash eds leeds april ios press netherlands 
skillicorn cai 
cost calculus parallel functional programming 
parallel distrib 
comput 
july 
skillicorn talia 
programming languages parallel processing 
ieee computer society press los alamitos ca 
skillicorn hill mccoll 
questions answers bsp 
sci 
program 

talia 
high level cellular programming model massively parallel processing 
proceedings second international workshop high level programming models supportive environments hips ieee computer society press los alamitos ca 
spivey 
categorical approach theory lists 
mathematics program construction june lncs 
springer verlag 
steele jr 
high performance fortran status report 
proceeding workshop languages compilers run time environments distributed memory multiprocessors sept appeared sigplan 
jan 
sunderam 
concurrent computing pvm 
proceedings workshop cluster computing fl dec 
supercomputing computations research institute florida state university 
proceedings available anonymous ftp ftp 
edu directory pub parallel workshop 
sunderam 
pvm framework parallel distributed computing 
tech 
rep ornl tm department math computer science emory university oak ridge national lab february 
concurrency pract 
exper 
dec 
swinehart 
structure cedar 
sigplan 
july 
szafron schaeffer wong chan lu smith 
enterprise interactive graphical programming environment distributed software 
available ftp cs ualberta ca 
talia 
survey parlog concurrent prolog integration logic parallelism 
comput 
lang 

talia 
parallel logic programming systems multicomputers 
program 
lang 
march 
thompson 
formulating haskell 
tech 
rep computing laboratory university kent canterbury uk 
tseng 

optimizing fortran compiler mimd distributed memory machines 
ph thesis rice university january 
rice comp tr 
ueda 
guarded horn clauses 
tech 
rep tr icot tokyo 
valiant 
bulk synchronous parallel computers 
tech 
rep tr computer science harvard university 
valiant 
bridging model parallel computation 
commun 
acm august 
valiant 
general purpose parallel architectures 
handbook theoretical computer science vol 
van leeuwen ed elsevier science publishers mit press 
van hentenryck 
parallel constraint satisfaction logic programming preliminary results chip 
proceedings sixth international congress logic programming mit cambridge ma 

mathematical theory environment parallel programming 
parallel process 
lett 

watanabe 
reflection object oriented concurrent language 
sigplan 
nov 
wilkinson bury kelly 
angel proposed multiprocessor operating system kernel 
proceedings european workshops parallel computing barcelona march 
wills 
pi parallel architecture interface multi model execution 
tech 
rep ai tr mit artificial intelligence laboratory 
skillicorn talia acm computing surveys vol 
june winkler 
programming obj maude 
functional programming concurrency simulation automated reasoning lauer ed lncs springer verlag berlin 
yang choo 
parallel program transformation metalanguage 
proceedings third acm sigplan symposium principles practice parallel programming 
yang choo 
formal derivation efficient parallel gauss seidel method mesh processors 
proceedings sixth international parallel processing symposium march ieee computer society press los alamitos ca 
yang choo 
metalinguistic features formal parallel program transformation 
proceedings fourth ieee international conference computer languages april ieee computer society press los alamitos ca 

concurrent programming concurrent smalltalk 
object oriented concurrent programming yonezawa ed mit press cambridge ma 
yonezawa 
object oriented concurrent programming 
mit press cambridge ma 
zenith 
axiomatic characterization ease 
linda systems implementation edinburgh parallel computing centre tr 
zenith 
rationale programming ease 
research directions high level parallel programming languages banatre le metayer eds lncs june springer verlag 
zenith 
programming ease 
centre de recherche en informatique cole nationale superieure des mines de paris sept 
zenith 
ease model implementation 
proceeding workshop languages compilers run time environments distributed memory multiprocessors sept appeared sigplan 
jan 
received april revised april accepted december parallel computation acm computing surveys vol 
june 
