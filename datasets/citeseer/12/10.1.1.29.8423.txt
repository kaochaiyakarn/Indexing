input output hmm architecture bengio dept informatique recherche op universit de montr eal qc iro umontreal ca paolo frasconi dipartimento di sistemi informatica universit di firenze italy paolo mcculloch ing introduce recurrent architecture having modular structure formulate training procedure em algorithm 
resulting model similarities hidden markov models supports recurrent networks processing style allows exploit supervised learning paradigm maximum likelihood estimation 
learning problems involving sequentially structured data effectively dealt static models feedforward networks 
recurrent networks allow model complex dynamical systems store retrieve contextual information flexible way 
time research efforts supervised learning recurrent networks exclusively focused error minimization gradient descent methods 
effective learning short term memories practical difficulties reported training recurrent neural networks perform tasks temporal contingencies input output sequences span long intervals bengio mozer 
previous alternative training algorithms bengio suggest root problem lies essentially discrete nature process storing information indefinite amount time 
potential solution propagate backward time targets discrete state space differential error information 
extending previous bengio frasconi propose statistical approach target propagation em algorithm :10.1.1.56.686
consider parametric dynamical system discrete states introduce modular architecture subnetworks associated discrete states 
architecture interpreted statistical model trained em generalized em gem algorithms dempster considering internal state trajectories missing data 
way learning decoupled bell labs holmdel nj temporal credit assignment subproblem static learning subproblem consists fitting parameters state output mappings defined estimated trajectories 
order iteratively tune parameters em gem algorithms system propagates forward backward discrete distribution states resulting procedure similar baum welch algorithm train standard hidden markov models hmms levinson 
hmms adjust parameters unsupervised learning em supervised fashion 
furthermore model called input output hmm iohmm learn map input sequences output sequences standard hmms learn output sequence distribution 
model seen recurrent version mixture experts architecture jacobs related model proposed cacciatore nowlan 
experiments artificial tasks bengio frasconi shown em recurrent learning deal long term dependencies effectively backpropagation time alternative algorithms :10.1.1.56.686
model bengio frasconi limited representational capabilities map input sequence final discrete state :10.1.1.56.686
describe extended architecture allows fully exploit input output portions data required supervised learning paradigm 
way general sequence processing tasks production classification prediction dealt 
proposed architecture consider discrete state dynamical system state space description gamma input vector time output vector ng discrete state 
equations define generalized mealy finite state machine inputs outputs may take continuous values 
consider probabilistic version dynamics current inputs current state distribution estimate state distribution output distribution time step 
admissible state transitions specified directed graph vertices correspond model states set successors state system defined equations modeled recurrent architecture depicted 
architecture composed set state networks set output networks state output networks uniquely associated states networks share input state network task predicting state distribution current input gamma similarly output network predicts output system current state input 
subnetworks assumed static defined means smooth mappings vectors adjustable parameters connection weights 
ranges functions may constrained order account underlying transition graph output ij state subnetwork time associated successors state layer units cardinality convenience notation suppose ij defined impose condition ij belonging softmax function layer ij ij ij intermediate variables thought 
current input delay current state distribution current expected output past input sequence softmax softmax convex weighted sum convex weighted sum hmm iohmm gamma gamma gamma gamma gamma proposed iohmm architecture 
bottom bayesian network expressing conditional dependencies iohmm top bayesian network standard hmm activations output units subnetwork way ij vector represents internal state model computed linear combination outputs state networks gated previously computed internal state gamma nj output networks compete predict global output system jt jt jt output subnetwork level need specify internal architecture state output subnetworks 
depending task designer may decide include hidden layers activation rule hidden units 
connectionist architecture interpreted probability model 
assume multinomial distribution state variable consider main variable temporal recurrence 
initialize vector positive numbers summing interpreted vector initial state probabilities 
general obtain relation having denoted subsequence inputs time 
equation probabilistic interpretation gamma gamma gamma subnetworks compute transition probabilities conditioned input sequence ij gamma neural networks trained minimize output squared error output architecture interpreted expected position parameter probability distribution output addition conditional input expectation conditional state 
actual form output density denoted chosen task 
example multinomial distribution suitable sequence classification symbolic mutually exclusive outputs 
gaussian distribution adequate producing continuous outputs 
case softmax function output subnetworks second case linear output units subnetworks order reduce amount computation introduce independency model variables involved probabilistic interpretation architecture 
shall bayesian network characterize probabilistic dependencies variables 
specifically suppose directed acyclic graph depicted bottom bayesian network dependency model associated variables evident consequences independency model previous state current input relevant determine state 
step memory property analogue markov assumption hidden markov models hmm 
fact bayesian network hmms obtained simply removing nodes arcs see top 
supervised learning algorithm learning algorithm proposed architecture derived maximum likelihood principle 
training data set pairs input output sequences length tp tp pg 
theta denote vector parameters obtained collecting parameters architecture 
likelihood function theta tp tp theta output values targets may specified intermittently 
example sequence classification tasks may interested output sequence 
modification likelihood account intermittent targets straightforward 
maximum likelihood principle optimal parameters obtained maximizing 
order apply em case noting state variables observed 
knowledge model state trajectories allow decompose temporal learning problem static learning subproblems 
known probabilities possible train subnetwork separately account temporal dependency 
observation allows link em learning target propagation approach discussed 
note viterbi approximation considering path static learning problems epoch 
order derive learning equations define complete data tp tp tp corresponding complete data log likelihood theta log tp tp tp theta theta depends hidden state variables maximized directly 
mle optimization solved introducing auxiliary function theta theta iterating steps estimation compute theta theta theta fi fi theta maximization update parameters theta arg max theta theta theta expectation expressed theta theta tp log theta ij log ij ij gamma fi fi theta denoting indicator variable 
hat ij means variables computed old parameters theta order compute ij introduce forward probabilities ff backward probabilities fi updated follows fi fy fi ff ff gamma ij fi ff gamma ij ff iteration em algorithm requires maximize theta theta 
consider simplified case inputs quantized belonging finite alphabet foe oe subnetworks behave lookup tables addressed input symbols oe interpret parameter ijk gamma oe 
simplicity restrict analysis classification tasks suppose targets specified desired final states sequence 
furthermore output subnetworks particular application algorithm 
case obtain reestimation formulae ijk oe fi gamma oe fi gamma general subnetworks hidden sigmoidal units softmax function constrain outputs sum maximum analytically 
cases resort gem algorithm simply produces increase example gradient ascent 
case derivatives respect parameters easily computed follows 
jk generic weight state subnetwork equation theta theta jk ij ij ij jk partial derivatives ij jk computed backpropagation 
similarly denoting ik generic weight output subnetwork theta theta ik log fy ik ik computed backpropagation 
intuitively parameters updated estimation step em provided targets outputs subnetworks time gem algorithms guaranteed find local maximum likelihood convergence may significantly slower compared em 
experiments noticed convergence accelerated stochastic gradient ascent 
comparisons appears natural find similarities recurrent architecture described far standard hmms levinson 
architecture proposed differs standard hmms respects computing style learning 
iohmms sequences processed similarly recurrent networks input sequence synchronously transformed output sequence 
computing style real time predictions outputs available input sequence processed 
architecture allows implement fundamental sequence processing tasks production prediction classification 
transition probabilities standard hmms fixed states form homogeneous markov chain 
iohmms transition probabilities conditional input depend time resulting inhomogeneous markov chain 
consequently dynamics system specified transition probabilities fixed adapted time depending input sequence 
fundamental difference learning procedure 
interesting capabilities modeling sequential phenomena major weakness standard hmms poor discrimination power due unsupervised learning 
approach useful improve discrimination hmms maximum mutual information mmi training 
pointed supervised learning discriminant learning criteria mmi strictly related bridle 
parameter adjusting procedure defined mle desired output response input resulting discriminant supervised learning 
worth mentioning number hybrid approaches proposed integrate connectionist approaches hmm framework 
example bengio observations hmm generated feedforward neural network 
bourlard feedforward network estimate state probabilities conditional acoustic sequence 
common feature algorithms proposed neural networks extract temporally local information markovian system integrates long term constraints 
establish link iohmms adaptive mixtures experts jacobs 
cacciatore nowlan proposed recurrent extension architecture called mixture controllers mc gating network feedback connections allowing take temporal context account 
iohmm architecture interpreted special case mc architecture set state subnetworks play role gating network having modular structure second order connections 
regular grammar inference section describe application architecture problem grammatical inference 
task learner set labeled strings requested infer set rules define formal language 
considered prototype complex language processing problems 
simplest case regular grammars task proved np complete angluin smith 
report experimental results set regular grammars introduced tomita researchers measure accuracy inference methods recurrent networks giles pollack watrous kuhn 
scalar output supervision final output modeled bernoulli variable gamma gammay string rejected accepted 
application apply table summary experimental results tomita grammars 
grammar sizes convergence accuracies fsa min average worst best best external inputs output networks 
corresponds modeling moore finite state machine 
absence prior knowledge plausible state paths ergodic transition graph fully connected experiments measured convergence generalization performance different sizes recurrent architecture 
setting ran trials different seeds initial weights 
considered trial successful trained network able correctly label training strings 
model size chosen cross validation criterion performance randomly generated strings length 
comparison table report grammar number states minimal recognizing fsa tomita 
tested trained networks corpus gamma binary strings length 
final results summarized table 
column convergence reports fraction trials succeeded separate training set 
columns report averages order statistics worst best trial fraction correctly classified strings measured successful trials 
grammar results refer model size selected cross validation 
generalization perfect grammars 
grammar best trial attained perfect generalization 
results compare favorably obtained second order networks trained gradient descent learning sets proposed tomita 
comparison column table reproduce results reported watrous kuhn best trials 
successful trials model learned actual fsa behavior transition probabilities asymptotically converging 
renders trivial extraction corresponding fsa 
grammars trained networks behave exactly minimal recognizing fsa 
potential training problem presence local maxima likelihood function 
example number converged trials grammars quite small difficulty discovering optimal solution serious restriction tasks involving large number states 
experiments bengio frasconi noticed restricting connectivity transition graph significantly help remove problems convergence :10.1.1.56.686
course approach effectively exploited prior knowledge state space available 
example applications hmms speech recognition rely structured topologies 
number open questions 
particular effectiveness model tasks involving large large state spaces needs carefully evaluated 
bengio frasconi show learning long term dependencies models difficult increase connectivity state transition graph 
transition probabilities iohmms change deal better problem long term dependencies standard hmms 
interesting aspect investigated capability model successfully perform tasks sequence production prediction 
example interesting tasks approached related time series modeling motor control learning 
angluin smith 

inductive inference theory methods 
computing surveys 
bengio frasconi 

credit assignment time alternatives backpropagation 
cowan tesauro alspector editors advances neural information processing systems 
morgan kaufmann 
bengio frasconi 

em approach learning sequential behavior 
tech 
rep rt dsi university florence 
bengio de mori 

global optimization neural network hidden markov model hybrid 
ieee transactions neural networks 
bengio simard frasconi 

learning long term dependencies gradient descent difficult 
ieee trans 
neural networks 
bourlard 

links hidden markov models multilayer perceptrons 
ieee trans 
pattern 
mach 
intell 
bridle 

training stochastic model recognition algorithms networks lead maximum mutual information estimation parameters 
touretzky ed nips pages 
morgan kaufmann 
cacciatore nowlan 

mixtures controllers jump linear non linear plants 
cowan editors advances neural information processing systems san mateo ca 
morgan kaufmann 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
royal stat 
soc 

giles miller chen sun chen lee 

learning extracting finite state automata second order recurrent neural networks 
neural computation 
jacobs jordan nowlan hinton 

adaptive mixture local experts 
neural computation 
levinson rabiner sondhi 

application theory probabilistic functions markov process automatic speech recognition 
bell system technical journal 
mozer 

induction multiscale temporal structure 
moody eds nips pages 
morgan kaufmann 
pollack 

induction dynamical recognizers 
machine learning 
tomita 

dynamic construction finite state automata examples hill climbing 
proc 
th cog 
science conf pp 
ann arbor mi 
watrous kuhn 

induction finite state languages second order recurrent networks 
neural computation 
