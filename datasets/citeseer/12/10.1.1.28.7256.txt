appear neural computation clustering conditional distributions auxiliary space sinkkonen samuel kaski neural networks research centre helsinki university technology box fin hut finland sinkkonen samuel hut 
study problem learning groups categories local continuous primary space homogeneous distributions associated auxiliary random variable discrete auxiliary space 
assuming variation auxiliary space meaningful categories emphasize similarly meaningful aspects primary space 
data set consisting pairs primary auxiliary items categories learned minimizing kullback leibler distortion implicitly estimated distributions auxiliary data conditioned primary data 
categories de ned terms primary space 
line algorithm resembling traditional hebb type competitive learning introduced learning categories 
minimizing distortion criterion turns equivalent maximizing mutual information categories auxiliary data 
addition connections density estimation distributional clustering paradigm outlined 
method demonstrated clustering yeast gene expression data dna chips biological knowledge functional classes genes auxiliary data 
clustering algorithms goals vary common aim clusters relatively homogeneous data di erent clusters dissimilar 
results depend totally criterion similarity 
dicult problem selecting suitable criterion commonly addressed feature extraction variable selection methods 
metrics additionally derived tting generative model data information geometric methods extracting metric model hofmann jaakkola haussler tipping 
study related case additional useful information exists data items modeling process 
information available auxiliary samples form pairs primary samples multinomial 
extra information may example labels functional classes genes case study 
assumed di erences auxiliary data indicate important primary data space 
precisely di erence samples signi cant corresponding values di erent 
usefulness assumption depends course choice auxiliary data 
relationship auxiliary data primary data stochastic get better description di erence values measuring di erences distributions conditional densities cjx cjx known 
set sample pairs available 
aim minimize cluster dissimilarities clusters homogeneous terms estimated distributions cjx 
order retain potentially useful structure primary space auxiliary data indicate importance de ne clusters terms localized basis functions primary space 
clustering new samples primary data space corresponding auxiliary samples available 
loosely speaking aim preserve topology primary space measure distances similarity auxiliary space 
clearly kind paired data applicable auxiliary data improves clustering 
auxiliary data closely related goal clustering task example performance index auxiliary data guides clustering emphasize important dimensions primary data space disregard rest 
automatic relevance detection main practical motivation 
earlier constructed local metric primary space measures distances primary space approximating corresponding di erences conditional distributions cjx auxiliary space kaski :10.1.1.26.6383
metric clustering maximally homogeneous clusters terms conditional distributions appear 
introduce alternative method contrary approach generating explicit metric need estimate conditional distributions cjx intermediate step 
additionally show minimizing cluster distortion equivalent maximizing mutual information basis functions de ning clusters interpreted multinomial random variable auxiliary data 
maximization mutual information previously constructing neural representations becker becker hinton 
related works paradigms include learning discrete dyadic data hofmann distributional clustering pereira information bottleneck tishby principle 
clustering kullback leibler divergence seek cluster items data space utilizing information set pairs data 
set consists paired samples random variables 
vector valued random variable takes values just values multinomial random variable wish keep clusters local respect measure similarities samples di erences corresponding conditional distributions cjx 
distributions unknown implicitly estimated data 
vector quantization vq equivalently means clustering approach categorization 
vq goal minimize average distortion data prototypes codebook vectors de ned dx denotes measure distortion cluster membership function ful lls 
classic hard vector quantization membership function binary valued 
functions de ne partitioning space discrete cells voronoi regions goal learning nd partitioning minimizes average distortion 
membership functions may attain values zero approach may called soft vector quantization nowlan studied maximum likelihood solution 
measure distortions di erences distributions cjx model distributions 
measure di erences kullback leibler divergence de ned discrete valued distributions event probabilities fp kl log 
case rst distribution multinomial distribution auxiliary space corresponds data jx 
second distribution prototype denote jth prototype kullback leibler distortion measure plugged equation error function vq average distortion kl kl cjx dx distortions vectorial samples vectorial prototypes compute point wise distortions distributions cjx prototypes prototypes distributions auxiliary space 
average distortion minimized parameterizing functions optimizing distortion respect cluster membership parameters prototypes 
parameters denoted average distortion written kl log ji dx const 
constant independent parameters 
membership functions interpreted conditional densities jx distributed random variable indicates cluster identity 
value random variable denoted fv value random variable corresponding distributed auxiliary distribution denoted fc choice cluster depend words conditionally independent cjx 
follows cjx dx 
shown see appendix membership distributions normalized exponential form exp exp gradient kl respect parameters kl log ji li dx prototypes probabilities multinomial distributions ful ll ji ji 
incorporate conditions model prototypes follows log ji ji log jm gradient average distortion respect new parameters prototypes kl lm lm mi dx kronecker symbol mi mi 
average distortion minimized stochastic approximation sampling 
leads line algorithm steps repeated gradually decreasing zero 
step stochastic approximation draw data sample 
assume value de nes value steps 

draw basis functions multinomial distribution probabilities fy 
adapt parameters lm log ji li lm lm lm mi number possible values random variable due symmetry possible adapt parameters twice swapping second adaptation 
note stochastic approximation ful ll conditions 
practice piecewise linear decreasing schedules 
consider special cases 
demonstrations basis functions normalized gaussians euclidean space location parameters small circles gaussian basis functions models optimized class dimensional threedimensional data sets 
shades gray background depict densities data sampled 
data roughly gaussian 
inset shows conditional density jx monotonically decreasing function dimension 
model learned represent dimension cjx changes 
projections data symmetric gaussian ideally form distribution ect solution 
show conditional density jx decreases monotonically function dimensional radius stays constant respect orthogonal third dimension dimensional cross section describing function dimensional radius shown inset model learned represent variation direction radius dimensions discards dimension irrelevant 
second case section gene expression data mapped hypersphere clustered normalized von mises fisher distributions mardia basis functions 
gaussians parameterized locations having diagonal covariance matrix variance equal dimension kx von mises fisher vmf distribution analogue gaussian distribution hypersphere mardia maximum entropy distribution rst moments xed 
density dimensional vmf distribution vmf zn exp parameter vector represents mean direction vector 
normalizing coecient zn relevant mixture model section 
function modi ed bessel function rst kind order clustering algorithm described vmf basis functions constant dispersion 
norm ect may normalize gradient shown kaski stochastic approximation algorithm de ned eqns converges probability normalized step 
connections mutual information density estimation easily shown information inequality minimum distortion kl prototype takes form li jv distortion expressed kl log const 
const 
mutual information random variables minimizing average distortion kl equivalent maximizing mutual information auxiliary variable cluster memberships minimization distortion connection density estimation details described appendix 
shown minimization kl minimizes upper limit mean kullback leibler divergence real distribution cjx certain estimate cjx 
estimate jx exp log ji normalizing coecient selected jx 
gaussian basis functions upper limit tight approaches zero cluster membership functions binary 
cluster membership functions approach indicator functions voronoi regions 
note solution computed practice smooth vector quantization results compromise solution tractable upper limit error minimized 
furthermore mean divergence expressed terms divergence joint distributions follows ex fd kl cjx cjx kl cjx ex 
denotes expectation values kl divergence joint distribution 
expression cost function estimate conditional probability cjx 
intuitively speaking minimizing resources wasted estimating marginal resources concentrated estimating cjx 
shown appendix maximum likelihood estimation model cjx nite data set asymptotically equivalent minimizing 
related works competitive learning known early works competitive learning adaptive feature detectors grossberg nass cooper erez close connection vector quantization gersho gray makhoul means clustering forgy macqueen :10.1.1.116.3824
neurons competitive learning network parameterized vectors describing synaptic weights denoted neuron simplest models activity neuron due external inputs nonlinear function inputs multiplied synaptic weights 
activities neurons compete activity neuron reduces activity negative feedback 
competition winner take type kaski kohonen neuron largest remains active 
neuron functions feature detector detects input comes particular domain input space 
hebbian type learning neurons gradually specialize representing di erent types domains 
detailed accounts see kohonen kohonen kohonen hari 
discussion winner de ned terms inner products possible generalize model arbitrary metric 
usual euclidean metric learning corresponds minimization mean squared vector quantization distortion equivalently minimization distance closest cluster center means clustering 
domain input space neuron detects interpreted voronoi region vector quantization 
relationship competitive learning cluster membership functions section may interpreted outputs set neurons limit crisp membership functions gaussians neuron having largest external input active interpreted winner 
learning algorithm corresponds traditional competitive network 
learning procedure di erence making network detect features homogeneous possible regard auxiliary data 
learning algorithm potentially interesting relation hebbian competitive learning 
assume neurons may active time probabilities 
learning algorithm vmf kernels reads log li log ji neuron adapted time swapping 
activity neurons binary valued neurons activity value value zero adaptation rule neuron expressed log ki log ji denotes activity neuron term hebbian kind forgetting term cf 
kohonen oja 
di erence common competitive learning lies parentheses parameter vector neuron active pair represents better class larger value log moved current sample neuron moved away sample 
note similarity lvq algorithms kohonen 
note normalized gaussian membership functions model kind variant gaussian mixture models soft vector quantization 
limit crisp feature detectors gaussians output network reduces coded discrete value 
similarly outputs soft version interpreted probability density functions multinomial random variable 
interpretation becker discussed section 
multinomial variables becker 
becker hinton becker introduced learning goal neural networks called imax 
networks consist separate modules having di erent inputs learning algorithms aim maximizing mutual information outputs modules 
example inputs dimensional arrays random dots stereoscopic displacements simulating views eyes networks able infer depth data 
variant called discrete imax becker closely related clustering algorithm 
imax outputs neurons module interpreted probabilities multinomial random variable goal learning maximize mutual information variables modules 
model di ers becker ways 
becker uses normalized exp basis functions parameterization basis functions invariant norms cf 
eqn 
invariance units largest norms may dominate representation phenomenon noted becker 
di erence becker optimizes model gradient descent batch input vectors simple line algorithm adapting basis data sample time 
gradient discrete imax respect parameters simpli cation notation log jv jv dx possible apply stochastic approximation sampling leads di erent adaptation rule 
becker gaussian basis functions approximations having di erent formula gradient 
continuous variables mutual information continuous valued outputs neurons maximized becker hinton becker 
assumptions continuously valued signals noise 
becker hinton outputs assumed consist gaussian signals corrupted independent additive gaussian noise 
multinomial imax reinterpreted soft vector quantization kullback leibler metric distance measured auxiliary space 
neural terms model builds representation input space neuron detector specialized represent certain domain 
contrast continuous version tries represent input space generating parametric transformation continuous lower dimensional output space 
parameterization assumptions noise correct continuous representations potentially accurate 
advantage quantized representation assumptions need model purely data driven semi parametric course useful clustering type applications 
particularly dicult maximize mutual information module continuous valued outputs 
works fisher principe torkkola campbell shannon entropy replaced quadratic renyi entropy yielding simpler formulae mutual information 
information bottleneck distributional clustering distributional clustering works pereira information bottleneck principle tishby mutual information discrete variables maximized 
tishby get motivation rate distortion theory shannon kolmogorov see cover thomas review 
rate distortion theory aim nd optimal codebook set discrete symbols cost form distortion function describing ects transmission line 
notation authors consider problem building optimal representation discrete random variable rate distortion theory real valued distortion function assumed known minimized respect representation conventionally codebook subject constraint ex fd minimum conditional distributions de ning codebook jx exp exp depends authors realized average distortion ex fd replaced mutual information rate distortion theory gives solution captures information relevance variable possible 
multinomial random variable role auxiliary data 
functional minimized variational optimization respect conditional densities leads solution kl cjx equations give characterization optimal representation accept criterion goodness representation 
characterization self referential algorithm nding tishby introduced algorithm nding solution case multinomial method bottleneck aims revealing nonlinear relationships data sets maximizing mutual information cost function 
relation approaches started clustering viewpoint error criterion kl turned equivalent negative mutual information 
bottleneck additional term error function keeping complexity representation low complexity clusters restricted number parameterization 
assumptions parameterizing 
model semi parametric scale parameter similar gaussians vmf kernels approaches zero 
fundamental di erence clustering approach published bottleneck works arises continuity random variable theoretical form bottleneck principle equation limited discrete nite spaces 
knowledge continuous applications principle far published 
continuous distortion readily evaluated kinds additional assumptions restrictions form cluster memberships 
solution parameterize allows optimize partitioning data space soft clusters 
case study clustering gene expression data tested approach clustering large high dimensional data set expressions genes yeast saccharomyces cerevisiae various experimental conditions 
measurements obtained called dna chips functional genomics infer similarity function di erent genes 
currently popular approaches analyzing expression data traditional clustering methods see eisen supervised classi cation methods support vector machines brown 
case study intend show method sides approaches 
majority yeast genes exists functional classi cation biological knowledge 
goal supervised classi ers learn classi cation order predict functions new genes 
classi ers may additionally useful errors genes having known classes may suggest original functional classi cation errors 
traditional unsupervised clustering methods group solely basis expression data utilize known functional classes 
applicable sets genes known classi cation may additionally generate new discoveries may hidden similarities classes hierarchical functional classi cation may exist new subclasses revealed experimental data collected 
clustering methods hypothesis generating machines 
disadvantage currently clustering algorithms results determined metric measuring similarity expression data 
metric somewhat arbitrary considerable amount knowledge functioning genes 
goal known functional classi cation implicitly de ne aspects expression data important data 
clusters local expression data space prototypes placed minimize average distortion space functional classes 
di erence supervised classi cation methods classi cation methods surpass original classes supervised clusters tied classi cation may reveal substructures relations known functional classes 
case study compare method empirically alternative methods demonstrate convergence properties potential usefulness results 
detailed biological interpretation results subsequent papers 
compared model standard state art mixture density models 
rst totally unsupervised mixture von mises fisher distri alternatively solving continuous problem partitioned prede ned clusters standard distributional clustering algorithms applicable 
butions 
model analogous usual mixture gaussians gaussian mixture components simply replaced von mises fisher components 
model vmf vmf de ned 
mixing parameters 
second model hastie joint distribution functional classes expression data modeled set additive components denoted ju xju ju parameters estimated xju vmf 
models tted data maximizing log likelihood em algorithm dempster 
data temporal expression patterns genes yeast measured dna chips experimental settings details see eisen data available rana stanford edu clustering 
sample measures expression level gene compared expression state 
altogether time points gene represented feature vector data preprocessed way brown logarithms individual values normalizing length unity 
data divided training set containing thirds samples test set containing remaining third 
reported results table computed test set 
functional classi cation obtained munich information center protein sequences yeast genome database 
classi cation system hierarchic chose highest level classes supervise clustering 
sample classes include metabolism transcription protein synthesis 
genes belonged classes 
genes removed missing classi cation highest level hierarchy 
experiments rst compared performance models mixture gaussians mda algorithms converged 
models clusters run doubt convergence mixture gaussians mda epochs data model stochastic iterations decreased rst piecewise linear approximation exponential curve linearly zero 
models run times di erent randomized initializations best results chosen 
measured quality resulting clusterings average distortion error equivalently empirical mutual information 
estimating empirical mutual information table joint distributions rst estimated 
model ith row table updated jx eqn www mips biochem mpg de proj yeast psfrag replacements empirical mutual information bits empirical mutual information generated gene expression categories functional classes genes function parameter governs width basis functions 
solid line model dashed line mixture dotted line mda 
de ned sample 
gaussian mixture model update jx mda jx xju 
table computed performance criterion obtained eqn 
constant 
results shown di erent values parameter governs width vmf kernels 
model clearly outperforms models wide range widths kernels produces best performance clusters model convey information functional classi cation genes alternative models 
somewhat surprising results gaussian mixture model mda utilize class information 
reason probably special property data data sets mda outperformed plain mixture model 
demonstrate number iterations required convergence 
empirical mutual information plotted function number iterations 
model schedule decreasing coecient run stretched cover number iterations decaying zero 
number complete data epochs mda comparable number stochastic iterations multiplying number data number kernels divided algorithm kernels updated iteration step 
performance mda attains maximum quickly model surpasses mda iterations fig 

note empirical mutual information upward biased estimate real mutual information bias grows decreasing data 
size sample large constant compared distributions number values discrete variables small bias markedly ect results 
psfrag replacements iterations empirical mutual information bits empirical mutual information function number iterations 
solid line model dashed line mda 

cluster number class table distribution genes learning test set combined sample functional subclasses clusters obtained method 
subclasses supervising clustering 
phosphate pathway acid pathway respiration ribosomal proteins degradation organization chromosome structure 
demonstrate quality clustering showing distribution genes clusters functional subclasses known regulated experimental conditions data eisen 
note subclasses included values auxiliary variable picked second third level functional gene classi cation 
characterize genes learning test sets combined 
table gene assigned cluster having largest value membership function gene 
table reveals subclasses concentrated clusters algorithm 
rst subclasses belong rst level class placed cluster number 
comparison distribution subset genes clusters formed mixture mda shown table 
classes di erent clusters summarized empirical mutual information table mutual information bits approach 
cluster number class table distribution genes learning test set combined sample functional subclasses clusters obtained mixture model mda 
methods yield table subclasses 
explanation classes see table 
table produced method subclasses clearly divided clusters suggesting possible biologically interesting division 
relevance determined biological inspection goal demonstrate semisupervised clustering approach explore data set provide potential hypotheses structure 
described soft clustering method continuous data minimizes cluster distortion distributions associated discrete auxiliary data 
approach inspired earlier explicit density estimator derive information geometric metric similar kinds clustering tasks kaski :10.1.1.26.6383
method conceptually simpler require explicit density estimation known dicult high dimensional spaces 
task analogous distributional clustering pereira multinomial data information bottleneck method tishby learning dyadic data hofmann 
main di erence method works operate discrete nite data space data continuous 
setup cost function connections information bottleneck method approaches equivalent 
showed minimizing kullback leibler divergence distortion criterion equivalent maximizing mutual information neural representations inputs discrete variable studied becker becker hinton becker 
distortion additionally shown bounded conditional likelihood approaches limit clusters sharpen voronoi regions 
derived simple line algorithm optimizing distortion measure 
convergence algorithm proven vmf basis functions kaski 
algorithm shown close connection competitive learning 
applied clustering method yeast gene expression data set augmented independent functional classi cation genes 
algorithm performs better algorithms available continuous data mixture gaussians mda model joint density expression data classes 
method turned relatively insensitive priori set width parameter gaussian parameterization outperforming competing methods wide range parameter values 
shown obtained clusters mediate information function genes results biologically analyzed potentially suggest novel cluster structures yeast genes 
topics include investigation exible parameterizations clusters relationship method metric de ned earlier variations algorithm clusterings continuous auxiliary distributions 
appendix gradient error criterion respect parameters basis functions denote il log ji kl il dx basis functions normalized exponential form gradient lj substituting result gives il log li log ji log li ji conditionally independent respect may write substituting eqn arrive 
appendix connection conditional density estimation denote brevity note conditional entropy cjx log jx dx independent note normalized versions densities called exponential family included interpreted densities random variables jx 
parameters ji distortion eqn expressed kl log ji dx cjx const 
log jx log ji dx const 
jx log jx exp log ji dx const 
jx log jx dx const 
exp log jv fq proper density 
jensen inequality 
jx log jx jx log jx jx jx minimizing clustering criterion kl minimizes upper limit jx log jx jx dx ex fd kl cjx cjx expected kl divergence conditional density cjx estimate cjx 
ex 
denotes expectation 
write ex fd kl cjx cjx log jx dx kl cjx maximizing likelihood model jx data fc sampled asymptotically equivalent minimizing kullback leibler divergence 
samples fc scaled log likelihood log jx converges log jx dx jx log jx jx dx const 
ex fd kl cjx cjx const 
probability maximizing minimizes 
special case binary valued jx proper density equality holds minimization approximation average distortion kl nite data set equivalent maximizing likelihood cjx sample 
acknowledgments supported academy finland part 
wish petri jaakko help simulations processing data 
becker 

mutual information maximization models cortical selforganization 
network computation neural systems 
becker hinton 

self organizing neural network discovers surfaces random dot stereograms 
nature 
brown grundy lin cristianini ares jr haussler 

knowledge analysis microarray gene expression data support vector machines 
proceedings national academy sciences usa 
cover thomas 

elements information theory 
new york wiley 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society series 


model visuomotor mechanisms frog optic 
mathematical biosciences 
eisen spellman brown botstein 

cluster analysis display genome wide expression patterns 
proceedings national academy sciences usa 
fisher iii principe 

methodology information theoretic feature extraction 
proc 
ijcnn international joint conference neural networks pp 

piscataway nj ieee service center 
forgy 

cluster analysis multivariate data eciency vs interpretability classi cations 
biometrics 
gersho 

asymptotically optimal block quantization 
ieee transactions information theory 
gray 

vector quantization 
ieee assp magazine april 
grossberg 

development feature detectors visual cortex applications learning reaction di usion systems 
biological cybernetics 
hastie tibshirani buja 

flexible discriminant mixture models 
kay titterington editors neural networks statistics 
oxford university press 
hofmann puzicha jordan 

learning dyadic data 
kearns solla cohn 
eds advances neural information processing systems pp 

san mateo ca morgan kau man publishers 
hofmann 

learning similarity documents approach document retrieval categorization 
solla leen uller 
eds advances neural information processing systems pp 

cambridge ma mit press 
jaakkola haussler 

exploiting generative models discriminative classi ers 
kearns solla cohn 
eds advances neural information processing systems pp 

san mateo ca morgan kaufmann 
kaski 

convergence stochastic semisupervised clustering algorithm 
technical report helsinki university technology publications computer information science espoo finland 
kaski kohonen 

winner take networks physiological models competitive learning 
neural networks 
kaski sinkkonen 

bankruptcy analysis selforganizing maps learning metrics 
ieee transactions neural networks 
accepted publication 
kohonen 

self organization associative memory rd ed 

berlin springer verlag 
kohonen 

physiological interpretation self organizing map algorithm 
neural networks 
kohonen 

self organizing maps 
springer berlin 
second extended edition 
kohonen hari 

feature maps brain come 
tins 
macqueen 

methods classi cation analysis multivariate observations 
le cam neyman 
eds proceedings fifth berkeley symposium mathematical statistics probability 
vol 
statistics pp 

berkeley ca university california press 
makhoul gish 

vector quantization speech coding 
proceedings ieee 
mardia 

statistics directional data 
journal royal statistical society 
nass cooper 

theory development feature detecting cells visual cortex 
biological cybernetics 
nowlan 

maximum likelihood competitive learning 
touretzky 
ed advances neural information processing systems pp 

san mateo ca morgan kaufmann 
oja 

simpli ed neuron model principal component analyzer 
math 
biology 
pereira tishby lee 

distributional clustering english words 
proceedings th annual meeting association computational linguistics pp 

erez glass 

development speci city cat visual cortex 
journal mathematical biology 
tipping 

deriving cluster analytic distance functions gaussian mixture models 
proc 
icann ninth international conference arti cial neural networks pp 

london iee 
tishby pereira bialek 

information bottleneck method 
th annual allerton conference communication control computing urbana illinois 
torkkola campbell 

mutual information learning feature transformations 
proc 
icml th international conference machine learning pp 

san mateo ca morgan kaufmann 

