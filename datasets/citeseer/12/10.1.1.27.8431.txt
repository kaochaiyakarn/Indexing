turbo factor analysis brendan frey computer science university waterloo waterloo ontario canada 
years ago pearl probability propagation algorithm graphs cycles shown produce excellent results error correcting 
wondered iterative probability propagation successfully machine learning 
step direction study iterative inference learning simple factor analyzer network layer densely connected network models bottom layer sensory inputs linear combination top layer factors plus independent gaussian sensor noise 
number bottom top iterations needed exactly infer factors network input scales number factors top layer 
online learning iterative procedure reinitialized pattern presentation learning prohibitively slow big networks face recognition large scale models cortex 
show probability propagation factor analyzer usually takes just iterations achieve low inference error networks sensors factors 
derive expression algorithm fixed point provide eigenvalue condition global convergence 
show iterative inference online learning give results method online dimensionality reduction purpose recognizing pixel images faces dimensional subspace 
suggests iterative probability propagation densely connected networks may lead broad class useful algorithms machine learning 
uw cs adaptive computation tr apr 
submitted neural computation may 
encoding data linearly combined components simple way encode input patterns suppose input approximated linear combination component vectors amplitudes vectors modulated match input 
training set appropriate set component vectors depend expect modulation levels behave measure distance input approximation 
effects captured generative probability model specifies distribution modulation levels distribution xjz sensors xn modulation levels 
linear combination xn layer generative network describes principal component analysis independent component analysis factor analysis 
constraint xjz lz xjz xp xjz dx column component vector 
matrix elements nk principal component analysis jolliffe linsker oja independent component analysis comon jutten herault bell sejnowski amari cichocki yang factor analysis rubin thayer everitt viewed maximum likelihood estimation models type assume appropriate modulation levels independent distortion sum individual sensor distortions 
layer belief network describes process shown fig 

distribution top layer activities modulation levels distribution bottom layer activities sensors top layer activities xjz jz choose probabilities form jz nk variance sensors maximum likelihood estimation produces components span principal components training data roweis ghahramani tipping bishop 
heavy tailed distribution distortion allowed achieving generally requires complete complete basis maximum likelihood estimation equivalent independent component analysis pearlmutter parra mackay 
example may take jz nk advantage independent component analysis principal component analysis factor analysis heavy tailed distributions modulation levels 
distributions effectively produce sparse representations data heavy tailed distribution places probability mass near zero large values high kurtosis 
factor analysis 
jz gaussian sensor noise variance jz nk maximum likelihood estimation performs factor analysis 
factor analysis modulation levels called factors called factor loading matrix 
contrast principal component analysis factor analysis model different level noise sensor 
contrast independent component analysis noise model factor analysis permits compact latent representation factor analysis model factor analyzer written xjz lz diagonal covariance matrix nth diagonal entry marginal distribution lz dz ll factor analyzer able represent rich nonlinear processes learned nonlinear gaussian belief networks hinton ghahramani frey frey hinton attias simplicity factor analyzer starting point studying iterative probability propagation 
inferring factors factor analyzer factor analyzer gaussian model distribution factors sensor activities gaussian 
posterior characterized expected value covariance matrix obtained normalizing xjz respect linear algebra zjx cov zjx notice covariance matrix depend value input depend input observed 
recall cov 
explore methods infer independent factors focus inferring factor means factor variances diag cov zjx diag cov zjx contains elements derived sensors zjx contains just elements derived sensors diag cov zjx contains just elements hope factor means variances computed order kn scalar operations small number iterations 
unfortunately turns network input order bottom top iterations needed exactly infer factor means variances 
simple way see looking computation generally requires order scalar operations compute exactly 
iteration network kn connections performs order kn scalar operations number iterations order number iterations prohibitively large practical applications large scale models cortex 
course multiplying takes just kn operations factor analyzer fixed parameters inference times say times typical batch processing idea precompute reuse input 
situations direct precomputation idea including online learning parameters change case case inference biological neural network local computations network single shot inference network goal inference pattern fast possible 
cases need algorithm infer factor means variances quickly network input 
describing iterative probability propagation algorithm takes just bottom top iterations inference review diagonal approximation inverse covariance matrix recognition networks conjugate iterative inference 
diagonal approximation inverse covariance matrix 
approximate inverse covariance matrix diagonal need compute diag takes just kn scalar operations 
diagonal approximation equivalent approximating zjx product form distribution minimizes kullback leibler divergence log zjx dz 
approximate diagonal covariance matrix cov pf zjx diag kth diagonal element cov pf zjx var pf jx nk variances computed bottom pass network fig 

recognition networks 
helmholtz machines dayan hinton recognition network infer factor means sensory input 
bottom pass recognition network computes matrix recognition weights 
exact inference factor means weights direct computation circumvented wake sleep algorithm hinton simulates samples generative network fig 
adjusts recognition weights local delta rule 
inference recognition network takes just iteration network learned recognition networks efficient doing shot inference 
learning uses sample statistics large number iterations may needed average noise neal dayan 
wake sleep algorithm helmholtz machine attractive biological model uses simple local computations feedforward network feedback network 
conjugate iterative inference 
approach computing zjx repeatedly update approximation ensuring successive update spoil improvements achieved previous updates 
idea conjugate gradient optimization method fletcher approach viewed form variational inference jordan uses product form gaussian approximation zjx 
inferring factor means equivalent maximizing xjz respect posterior proportional joint distribution 
negative exponent xjz cost function estimates factor means step give cost nk step conjugate gradient optimization computes current gradient cost function scalar factor uses gradient compute new search direction scalar factor finds minimum cost new search direction 
quadratic function variables procedure finds minimum exact values factor means steps top bottom iterations 
current estimates factor means error input computed top pass nk gradient kth top layer unit computed subsequent bottomup pass nk search direction kth top layer unit compute new estimates factor means computation local graph fig 
implemented network connecting top layer units 
current gradient gradient previous iteration need stored 
new estimate kth factor nl computation requires communication top layer units 
initially take nk applying compute pair summations denominator require additional top bottom pass 
step conjugate iterative inference takes top bottom iterations network 
approximate inference performed fewer iterations simply stopping procedure small number iterations 
iterative probability propagation fast local method networks trees probability propagation algorithm exact inference 
algorithm extensively th century gallager tanner pearl lauritzen spiegelhalter dates back th century thiele steffen lauritzen personal communication 
probability propagation algorithm uses simple local computations produce numbers passed edges network combines numbers locally compute marginal distributions exactly 
results error correcting coding show cases probability propagation gives excellent performance network contains cycles exact inference exponentially difficult mackay neal frey kschischang frey frey mackay kschischang frey mceliece mackay cheng :10.1.1.15.6659
probability propagation error correction berrou glavieux low density parity check codes mackay widely considered major breakthrough information theory community network contains cycles local computations give rise iterative algorithm hopefully converges answer 
despite excellent performance error correcting coding applications little known convergence properties iterative probability propagation 
probability propagation networks containing single cycle successfully analyzed weiss smyth 
case called tail biting forney 
aji 

results networks containing cycles revealing wiberg richardson frey koetter vardy mackay 
show iterative probability propagation converges correct answer iterations random factor analysis networks derive expression algorithm fixed point provide eigenvalue condition global convergence 
sec 
show fast inference method successfully online learning turbo factor analysis perform dimensionality reduction face modeling recognition 
iterative propagation means variances 
factor analyzer gaussian model probability message produced probability propagation characterized mean variance 
iteration probability propagation factor analysis network shown fig 
consists passing mean variance edge bottom fact gallager invented algorithm decades earlier computers available time powerful manipulate large amounts data needed show algorithm works amazingly 
pass followed passing mean variance edge top pass 
instant bottom means variances combined form estimates means variances factors input 
prior distribution top layer unit standard normal distribution initially set variance mean sent kth top layer unit nth sensor kn kn 
bottom pass begins computing noise level error signal sensor top variances means previous iteration nk kn nk kn compute bottom variances means follows nk nk kn nk nk kn nk allowed nk nk nk nk propagated nk nk assume nk 
bottom variances means combined form current estimates factor variances means nk nk nk top pass proceeds computing top variances means follows kn nk kn kn nk nk notice variance updates independent mean updates mean updates depend variance updates 
performance iterative probability propagation 
created total factor analysis networks different sizes ranging size network measured inference error function number iterations probability propagation 
networks size produced drawing nk independent standard normal distributions drawing sensor variance exponential distribution mean equal nk procedure ensured scales components varied significantly sensor noise levels varied mean level equal variation due components 
similar procedure neal dayan 
random network pattern simulated network probability propagation applied simulated pattern input 
measured error estimate correct value zjx computing difference coding costs exact posterior distribution normalizing get average number nats top layer unit error log jx log zjx jx zjx cov zjx zjx equivalent normalized mahalanobis distance zjx 
fig 
shows inference error logarithmic scale versus number iterations maximum probability propagation different network sizes 
plot shows median error bold curve range errors lie curves adjacent bold curve error errors lie fourth topmost curve 
cases median error reduced nats iterations 
number iterations needed achieve inference error increases sublinearly comparing median error median error see increases factor number iterations needed reduce error nats increases factor just 
rate convergence error appears increase larger indicated general trend error curves increased 
network sizes networks converge nat iterations 
network sizes networks converge nat iterations 
network sizes networks diverge 
convergent variances divergent means 
better understand divergent cases plotted means variances function number iterations networks diverge 
call network divergent inference error iterations greater error iterations 
networks diverged networks diverged networks diverged 
performance iterative probability propagation 
median inference error bold curve logarithmic scale function number iterations different sizes network parameterized curves adjacent bold curve show range errors lie 
errors fourth topmost curve 
fig 
shows error log scale bottom variances log scale top means iterations divergent networks sizes 
cases variances converge iterations means oscillate diverge 
fact networks explored variances converged iterations 
observation suggests general dynamics iterative probability propagation factor analysis networks determined dynamics mean updates 
course mean updates depend values converged variances 
fixed points condition global convergence turns variance updates converge dynamics iterative probability propagation factor analysis networks linear 
allows derive fixed point probability propagation closed form write eigenvalue condition global convergence 
error bottom variances top means function number iterations maximum divergent networks sizes 
analyze system mean updates define length kn vectors means input kn nk xn xn repeated times vector 
network parameters represented kn kn diagonal matrices nk identity matrix 
converged bottom variances represented diagonal matrix nk summation operations propagation formulas represented kn kn matrix sums means sent top layer kn kn matrix sums means sent sensory input matrices blocks block ones identity matrix 
representations bottom pass top pass diag 
substituting get linear update diag 
variance updates cause variances strictly greater inverses formula exist 
notice matrix diag fact diagonal matrix 
set means fixed point implies diag 
unique solution equation exists fixed point diag 
fixed point exists determinant nk nk matrix large braces nonzero 
computing determinant directly costly simplified 
determinant written matrix blocks det andn andn dn nk nk nk mk nk mk assuming nk variance updates leave variances strictly greater exists exists 
sake determining nonzero post multiply row obtaining det ji expression existence fixed point determined inverting matrices computing determinant matrix 
reinterpreting dynamics dynamics stability fixed point determined largest eigenvalue update matrix diag 
dn dn modulus largest eigenvalue fixed point stable 
system linear stable fixed point exists system globally convergent point 
error logarithmic scale versus number iterations logarithmic scale maximum probability propagation networks fig 
means initialized fixed point solutions 
numerical errors due machine precision cause divergence fixed points errors shown horizontal lines 
modulus largest eigenvalue beneath plot 
networks size 
examples unstable fixed points 
networks explored networks converged 
stable fixed point implies globally convergent network follows divergent networks stable fixed points 
divergent networks fig 
iterations probability propagation compute steady state variances 
computed modulus largest eigenvalue system computed fixed point 
initializing bottom means fixed point values performed iterations probability propagation see numerical errors due machine precision cause divergence fixed point 
fig 
shows error versus number iterations logarithmic scales network modulus largest eigenvalue 
expected modulus largest eigenvalue greater case 
interesting cases network diverges fixed point reaches dynamic attractor lower average error fixed point 
basins attraction dynamic attractors local cases dynamic attractor top means initialized see corresponding plots fig 

turbo factor analysis perform maximum likelihood factor analysis online fashion parameter modified slightly increase log probability current sensory input log 
derivatives log easy compute derivatives log easy compute log includes terms log yj 
derivatives log unobserved variables filled probabilistic inference 
distribution inference method log bounded jensen inequality see neal hinton log dz log inference determined current input derivatives averaging derivatives log respect 
rewritten show consists true log probability input minus kullback leibler inference distribution true posterior log log zjx dz perform online maximum likelihood factor analysis zjx bound tight log 
inexact inference zjx gradient compromise increasing likelihood current input making generative model better suited approximate inference distribution 
explore methods infer independent gaussian factors 
estimated mean variance kth factor leaving terms don depend parameters log nk nk parameter nk adjusted gradient step nk nk nk learning rate 
solved exactly 
neal dayan set equal linear combination old value exact solution different learning rate updating weights variances equal learning rates 
turbo factor analysis consists performing number iterations probability propagation current input iterations modifying parameters described processing input 
method simulate data train networks extensively explore performance turbo factor analysis produced training sets cases input sizes ranging sensors sensors 
sizes factor analyzer randomly selected sets parameters generated training set 
factor analyzer sizes elements independently drawn standard normal distribution sensor variance set equal noise free variance nk 
factor analyzer simulated data set estimated optimal log probability data iterations exact em try find best model 
training set quite small possible factor analyzer data came basin attraction local optimum log probability data 
learning methods size model trained set equal size model generate data 
parameters initialized randomly sensor variances estimated training cases 
difficult fair comparing online learning algorithms speed learning sensitive learning rate different methods may different suitable ranges learning rates 
learning rate allowed change learning usually trade speed convergence final log probability training set 
allowing learning rate change fair comparisons difficult different methods may different ideal strategies adapting learning rate 
way try circumvent ambiguities search achievable learning curves regardless simple expression modifying learning rate exists 
method randomly drawn set initial parameters performed separate epoch learning learning rates picked model learning rate gave greatest increase log probability data 
successive learning rate determined performing epoch learning old learning rate performing epoch learning old learning rate scaled 
model learning rate gave greatest increase log probability data kept 
achievable errors number epochs learning turbo factor analysis iterations versus turbo factor analysis iteration 
plot horizontal axis gives log probability error logarithmic scale turbo factor analysis iteration vertical axis gives error number epochs turbo factor analysis iterations 
discussion results simulated data analyzing results mainly interested comparing achievable curves different methods differences scale methods trained data plot error optimal log probability minus log probability learned model method log probability error method 
approach removes number epochs comparison illuminates error achievable method error achieved method number epochs 
fig 
shows achievable errors number epochs learning turbo factor analysis iterations versus turbo factor analysis iteration 
aside small number local optima iterations probability propagation produces networks lower errors achievable errors number epochs learning turbo factor analysis iterations versus online factor analysis diagonal approximation inverse covariance matrix 
plot horizontal axis gives log probability error logarithmic scale diagonal approximation vertical axis gives error number epochs turbo factor analysis iterations 
learned iteration probability propagation 
difference significant networks large sec 
convergence inference error slower 
fig 
shows achievable errors number epochs learning turbo factor analysis iterations versus online factor analysis diagonal approximation described sec 

diagonal approximation appears adequate smaller networks effective larger networks 
fig 
shows achievable errors number epochs learning turbo factor analysis iterations versus wake sleep learning iterations 
generally turbo factor analysis achieves smaller errors wake sleep learning small wake sleep performs achievable errors number epochs learning turbo factor analysis iterations versus wake sleep learning iterations 
plot horizontal axis gives log probability error logarithmic scale wake sleep learning vertical axis gives error number epochs turbo factor analysis 
better close optimum log probability 
may due fact noise introduced wake sleep sampling smaller lower significant difference methods occurs large aside local optima turbo factor analysis achieves nearly optimal log probabilities log probabilities wake sleep learning close values start learning 
fig 
shows achievable errors number epochs learning turbo factor analysis iterations versus online learning iterations conjugate inference 
methods perform similarly cases aside small number local optima turbo factor analysis 
achievable errors number epochs learning turbo factor analysis iterations versus online learning iterations conjugate inference 
plot horizontal axis gives log probability error logarithmic scale conjugate iterative inference vertical axis gives error number epochs turbo factor analysis 
online face recognition 
factor analyzer probabilistic dimensionality reduction want train flexible model multilayer perceptron high dimensional data 
fig 
shows examples set greyscale face images different people 
contrast data sets test face recognition methods notably feret data set phillips faces data set include wide variation expression pose 
classification difficult normalized images person pixel mean variance 
examples normalized images shown fig 

turbo factor analysis wake sleep learning reduce dimensionality data online dimensions dimensions 
turbo factor analysis arbitrarily chose learning rate examples data set face images people 
normalized versions images 
person pixel scaled offset order statistics images different people 
wake sleep learning tried learning rates ranging 
time factor analyzer trained online multilayer perceptron hidden layer tanh units output layer softmax units trained gradient descent predict face identity factor means 
learning rate multilayer perceptron set highest value generally value regardless method dimensionality reduction 
image presentation prediction identity person image loss measured factor analyzer parameters multilayer perceptron parameters modified 
note classification error backpropagated factor analyzer network 
fig 
shows online error curves obtained filtering losses uniform window samples long 
performance turbo factor analysis solid line compared versions wake sleep learning number pattern presentations online face recognition error curves turbo factor analysis solid wake sleep learning different learning rates dashed forms online nearest neighbor classification dot dashed 
dotted line top plot shows error rate obtained guessing 
different learning rates dashed lines error rate obtained guessing dotted line 
shows error curves forms online nearest neighbor classification cases prediction 
form nearest neighbor classification performs worst set storage requirements factor analysis multilayer perceptron method 
better form nearest neighbor classification set number computations turbo factor analysis multilayer perceptron method 
final error rate turbo factor analysis method lower final error rates models wake sleep learning train factor analyzer 
comparison nearest neighbor classifiers performance turbo factor analysis similar expected parametric methods performs better nearest neighbor method constrained memory 
discussion investigation behavior performance iterative probability propagation densely connected adaptive network inspired record breaking performance technique applied error correcting decoding 
chose explore factor analyzer network exact inference measure error iterative probability propagation method propagation equations analyzed 
showed iterative probability propagation densely connected factor analyzer network usually takes just iterations achieve low inference error networks sensors factors 
derived expression algorithm fixed point provided eigenvalue condition global convergence 
recommend iterative probability propagation replacement exact inference factor analysis networks results simulated data real data show method performs remarkably inference learning considering network densely connected 
pointed frey mackay iterative probability propagation networks error correcting codes may posterior probability mass concentrated single mode microstate 
results partly support hypothesis show posterior unimodal iterative probability propagation infrequently diverges 
divergent cases error correcting decoding small networks mceliece cheng reports anecdotal divergence large networks practical interest 
results show factor analyzer networks divergence occurs larger networks 
direction research study eigenvalue condition global convergence infinite limit motivated research iterative probability propagation networks error correcting decoding important differences clarified 
factor analyzer contains real valued variables networks studied far error correcting decoding contain discrete variables 
second techniques analyze iterative probability propagation networks error correcting decoding gallager mackay rely minimal cycle length growing obviously minimal cycle length factor analyzer network independent possible direction analysis probability propagation factor analyzer network account double counting probability messages pearl 
double counting occurs message circulating network passes vertex second time 
double counting occurs network appears tree inference exact subgraph forms tree 
factor analyzer network double counting occurs iteration 
hope equal double counting occurs weiss errors due double counting balance 
approach related balancing computation tree frey koetter vardy graph obtained unrolling messages passed original network time 
currently interested iterative probability propagation inference learning hierarchical nonlinear models vision exact inference exponentially difficult 
analyzing iterative probability propagation models straightforward believe potential benefit greater 
acknowledgments research supported nserc beckman foundation 
aji horn mceliece 
convergence iterative decoding graphs single cycle 
proceedings ieee international symposium information theory 
amari cichocki yang 
new learning algorithm blind signal separation 
touretzky mozer hasselmo editors advances neural information processing systems 
mit press cambridge ma 
attias 
independent factor analysis 
neural computation 
bell sejnowski 
information maximization approach blind separation blind deconvolution 
neural computation 
berrou glavieux 
near optimum error correcting coding decoding turbo codes 
ieee transactions communications 
comon jutten herault 
blind separation sources 
signal processing 
dayan hinton neal zemel 
helmholtz machine 
neural computation 
everitt 
latent variable models 
chapman hall new york ny 
fletcher 
practical methods optimization 
john wiley sons new york ny 
forney jr kschischang marcus 
iterative decoding tail biting 
proceedings information theory workshop 
frey 
graphical models machine learning digital communication 
mit press cambridge ma 
see www cs utoronto ca frey 
frey hinton 
variational learning non linear gaussian belief networks 
neural computation 
available www cs utoronto ca frey 
frey koetter vardy 
skewness iterative decoding 
proceedings ieee international symposium information theory 
frey kschischang 
probability propagation iterative decoding 
proceedings th allerton conference communication control computing 
frey mackay 
revolution belief propagation graphs cycles 
jordan kearns solla editors advances neural information processing systems volume pages 
mit press 
available www cs utoronto ca frey 
frey mackay 
trellis constrained codes 
proceedings th allerton conference communication control computing 
available www cs utoronto ca frey 
gallager 
low density parity check codes 
mit press cambridge ma 
hinton dayan frey neal 
wake sleep algorithm unsupervised neural networks 
science 
hinton ghahramani 
generative models discovering sparse distributed representations 
philosophical transactions royal society london 
jolliffe 
principal component analysis 
springer verlag new york ny 
jordan ghahramani jaakkola saul 
variational methods graphical models 
jordan editor learning graphical models 
kluwer academic publishers norwell ma 
kschischang frey 
iterative decoding compound codes probability propagation graphical models 
ieee journal selected areas communications 
lauritzen spiegelhalter 
local computations probabilities graphical structures application expert systems 
journal royal statistical society 
linsker 
self organization perceptual network 
computer 
mackay 
maximum likelihood covariant algorithms independent component analysis 
unpublished manuscript available wol ra phy cam ac uk mackay 
mackay 
error correcting codes sparse matrices 
ieee transactions information theory 
mackay neal 
near shannon limit performance low density parity check codes 
electronics letters 
reprinted electronics letters vol 
march 
mceliece mackay cheng 
turbo decoding instance pearl belief propagation algorithm 
ieee journal selected areas communications 
mceliece cheng 

turbo decision algorithm 
proceedings rd allerton conference communication control computing 
neal dayan 
factor analysis delta rule wake sleep learning 
neural computation 
neal hinton 
view em algorithm justifies incremental sparse variants 
jordan editor learning graphical models pages 
kluwer academic publishers norwell ma 
oja 
neural networks principal components subspaces 
international journal neural systems 
pearl 
probabilistic reasoning intelligent systems 
morgan kaufmann san mateo ca 
pearlmutter parra 
maximum likelihood blind source separation context sensitive generalization ica 
mozer jordan petsche editors advances neural information processing systems 
mit press cambridge ma 
phillips moon rizvi rauss 
feret evaluation methodology face recognition algorithms 
submitted ieee transactions pattern analysis machine intelligence 
richardson 
geometric perspective 
proceedings ieee international symposium information theory 
roweis ghahramani 
unifying review linear gaussian models 
neural computation 
rubin thayer 
em algorithms ml factor analysis 
psychometrika 
smyth mceliece xu aji horn 
probability propagation graphs cycles 
workshop inference learning graphical models vail colorado 
tanner 
recursive approach low complexity codes 
ieee transactions information theory 
tipping bishop 
mixtures probabilistic principal component analyzers 
neural computation 
weiss 
correctness local probability propagation graphical models 
appear neural computation 
wiberg 
codes decoding general graphs 
department electrical engineering link oping university link oping sweden 
doctoral dissertation 

