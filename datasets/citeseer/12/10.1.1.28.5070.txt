category statistical language models thomas niesler st john college june thesis submitted university cambridge partial fulfilment requirements degree doctor philosophy synopsis language models computational techniques structures describe word sequences produced human subjects considers primarily application automatic systems 
due complex nature natural languages need robust recognition statistically language models assign probabilities word sequences proved successful 
thesis focuses linguistically defined word categories means improving performance statistical language models 
particular approach aims capture general grammatical patterns particular word dependencies different model components proposed developed evaluated 
account grammatical patterns model employing variable length grams part speech word categories developed 
local syntactic patterns english text captured conveniently gram structure reduced sparseness data allows larger employed 
technique optimises length individual grams proposed experimental tests show lead improved results 
model allows words belong multiple categories order cater different grammatical functions may employed tagger assign category classifications new text 
category model important advantage generalisation unseen word sequences nature able capture relationships particular words 
experimental comparison word gram approaches reveals ability important language model quality consequently methods allowing inclusion word relations developed 
method allows incorporation selected word grams backoff framework 
number word grams added may controlled resulting tradeoff size accuracy shown surpass standard techniques gram cutoffs 
second technique addresses longer range word pair relationships arise due factors topic style text 
empirical evidence demonstrating approximately exponentially decaying behaviour considering probabilities related words function appropriately defined separating distance 
definition fundamental approach terms category assignments words 
minimises effect syntax word occurrences particular advantage grammatical word classifications implicit operation category model 
related words treated model size may constrained reasonable levels 
methods means related word pairs may identified large corpus techniques allowing estimation parameters functional dependence shown lead performance improvements 
proposed combination modelling approaches shown lead considerable perplexity reductions especially sparse training sets 
incorporation models led significant improvement word error rate high performance baseline speech recognition system 
declaration thesis result original draws acknowledged appropriate points text 
thesis submitted part degree institution 
published previously conference proceedings 
length thesis including appendices footnotes approximately words 
foremost supervisor phil woodland 
experienced opinion sound advice guided course research successful consistent support able publish parts research respected conferences 
privilege supervision 
express gratitude st john college provided financial support form scholarship doctoral studies possible 
furthermore acknowledge generous additional funds provided harry crossley foundation cambridge university engineering department allowed research conferences atlanta philadelphia munich 
am grateful claudia arena anna lindsay christophe molina eric piotr sister proofreading sections manuscript 
help result research appears 
fortunate spend working hours colleagues speech laboratory continued assistance tea 
furthermore owe great deal patrick gosling andrew gee carl seymour maintainers excellent computing facilities certainly possible 
recognition experiments thesis accomplished entropic lattice language modelling toolkit julian odell major contribution preparation software phase research simpler 
say grateful am family enduring support duration studies faith abilities 
page table contents 

speech recognition problem 
preprocessing speech signal 
acoustic model 
language model 
dominant language modelling techniques 
scope thesis 
modelling syntactic relations 
modelling fixed short range semantic relations 
modelling long range semantic relations 
thesis organisation 
overview language modelling techniques 
perplexity 
equivalence mappings word history 
probability estimation sparse data 
turing estimate 
deleted estimation 
discounting methods 
linear discounting 
absolute discounting 
backing refined distributions 
deleted interpolation 
modified absolute discounting 
gram models 
category language models 
word categories 
automatic category membership determination 
word groups phrases ph dissertation thomas niesler cambridge university june page ii 
longer term dependencies 
pairwise dependencies 
cache models 
stochastic decision tree language models 
domain adaptation 
specialisation target domain 
mixtures topic specific language models 
language models applications 
character handwriting recognition 
machine translation 
spelling correction 
tagging 
variable length category grams 
overview 
structure language model 
estimating jh 
estimating jv 
estimating gamma 
beam pruning 
employing language model tagger 
performance evaluation 
tagging accuracy 
lexicon augmentation 
beam pruning 
tagging switchboard wsj corpora 
constructing category trees 
word perplexity 
comparing word category models 
probability estimates 
effect backoffs 
category analysis 
gram analysis 
robustness domain change 
summary ph dissertation thomas niesler cambridge university june page iii 
word category backoff models 

exact model 
approximate model 
model complexity determining 
testing gram counts 
testing effect probability 
model building procedure 
results 
lob corpus 
switchboard corpus 
wsj corpus 
summary 
word pair dependencies category language models 

terminology 
probabilistic framework 
estimating 
estimating fl ae 
typical estimates 
determining trigger target pairs 
pass 
second pass 
regulating memory usage 
example pairs 
perplexity results 
discussion 
summary ph dissertation thomas niesler cambridge university june page iv 
word error rate performance 
language models recognition search 
best rescoring 

lattice rescoring 

recognition experiments 
baseline results 
rescoring results 
category model 
word category backoff model 
category model long range correlations 
word category backoff model long range correlations 
summary 
summary 
review conducted 
category syntactic model 
inclusion word grams 
inclusion long range word pair relations 
topics investigation 
tagging 
data driven refinement category definitions 
punctuation 
lattice rescoring 
final summary 
ph dissertation thomas niesler cambridge university june page appendix major text corpora appendix leaving cross validation appendix dealing unknown words appendix experimental corpora baseline language models 

lob corpus 
preprocessing 
corpus statistics 
baseline word gram models 
switchboard corpus 
preprocessing 
corpus statistics 
baseline word gram models 
wall street journal wsj corpus 
preprocessing 
corpus statistics 
baseline word gram models appendix lob corpus word categories 
part speech tags lob corpus 
part speech tag assignments appendix tag mappings appendix trigger approximations 
approximation 
second approximation appendix truncated geometric distribution ph dissertation thomas niesler cambridge university june chapter page chapter research language modelling aims develop computational techniques structures describe word sequences produced human subjects 
models employed deliver assessments correctness plausibility samples text essential tools research fields 
thesis primarily considers application speech recognition developed major research area past years 
current principal objective development large vocabulary recognisers natural unconstrained connected speech 
despite consistent progress considerable advances necessary widespread industrial application feasible 
broad spectrum potential applications ensure technology gain extreme importance matures 
major approaches modelling human language may identified 
relies syntactic semantic analyses sample text determine hierarchical sentence structure 
analyses employ set rules ascertain sentence permissible possible describe significant proportion english usage way complete coverage remained elusive 
due part continuous changes place living language 
furthermore utterances clearly grammatical occur natural language dealt analyses 
significant likelihood failure naturally occurring circumstances led infrequent rule approach speech recognition systems 
second approach statistical techniques intrinsically greater robustness grammatical irregularities usually taken 
statistical language models assign word utterance probability value deemed likelihood context surrounding word sequence 
probabilities inferred large body example text referred training corpus 
way model may come reflect language usage practice grammatical 
advantage may taken vast increases amount available training text runs hundreds millions words 
speech recognition system find sentence hypothesis spoken utterance 
dealing connected speech words unknown number boundaries time 
large vocabularies leads extremely high number possible alternative segmentations acoustic signal words 
context language model evaluates linguistic plausibility partial complete hypotheses 
conjunction remainder recognition system estimate assists finding hypotheses lead correct result may discarded order limit number practical levels 
examples applications include automatic dictation systems hearing systems deaf automated telephone enquiry automated teaching foreign languages voice control electronic computer equipment 
ph dissertation thomas niesler cambridge university june chapter page focuses application speech recognition systems language models important components areas handwriting recognition machine translation spelling correction part speech tagging 
sections describe main elements speech recognition system highlight role language model outline scope remainder thesis 

speech recognition problem speech recognition aim find sequence words observed acoustic data achieved finding maximises conditional probability 
bayes rule delta constant acoustic signal equivalent strategy find arg max delta acoustic component speech recogniser compute language model estimate prior probability certain sequence words 
focusing attention background acoustic processing 

preprocessing speech signal speech obtained microphone recording device form analogue electrical signal 
processed form suitable speech recognition system series operations commonly referred collectively front 
usually steps include band limiting signal sampling applying spectral transformation encode frequency characteristics 
step normally employs short term discrete fourier transform particular popular choice encode speech mel frequency cepstral coefficients capture spectral characteristics signal mel frequency scale 
resulting discrete time sequence observation vectors output front passed recognition algorithm 

acoustic model central speech recognition system means encoding sounds comprising human speech 
successfully achieved hidden markov models hmms approaches met success 
describing observation vectors probabilistic time series hmm takes inherent natural variability human speech characteristics account 
set examples particular sound form corresponding observation vector sequences parameters model may adjusted best represent data probabilistic sense optimising 
model may consequently employed evaluate likelihood new observation sequence respect parameters giving indication similar new measurement originally determine parameters 
likelihood statistical decision regarding utterance recognised illustrated brief overview section 
ph dissertation thomas niesler cambridge university june chapter page depicts simple speech recognition system capable distinguishing words hello goodbye 
front preprocessor acoustic model hello acoustic model goodbye likelihood hello speech waveform hello time observation vectors simple speech recognition system 
hmms may trained model entire words directly model subword units phonemes concatenated obtain words 
approach usually adopted moderately sized vocabulary may sufficient training material determine word models reliably 

language model acoustic model indicates certain sequence words matches measured acoustic evidence task language model estimate prior probability word sequence gamma gamma gamma sequence words question denotes estimate 
probability assist speech recognition system deciding possibly acoustically similar competing ways segmenting observation vectors words linguistic likelihood 
definition conditional probabilities may decompose joint probability equation product conditional probabilities gamma gamma gamma practice gamma indicates start sentence symbol 
language model speech recognition process usually requires evaluation conditional probabilities appearing right hand side equation normally estimated directly 
developing language model task find suitable structures modelling probabilistic dependencies words natural language structures estimate conditional joint probabilities word sequences 

dominant language modelling techniques section presents brief summary language modelling approaches prevalent systems 
aim merely place scope thesis context extensive review chapter 
currently popular statistical language model word gram estimates probabilities observed frequencies word tuples training corpus 
particular probability sequence consecutive words 
ph dissertation thomas niesler cambridge university june chapter page word calculated frequency tuple constituted preceding gamma words utterance word 
models advantage quite simple implement computationally efficient recognition able benefit increasing amount available training data 
tuple treated independently models fail capture general linguistic patterns fact adjectives normally precede verbs 
attempt model possible english tuple individually 
inefficient information corpus lead data fragmentation consequent poor generalisation tuples occur training set possible real utterances 
number tuples extremely large increases models complex terms number parameters employ 
large size consequent memory requirements training set sparseness associated large numbers parameters limits 
models capture associations span number words 
despite restrictions remain successful type language model currently 
order counter sparseness training corpus improve model generalisation language models group words categories proposed 
pooling data words category model parameters may estimated reliably capturing patterns category opposed word level possible generalise word sequences training data 
category definitions may available priori instance part speech classifications indicating grammatical function word may determined automatically means optimisation process 
models bigrams part speech categories exhibited competitive performance relative word counterparts sparse training corpora fare amount training material increases 
optimisation algorithms category assignments allow gap narrowed particularly number categories increased sympathy size training set suffer high computational complexity 
bigram trigram language models automatically determined categories successfully recognition systems training set small success larger tasks conjunction word gram models 
fundamental limitation gram approach possible capture dependencies spanning words 
consequently model generally unable capture long range relations arising factors topic style text 
empirical evidence suggests word seen passage significantly recur near expected 
cache language model component addresses dynamically increasing probability words seen text history way adapts local characteristics training set 
caches usually employed conjunction gram models led performance improvements 
capture relationships different words carried finding associations word pairs 
correlated word pairs detected measuring mutual information related measure incorporated language model various techniques 
performance improvements achieved shown due correlations words effect addressed cache 
state art speech recognition systems continue employ gram language models words training sets large word categories smaller possible addition cache component 
ph dissertation thomas niesler cambridge university june chapter page 
scope thesis basic assumption thesis may classify patterns language manner 
syntactic patterns refer aspects text structure imposed grammatical constraints instance phenomenon adjectives followed nouns 

semantic patterns result meaning words may fixed short range relations example particular adjective bright immediately precedes particular noun light 
loose long range relations example nouns sand desert may expected occur sentence 
word grams attempt model information simultaneously treating possible tuple individually 
failing identify general linguistic patterns proven success approach emphasises importance detailed word relations 
objective develop model deals separately type pattern classification reduce data fragmentation inherent word grams 
particular syntactic behaviour modelled explicitly may take advantage available prior linguistic knowledge neglected word approaches 
syntactic patterns english expected universally consistent word tuple frequencies lead better generalisation text styles topics different training material 

modelling syntactic relations word order important grammatical correctness english captured naturally effectively grams 
furthermore english local syntactic constructs may captured grams despite limited range 
reasons syntactic language model component described chapter grams word categories categories chosen correspond part speech classifications means incorporating priori grammatical information straightforward manner 
words generally grammatical function necessary model maintain possible classifications sentence history terms category sequences 
generally smaller number categories words lowers training corpus sparseness allows increased longer range syntactic correlations may accounted 
particular technique proposed selectively extends length individual grams expected benefit performance allows number parameters minimised improving performance 
result variable length category gram language model 

modelling fixed short range semantic relations certain word combinations occur frequently extrapolation syntactic behaviour suggest bigram proper nouns united kingdom 
sequences best modelled word grams 
chapter presents technique means important word grams may combined syntactic model component backoff framework 
particular word gram calculate probability possible syntactic ph dissertation thomas niesler cambridge university june chapter page model serves fallback cases 
allows frequent word tuples modelled directly captured syntactic model 
care necessary designing mechanism multiple classifications sentence history category sequences complicate normalisation statistical model 

modelling long range semantic relations semantic relations due factors topic style text may span words modelled fixed length sequences 
chapter appropriate definition distance words occurrence probability member related word pair displays exponentially decaying behaviour function separation 
distance measure defined particular goal minimising effect syntax word occurrences takes advantage grammatical word classifications implicit operation syntactic model 
assuming exponentially decaying functional dependence occurrence probability word distance related word long range correlations may captured 
related words treated restrict word pairs model size may constrained reasonable levels 
methods means related word pairs may identified large corpus techniques allowing parameters functional dependence estimated 

thesis organisation chapter presents detailed exposition statistical language modelling field 
chapter describes development syntactic language model component chapter incorporation short range semantic relations word grams chapter modelling long range semantic relations means word pair dependencies 
chapter shows language models may integrated speech recognition system presents recognition results models developed thesis 
chapter presents summary 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page chapter overview language modelling techniques chapter introduces major statistical language modelling concepts approaches 
appendix gives brief summary text corpora experimental results encountered frequently referred simply name 
frequent corpora described individually relevant 

perplexity true quality language model evaluated execution entire recognition experiment utility implicitly linked behaviour acoustic model 
particular language model important distinguishing acoustically similar word hypotheses grounds possible linguistic dissimilarity 
conversely language model need discriminate strongly words significantly dissimilar acoustic point view 
execution complete recognition experiment computationally costly desirable way evaluating quality language model independently linking acoustic component recogniser 
consequently language model viewed isolation 
regard natural language study produced information source emits symbols discrete time intervals certain finite set statistical law 
symbols words language refer general concept syntactic word categories 
process emission symbol source referred event 
assuming symbols gamma fz gamma emitted source probability emission sequence gamma event self information sequence gamma gamma gamma delta log gamma self information information theoretic measure amount information gained witnessing sequence gamma 
rare sequences correspondingly low associated probabilities carry larger amount information sequences seen frequently 
event entropy ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page average event self information source gamma lim delta gamma gamma delta log gamma ji summation taken possible event sequences length may produced source 
entropy average measure amount information contained set sequences source capable producing 
source able emit wide variety different sequences higher entropy capable producing limited number 
assuming information source ergodic may write ensemble average gamma lim delta log gamma point assumed true probability associated sequence gamma known equation yield actual entropy source 
true mechanism means language produced unknown able calculate approximations probabilities gamma 
furthermore length sequences gamma disposal finite practical situations approximate equation gamma delta log gamma entropy source approximated average event log probability observed sequence 
language modelling framework sequence training corpus 
shown ergodic source true due ergodicity source analysis sufficiently long sequence results entropy estimate lim gamma corresponding obtained probability estimate gamma approximation perfect delta delta imperfect approximation delta assign nonzero probability invalid sequences delta property ergodicity implies ensemble averages certain desired statistical characteristic mean example equal corresponding time averages 
ensemble average average time averages possible member functions member function single observed sequence 
member function gamma statistical property gamma gamma delta time average case lim delta gamma gamma delta ensemble average lim delta gamma gamma gamma delta delta gamma gamma delta summation member functions 
process producing member functions gamma ergodic shown ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page probabilities sum unity gamma gamma gamma gamma estimated probability observed valid sequence lower exact result intuitively appealing implies perfect model source assign highest probability observed sequence gamma imperfections model due approximation delta lead lower probability 
model assigns highest probabilities data gathered source output probability logarithm may taken measure model quality 
measure independent sequence length event average may gamma gamma logarithm base obtain entropy estimate gamma delta log gamma customary language modelling applications perplexity pp measure model quality 
perplexity uniquely related estimated entropy pp gamma gamma perplexity reciprocal geometric mean probability sequence gamma may interpreted average branching factor sequence time instant source model 
example vocabulary size grammar assume gamma sequence words gamma gamma equation leads result pp absence grammar word may follow word equal probability ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page decomposition formula perplexity corresponds log pp gamma gamma log gamma perplexity passage text language model may calculated substituting conditional probabilities computed model right hand side equation 
apart constant gamma perplexity identical log probability observing sequence gamma minimising perplexity equivalent maximising log probability word sequence 
note equation language model probability component bayes rule optimised conditions fixed sequence length assessment language model quality perplexity effectively separates acoustic language model probability components approximation 
perplexity measure proposed jelinek mercer bahl 
perplexity allows independent computationally demanding assessment language model quality 
average quantity indicate local changes corpus calculated 
particular interaction acoustic probabilities considered perplexity measure account varying acoustic difficulty distinguishing words 
improved quality measure addresses aspect proposed disadvantage specific nature particular acoustic component speech recognition system 

equivalence mappings word history recalling equation language model estimates conditional probabilities gamma henceforth refer gamma history word 
due extremely large number possible different histories statistics gathered estimation conditional probability grounds broader classification gamma 
define operator maps history gamma word distinct history equivalence classes 
denote nh gamma nh number different equivalence classes training corpus classification delta segments words corpus nh subsets referred collectively gamma history operator delta word history corresponds exactly equivalence class conditional probabilities may estimated gamma general operator delta case calculation probability ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page involves summing history equivalence classes correspond gamma gamma delta gamma gamma gives probability word history gamma belongs equivalence class nh gamma gamma gamma note equation reduces equation gamma gamma nonzero exactly equivalence class 
examples bigram language models define def gamma gamma fw gamma trigram language models define def gamma gamma fw gamma gamma gamma refers sequence words fw gamma gamma contrast approach taken equations decision tree employed map word histories equivalence classes 

probability estimation sparse data instances language modelling involves estimation probabilities sparse set measurements 
specialised estimation techniques required circumstances prevalent language model field described section 
formal notation framework ensuing statistical problems may formulated 
consider population sigma measurements taken possible text language interest 
practice obtain subset sigma measurements training corpus sigma sample population sigma sigma 
set possible unique measurements sigma sample space denoted sigmaj 
furthermore define events population sample space correspond exactly outcomes measurements note events sigma sigma members sigmaj may events sigmaj sigma members sigma 
measurements detection particular word patterns specific nature depend method choose model language furthermore assume measurements finite number possible discrete values total number different events sample space sigmaj may defined oe individual event denoted oe oe gamma 
way illustration outcomes correspond words vocabulary 
frequent choice pattern tuple consecutive words chosen basis gram language model 
note formalism introduced bigrams trigrams example differ represent different measurements 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page total number events sigma may expressed sigma oe gamma oe oe number times oe occurs sigma may oe oe order conceptual framework concrete consider example word trigram language model details section 
case events words vocabulary oe oe nw nw vocabulary size 
trigram computes probability word considering previous words gamma gamma referred bigram context 
consider isolation particular bigram context fw measurement entails determining word follows different occurrence bigram 
population sigma collection words followed bigram language interest sigma particular subset obtained training corpus 
distinct bigram context training corpus defines different population populations sample space sigmaj 
general history classification segments training corpus nh subsets sigma nh gamma nh number distinct history classifications corpus 
history classifications delta refined example move bigram trigram discrimination language model improves treat greater variety histories separately time nh increases number events sample sigma decreases 
increasing sparseness sigma difficult estimate event probabilities reliably 
particular consider maximum likelihood probability estimate event oe sample sigma corresponds relative frequency pml oe sigma oe sigma sigma sigma total number events sigma estimator assign probability zero event seen sample sigma sigma sparse events sigma may expected occur sigma important probability estimator refined allow estimation probabilities unseen events 
sections review turing deleted discounted estimation techniques address estimation marginal probabilities sparse data give particular attention computation unseen event probability 
deleted interpolation approaches suggest unseen probability may distributed sensibly possible unseen events 
discounting estimator fit neatly categories described 

turing estimate turing probability estimation technique hinges symmetry requirement assumption events occurring number times sample probability occurrence 
original development turing estimate refer alternative ways arriving result see 
introduced section 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page letting refer number different events occurring exactly times sigma oe gamma ffi oe ffi true false definition note sigma delta sigma largest number times event occurs sigma sigma denotes number different events sigma sigma sigma denote set events occur exactly times sigma oe oe oe gamma strategy employed advocated turing estimate probability event occurs sigma known appear exactly times sigma 
denote estimate symmetry requirement may write probability sigma event occurring times sigma expected number events occurring exactly times sigma efc assuming events occur independently bernoulli trials probability precisely sightings oe sigma binomial probability distribution oe sigma delta delta gamma sigma gammar notational convenience defined oe 
note en sigma fc oe gamma ffi oe jo subscript expectation operator denotes sample size 
note furthermore en sigma ffi oe jo probability oe occurring times sigma sigma delta delta gamma sigma gammar ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page en sigma fc sigma delta oe gamma delta gamma sigma gammar events oe oe gamma form partitions sigma oe gamma oe sigma deltap delta gammap sigma gammar deltap oe sigma delta oe gamma delta gamma sigma gammar clarity define nw delta oe gamma delta gamma gammar note nw sigma sigma delta oe gamma delta gamma sigma gammar gamma sigma delta equations nw sigma en sigma fc follows en sigma fc sigma delta substitution result leads delta en sigma fc sigma delta en sigma fc approximating expectations counts en sigma fc en sigma fc obtain turing formula delta sigma delta ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page approximated delta sigma delta section discusses expression may shown optimal cross validation framework 
equation probability unseen event may calculated easily delta delta delta delta sigma delta delta delta sigma delta delta gamma sigma applying obtain result delta sigma quantity delta probability occurrence unseen event sigma probability corresponding particular unseen event 
adopting relative frequency interpretation may view estimate number unseen events sample size sigma training set sparse assumption expectations may approximated event counts may founded 
example may occur case estimate requires division zero 
suggests smoothing counts application remedy katz suggests small sparse counts occur large 
particular exceeds threshold maximum likelihood estimate advocated judging counts reliable case sigma requiring probability estimate unseen events remain sigma katz finds possible choice may possible delta delta gamma delta gamma delta delta sigma typical value 
approach may alleviate problems associated sparse large experiments lob corpus shown equation lack robustness lead unacceptable probability estimates 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page different approach development describes methods means constraints may placed probability estimates gamma requires probability frequent events equal exceed frequent ones gamma sigma sigma requires estimates lie close relative frequencies 
estimation methods simpler successful modifications considered 

deleted estimation probabilities may estimated employing rotation method cross validation 
define sum probabilities sigma events occurring times sigma apply symmetry requirement order write delta assume training set partitioned sets termed retained heldout parts respectively 
counts determined maximum likelihood estimate respect heldout set number occurrences heldout part events appearing times retained part highest frequency event occurs retained part 
assume training corpus split disjoint sets quantity case th partition forms heldout part denoted partitions form heldout part turn remaining gamma constituting retained part 
deleted estimate defined maximises product heldout probabilities choose sigma heldout part consists exactly sample result reduces turing estimate establishing optimality crossvalidation framework 
comparison turing deleted estimates approximately words ap news text training testing 
results indicate method consistently superior emphasis 
comparing equations note gamma delta corresponds leaving method cross validation 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page 
discounting methods katz interprets process probability estimation unseen events estimates seen events decreased discounted certain amount resulting discounted probability mass assigned unseen events 
interpretation postulate form probability estimate explicitly contains discounting factor gamma sigma opposed turing formula sensitive sparse counts estimate robust guarantees long condition control designer independent counts obtained training set 
order obtain expression discounted probability mass recall delta solve delta follows delta delta gamma sigma delta sigma delta delta gamma sigma delta delta apply equation obtain delta sigma delta delta probability unseen events delta fact typically distributed possible unseen events means general distribution katz backing approach described section 
specific cases discounting function treated model absolute discounting linear discounting 

linear discounting linear discounting method chooses discounting factor equation proportional delta require unseen probability correspond delivered turing estimate obtain sigma sigma delta delta delta ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page applying find sigma solution shown optimise leaving probability employed speech recognition system described 
katz states performance linear discounting estimator influenced strongly particular choice discounting factor 

absolute discounting method absolute discounting chooses discounting factor equation positive constant unity may require unseen probability equal obtained turing estimate equations find delta sigma delta sigma delta sigma applying equation follows sigma opposed turing estimator estimate robust sample counts sigma practical corpora equality require event seen exactly 
aim finding optimal value discounting factor leaving log probability determined subsequently differentiated respect 
resulting equation solved exactly bound optimal determined delta empirical tests gram language models show absolute discounting give perplexity reduction linear discounting lob word german corpus 

backing refined distributions previous sections addressed strategies useful estimating probability event oe sample sigma drawn population sigma sample sparse 
case probability encountering unseen event may computed method distributing possible unseen evens specified 
absence information reasonable assume events equally redistribute probability mass delta uniformly 
language modelling applications information available form general ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page distribution obtained specific definition history equivalence classes example trigram bigram distribution may considered 
katz advocates redistributing unseen probability possible unseen events general distribution process terms backing 
consider populations sigma sigma specific second general definition history equivalence class 
denote sample population sigma sigma respectively corresponding probability estimates oe sigma oe sigma 
assume method calculating probabilities seen events exist 
total probability occurrence unseen event sample sigma sigma oe oe sigma oe sigma gamma oe oe sigma oe sigma katz distributes probability mass unseen events lower order probability oe sigma follows oe sigma def sigma ff sigma delta oe sigma oe oe sigma follows sigma sigma ff sigma delta oe oe sigma oe sigma ff sigma oe oe sigma oe sigma convenient calculate ff sigma gamma oe oe sigma oe sigma notational convenience define ff sigma def sigma ff sigma find ff sigma gamma oe oe sigma oe sigma gamma oe oe sigma oe sigma refer sections 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page complete probability estimate event oe oe sigma sigma oe sigma oe sigma ff sigma delta oe sigma oe sigma note events unseen sigma sigma probability estimate oe sigma employ backoff general distribution say oe sigma 
recursion terminates distribution oe longer unseen reached 
note katz method computationally efficient ff sigma may precomputed nh gamma 
principle probability estimator takes unseen events account may backing framework 
particular turing estimate may directly modified form 
estimators robust strongly sparse counts practice discounted estimates successful 
linear discounted estimate employed nonlinear discounting advocated 

deleted interpolation backing approach means combining probability estimates different populations sigma sigma order address problem unseen events 
assume general populations differing generality 
denote sample population sigma gamma 
method deleted interpolation combines probability estimates obtained samples means linear combination follows di oe sigma np gamma sigma sigma np gamma np gamma sigma np gamma delta oe sigma sigma np gamma assumed correspond refined history equivalence class sigma np gamma gamma np gamma sigma np gamma smoothing parameters sigma np gamma chosen maximise probability calculated di delta delta cross validation set component probabilities right hand side equation weighted utility respect predictive quality language model 
order obtain optimal smoothing parameters approach hidden markov model interpretation equation proposed 
model states initial state gamma corresponding terms right hand side equation denoted np gamma sigma np gamma interpreted transition probabilities oe sigma delta output probabilities 
context sigma np gamma encountered model executes transitions gamma states np gamma transition probabilities event oe emitted state appropriate output probability 
framework optimal sigma np gamma may obtained training model forward backward reestimation initial choice sigma np gamma constraints satisfied 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page distinct context generally different smoothing parameters limited quantity training data may require tied appropriately 
alternatively need forward backward reestimation may alleviated empirically determined weighting function calculate smoothing parameters 
results word train information enquiry database word brown corpus show perplexities increased slightly simplification 
performance comparison backing technique method deleted interpolation shown give practically identical results due higher implementational computational complexity pursued 

modified absolute discounting variation discounting model 
distributing discounted probability mass unseen events backoff framework smooth probability estimate times 
general discounting function postulated achieve oe sigma oe sigma gamma oe sigma sigma delta oe sigma discounting function oe sigma general different event oe samples sigma sigma related section 
oe gamma oe sigma oe gamma oe sigma gamma oe sigma sigma oe gamma delta oe sigma sigma delta oe gamma oe sigma choose discounting function oe sigma oe sigma oe sigma 
choice probability mass may sigma delta oe oe sigma delta sigma sigma comparison carried bigram trigram language model word corpus office correspondence 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page substitution yields oe sigma max oe sigma gamma sigma delta sigma sigma delta oe sigma approach advantage resulting strong smoothing proportion number different events total number events occurring context large condition hold counts sparse 
ideally values chosen maximise cross validation function exact solution appears mathematically intractable 
furthermore strictly different sigma may practically advantageous tie value way 
empirical tests performed choices 
modelling individually sigma determining optimal values iterative adjustment parameters fixed step size allowable ranges maximise training set probability 

modelling sigma pooling counts determining optimal value similar fashion 

modelling value sigma time approximating upper bound 
approaches give practically identical results indicating estimator insensitive choice estimator employed successfully 

gram models gram sequence consecutively occurring items 
section assume items words bearing mind principles may easily extended forms gram choice language model estimates probability word conditioned identity preceding gamma words gamma gamma termed gamma gram context 
section see history equivalence classes defined ae gamma gamma gamma gamma oe nh number distinct gamma tuples training corpus 
furthermore mapping operator delta defined gamma gamma clarity define gamma gram context corresponding gamma gamma section example introduces grams word categories 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page consider sample space sigmaj words vocabulary events oe oe gamma oe nw formalism introduced section denote population sample corresponding history classification sigma sigma respectively sigma bearing mind sigma continue sample space sigmaj 
population sigma corresponding context comprises words followed follow language interest subset sigma population consists words fact followed context training corpus 
number occurrences event sample sigma number times gram formed concatenation context word seen training corpus sigma fw total number events sample number occurrences context sigma nw sigma increases potential number distinct contexts nh increases exponentially number words sample decreases fixed training corpus 
small data sample sparse practice restricted bigrams trigrams technique section applied gram probability estimation turing estimator consistently superior deleted estimate nonlinear discounting deliver better performance turing estimator 
particular absolute discounting backoff framework gamma gram estimate general distribution 
letting gamma find gamma gamma gamma gamma gamma gamma gamma ff gamma gamma delta gamma gamma gamma gamma gamma sigma gamma gamma results indicate small performance improvements larger restricted domain task word train information enquiry database larger diverse domain word brown corpus 
larger tasks gram language models employed uncommon due extremely large size 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page ff gamma gamma gamma jw gammap gamma gamma gamma gamma jw gammap gamma gamma gamma backing gamma gram distribution reasonable data sparse convenient implementational point view cases may lead overestimation unseen probability 
particular absence gram may result linguistic lack data 
issue addressed experiments employ wsj corpus cases word corpus newspaper text 
case perplexities reduced word error rate reductions reported 

category language models finding patterns individual words language model may designed discover relationships word groupings categories 
approach advantages ffl category models share statistics words category able generalise word patterns encountered training corpus 
ability sensibly process unseen events termed language model robustness 
ffl grouping words categories reduce number contexts model counter training set sparseness 
ffl reduction number contexts leads compact model employing fewer parameters having modest storage requirements may important practical standpoint 
sections introduce formal notation dealing word categories describe may applied development language models 

word categories category taken refer grouping words 
categories denote fv nv define operator delta maps word nw categories gamma nw gamma category assigned operation delta 
mapping speak deterministic category membership referring stochastic membership 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page assume probability witnessing word completely defined knowledge category belongs may write gamma stochastic category membership allows decompose conditional probability estimates way gamma delta gamma furthermore classifying history equivalence classes find equation gamma delta gamma framework natural choice history equivalence class mapping identity gamma categories gamma gamma gamma obtain category gram language models 
note equation represents mapping operator delta 
equations employed construct bigram language model linguistic part speech word categories 
word assigned categories human experts syntactic function 
lob corpus summing category history equivalence class assignments opposed choosing ones reduces perplexity agrees findings indicates multiple category membership improves modelling performance 
furthermore part speech model slightly lower perplexity word bigram employing fewer parameters 
larger improvements obtained smaller german corpus approximately words illustrating improved generalisation category models 
closed vocabulary employed cases 
history equivalence class mapping equation simplifies gamma may rewrite gamma delta equation synonym language model proposed similar part speech approach categories need strict grammatical definitions 
core vocabulary core defined consisting set words assumed exhibit significant types grammatical behaviour may encountered 
associated word ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page core synonym list sw list words display similar grammatical characteristics synonym lists compiled automatically training corpus identifying word core vocabulary context new word agrees best 
light core corresponds set categories synonym sets sw category membership definitions 
restrict deterministic membership equation simplifies gamma delta furthermore category gram equation obtain gamma delta gamma category bigram language model conjunction automatically determined category membership described section 

automatic category membership determination word categories history equivalence classes defined way category language models 
smaller corpora words tagged part speech labels linguistic experts available hand labelling impractical large amounts text consequently methods allowing automatic grouping words categories investigated 
option rule linguistic parser tag training corpus linguistic part ofspeech information classifications word categories 
parsing computationally intensive operation may impractical process entire training corpus 
parser tag initial part training corpus initialise statistical tagger remaining text assigned category labels 
alternatively method clustering words categories part optimisation process may employed 
optimisation algorithm maximises training set probability language model described 
category bigram language model form word may belong category 
starting initial assignment algorithm evaluates change training text log probability brought moving word vocabulary current possible category 
move resulting largest increase log probability selected executed process repeated 
continues convergence criterion met 
number categories assumed fixed optimisation run number varied repeated trials 
test set perplexity achieved optimisation decreases initially number categories increases reaches minimum increases tending word bigram perplexity 
similar observations category assignments hand syntactic semantic word functions 
arguments account behaviour 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page ffl categories language model able discriminate strongly different patterns text 
ffl tradeoff number categories increases extent model generalise diminishes discrimination ability improves 
ffl overfitting categories language model begins reflect peculiarities training set generalises test set 
bigram relative frequencies maximum likelihood probability estimates training set log probability continue improve number categories grows number categories equals number different words vocabulary left word bigram language model 
overfitting may detected employing cross validation calculating training set log probability 
particular optimal number categories may determined automatically way 
leaving method cross validation employed achieve conjunction described optimisation algorithm possible estimate best number categories fairly accurately 
algorithm extended allow clustering maximising category trigram log probability 
employing large data sets extension leads large increase required computation necessary limit number categories extent remains possible build language models better performance aforementioned bigram clustering 
simulated annealing methods find optimal category membership assignments advocated 
bigram model structure equation employed training set log probability optimisation criterion word category moved chosen monte carlo selection 
decision move accepted taken metropolis algorithm occasionally allow perplexity increase 
conditions may occur governed control parameter follows annealing schedule moves leading perplexity increases increasingly time 
monte carlo carried simulated annealing lower perplexities achieved case demonstrate existence locally optimal category assignments 
experimental results obtained automatically determined categories indicate category models improve performance corresponding word models training corpus small dealing larger bodies text 
examples category membership optimisation indicate frequent words cluster appear grouped syntactic semantic function 
categories difficult justify intuitively 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page greedy agglomerative algorithm clustering words word categories :10.1.1.13.9919
bigram language model structure assumed shown model training set log probability may written sum terms unigram distribution entropy average mutual information adjacent categories 
ll gammah optimal set category assignments maximises mutual information component entropy independent choice 
greedy algorithm proposed initially assigns word training corpus category iteration merges category pair necessarily adjacent decreases :10.1.1.13.9919
process continues desired number categories reached 
framework assumed word may belong category 
due large number potential merges investigated step considerable care required implementation algorithm order ensure computational feasibility 
particular inherent redundancies calculation mutual information taken advantage order yield efficient algorithm ultimately cluster large corpus consisting words 
word categories way build category trigram language model analogy equation 
model perplexity higher word trigram model 
interpolation models leads slight perplexity improvement 

word groups phrases order improve modelling frequent word groups proposed language model vocabulary restricted single words allowed contain frequently occurring phrases 
particular technique relying concept mutual information allows identified automatically text corpus described 
mutual information events log delta events taken occurring words probability immediately follows unigram distributions respectively 
relative frequency approximations may estimate probabilities delta delta delta delta large corpus delta delta delta may estimate mutual information log delta delta mutual information defined section 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page high value indicates followed frequently text expected generated independently 
procedure find consecutive word pairs high mutual information add single units vocabulary 
estimate equation may unreliable small counts word pairs occurring threshold number times considered 
way phrases nuclear magnetic resonance may treated single units language model 
similar approach taken associations termed sticky pairs :10.1.1.13.9919
treatment frequently occurring word groups single lexicon entries led gamma word error rate improvements relative word bigram model small task words training data 
mutual information measure identify word groups training set log probability maximised directly 
approach taken minimising perplexity dedicated cross validation set second minimising training set leaving probability 
determination phrases automatic proceeds identifying pair decrease perplexity merging subsequent execution merge repetition 
perplexity word error rate reduction achieved experiments dialogue system having training set size words results show small relative improvement word error rate verbmobil task improvement larger switchboard corpus 
difference ascribed constrained nature corpus argued fixed phrases useful case 

longer term dependencies grams language model able discriminate word histories differ gamma words rarely exceeds 
may classify histories equivalent differ important respect due event taken place distant past 
semantic relationships example may span large distances text affect probabilities words may expected occur 
reason methods explicitly longer range dependencies account relevant 

pairwise dependencies certain long range dependencies may taken account postulating probability word history gamma may decomposed set independent pairwise probabilities gamma gamma gamma gamma delta undetermined function 
restricting word pairs combinational explosion associated grams large avoided 
longer limiting consecutive words number pairs bigram case significant fraction nw vocabulary size 
reason necessary filter set possible word pairs retaining conveying useful amount information 
experimental results treating incorporation pairs language model source 
pairwise dependencies described section referred word associations word triggers 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page study word association types discovered measure termed association ratio closely related mutual information 
lw nw delta lw gamma nw delta nw nw size vocabulary lw number times follows corpus window length lw typically words number observations corpus 
association ratio differs mutual information symmetric encodes linear precedence 
shown measure selection criterion interesting word pair associations may 
particular attention range sequential order association 
ordering information play significant role certain word combinations doctor precedes nurse approximately times nurse precedes doctor 
association range shown vary widely type relationship words types identified ffl compound multiple word expressions referring single concepts computer scientist 
ffl fixed words interest separated fixed number words bread butter 
ffl lexical words related syntactic factors 
ffl semantic words related meaning man woman 
types association words usually occur narrow range 
lexical semantic associations particular exhibiting large variance association range 
table reproduced illustrates separation words relation word word mean variance compound computer scientist united states fixed bread butter drink drive lexical refraining coming keeping semantic man woman man women table word relationships association ration 
measure similar identify associations employed long distance bigram model 
approach constrain probability estimate conditioned preceding word number words separating word pair question 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page allows depend informative word occurring back sentence 
technique perplexity improvements reported constrained corpus sentences 
mutual information filter word pairs 
pairs way conjunction conventional trigram language model means maximum entropy principle allows combination various knowledge sources making minimal assumptions concerning distributions 
doing reduction perplexity conventional trigram model achieved words wall street journal training text 
improvement noted due correlations words self triggers 
having selected word pairs significant correlation functional forms probability distribution delta may postulated 
suggested example probability estimate gamma take form linear combination individual pairwise probabilities words history pg gamma delta gamma constraint guarantees probabilities sum unity 
note probabilistic model probabilities depend identities positional distance 
weights may explicitly depend distance interpreted representing type window function words text 
various choices possible including rectangular triangular gaussian hamming type functions 
alternatively assuming probabilities known may chosen maximise probability training data 
order find maximum method lagrange multipliers applied 
constraint equation gamma likelihood function ll delta log delta gamma function maximised delta log delta gamma gamma delta gamma ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page setting delta may shown delta delta gamma delta gamma equation iteratively determine memory weights pg uniform set memory weights constituting suitable initial condition 
equation applied lob corpus 
association probabilities estimated counting occurrences words window length bigrams trigrams disallowed order eliminate short term dependencies 
avoid spurious cooccurrences minimum threshold placed word counts weights noncritical constant entire window 
various window sizes investigated optimum lie approximately 
resulting model interpolated unigram language model resulted perplexity reduction 
significant improvement achieved conjunction bigram language model 

cache models language models usually employ probability estimates chosen perform average entire training corpus 
precludes adaptation dynamic changes text characteristics models described static 
underlying philosophy cache due local text characteristics topic author words word patterns occurred recur immediate static language model predict 
cache addresses dynamically increasing probability events 
cache consists buffer words text language model probabilities calculated 
cache language model probabilities cache combined static model probabilities linear interpolation gamma gamma fi cache delta gamma fi cache delta cache gamma interpolation weights fi cache typically chosen optimise performance development test set deleted interpolation employed 
calculation cache frequently unigrams bigrams trigrams 
unigram cache example cache gamma cache cache number occurrences word cache length matter interest equation may interpreted special case word association model setting ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page pg choosing association probabilities gamma gamma word classifications available discrimination cache component may enhanced maintaining separate buffer word categories 
example trigram language model part speech categories conjunction unigram cache 
denote category hypothesised trigram component language model computes gamma gamma probability word category estimated relative frequency cache maintained individual category cache cache cache probability estimate cache category cache number occurrences word cache cache size 
combined probability estimate belongs obtained linear combination fi delta gamma fi delta cache language model probability calculated equation gamma gamma delta gamma gamma cache models local variations text character usually reset point known change instance article boundaries 
occurred necessary wait entries cache sensible probability estimates expected example threshold employed 
practice addition cache component language model led significant perplexity reduction 
part speech trigram model addition cache leads drop perplexity lob corpus 
tests word gram models show addition unigram bigram cache improve perplexity wsj corpus lead small relative word error rate improvements 
figures supported similar results reported tests arpa evaluation tasks addition unigram bigram cache lead consistent improvements perplexity relative word error rate 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page 
stochastic decision tree language models fundamental alternative gram history equivalence class defined equation stochastic decision tree perform mapping described section 
decision tree consists hierarchically nested set binary questions associated node tree 
excepting leaf terminal nodes descendants possible outcome question 
denote decision tree set nodes ft tn gamma refers root node 
furthermore denote leaves tree set fl development assumes set predictor variables extracted history 
denote particular model studied variables chosen simply words gamma gamma gamma choices possible syntactic word category identities semantic labels instance 
questions concern nature history gamma 
denote particular question symbol theta index signifies belong node theta initially assumed form theta 
subset set values may take 
word example subset known vocabulary 
theta predictor variables member certain set returns true false result 
denote possible outcomes theta theta gamma respectively 
obtain classification history gamma tree traversed root leaf nodes answering succession question theta posed node path 
leaves tree correspond history equivalence classifications 
associated appropriate distribution nw gamma reflecting probability word equivalence class note number indicator variables may held large encountering combinational explosion experienced gram models equivalence class generally subset indicator variables need examined 
starting root node ft tree structure grown incrementally training corpus 
growth occurs leaves non terminal nodes remaining fixed 
particular leaf involves determination assignment suitable new question theta corresponding addition descendant nodes new leaf nodes 
order control process criterion indicating growth terminal node affects performance tree needed 
common choice employed average entropy ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page leaf probability distributions 
minimising directly minimises uncertainty associated prediction word 
entropy leaf gamma nw gamma delta log average entropy tree tree delta probability visiting leaf argument signifies entropy calculated word probability distribution 
questions type theta uniquely defined choice predictor variable choice subset certain process finding optimal theta reduces task determining optimal partition range subset complement calculation globally optimal solution problem extremely computationally expensive locally optimal greedy hill climbing algorithms employed 
example variant means clustering employed 
considerable fraction computation required tree growing devoted set construction 
note assumed true probabilities valid language general 
practice replaced estimates derived training corpus 
limited size corpus truly representative entire language care taken avoid overfitting model 
may accomplished employing form cross validation instance heldout set 
form questions simple conveniently illustrates concept tree language model may argued unacceptably restrictive 
optimal question form theta example split nodes theta theta 
lead unnecessarily large trees fragment limited training data 
alternative form question employing directed graph structure termed trellis 
trellis structure allows nodes children children multiple parents allowing sum products boolean expressions encoded preserving elementary questions nodes 
tree language model compared trigram language model word vocabulary task word corpus 
tree model contained leaves exhibited perplexity 
slight improvement relation trigram perplexity 
tree distinct probability distributions opposed trigram storage required approximately equal distributions generally nonzero entries 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page 
domain adaptation adaptivity language model concerns capacity alter probability estimate gamma accordance particular nature text 
text domains may distinguished attributes example topic discussion style writing time writing 
cache models described section example adaptation may achieved dynamically 
know domain application priori may produce static model adapted particular task 

specialisation target domain language models trained large quantities text covering subject areas styles writing display average performance able take advantage particularities domains applied 
insufficient data specialised domain allow specialised models built directly 
cases desirable adapt parameters general language model relatively small amount material target domain 
denote general sample sigma sample obtained target domain sigma parameters probability distribution defined sample space sigmaj starting general sample sigma prior distribution may write posterior distribution sigma sigma sigma sigma delta sigma sigma sigma delta sigma study compares maximum posteriori map estimate map arg max sigma sigma classical bayes estimate bayes sigma sigma delta sigma sigma delta competing techniques adapting parameters probability distribution new data 
experiments encompass adaptation language model obtained word corpus radiological reports particular hospital smaller set reports obtained 
map outperforms bayes adaptation better results obtained optimal linear interpolation language models built general domain specific samples directly 
word error rate improvements map optimal interpolation achieved considering words text new domain respectively 
figures single model obtained pooling training data samples 
related approach taken general language model adapted particular domain minimum discrimination information method adapted model required close possible general distribution kullback liebler sense satisfying set constraints imposed new domain 
constraints include instance unigram bigram probabilities 
technique applied switchboard corpus performance reductions relative language model trained exclusively general sample 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page measured relative language model constructed general sample model constructed pooled data 
cases word error rate improvements achieved respectively 
contrast results reported similar word error rate improvements achieved optimal linear interpolation general domain specific models 

mixtures topic specific language models order account set number distinct themes appearing corpus text may divided partitions corresponding common subject matter termed topics trigram language model built individual topic 
resulting models combined linearly obtain probability estimate gamma gamma topic delta gamma gamma topic topic number topics gamma gamma trigram estimate th topic weighting contribution th topic 
subdivision training corpus may lead topic models equation linearly interpolated general trigram pg delta trained entire training set 
gamma gamma delta delta gamma gamma gamma delta pg gamma gamma division corpus topic partitions proceeds agglomerative clustering assuming paragraph text concerned particular topic 
assigning paragraph corpus individual topic cluster initially similar pairs clusters merged successively desired number topics reached 
normalised number content words common clusters similarity measure cluster refinement achieved reassigning member sentences greedy algorithm maximise training set probability 
note component trigram models gamma gamma recomputed reassignment new partition training corpus 
truly adaptive language model values adjust characteristics current text fixed values employed optimal values obtained viterbi type reestimation approach 
model topic classes relative reduction recogniser error rate achieved nab corpus 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page 
language models applications thesis focuses application speech recognition systems language models important components fields introduced briefly sections 

character handwriting recognition recognition printed handwritten text search sequence words respect graphic written evidence surprising techniques similar speech recogniser may applied 
particular acoustic model model geometry trajectory writing employed 
gamma parameterisation written evidence may formulate recognition problem solution arg max ae delta oe language model supplies estimate grammatical plausibility hypothesised sequence words shown great impact recognition accuracy 

machine translation greater availability aligned bilingual corpora prompted renewed research statistical methods automatic translation 
particular considers problem translating sentence sl source language french corresponding sentence tl target language english probabilistic model tl arg max tl sl tl arg max tl sl tl delta tl sl tl translation model reflecting extent english words express ideas french tl english language model reflects extent english hypothesis grammatical 
formalism translation model analogy acoustic model speech recognition framework internal mechanisms course different 
finding best english translation french sentence search english hypothesis maximising tl sl 
human translators observed translate times quickly dictating having write type translations integration translation system speech recogniser investigated 
text source language available recognition occurs target language speech recognition problem reformulated tl arg max tl sl tl acoustic observation target language obtained speech dictation 
ph dissertation thomas niesler cambridge university june chapter overview language modelling techniques page may assume independent text source language sl tl tl obtain tl arg max tl tl delta sl tl delta tl results indicate improved recognition performance additional knowledge sl incorporated way 

spelling correction automatic methods correcting spelling errors important number areas including document preparation database interaction text speech systems 
types error may distinguished firstly nonword errors misspelling results word longer valid language interest example misspelled secondly real word errors error results different valid word alternate meaning example misspelled form 
language models applied successfully correction types error 
particular led improvements relative isolated word methods treat misspelling account surrounding words unable address real word errors 
nonword error detected number similar valid spellings determined generally means database valid words subword units character grams suitable distance measures indicating similarity strings 
language models employed successfully ranking alternatives best constituting result automatic correction 
case real word errors language model detect misspelled words sensing associated low language model probability choose list subsequently generated alternatives 

tagging words may classified groups grammatical function part speech sentence 
analyses important steps discovering higher level linguistic structure instance identification noun phrases 
words possible part speech assignment instance light may act adjective verb noun classification may ambiguous considering lexical identity word 
language models part speech information bigram trigram dependencies success automatic annotation unlabelled text part speech information process referred part speech tagging 
ph dissertation thomas niesler cambridge university june chapter variable length category grams page chapter variable length category grams 
overview chapter develop language model designed capture syntactic patterns english text 
part speech classification word training corpus assumed known constitutes priori grammatical information exploited statistical model 
model employs grams part speech word categories capture sequential grammatical dependencies :10.1.1.28.4818
far fewer parts speech words typical vocabulary number different grams smaller value word gram model 
reduces problem data sparseness possible increase value statistical storage viewpoint 
furthermore grams categories intrinsically able generalise word tuples witnessed training 
categories embed syntactic information generalisation proceeds measure grammatical correctness assigned unseen sequence model 
length individual gram optimised allowing increase point improvement predictive capability detected allowing model size traded model performance defined way 

structure language model drawing exposition section denote different part speech categories nv gamma word may multiple grammatical functions mapping operator delta set possible part speech classifications definitions section define history equivalence class gram categories 
nh grams denoting collectively gamma length category gram associated particular history equivalence class employs definitions lob corpus listing appears appendix ph dissertation thomas niesler cambridge university june chapter variable length category grams page lh gram may denoted lh gamma delta particular category gram may describe multiple word grams 
mapping delta returning particular word sequence gamma set history equivalence classes gamma category word grams match gamma ae ha gamma lh rh rla lh oe rh nh gamma rla lh gamma 
assume equation word occurrence probability depends believed category employ write gamma delta gamma furthermore employing decomposition gram assumption may decompose second term right hand side equation follows gamma delta gamma category gram context 
summation accounts part speech categories may assigned summation history equivalence classes matching word history gamma 
interrelation probability functions equations illustrated subsequent sections treat estimation individually 
gram probability estimate category tuples 
categories category category level hypothesised context word previous word word history 
word operation category language model 
ph dissertation thomas niesler cambridge university june chapter variable length category grams page 
estimating jh compact storage category grams tree data structure employed associating node particular word category paths originating root correspond category grams 
way node represents distinct history equivalence class hm associated conditional probability distribution function set nodes corresponds set history equivalence classes restricting length individual paths tree contexts arbitrary depth catered 
illustrates structure means example 
nodes labelled specific history equivalence class hm represent category defining gram respect parent node 
particular example history equivalence class corresponds bigram context gamma gamma fv history equivalence class trigram context gamma gamma fv null level level level root unigram context bigram contexts trigram contexts organisation category gram tree 
probabilities form gamma calculated tree determining history equivalence class corresponding category context gamma applying gram probability estimator 
particular define operator delta maps category grams history equivalence classes hm gamma hm history equivalence class corresponding deepest match category sequence gamma gram tree 
probabilities estimated application katz back conjunction nonlinear discounting equations 
model construction proceeds level level tree growing strategy retains contexts improve performance quality estimate 
allows model compactness maintained employing longer grams benefit performance 
ph dissertation thomas niesler cambridge university june chapter variable length category grams page 
initialisation gamma 

grow add level level gamma adding grams occurring training set grams exist tree 

prune newly created leaf level apply quality criterion discard leaf fails 

termination nonzero number leaves remaining level goto step 
quality criterion training set probability delivered model reflect predictive performance 
order avoid overfitting leaving cross validation employed calculating probability 
category gram model method particular advantage computationally efficient gram counts available memory 
particular referring appendix leaving log probability may expressed ll cum omega tot nc gamma log gamma omega rt number words training corpus omega tot gamma omega rt probability estimated gram model obtained omega rt retained part formed removal heldout part omega ho omega tot definition log probability may rewritten sum contributions node ll cum omega tot nh gamma gamma log omega rt nh gamma ll hn cum ll hn cum nv gamma hn delta log gamma omega rt delta ll cum log probability entire training corpus omega tot ll hn cum log probability events occurring context hn total number times seen context omega tot jh omega rt probability occurring context retained part training set omega rt formed constitutes heldout part 
assume node leaf change training set log probability resulting addition child ffl calculated 
refers original parent node denote node addition child 
change log probability hn cum ll hn cum ll hn ffl cum gamma ll hn cum ph dissertation thomas niesler cambridge university june chapter variable length category grams page terms quantities pruning criterion hn cum gamma ct delta ll cum omega tot requires new node lead increase threshold defined fraction ct total log probability choice threshold fairly problem independent 
probability jh may calculate perplexity indicating confidence tree predicts category 
language model evaluation referred category perplexity 

estimating jv assuming category sufficiently large membership apply relative frequency jv jv language model hypothesise categories vocabulary oov words probability occur category estimated 
accordingly word named uw added category count uw estimated leaving method uw delta gamma number words seen exactly training set total number words uw estimated count uw small constant introduced heuristically ensure denominator exceeds numerator 
effect significant sparsely trained categories consequently small 
effect performance seen empirically weak yields satisfactory results lob corpus 
complete details appendix 
estimating gamma language model component estimates probability particular word history gamma corresponds category gram context associated hm word may multiple part ofspeech classifications general possible contexts gamma belong 
set contexts probabilities associated may calculated recursive approach develop assuming contexts probabilities known gamma deriving corresponding results 
define hyp possible category sequence words termed hypothesis individual hypotheses distinguished index hyp number hypotheses word sequence 
string uw abbreviation unknown word 
word vocabulary course 
ph dissertation thomas niesler cambridge university june chapter variable length category grams page possible hypothesis require indication likelihood correct classification word string 
denote probability hyp hyp gamma hyp determine expressions hyp desired probability history equivalence class may obtained follows hm hyp hm hyp explicit maintenance hypotheses necessary opposed simply keeping record history equivalence classes due varying lengths grams 
particular may occur lh gamma lh gamma case gram probability estimate contextual information implicit history equivalence class gamma 
practice necessary maintain set hypotheses gamma gamma depth equals exceeds maximum length gram stored tree 
guarantees hypotheses deep path tree 
hypotheses arising recursive calculations differ elements gamma may merged summing probabilities 
set existing hypotheses fv hyp gamma set new hypotheses fv hyp gamma hyp gamma gamma gamma number different categories 
consider particular postulate hyp fv hyp gamma prime index indicating general fixed relation ordering sets hypotheses 
bayes rule may write hyp hyp delta hyp hyp recalling assumption follows hyp hyp hyp delta gamma hyp gamma applying equation gram assumption may write ph dissertation thomas niesler cambridge university june chapter variable length category grams page hyp hyp hyp gamma hyp hyp gamma delta hyp gamma hyp gamma single initial empty hypothesis hyp gamma associated unigram context hyp gamma 
follows hyp jv hyp delta hyp hyp gamma delta gamma hyp gamma note hyp hyp instant postulate hyp jw maximum 
number possible hypotheses extremely large increases necessary restrict storage max hyp candidates choosing probability greatest 
implies valid hypotheses may discarded equation longer satisfied max hyp hyp max hyp hyp hyp taken case refer th hypothesis 
equation requires probabilities sum unity 
replacing equation def max hyp hyp conditional probabilities application equation 
effect probability mass associated discarded hypotheses distributed proportionally retained 
note equation quantity common new hypotheses choice max hyp best candidates considering joint probabilities hyp conditional probabilities hyp complete recursive procedure summarised 
assumed set old hyp max hyp best previous hypotheses hyp gamma corresponding probabilities hyp gamma available arrays collectively referred old similarly set new hyp max hyp updated context hypotheses corresponding probabilities hyp hyp stored new initialisation accomplished setting gamma placing single empty hypothesis new hyp fg new hyp 
ph dissertation thomas niesler cambridge university june chapter variable length category grams page 
copy hyp corresponding hyp new old 
clear new 

hypothesis hyp old old hyp gamma 
category 
hyp hyp 
calculate hyp 
hyp exceeds entry new insert hyp hyp new possibly overwriting smallest entry process 

calculate 
calculate hyp new hyp gamma new 
new contains set best new hypotheses corresponding probabilities hyp hyp calculate 

beam pruning procedure described previous section maintains fixed maximum number hypotheses word history low associated hyp discarding hypotheses computational efficiency may improved considerably 
beam pruning maintains hypotheses associated probabilities certain fraction hypothesis 
particular letting max denote maximum hyp entry new condition met hyp ffi delta max practice means step reformulated follows 
satisfied new hypothesis insert hyp hyp new possibly overwriting smallest entry process 

remove new hypothesis fails 
second step discards hypotheses moved outside beam 
incorporation allows accuracy traded computational efficiency issue particularly important language model tag large quantities text 

employing language model tagger knowledge correct category assignment word training corpus assumed constructing language model 
information normally available automatic means annotating large volumes text grammatical word classifications sought 
ph dissertation thomas niesler cambridge university june chapter variable length category grams page assigning category word sentence process referred tagging denoting sentence gamma corresponds finding sequence gamma probability gamma gamma maximum 
recall list probabilities corresponding category assignments maintained procedure described section calculation probability implicitly involves tagging operation 
particular maintains list category sequences words respect language model statistics 
procedure page sentences may tagged time follows 
initialise new set gamma 

execute steps word current sentence turn 

hypothesis gamma highest gamma gamma sequence tags sentence 
technique language model constructed tagged corpus may assign tags untagged corpus making possible build language models result 
untagged corpus may contain millions words beam pruning technique particular importance tagging 

performance evaluation evaluate language model construction application techniques described part chapter 
experiments carried lob switchboard wsj corpora contain part speech information application language model tagger treated 
details regarding corpora division test training sets appendix 
tagging accuracy category language model built lob training set tag test set means procedure described section result compared actual tags order determine tagging accuracy 
benchmark experiment carried tagger shown table 
category model tagging accuracy tagging accuracy known words tagging accuracy oov words table tagging accuracies lob test set 
particular category assignment word referred tag 
ph dissertation thomas niesler cambridge university june chapter variable length category grams page tagging words falling vocabulary defined training corpus language model achieves reduction tagging error benchmark tagging rises improvements attributed longer gram contexts category model method calculate probabilities unknown words described briefly section detail appendix 
lexicon augmentation results table show tagging accuracy oov words significantly lower words seen training reason effect employing various additional information sources augment lexicon investigated 
particular sources 
word spellings part speech assignments oxford advanced learner dictionary available electronically 
mapping convert dictionary grammatical classifications employed language model described appendix 
list frequent names surnames 
included oov words seen include high proportion proper nouns approximately 

genitive cases words lexicon standard form 
table shows tagging accuracies augmented lexicon 
note oov rate halved tagging error reduced augmentation augmentation oov rate tagging accuracy tagging accuracy known words tagging accuracy oov words table tagging accuracies augmented lexicon 

beam pruning text corpora tagged may large required computational effort important practical issue determines required processing time 
shows tagging rate accuracy vary functions beam pruning parameter 
note tagging may accelerated factor slight decrease accuracy 
ph dissertation thomas niesler cambridge university june chapter variable length category grams page normalised tagging rate tagging rate accuracy function beam pruning parameter 

tagging switchboard wsj corpora switchboard wall street journal wsj corpora contain part speech classifications tagged category models constructed 
accomplish language model built entire lob corpus lexicon augmented section 
frequent oov words respective corpora determined tagged hand added lexicon 
table summarises process 
switchboard wsj words lob corpus lexicon words added dictionary additional proper nouns hand tagged oov words total oov rate training corpus table constitution augmented lexica tagging wsj 
category language model conjunction corresponding augmented lexicon produce tagged versions switchboard wsj corpora 
ph dissertation thomas niesler cambridge university june chapter variable length category grams page 
constructing category trees procedure detailed section category gram language model trees constructed lob switchboard wsj corpora 
proposed tree pruning technique probability termed likelihood pruning evaluated comparing context count pruning gram count pruning 
discards grams gamma gram context seen fewer threshold number times 
second eliminates grams tree seen threshold regarding part unseen mass probability calculations 
note approach word gram models compact 
illustrate models obtained different pruning thresholds graphs model size total number grams tree resulting category perplexity introduced section lob corpus 
models obtained gram count pruning shown perplexities scale lying 
number grams tree likelihood criterion context count threshold model size versus performance context count likelihood pruning lob corpus 
see complexity tree increases test set perplexity moves minimum 
initial decrease may ascribed underfitting data due insufficient number parameters subsequent increase overfitting data due excessive number parameters language model tree 
effect significantly reduced comparison context count pruning overfitting occurs despite leaving cross validation remains approximate way modelling test set 
values pruning threshold trees constructed likelihood pruning exhibit better size versus performance characteristic obtained thresholding context counts 
similar behaviour seen switchboard wall street journal corpora case perplexity increase due overfitting smaller effect ascribed larger amount training material 
ph dissertation thomas niesler cambridge university june chapter variable length category grams page order determine longer grams affect model performance category perplexity measured limiting maximum gram length tree various values particular trees obtained lob training set likelihood pruning threshold ct gamma context count pruning threshold gram count pruning threshold experiments results graphed 
conclude context count pruning performs better gram count pruning difference negligible 
due increasing sparseness grams contexts higher particular individual grams occurring threshold cause gram pruning maintain context may deliver bad generalisation due high proportion unseen events test set 
pruning log probability outperforms approaches count thresholds 
depth tree unigram likelihood pruning gram count pruning context count pruning evolution category perplexity function maximum gram length 
consider corpora tree obtained likelihood pruning ct gamma 
analyses proportion grams model conclude ffl small unigrams bigrams number possible grams small number grams model small 
ffl large data sparse consequently cross validated log probability permits fewer grams added model 
ffl wsj corpus larger sparse larger proportion grams larger results shown similar behaviour observed word perplexity 
may possible achieve better results varying threshold counts function gram length investigated 
ph dissertation thomas niesler cambridge university june chapter variable length category grams page length gram unigram bigram fraction total grams lob wsj proportion grams tree function furthermore considering evolution category perplexities model addition level tree find behaviour illustrated 
curve normalised respect bigram perplexity order facilitate comparison 
greater size wsj corpus allows longer grams benefit reflected larger relative decreases perplexity addition grams higher 
contrast switchboard corpus sparse merit addition grams 
length gram unigram bigram normalised perplexity lob wsj perplexity function maximum tree depth 
ph dissertation thomas niesler cambridge university june chapter variable length category grams page 
word perplexity previous section investigated behaviour variable gram model category perplexity consider perplexities measured word level implementing language model described section 
corpora category trees produced likelihood pruning threshold ct gamma equation employed 
effect limiting number maintained history postulates max hyp performance shown table beam pruning employed experiment 
number hypotheses max hyp lob switchboard wsj table perplexities varying maximum number history postulates max hyp word perplexities decrease monotonically number hypotheses increased demonstrating history equivalence class ambiguity significant effect language model performance 
largest decrease occurs max hyp increased increments leading smaller reductions 
figures table indicate value approximately yield near optimal results max hyp henceforth 
having fixed max hyp remains decide value beam pruning parameter 
shows effect perplexity lob corpus conclude value gamma chosen 
wider beam increases computational effort tradeoff model accuracy computational cost 
specified experiments carried value gamma beam pruning threshold word perplexity effect beam pruning parameter perplexity lob corpus 
comparison perplexities word gram language models corpus appendix ph dissertation thomas niesler cambridge university june chapter variable length category grams page 
comparing word category models chapter introduced language model variable length part speech grams 
bigram trigram models remain successful popular choice 
view sections detailed investigation relative performance approaches attempt determine strengths weaknesses 
tests employ language models trained wsj corpus category model having produced pruning threshold ct gamma defined equation 

probability estimates firstly investigate differences values probability estimates delivered model 
histograms shows number words test set predicted various log probabilities model 
note particular ffl category model assigns probabilities gamma smaller proportion words 
ffl larger number words predicted high probability category model 
log probability category model word model distribution log probabilities produced word category models 
conclude ability category model generalise unseen word sequences leads smaller number low probabilities 
characteristic allow capture word specific relations strongest responsible high probability estimates delivered word model 
section 
bin size 
ph dissertation thomas niesler cambridge university june chapter variable length category grams page 
effect backoffs objective investigate probability estimates category model word trigram back backs various degrees 
table presents perplexities calculated type word model gram request backoff 
category word models denoted cbm respectively 
type gram request test set cbm perplexity perplexity unigram bigram bigram backed unigram trigram trigram backed bigram trigram backed unigram table perplexities various types word gram requests 
results may conclude word model back situation true approximately words test set performs significantly better average category model 
backoffs occur longer true intrinsic ability generalise unseen word tuples allows category model deliver better probability estimates 
interesting note significant proportion approximately predictions category model better word model backing occur 

category analysis objective analyse contribution test set log probability individual categories average log probability associated non backed word grams 
category taken assigned word tagger tag training corpus 
figures illustrate absolute fraction total log probability category accountable shows category average log probability 
horizontal axis indicates frequency category occurs test set 
figures conclude ffl previous section backing occur word model performs better category model 
ffl clear relationship frequency occurrence category perplexity categories lowest average log probability seen small number times visible evident expansion horizontal axis orders magnitude 
categories sparsely trained seen small number times training set 
significant grammatical categories labelled figures common noun nn plural common noun nns adjective jj proper noun np verb base form vb past tense verb vbd past participle vbn participle vbg singular verb vbz adverb rb preposition cardinal cd singular plural article ati singular article coordinating conjunction cc subordinating conjunction cs letter alphabet zz sentence marker se infinitival unit measurement 
ph dissertation thomas niesler cambridge university june chapter variable length category grams page ffl categories low average log probabilities infrequent minor contribution 
ffl common nouns nn significant log probability followed plural common nouns nns adjectives jj proper nouns np 
ffl general words semantic content nouns adjectives harder predict lower probability syntactic function words prepositions articles conjunctions personal pronouns 
true word category models 
vertically separated groupings illustrate point 
ffl appears approximately linear relationship frequency category occurs contribution log probability 
constant proportionality different words significant semantic content syntactic function words respectively 
emphasised elongated groupings figures 
relative frequency occurrence test set fractional contribution log probability nn nns np cd ati se zz vb cc jj rb function syntactic categories categories semantic vbd vbn vbg contribution category test set log probability word model 
ph dissertation thomas niesler cambridge university june chapter variable length category grams page relative frequency occurrence test set fractional contribution log probability nns cd se cc cs zz rb vbg np jj categories function semantic vbn vb ati nn vbd contribution category test set log probability category model 
relative frequency occurrence test set nn zz se ati cd syntactic function categories vbd vb nns jj np rb vbg cc semantic categories vbz vbn average log probability category word fi category models 
ph dissertation thomas niesler cambridge university june chapter variable length category grams page 
gram analysis findings preceding sections clear word grams carry significant amount information captured category counterparts 
objective section investigate proportion word grams play significant role improving category model 
order contribution difference total log probability word category model distinct word trigram non backoff situation calculated 
contributions subsequently sorted order decreasing value normalised graphed 
note portion curve due trigrams assigned higher probabilities category model 
fraction distinct trigrams test set fraction difference total contribution difference log probabilities generated word category models 
may deduce approximately half word trigrams contribute lead word model category counterpart half predicted equally better 
furthermore improvement contributed trigrams 
category word models conjunction possible significantly compact 
matter interest table lists examples trigrams word fare better respectively 
ph dissertation thomas niesler cambridge university june chapter variable length category grams page word model better category model better border patrol rising roman catholic compared accepted accounting principles analysts national borders point adam eve announced zero fahrenheit declined caught surprise closely held declaration independence declined quote year old farmers raise investment banker percent great barrier says grown accustomed agreed increase cash flow italian french lowest discount fares year shares mcdonnell douglas astronautics minister president doubled nobel peace prize new orders open heart surgery open closed possible business combination previously reported president francois president rest registered unemployed rose say satellite orbit futures sentiment remained healthy soviet air spokesman television sports percent vancouver british columbia vision chief table trigrams assigned higher probabilities word category models respectively 

robustness domain change experiments point employed test set character matches training set closely derived newspaper wall street journal period 
important issue language modelling model fare data different test set domain different newspaper entirely different character style text 
order investigate performance language models domains wsj additional test sets compiled lob corpus 
ffl press categories lob corpus various newspaper articles reviews wsj 
ffl religion category lob corpus contains text concerning religious topics 
ph dissertation thomas niesler cambridge university june chapter variable length category grams page ffl scientific writing category lob corpus 
ffl fiction categories lob corpus consisting adventure mystery western romantic humorous fiction 
table summarises performance word category language models cbm respectively test sets 
performance wsj test set shown comparison 
category model experiment constructed pruning threshold ct gamma 
test set 
words oov perplexity perplexity cbm wsj baseline press religion scientific writing fiction table performance category word models different test sets 
word model outperforms category model cases interesting see perplexity increases factor perplexity factor indicating reduced sensitivity change test domain 

summary category language model employing grams varying lengths described 
procedure allowing gram length optimised respect estimated performance experiments lob corpus show outperform conventional gram approaches reducing number model parameters 
words may belong multiple categories consequently model bases probability estimates set possible classifications word history category sequences 
classification associated probability updated recursively successive word sentence 
order capture syntactic patterns language model categories chosen correspond part speech classifications 
classifications available large untagged text corpora application language model statistical tagger described experimental evaluation shows improved tagging accuracy compared benchmark 
category language models significantly compact word models offer competitive performance terms perplexity especially sparse corpora 
category models able capture relationships particular words categories words belong able fully exploit information available large training corpus 
reason word models perform significantly better amount training material increases 
closer analysis reveals lead due relatively small proportion grams word model 
particular category probability estimates remain competitive superior instances notably word models needs back 
reason category models display greater robustness changes characteristics testing domain 
ph dissertation thomas niesler cambridge university june chapter word category backoff models page chapter word category backoff models 
section shown category language model section delivers better probability estimates backoff situations due intrinsic ability generalise unseen word sequences 
shown small fraction word grams contribute largest part difference performance 
chapter consider technique allows backoffs take place word category gram probability estimate aim retaining advantages offered approach 

exact model consider language model backs word category probability estimate pwc phi pw phi phi fi phi delta phi 
ffl word estimate probability occurrence 
ffl phi word history probability estimate word gram language model depends referred word level context 
bigram preceding word trigram preceding words 
ffl phi word history associated set category history equivalence class postulates probability estimate category model depends termed category level context 
due recursive nature category history equivalence class postulates maintained category level context general completely defined entire word history current sentence 
number contexts potentially huge mapping phi phi 
ffl pw phi probability estimate obtained word language model 
ffl phi corresponding probability obtained category language model 
refer section 
ph dissertation thomas niesler cambridge university june chapter word category backoff models page ffl phi set words word context phi word model estimates backoffs occurring cases 
ffl fi delta backoff weight fi delta 
estimate designed employ word grams capture significant sequential dependencies particular words category model frequent word combinations 
requirement pwc phi phi follows fi phi gamma wt pw phi gamma wt phi 
approximate model due large number different possible phi category model fi delta equation feasible 
convenient obtain backoff constants word level context dependence denominator phi permit uniquely fixed phi run time calculation fi delta equation increases computational complexity probability calculation particular context factor approximately comparison model parameters precalculated represents significant computational burden 
circumvent note context strongly influenced words approximation phi phi phi category level context corresponding phi assuming prior knowledge words preceding phi note unique phi phi may define phi def phi phi phi approximate backoff weights fi phi fi phi def fi phi phi phi find fi phi gamma wt pw phi gamma wt phi ph dissertation thomas niesler cambridge university june chapter word category backoff models page choice fi delta general longer satisfies equation adjust backoff model follows pwc phi phi phi fi phi delta phi 
phi approximation pw phi 
equation satisfied follows wt phi gamma fi phi fi phi delta wt phi may choose define phi def phi delta gamma fi phi fi phi delta phi order satisfy require wt phi quantity gamma fi phi may interpreted probability mass distributed elements suitable choice phi 
adopted approach distribute mass ratio pw phi wt pw phi distribute probability mass grams approximately proportion language model 
proceeding equations employing approximation led require phi delta gamma fi phi fi phi delta phi gamma fi phi fi phi delta wt phi pw phi wt pw phi follows ff phi gamma fi phi fi phi delta wt phi delta pw phi wt pw phi gammafi phi delta phi ff phi phi delta gamma fi phi ph dissertation thomas niesler cambridge university june chapter word category backoff models page choice phi satisfies equation 
furthermore phi phi find phi gamma gamma wt pw phi gamma wt phi gamma wt pw phi gamma wt phi delta wt phi delta pw phi wt pw phi gamma gamma wt pw phi gamma wt phi delta gamma wt phi delta pw phi wt pw phi pw phi means approximate backoff converges exact backoff estimates phi approach exact values phi 
equation guarantees phi approximation perfect general 
order guarantee sufficient require ff phi equation follows fi phi pw phi phi delta wt pw phi pw phi delta gamma wt phi 
calculating fi phi equality equation enforced inequality violated 
referring back equation equivalent demanding probability mass distributed word positive 
practice adjustment required infrequently 
estimates delta allows backoff constants ff phi fi phi precalculated making model significantly computationally efficient exact model continuing employ backoff situations probabilities delivered category model 
related research carried independently concurrently reported 
describes method allows backoffs occur word category bigram language ph dissertation thomas niesler cambridge university june chapter word category backoff models page model 
case words belong unique category greatly simplifies calculation backoff weights 

model complexity determining point assumed word level context phi set words established probabilities calculated word language model 
simple choice set words seen context phi training set 
denote choice note backing occurs truly unseen events 
approach taken reduce size eliminating words presence afford word model predictive power relation category model 
process eliminates grams word model component allows complexity word category backoff language model reduced 
note category component fixed determines minimum complexity 
number words reduced ways leading similar results 

testing gram counts consider word seen context phi total phi times denote number times context phi seen phi 
category model probability estimate phi number times expect see phi phi phi delta phi assuming words occur independently context exhibiting binomial distribution variance count phi oe phi phi delta gamma phi delta phi normal approximation binomial distribution determine actual count phi exceeds expected count phi certain fraction ffi certain confidence testing phi gamma ffi delta phi delta oe phi retained test succeeds 
practice value fixed confidence levels value ffi varied control size 
testing effect probability testing gram counts described previous section allows grams retained counts seen differ expected ones statistically significant manner 
effect pruning decisions perplexity resulting language model clear 
precisely frequent gram occurring slightly word category model discarded significant effect probability due high complexity word category backoff model taken sum total number grams word components 
ph dissertation thomas niesler cambridge university june chapter word category backoff models page frequency 
avoid second pruning criterion implemented discards grams smallest effect training set log probability 
perplexity versus complexity tradeoff resulting models similar achieved count pruning method 
particular gram retained ffi change mean word log probability word model category model calculated phi delta log pw phi gamma log phi ij total number words training corpus 
model building procedure summary sections steps need taken order construct word backoff language model ffl build category language model described chapter 
ffl build word gram model 
ffl application equation determine probabilities phi gram word model 
ffl context word model determine set equation equation 
essentially process pruning grams word model 
ffl remaining grams word model calculate ff fi values equations 
ffl apply language model equation 

results gauge performance backoff technique applied lob switchboard wsj text corpora case language models various complexities generated varying size described section resulting perplexities compared achieved word trigram trained data 
size controlled standard technique discarding grams occurring fewer threshold number times training text varying gram cutoffs 
identical thresholds employed bigrams trigrams cases 
normalisation quantity nc ffi fairly corpus independent 
descriptions corpora division test training sets may appendix ph dissertation thomas niesler cambridge university june chapter word category backoff models page 
lob corpus category language models differing complexities built pruning thresholds ct gamma ct gamma described chapter 
table shows details performance resulting word category backoff models category model category model word trigram parameters perplexity table language models lob corpus 
complexity number grams perplexity word trigram performance word category backoff trigram language models lob corpus 
models significant perplexity reductions achieved 
example model approximately grams results perplexity improvement word trigram 
furthermore favourable size versus performance tradeoff achieved particularly smaller ct gamma category model 
built category model respectively 
ph dissertation thomas niesler cambridge university june chapter word category backoff models page 
switchboard corpus category language models constructed pruning thresholds ct gamma ct gamma table shows characteristics individual models shows performance resulting word category backoff models 
category model category model word trigram parameters perplexity table language models switchboard corpus 
complexity number grams perplexity word trigram performance word category backoff trigram models switchboard corpus 
larger category model leads word category backoff model lower minimum perplexity improved performance model complexities exceeding approximately grams smaller leads performance slightly diminished region better smaller numbers grams 
word category backoff model offers slight improvement perplexity respect trigram approximately depending choice category model complexity significantly improved complexity versus performance tradeoff characteristic 
limited number conversational topics switchboard corpus leads reduction training set sparseness better coverage word trigram lob trigram backoff rate drops 
need generalising ability consequently smaller perplexity improvement 
ph dissertation thomas niesler cambridge university june chapter word category backoff models page 
wsj corpus corpus results determined category model constructed pruning threshold ct gamma 
table shows individual language model details performance word category backoff trigram models 
category model word trigram parameters perplexity table language models wsj corpus 
complexity number grams word trigram performance word category backoff trigram language models wsj corpus 
see case word category backoff model offer substantial perplexity improvements word trigram number parameters large 
continues offer better complexity versus performance tradeoff 
switchboard corpus perplexity improvements small word model trained wsj due large quantity training data 

summary chapter language model backs word category gram estimate 
category model able generalise unseen word sequences appropriate backoff situations 
compared standard trigram models technique greatly improves perplexities sparse corpora offers significantly enhanced complexity versus performance tradeoffs 
ph dissertation thomas niesler cambridge university june chapter word pair dependencies category language models page chapter word pair dependencies category language models 
underlying assumption category language model chapter probability word depends category belongs occurrence equally point corpus category occurs 
factors topic style text may cause certain words occur groups violating assumption 
short term fixed length word dependencies may incorporated means word grams detailed chapter longer term relations treated separately 
chapter presents technique means long range word associations may captured category language model 
explicit account taken transient strength relationship function particular definition separating distance 
word pairs combined word gram language models maximum entropy framework linear interpolation long distance bigrams 
development differs takes explicit account distance word occurrences takes specific advantage category language model 

terminology consider effect occurrence trigger word category pair trig trig subsequent probability occurrence target pair targ targ 
refer sequence consisting trigger occurrences words belonging target category trigger target stream denote trig targ 
total number words stream trig targ number occurrences trigger target words respectively trig targ 
henceforth assumed stream taken training corpus condition note trig trig trig targ targ targ number times corpus occurs member category furthermore targ trig targ targ trig trig number times category occurs training corpus 
ph dissertation thomas niesler cambridge university june chapter word pair dependencies category language models page assuming trigger target statistically independent occurrence probabilities trig targ respectively trig trig targ targ targ jv targ targ targ targ see stream category conditional word probabilities target related targ jv targ delta targ targ define separating distance trigger target pair number times word belonging category targ seen witnessing trigger sighting target 
separating distance trigger target stream 
definition distance employed way minimising syntactic effects word occurrences notably phenomenon certain categories rarely follow grammatical reasons 
syntactic effects reduced possible modelled category gram component language model 
distinction drawn case trigger target word termed self triggers case differ referred trigger target pairs 

probabilistic framework assumption probability word depends designated category stipulated equation referred independence assumption 
empirical investigation category conditional probability jv function distance reveals exponential decay constant words correlation exists 
illustrates case trigger noun president target proper noun congress 
data drawn wsj corpus refer appendix 
ph dissertation thomas niesler cambridge university june chapter word pair dependencies category language models page distance measured conditional probability independence assumption measured conditional probability jv congress having seen president 
transient behaviour displayed graph typical motivated postulated form category conditional probability targ jv targ fl delta gammaae deltad exponential decay constant probability fl ae define strength rate decay respectively 
stream probability scaling equation targ fl delta gammaae deltad delta fl delta fl 
assuming triggers occur independently stream probability equation trig follows probability mass function target occurrence sighting trigger delta gamma gamma gamma gamma fl delta gammaae deltai delta fl delta gammaae deltad normalising constant accounts probability mass associated cases trigger follows trigger sighting target 
empirical estimates obtained binning counts graphed distance range 
storage point view potentially extremely large number word pair relations approach infeasible large scale application possible obtain parameters equation direct fit data 
estimation fl ae treated sections 
ph dissertation thomas niesler cambridge university june chapter word pair dependencies category language models page 
estimating probability may estimated tail distribution transient effect exponential term assumed insignificant 
trigger target occur independently separating distance geometric distribution mean rough indication point exponential term negligible gamma trig gamma targ trig targ estimate counts trigger pair occurrences distances mean targ targ trig targ targ trig trig targ targ trig respective number times target word targ target category targ trigger word trig seen distances exceeding trigger target stream targ small estimate may unreliable introduce smoothed estimate delta targ gamma delta targ parameter chosen targ targ equation interpolation relative frequency relative frequency obtained pooling data irrespective 
interpolation weight depends counts tends unity number sightings increases 
value related number sightings consider give confidence estimate determines quickly approaches 
chosen small constant greater zero 
experiments carried particular value observed influence performance strongly 

estimating fl ae expressions allowing determination fl ae knowledge mean mean square distances separating trigger target derived 
mean mean square calculation requires little storage represents memory efficient alternative direct fit conditional practice ae constrained exceed gamma clamped maximum time constants 
equation approaches equation 
ph dissertation thomas niesler cambridge university june chapter word pair dependencies category language models page probability function measured binned data 
order obtain closed form expressions mean mean square exact distribution approximated form delta ffl delta gamma delta ffl delta gamma delta ffl ffl equations relate parameters exact approximate distributions details derivation shown appendix psi gammafl gammap gammap delta gammae gammaae ffl delta psi fl gamma ffl delta ffl gamma gamma delta ae delta fl psi gammafl fl gammap delta psi values calculated stream counts equations 
order solve ffl ffl fl ae equations measured mean mean square distance trigger target employed 
estimated data quantities sensitive outliers 
particular may happen trigger target occur unrelated parts training corpus consequently separated large quantities text robustness significantly improved measuring mean mean square predetermined distance range delta delta delta gamma 
expressions mean mean square expected truncated measurements independence assumption derived appendix equation superposition geometric terms may employ results appendix express truncated mean mean square linear combination corresponding terms truncated geometric distributions delta ffl delta ffl delta delta ffl delta ffl delta equations may calculate values fl ae 
possible solve explicitly fl ae terms parameters values determined numerically means nested bisection searches 
particular problem information regarding segmentation corpus article boundary markers available 
geometric mean calculated equation employed truncation interval practice 
ph dissertation thomas niesler cambridge university june chapter word pair dependencies category language models page 
typical estimates repeats curves adds plot equation parameters fl ae determined results sections 
estimated conditional probability reflects true nature data closely independence assumption 
distance measured conditional probability estimated conditional probability independence assumption measured estimated conditional probability jv congress having seen president 

determining trigger target pairs number possible self triggers bounded vocabulary size number potential trigger target pairs equals square number possible consider relations exhaustively small vocabularies 
order identify suitable candidates feasible manner approach employing passes training corpus developed 

pass stage processing provides potential target word lexicon tentative list fixed list trigger candidates 
contains words reliable correlation target established holds decision reached regarding presence absence relationship 
tentative list includes storage cumulative totals required distance mean variance calculations memory intensive fixed list 
certain point processing trigger target pair seen nm times separations fd dnm gamma falling chosen truncation interval nm gamma measured mean meas nm delta nm gamma ph dissertation thomas niesler cambridge university june chapter word pair dependencies category language models page measured variance oe meas nm gamma delta nm gamma gamma meas assuming trigger target occur independently expected mean obtained truncated binomial distribution 
drawing results appendix may write exp gamma tt delta gamma tt nt gamma gamma delta gamma tt gamma tt delta gamma gamma tt nt tt trig targ probability occurrence trigger target calculated uniformly stream 
truncated mean variance measurements reduce sensitivity measurements outliers 
statistics members tentative list updated sighting associated target sequential processing corpus 
update hypothesis tests termed kill fix tests respectively decide strength correlation 
mean variance measured data empirical observation samples indicated possess approximately normal distributions test employed decision manner ffl kill test 
measured mean meas exceed expected mean exp specified margin ffi kill confidence gamma ff kill kill test succeeds trigger candidate deleted tentative list 
particular kill exp delta ffi kill critical value mean meas gamma ff kill nm gamma delta oe meas nm ff kill nm gamma value obtained distribution confidence gamma ff kill nm gamma degrees freedom 
kill test succeeds kill illustrates conditions 
kill kill exp kill meas kill margin confidence margin kill exp margin don kill kill kill meas confidence margin illustration kill test 
ph dissertation thomas niesler cambridge university june chapter word pair dependencies category language models page ffl fix test expected mean exp exceed measured mean meas specified margin ffi fix confidence gamma ff fix fix test succeeds trigger candidate moved tentative fixed list 
particular fix exp delta gamma ffi fix critical value mean meas ff fix nm gamma delta oe meas nm ff fix nm gamma value obtained distribution confidence gamma ff fix nm gamma degrees freedom 
fix test succeeds fix illustrates conditions 
meas exp fix fix fix confidence margin fix margin meas exp fix fix margin confidence margin fix don fix illustration fix test 
kill test allows unpromising candidates pruned continually tentative list explosion number considered word pairs arises 
illustrates showing growth number tentative triggers kill test disabled active 
number words processed kill test disabled kill test enabled effect kill test total number tentative triggers 
ph dissertation thomas niesler cambridge university june chapter word pair dependencies category language models page correlation established fix test succeeds trigger moved tentative fixed list statistics need gathered 
separate tentative fixed lists maintained compact including storage means variances 
extremely important view generally large number trigger target candidates considered pass 
initially tentative fixed lists empty 
furthermore record unique words maintained processing 
word corpus processed sequentially member history hypothesised possible trigger word candidate processing steps performed ffl sighting target trigger 
ffl trigger fixed list 
ffl trigger tentative list add tentative list 
initialise cumulative sum distance sum squared distance fields measurement 

ffl trigger tentative list update sum distance sum squared distance new measurement 
calculate distance mean variance 
calculate expected mean independence assumption 
perform kill fix tests case measured mean exceeds independence mean desired margin desired level confidence conclude correlation 
kill remove trigger tentative list 
case measured mean lower independence mean desired margin desired level confidence conclude correlation 
fix remove trigger tentative list add fixed list 
case conclude insufficient data reach decision 

result pass set trigger target relations fixed lists completion remaining tentative lists discarded 

second pass fix test pass uses means variances gathered small portion training set detected correlations may due local anomalies generalise corpus 
consequently second pass recalculates means variances candidates entire training corpus applies fix test discarding fail 
measured means mean squares remaining candidates calculate parameters fl ae postulated conditional probability function 
ph dissertation thomas niesler cambridge university june chapter word pair dependencies category language models page 
regulating memory usage selection fix test margin confidence level allows rate tentative fixed list regulated provides control growth final number 
kill test parameters hand affect rate deletions tentative list 
length history determines rate addition new tentative trigger candidates current target 
size tentative list prime practical importance pass processing entry requires significantly storage fixed list 
despite control size afforded choice kill test parameters may difficult limit number pairs considered practical levels 
refinements employed additional measures regard 

exclusion lists semantic correlations may expected chiefly content words grammatical functions words known possible exclude non content words consideration triggers targets processing 
practically achieved means exclusion list containing grammatical categories disregarded way 

background culling observations pass processing shown large number tentative list members predominantly idle 
infrequent trigger target candidates added list fixed killed due insufficient number measurements long periods updates 
order reduce number cases process termed background culling introduced 
processing distance update monitored members tentative list decision boundary kill threshold moved gradually fix threshold time increases 
relaxes kill test ultimately forces kill decision 
rate occurs normalised respect frequency trigger single global parameter may set pruning 
background culling approximation necessitated practical considerations generally introduces errors eliminating valid infrequent trigger target relations 
allows size tentative list regulated practical levels large corpora vocabularies illustrated 
furthermore number trigger target pairs remaining appears practice affected strongly background culling 
particular tests lob corpus showed number tentative triggers pass reduced number fixed triggers surviving second pass fell figures depend fix kill thresholds chosen pass 

example pairs table lists examples typical targets triggers described technique applied lob corpus 
bracketed designations grammatical categories words question appealing find intuitive relationships meaning word pairs gathered purely statistical criteria 
jj adjective nn common noun nns plural common noun unit measurement np proper noun nr singular adverbial noun vb verb base form vbn past participle 
ph dissertation thomas niesler cambridge university june chapter word pair dependencies category language models page words training set processed severe culling mild culling culling effect background culling total number tentative triggers 
target triggers discharged jj prison nn period nn supervision nn need nn prisoner nn voluntary jj assistance nn advocate nn truth nn box nn defence nn honest jj face vb case nn witness nn evidence nn cambridge np university nn educational jj affected vbn np tomorrow nr universities nns worked vbn demand nn changes nns cost nn strength nn dry jj nns nn 
wines nns nns judicial jj legal jj binding jj rules nns jj world nn substantial jj fall nn trade nn demand nn supply nn cinema nn directors nns viewing nn film nn festival nn tastes nns current nn inductance nn constant nn capacitor nn voltage nn nn respiration nn failure nn vbn body nn nn sea nn salt jj minutes nns jj water nn nn recovery nn nn survival nn rotor nn 
values nns blade nn nn speed nn flapping nn wind nn tunnel nn helicopter nn body nn nns syntax nn language nn categories nns formal jj syntactic jj grammatical jj morphology nn nn bleeding nn blood nn cells nns ml reaction nn nn nns patient nn group nn treated vbn increases nns salary nn agreement nn salaries nns nns list nn lemon nn milk nn salt nn 
brandy nn mixed jj nns nn sugar nn nn oz nns eggs nns peel nn apples nns np np np tale nn nn np np policy nn africa np south np table examples triggers targets collected lob corpus 
ph dissertation thomas niesler cambridge university june chapter word pair dependencies category language models page 
perplexity results benefit characterising trigger pairs described previous sections gauged comparing performance category language model employing independence assumption equation identical respects 
experiments carried lob wall street journal wsj corpora category language models constructed corpus respective pruning thresholds ct gamma ct gamma construction variable length category grams 
word pair distances calculated article boundaries 
details language models constructed corpora summarised table 
information standard trigram language model katz backoff turing discounting order establish baseline 
symbols nw cng refer number words vocabulary number grams trigram number grams category language model respectively 
number self triggers trigger target pairs parameters estimated indicated st tt corpus nw cng st tt lob wsj table language models word pair relations lob wsj corpora 
table shows perplexities pp trigram tg category model cm category model self triggers cm st trigger target pairs cm tt lastly self triggers trigger target pairs cm st tt 
corpus tg cm cm st cm tt cm st tt pp pp pp lob wsj table perplexities including word pair relations category model 
table shows perplexities pp obtained word category backoff language models developed chapter conjunction category model employing self trigger trigger target pairs 
corpus tg st st tt total grams pp pp pp total lob wsj table perplexities including word pair relations word category backoff language model 
descriptions corpora appendix function words excluded means suitable exclusion list described section 
ph dissertation thomas niesler cambridge university june chapter word pair dependencies category language models page table see considerable reductions obtained combining approaches described chapters respectively 
note lob corpus perplexity reduced respect word trigram parameters 

discussion largest perplexity improvement obtained wsj corpus largest number self trigger trigger target pairs collected 
stems greater corpus size consequent lower sparseness 
lob hand words occur infrequently estimation conditional probability parameters possible leading reduced number word relations 
corpora addition self triggers significant impact perplexity trigger target pairs 
agrees reports literature 
reliable target trigger seen predicted occur 
trigger target pairs hand predict words seen occurred distant past 
correlations heavily dependent topic passage effectiveness trigger target association depends topics associated trigger coincide training test set 
lob corpus diverse material contains significant mismatch regard leading observed small impact self triggers performance 
wsj corpus mismatch smaller leading greater success 
performance improvements obtained exponential decay self trigger model appears compare favourably obtained cache component similar philosophy usually allow cache probability decay distance comparable way 
example addition cache component decreases perplexity part speech language model lob corpus 
experiments addition self triggers causes fall table 
wsj corpus perplexity improvements achieved harder compare directly results include effect bigram cache 
little information literature regarding performance language models incorporating trigger target pairs 
experiments addition means equation improves perplexity unigram language model 
small improvements shown table promising respect 
reported unfortunately show individual impact performance cache trigger target pairs added components 
probably due numerically intensive maximum entropy technique combine additional knowledge sources baseline trigram language model 
affirms performance standard cache component may enhanced allowing probabilities decay exponentially 
addition self triggers increases number parameters category model delta st storage fl ae 
increase mild offers favourable size versus performance tradeoff 
instance category model self triggers lob uses parameters achieves lower perplexity word trigram parameters 
furthermore effectiveness types word pair modelling improves corpus size parameter determination final implementation model low memory requirements technique suitable large training sets 
complements category model performance improve way 
ph dissertation thomas niesler cambridge university june chapter word pair dependencies category language models page inspection values ae assigned trigger target pairs cases trigger successfully predicts target shows correlations range conventional gram models captured 
proposed technique able model long range dependencies 
may deduced reductions obtained adding word pair relations category model word category backoff framework table 
particular improvements indicate components contribute different information model 
final component models exhibit significantly lower perplexity baseline word trigram 
true lob larger wsj corpus particularly striking indicates imposition structure advocated section particularly effective training set sparse 

summary new technique modelling empirically observed transient character occurrence probability related words body text introduced 
procedures identification word pairs estimation parameters required parametric model developed 
experiments demonstrate meaningful relations identified transient behaviour spans words successfully captured proposed model 
perplexity reductions achieved greatest improvement largest sparse corpus 
words correlated self triggers significant impact performance 
modelling technique able reduce performance limit displayed category models large corpora maintaining performance versus size tradeoff 
integrated word category backoff framework improvements achieved allowing baseline trigram language model perplexities surpassed 
reduction particularly pronounced smaller lob corpus totalling ph dissertation thomas niesler cambridge university june chapter word error rate performance page chapter word error rate performance perplexity popular measure language model performance due lower computational demands compared complete recognition experiment 
reductions perplexity guarantee reductions recognition word error rates remain ultimate measure language model quality 
sections introduce methods means language models may incorporated recognition search recognition results obtained wall street journal task language models developed thesis 

language models recognition search ideally connected speech recogniser search space possible concatenations words find combination respect acoustic evidence 
small vocabularies exhaustive approach may impractical refined methods necessary 
important practical standpoint understand mechanism decoding process nature language model may heavily influence complexity search problem 
particular various alternative means language model application available illustrated remainder section 
consider example speech recogniser recognise word sentence containing words says hello 
set possible sentences may visualised tree structure depicted 
connected speech times word boundaries occur known 
affect probabilities obtained acoustic models determining segments observation sequence assigned model possible choice word boundaries considered separate hypothesis recogniser 
total number distinct hypotheses considered search larger number sentences tree shown come surprise small vocabularies search space practice large considered exhaustively 
size search space limited practical constraints processing time available storage techniques instrumental making recognition practical 

path pruning search proceeds certain hypotheses may discarded save computation storage 
guaranteed paths correct result probable pruning may lead search errors 
ph dissertation thomas niesler cambridge university june chapter word error rate performance page says hello says hello says hello hello says says hello hello says says hello hello says says hello hello hello says hello says says says hello start tree representation sentences considered search 

path merging paths meet may possible merge consider single continuation 
permissible depends extent computation involved extending path dependent history 
particular paths may merge histories considered equivalent computational point view 
language models generally context acoustic models dictate points merges may occur 
particular merges may occur respective history equivalence classifications discussed section match 
consider example 
language model probabilities calculated word boundaries bigram search space illustrated reduced shown 
hello says says says hello hello start search space bigram language model 
trigram employed language model probability calculation word pair paths merge exiting word illustrated 
path merging allows substantial reductions number separate paths requiring consideration time 
context independent word internal context dependent phone models employed calculation acoustic probabilities dependent current word 
cross word context dependent models calculations may influenced identity phones preceding word boundary 
information language model generally exceeds 
ph dissertation thomas niesler cambridge university june chapter word error rate performance page says hello hello hello says says hello hello says says says hello hello says start search space trigram language model 
trigram longer span language models number paths may large efficient path merging 
containing search require severe pruning unacceptably increase number search errors 
cases may adopt pass strategy 
apply simpler bigram language model recognition search output subset search space containing number paths termed intermediate hypotheses 

post process rescore intermediate hypotheses applying sophisticated language model pick final recognition result 
long result obtained recognition search sophisticated language model intermediate hypotheses final result remain unaffected division recognition process stages 
pass eliminates hypothesis search error occurs second pass unable correct 
accuracy intermediate hypotheses affected exactness pass size intermediate hypotheses important issue pass approach 
forms intermediate hypotheses best lists lattices described sections 

best rescoring 
best method determines recognition search list containing hypotheses acoustic language model probabilities word 
new language model replace modify existing probabilities hypothesis likelihood recalculated 
recognition result highest ranking hypothesis new list 
proportion containing result full recognition search 
number alternatives 
ph dissertation thomas niesler cambridge university june chapter word error rate performance page 
lattice rescoring 
list recognition search may output network containing paths recognition search space 
lattice directed graph nodes correspond word boundaries time links nodes correspond particular word hypotheses 
acoustic language model likelihoods word stored lattice path start node represents distinct hypothesis 
path lattice may suitable search algorithm having applied successfully 
lattice compact representation set alternatives best list certain situations lattice search may performed efficiently 
prerequisite significant path recombination possible number separate paths maintained lattice search may unmanageable 
best lists hand storage computational effort required rescoring defined priori 
note best lists may generated lattices 

recognition experiments section presents recognition results terms word error rates obtained language models developed thesis 
experiments conducted wsj corpus lattices generated cambridge university htk large vocabulary speech recognition system part november arpa csr evaluation 
htk recogniser uses mixture gaussian cross word context dependent hidden markov acoustic models allows scoring single pass incorporating gram language model deliver state art performance 
lattices produced word vocabulary development test baseline bigram trigram language models described appendix resulting trigram lattices generate best lists subsequently new language models obtain final recognition results described section 
section presents baseline recognition performance ensuing sections show performance changes application various language models 
language model scaling factors rescoring optimised approximately test set cases including baseline 

baseline results lattices development test set comprising approximately sentences speakers leading total words transcription 
vocabulary rate word vocabulary respect transcription original lattices constructed language model trained substantially data wsj bigram trigram language models described appendix obtain baseline recognition accuracies shown table 
perplexities shown language model lm test set appendix development test transcription 
perplexity word error lm test set dev test ref 
baseline bigram baseline trigram table baseline performance development test set 
ph dissertation thomas niesler cambridge university june chapter word error rate performance page changes system performance application language models measured relative baseline trigram error rate obtain indication best possible recognition performance obtainable lattices best lists error rate shown table 
error rate lattices best table lower bounds word error rates 
recognition experiments sections rescore best hypotheses lattices lowest achievable word error rate 
rescoring results entropic lattice language modelling toolkit employed interface allowing convenient best rescoring new language models 
particular tools determine best hypotheses lattice rescore new language models re rank result output best hypothesis list 
new language models replace baseline language model probabilities entirely combined baseline trigram linear interpolation 
various combinations language model components thesis rescoring process results shown sections 

category model variable length category gram model developed chapter constructed pruning threshold ct gamma rescore best lists means linear interpolation baseline trigram probabilities lattice 
table summarises results obtained various interpolation conditions perplexities shown test set described appendix transcription 
interpolation parameter weights category model value zero corresponds rescoring baseline trigram 
weight perplexity word error improvement lm test set dev test ref 
table rescoring category model interpolated baseline trigram 
interpolation able reduce perplexity baseline trigram approximately test ph dissertation thomas niesler cambridge university june chapter word error rate performance page set transcription resulting improvement error rate 
note optimum performance achieved lowest perplexity test set 
similar observations 
performance appears relatively insensitive interpolation weight range 

word category backoff model word category backoff models developed chapter rescore best lists replacing lattice scores entirely 
category model constructed pruning threshold gamma experiments carried word category backoff models different complexities shown table 
number grams perplexity word error improvement lm test set dev test ref 
table rescoring word category language model 
results show word error rate improvement complex model retains word model grams smaller improvements eventual deterioration performance number parameters reduced 
better performance terms perplexity word error rate achieved linear interpolation baseline shown table 

category model long range correlations word pair relations described chapter require determination separating distance record kept document history 
achieved retaining best recognition output lattice current article processing lattices order spoken 
table shows perplexities recognition results obtained incorporating self triggers st trigger target tt pairs category model section 
linear interpolation weight employed rescoring process 
configuration perplexity word error improvement lm test set dev test ref 
category st category st tt table category model word pair correlations interpolated baseline trigram 
linear interpolation category model including word pair relations decreased perplexities respect baseline test set transcription respectively led word error rate improvement improvement verified ph dissertation thomas niesler cambridge university june chapter word error rate performance page significant level nist scoring package 
case perplexity largest part reduction word error rate brought addition self triggers 

word category backoff model long range correlations word category backoff model parameters rescore best lists section employing category model relations evaluated previous section 
perplexities recognition results including self triggers st trigger target tt pairs shown table 
configuration perplexity word error improvement lm test set dev test ref 
st tt table rescoring word category backoff model word pair correlations 
perplexities similar reported table improvements word error rate smaller 
furthermore disappointing note addition word pair relations led deterioration word category backoff model results shown table 

summary category language model best rescoring framework led improvements recognition word error rate applied linear interpolation lattice internal trigram probabilities means word category backoff method 
subsequent addition word correlation models lead improvements interpolating category model baseline trigram conjunction word category backoff method 
baseline trigram language model probabilities lattice linear interpolation simpler implement memory intensive method choice similar recognition problems 
interpolation category model employing self triggers trigger target pairs led relative word error rate improvement baseline verified statistically significant 
probability improvement chance 
ph dissertation thomas niesler cambridge university june chapter summary page chapter summary thesis focussed linguistically defined word categories means improving performance statistical language models 
particular approach aims capture general grammatical patterns particular word dependencies different model components developed evaluated 

review conducted separate treatment patterns due syntax patterns due semantic relationships led distinct sections document 
section chapter develops model syntactic dependencies word category grams 
second section chapter extends model allowing short range word relations captured incorporation selected word grams 
technique permits inclusion long range word pair relationships chapter 

category syntactic model gram proved successful model short term word dependencies 
noting english grammatical constructs quite local nature grams part speech word categories adopted means capturing general sequential grammatical patterns 
significantly fewer parts speech words typical vocabulary models contain smaller number grams comparable word model 
reduces sparseness data respect number parameters turn allows capture longer range effects increasing furthermore important advantage category grams intrinsic ability generalise word tuples seen training 
part speech categories embed syntactic information generalisation proceeds measure grammatical correctness assigned unseen sequence model 
chapter proposes develops evaluates model employing category grams variable length 
word may belong category order account different grammatical functions 
length individual grams optimised allowing increase long extension benefits predictive quality 
order avoid overfitting training set criterion estimate predictive quality leaving cross validation 
language models produced way seen contain grams length varies unigram maximum fifteen depending nature amount training material parameters model construction 
ph dissertation thomas niesler cambridge university june chapter summary page words may belong part speech categories account multiple grammatical functions 
calculating probability particular word sentence words seen language model take account possible category sequences correspond word history 
algorithm means may achieved variable length category model developed chapter 
algorithm consists recursive procedure maintains set possible category sequences word history associated probability 
probabilities employed assigning category classifications new text configuring language model tagger 
way words transcribed telephone conversations words wall street journal text assigned part speech tags making possible category language models trained corpora 
experimental evaluation shown tagger variable length category language model able deliver better results baseline system 
particularly significant improvements achieved tagging vocabulary words result proposed method category membership probability estimation cross validation 
algorithm developed selectively extend length individual grams led better results achieved commonly pruning methods count thresholds 
consequently variable length models outperform conventional fixed length approaches 
language model perplexities seen competitive sparse training sets larger ones large reductions number model parameters cases 
interpolated word trigram category model led word error rate improvement wall street journal recognition experiment best rescoring framework 

inclusion word grams language model purely category grams able capture relationships particular words categories words belong 
chapter detailed comparison conventional trigram variable length category language model shown gives better estimates word grams seen training set cases 
word grams effective means encoding short term relationships particular words category grams generalise unseen word sequences particularly appropriate backoff situations 
chapter develops technique allows word gram language model back category model chapter combining strengths approaches 
exact formulation require excessive number backoff weights due complex representation word history category model probability calculations 
approximate model developed continues employ category model backoff situations ensures correct normalisation approximating word model probabilities 
furthermore procedure proposed selects important word grams inclusion word model allowing number parameters traded modelling accuracy 
experiments show methods deliver greatly reduced perplexities sparse training sets significantly improved size versus performance tradeoffs compared standard trigram models large corpora 
recognition experiments performed best rescoring framework show proposed technique fares approximately linear interpolation category model baseline trigram 
ph dissertation thomas niesler cambridge university june chapter summary page 
inclusion long range word pair relations underlying assumption category model probability word depends category belongs occurrence equally point corpus category occurs 
factors topic style text cause certain words occur groups violating assumption 
inclusion word grams described previous section allows short term relationships taken account possible capture dependencies spanning words way 
view chapter presents technique means long range relationships may considered 
central technique definition distance words terms word categories 
empirical observations measure distance indicate conditional probability word category exhibits exponential decay constant maintaining uniform value normally assumed 
consequently functional dependence occurrence probability separation postulated 
methods developed determining related word pairs function parameters large corpus 
methods possible identify word pairs subjectively appear strongly related semantic content separated words text 
incorporation word pair relations category language model leads significant perplexity reduction sparse large corpora 
true employed word category backoff scheme demonstrating additional information captured word pairs 
interpolating category language model incorporating word pair dependencies baseline word trigram significant word error rate reduction achieved wall street journal recognition task employing best rescoring framework 

topics investigation results obtained techniques proposed thesis illustrated grammatical word category classifications may improve performance language models 
applies large bodies text especially small sparse corpora conventional word gram language models poorly trained 
particular category grams relations convey useful information captured standard word gram language models 
respect imposition classification scheme word patterns advocated section proved successful 
significant improvements obtained high performance baseline recognition system attention proposed approaches warranted 
developed techniques manner integrated system leave room refinement research 
proposals respect 

tagging nature category definitions accuracy words training corpus tagged key factors effectiveness category language modelling approach 
category language model tagger performs controlled tests particular attention tagging operation may worthwhile investigating methods means large quantities new text may tagged accurately greater detail 
related topic larger training corpora british national corpus possible obtain reliable tagging language model 
ph dissertation thomas niesler cambridge university june chapter summary page 
data driven refinement category definitions category definitions syntactic model fixed outset lexicon constructed training corpus information sources electronic dictionaries 
takes advantage priori grammatical knowledge clear fundamental way definitions necessarily optimal modelling task hand 
particular members category assignments general sufficient training data better predictive discrimination attained refined definitions 
research investigate automatic procedures determining refined categories 
allow extent model generalises balanced extent captures detailed patterns 
possible optimise language model performance accordance size character particular training corpora ultimate application defined controlled way 

punctuation grammatically motivated category definitions employed means capturing syntactic patterns text punctuation ignored generally included output speech recognition system 
punctuation marks grammatical function retaining improve grammatical consistency training material 
may benefit investigate methods allow category model include punctuation symbols possibly interaction features pitch pauses stress acoustic signal 

lattice rescoring recognition results best rescoring 
accuracy lists generally considerably lower lattices generated better allow lattices directly 
straightforward due complex representation document history employed category language model word pair relations 
suitable approximations possibly specialised search strategies need investigated 

final summary language modelling approach centred grammatical word categories proposed developed evaluated 
variable length grams categories employed capture general grammatical patterns grams words long range word pair correlations model semantic relationships particular words 
language modelling approach shown experimentally offer improved generalisation previously unseen word sequences employing fewer parameters offering better performance standard word gram techniques 
incorporated high performance baseline speech recognition system proposed language models led significant improvements word error rate 
ph dissertation thomas niesler cambridge university june page antoniol federico language model estimations representations real time continuous speech recognition proceedings international conference spoken language processing yokohama vol 
pp 

bahl jelinek mercer maximum likelihood approach continuous speech recognition ieee transactions pattern analysis machine intelligence vol 
march 
bahl brown de souza mercer tree statistical language model natural language speech recognition ieee transactions acoustics speech signal processing vol 
july 
berger brown della pietra della pietra lafferty mercer ures candide system fo machine translation proceedings human language technology workshop pp 
march 
breiman friedman stone classification regression trees wadsworth brooks cole 
brew thompson automatic evaluation computer generated text progress report project proceedings human language technology workshop pp 
march 
brown cocke della pietra della pietra jelinek lafferty mercer statistical approach machine translation computational linguistics vol 
pp 

brown de souza mercer della pietra lai class gram models natural language computational linguistics vol :10.1.1.13.9919

brown chen della pietra della pietra kehler mercer automatic speech recognition machine aided translation computer speech language vol 
pp 

users guide british national corpus oxford university computing services may 
byrne lm switchboard opening closing day reports language modelling workshop lm centre language speech processing john hopkins university baltimore july august 
chase rosenfeld ward error responsive modifications speech recognisers negative grams proceedings international conference spoken language processing yokohama pp 

church stochastic parts program noun phrase parser unrestricted text proceedings second conference applied natural language processing acl pp 

ph dissertation thomas niesler cambridge university june page church hanks word association norms mutual information lexicography computational linguistics vol 
pp 
march 
church gale comparison enhanced turing deleted estimation methods estimating probabilities english bigrams computer speech language vol 
pp 

clarkson robinson language model adaptation mixtures exponentially decaying cache proceedings international conference acoustics speech signal processing munich vol 
pp 
april 
cutting kupiec pedersen sibun practical part speech tagger proceedings third conference applied natural language processing acl pp 

dougherty probability statistics engineering computing physical sciences prentice hall englewood cliffs 
duda hart pattern classification scene analysis wiley new york 
tagger suite user manual may 
isabelle clustering words statistical language models contextual word similarity proceedings international conference acoustics speech signal processing atlanta vol 
pp 

federico bayesian estimation methods gram language model adaptation proceedings international conference spoken language processing philadelphia vol 
pp 

introducing linguistic constraints statistical language modelling proceedings international conference spoken language processing philadelphia vol 
pp 

ney wessel extensions absolute discounting language modelling proceedings eurospeech madrid pp 

language models spontaneous speech recognition bootstrap method learning phrase bigrams proceedings international conference spoken language processing yokohama pp 

population frequencies species estimation population parameters biometrika vol 
pp 

hull incorporating syntax visual text recognition statistical model ieee transactions pattern analysis machine intelligence vol 
pp 
december 
iyer ostendorf rohlicek language modelling sentence level mixtures proceedings arpa human language technology workshop princeton pp 

iyer ostendorf modelling long distance dependence language topic mixtures vs dynamic cache models proceedings international conference spoken language processing philadelphia vol 
pp 

adda automatic word classification simulated annealing language modelling proceedings international conference acoustics speech signal processing vol 
pp 

adda language modelling csr large corpus automatic classification words proceedings eurospeech berlin pp 

ph dissertation thomas niesler cambridge university june page class bigram model large corpus proceedings international conference spoken language processing yokohama vol pp 

multilingual stochastic gram class language models proceedings international conference acoustics speech signal processing atlanta vol pp 

jelinek mercer bahl maximum likelihood approach continuous speech recognition ieee transactions pattern analysis machine intelligence vol 
pp 
march 
jelinek mercer probability distribution estimation sparse data ibm technical disclosure bulletin 
jelinek development experimental discrete dictation recogniser proceedings ieee vol 
pp 
november 
jelinek trigrams struggle improved language models proceedings eurospeech genoa vol 
pp 

jelinek mercer roukos principles lexical language modelling speech recognition advances speech signal processing furui sondhi 
eds 
marcel dekker 
jelinek merialdo roukos strauss dynamic language model speech recognition proceedings darpa speech language workshop pp 
february 
johansson atwell garside leech tagged lob corpus user manual norwegian computing centre humanities bergen norway 
katz estimation probabilities sparse data language model component speech recogniser ieee transactions acoustics speech signal processing vol 
pp 
march 
kneser ney improved clustering techniques class statistical language modelling proceedings eurospeech berlin vol 
pp 

kneser personal communication 
kneser ney improved backing gram language modelling proceedings international conference acoustics speech signal processing detroit pp 

kuhn de mori cache natural language model speech recognition ieee transactions pattern analysis machine intelligence vol 
pp 
june 
corrected ieee transactions pattern analysis machine intelligence vol 
pp 

kukich techniques automatically correcting words text acm computing surveys vol 
december 
lau rosenfeld roukos trigger language models maximum entropy approach proceedings international conference acoustics speech signal processing vol 
pp 

martin ney algorithms bigram trigram clustering proceedings eurospeech madrid pp 

miller evaluation language model clustered model backoff proceedings international conference spoken language processing philadelphia pp 

ph dissertation thomas niesler cambridge university june page description computer usable dictionary file oxford advanced learner dictionary current english department computer science college university london june 
nadas estimation probabilities language model ibm speech recognition system ieee transactions acoustics speech signal processing vol 
pp 
august 
nadas turing formula word probabilities ieee transactions acoustics speech signal processing vol 
pp 
december 
ney essen kneser structuring probabilistic dependencies stochastic language modelling computer speech language vol 
pp 

ney personal communication 
ney essen kneser estimation small probabilities leaving ieee transactions pattern analysis machine intelligence vol 
december 
niesler woodland variable length category grams language modelling technical report cued infeng tr dept engineering university cambridge april :10.1.1.28.4818
niesler woodland variable length category gram language model proceedings ieee international conference acoustics speech signal processing atlanta vol 
pp 
april 
niesler woodland word category backoff language models technical report cued infeng tr department engineering university cambridge may 
niesler woodland comparative evaluation word category language models technical report cued infeng tr department engineering university cambridge july 
niesler woodland combination word category language models proceedings fourth international conference spoken language processing philadelphia vol 
pp 
october 
niesler woodland word pair relations category language models technical report cued infeng tr department engineering university cambridge february 
niesler woodland modelling word pair relations category language model proceedings international conference acoustics speech signal processing munich vol 
pp 
april 
nist national institute standards technology nist speech recognition scoring package score version available anonymous ftp jaguar ncsl nist gov pub score tar boyle owens smith weighted average gram model natural language computer speech language vol 
pp 

boyle ming mcmahon smith improving gram models incorporating enhanced distributions proceedings ieee international conference acoustics speech signal processing atlanta vol 
pp 
april 
ph dissertation thomas niesler cambridge university june page odell context large vocabulary speech recognition ph thesis department engineering university cambridge 
odell niesler lattice language modelling toolkit manual entropic cambridge research laboratories 
paul baker design wall street journal csr corpus proceedings international conference spoken language processing pp 

schwartz fung nguyen estimation powerful language models small large corpora proceedings international conference acoustics speech signal processing vol 
pg 

woodland large vocabulary speech recognition cache language model adaptation technical report cued infeng tr department engineering university cambridge january 
rabiner tutorial hidden markov models selected applications speech recognition proceedings ieee vol 
pp 
february 
rabiner juang fundamentals speech recognition prentice hall 
rao roukos language model adaptation minimum discrimination information proceedings international conference acoustics speech signal processing detroit pp 

reza information theory mcgraw hill 
ries waibel class phrase models language modelling proceedings international conference spoken language processing philadelphia pp 

riseman ehrich contextual word recognition binary diagrams ieee transactions computers vol 
pp 

robinson hochberg renals ipa improved phone modelling recurrent neural networks proceedings international conference acoustics speech signal processing adelaide vol 
pp 

rosenfeld huang improvements stochastic language modelling proceedings darpa speech natural language workshop new york pp 

rosenfeld adaptive statistical language modelling maximum entropy approach ph dissertation cmu cs school computer science carnegie mellon university april 
rosenfeld hybrid approach adaptive statistical language modelling proceedings arpa human language technology workshop princeton pp 

rosenfeld cmu statistical language modelling toolkit arpa csr evaluation arpa spoken language technology workshop austin texas january 
schwartz chow best algorithm efficient exact procedure finding sentence hypotheses proceedings international conference acoustics speech signal processing pp 
albuquerque april 
senior line cursive handwriting recognition recurrent neural networks ph thesis department engineering university cambridge 
random signals detection estimation data analysis wiley 
ph dissertation thomas niesler cambridge university june page shannon communication theory exposition fundamentals ire transactions information theory feb 
smadja mckeown translating collocations bilingual lexicons proceedings human language technology workshop pp 
march 
starner makhoul schwartz chou line cursive handwriting recognition speech recognition methods proceedings international conference acoustics speech signal processing adelaide vol 
pp 

suhm waibel better language models spontaneous speech proceedings international conference spoken language processing yokohama pp 

stochastic models language acquisition ph thesis university cambridge department engineering 
constructing linguistic oriented language models large vocabulary speech recognition proceedings eurospeech berlin vol 
pp 

woodland odell valtchev young htk large vocabulary speech recognition system proceedings international conference acoustics speech signal processing atlanta pp 

woodland gales valtchev htk large vocabulary recognition system arpa task proceedings arpa speech recognition workshop house new york 
wright jones lloyd thomas consolidated language model proceedings eurospeech berlin vol 
pp 

young jansen odell woodland htk book htk version cambridge university march 
kenny issues large scale statistical language modelling proceedings eurospeech berlin vol 
pp 

ph dissertation thomas niesler cambridge university june appendix major text corpora page appendix major text corpora statistical language models derive parameters large body example text referred training corpus alternatively training set 
table briefly describes corpora frequently encountered literature 
corpus name size words details ap ap general news 
ap atis air travel enquiries 
wsj press wall street journal 
nab superset wsj ap including press san jose mercury general business news material 
lob different text categories including press fiction scientific writing 
tagged part speech classifications verbmobil appointment negotiations german 
switchboard spontaneous telephone conversations concerning different predetermined topics 
ph dissertation thomas niesler cambridge university june appendix leaving cross validation page appendix leaving cross validation training set probability performance criterion testing quality statistical model may lead overfitting consequent poor generalisation 
techniques cross validation normally remedy employing heldout set reduces amount material available training purposes 
leaving cross validation approximate way modelling heldout set making maximum training data 
consider training set omega tot containing members divided subsets omega rt termed retained part omega ho termed heldout part 
cross validation approaches select models omega rt optimise model performance omega ho case probabilistic models may achieved maximising probability omega ho leaving method special case procedure omega ho chosen contain exactly member omega tot omega rt consists remaining gamma 
omega tot comprise events fx gamma drawn finite alphabet gamma na alphabet size 
denoting single member omega ho log probability heldout part may written ll gamma omega ho omega rt delta log omega rt probability estimate delta exclusively grounds data retained part omega rt leaving cross validation involves consideration possible ways omega tot may partitioned omega ho omega rt denote partitions formed assigning omega ho omega ho omega rt respectively gamma 
cumulative log probability partitions ll cum omega tot 
gamma log omega rt denote number occurrences omega tot tot rewrite ll cum omega tot nv gamma tot delta log omega rt additional assumptions regarding form omega rt may allow simplified 
ph dissertation thomas niesler cambridge university june appendix leaving cross validation page making possible subdivisions retained heldout parts leaving approach optimal available training data important consideration situations data sparse case language modelling problems 
drawback increased computation implied exhaustive partitioning operation efficient results may obtained simplifications possible specific forms omega rt 
ph dissertation thomas niesler cambridge university june appendix dealing unknown words page appendix dealing unknown words open vocabulary tasks test set generally contains words training corpus 
order process vocabulary simply oov words add dedicated entry labelled uw language model lexicon 
entry refers particular unknown word fact word category undetermined number members 
uw entry allow language model predict occurrence oov words employ part context probability estimates 
appendix describes incorporation uw entry category language model 
definition oov words encountered training employ leaving oneout cross validation framework 
particular estimate probability unknown word uw category lexicon 
total number words training corpus 
consider possible ways may split training corpus partitions rt ho contains gamma members second exactly gamma 
denote set categories word belongs training corpus define ffi gamma ho delta ho rt ho probability uw encountering unknown event category sub corpus rt size gamma may estimated relative frequency uw phi ffi gamma ho delta psi nc gamma ffi ho total number events seen category note numerator simply number events occur category occur entire corpus 
number unknown events uw may expected seen category assume vocabulary contains distinct words training corpus 
ph dissertation thomas niesler cambridge university june appendix dealing unknown words page sub corpus size gamma may estimated relative frequencies uw uw uw uw uw delta gamma uw equation may estimate count assigned uw entry category precautions taken training data certain categories sparse may happen numerator approaches equals denominator leading extremely large estimate uw 
order avoid probability estimate altered heuristically follows uw def nc gamma ffi ho constant ensures denominator larger numerator permitting uw 
small indicating category sparsely trained significant limiting effect uw 
increases significant estimate approaches 
intuitively quantity may interpreted indication number observations category relative frequency estimates confidence 
effect language model performance seen weak value yield satisfactory results lob corpus 
ph dissertation thomas niesler cambridge university june appendix experimental corpora baseline language models page appendix experimental corpora baseline language models 
different text corpora practical evaluation proposed language models described sections 
addition word bigram trigram models employing katz back conjunction turing discounting constructed corpus serve benchmarks experimental results 

lob corpus lancaster oslo bergen lob consists approximately words british english text drawn wide spectrum sources including press religious writing popular lore scientific matter fiction humorous writings 
word corpus tagged part ofspeech pos classification describes grammatical function sentence appears 
list classifications appendix 
preprocessing order ensure compatibility corpora preprocessing steps applied lob corpus ffl headings headings discarded corpus general grammatically correct sentences 
ffl numbers numerics corpus converted text instance converted 
ffl punctuation punctuation sentence start markers discarded language model 
ffl contractions contractions tagged separately lob corpus collapsed tagged single words pp bem 
done contractions pronunciations need treated distinct words language model appear single words corpora 
appendix describes individual contraction tagged 
ph dissertation thomas niesler cambridge university june appendix experimental corpora baseline language models page ffl de hyphenation hyphenated words treated separately convention switchboard wsj corpora 
reduces lexicon size oov rate hyphenated pair appear vocabulary constituent words 
common examples numbers 
removal hyphen leads untagged word 
remedied way 
untagged word belong category lexicon assigned tag 

remaining words assigned category statistical tagger trained part corpus intact 

corpus statistics preprocessed corpus allotted training corpus test set 
done evenly text categories assigning source files training set remainder test set division occurring closest sentence break 
table summarises important corpus statistics corpus training set test set total sentences total words vocabulary size lexicon size oov rate statistics lob corpus 

baseline word gram models word bigram trigram language models constructed training corpus serve benchmarks evaluations 
models employ katz back conjunction turing discounting 
bigram trigram unique bigrams unique trigrams total grams perplexity baseline word language models lob corpus 
lexicon refers collection unique word category pairs 
words may belong category lexicon entries vocabulary 
ph dissertation thomas niesler cambridge university june appendix experimental corpora baseline language models page 
switchboard corpus corpus consists transcribed spontaneous telephone conversations concerning predefined set topics language modelling workshop lm 
part speech information corpus 

preprocessing tagger lob corpus supply part speech information punctuation spelling conventions agree 
achieve steps taken ffl noises noise markers laughter removed 
ffl spelling conversion american british spelling effected maintain compatibility lob corpus consists british english 
example occurrences color replaced colour 
total changes automatically software developed part research 

corpus statistics switchboard dev test text test set closed vocabulary employed 
table summarises important corpus statistics training set test set total words total sentences vocabulary size lexicon size oov rate zero statistics switchboard corpus 

baseline word gram models word bigram trigram language models constructed training corpus serve benchmarks evaluations 
models employ katz back conjunction turing discounting 
bigram trigram unique bigrams unique trigrams total grams perplexity baseline word language models switchboard corpus 
ph dissertation thomas niesler cambridge university june appendix experimental corpora baseline language models page 
wall street journal wsj corpus wsj corpus contains newspaper text collected wall street journal period inclusive 
considerably sized body text findings expected hold larger corpora nab state art speech recognition systems 

preprocessing pronunciation processed version wsj corpus build language models 
filters vp svp sgml text employed obtain plain text output steps preprocessing carried prior model construction ffl trailing periods remove periods ends words convention lob switchboard corpora 
ffl typographical errors correct common misspellings wsj corpus frequently misspelled 
total corrections type 
ffl spelling american british spelling conversion effected maintain compatibility lob corpus 
total corrections automatically software developed part research 

corpus statistics approximately words standard wsj dev test text years employed test set subjected preprocessing steps training corpus 
vocabulary darpa evaluation 
table summarises important corpus statistics training set test set total words total sentences different words vocabulary size lexicon size oov rate statistics wsj corpus 
filters part cmu language modelling toolkit 
ph dissertation thomas niesler cambridge university june appendix experimental corpora baseline language models page 
baseline word gram models word bigram trigram language models constructed training corpus serve benchmarks evaluations 
models employ katz back conjunction turing discounting 
bigram trigram unique bigrams unique trigrams total grams perplexity baseline word language models wsj corpus 
ph dissertation thomas niesler cambridge university june appendix lob corpus word categories page appendix lob corpus word categories 
part speech tags lob corpus table lists grammatical word classifications lob corpus 
punctuation symbols omitted tag se referring sentence marker added 
category name description fw foreign word abl pre quantifier quite abn pre quantifier half abx pre quantifier double conjunction ap post determiner fewer little ap aps aps singular article ati singular plural article bed beg bem am ben ber re cc coordinating conjunction cd cardinal dozen zero cd cardinal genitive cd cd hyphenated pair cardinals cd cd cd ones cds cardinal plural tens millions dozens cs subordinating conjunction ph dissertation thomas niesler cambridge university june appendix lob corpus word categories page category name description dod dt singular determiner dt singular determiner genitive dti singular plural determiner dts plural determiner determiner double conjunction ex existential hv having past participle preposition jj adjective jjb attributive adjective chief entire main jjr comparative adjective superlative adjective adjective word initial capital english german md modal auxiliary ll nc cited word nn singular common noun nn singular common noun genitive nnp singular common noun word initial capital german nnp singular common noun word initial capital genitive plural common noun word initial capital plural common noun word initial capital genitive nns plural common noun nns plural common noun genitive abbreviated unit measurement unmarked number abbreviated plural unit measurement np singular proper noun np singular proper noun genitive npl singular locative noun word initial capital bridge npl singular locative noun word initial capital genitive plural locative noun word initial capital plural locative noun word initial capital genitive nps plural proper noun nps plural proper noun genitive npt singular noun word initial capital captain npt singular noun word initial capital genitive plural noun word initial capital plural noun word initial capital genitive nr singular adverbial noun january february sunday monday east west today tomorrow tonight downtown home nr singular adverbial noun genitive nrs plural adverbial noun nrs plural adverbial noun genitive od ordinal st od ordinal genitive ph dissertation thomas niesler cambridge university june appendix lob corpus word categories page category name description pn nominal pronoun anybody everybody somebody pn nominal pronoun genitive pp possessive determiner pp possessive pronoun mine pp personal pronoun st person singular nominative pp personal pronoun st person plural nominative pp personal pronoun st person singular accusative pp os personal pronoun st person plural accusative pp personal pronoun nd person thou thee ye pp personal pronoun rd person singular nominative accusative pp personal pronoun rd person singular nominative pp personal pronoun rd person plural nominative pp personal pronoun rd person singular accusative pp os personal pronoun rd person plural accusative em ppl singular reflexive pronoun plural reflexive pronoun reciprocal pronoun ql qualifier post quantifier rb adverb rb adverb genitive rbr comparative adverb rbt superlative adverb ri adverb homograph preposition near rn nominal adverb thee rp adverbial particle back se marker infinitival uh interjection vb base form verb vbd past tense verb vbg participle gerund vbn past participle vbz rd person singular verb wdt wh determiner whatsoever interrogative whichever wh determiner relative wp wh pronoun interrogative nominative accusative wp wh pronoun interrogative gen wp wh pronoun relative gen wpa wh pronoun nominative wh pronoun interrogative accusative wh pronoun relative accusative wh pronoun relative nominative accusative relative wh adverb zz letter alphabet pi ph dissertation thomas niesler cambridge university june appendix lob corpus word categories page 
part speech tag assignments contraction pos tag contraction pos tag ll ll ll ll ll ll ll ll ll ll ll re re re re re ain isn aren wasn weren hadn hasn couldn didn doesn don haven needn shan shouldn won wouldn notes ffl tags contractions chosen part speech assignments constituent words 
ffl contractions may tags depending contraction md 
ffl contractions may tags depending contraction 
ph dissertation thomas niesler cambridge university june appendix tag mappings page appendix tag mappings electronic version oxford advanced learner dictionary containing sufficiently detailed word tagging information augment lexicon extracted lob corpus purpose reducing oov rate 
table lists tags process lob tags mapped 
cases necessary map tag lob tag indicated separating colons 
tag lob tag tag lob tag gb vbg ki nn gc vbd kj nns gd vbn nn ha vbz nn hb vbg nn hc vbd nn nns hd vbn lk nn vb nn vb mi nn vb mj nns vb nn vb nn vb nn ia vbz nn nns ib vbg nn ic vbd nl np id vbn nm np vb nn np vb np vb oa jj vb ob jj vb oc jj vb od jj ja vbz oe jj jb vbg op jj jc vbd oq jjb jd vbn jjr vb os vb ot jj vb pu rb vb rp vb vb uh ph dissertation thomas niesler cambridge university june appendix trigger approximations page appendix trigger approximations virtue category conditional probability function chosen model occurrence correlations trigger target words distribution function delta gamma gamma gamma gamma fl delta gammaae deltai delta fl delta gammaae deltad ffl distance separating trigger target 
ffl probability occurrence trigger 
ffl fl ae parameters describing transient occurrence probability target respect trigger sighting 
ffl normalising constant 
appendix describes stage algebraic approximation distribution form delta ffl delta gamma delta ffl delta gamma delta 
approximation objective eliminate product operator equation presence algebraic manipulation difficult 
consider term right hand side gamma gamma gamma gamma fl delta gammaae deltai gamma gamma delta gamma gamma delta gammaae deltai fl gamma gamma ph dissertation thomas niesler cambridge university june appendix trigger approximations page take logarithm apply order taylor approximation ln find ln gamma gamma gamma gamma fl delta gammaae deltai delta ln gamma gamma gamma gamma delta gammaae deltai delta ln gamma gamma gamma gamma gammaae deltad gamma gammaae take inverse logarithm obtain gamma gamma gamma gamma fl delta gammaae deltai gamma gamma delta gamma fl delta gammae gammaae deltad gammap gammap delta gammae gammaae clarity define psi gamma fl gammap gammap delta gammae gammaae follows delta fl delta gammaae deltad delta gamma gamma delta psi gammae gammaae deltad approximation fl gammap gammap true fl may expected content words 

second approximation ultimately find closed form expressions approximate mean probability distribution possible approximate delta ffl delta gamma delta ffl delta gamma delta ffl ffl functional form motivations ffl superposition geometric terms retains geometric character exhibited empirically distribution 
ffl faster geometric component model initially rapid decay observed distribution turn due higher conditional probability small 
ffl slower geometric component model tail observed distribution 
ffl closed form expressions mean mean square exist 
ph dissertation thomas niesler cambridge university june appendix trigger approximations page note firstly equation represents valid probability mass function 
order solve parameters terms parameters impose constraints 
equality limit find lim delta delta gamma gamma delta psi assuming lim delta ffl delta gamma delta requiring lim lim may choose ffl delta psi 
equality find delta fl ffl delta ffl delta find fl gamma ffl delta ffl ph dissertation thomas niesler cambridge university june appendix trigger approximations page 
equality derivative find gamma ae delta fl delta ln gamma gamma delta fl delta ln psi delta ae delta delta fl delta delta gamma gamma delta psi gammae obtain fi fi fi fi delta gamma ae delta fl fl delta ln gamma gamma ae delta fl delta ln psi similarly delta ffl delta ln gamma delta gamma delta ffl delta ln gamma delta gamma delta obtain fi fi fi fi delta ffl delta ln gamma delta ffl delta ln gamma delta may write ffl delta ln gamma delta delta psi delta ln gamma delta delta psi delta ln gamma requiring fi fi fi fi fi fi fi fi find ffl delta ln gamma delta ln gamma delta fl gamma delta psi ae delta fl delta ln psi gamma fl fl gamma delta psi delta ln gamma ln gamma delta fl gamma delta psi ae delta fl delta ln psi gamma fl ln gamma ln gamma delta fl gamma delta psi ae delta fl delta ln psi gamma fl fl gamma delta psi ln gamma ae delta fl delta ln psi fl gamma delta psi obtain gamma gamma delta ae delta fl psi gammafl fl gammap delta psi ph dissertation thomas niesler cambridge university june appendix truncated geometric distribution page appendix truncated geometric distribution consider experiment consisting bernoulli trials probability success number repetitions witnessing positive result described geometric distribution gamma delta select subset trials integer greater zero 
probability distribution interval may determined applying normalisation requirement gamma equation obtaining gamma gamma delta gamma gamma gamma appendix find expressions mean mean square truncated geometric distribution calculating moment generating function gamma deltad delta gamma gamma delta gamma gamma delta deltad delta gamma delta gamma gamma gamma delta gamma delta gamma upsilon delta gamma delta gamma gamma delta gamma ph dissertation thomas niesler cambridge university june appendix truncated geometric distribution page upsilon gamma gamma derivative respect find upsilon delta delta gamma delta gamma deltat delta gamma gamma delta nt delta gamma delta gamma delta gamma gamma delta gamma upsilon delta delta gamma gamma delta deltat delta gamma gamma delta delta gamma gamma delta gamma obtain mean set fi fi fi fi upsilon delta gamma delta gamma gamma gamma delta gamma gamma second derivative respect find md upsilon delta gamma gamma gamma px delta delta theta gamma px gamma nt gamma px gamma gamma px gamma gamma px delta upsilon delta theta gamma px gamma delta nt gamma px gamma delta delta gamma px delta gamma gamma gamma delta delta gamma px gamma gamma px obtain mean square distribution setting fi fi fi fi fi upsilon delta delta gamma delta gamma delta gamma gamma gamma delta delta gamma delta upsilon delta gamma delta gamma delta gamma gamma gamma delta gamma delta delta gamma ph dissertation thomas niesler cambridge university june 
