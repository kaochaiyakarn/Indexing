inference hybrid networks theoretical limits practical algorithms uri lerner computer science department stanford university uri cs stanford edu ronald parr computer science department duke university parr cs duke edu important subclass hybrid bayesian networks represent conditional linear gaussian clg distributions distribution multivariate gaussian component instantiation discrete variables 
explore problem inference clgs provide complexity results important class clgs includes switching kalman filters 
particular prove clg restricted extremely simple structure polytree inference task np hard 
furthermore show np approximate inference simple networks intractable 
prohibitive computational cost approximate inference take advantage special domain properties may enable efficient inference 
concentrate fault diagnosis domain explore approximate inference algorithms 
algorithms try find small subset gaussians approximation full mixture distribution 
consider monte carlo approaches novel approach enumerates mixture components order prior probability 
compare methods variety problems show novel algorithm promising large hybrid diagnosis problems 
bayesian networks useful modeling language complex stochastic domains 
lately growing interest hybrid models contain discrete continuous variables 
important class hybrid models conditional linear gaussian clg bayesian networks 
models conditional distribution continuous variables discrete ones multivariate gaussian 
clg models popular variety applications static dynamic settings 
example applications include target tracking continuous variables represent state targets discrete variables model maneuver type visual tracking continuous variables represent head legs torso position person discrete variables type movement fault diagnosis discrete events affect continuous process speech recognition ch discrete phoneme determines distribution acoustic signal 
part deals complexity inference clg models 
clg models commonly surprisingly little formal done analyzing complexity 
purposes analysis assume working finite precision continuous variables need answer queries involving discrete variables 
interested solving questions form evidence probability distribution discrete variable phrased decision problem evidence probability discrete variable takes value range 
obviously clgs generalization discrete bayesian networks difficult 
obvious network structures easy discrete case remain easy clgs 
prove case network structures inference discrete case easy inference clgs np hard 
particular consider restricted class clg models network structure polytree continuous variable binary discrete ancestor prove extraordinarily simple case inference np hard 
establishing exact inference np hard simple networks consider question approximate inference 
prove np exist polynomial time approximate inference algorithm absolute error smaller 
class networks consider include switching kalman filters special case provide formal complexity results important class models :10.1.1.34.4936
second part addresses question perform inference clg models light complexity results 
commonly approach clg models algorithm proposed lauritzen extension standard clique tree algorithm 
surprisingly inference clgs np hard simple networks size resulting clique tree exponential leading unacceptable performance 
domains reasonable expect exact answer require exponential number gaussians possibly assignment discrete variables small subset gaussians sufficient produce reasonable approximation 
shall discuss believe case surprisingly large number applications 
course difficult part efficiently generating relatively small set 
consider monte carlo approaches stochastic sampling likelihood weighting mcmc 
consider novel approach generates gaussians order prior likelihood 
discuss advantages disadvantages approaches empirical results synthetic bayesian network 
test algorithm real life domain apply techniques problem tracking dynamic bayesian networks fault diagnosis domain 
need track state system discrete events fault happened hidden 
classical algorithms problem assume small number possible discrete events time step scale large systems 
show techniques combination techniques described circumvent difficulty leading practical algorithm complex hybrid dynamic systems 
preliminaries bayesian networks bns compact graphical representation probability distributions 
xn set random variables takes values domain dom 
graphical model xn consists components directed acyclic graph nodes correspond random variables xn set conditional probability distributions cpds parents 
structure network encodes set conditional independence assumptions cpds uniquely define joint distribution xn delta denote discrete variables model gamma denote continuous ones 
semantics allows type cpd involving discrete continuous variables 
particularly important subclass hybrid bayesian networks conditional linear gaussian clg models 
clg model discrete node continuous parents 
furthermore cpd continuous node conditional linear gaussian combination discrete parents node weighted linear sum continuous parents gaussian noise 
formally node continuous parents fy discrete parents fd define cpd parameters dom ad ad oe cpd defined normal ad ad oe easily shown clg model represents joint distribution multivariate gaussian continuous variables instantiation discrete variables 
conversely distribution represented clg model 
call possible discrete instantiations resulting gaussian hypothesis 
note general number hypotheses exponential number discrete variables 
np hardness simple clgs simplest class discrete bayesian networks polytrees inference done linear time 
important ask perform inference efficiently polytree clgs 
section prove np answer 
stated consider queries discrete variable evidence 
goal analyze polytree clgs continuous variable discrete ancestor start simpler case theorem inference clg models binary discrete variables polytree graphical structure np hard 
furthermore np exist polynomial approximate inference algorithm absolute error smaller 
proof consider np complete subset sum problem 
set fs ng element non negative integer positive integer question exists subset sum elements exactly reduce problem polytree clg model shown fig 

discrete variables shown squares binary uniform prior distribution 
cpds continuous variables ae normal oe normal oe ae normal gamma oe normal gamma oe ae normal gamma normal xn oe networks np hardness reductions choose oe cn constant simply assume 
prove exists subset elements sum iff jy 
uniform jy iff jy jy ljb ljb compute ljb ljb exp gamma gamma gamma ha assignment ha 
easy show ja normal koe direction assume elements sum show jy 
define iff clearly ljb normal oe 
ljb jb jb ljb delta oe cn ljb ljb cn conversely assume exist show jy 
exist get integer different jl gamma 
ljb jb ljb delta oe exp gamma gamma oe cn exp gamma oe cn ljb ljb cn cn gamma explain constant making big jy gets arbitrarily close exists arbitrarily close 
approximation algorithm absolute error ffl answer decision problem construct problem instance jy gammaffl jy ffl answer iff algorithm answers jy 
np exist polynomial time approximate inference algorithm absolute error smaller 
theorem inference clg models binary discrete variables polytree graphical structure np hard continuous variable discrete ancestor 
furthermore np exist polynomial approximate inference algorithm absolute error smaller 
proof reduction similar previous proof different network structure 
structure shown fig 

discrete variables binary uniform distribution normal oe normal oe ae normal gamma gamma oe normal gamma gamma gamma oe ae normal gamma normal xn oe query notation 
show choose oe 
intuitively oe big representing weak prior oe small 
evidence distribution close normal gamma koe normal gamma koe depending value get exactly distribution situation theorem proof 
get exactly desired distribution get close 
get exactly desired variance ffl away mean 
ffl choose oe ffl delta oe oe oe oe induction prove ja normal koe gamma situation theorem 
xn ja having distribution normal noe distribution normal noe gamma ffl 
choose ffl oe easily modify inequalities theorem prove desired result 
interesting note fact exact inference polytree clgs np hard surprising 
possibly popular clg models switching kalman filters see models date literature survey 
unroll time steps switching kalman filter get exactly network fig 

continuous variable chain discrete variables ancestors distribution mixture exponentially gaussians 
reasonable assume inference case intractable formal proof known 
results go giving formal proof intuition 
concentrate queries involving just discrete variables posterior distribution easy represent infer 
second theorem prior distribution continuous variable gaussian mixture gaussians 
somewhat surprising seemingly benign structure continuous variables produce np hard decision problem 
important obvious fact simplest type approximate inference absolute error smaller intractable 
corollary finding instantiation discrete variables evidence np hard 
proof direct result reductions known instantiation easy determine evidence 
conclude technical issue interest complexity theory point view 
subset sum pseudo polynomial possible results hold parameters polynomial show case subset product problem problem sp np complete strong sense 
set ft ng number set elements positive integers 
question exists subset product elements exactly main idea reduction theorem 
set log log question find subset sum exactly note integers instance subset sum problem 
technical issues need addressed 
numbers rational sure represent precision requires polynomial space 
second difference fixed quite small 
particular subset sum get close situation difference subset sum 
fortunately difference bounded polynomial modify proof theorem case means polynomials log variances polynomials exact proof provide insights clgs omit 
approximate inference algorithms domain properties inference simple clgs np hard approximate inference tractable conclude inference clg models lost cause 
fortunately real life domains special features exploited fast algorithms 
concentrate fault diagnosis domain 
domain case relatively small subset mixture approximation entire mixture 
may need deal exponential number hypotheses answer query exactly get approximations find small subset approximates mixture 
unfortunately problem np hard 
fortunately problem finding hypotheses simpler fault diagnosis domain 
probability failures relatively small know priori hypotheses correspond small numbers faults 
note situation unique fault diagnosis domain 
example visual tracking past evidence hypotheses people having conversation person start running 
consider monte carlo approaches problem sampling mcmc 
turns sampling mcmc difficulty fault diagnosis faults generally quite sampled frequently 
overcome difficulty consider novel algorithm uses low probability faults advantage algorithm tries find approximation full set hypotheses considering hypotheses priori hypotheses small number failures failures tend happen 
idea concentrating hypotheses small number faults natural probabilistic model discrete strong independence assumptions 
monte carlo methods presenting naive algorithm enumerates hypotheses resulting mixture gaussians 
show sampling incremental algorithm viewed approximations naive algorithm 
formally consider general query form delta gamma delta discrete gamma continuous 
answering query directly easier compute delta gamma 
result mixture gaussians fq gamma xg combination delta get answer original query conditioning gaussian mixture evidence adjusting weight mixture component accordingly multiply weight probability multivariate gaussian 
order compute delta gamma compute delta gamma possible value delta delta explicitly enumerating possible instantiations ffiffiffi delta 
note instantiation ffiffiffi defines single multivariate gaussian distribution gamma delta gamma ffiffiffi dom delta ffiffiffi delta gamma ffiffiffi ffiffiffi dom delta delta ffiffiffi ffiffiffi gamma ffiffiffi delta ffiffiffi means delta agrees ffiffiffi values variables share 
resulting expression easy compute ffiffiffi involves discrete inference gamma ffiffiffi multivariate gaussian 
problem course summation discrete combinations 
observe need sum discrete variables network 
sum discrete variables determine distribution continuous variables direct parents continuous nodes note delta dp assume ffiffiffi ranges delta def delta delta dp summation simply perform different discrete inference ffiffiffi 
case summing just delta infeasible 
methods attempt find small subset gaussians approximates full sum 
method sampling probability distribution delta sample assignments delta samples subset hypotheses 
create clique tree discrete variables set evidence sample tree ignoring delta view method static version rao blackwellized particle filtering rbpf sample discrete variables solve remaining continuous problem analytically 
sampling method runs problems prior probability mass small number hypotheses dominates rest 
typical fault diagnosis problems reasonably reliable systems prior probability fault hypothesis bigger 
case waste computational resources generating duplicate samples exploring hypotheses 
systems approach generate fault hypothesis reasonable number samples 
means system fail precisely needed fault occurred 
problem approach sampling generates hypotheses delta ideally wish generate hypotheses posterior distribution delta generate hypotheses posteriori 
samples hypotheses prior sampling mcmc mixture components prior mcmc sampling results subset sum problem results unrolled tank network alternative approach try sampling hypotheses true posterior distribution mcmc approach gibbs sampling 
need find conditional distribution discrete variable rest variables delta continuous evidence 
assignment delta assignment delta need compute delta delta xjd delta xj delta term involves discrete inference second involves just gaussian defined delta 
follows transition probabilities computed efficiently 
gibbs sampling generate samples correct posterior distribution delta samples subset hypotheses 
generating order prior likelihood different approach problem concentrated prior probability generate hypotheses deterministically decreasing order probability delta duplicates 
method algorithm uses run time consider distinct hypotheses possible 
note ignore continuous variables bn purposes enumeration care discrete problem enumerating delta 
key subroutine method algorithm enumerating configurations discrete bn evidence 
algorithm generates clique tree uses generate configurations anytime fashion 
complexity algorithm jj kn kc log kc jj size clique tree number table entries number cliques number variables 
difficulty subroutine fact want enumerate instantiations delta discrete variables 
need create clique tree just delta way run variable elimination eliminating variables delta build clique tree remaining factors 
done network assume structure network simple operation relatively cheap 
tree delta large possible fall back full clique tree enumerate full instantiations delta 
approximate inference discussion conclude general comments 
decide hypotheses representative subset need determine weights 
way compute discrete likelihood hypothesis multiply continuous evidence likelihood normalize ignoring duplicate hypotheses 
way possible sampling mcmc approaches give weights number times hypothesis sampled multiplied continuous evidence likelihood 
unbiased reduces variance removes randomness associated sampling 
experiments show setting weights likelihoods works practice 
important issue bounding error approximation 
possible approach bound probability mass hypotheses generated 
mass sum ffiffiffi ffiffiffi ffiffiffi ffiffiffi generated 
bound sum bound densities ffiffiffi may bigger 
easy bound densities network polytree just variable greedily minimizing variance continuous variable topological order 
network polytree variable consider covariances variables problem difficult minimizing variance single query variable discrete instantiations general network structures np hard 
hope address problem 
tank tank tank tank tank measurement subsystem subsystem subsystem subsystem subsystem tank system dbn model tanks far assumed algorithm returns mixture gaussians instantiation delta relax assumption letting algorithm return single gaussian instantiation delta gaussian second order moments mixture similar lauritzen algorithm 
case need keep gaussians memory generating gaussian conditioning evidence collapse previous gaussians discard reducing space complexity algorithm 
assumption generate hypotheses assignment delta alternatively enumerate hypotheses globally initially ignoring delta estimate probabilityof variables hypotheses generated 
approach guarantees get hypotheses assignment delta may efficient assignments extremely 
conclude comparing algorithm lauritzen algorithm 
simple networks algorithm enumerate hypotheses complexity lauritzen algorithm 
cases lauritzen algorithm usually preferable leads efficient implementation 
large networks algorithm important advantages 
anytime algorithm give answer albeit approximate request 
second advantage space complexity lauritzen algorithm performs strong triangulation forces direct parents clique leading exponentially sized cliques simple networks networks fig 

contrast storage requirements dictated size query exponentially smaller 
results static networks began testing algorithms network type theorem 
considered subset sum problem binary variables picked value evidence corresponded valid subset sum queried jy 
results shown fig 

note correct value extremely close 
monte carlo algorithms averaged runs plot enumeration algorithm computed analytically 
recall discrete hypotheses probability model actual time correct hypothesis generated uniformly distributed number hypotheses 
compute expectation number samples required find correct hypothesis performed runs confirm algorithm worked correctly 
main draw experiments uniformly discrete instantiations enumeration expensive preferable sampling 
note results function number samples generated algorithms spent time generating gaussians corresponding discrete assignment discrete assignment get similar graph axis represents cpu time 
considered variant model closely related diagnosis domain 
uniform prior gave discrete probability considered corresponded events sum fault diagnosis domain correspond simultaneous faults event 
may lifetime real system cycle time order tenths second 
show performance graph problem sampling mcmc able find reasonable hypotheses samples 
enumeration prior correct hypothesis concluded jy considering hypotheses 
fault probabilities impose partial ordering hypotheses failures hypotheses elements order 
considered additional evidence help monte carlo methods catch ation algorithm 
allowed sampling mcmc observe variable selected value partitioned problem halves containing single subset element 
help sampling mcmc estimated jy samples slight improvement 
enumeration prior take continuous evidence account generating hypotheses performance unaffected 
application dynamic systems hoped evaluate algorithms realistic large static clg bayesian network difficulty finding network lack efficient inference algorithms 
turned dynamic systems generate large realistic static bns 
dynamic bayesian networks dbns extension bayesian networks dynamic systems 
dbn time partitioned regular time slices 
state time slice value random variables transition model representing observation model representing represented bn fragment called tbn 
create large static bayesian networks dbn considering result unrolling network time steps 
dynamic systems natural important domains clg models applied problems visual tracking fault diagnosis 
evaluate performance sampling mcmc enumeration prior applied diagnosis problem shown fig 

physical system consists water tanks connected pipes includes measurements flows pipes 
continuous variables represent flows pressures conductances flow measurements various pipes 
discrete variables represent various failure modes 
fig 
shows dbn model system due space considerations dbn describes system water tanks 
unrolled dbn time slices created scenario failures burst pipe tanks burst pipe tanks 
fig 
shows estimate probability pipes burst measurements time slices 
correct value close sampling mcmc samples 
enumeration prior guaranteed find enumerating hypotheses 
actual physical system non linear relation pressures flows described clg 
deal problem linear approximation modeling flow product pressure conductance 
approximation appropriate slow flows 
plan better approximations purposes testing algorithm simulated data physical accuracy model issue treat simply clg 
final test algorithm turn attention inference actual dbns unrolled static networks 
common inference task dbn track state system evolves sequence observations precisely goal maintain belief state distribution 
belief state time compute new belief state time performing inference tbn treated static network 
system large keeping gaussian possible combination discrete variables belief state may expensive 
algorithm integrated effectively bk algorithm adopted hybrid systems 
key idea exploit fact large systems composed subsystems subsystems correlated interaction strong 
bk algorithm approximates true belief state separate belief states subsystems 
plug belief states dbn inference find belief states time accounting correlations subsystems 
easily apply inference algorithm task 
modified algorithm described enumerate hypotheses prior distribution 
systems interesting track state continuous variables time results fig 

picked challenging sequence events conductance pipe tanks starts negative drift simultaneous measurement failures flow tanks flow tank bursts starts negative drift bursts bursts 
track show belief state ground truth continuous variables pressure tank 
considered probability discrete variables tracked true events correctly show due space considerations 
interesting somewhat unfair compare algorithm omniscient kalman filter 
omniscient kalman filter fig 
knows value discrete variable time step needs track continuous state 
clearly better job tracking system method imperfect partial information 
algorithm mirrors gold standard closely reflecting success algorithm finding correct hypotheses 
discussion proven simple clg models challenging 
provided np hardness results simple class clg models showed np efficient approximate infer time steps truth belief truth belief truth belief time steps truth belief truth belief truth belief runs difficult trajectory tank system algorithm omniscient kalman filter 
ence algorithm cases 
immediate provided complexity results important class switching kalman filters 
stress results imply clgs useful suggest order efficiently assumptions observations inference approximate inference tractable small number hypotheses fault diagnosis representing small number faults faults tend happen simultaneously feel characterizing sub classes clgs open problem requiring 
compared approximate inference algorithms advantages exact inference 
anytime nature reduced space complexity 
novel algorithm takes advantage partial ordering combinations faults imposed fault diagnosis problems generating hypotheses decreasing order likelihood 
incremental algorithm superior performance sampling mcmc type evidence greatest importance fault diagnosis domains 
inference continuous variables emphasis superior tracking discrete variables leads superior tracking continuous variables 
optimistic incremental algorithm generating hypotheses combined architecture provide basis efficient robust tracking diagnosis large systems practical interest 
acknowledgments 
pleased daphne koller carlos guestrin simon tong useful discussions 
research supported onr young investigator number onr muri program decision making uncertainty number 
bar shalom fortmann 
tracking data association 
academic press 
boyen koller 
tractable inference complex stochastic processes 
proc 
uai pages 
johan de kleer brian williams 
diagnosing multiple faults 
matthew ginsberg editor readings nonmonotonic reasoning pages 
morgan kaufmann los altos california 
doucet godsill andrieu 
sequential monte carlo sampling methods bayesian filtering 
statistics computing 
michael garey david johnson 
computers intractability 
freeman freeman ad 
zoubin ghahramani geoffrey hinton 
variational learning switching state space models 
neural computation 
jelinek 
statistical methods speech recognition 
mit press cambridge ma 
lauritzen 
propagation probabilities means variances mixed graphical association models 
jasa 
lauritzen jensen 
stable local conditional gaussian distributions 
technical report dept math 
sciences aalborg univ 
lerner parr koller biswas 
bayesian fault detection diagnosis dynamic systems 
proc 
aaai pages 
neal 
probabilistic inference markov chain monte carlo methods 
technical report crg tr university toronto 
nilsson 
efficient algorithm finding probable configurations probabilistic expert systems 
statistics computing 
pavlovic rehg 
cham murphy 
dynamic bayesian network approach tracking learned dynamical models 
proc 
iccv 
