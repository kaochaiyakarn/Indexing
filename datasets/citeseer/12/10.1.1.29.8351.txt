induction dynamical recognizers jordan pollack laboratory ai research computer information science department ohio state university neil avenue columbus oh pollack cis ohio state edu higher order recurrent neural network architecture learns recognize generate languages trained categorized exemplars 
studying networks perspective dynamical systems yields interesting discoveries longitudinal examination learning process illustrates new form mechanical inference induction phase transition 
small weight adjustment causes bifurcation limit behavior network 
phase transition corresponds onset network capacity generalizing arbitrary length strings 
second study automata resulting acquisition previously published training sets indicates architecture guaranteed find minimal finite automaton consistent exemplars np hard problem architecture appear capable generating non regular languages exploiting fractal chaotic dynamics 
hypothesis relating linguistic generative capacity behavioral regimes non linear dynamical systems 
table accept reject pollack 
consider categories binary strings table 
brief study human machine learner decide characterize accept strings containing odd number reject strings containing number language acquisition problem long time 
narrowest formulation version inductive inference theory data problem syntax discover compact mathematical description string acceptability generalizes finite presentation examples 
broadest formulation involves accounting linguistic facts native language acquisition human children acquisition language homo sapiens natural selection lieberman pinker bloom problem specialized scientific disciplines voluminous literature 
mathematical computational theorists concerned basic questions definitions language learning gold understanding complexity problem angluin gold algorithms berwick rivest schapire 
excellent survey approach problem written angluin smith 
linguists concerned grammatical frameworks adequately explain basic fact children acquire language chomsky wexler psychologists concerned detail acquisition mechanism predicts empirically testable phenomena child language acquisition 
macwhinney pinker 
goals limited best algorithm precise psychological model fact avoid strong claims algorithmic efficiency neural psychological plausibility initial 
take central research question connectionism neural computational system slowly changing structure numeric calculations iterative processes come possess linguistic generative capacity require dynamic representations symbolic computation recursive processes 
rigorous theory may take time develop report address question 
expose recurrent higher order back propagation network positive negative examples boolean strings find dynamical recognizers network converge minimal description finite state automaton data np hard induction novel interesting fashion searches hypothesis space theoretically constrained machines finite state 
results import related neural models currently development 
elman giles servan schreiber ultimately relates question linguistic capacity arise nature 
necessity terminology non linear dynamical systems remainder article 
terminology common language computer cognitive scientists warrants 
view neural networks non linear dynamical systems commonly held physicists helped define modern field neural networks hopfield smolensky complex dynamics generally suppressed favor tractable convergence limit point dynamics 
chaotic behavior shown repeatedly studies neural networks meir huberman hogg kolen pollack van der scientists begun explore dynamical complexity exploited useful purposes :10.1.1.30.6437
pollack freeman 
short discrete dynamical system just iterative computation 
starting initial condition state state computed mathematical function current state involving parameters input noise environment 
studying function computations field concerned explaining universal temporal behaviors 
iterative systems interesting properties behavior limit reaches steady state limit point oscillation limit cycle aperiodic instability chaos 
terms computer programs regimes correspond respectively programs halt simple repetitive loops creative infinite loops broken self modifying codes area mechanical behavior extensively studied 
state spaces dynamical systems plotted regimes characteristic figures called attractors limit points show point attractors limit cycles periodic attractors chaos strange attractors usually fractal nature 
small changes pollack controlling parameters lead phase transitions qualitatively different behavioral regimes bifurcation change periodicity limit behavior system route steady state periodic aperiodic behavior follows universal pattern 
characteristics chaotic systems sensitive initial conditions slight change initial condition lead radically different outcomes 
details articles books field 
crutchfield devaney 
automata recurrent networks dynamical recognizers clear outset problem inducing recognizer finite set examples easy infinite number regular languages account finite sample infinite number automata language 
difficult problem finding minimal description solution asymptotically better learning enumeration brute force searching automata order ascending complexity 
difficult issue determination grammatical class 
finite set examples give clue complexity class source language apparently find parsimonious regular grammar context free grammar context sensitive grammar compare 
quite formidable challenge problem solver 
language acquisition done inductive bias grammatical framework hypothesis space 
attacked problem inducing finite state recognizers regular languages feldman tomita 
finite state recognizer quadruple set states denotes initial state finite input alphabet transition function set final accepting states subset string accepted device starting sequence transitions dictated tokens string ends final states 
usually specified table lists new state state input 
example machine accepts boolean strings odd parity specified shown table 
dynamical recognizers table input state machines usually described fully explicit tables graphs transition functions specified mathematical function codes current state input 
example variable length parity specified exclusive current state input coded single bit 
primary result field neural networks simplified assumptions networks capacity perform arbitrary logical functions act finite state controllers mcculloch pitts minsky 
various configurations modern multilayer feed forward networks able perform arbitrary boolean functions hornik lapedes farber lippman 
recurrently networks capacity finite state recognizer 
states tokens assigned binary codes say bit indicating states code state simply computed set boolean functions codes current state current input 
mathematical models neural nets richer boolean functions polynomials 
mean automata 
order confuse theory implementation define general mathematical object language recognition forced discrete time continuous space dynamical system plus precise initial condition decision function 
recurrent neural network architecture section constrained implementation object 
analogy finite state recognizer dynamical recognizer quadruple rk space states zk initial condition 
finite input alphabet 
dynamic parameterized set token transformations space decision function 
finite length string tokens 
final state associated computed applying precise sequence transformations initial state zk 
zk 
language accepted generated cal recognizer set strings final states pass decision test 
turn recognizer generator simply enumerate strings filter recognizer rejects 
pollack mealy machine formulation mealy model decision function applies penultimate state final token zk 
just case finite automata labeling arcs nodes result smaller machines 
variants possible constrained avoid vacuous case powerful turing machine 
purposes assume weak conventional neural network decision function hyperplane convex region weak linear quasi linear transformation 
graded function forced decision lead notion string acceptability function returned complex categorization representation case discussing dynamical parsers 
generalize discrete symbols continuous symbols touretzky geva discrete time continuous time systems pearlmutter pineda 
difficult questions asked immediately dynamical recognizers 
kind languages recognize generate 
mathematical description compare various formal grammars grounds parsimony efficiency parsing neural psychological plausibility learnability 
definitive answers questions study touch issues 
thing clear outset linear dynamical recognizer model function arbitrary finite state automaton 
states automaton embedded finite dimensional space linear transformation account state transitions associated token 
consider case states dimensional binary unit vector code simply tion matrix lists state transitions token decision function just logical mask selects states interesting theoretical question determine minimum dimensionality linear embedding arbitrary regular language 
non linearities complex grammars accounted 
consider dimensional system unit line tests 
transformation multiply dynamical recognizers multiply modulo applies recognizer accepts balanced parentheses language 
words just mathematically possible embed infinite state machine dynamical recognizer embed finite state machine 
return issues 
address question learnability elaborate earlier cascaded networks pollack recurrent fashion learn parity depth limited parenthesis balancing map word sequences propositional representations pollack :10.1.1.30.6437

model cascaded network behaved higher order sigma pi connectionist architecture back propagation technique weight adjustment rumelhart applied 
basically consists subnetworks master slave relationship function slave network standard feed forward network hidden layers 
weights function network dynamically computed linear context master network 
context network outputs weights function network 
input context network multiplex function computed divide conquer heuristic learning easier 
near 
sequential cascaded network 
outputs master net left weights slave net right outputs slave net recurrent inputs master net 
outputs function network recurrent inputs context network system built learns associate specific outputs variable length input sequences 
block diagram sequential cascaded network shown 
multiplicative connections input effect processed different function 
initial context zk default sequence inputs yj network computes sequence output state vectors zi dynamically changing set weights wij 
hidden units forward pass computation pollack reduces ij wijk zk zi wij yj ijk usual sigmoid function back propagation systems 
previous assumed teacher supply consistent generalizable final output member set strings turned significant overconstraint 
learning state machine parity matter bit state fully determines output 
case higher dimensional system may know final output system don care final state jordan showed recurrent back propagation networks trained don care conditions 
specific target output unit particular training example simply consider error gradient 
long unit receives feedback examples 
don cares line weights units change 
possible fix called back propagation time rumelhart involves complete unrolling recurrent loop modest success mozer probably conflicts arising equivalence constraints interdependent layers 
fix involves single backspace unrolling loop 
particular string leads calculation error term weight conflict follows 
propagating errors determined subset weights acceptance unit za da za za yj zk error remainder weights calculated values wijk dynamical recognizers penultimate time step zk yj wij zi zk wijk wij near 
backspace trick 
partial information available computing error gradients weights penultimate configuration calculate gradients remaining weights 
schematic mode back propagation shown gradient calculations weights highlighted 
method applies small variations hidden units function context network system trained single accept bit desired output larger pattern representing tree structure example pollack :10.1.1.30.6437:10.1.1.30.6437
important point gradients connected subset outputs calculated directly gradients connected don care recurrent states calculated step back time 
forward backward calculations performed corpus variablelength input patterns weights updated 
squared sum errors approaches network improves calculation final outputs set strings training set 
threshold example network responds accept strings reject strings training halted 
network classifies training set tested generalization transfer set 
unfortunately language generalization infinite 

induction phase transition original studies learning simple regular language odd parity expected network merely implement exclusive feedback link 
turns quite 
termination back propagation usually pollack defined error logical recurrent logic tends limit point 
words separation finite exemplars guarantee network recognize sequential parity limit 
possible illustrated figures 
small cascaded network composed input output function net bias connections weights context net compute input output context net bias connections weights trained odd parity small set strings length table 
outputs fed back recurrently state third accept unit 
epoch weights network saved file subsequent study 
trained epochs network tested successfully longer strings 
important show network recognizing parity limit 
near 
stages adaptation network learning parity 
test cases separated limit point 
epoch odd sequences slightly separated 
little training oscillating cycle pronounced 
order observe limit behavior recognizer various stages adaptation observe response long characteristic string best chance breaking 
parity characteristic string sequence cause state changes 
shows stages adaptation network parity testing response intermediate configurations strings 
despite success separating small training set single attractor exists limit long strings indistinguishable 
epoch training odd strings slightly separated training separation significant drive threshold 
near dynamical recognizers 
bifurcation diagram showing response parity learner characteristic strings epochs training 
phase transition shown completely 
vertical axis represents network accept reject response characteristic strings horizontal axis shows evolution response epochs 
vertical column contains overlapping dots marking network response characteristic strings 
horizontal line graph plots evolution network response strings 
initially strings longer length distinguished 
epoch epoch network improving separating finite strings 
epoch network failing limit epoch network undergoes bifurcation small change weights transforms network limit behavior limit point limit cycle 
phase transition adaptive classification task network rapidly exploits 
want stress new interesting form mechanical induction 
phase transition machine principle capable performing serial parity task phase transition change abilities rapidly exploited adaptive search 
kind learning dynamic may related biological evolution natural selection insight problem solving aha phenomenon 
induction shot instantaneous punctuated equilibria evolution pre adaptive capacity enables population advantage drives rapid change 
metcalfe wiebe report psychological experiments insight problems human subjects measurably undergo similar cognitive phase transition reporting progress problems solution appears 

benchmarking results connectionist machine learning algorithms unfortunately sensitive statistical properties set exemplars learning environment data set 
researchers develop learning environments simple low dimensional dynamical systems usually studied knob control parameter bifurcation diagram scalar variable control parameter entire vector weights network back propagation turns knob 
pollack difficult methodological issue bearing status repetitive data set refinement especially experimental results bear psychologically measured statistics evolution data set considered irrelevant publish 
correctly led researchers include learning environment variable manipulate plunkett 
complicated path methodologically clean choices real world noisy data choose data refine published training data 
chose tomita performed elegant experiments inducing finite automata positive negative exemplars 
genetically inspired step hill climbing procedure manipulated state automata randomly adding deleting moving transitions inverting acceptability state 
starting random machine current machine compared mutated machine changed improvement result heuristic evaluation function 
hillclimber evaluation function maximized difference number positive examples accepted number negative examples accepted 
second hill climber evaluation function maintained correctness examples minimizing automaton description number states number transitions tomita randomly choose test cases chose consistently regular languages mind see table 
difficulty problems lies languages tomita mind arbitrary impoverished data sets 
training environment simply defined sets boolean strings table 
uniformity ran cases sequential cascaded network input output function network bias connections making weights context net compute input output context network bias connections 
total context weights essentially arranged array 
outputs function net fed back context network fourth output unit accept bit 
standard back propagation learning rate set momentum 
weights reset random numbers run 
training halted accept strings returned output bits reject strings 
dynamical recognizers 
results table set accept set reject set accept set reject set accept set reject set accept set reject tomita cases data sets converged problem epochs 
case converge kept treating negative case correct modify training set adding reject strings pollack set accept set reject set accept set reject set accept set reject table language description odd zero strings odd strings pairwise sum number number mod order overcome problem 
case took restarts thousands cycles converge 
dynamical recognizers table 
mutations avg 
epochs convergent language hill climber backprop backprop spirit machine learning community ran series experiments results empirical 
table compares tomita stage number mutations average number epochs 
back propagation sensitive initial conditions kolen pollack running problem give indication difficulty running times different random starting weights result widely disparate timings :10.1.1.30.6437
ran problem times epochs averaged runs separated training sets accepts rejects 
column labeled convergent shows percent runs problem separated accept reject strings cycles 
difficult compare results completely different methods taken averaged epochs percent convergent numbers give idea difficulty tomita data sets learning architecture 

analysis near 
minimal fsa recognizing tomita data sets 
near 
recursive rectangle figures induced languages 
see text detail 
tomita ran brute force enumeration find minimal automaton language verified hill climber able find 
displayed 
unfortunately ran difficulty trying exactly pollack finite state automata regular languages induced architecture 
reason prefixes languages recognized generated run networks 
rectangle assigns number white black boolean strings length limit visibility indicates black strings accepted respective network 
starting top rectangle row contains strings length lexical order string sitting right prefix 
top left shows number string top right shows number string 
strings 
training sets table indicated figures inverted corresponding training strings 
note figures display simple recursive patterns ideal minimal automata induced architecture 
language followed long string accepted network 
architecture generally problem inducing trap error states 
argued fsa inducing methods get problem learning trap states 
near 
number states th machine grew dramatically lowered 
network inducing smallest consistent fsa doing 
physical constraint implemented network finitely specified weights means states transitions arbitrary geometric relationship 
studies parity initial hypothesis set clusters organized geometric fashion embedding finite state machine finite dimensional geometry token transitions tomita assumed trap state mutate servan schreiber compared incoming token thresholded set token predictions trapping token predicted 
dynamical recognizers correspond simple transformation space 
wrote program examined state space networks recursively unexplored state combining inputs 
state dimensional vector values recurrently output units 
remove floating point noise program parameter counted states cube 
unfortunately machines grow drastically size lowered 
particular shows log log graph number unique states versus machine resulting training environment 
method grassberger procaccia set correlation dimension evidence fractal 
near 
images state spaces tomita training environments 
box contains points states corresponding boolean strings length 
states benchmark networks box anderson low dimension view machines graphically gain understanding state space arranged 
state vector plotted point unit cube 
partial graphs state spaces run networks shown 
states computed boolean strings including length contains points overlapping 
images initially expected clumps points closely map states equivalent fsa images limit considered states 
state spaces dynamical recognizers tomita cases interesting theoretically infinite state machines states arbitrary random requiring infinite table transitions constrained powerful way mathematical principle 
thinking principle consider systems extreme observed complexity emerges algorithmic simplicity plus computational power 
saw state space graphs reminded barnsley iterated functional systems barnsley compactly coded set affine transformations iteratively construct displays fractals previously described recursively pollack line segment rewrite rules mandelbrot 
calculation simply repetitive transformation plotting state vector sequence randomly chosen affine transformations 
infinite limit process fractal attractors emerge widely reproduced fern 
eliminating sigmoid commuting terms eq 
zi wijk yj zk treating yj infinite random sequence binary unit vectors codes forward pass calculation network seen process iterated function system ifs 
figures state spaces emerge projection fractal attractors defined barnsley 

related architecture learning paradigm studied lee giles colleagues closely related elman servan schreiber simple recurrent networks 
architectures rely extending jordan recurrent networks direction separates visible output states hidden recurrent states making unstable back propagation time assumption 
choice language data model main differences predictive paradigm error feedback provided time step computation classification paradigm feeding back examples 
certainly predictive paradigm psychologically plausible model positive presentation wexler pp tomita learning environments impoverished 
commitment negative information required desired output discriminates input strings generalizable way 
positive barnsley term attractor different conventional term technically correct refers limit iterative process 
thought happens randomly drop infinite number microscopic iron filing points piece magnetic field lying point land underlying attractor 
dynamical recognizers versus negative evidence merely simplest way bit provide discrimination 
single layer order recurrence states higher order quadratic recurrence 
multiplicative connections enable model fractal dynamics equivalent limit ifs may order recurrence weak general boolean functions minsky papert arbitrary regular languages parity results simple steady state periodic dynamics 
continued analysis scaling network binary symbol alphabets syntax immediate follow involve comparing contrasting respective models possible models higher order network trained prediction simple recurrent network model trained classification 

near 
slicing fractal state space balanced parenthesis dynamical recognizer 
take state space picture dimensional dynamical recognizer parenthesis balancing developed earlier looks 
infinite state machine embedded finite geometry fractal self similarity decision function cutting set 
emergence fractal attractors interesting believe bears question neural systems achieve power handle regular languages 
serious question connectionism answer chomsky firmly established regular languages recognizable markov chains finite state machines simple iterative associative means inadequate parsimoniously describe syntactic structures natural languages 
certain phenomena center embedding compactly described context free grammars recognized push automata phenomena crossed serial dependencies agreement better described context pollack sensitive grammars recognized linear bounded automata 
hand quite clear human languages formal related mathematical syntactic structures 
lead connectionists erroneously claim recursive computational power essence human computation 
quite clear understanding complexity issues connectionists stumble trap making strong claims models easy attack offering adequate replacement established theory 
fodor pylyshyn pinker prince 
long term lack competition descriptive theories involving rules representations defended explanatory theories 
alternative hypothesis complex syntactic structure state space limit dynamical recognizer attractor cut threshold similar decision function 
complexity generated language regular cut falls disjoint limit points cycles context free cuts self similar recursive region context sensitive cuts chaotic pseudo random region 
certainly substantial need theoretical front thoroughly formalize prove disprove main theorems implied hypothesis 
expect full range context free context sensitive systems covered conventional quasi linear processing constraints question remains wide open syntactic systems described neural dynamical recognizers convergence needs natural language systems 
information processing provides essence complex forms cognition language important understand relationship complex emergent behaviors dynamical systems including neural systems traditional notions computational complexity including chomsky hierarchy algorithmic information theory chaitin study relationship infancy 
pollack constructed turing machine connectionist parts essentially showed rational values constants precise thresholds multiplicative connections sequential cascaded network architecture sufficient primitives computationally universal recurrent neural networks 
rumelhart mcclelland 
dynamical recognizers cellular automata view kind low density synchronous uniform digital restriction neural networks studied dynamical systems wolfram proven powerful universal turing machines 
lindgren 
furthermore moore shown simple mathematical models dynamical systems universal follows directly determination behavior dynamical systems limit undecidable unpredictable precise initial conditions 
stronger terms theoretical foundations computer information science may accord lack predictability universe 
crutchfield young studied computational complexity dynamical systems reaching onset chaos period doubling 
shown systems regular finitely described indexed context free grammars 
may course just coincidence modern computational linguistic grammatical theories fall class joshi joshi pollard 
merely illuminated possibility existence naturalistic alternative explicit recursive rules description complexity language 
mathematical description compact parsimonious fact infinite state machine require infinite description finitely described set weights 
shown feasible learn type description finite set examples pseudo continuous hill climbing parameter adaptation words back propagation 
performance limit appears jump discrete steps inductive phase transitions correspond psychological stages acquisition 
languages described recognized generated efficiently neural computation systems 

acknowledgments partially sponsored office naval research 
numerous colleagues discussed criticized various aspects presentation including bylander chandrasekaran crutchfield giles hanson kasper kolen ogden port smolensky touretzky 
pollack 
anderson silverstein ritz jones 

distinctive features categorical perception probability learning applications neural model 
psychological review 
angluin 

complexity minimum inference regular sets 
information control 
angluin smith 

inductive inference theory methods 
computing surveys 
barnsley 

fractals 
san diego academic press 
berwick 

acquisition syntactic knowledge 
cambridge mit press 
chaitin 

length programs computing finite binary sequences 
journal am 
chomsky 

models description language 
ire transactions information theory 
chomsky 

aspects theory syntax 
cambridge ma mit press 
crutchfield farmer packard shaw 

chaos 
scientific american 
crutchfield young 

computation onset chaos 
zurek ed complexity entropy physics information 
reading ma addison wesley 
meir 

chaotic behavior layered neural network 
phys 
rev 
devaney 

chaotic dynamical systems 
reading ma addison wesley 
elman 

finding structure time 
cognitive science 
feldman 

decidability results grammatical inference 
information control 
fodor pylyshyn 

connectionism cognitive architecture critical analysis 
cognition 
dynamical recognizers giles sun chen lee chen 

higher order recurrent networks grammatical inference 
touretzky ed advances neural information processing systems 
los ca morgan kaufman 


chaos making new science 
new york viking 
gold 

language identification limit 
information control 
gold 

complexity automaton identification data 
information control 
grassberger procaccia 

measuring strange attractors 
physica 
ott 

chaos strange attractors fractal basin boundaries nonlinear dynamics 
science 
horn usher 

chaotic behavior neural network dynamical thresholds 
int 
journal neural systems appear 
hopfield 

neural networks physical systems emergent collective computational abilities 
proceedings national academy sciences usa 
hornik stinchcombe white 

multi layer feedforward networks universal approximators 
neural networks 
huberman hogg 

phase transitions artificial intelligence systems 
artificial intelligence 
joshi 

tree adjoining grammars context sensitivity required provide reasonable structural descriptions 
dowty karttunen eds natural language parsing 
cambridge university press 
cambridge joshi vijay shanker weir 

convergence mildly contextsensitive grammar formalism 
sells eds processing linguistic structure 
cambridge mit press 
kolen pollack 

backpropagation sensitive initial conditions 
complex systems 
pollack 

phase transitions neural networks 
institute electrical electronics engineers international conference neural networks 
san diego ii 
lapedes farber 

neural nets 
los alamos 
lieberman 

biology evolution language 
cambridge harvard university press 
lindgren 

universal computation simple onedimensional cellular automata 
complex systems 
lippman 

computing neural networks 
institute electrical electronics engineers assp magazine april 


continous computation 
cs knoxville computer science dept university tennessee 
macwhinney 

mechanisms language acquisition 
hillsdale lawrence erlbaum associates 
mandelbrot 

fractal geometry nature 
san francisco freeman 
mcculloch pitts 

logical calculus ideas nervous activity 
bulletin mathematical biophysics 
mealy 

method synthesizing sequential circuits 
bell system technical journal 
metcalfe wiebe 

intuition insight problem solving 
memory cognition 
minsky 

computation finite infinite machines 
cambridge ma mit press 
minsky papert 

perceptrons 
cambridge ma mit press 
moore 

unpredictability undecidability dynamical systems 
physical review letters 
mozer 

focused back propagation algorithm temporal pattern recognition 
crg technical report university toronto 
dynamical recognizers pearlmutter 

learning state space trajectories recurrent neural networks 
neural computation 
pineda 

generalization back propagation recurrent neural networks 
physical review letters 
pinker 

language learnability language development 
cambridge harvard university press 
pinker prince 

language connectionism analysis parallel distributed processing model language 
cognition 
pinker bloom 

natural language natural selection 
brain behavioral sciences 
plunkett 

pattern association back propagation network implications child language acquisition 
technical report san diego ucsd center research 
pollack 

cascaded back propagation dynamic connectionist networks 
proceedings ninth conference cognitive science society 
seattle 
pollack 

connectionist models natural language processing 
ph thesis urbana computer science department university illinois 
available computing research laboratory las cruces nm pollack 

implications recursive distributed representations 
touretzky ed advances neural information processing systems 
los ca morgan kaufman 
pollack 

recursive distributed representation 
artificial intelligence 
pollard 

generalized context free grammars head grammars natural language 
doctoral dissertation palo alto dept linguistics stanford university 
rivest schapire 

new approach unsupervised learning deterministic environments 
proceedings fourth international workshop machine learning 
irvine 
pollack rumelhart mcclelland 

pdp models general issues cognitive science 
rumelhart mcclelland pdp research group eds parallel distributed processing experiments microstructure cognition vol 

cambridge mit press 
rumelhart hinton williams 

learning internal representations error propagation 
rumelhart mcclelland pdp research group eds parallel distributed processing experiments microstructure cognition vol 

cambridge mit press 
servan schreiber cleeremans mcclelland 
encoding sequential structure simple recurrent networks 
touretzky ed advances neural information processing systems 
los ca morgan kaufman 
freeman 

brains chaos 
brain behavioral science 
smolensky 

information processing dynamical systems foundations harmony theory 
rumelhart mcclelland pdp research group eds parallel distributed processing experiments microstructure cognition vol 

cambridge mit press 
tomita 

dynamic construction finite state automata examples hill climbing 
proceedings fourth annual cognitive science conference 
ann arbor mi 
touretzky geva 

distributed connectionist representation concept structures 
proceedings ninth annual conference cognitive science society 
seattle 
van der 

note chaotic behavior simple neural networks 
neural networks 

wexler 

formal principles language acquisition 
cambridge mit press 
wolfram 

universality complexity cellular automata 
physica 
pollack input sequence ijk ij state output pollack ijk ij ijk ij accept reject response length string pollack pollack number states round distance pollack pollack pollack decision threshold 
