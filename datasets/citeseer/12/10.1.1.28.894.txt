policy policy search nicolas meuleau leonid peshkin leslie kaelbling mit artificial intelligence laboratory technology square cambridge ma ai mit edu kee kim computer science dept brown university providence ri kek cs brown edu gradient policy search alternative value function methods reinforcement learning non markovian domains 
apparent drawback policy search requirement actions policy explicit exploration 
provide method importance sampling allow wellbehaved directed exploration policy learning 
show theoretically experimentally method achieve dramatic performance improvements 
main approaches solving reinforcement learning rl problems 
side value search algorithms learning ql sarsa find optimal policy searching optimal value function deducing optimal policy optimal value function :10.1.1.32.7692
side policy search algorithms reinforce directly policy space trying maximize expected reward help value functions 
advantage value search usually guarantees global optimality policy search typically finds local optima 
value search faster sense bellman optimality principle powerful heuristic guide search policy search informed value search 
big drawback value search works completely observable markovian environments 
attempts value search partially observable settings techniques guaranteed find optimal solution 
simply bellman equation transfer non markovian environments 
policy search hand accommodates partial observability non 
find locally optimal controllers kinds constraints different forms memory 
policy search particularly interesting partially observable settings 
basic policy search algorithm williams episodic reinforce 
performs bayesian belief states original states increases complexity problem dramatically 
follow approach 
precisely rewrite bellman fundamental equation replacing states observations partially observable environment 
provided accept general definition reinforce strictly stochastic gradient descent expected reward 
reinforce basically policy algorithm gradient point policy space estimated precisely policy learning trials :10.1.1.32.7692
corresponds naive way estimating expectation sampling 
sophisticated way technique known importance sampling 
applied reinforce allows policy implementations algorithm may execute policy different current policy learning trials 
interesting respect issues number learning trials importance sampling classically increase accuracy estimates reducing variance 
context reinforce complete learning trial needed get estimate gradient step policy space means better estimate gradient trial faster learning terms number trials required reach performance 
length learning trials number trials reflect perfectly complexity learning trials may different lengths 
total number interactions environment length learning trials important issue 
show importance sampling may reinforce reduce length learning trials number speed learning dramatically terms number time steps interaction environment 
save space suppose reader familiar basic notions markov decision processes mdps reinforcement learning rl 
brief reinforce section 
focus fully observable mdps solved reinforce implementing reactive policy 
technique importance sampling variation reinforce including partially observable environments 
contains simulation results obtained environment 
gradient policy search reinforce simplify presentation suppose environment finite mdp reinforce optimize parameters action probabilities stochastic memoryless policy sufficient environment see complex setting 
suppose problem goal achievement task absorbing goal state reached fast possible 
insure policies proper reach goal probability policy preventing action probabilities going 
typically policy stored set weights sa ir decoded boltzmann law def pr wsa sa state action time constant temperature parameter 
episodic reinforce introduced williams 
performs stochastic gradient ascent expected cumulative reward discount factor reward time current policy 
implementations algorithm 
sophisticated equality ht ht pr experience sequence length different goal state goal state set williams papers 
notably consider algorithm independently neural network encode policy 

initialize controller weights wsa 
initialize controller weights wsa 
trial 
trial ns nsa ns nsa 
wsa 
time step trial 
time step trial draw action random draw action random 

ns ns ns ns ns ns ns ns 
wsa 
wsa nsa ns 
trial wsa wsa nsa ns 
trial wsa wsa 
wsa 
loop return 
loop return 
brute reinforce vaps episodic reinforce look tables boltzmann law 
stepsize parameter learning rate 
sequences reward time sa ht ht pr sa sa sa contribution partial derivative respect sa sa def sa ln experience sequences sampled pr 
current policy followed learning trials contribution sa experience trial unbiased estimate gradient may step policy space 
contribution generally easy calculate 
instance policy stored look tables boltzmann law sa ln algorithm obtained case williams stressed brute implementation reinforce perform optimal temporal credit assignment ignores fact reward time depend actions performed time baird moore vaps immediate error function called authors policy efficient implementation reinforce takes accounts causality rewards 
basic principle rewrite expected reward form pr experience prefix length may different goal state set experience prefixes 
gradient may expressed form sa pr sa csa sa contribution prefix sa def sa ln csa random variable takes value sa experience trial ends time 
experience trial current policy vaps calculates unbiased estimate csa 
sum estimates unbiased estimate gradient update policy trial 
different estimates mutually independent hurt summing 
presents instance algorithm 
note implementations reinforce exactly equivalent goal reward model reward goal 
complexity reinforce trials studies complexity reinforcement learning focused length learning trial goal achievement problems 
whitehead showed expected length ql trial grow exponentially size environment 
basically ql dynamics may result random walk trial random walks take exponential time 
typically difficult problems actions take away goal bring closer state reset action brings back starting state reset problems 
thrun showed directed counter exploration techniques alleviate problem ql guaranteeing polynomial complexity trial deterministic mdp 
simultaneously koenig simmons showed changes parameters initial values reward function effect mdp 
easy see reinforce faces difficulties undirected ql kind environments 
depending way controller initialized learning complexity trial may bad due initial random walk algorithm 
instance may show finite mdp controller initialized drawing action probabilities 
uniformly simplex expected length reinforce trial infinite 
update consecutive trial may change policy lot may expect bad performance trials learning 
clear changing reward model suggested koenig simmons may reduce expected length reinforce trial update weights trial length trial depends initial controller 
clear impossible directed exploration policy reinforce current state constrained execute current policy learning 
policy implementations reinforce key policy implementations reinforce monte carlo technique known importance sampling 
reinforce uses naive sampling estimates expectations sa csa conditionally current policy sampling experience sequences prefixes current policy 
sophisticated solution consists defining policy pr pr executing learning trials multiplying contribution sa sa experience sequence prefix importance coefficient def pr pr pr trunc trunc truncated time random variables get unbiased estimate gradient 
note importance coefficients may calculated incrementally knowing dynamics environment 
may policy stationary 
notably type extra information guide exploration avoid random walk trials instance counters state visits 
requirement trajectory possible possible directly deterministic exploration policy 
instance mix policy current policy time step follow deterministic exploration policy probability current policy probability 
shown mix current policy thrun counter exploration policy expected length trial deterministic mdp tends polynomial size mdp tends provided controller initialized uniformly 
represents new way advantage importance sampling 
usually reduce variance estimate number samples 
framework reinforce get sample variable trial corresponds faster learning terms number trials 
show context reinforce reduce length learning trials 
presents algorithms obtained mixture deterministic nonstationary exploration policy thrun counter probability current policy probability 
variable store history trial formally necessary defined function past history 
exploration techniques need complete history 
instance counter exploration policy choice action depends counters remember previous history just maintain necessary counters 
numerical simulations algorithms showed big instability increases see section 
instability expression known drawback sampling distribution strategy learning differs lot target distribution current policy 
case events associated huge importance coefficients 
happen induce devastating weight updates stick algorithm bad policy 
technique known weighted importance sampling designed alleviate drawback 
reinforce consists estimating stands sa csa drawing samples xn sampling policy calculating associated importance coefficients kn quantity estimate 
estimate biased bias tends tends infinity 
faster stable estimate simple estimate 
straightforward apply weighted importance sampling brute implementation reinforce 
steps algorithm repeated times gradient estimates importance coefficients trials summed step weighted estimate performed th trial 
unfortunately incremental implementation weighted vaps 
reason prefer brute implementation reinforce sacrifice quality temporal credit assignment possibility efficient sampling policies 

initialize controller weights wsa 
initialize controller weights wsa 
trial 
trial ns nsa ns nsa 
wsa 
time step trial 
time step trial probability probability probability probability draw random 
draw random 
ns ns ns ns ns ns ns ns 
wsa 
wsa nsa ns append triple 
trial append triple wsa wsa nsa ns 
trial wsa wsa 
wsa 
loop return 
loop return 
brute reinforce vaps reinforce look tables boltzmann law executing learning trials mixture deterministic non stationary policy probability current policy probability 
numerical simulations tested policy algorithms simple grid world problem consisting square room starting state goal opposite corners 
variants problem tried may may reset action problem fully observable partially observable 
simple versions algorithms figures weighted implementation brute reinforce 
tried parameter settings exploration policies including greedy thrun counter indirect ql implementation global exploration policy proposed meuleau bourgine 
measure quality learned policy perform learning trial test trial variables frozen exclusively current policy executed 
results experiments qualitatively independent variant size problem 
naive sampling stable slow 
small values simple allows reducing length learning trials affecting quality policy learned 
rapidly unstable systematically jumps bad policies increases 
brute reinforce weighted far efficient algorithm 
stays stable approaches relatively small values 
high values allows dramatic reduction trials length 
experiments show comes considerable improvement quality policy learned trials 
presents sample results 
proposed policy implementations reinforce technique importance sampling 
demonstrated theoretically experimentally allow reducing length learning trials number results naive sampling learning test optimal trial length vs trial number naive sampling weighted optimal performance learned policy vs number time steps learning results obtained partially observable reset variant problem meuleau bourgine global exploration policy goal reward average runs 
dramatic acceleration learning terms number time steps interaction 
may prove crucial tackling real world huge partially observable problems 
policy implementations reinforce possible simultaneous optimization controllers possibly different architecture 
lead evolving controller architecture 
nicolas meuleau supported part research ntt leonid peshkin nsf ntt leslie pack kaelbling part ntt part darpa contract dabt kee kim part afosr rl 
baird moore 
gradient descent general reinforcement learning 
advances neural information processing systems 
mit press cambridge ma 
koenig simmons 
effect representation knowledge goal directed exploration reinforcement learning 
machine learning 
littman 
memoryless policies theoretical limitations practical results 
proceedings sab 
mit press cambridge ma 
meuleau 
complexity trial reinforce 
unpublished manuscript 
meuleau bourgine 
exploration multi state environments local measures back propagation uncertainty 
machine learning 
meuleau peshkin kim kaelbling 
learning finite state controllers partially observable environments 
proceedings uai pages 
rubinstein 
simulation monte carlo method 
wiley new york ny 
sutton barto :10.1.1.32.7692
reinforcement learning 
mit press cambridge ma 
thrun 
efficient exploration reinforcement learning 
technical report cs carnegie mellon university pittsburgh pa 
whitehead 
complexity analysis cooperative mechanisms reinforcement learning 
proceedings aaai 
williams 
simple statistical gradient algorithms connectionist reinforcement learning 
machine learning 
