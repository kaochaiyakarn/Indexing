unsupervised learning finite mixture models figueiredo member ieee anil jain fellow ieee new unsupervised algorithm learning finite mixture model multivariate data proposed 
adjective unsupervised justified properties algorithm capable selecting number components ii standard expectation maximization em algorithm require careful initialization parameters 
proposed method avoids wellknown drawback em mixture fitting possibility convergence singular estimate boundary parameter space 
novelty approach model selection criterion choose set candidate models seamlessly integrate estimation model selection single algorithm 
technique applied type parametric mixture model possible write em algorithm illustrate experiments involving gaussian mixtures mixtures factor analyzers 
experiments testify performance approach simpler faster methods proposed fitting mixture model unknown number components 
index terms unsupervised learning finite mixtures model selection minimum message length criterion bayesian methods expectation maximization algorithm clustering 
corresponding author figueiredo 
figueiredo institute telecommunications department electrical computer engineering instituto superior lisboa portugal mail mtf lx pt phone fax jain computer science engineering michigan state university east lansing mi 
mail jain cse msu edu earlier shorter version 
finite mixtures flexible powerful probabilistic modeling tool univariate multivariate data 
usefulness mixture models area involves statistical modelling data pattern recognition computer vision signal image analysis machine learning currently widely acknowledged 
statistical pattern recognition finite mixtures allow formal probabilistic modelbased approach unsupervised learning clustering 
fact finite mixtures adequately model situations observation assumed produced randomly selected unknown set alternative random sources 
inferring parameters sources identifying source produced observation naturally leads probabilistic clustering set observations 
adoption model approach clustering opposed heuristic techniques means hierarchical agglomerative methods brings important advantages issues selection number clusters assessment validity model addressed principled formal way 
usefulness mixture models limited unsupervised learning applications 
mixture models able represent arbitrarily complex probability density functions pdf 
fact excellent choice representing complex pdf likelihood functions bayesian supervised learning scenarios priors bayesian parameter estimation 
mixture models perform feature selection 
standard method fit finite mixture models observed data wellknown expectation maximization em algorithm 
em algorithm locate maximum likelihood ml estimate mixture parameters 
em algorithm finite mixture fitting drawbacks local greedy method sensitive initialization likelihood function mixture model usually multi modal certain classes mixtures may converge boundary parameter space likelihood unbounded leading meaningless estimates 
fundamental issue mixture modelling selection number components 
usual tradeoff model order selection problems arises components mixture may fit data mixture components may flexible approximate true underlying model 
deal simultaneously mentioned problems 
propose inference criterion mixture models algorithm implement autonomously selects number components sensitive initialization em avoids boundary parameters space standard em known run difficulties 
vast majority literature focuses mixtures gaussian densities mixtures built different types probability density functions appeared literature 
increased interest sophisticated gaussian mixture models able perform local dimensionality reduction mixtures factor analyzers mfa mixtures probabilistic principal component analyzers 
approach proposed applied type parametric mixture model possible write em algorithm mfa 
rest organized follows 
section ii review basic concepts finite mixture models em algorithm course standard material purpose mainly introduce problem define notation 
take opportunity bring results em attention readers 
section iii review previous devoted problem learning mixtures unknown number components eliminating various drawbacks em algorithm 
section iv describe proposed inference criterion algorithm implements 
section reports experimental results section vi ends presenting concluding remarks 
ii 
learning finite mixture models finite mixture models dimensional random variable representing particular outcome said follows component finite mixture distribution probability density function written yj ff yj ff ff mixing probabilities set parameters defining th component ff ff complete set parameters needed specify mixture 
course probabilities ff verify ff ff assume components functional form example variate gaussian fully characterized parameter vector detailed comprehensive accounts mixture models see simply review fundamental ideas define notation 
set independent identically distributed samples fy log likelihood corresponding component mixture log yj log log ff known maximum likelihood ml estimate ml arg max flog yj analytically 
true bayesian maximum posteriori map criterion map arg max flog yj log prior parameters 
course defining ml map estimates constraints eq 

em algorithm usual choice obtaining ml map estimates mixture parameters classical em algorithm 
em iterative procedure find local maxima log yj log yj log 
case gaussian mixtures convergence behavior em algorithm studied :10.1.1.18.5213
em algorithm interpretation incomplete data 
case finite mixtures missing part set labels fz associated samples indicating component produced sample 
label binary vector means sample produced th component 
complete log likelihood estimate complete data fy zg observed log zj log ff em algorithm produces sequence estimates alternatingly applying steps convergence criterion met ffl step computes conditional expectation complete log likelihood eq 
current estimate 
log zj linear respect missing simply compute conditional expectation zjy plug log zj 
result called function log zj log wj elements binary conditional expectations elements jy pr jy ff ff equality simply bayes law notice ff priori probability 
ffl step updates parameter estimates arg max fq log case map estimation arg max ml criterion cases constraints eq 

em algorithm shown monotonically increase log likelihood function 
known result appendix briefly review extremely simple elegant proof proposed 
proof sees em new light opens door extensions generalizations em belongs class iterative methods called proximal point algorithms ppa 
related earlier result identifying em ppa 
iii 
previous estimating number components start defining class possible component mixtures built certain type pdf variate gaussian mixtures unconstrained covariance matrices 
ml criterion estimate number mixture components classes nested 
illustrate fact consider ff ff gamma ff defines mixture ff ff gamma ff ff defines mixture class ff ff ff represent intrinsically indistinguishable mixture densities 
consequently maximized likelihood yj ml non decreasing function useless criterion estimate number components 
model selection methods proposed estimate number components mixture 
vast majority methods classified computational point view classes deterministic stochastic 
deterministic methods 
methods class start obtaining set candidate models usually em range values min max true optimal known belong 
optimal value selected arg min fc min max model selection criterion estimate mixture parameters assuming components 
usually gamma log yj increasing function penalizing higher values cost functions type proposed bayesian model selection framework 
examples evidence bayesian ebb criterion approximate weight evidence awe schwarz bayesian inference criterion bic :10.1.1.101.5035
criteria include rissanen minimum description length mdl formally coincides bic minimum message length mml akaike information criterion aic bezdek partition coefficient pc :10.1.1.17.321
reported ebb mdl bic mml perform comparably outperform methods tested 
concerning awe shown mdl bic provides better approximation true bayes factor :10.1.1.101.5035
aic pc criteria shown outperformed mml mdl bic 
resampling schemes cross validation approaches estimate number mixture components 
terms computational load methods closer stochastic techniques see deterministic ones 
stochastic methods 
methods resort markov chain monte carlo mcmc sampling far computationally intensive em 
mcmc different ways implement model selection criteria estimate fully bayesian way sample full posteriori distribution considered unknown :10.1.1.27.3667
despite formal appeal think mcmc techniques far computationally demanding useful pattern recognition applications 
example tests reported small samples univariate data require mcmc sweeps called burn period sweeps huge amount computation small problems orders magnitude required method proposed 
drawbacks em methods mentioned vast majority deterministic algorithms fitting mixtures unknown numbers components supported em algorithm 
methods exhibit model selection performance see comparisons major draw back remains set candidate models obtained known problems associated em emerge 
em highly dependent initialization 
common time consuming solutions include combination strategies random starts choosing final estimate highest likelihood initialization clustering algorithms deterministic annealing versions em 
smarter efficient method split merge operations proposed 
em may converge boundary parameter space 
example fitting gaussian mixture unconstrained covariance matrices ff may approach zero corresponding covariance matrix may arbitrarily close singular 
number components assumed larger optimal true tends happen frequently accordingly serious problem methods require mixture estimates various values iv 
proposed approach criteria eq 
perform model class selection select best representative model 
mixture models distinction model class selection model estimation unclear distinguishes component mixture mixing probabilities equal zero component mixture 
observations suggest shift approach selecting may arbitrary large value infer structure mixture letting estimates mixing probabilities zero 
achieved mml type criterion :10.1.1.17.321
mml philosophy adopt model class model hierarchy simply aims directly finding best model set available models kmax min selecting set candidate models min min previous uses mml mixtures strictly adhere perspective mml model class selection criterion 
em compute set candidate models drawbacks mentioned able directly implement mml criterion variant em 
new algorithm turns initialization dependent standard em built behavior avoids approaching boundary parameter space 
minimum message length criterion minimum encoding length criteria mdl mml basic idea build short code observed data means model data generated :10.1.1.17.321
formalize idea consider thought experiment 
wished encode transmit data set known generated yj 
shannon theory shortest code length gamma log yj dae denotes smallest integer 
moderately large data sets gamma log yj ae deltae operator usually dropped 
source model yj fully known transmitter receiver build code expected length equals entropy gamma yj log yj dy see communication measured bits base logarithms nats natural logarithms adopted 
proceed 
priori unknown transmitter start estimating transmitting 
leads part message total length length length length yj minimum encoding length criteria mdl mml state parameter estimate minimizing length 
delicate issue approach vector real parameters finite code length obtained truncating finite precision 
points flavors minimum encoding length idea mdl mml differ 
course data may real valued 
cause difficulty simply truncate arbitrary fine precision ffi replace density yj probability yj ffi dimensionality 
resulting code length gamma log yj gamma log ffi gammad log ffi independent irrelevant minimum encoding length criterion 
truncation simple different approaches handle 
main common idea involves tradeoff 
finite precision version obtained truncating elements finite precision 
fine precision length yj small come close optimal value length large 
conversely coarse precision length small length yj far optimal 
ways formalize deal technical details tradeoff interested reader referred comprehensive review pointers literature 
adopting mml approach see leads criterion minimization respect understood simultaneously dimension arg min ae gamma log gamma log yj log ji log oe gammae log yj expected fisher information matrix ji determinant called optimal quantizing lattice constant dimensional space :10.1.1.17.321
constant appears mml axis aligned quantization truncation optimal quantizing lattices considered example ir optimal lattice hexagonal grid 
influence small vary fact grows approaches asymptotic value see 
accordingly replace corresponds considering quantization regions hyper cubes regardless dimension space 
recall standard mdl criterion formally conceptually coincides bic obtained eq 
steps 
drop log assume flat prior 
ni fisher information corresponding single observation sample size write log ji log log ji consider large drop order terms log ji log 
certain value take gamma log yj gamma log yj corresponding ml estimate consider minimization respect model class 
result known classical mdl criterion mdl arg min ae gamma log yj log oe part code interpretation clear data code length gamma log yj components requires code length proportional log intuitively sense precision encoding parameter estimates denotes matrix second derivatives hessian 
proportional precision estimated estimation precision measured estimation error standard deviation 
certain regularity conditions standard deviation estimation error decreases led log term 
proposed criterion mixtures consider prior number known larger true optimal number mixture components 
nk number parameters specifying mixture component dimensionality mml criterion eq 
arg min gamma log gamma log yj log ji gamma log mixtures general obtained analytically 
fact course previously noticed trying apply mml criterion mixtures 
side step difficulty replace complete data fisher information matrix gammae zj upper bounds 
matrix block diagonal structure block diag ff ff fisher matrix single observation known produced th component fisher matrix multinomial distribution recall jmj ff ff delta delta delta ff gamma 
approximation motivated fact exact limit non overlapping components common situation clustering problems 
specify prior expresses lack knowledge mixture parameters 
naturally model parameters different components priori independent independent mixing probabilities ff ff factors adopt non informative jeffreys priors see example details jeffreys priors express lack knowledge ji ff ff jmj ff ff delta delta delta ff gamma choices criterion eq 
arg min log nff log gamma log yj apart order term gammalog criterion intuitively appealing interpretation spirit standard part code formulation mdl mml 
usual gamma log yj code length data expected number data points produced th mixture component nff seen effective sample size estimated optimal code length log nff ff estimated observations giving rise log term 
objective function eq 
sense allow ff zero goes gamma 
difficulty easily removed invoking code length interpretation clearly fully specify mixture model just code parameters components probability non zero 
letting nz ffm denote number non zero probability components ffm log nff nz log gamma log yj additional term needed encode nz code length constant specifically log nz kg irrelevant 
final cost function minimization respect constitute mixture estimate 
implementation em bayesian point view eq 
equivalent fixed nz posteriori density resulting adoption dirichlet type prior ff ff ff exp gamma log ff negative parameters improper flat prior leading ml estimates 
dirichlet priors conjugate multinomial likelihoods em algorithm minimize cost function eq 
step recall constraints eq 
ff max gamma max gamma arg max ff step equation eq 

corresponding components ff irrelevant notice eq 
components ff contribute log likelihood 
highlight aspects algorithm 
ffl important feature step defined eq 
performs component annihilation explicit rule moving current value nz smaller 
notice prevents algorithm approaching boundary parameter space components weak meaning supported data simply annihilated 
drawbacks standard em mixtures avoided 
ffl starting nz larger true optimal number mixture components algorithm robust respect initialization 
referred local maxima likelihood arise components region space 
em unable move components region crossing low likelihood regions 
starting components space problem avoided done remove unnecessary components 
previously exploited idea different technique 
ffl type local maximum standard em difficulty escaping corresponds situations components similar parameters sharing approximately data points 
dirichlet type prior negative parameters eq 
situations unstable promotes competition component estimates result components eventually win annihilated 
unstable nature negative parameter prior clear plot shown fig 
ff gamma ff ff ff gamma ff gamma ffl may component annihilation behavior new step eq 
strong 
proof optimality interesting connections particular case gaussian mixtures 
consider fitting gaussian mixture arbitrary covariance matrices variate data meaning minimum value sufficient statistic needed support component grows quadratically notice sufficient statistic seen equivalent number points assigned th component 
accordance fig 

plot competition inducing dirichlet prior negative parameters ff gamma ff ff ff gamma ff gamma observe prior encourages configurations ff equals zero 
known results concerning relation sample size dimensionality error probability supervised classification learning plug quadratic discriminants sample size needed guarantee error probability grows approximately quadratically dimensionality feature space 
similar connection exists case gaussian mixture components sharing common covariance matrix case agreement fact linear discriminants sample size needed guarantee error probability grows linearly dimensionality 
connection supervised classification surprising due complete data fisher information 
component wise em direct em algorithm step eqs 
large happen component initial support ff undetermined 
avoid problem component wise em cem algorithm proposed 
basically simultaneously updating ff update sequentially update ff recompute update ff recompute 
convergence cem easily proved see help proximal point algorithm interpretation 
purposes key feature cem component annihilated ff set zero immediately recomputing probability mass redistributed components increasing chance survival 
method allows initialization arbitrarily large problems 
complete algorithm convergence cem guarantee minimum 
component annihilation eq 
take account additional decrease caused decrease nz consequently possible smaller values achieved setting zero components annihilated step eq 

explore configurations lower values nz simply annihilate probable component smallest ff rerun cem convergence 
procedure repeated nz 
choose estimate led minimum value 
course reason believe number components min nz min going way 
convergence cem run declared relative decrease falls threshold ffl 
experiments reported ffl gamma experiments gaussian mixtures consider variate gaussian mixture models arbitrary covariance matrices yj gamma exp ae gamma gamma gamma gamma oe cm cm step eq 
see cm gamma gamma initialize mean vectors randomly chosen data points 
initial covariances proportional identity matrix oe oe fraction examples mean variances dimension data oe trace gamma gamma global data mean 
initialization strategies possible fact verified experimentally method spreads large number components data space leads results 
possible quantify large means noticing condition success type strategy component true mixture left initialization 
assume sample size large proportion points generated component close probabilities fff ff ff min ff probability probable component probably left initialization 
probability component unrepresented initialization approximately large gamma ff min probability successful initialization gamma desired necessary log log gamma ff min example ff min obtain 
example fig 
serves illustrate behavior algorithm samples component mixture similar 
observe initial large set components successively purged components non zero probability represented leading estimate true mixture components 
evolution loss function shown fig reveals model nz selected considering mixtures values nz way 
notice component forced zero outside cem algorithm cost function may jump mixture obtained annihilating component may optimal number components decreases 
example illustrates proposed algorithm addition avoiding initialization problem method selects number mixture components 
second test data produced component mixture considered ff ff ff means gamma equal covariances 
fig 
shows initialization intermediate configurations successful final estimate 
seen plot evolution fig 
mixtures nz nz tested final values larger nz consequently discarded 
repeated experiment times success 
true mixture data nz initialization nz nz nz selected mixture nz fig 

fitting gaussian mixture true mixture generated data initial configuration intermediate estimate nz mixture estimates nz nz selected nz 
mixture method successfully solves initialization issue deterministic annealing version em proposed method require cooling schedule ii autonomously selects number components iii faster 
consider known iris data set dimensional points classes class fit gaussian mixture free covariance matrices 

ran algorithm times study robustness respect random initialization time correctly identified classes 
data estimated components resulting runs shown fig 
projected principal components data 
map classifier built cost function iterations nz nz minimum cost function fig 

evolution cost function example fig 

vertical solid lines signal annihilation components inside cem algorithm vertical dotted lines show probable component forced zero convergence cem 
final mixture parameters trials yields errors 
knowledge best result obtained iris data unsupervised classification method see 
example considers gaussian mixtures model class conditional densities mixture discriminant analysis mda high dimensional feature space 
specific problem address equiprobable classes dimensional space studied 
observation defined gamma class gamma class gamma class uniform zero mean unit variance initialization nz nz nz nz nz fig 

fitting gaussian mixture initial configuration dashed ellipses show true mixture intermediate estimates nz final estimate nz 
solid ellipses level curves component estimate components ff shown 
gaussian samples functions max gamma jj gamma gamma classes basically side triangle ir vertices done mixtures fitted sets samples class resulting map classifier tested independent set samples 
compared cost function iterations nz nz nz fig 

evolution cost function example fig 

vertical solid lines signal annihilation components inside cem algorithm vertical dotted lines show probable component forced zero convergence cem 
classification methods mixture discriminant analysis mda new method diagonal covariance matrices 
mixture discriminant analysis mda common covariance components class estimated em initialized described means clustering algorithm run random starts results initialize em best highest likelihood estimate chosen 
quadratic discriminant analysis qda class conditional densities modelled gaussian arbitrary covariance matrices 
linear analysis lda class conditional densities modelled gaussian common covariance matrix 
method shown outperform lda qda 
results table show mda method better mda confirms fig 

iris data projected principal components estimated gaussian mixture 
classes plotted different symbols pi course mixture obtained knowledge class data point 
superiority mda lda qda 
importantly method proposed require external initialization adaptively selects number components usually cases 
table mixture discriminant analysis problem average error rates simulations 
method average standard deviation classification error mda new method mda lda qda mixtures factor analyzers mixtures factor analyzers mfa proposed basically mixtures gaussians reduced parameterization covariance matrices ff mn psi data produced component modelled having generated mv psi psi diagonal matrix 
dimension smaller means mfa able perform local dimensionality reduction 
mfa closely related mixtures probabilistic principal component analyzers proposed 
em algorithm mfa proposed split merge em algorithm proposed successfully applied mfa means overcoming initialization sensitivity em 
algorithms estimates number components 
tested algorithm proposed noisy shrinking spiral data 
described goal extract piece wise linear approximation dimensional nonlinear manifold dimensional data 
case psi theta 
data generated gamma cos gamma sin uniformly distributed zero mean gaussian samples 
fig 
shows data set initial mixture nz estimates nz selected nz min 
repeated test different samples data model algorithm selected nz times nz nz 
got trapped poor local minima 
min number iterations typically similar algorithm select number components 
data initialization nz selected model nz nz fig 

noisy shrinking spiral data initial configuration nz estimates nz selected nz 
line segment ends gamma 
vi 
described new method fitting finite mixture models multivariate data able select number components unsupervised way 
proposed algorithm avoids drawbacks standard expectation maximization em algorithm sensitivity initialization possible convergence boundary parameter space 
method mml criterion directly implemented modified em algorithm 
fundamental novelty approach mml model selection criterion choose set candidate models seamlessly integrate estimation model selection single criterion 
resulting technique simple fast 
experimental results showed performance approach learning mixtures gaussians mixtures factor analyzers 
mention choosing number components model selection issue mixture fitting 
important aspect decide shape component 
example fitting gaussian mixture may want decide arbitrary covariance matrices diagonal different covariance matrices common covariance matrix 
mixture factor analyzers may want select dimensionality component 
foreseen includes extending approach developed handle questions 
important issue detection outliers observations modelled component mixture 
mixture fitting outliers handled considering extra component uniform gaussian high variance role absorb anomalous observations 
currently investigating idea incorporated technique proposed 
appendix em proximal point algorithm kullback leibler penalty consider function ir ir maximum respect sought log yj ml estimation mixture parameters log yj log map estimation function 
generalized ppa defined iteration arg max gamma fi distance penalty function fi sequence positive numbers 
original ppa gamma proposed studied 
generalized versions penalty functions gamma considered authors 
ppa comprehensive set see chap 

monotonicity ppa iterations trivial consequence properties penalty function 
definition iteration eq 
gamma fi gamma fi 
consequently gamma fi establishes fe nondecreasing sequence 
turns em ppa log yj log yj log ml map estimation respectively fi kl zjy zjy log zjy zjy fi fi fi fi fi kullback leibler divergence zjy zjy see 
kullback leibler divergence satisfies conditions monotonicity em results immediately 
acknowledgments acknowledge dr ueda providing noisy shrinking spiral code 
figueiredo jain unsupervised selection estimation finite mixture models proceedings international conference pattern recognition icpr barcelona 
appear 
jain dubes algorithms clustering data 
englewood cliffs prentice hall 
jain duin mao statistical pattern recognition review ieee transactions pattern analysis machine intelligence vol 
pp 

mclachlan basford mixture models inference application clustering 
new york marcel dekker 
titterington smith makov statistical analysis finite mixture distributions 
chichester john wiley sons 
hastie tibshirani discriminant analysis gaussian mixtures journal royal statistical society vol 
pp 

hinton dayan revow modeling manifolds images handwritten digits ieee transactions neural networks vol 
pp 

dalal hall approximating priors mixtures natural conjugate priors journal royal statistical society vol 

kittler feature selection approximation class densities finite mixtures special type pattern recognition vol 
pp 

dempster laird rubin maximum likelihood estimation incomplete data em algorithm journal royal statistical society vol 
pp 

mclachlan krishnan em algorithm extensions 
new york john wiley sons 
ghahramani hinton em algorithm mixtures factor analyzers tech 
rep crg tr university toronto canada 
tipping bishop mixtures probabilistic principal component analyzers neural computation vol 
pp 

xu jordan convergence properties em algorithm gaussian mixtures neural computation vol :10.1.1.18.5213
pp 

hero kullback proximal algorithms maximum likelihood estimation submitted ieee trans 
info 
theo 
neal hinton view em algorithm justifies incremental sparse variants learning graphical models jordan ed pp 
kluwer academic publishers 
roberts penny bayesian approaches gaussian mixture modelling ieee transactions pattern analysis machine intelligence vol 
pp 
november 
banfield raftery model gaussian non gaussian clustering biometrics vol 
pp 

schwarz estimating dimension model annals statistics vol 
pp 

fraley raftery clusters :10.1.1.101.5035
clustering method 
answers model cluster analysis tech 
rep department statistics university washington seattle wa 
rissanen stochastic complexity inquiry 
singapore world scientific 
oliver baxter wallace unsupervised learning mml proceedings international conference machine learning pp 
san francisco ca morgan kaufmann 
wallace freeman estimation inference compact coding journal royal statistical society vol 
pp 

wallace dowe minimum message length kolmogorov complexity computer journal vol :10.1.1.17.321
pp 

cutler information ratios validating mixture analysis journal american association vol 
pp 

bezdek pattern recognition fuzzy objective function algorithms 
new york plenum press 
mclachlan bootstrapping likelihood ratio test statistic number components normal mixture journal royal statistical society series vol 
pp 

smyth model selection probabilistic clustering cross validated likelihood tech 
rep uci ics information computer science university california irvine ca 
celeux raftery robert inference model cluster analysis statistics computing vol 
pp 

mengersen robert testing mixtures bayesian entropic approach bayesian proceedings fifth valencia international meeting bernardo berger dawid smith eds pp 
oxford university press 
roeder wasserman practical bayesian density estimation mixtures normals journal american statistical association vol 
pp 

neal bayesian mixture modeling proceedings th international workshop maximum entropy bayesian methods statistical analysis pp 
dordrecht netherlands kluwer 
richardson green bayesian analysis mixtures unknown number components journal royal statistical society vol 
pp 

rasmussen infinite gaussian mixture model advances neural information processing systems solla leen 
muller eds pp 
mit press 
oliver baxter wallace unsupervised learning mml proc 
th int 
conf 
machine learning san francisco pp 

ueda nakano deterministic annealing em algorithm neural networks vol 
pp 

deterministic annealing density estimation multivariate normal mixtures physical review vol 
pp 

ueda nakano hinton algorithm mixture models neural com putation 
appear 
cover thomas elements information theory 
new york john wiley sons 
schwarz wallace rissanen intertwining themes theories model order estimation international statistical review 
appear 
available www ifp uiuc edu adl monographs html 
conway sloane sphere packings lattices groups 
new york springer verlag 
bernardo smith bayesian theory 
chichester uk wiley sons 
figueiredo leit ao jain fitting mixture models energy minimization methods computer vision pattern recognition hancock eds pp 
springer verlag 
dimensionality sample size classification error complexity classification algorithms pattern recognition ieee transactions pattern analysis machine intelligence vol 
pp 

jain small sample size effects statistical pattern recognition recommendations practitioners ieee transactions pattern analysis machine intelligence vol 
pp 

celeux chr forbes component wise em algorithm mixtures tech 
rep inria alpes france 
available www inria fr rr html 
regularisation equations par approximations revue informatique de recherche vol 
pp 

rockafellar monotone operators proximal point algorithm siam journal control optimization vol 
pp 

bertsekas nonlinear programming 
belmont ma athena scientific 
