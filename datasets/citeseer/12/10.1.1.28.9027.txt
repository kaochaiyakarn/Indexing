boosting loss regression classi cation peter uhlmann eth urich bin yu university california berkeley august investigates variant boosting boost constructed functional gradient descent algorithm loss function 
explicit stagewise re tting expression boost case symmetric linear weak learners studied detail regression class classi cation 
particular boosting iteration working smoothing regularization parameter new exponential bias variance trade variance complexity term bounded tends nity 
weak learner smoothing spline optimal rate convergence result holds regression class classi cation 
boosted smoothing spline adapts higher order unknown smoothness 
simple expansion loss function derived reveal importance decision boundary bias reduction impossibility additive bias variance decomposition classi cation 
simulation real data set results obtained demonstrate attractiveness boost particularly novel component wise cubic smoothing spline ective practical weak learner 
boosting successful practical methods come machine learning community 
inception schapire freund freund schapire tried amazing array large data sets 
improved performance weak learner boosting impressive associated boosting resistance tting 
burning question 
rationale boosting separates traditional statistical procedures 
starts sensible estimator classi er weak learner seeks improvements iteratively performance training data set 
possibility boosting procedure comes availability large data sets easily set aside part test set cross validation random splits 
seemingly bypasses need get model data pursuit optimal solution model common practice traditional statistics 
large data set problems model problem hard come sensible procedure 
may explain empirical success boosting large data sets 
bounding generalization error boosted procedure vc dimensions distribution margins schapire developments boosting gradient descent gd view boosting 
results orts researchers breiman mason friedman collins 
gd view connects boosting common optimization view statistical inference obvious consequence emergence variants original adaboost various loss objective functions mason friedman friedman 
satisfactory explanation boosting works follow directly gd view new boosting variants easily accessible analysis 
take advantage new analytic possibility boosting procedures build case understanding boosting regression class classi cation 
worth pointing boost studied procedure yielding competitive results regression class classi cation addition analytical tractability 
brief overview boosting gd point view section section deals case symmetric weak learners regression building known fact boost stagewise re tting residuals cf 
friedman 
derive main rigorous results boosting iteration working smoothing regularization parameter new exponential bias variance trade 
iteration increases term added tted procedure due dependence new term previous terms complexity tted procedure increased constant amount got linear regression exponentially diminishing amount gets large 
iteration limit complexity variance term bounded noise variance regression model 
ii weak learner smoothing spline boost achieves optimal rate convergence dimensional function estimation 
boosted smoothing spline adapts higher order unknown smoothness 
item partially explains tting resistance mystery boosting 
phenomenon radically di erent known algebraic bias variance trade nonparametric regression 
item ii shows interesting result boosting adaptive estimation smoothness unknown boost achieves optimal minimax rate convergence 
section proposes boost novel component wise smoothing spline learner ective procedure carry boosting high dimensional regression problems continuous predictors 
shown outperform boost stumps tree terminal nodes traditional competitors particularly predictor space high dimensional 
section deals class classi cation problem 
optimality item ii holds classi cation boost achieves optimal minimax rate convergence bayes risk appropriate smoothness function class risk best classi cation procedures 
furthermore approximate loss function smoothed version show generalization error procedure approximately addition bayes risk sum tapered moments 
consequence approximation get insight bias plays bigger role loss classi cation regression resistance tting classi cation regression previous attempts successful decomposing generalization error additive bias variance terms cf 
geman breiman 
support theory explanations simulated real data sets demonstrate attractiveness boost particularly presence high dimensional predictors novel component wise cubic spline ective weak learner 
section contains discussion role weak learner summary 
boosting stagewise functional gradient descent boosting algorithms seen functional gradient descent techniques 
task estimate function minimizing expected cost 
data 
consider cases response continuous regression problem discrete classi cation problem boosting potentially useful cases denotes dimensional 
cost function 
assumed smooth convex second argument ensure gradient method works 
prominent examples exp yf adaboost cost function log exp yf logitboost cost function boost cost function population minimizers log jx jx adaboost logitboost cost jx boost cost estimation 
data done functional gradient descent constrained minimization empirical risk gradient descent view recognized re ned various authors including breiman mason 
friedman 
friedman 
summary minimizer imposed satisfy smoothness regularization constraint terms additive expansion weak simple learners estimated nite nite dimensional parameter 
example weak learner 
decision tree describes axis split split points location parameter terminal nodes 
data part weak learner done base algorithm 
example squares tting yields argmin data fu ng 
general description follows cf 
friedman 
generic functional gradient descent step initialization 
data fy ng initial weak learner squares argmin set 
step projecting gradient weak learner 
compute negative gradient vector fm evaluated current fm 
weak learner gradient vector squares argmin step line search 
dimensional numerical search best step size wm argmin fm wm update fm 
fm 
wm 
step iteration 
increase repeat steps 
call fm 
adaboost logitboost boost estimate implementing cost function 
note boost simple structure negative gradient step classical residual vector fm line search step trivial wm 
boosting repeated squares tting residuals cf 
friedman 
boosting step proposed tukey name 
continuous regression estimate jx directly boost estimate fm 
class problem classi er equal misclassi cation costs sign fm jx jx jx 
adaboost aim estimate log jx jx appropriate classi er 
mason 
collins 
describe boosting type algorithms functional gradient descent converge numerically 
tells certain conditions test set generalization error boosting eventually stabilizes 
doesn imply eventually stable solution best tting happen long reaching convergence 
show section boost contracting linear learners converges fully saturated model tting data perfectly 
theory boosting linear learners regression nature stagewise tting responsible large extent boosting resistance tting 
view expressed buja discussion friedman 

amply clear stagewise tting gotten bad reputation statisticians didn get attention deserved 
success boosting de nitely serves eye opener take fresh look stagewise tting 
consider regression model var 
real valued typically nonlinear function deterministic conditioning design 
represent weak learner operator mapping responses tted values absorbed operator notation sequel notation vector vector analogously clear context mean single variable function 
vectors 
proposition 
boost estimate iteration represented fm proof appendix 
de ne boosting operator bm fm proposition relatively direct link boosting operator weak learner exploit sequel 
focus linear weak learners examples include squares tting linear models general projectors class basis functions regression splines smoothing operators kernel smoothing spline estimators 
proposition 
consider linear weak learner eigenvalues ng deterministic eigenvalues boost operator bm ng 
proof direct consequence proposition 
analysis transparent specializing case symmetric 
important example smoothing spline operator see wahba hastie tibshirani data adaptive smoothing technique say kernel global bandwidth 
eigenvalues real bm diagonalized orthonormal transform bm dm diag kth column vector kth eigenvector eigenvalue matrix orthonormal satisfying uu able analyze relevant generalization measure setting expected mean squared error mse fm averages observed 
note design stochastic probability distribution mse measure asymptotically equivalent generalization error fm new test observation design generating distribution independent training set expectation training test set 
show di erence measures nite sample case 
theorem 
consider linear symmetric weak learner eigenvalues ng eigenvectors building columns orthonormal matrix assume data generated model denote vector true regression function 
evaluated 
bias variance averaged mean squared error boost bias fm variance var fm mse bias variance proof appendix 
theorem describes exact result mse terms chosen boost procedure 
clear iteration index acts smoothing parameter control bias variance trade underlying problem learner implying set eigenvalues analyze bias variance trade function boosting iterations 
purpose assume eigenvalues satisfy 
important example linear weak learner cubic smoothing splines eigenvalues equal strictly zero treated detail section 
theorem 
assumptions theorem bias decays exponentially fast increasing variance exhibits exponentially small increase increasing lim mse 
function vector linear space spanned column vectors 
boosting improves mse linear learner 
ii ng identity operator mth iteration boosting mse strictly lower boosting varying df degrees freedom generalization mean squared error solid line mse criterion dotted line simulations model design uniformly distributed 
left boost cubic smoothing spline having df function boosting iterations right cubic smoothing spline various degrees freedom various amount smoothing 
assertion direct consequence theorem 
proof assertion appendix 
theorem comes surprise shows interesting bias variance trade hasn seen literature 
boosting iteration smoothing parameter varies bias variance complexity term change exponentially bias decreasing exponentially fast variance increasing exponentially diminishing terms eventually 
contrasts standard algebraic trade commonly seen nonparametric estimation 
illustrates di erence cubic smoothing spline learner data model section 
exponential trade gets close optimal mse smoothing splines varying smoothing parameter stays really due exponential increase decrease bias variance terms 
condition part interpreted relatively complex compared linear learner th component direction 
example large right hand side implies close learner employs little shrinkage smoothing kth direction learner strong kth direction 
boosting bring improvement large relative noise level function complex direction 
right hand side condition small learner smoothing shrinkage weak condition easily satis ed 
see improvements boosting weak learners time 
assertion ii shows boosting beats unbiased estimator james stein estimator space spanned space original linear estimator component wise shrinkage estimator james stein estimator uses uniform shrinkage 
phenomenon theorem generalizes qualitatively higher order moments 
theorem 
assumptions theorem assuming ej fm exp cm constant independent depending 
proof appendix 
theorem argue expected loss classi cation exhibits exponentially small amount tting boosting iterations 
boosting theoretical convergence typically advice mse smaller noise level reason boosting nitely yields fully saturated model ts data perfectly 
monitor estimate mse example test set cross validation 
boosting doesn change procedure 
corollary 
assumptions theorem equivalently linear projection operator bm 
smoothing splines weak learners special class symmetric linear learners smoothing spline operators predictors dimensional 
denote function class th order smoothness de ned interval ff dx sy smoothing spline solution penalized squares problem argmin dx theorem 
optimality boost smoothing splines 
suppose smoothing spline learner degree corresponding xed smoothing parameter true function achieves optimal minimax rate smoother function class terms mse 
proof appendix 
result states rst boosting smoothing splines minimax optimal smoothness class second boosting adapts higher order smoothness 
gu analyzes shows adapt higher order smoothness 
boosting adapt arbitrarily higher order smoothness re times want 
cubic smoothing spline learners optimal rate achieved 
underlying smoothness say boosted cubic smoothing spline achieve optimal rate smoother class 
practice optimal boosting iteration selected xed test set cross validation random splits original data set 
note boosting adapts lower order smoothness true smoothing splines boosting 
boosting ering case 
smoothness ordinary smoothing spline boosting achieve optimal rate trade bias variance di erent regularization paths 
algebraic exponential 
advantage new exponential trade near optimal region optimal smoothing parameter boosting iteration bounded complexity variance smoothing parameter values boosting iterations 
example shown simulated data section 
simulation results cubic smoothing spline weak learners relevance theoretical results depends underlying problem sample size 
consider representative example model sin weak learner chosen cubic smoothing spline satis es linearity symmetry eigenvalue conditions theorems 
complexity strength weak chosen terms called degrees freedom df equals trace hastie tibshirani 
study interaction weak learner underlying problem model cubic smoothing spline learner df 
decrease learner complexity shrinkage friedman replace smoothing spline estimator shrinkage small corresponds linear operator eigenvalues closer zero original small get weaker learner original shrinkage acts similarly changing degrees freedom original lower value 
see ect complex examples sections 
boosting question weak small boosted large achieve optimal performance de ned numerical search estimators rising di erent shrinkage factors di erent iterations boosting 
displays mse speci cation realizations function 
shows boosting question positive answer case 
observe example boosting large number iterations potential weak learner small optimal compared best shrunken learner opt consistent asymptotic result theorem 
provided learner suciently weak boosting improves show theorem 
initial degrees freedom strong small amount smoothing boosting decreases performance due tting 
shrinkage shrinkage shrinkage shrinkage traces mse function boosting iterations di erent shrinkage factors 
dotted line represents minimum achieved 
data model sample size 
boosting weak learners relatively safe provided number iterations large mse low large number iterations statements numerically example linear weak learner shown hold empirically data sets nonlinear learners adaboost logitboost classi cation 
statement dealing optimality asymptotically explained theorem 
jiang hints consistency result adaboost shows asymptotic sense classi cation adaboost visits nearly optimal nearly bayes procedures evolution adaboost 
asymptotic consistency result doesn explain optimality adaboost nite sample performance 
behaviors happen convergence boosting algorithm 
having perfect knowledge mse boosting iteration simulation example suitable stopping criterion relative di erence mse current previous iteration 
upper bound say relative di erence need resulting mse stopping 
need mse stopping 
far stabilizing mse see theorem 
positive aspect reaching extreme tting situation 
little illustration describes impressively slow convergence boosting 
illustrate adaptivity boost smoothing splines higher smoothness see theorem 
simulated data model realizations uniform sample sizes 
table reports sample size optimal smoothing spline optimal boost gain 









table generalization mean squared error simulated data 
optimal cubic smoothing spline best penalty parameter optimal boost smoothing spline best number boosting iterations 
positive gain measures relative improvement mean squared error boost indicates advantage boosting 
performance best cubic smoothing spline optimal penalty parameter comparison boosting cubic smoothing spline xed penalty constant optimal number boosting iterations 
evaluate generalization error averaging simulations model 
observe table 
smoothing spline learner boost strong best performance boosting 
mid range di erences optimal smoothing spline optimal boost negligible 
large sample size see advantage boost consistent theory underlying regression function nitely smooth boost adapts higher order smoothness 
boosting regression high dimensions dimension predictor space large learner typically nonlinear 
high dimensions necessity learner doing sort variable selection prominent examples trees 
component wise smoothing spline weak learner alternative tree learners terminal nodes stumps propose component wise smoothing splines 
component wise smoothing spline de ned smoothing spline selected explanatory variable dg argmin smoothing spline de ned 
component wise smoothing spline learner function 
boosting stumps component wise smoothing splines yields additive model terms tted stagewise fashion 
reason additive combination stump component wise smoothing spline depending functionally component dg re expressed estimated functions 
boosting tted stagewise fashion di erent back tting estimates additive models cf 
hastie tibshirani 
boosting stumps component wise smoothing splines particularly attractive aiming additive model high dimensions larger order sample size boosting greater exibility add complexity stagewise fashion certain components dg may drop variables components show section dimension large boosting outperforms alternative classical additive modeling variable selection back tting 
numerical results rst consider real data set ozone concentration los angeles basin analyzed breiman 
dimension predictor space sample size 
compare boosting classical additive models back tting mars 
boost stumps component wise cubic smoothing splines having degrees freedom cf 
hastie tibshirani additive model back tting smoothing splines default plus mars run default parameters implemented plus library mda 
estimate generalization mean squared error error jx randomly splitting data training test observations averaging times random partitions 
table displays results 
conclude boost method mean squared error boost component wise spline boost stumps additive model back tted mars table test set mean squared errors ozone data 
boost optimal number boosting iterations parentheses 
component wise spline cubic smoothing spline df 
component wise splines better trees best classical back tting additive models 
show simulated example high dimensions relative sample size boost stagewise method better back tting additive models variable selection 
simulation model sin unif unif independent unif components unif samples size generated simulation pairs model 
methods 
classical additive modeling forward variable selection inclusion strategy large compared 
evaluate jx true conditional expectation done simulations note jx random depending realizations random coecients stump appeared strong boost shrunken learners chosen ad hoc allow non boosting procedures shrunken 
table shows average performance simulations model 
boost method mean squared error boost shrunken component wise spline boost shrunken stumps additive model back tted forward selection shrunken additive model back tted forward selection mars shrunken mars table generalization mean squared errors simulated data 
boost optimal number boosting iterations parentheses additive model optimal number selected variables parentheses 
component wise spline cubic smoothing spline df shrinkage factor 
winner additive models mars component wise spline better learner stumps 
mainly high dimensional situations boosting clear advantage exible nonparametric methods examples dimension mid range relative sample size 
boosting class problem classi cation evaluating performance criterion boosting procedure misclassi cation rate 
consider training sample new test observation independent training sample having distribution 
classi ers form misclassi cation rate expressed terms estimated margin fm fm fm usual random variables training set testing observation speci ed 
insights expected zero loss function gained approximating theoretical purposes smoothed version fm exp exp parameter controls quality approximation 
proposition 
assume joint distribution fm density bounded neighborhood zero 
jp fm fm log proof appendix 
proposition shows misclassi cation rate approximated expected cost function nitely di erentiable 
misclassi cation rate tapered moments margin proposition motivates study misclassi cation rate fm 
applying taylor series expansion 
margin true 
obtain 
derivatives 
exp jzj conditioning test observations moments expressed yf yjx dp fm expectation training set px 
denotes distribution see generalization error procedure approximately addition approximate bayes risk sum moments tapered yf 
small value ensure approximation proposition tapering weights decay quickly yf moves away zero 
exploits di erent view known fact behavior neighborhood classi cation boundary fx matters misclassi cation rate 
may dicult observe tting test data test data near decision boundary tting hardly visible 
rst terms approximation tapered bias tapered term see 
higher order terms expanded terms interactions centered moments bias term tapered fm fm fm seemingly trivial approximation important consequences 
rst bias tapering rst term multiplicative terms higher moments see plays bigger role loss classi cation regression 
second case boosting tapered centered moment terms bounded expressions exponentially diminishing increments boosting iterations gets large see section particularly theorem driving tapered bias zero boosting iterations exponentially fast mild increasing ect terms summation exponentially small worthwhile lowering generalization error 
third consequence approximation explain previous attempts successful decomposing generalization error additive bias variance terms cf 
geman breiman 
rst terms important terms include bias term multiplicative fashion see term summation pure additive way 
conclude heuristically exponentially diminishing centered moment increase number boosting iterations stated theorem tapering smoothed loss yield strong tting resistance performance boosting classi cation 
acceleration classi cation noise seen section resistance tting closely related behavior 
classi cation boundary 
true 
moves away quickly classi cation boundary fx relevant tapering weights yf decay fast 
measured grad gradient zero 

said large acceleration gradient large element wise absolute values euclidean norm 
large acceleration 
result strong resistance tting boosting 
noise negatively ects acceleration 
noise model called classi cation noise thought constructive way 
consider random variable independent 
noisy response variable wy changing sign probability 
conditional probability easily seen jx jx denote 
noisy version 
jx replacing jx 
half log odds ratio conditional expectation see 
noise noise noise noise test set misclassi cation rates mcr logitboost stumps 
breast cancer data di erent noise levels described 
straightforward calculation shows grad noisier problem smaller acceleration 
resistance tting tapering weights larger noisy problems 
adds deeper insight known empirical fact boosting doesn noisy problems see dietterich 
reason tting kicks early learners strong noisy problems 
logitboost friedman ect demonstrated breast cancer data www ics uci edu mlearn mlrepository analyzed 
see stump strong logitboost noise added breast cancer data 
course adding noise classi cation problem harder 
optimal bayes classi er 
noisy problem misclassi cation rate relating bayes classi er original non noisy problem 
easily derived 
high noise expression largely dominated constant term indicating isn gain clever classi er say close optimal bayes naive 
relative improvement demonstrate better performance powerful classi er benchmark dramatic due high noise level causing large misclassi cation benchmark rate 
boost classi cation true margin referred lo lo log jx emphasize notation lo 
denote half log odds ratio 
reasonable loss functions classi cation functions margin 
example exp lo exponential loss adaboost log exp lo negative log likelihood logitboost 
loss function margin minimizer jx written function margin lo stated proposition 
proposition 
jy jx jy exp lo proof appendix 
friedman 
called squared error 
loss functions margin yf exponential log likelihood various loss functions lo shows loss competitive approximation loss relative negative log likelihood exponential loss particularly margin takes large negative values strong misclassi cations 
near classi cation boundary small positive margins loss tighter bound exponential loglikelihood small negative margins reversed 
loss exponential log likelihood loss bound loss 
estimating jx classi cation seen estimating regression function model independent mean zero variables variance 
variances bounded arguments regression case modi ed give optimal rates convergence results estimating theorem 
optimality boost smoothing splines classi cation 
consider case dimensional predictor suppose see smoothing spline linear learner degree corresponding xed smoothing parameter achieves optimal minimax rate smoother function class estimating 
terms mse 
follows underlying belongs smooth function class boosting achieves bayes risk 
view results marron theorem implies boost smoothing splines gives minimax optimal rates convergence classi cation problem global smoothness class de ned min fn max fp classi cation rule observations 
tsybakov consider di erent function classes locally constrained near decision boundary show faster parametric rate achieved 
local constrained classes natural classi cation setting loss seen smoothed loss expansion section actions happen near decision boundary 
new rates achieved avoiding plug classi cation rules estimating going directly empirical minimizations loss function regularized classes decision regions 
computationally minimization dicult 
remains open boosting achieve new optimal convergence rates tsybakov 
comparing boost logitboost real data sets addition boost algorithm propose version called boost constraints 
proceeds boost fm constrained natural target jx 
algorithm step 

set 
step 
compute fm 
residuals update fm 
fm 

fm sign fm min fm note fm updating step boost algorithm 
step 
increase go back step 
compare boost logitboost tree learners component wise smoothing spline learner section 
consider rst variety class problems uci machine learning repository www ics uci edu mlearn mlrepository breast cancer ionosphere monk full data set dataset learner boost logitboost breast cancer stumps comp 
spline sonar stumps comp 
spline ionosphere stumps heart costs stumps australian credit stumps monk larger tree table test set misclassi cation rates boost constraints logitboost 
optimal number boosts parentheses optimum unique minimum 
larger tree denotes tree learner ancestor nodes terminal leaves contain observations 
heart sonar australian credit data sets statlog project 
monk bayes error equal zero 
estimated test set misclassi cation rates average random divisions training test set data table 
comparison optimal number boosting boosting algorithm numbers parentheses 
component wise learner breast cancer data ordinal predictors sonar data continuous predictors 
spline smoothing sense empirically improves tree stumps 
performs bit better boost half data sets boost better 
logitboost better boost better data sets small amount 
biggest di erence performance sonar data extreme ratio dimension sample size data set di erence far signi cant sample size small 
consider simulated data compare slightly better procedures logitboost algorithm 
allows accurate comparison choosing large test sets 
generate data classes model friedman 
non additive decision boundary jx bernoulli log training sample size 
interesting consider performance single training test data resembles situation practice 
purpose choose large test set size variability test set error training data small 
additionally consider average performance independent realizations model choose test set size large compared training sample size 
set friedman 

displays results single data set 
logitboost perform equally 
slight advantage larger tree learner 
pointed friedman 
stumps aren stumps single data set logitboost larger tree single data set logitboost test set misclassi cation rates mcr single realization model 
top stumps learner 
bottom larger tree learner 
larger tree simulations logitboost test set misclassi cation rates mcr averaged realization model 
larger tree learner 
learners true decision boundary non additive 
stumps learners optimally stopped logitboost tiny advantage estimated standard errors test set error estimate training data 
larger trees boost substantial advantage logitboost standard errors data 
shows averaged performance independent realizations larger trees weak learner indicates substantial advantage single data represented 
independent realizations test optimally stopped boosting algorithm yields signi cantly better test set misclassi cation rate 
simulations optimally stopped better logitboost 
values testing hypothesis equal performance sided alternative table 
test value wilcoxon sign table comparison logitboost sided testing equal test set performance 
low values favor 
model friedman 
nd signi cant advantage logitboost 
discussion concluding remarks seen section see boosting successful learner weak 
learning procedure strong rst boosting iteration mse improved original 
classi cation case generalization error increase number boosting steps stabilizing eventually due numerical convergence boosting algorithm fully saturated model linear learners eigenvalues bounded away zero 
course weakness learner depends underlying problem data generating distribution see assertion theorem 
hard decide priori learner weak posteriori estimate generalization error boosting obtain information strength learner 
degree weakness learner increased additional shrinkage shrinkage factor 
boost dimensional predictors asymptotic results section saying boosting weak smoothing splines better optimal smoothing spline strong learner boosting adapts unknown smoothness 
high dimensions see practical advantages boosting particularly dimension predictor space large relative sample size exempli ed section 
fact boost depends critically strength learner empirically true versions boosting 
show results logitboost trees projection pursuit learners ridge function term breast cancer data 
tree learners suciently weak problem shrunken large tree best 
interestingly projection pursuit learner strong boost tree learners ppr learners test set misclassi cation rates mcr logitboost breast cancer data 
top decision tree learners stumps solid line large unpruned tree dotted line shrunken large unpruned tree dashed line shrinkage factor 
bottom projection pursuit ppr learners term ppr solid line shrunken term ppr dotted line shrinkage factor 
ing doesn pay matches intuition projection pursuit exible strong 
boosting projection pursuit second time reduces performance estimate improved boosting 
eventually misclassi cation curve shows tting stabilizes relatively high value 
presumably boosting estimate overshoots second functional gradient descent step learner strong 
additional boosts correct increase performance 
observed similar pattern projection pursuit learners simulated example allows accurate estimation misclassi cation rate 
phenomenon typically observed tree learners inconsistent theoretical arguments 
empirical studies boosting literature tree structured learner 
complexity low tree procedures tted stagewise selection variables split points cart tree algorithms complexity higher stronger learner tree tted global search split points course infeasible 
example local greedy method leading low increase complexity similarly analyzed boosting 
predictor space high dimensional substantial amount variable selection done tree procedure particularly helpful leading low complexity weak learner 
feature may desirable 
mainly investigated boost advantage analytical tractability demonstrated practical ectiveness 
particular showed 
boost appropriate regression classi cation 
leads competitive performance classi cation relative logitboost 

boost stagewise tting procedure iteration acting smoothing regularization parameter true boosting algorithms 
linear weak learner case controls new exponential bias variance trade 
boost smoothing splines results optimal minimax rates convergence regression classi cation 
algorithm adapts unknown smoothness 

weighting observations boost doubt success general boosting algorithms due giving large weights heavily misclassi ed instances freund schapire conjectured adaboost 
weighting adaboost logitboost comes consequence choice loss function reason successes 
interesting note breiman conjecture case adaboost weighting reason success 

simple expansion smoothed loss reveals new insights classi cation problem particularly additional resistance boosting tting 

boosting learners involve selected predictor variable yields additive model propose component wise cubic smoothing splines type better learners tree structured stumps especially continuous predictors 
trevor hastie leo breiman helpful discussions 
partial support yu gratefully acknowledged national science foundation dms fd army research oce daag daad 
appendix proof proposition 
boost cost function negative gradient stage classical residual vector need line search su su implying sy obtain fm telescope sum argument equals proof theorem 
bias term bias orthonormality bm dm orthonormality formula bias follows 
variance consider cov bmb orthonormality variance tr cov orthonormality proof theorem 
assertion immediate consequence theorem 
loss generality assume restrict summation satisfy 
denote mse bias variance easy derive log follows log log negative inequality condition condition 
means boosting improves 
ii rewrite exists 
follows obvious proof theorem 
write summands higher order moment fm fm fm fm bias term 
transformed higher order moment sum higher order centered moments bias multiplier goes zero 
centered moments written fm fm bm assertion proposition 
map matrix values exponentially close zero obtain fm fm exp constant 
bound fact bias exp constant see theorem complete proof 
proof theorem 
smoothing spline operator corresponding smoothness smoothing parameter avoid notational confusion eigenvalues 
known cf 
wahba eigenvalues take form decreasing order nq nq large ak aq universal depends asymptotic density design points true function bias term bounded follows 
bias max max exp log log log derivative gives positive integer increasing exp exp exp exp putting get bias mn order deal variance term 
variance fr ax follows dx take 
choice variance bound bias get mse minimized 
minimized mse minimax optimal rate smoother function class proof proposition 
denote rst show sup jzj log jc symmetry suces consider sup log jc log exp log proving 
hand jzj log jc jg dz sup jg log log je jzj log jzj log jc jg dz log log proof proposition 
minimizer loss minimized loss yf exp exp give exp exp exp exp exp exp exp give exp exp exp exp exp buja 

comment additive logistic regression statistical view boosting 
ann 
statist 

breiman 

arcing classi ers 
ann 
statist 

breiman 

prediction games arcing algorithms 
neural computation 
breiman 

nity theory predictor ensembles 
tech 
report dept statist univ calif berkeley 
collins schapire singer 

logistic regression adaboost bregman distances 
proc 
thirteenth annual conference computational learning theory 
dietterich 

experimental comparison methods constructing ensembles decision trees bagging boosting randomization 
machine learning 
freund 

boosting weak learning algorithm majority 
information computation 
freund schapire 

experiments new boosting algorithm 
machine learning proc 
thirteenth international conference pp 

morgan kau man san francisco 
friedman 

greedy function approximation gradient boosting machine 
appear annals statist 
friedman hastie tibshirani 

additive logistic regression statistical view boosting 
annals statist 
discussion 
geman bienenstock doursat 

neural networks bias variance dilemma 
neural computations 
gu 

happens bootstrapping smoothing spline 
commun 
statist 
part theory meth 

hastie tibshirani 

generalized additive models 
chapman hall 
jiang 

process consistency adaboost 
tech 
report dept statistics northwestern university 
tsybakov 

smooth discriminant analysis 
ann 
statist 

marron 

optimal rates convergence bayes risk nonparametric discrimination 
ann 
statist 

mason baxter bartlett frean 

functional gradient techniques combining hypotheses 
advances large margin classi ers 
mit press 
schapire 

strength weak learnability 
machine learning 
schapire freund bartlett lee 

boosting margin new explanation ectiveness voting methods 
ann 
statist 

tukey 

exploratory data analysis 
addison wesley reading ma 


natural spline functions associated eigenvalue problem 
numer 
math 

wahba 

spline models observational data 
soc 
industrial applied mathematics 

