aij special issue relevance wrappers feature subset selection ron kohavi data mining visualization silicon graphics shoreline blvd mountain view ca ronnyk sgi com george john computer science department stanford university stanford ca cs stanford edu robotics stanford edu august feature subset selection problem learning algorithm faced problem selecting relevant subset features focus attention ignoring rest 
achieve best possible performance particular learning algorithm particular domain feature subset selection method consider algorithm training data interact 
explore relation optimal feature subset selection relevance 
wrapper method searches optimal feature subset tailored particular algorithm domain 
study strengths weaknesses wrapper approach show improvements original design 
compare wrapper approach induction feature subset selection relief filter approach feature subset selection 
significant improvement accuracy real problems achieved families induction algorithms decision trees naive bayes 
universal problem intelligent agents face focus attention 
problem solving agent decide aspects problem relevant expert system designer decide features rules forth 
learning agent learn experience discriminating relevant irrelevant parts experience ubiquitous problem 
supervised machine learning induction algorithm typically set training instances instance described vector feature attribute values class label 
example medical diagnosis problems features include age weight blood pressure patient class label indicate physician determined patient suffering heart disease 
task induction algorithm inducer induce classifier useful classifying cases 
classifier mapping space feature values set class values 
feature subset selection problem learning algorithm faced problem selecting subset features focus attention ignoring rest 
wrapper approach proposed john kohavi pfleger feature subset selection algorithm exists wrapper induction algorithm 
feature subset selection algorithm conducts search subset induction algorithm part function evaluating feature subsets 
idea wrapper approach shown simple induction algorithm considered black box 
induction algorithm run dataset usually partitioned internal training holdout sets different sets features removed data 
feature subset highest estimated value chosen final set run induction algorithm 
resulting classifier evaluated independent test set search 
typical goal supervised learning algorithms maximize classification accuracy unseen test set adopted goal guiding feature subset selection 
trying maximize accuracy tried identify feature relevant features learning 
think goals equivalent show examples problems differ 
organized follows 
section review feature subset selection problem investigate notion relevance define task finding optimal features describe filter wrapper approaches 
section investigate search engine search feature subsets show greedy search hill climbing inferior best search 
section modify organization search space improve running time 
section contains global comparison best methods 
section discuss potential problem approach overfitting suggest theoretical model generalizes problem section 
related section discussed section conclude summary section 
feature subset selection variable elimination sorted decades assisted high speed computing time come move problems 

discussion miller section look problem finding feature subset relation set relevant features 
show problems existing definitions relevance show partitioning relevant features families weak strong helps understand issue better 
examine general approaches feature subset selection filter approach wrapper approach investigate detail 
problem practical machine learning algorithms including top induction decision tree algorithms id quinlan quinlan cart breiman friedman olshen stone instance algorithms ibl dasarathy aha kibler albert known degrade training set training set induction algorithm performance hypothesis test set final evaluation estimated accuracy estimation feature selection search feature set feature evaluation induction algorithm feature set feature set wrapper approach feature subset selection 
induction algorithm black box subset selection algorithm 
performance prediction accuracy faced features necessary predicting desired output 
algorithms naive bayes langley iba thompson duda hart robust respect irrelevant features performance degrades slowly irrelevant features added performance may degrade fast correlated relevant features added 
example running default mode monk problem thrun irrelevant features generates tree interior nodes test irrelevant features 
generated tree error rate reduced relevant features 
aha noted ib storage requirement increases exponentially number irrelevant attributes ib nearest neighbor algorithm attempts save important prototypes 
performance likewise degrades rapidly irrelevant features 
problem feature subset selection finding subset original features dataset induction algorithm run data containing features generates classifier highest possible accuracy 
note feature subset selection chooses set features existing features construct new ones feature extraction construction kittler rendell 
purely classification theoretical standpoint question features interest 
bayes rule bayes classifier rule predicts probable class instance full distribution assumed known 
accuracy bayes rule highest possible accuracy theoretical interest 
optimal bayes rule monotonic adding features decrease accuracy restricting bayes rule subset features advised 
learning scenarios face problems learning algorithms access underlying distribution practical algorithms attempt find hypothesis approximating np hard optimization problems 
problem closely related bias variance tradeoff geman bienenstock doursat kohavi tradeoff estimation parameters bias reduction accurately estimating parameters variance reduction 
problem independent computational power available learner 
second problem finding best hypothesis usually intractable poses added computational burden 
example decision tree induction algorithms usually attempt find small tree fits data finding optimal binary decision tree np hard rivest hancock 
neural networks problem harder problem loading node neural network training set np hard nodes compute linear threshold functions judd blum rivest 
problems define optimal feature subset respect particular induction algorithm account heuristics biases tradeoffs 
problem feature subset selection reduced problem finding optimal subset 
definition inducer dataset features xn distribution labeled instance space optimal feature subset opt subset features accuracy induced classifier maximal 
optimal feature subset need unique may possible achieve accuracy different sets features features perfectly correlated replaced 
definition get highest possible accuracy best subset feature subset selection algorithm select optimal feature subset 
main problem definition learning algorithms algorithm access underlying distribution estimate classifier accuracy data 
relevance features important question relation optimal features relevance 
section definitions relevance suggested literature 
show single example definitions give unexpected answers suggest degrees relevance needed weak strong 
existing definitions almuallim dietterich define relevance assumptions features label boolean noise 
definition feature said relevant concept appears boolean formula represents irrelevant 
gennari langley fisher section allow noise multi valued features define relevance definition relevant iff exists definition relevant knowing value change estimates class label words conditionally dependent note definition fails capture relevance features parity concept unlabeled instances equiprobable may changed follows 
fx gamma xm set features denote value assignment features definition relevant iff exists definition relevant probability label features change eliminate knowledge value definition relevant iff exists definition relevant irrelevant definition definition definition definition table feature relevance correlated xor problem definitions 
example shows definitions give unexpected results 
example correlated xor features boolean 
instance space negatively correlated respectively possible instances assume equiprobable 
deterministic target concept phi phi denotes xor note target concept equivalent boolean expression phi features irrelevant strongest possible sense 
indispensable fx disposed 
table shows definition features relevant 
definition clearly irrelevant irrelevant replaced negation 
definition features irrelevant output value feature value instances agree values 
definition feature relevant knowing value changes probability possible instances zero 
definition clearly irrelevant irrelevant add information respectively 
simple negative correlations occur domain constraints create similar effect 
nominal feature color encoded input neural network customary local encoding value represented indicator feature 
example local encoding valued nominal fa dg 
encoding single indicator feature redundant determined rest 
definitions relevance declare indicator features irrelevant 
strong weak relevance claim degrees relevance required weak strong 
relevance defined terms bayes classifier optimal classifier problem 
feature strongly relevant removal result performance deterioration optimal bayes classifier 
feature weakly relevant strongly relevant exists subset features performance bayes classifier worse performance fxg 
feature irrelevant strongly weakly relevant 
definition repeated defines strong relevance 
strong relevance implies feature indispensable sense removed loss prediction accuracy 
weak relevance implies feature contribute prediction accuracy 
definition strong relevance feature strongly relevant iff exists definition formalization statement features relevant values vary systematically category membership 
general definitions applicable discrete features extended continuous features changing 
definition weak relevance feature weakly relevant iff strongly relevant exists subset features exists feature relevant weakly relevant strongly relevant irrelevant 
example feature strongly relevant features weakly relevant irrelevant 
relevance optimality features bayes classifier strongly relevant features core possibly weakly relevant features 
classifiers induced data suboptimal access underlying distribution furthermore may restricted hypothesis spaces utilize features see example 
practical induction algorithms generate classifiers may benefit omission features including strongly relevant features 
relevance feature imply optimal feature subset somewhat surprisingly irrelevance imply optimal feature subset example 
example relevance imply optimality universe possible instances boolean features say distribution universe uniform assume target concept reasonable definition relevance features relevant target function 
hypothesis space space monomials conjunctions literals optimal feature subset fx accuracy monomial highest accuracy achievable hypothesis space 
adding feature decrease accuracy 
example shows relevance strong relevance imply feature optimal feature subset 
example section hiding features id improves performance know strongly relevant artificial target concept monk 
question irrelevant feature optimal feature subset 
example shows may true 
example optimality imply relevance assume exists feature takes value 
definitions relevance described feature irrelevant 
consider limited perceptron classifier rosenblatt minsky papert associated weight feature classifies instances linear combination greater zero threshold fixed zero 
contrast regular perceptron classifies instances depending linear combination greater threshold necessarily zero 
extra feature set limited perceptron equivalent representation power regular perceptron 
removal irrelevant features remove crucial feature 
section show interesting problem filter approach naive bayes 
artificial datasets represents symmetric target function implying features ranked equally filtering method 
naive bayes improves single feature removed 
believe cases depicted example rare practice irrelevant features generally removed 
important realize relevance definitions imply membership optimal feature subset irrelevance imply feature optimal feature subset 
subset selection feature input features algorithm induction feature filter approach features filtered independently induction algorithm 
filter approach number different approaches subset selection 
section review existing approaches machine learning 
refer reader section related statistics pattern recognition 
reviewed methods feature subset selection follow filter approach attempt assess merits features data ignoring induction algorithm 
filter approach shown selects features preprocessing step 
main disadvantage filter approach totally ignores effects selected feature subset performance induction algorithm 
review existing algorithms fall filter approach 
focus algorithm focus algorithm almuallim dietterich almuallim dietterich originally defined noise free boolean domains exhaustively examines subsets features selecting minimal subset features sufficient determine label value instances training set 
preference small set features referred min features bias 
bias severe implications applied blindly regard resulting induced concept 
example medical diagnosis task set features describing patient include patient social security number ssn 
assume features ssn sufficient determine correct diagnosis 
focus searches minimum set features pick ssn feature needed uniquely determine label ssn induction algorithm expected generalize poorly 
relief algorithm relief algorithm kira rendell kira rendell kononenko assigns relevance weight feature meant denote relevance feature target concept 
relief randomized algorithm 
samples instances randomly training set updates relevance values difference selected instance nearest instances opposite class near hit near 
relief algorithm attempts find weakly relevant features relief help redundant features 
features relevant concept select fraction necessary concept description kira rendell page 
real domains features high correlations label weakly relevant removed relief 
simple parity example kira rendell kira rendell strongly relevant irrelevant features relief strongly relevant features time 
nearest neighbors hurt weakly relevant features naive bayes affected feature replicated affect posterior twice 
relief algorithm motivated nearest neighbors specifically similar types induction algorithms 
preliminary experiments significant variance relevance rankings relief 
relief randomly samples instances neighbors training set answers gives unreliable high number samples order times number cases true ssn encoded binary features long binary features required determine diagnosis 
specifically real valued attributes bits precision inferior scheme 
relief focus totally irrelevant features strongly relevant features weakly relevant features view feature set relevance 
training set 
worried variance implemented deterministic version relief uses instances nearest hits nearest misses instance 
example nearest instances equally close instance average contributions picking 
gives results expect relief run infinite amount time requires time standard relief algorithm number samples equal size training set 
longer worried high variance call deterministic variant relieved 
handle unknown values setting difference unknown values difference unknown known value 
relief originally described run binary classification problems relief method described kononenko generalizes relief multiple classes 
combined relief deterministic enhancement yield final algorithm relieved 
experiments features relevance rankings removed 
feature filtering decision trees cardie decision tree algorithm select subset features nearest neighbor algorithm 
decision tree typically contains subset features appeared final tree selected nearest neighbor 
decision tree serves filter nearest neighbor algorithm 
approach worked datasets major shortcomings 
features decision trees necessarily useful nearest neighbor 
relief expects totally irrelevant features probably major effect led improvements datasets studied 
nearest neighbor algorithm take account effect relevant features current methods building decision trees suffer data fragmentation splits number instances exhausted 
tree approximately balanced number training instances subtree approximately decision tree test lg features path 
summary filter approaches shows set features focus relief search 
focus searching minimal set features relief searches relevant features weak strong 
filter approaches feature subset selection take account biases induction algorithms select feature subsets independent induction algorithms 
cases measures devised algorithm specific may computed efficiently 
example linear regression measures mallows press prediction sum squares wasserman correlated irrelevant corral dataset top decision tree algorithms picking correlated feature root causing fragmentation turns causes irrelevant feature chosen 
devised require running regression times avoid cross validation step default wrapper setup 
measures relevance measure assigned relief appropriate feature subset selectors naive bayes cases performance naive bayes improves removal relevant features 
corral dataset artificial dataset john gives possible scenario filter approaches fail miserably 
instances boolean domain 
target concept feature named irrelevant uniformly random feature correlated matches class label time specific instances 
greedy strategies building decision trees pick correlated feature best known selection criteria 
wrong root split instances fragmented instances subtree describe correct concept 
shows decision tree induced 
cart induces similar decision tree correlated feature root 
feature removed correct tree 
correlated feature highly correlated label filter algorithms generally select 
wrapper approaches hand may discover feature hurting performance avoid selecting 
examples discussion relevance versus optimality section show feature selection scheme take induction algorithm account done wrapper approach 
wrapper approach wrapper approach shown feature subset selection done induction algorithm black box knowledge algorithm needed just interface 
feature subset selection algorithm conducts search subset induction algorithm part evaluation function 
accuracy induced classifiers estimated accuracy estimation techniques described kohavi 
problem investigating state space search different search engines investigated sections 
wrapper approach conducts search space possible parameters 
search requires state space initial state termination condition search engine ginsberg russell norvig 
section focuses comparing search engines way search space possible parameters 
eval eval eval avg test training set train feature subset induction induction induction cross validation method accuracy estimation 
fold cross validation shown 
search space organization chose state represents feature subset 
features bits state bit indicates feature absent 
operators determine partial ordering states chosen operators add delete single feature state corresponding search space commonly stepwise methods statistics 
shows state space operators feature problem 
size search space features impractical search space exhaustively small 
shortly describe different search engines compared 
goal search find state highest evaluation heuristic function guide 
know actual accuracy induced classifier accuracy estimation heuristic function evaluation function see section details problem 
evaluation function fold cross validation repeated multiple times 
number repetitions determined fly looking standard deviation accuracy estimate assuming independent 
standard deviation accuracy estimate cross validations executed execute cross validation run 
heuristic practice avoids multiple cross validation runs large datasets 
heuristic nice property forces accuracy estimation run longer execute cross validation times small datasets takes run large datasets 
small datasets require time learn accuracy estimation time product induction algorithm running time cross validation time grow fast 
conservation hardness heuristic small datasets cross validated times overcome high variance resulting small amounts data 
larger datasets switch holdout heuristic save time factor necessary datasets 
term forward selection refers search begins empty set features term backward elimination refers search begins full set features devijver kittler miller 
initial state experiments empty set features forward selection approach 
main reason choice computational building classifiers features data faster 
theory going backward full set features may capture interacting features easily method extremely expensive add feature delete feature operators 
section introduce compound operators backward elimination approach practical 
summary shows instantiation search problem state space search feature subset selection 
node connected nodes feature deleted added 
state boolean vector bit feature initial state empty set features 
heuristic evaluation fold cross validation repeated multiple times small penalty feature 
search algorithm hill climbing best search termination condition algorithm dependent see complexity penalty added evaluation function penalizing feature subsets features break ties favor smaller subsets 
penalty set small compared standard deviation accuracy estimation aimed 
attempts set value optimally specific datasets 
simply added pick smallest feature subsets estimated accuracy 
search engine section evaluate different search engines wrapper approach 
description experimental methodology rest 
describe hill climbing greedy search engine show terminates local maxima 
best search engine show works better 
experimental methodology describe datasets chose algorithms experimental methodology 
datasets table provides summary characteristics datasets chosen 
datasets corral obtained university california irvine repository murphy aha full documentation datasets obtained 
corral introduced john 
defined table summary datasets 
datasets horizontal line real artificial 
cv indicates fold cross validation 
dataset features 
train test baseline nominal cont classes size size accuracy breast cancer cv cleve cv crx cv dna horse colic cv pima cv sick euthyroid soybean large cv corral monk monk local monk monk 
primary criteria size real datasets instances difficulty accuracy high seeing small number instances age old datasets uc irvine repository chess hypothyroid vote considered possible influence development algorithms 
detailed description datasets considerations kohavi 
small datasets tested fold cross validation artificial datasets large datasets split train test sets artificial datasets defined training set dna dataset form statlog 
baseline accuracy accuracy dataset predicting majority class 
algorithms induction algorithms basis comparisons 
induction algorithms naive bayes induction algorithm 
known machine learning community represent completely different approaches learning hope results general nature generalize induction algorithms 
decision trees documented quinlan breiman 
fayyad buntine moret describe detail 
naive bayes algorithm explained 
specific details essential rest 
algorithm quinlan descendent id quinlan builds decision trees topdown prunes 
tree constructed finding best single feature test conduct root node tree 
test chosen instances split test subproblems solved recursively 
uses gain ratio variant mutual information feature selection measure measures proposed gini index breiman separators fayyad irani distance measures de relief kononenko 
prunes upper bound confidence interval resubstitution error error estimate nodes fewer instances wider confidence interval removed difference error parents significant 
reserve term id run execute pruning step builds full tree nodes split pure impossible split node due conflicting instances 
id induction algorithm really parameters cause full tree grown prune absolutely increase resubstitution error rate 
relatively unknown post processing step replaces node children accuracy child considered better quinlan page 
case corral database described significant impact resulting tree root split incorrect replaced children 
naive bayesian classifier langley duda hart anderson taylor michie uses bayes rule compute probability class instance assuming features conditionally independent label 
formally bayes rule delta label values 
xn xn delta independence delta version naive bayes experiments implemented mlc kohavi sommerfield dougherty 
probabilities nominal features estimated data maximum likelihood estimation 
continuous features discretized minimum description length procedure described dougherty kohavi sahami treated multi valued nominals 
unknown values test instance instance needs labeled ignored participate product 
case zero occurrences label value feature value probability number instances 
approaches possible laplace law succession beta prior cestnik 
approaches probability successes trials estimated parameters beta function 
common choice set estimating probability laplace law succession 
results comparing pair algorithms accuracy results algorithm dataset 
critical understand accuracy results fold cross validation cross validation independent outer loop cross validation inner repeated cross validation part feature subset selection algorithms 
previously researchers reported accuracy results inner cross validation loop results optimistically biased subtle means training test set 
reported accuracies mean accuracies fold cross validation 
show standard deviation mean 
determine difference algorithms significant report values indicate probability algorithm better variance test average variance algorithms normal distribution assumed 
powerful method conduct paired test instance tested fold picture change 
compare algorithms give table accuracies show bar graphs 
bar graph shows absolute difference gamma accuracies second bar graph shows mean accuracy difference divided standard deviation gamma std dev 
length bars standard deviation chart higher results significant confidence level 
comparisons generally algorithm proposed just prior comparison new algorithm standard algorithm previous proposed algorithm 
bar zero proposed algorithm outperforms comparing 
report cpu time results units cpu seconds minutes hours sun sparc single train test sequence 
hill climbing search engine simplest search technique hill climbing called greedy search steepest ascent 
table describes algorithm expands current node moves child highest accuracy terminating table hill climbing search algorithm 
initial state 

expand apply operators giving children 

apply evaluation function child 
child highest evaluation 

goto 
return child improves current node 
table figures show comparison id naive bayes feature subset selection 
table show average number features algorithm averaged folds relevant 
observations ffl real datasets id simple version feature subset selection provides regularization mechanism reduces variance algorithm geman kohavi wolpert 
hiding features id smaller tree grown 
type regularization different pruning regularization method global feature absent pruning local operation 
shown table figures number features selected small compared original set compared selected id 
id average accuracy increases relative reduction error rate 
accuracy uniformly improves real datasets 
ffl artificial datasets id story different 
artificial datasets monk involve high order interactions 
corral dataset correlated feature chosen single addition feature lead improvement hill climbing process stops early similar scenarios happen artificial datasets adding single feature time help 
cases monk local monk zero features chosen causing prediction majority class independent attribute values 
concept monk jacket color green holding sword jacket color blue body shape training set contains instances 
feature subset selection algorithm quickly finds body shape jacket color yield second conjunction expression accuracy 
features larger tree built inferior 
example optimal feature subset different subset relevant features 
ffl real datasets naive bayes average accuracy features 
ffl artificial datasets naive bayes average accuracy degrades corral relative error increases 
require better search hill climbing provide 
interesting observation fact performance monk monk local datasets improves simply hiding features forcing naive bayes predict majority class 
independence assumption inappropriate dataset better predict majority class 
ffl dna dataset algorithms selected features 
selected set differed features indicating crucial types inducers 
table comparison id naive bayes feature subset selection wrapper 
fss suffix indicates algorithm run feature subset selection 
val column indicates probability feature subset selection fss improves id second column indicates probability fss improves naive bayes 
dataset id id fss val naive bayes nb fss val breast cancer sigma sigma sigma sigma cleve sigma sigma sigma sigma crx sigma sigma sigma sigma dna sigma sigma sigma sigma horse colic sigma sigma sigma sigma pima sigma sigma sigma sigma sick euthyroid sigma sigma sigma sigma soybean large sigma sigma sigma sigma corral sigma sigma sigma sigma sigma sigma sigma sigma monk sigma sigma sigma sigma monk local sigma sigma sigma sigma monk sigma sigma sigma sigma monk sigma sigma sigma sigma average real average artif 
dataset id hc fss minus id abs acc acc dataset id hc fss minus id id absolute difference fss minus id accuracy left std devs right 
dataset nb hc fss minus nb abs acc acc dataset nb hc fss minus nb naive bayes absolute difference accuracy left std devs right 
dataset 
features dataset id id hc fss features id number features original dataset left id middle selected hillclimbing feature subset selection right 
dna features shown 
dataset 
features dataset nb hc fss features naive bayes number features original dataset left selected hill climbing feature subset selection right 
table number features dataset number id feature subset selection number selected feature subset selection fss id number selected fss naive bayes 
numbers decimal point single runs number decimal point averages fold cross validation 
dataset number features original dataset id id fss nb fss breast cancer cleve crx dna horse colic pima sick euthyroid soybean large corral monk monk local monk monk results especially artificial datasets know relevant features indicate feature subset selection getting stuck local maxima 
section deals improving search engine 
best search engine best search russell norvig ginsberg robust method hill climbing 
idea select promising node generated far expanded 
table describes algorithm varies slightly standard version explicit goal condition problem 
best search usually terminates reaching goal 
problem optimization problem search stopped point best solution far returned theoretically improving time making anytime algorithm boddy dean 
practice run stage call stale search improved node expansions terminate search 
improved node defined node accuracy estimation ffl higher best far 
experiments set epsilon 
best search general search technique obvious better feature subset selection 
bias variance tradeoff geman kohavi wolpert possible general search increase variance reduce accuracy 
quinlan murthy salzberg showed examples increasing search effort degraded performance 
table figures show comparison id naive bayes hill climbing feature subset selection best search feature subset selection 
table shows average number features algorithm averaged folds relevant 
observations ffl real datasets algorithms id naive bayes difference hill climbing best search 
best search usually finds larger feature subset accuracies approximately 
statistically significant difference naive bayes soybean significant improvement value 
ffl artificial datasets large improvement id 
performance drastically improves datasets corral monk monk local remains monk table best search algorithm 
put initial state open list closed list best initial state 

arg max open get state open maximal 

remove open add closed 

gamma ffl best best 
expand apply operators giving children 

child closed open list evaluate add open list 

best changed expansions goto 
return best 
table comparison hill climbing search best search 
val column indicates probability best search feature subset selection bfs fss improves hill climbing feature subset selection hc fss id second column analogous naive bayes 
dataset id val naive bayes val hc fss bfs fss hc fss bfs fss breast cancer sigma sigma sigma sigma cleve sigma sigma sigma sigma crx sigma sigma sigma sigma dna sigma sigma sigma sigma horse colic sigma sigma sigma sigma pima sigma sigma sigma sigma sick euthyroid sigma sigma sigma sigma soybean large sigma sigma sigma sigma corral sigma sigma sigma sigma sigma sigma sigma sigma monk sigma sigma sigma sigma monk local sigma sigma sigma sigma monk sigma sigma sigma sigma monk sigma sigma sigma sigma average real average artif 
dataset id bfs minus id hc abs acc acc dataset id bfs minus id hc id absolute difference best search fss minus hill climbing fss accuracy left std devs right 
dataset nb bfs minus nb hc abs acc acc dataset nb bfs minus nb hc naive bayes absolute difference accuracy left std devs right 
table number features dataset number id feature subset selection number selected hill climbing fss id best search fss id analogously naive bayes 
dataset number features original id id fss nb fss dataset hc bfs hc bfs breast cancer cleve crx dna horse colic pima sick euthyroid soybean large corral monk monk local monk monk degrades monk 
analyzing selected features optimal feature subset corral monk monk local monk features relevant ones selected monk correctly led better prediction accuracy 
improvement id fss table dramatic positive absolute difference accuracy translates relative error reduction 
search unable find relevant features 
complexity penalty extra features subsets features tried subsets improved majority prediction ignoring features search considered stale non improving node expansions 
local maxima dataset large current setting best search overcome 
specific experiment conducted determine long take best search find correct feature subset 
stale limit originally set increased node better node zero features predicting majority label value 
stale setting overcame local maximum number 
setting node features accurate majority 
node expansions lead correct feature subset 
nodes evaluated possibilities 
total running time find correct feature subset cpu minutes prediction accuracy 
monk dataset set features chosen accuracy significantly degraded compared hill climbing selected empty feature subset 
significant accuracy difference performance degraded best search value 
monk concept encoding unsuitable decision trees correct tree built full space contains nodes leaves 
standard training set contains instances impossible build correct tree standard recursive partitioning techniques 
ffl artificial datasets significant improvement naive bayes corral value performance significantly degraded monk value 
rest datasets unaffected 
chosen feature subset corral contained features correlated feature 
known needed limited representation power naive bayes performance correlated feature better performance features 
naive bayes access features accuracy degrades 
dataset example optimal feature subset different induction algorithms known different 
decision trees hurt addition correlated feature performance degrades naive bayes improves feature 
monk dataset degrades performance features head shape body shape smiling jacket color chosen performance better jacket color 
note head shape body shape part target concept representation power naivebayes limited utilize information 
monk dataset id may example search overfitting sense subset slightly improve accuracy estimation accuracy independent test set see section discussion issues overfitting 
datasets monk local monk monk accuracy search hill climbing 
monk dataset improved feature subset 
id search unable find feature subset correct feature subset allows improving accuracy 
monk monk local datasets optimal feature subset empty set 
naive bayes set relevant features yields inferior performance majority inducer naive bayes behaves empty set features 
best search gives better performance hill climbing high level interactions occurring caught search starts empty feature subset stale parameter drastically increased 
alternative approach suffers feature interaction starts full set features running time approach infeasible practice especially features 
running times best search starting empty set features relevant weakly relevant strongly relevant irrelevant relevant features feature features features features weakly relevant delete operator compound operator features state space 
feature subset contains irrelevant feature irrelevant area contains strongly relevant features core region relevant region 
dotted arrows indicate compound operators 
features range minutes cpu time small problems monk monk monk corral hours dna 
section attempt reorder search space dynamically allow search reach better nodes faster backward feature subset selection feasible 
state space compound operators try lily options 
quinlan previous section looked search engines 
section look topology state space dynamically modify accuracy estimation results 
previously described state space commonly organized node represents feature subset operator represents addition deletion feature 
main problem organization search expand generate successors node initial feature subset path best feature subset 
section introduces new way change search space topology creating dynamic operators directly connect nodes considered promising evaluation children 
operators better utilize information available evaluated children 
motivation compound operators comes partitions feature subsets core features strongly relevant weakly relevant features irrelevant features 
optimal feature subset hypothesis space relevant feature subset strongly weakly relevant features 
backward elimination search starting full set features depicted removes feature time expanding children reachable operator expand children node removing single feature 
irrelevant features features delta nodes evaluated 
similar reasoning applies forward selection search starting empty set features 
domains feature subset selection useful features search may prohibitively expensive 
compound operators operators dynamically created standard set children created add delete operators evaluated 
single node expansion state space search dotted arrows indicating compound operators 
root children nodes highest evaluation values followed 
discarded 
intuitively information evaluation children just node maximum evaluation 
compound operators combine operators led best children single dynamic operator 
depicts possible set compound operators forward selection 
root node containing features expanded applying add operators adding single feature 
operators led combined compound operator shown dashed line going left led nodes highest evaluation evaluation shown 
compound operator led node improved estimate second compound operator shown dashed line going right created combines best original operators formally rank operators estimated accuracy children define compound operator combination best operators 
example compound operator combine best operators 
best operators added feature compound operator add operator added operator deleted try operation 
compound operators applied parent creating children nodes farther away state space 
compound node evaluated generation compound operators continues long estimated accuracy compound nodes improves 
compound operators generalize suggestions previously literature 
kohavi suggested search start set strongly relevant features core 
starts full set features removal single strongly relevant feature cause degradation performance removal irrelevant weakly relevant feature 
compound operator representing combination delete operators connects full feature subset empty set features compound operators full feature subset plot path core set features 
path explored removing feature time estimated accuracy deteriorates 
caruana freitag implemented slash version feature subset selection eliminates features derived decision tree 
features improve performance deleted ignoring orderings due ties compound operators lead node slash take search 
slash approach applicable backward elimination compound operators applicable forward selection 
shows searches compound operators 
compound operators improve search finding nodes higher accuracy faster easy overfit cause nodes crx backward real acc nodes soybean forward real acc comparison compound dotted line non compound solid line searches 
accuracy axis best node determined algorithm independent test set number node evaluations axis 
running time proportional number nodes evaluated 
overfitting earlier see section 
experimental results compound operators similar faster 
significant time differences achieved decision trees pruned 
detailed results case shown table 
main advantage compound operators backward feature subset selection computationally feasible 
table figures show results running best search algorithm compound operators starting full set features backward elimination compared best search forward selection compound operators 
results forward selection compound operators significantly differ file 
table shows number features different methods 
starts full set features feature interactions easier search identify 
observations ffl accuracy results backward fss id generally degraded 
main improvement correct bits correctly identified resulting accuracy 
feature subsets generally larger apparently best search overcome local maxima 
example dna stopped features pruning features improve performance forward search subset features significantly better accuracy estimation feature subset higher feature subset folds best search get feature node prefer final node selected backward search 
section backward search prunes backward search easier best search algorithm 
ffl naive bayes backward fss slight win terms accuracy 
crx accuracy degrade significantly val dna significantly improved val respectively 
fact dna dataset known algorithm outperformed naivebayes selected feature subset 
taylor 
page compared algorithms dataset split train test sets highest ranking rbf radial basis functions centers accuracy 
naive bayes backward elimination accuracy 
ffl dataset naive bayes interesting case 
feature subset selection finds relevant features seventh selected feature irrelevant 
represented hyperplane boolean domain surface represented naive bayes hyperplane turns naive bayes unable learn target concept 
table constructed giving naive bayes possible instances correct classification concept testing instances 
see naive bayes unable learn intriguing fact hiding bit improves accuracy 
table comparison forward best search compound operators backward best search compound operators 
val columns indicates probability backward better forward 
dataset id val naive bayes val bfs fss bfs fss bfs fss bfs fss forward back forward back breast cancer sigma sigma sigma sigma cleve sigma sigma sigma sigma crx sigma sigma sigma sigma dna sigma sigma sigma sigma horse colic sigma sigma sigma sigma pima sigma sigma sigma sigma sick euthyroid sigma sigma sigma sigma soybean large sigma sigma sigma sigma corral sigma sigma sigma sigma sigma sigma sigma sigma monk sigma sigma sigma sigma monk local sigma sigma sigma sigma monk sigma sigma sigma sigma monk sigma sigma sigma sigma average real average artif 
dataset id minus id abs acc acc dataset id minus id id absolute difference best search fss backward compound operators minus forward accuracy left std devs right 
dataset nb minus nb abs acc acc dataset nb minus nb naive bayes absolute difference accuracy left std devs right 
table number features dataset number id feature subset selection number selected best search fss id forward compound backwards compound analogously naive bayes 
dataset number features original id id fss nb fss dataset forward backward forward backward breast cancer cleve crx dna horse colic pima sick euthyroid soybean large corral monk monk local monk monk features naive bayes perceptron accuracy accuracy explanation result follows 
gamma delta gamma delta gamma delta instances label 
gamma delta gamma delta delta ones instances features ones 
get similarly gamma delta features ones giving ones instance probabilities computed naive bayes delta delta delta delta giving label small advantage making wrong prediction 
gamma delta mistakes possible instances exactly accuracy 
features best thing predict label bits naive bayes calculation omitted 
correctly capture instances originally bits continue wrong instances bits 
instances bits bit bits total bits times 
naive bayes gamma mistakes yields accuracy 
table cpu time different versions wrapper approach 
time single fold crossvalidation done outer loop estimate accuracy 
tests compound operators id fss forward 
time command overflowed id fss back dna sun solaris operating system 
command gave negative number execution time 
cpu time seconds id fss id fss fss nb fss dataset forward back back back breast cancer cleve crx dna overflow horse colic pima sick euthyroid soybean large corral monk monk local monk monk example shows hypothesis space naive bayes boolean domains space hyperplanes unable correctly identify target concept perceptron 
interesting fact approach feature subset selection relevance independent induction algorithm ranks feature conditioned label give rank relevant features due symmetry approach pick subset features wrapper approach 
wrapper approach finds optimal subset target concept 
running times backward feature subset selection times longer forward bad considering fact started full set features see section compound operators help 
global comparison id naive bayes basic inducers feature subset selection pruning effect feature subset selection seen clearly 
seen improvements algorithms important remaining question wrapper algorithm developed sections compares filter algorithm feature subset selection versions algorithms compare original versions 
arguments favor wrapper approach section develop high performance wrapper algorithm empirical comparisons purpose preceding sections 
hill climbing wrapper gets stuck local minima best search wrapper took long previous sections necessary experiments section 
compound operators running wrapper faster running wrapper id compound operators tend quickly remove features pruned 
features appear tree removed accuracy estimate change small complexity penalty feature evaluation function improves 
compound operators remove features single node expansion 
pruning features tree node evals dna number features features dna number features evaluated search progresses best search backward 
vertical lines signify node expansion children best node expanded 
slanted line top shows ordinary backward selection progress 
node evals soybean number features features soybean number features evaluated search progresses best search backward table comparison feature selection relieved filter rlf wrapper backward best search compound operators bfs 
val columns indicates probability top algorithm improving lower algorithm 
rlf bfs bfs dataset rlf bfs vs vs vs rlf breast cancer sigma sigma sigma cleve sigma sigma sigma crx sigma sigma sigma dna sigma sigma sigma horse colic sigma sigma sigma pima sigma sigma sigma sick euthyroid sigma sigma sigma soybean large sigma sigma sigma corral sigma sigma sigma sigma sigma sigma monk sigma sigma sigma monk local sigma sigma sigma monk sigma sigma sigma monk sigma sigma sigma average real average artif 
cause slight random variations accuracy estimates 
sense run feature subset selection search backwards done 
figures show number features changes search progresses nodes evaluated 
notice node expansion compound operators applied combine operators leading best children drastically decreasing number nodes 
compound operators number features decrease increase node expansion 
example dna dataset nodes evaluated subset features selected compound operators algorithm expand gamma delta nodes just get feature subset 
running times backward fss slow generally faster backward fss id 
table shows running time different versions algorithms 
comparison original algorithm order magnitude slower 
example running dna dataset takes minutes 
wrapper model run times node evaluated state space dna hundreds nodes 
shall investigate hypotheses filter method improve accuracy id naive bayes real datasets fairly erratic hurting performance second improvements wrapper approach surpass gains filter consistent 
representative filter methods chose relieved algorithm section desirable properties filter algorithms discussed 
reasons outlined preceding paragraphs backward best search wrapper compound operators representative wrapper algorithms 
experimental methodology run compare algorithms described section 
modern algorithm performs variety real databases expect difficult improve performance feature selection 
table shows case accuracy real datasets decreased relieved accuracy slightly increased wrapper relative reduction error 
note relieved perform artificial databases corral contain strongly relevant totally irrelevant attributes 
artificial datasets relieved significantly better plain confidence level 
real datasets relevance ill determined relieved worse table comparison naive bayes nb feature selection relieved filter rlf wrapper backward best search compound operators bfs 
val columns indicates probability top algorithm improving lower algorithm 
nb rlf nb bfs nb bfs dataset nb nb rlf nb bfs vs nb vs nb vs nb rlf breast cancer sigma sigma sigma cleve sigma sigma sigma crx sigma sigma sigma dna sigma sigma sigma horse colic sigma sigma sigma pima sigma sigma sigma sick euthyroid sigma sigma sigma soybean large sigma sigma sigma corral sigma sigma sigma sigma sigma sigma monk sigma sigma sigma monk local sigma sigma sigma monk sigma sigma sigma monk sigma sigma sigma average real average artif 
plain dataset performance significantly worse confidence level case performance better confidence level 
wrapper algorithm significantly better plain real databases artificial databases significantly worse 
note significant improvement real database real dataset features dna 
relieved outperformed wrapper significantly real datasets outperformed wrapper dataset 
corral dataset wrapper selected correct features fa best node early search settled features gave better cross validation accuracy 
training set small instances problem wrapper gave ideal feature set built correct tree accurate pruned back felt training set data insufficient warrant large tree 
surprisingly naive bayes algorithm turned difficult improve feature selection table 
filter wrapper approaches significantly degraded performance breast cancer crx databases 
cases wrapper approach chose feature subsets high estimated accuracy turned poor performers real test data 
filter caused significantly worse performance dataset pima diabetes significantly improved plain naive bayes artificial datasets 
partly due fact severe restricted hypothesis space bias naive bayes prevents doing artificial problems monk reasons discussed section partly naive bayes accuracy hurt conditional dependence features presence irrelevant features 
contrast wrapper approach significantly improved performance databases plain naive bayes accuracy 
monk dataset discarding features 
conditional independence assumption violated obtains better performance naive bayes throwing features marginal probability distribution classes 
wrapper approach significantly improved filter cases significantly outperformed filter approach 
table shows similar results id 
case filter approach significantly degraded performance real dataset significantly improved artificial datasets monk wrapper approach 
monk concept exactly features relevant 
relieved judged table comparison id feature selection relieved filter rlf wrapper backward best search compound operators bfs 
val columns indicates probability top algorithm improving lower algorithm 
id rlf id bfs id bfs dataset id id rlf id bfs vs id vs id vs id rlf breast cancer sigma sigma sigma cleve sigma sigma sigma crx sigma sigma sigma dna sigma sigma sigma horse colic sigma sigma sigma pima sigma sigma sigma sick euthyroid sigma sigma sigma soybean large sigma sigma sigma corral sigma sigma sigma sigma sigma sigma monk sigma sigma sigma monk local sigma sigma sigma monk sigma sigma sigma monk sigma sigma sigma average real average artif 
features irrelevant due poor statistics small training set wrapper internal cross validation gave overly pessimistic estimate node representing subset features optimal 
note id algorithm command line arguments specifying pruning 
happens command line arguments turn tree postprocessing step swap parent decision node child swapping results accuracy corral plain id 
wrapper significantly outperformed filter real datasets performed significantly worse filter monk dataset 
monk feature subset search test node test set accuracy internal cross validation estimated accuracy lower node test set accuracy 
focused accuracy criteria merit consideration 
wrapper method extends directly minimizing misclassification cost 
irvine datasets include cost information accuracy natural performance metric trivially cost function accuracy evaluation function wrapper 
filter approaches adapting misclassification costs research topic 
second compare number features selected filter wrapper 
table shows number features dataset number selected relieved filter note filter independent induction algorithm prescribes set features id naive bayes number selected plain versions algorithms wrapper enhanced versions 
plain naive bayes uses features column 
average reduction column shows average percentage decrease number features column natural benchmark rlf compared original dataset bfs compared plain 
interesting compare results original algorithm wrapped versions id naive bayes 
table shows accuracy results id best forward feature subset selection best backward fss compound operators naive bayes backward compound operator fss 
observations ffl real datasets id fss perform approximately id fss uses fewer features 
artificial datasets id fss significantly outperforms datasets corral monk monk local significantly inferior 
table number features dataset number selected relieved number plain versions algorithms number wrapped versions 
dataset rlf bfs nb bfs id id bfs breast cancer cleve crx dna horse colic pima sick euthyroid soybean large corral monk monk local monk monk average reduction table comparison id fss fss naive bayes fss 
val columns indicates probability column improving dataset id fss val fss val nb fss val original bfs back bfs back bfs breast cancer sigma sigma sigma sigma cleve sigma sigma sigma sigma crx sigma sigma sigma sigma dna sigma sigma sigma sigma horse colic sigma sigma sigma sigma pima sigma sigma sigma sigma sick euthyroid sigma sigma sigma sigma soybean large sigma sigma sigma sigma corral sigma sigma sigma sigma sigma sigma sigma sigma monk sigma sigma sigma sigma monk local sigma sigma sigma sigma monk sigma sigma sigma sigma monk sigma sigma sigma sigma average real average artif 
ffl fss significantly outperforms real datasets cleve dna artificial datasets monk monk local significantly outperformed 
relative error reduced real datasets artificial datasets 
ffl interesting naive bayes feature subset selection compare 
datasets better real datasets significantly better horse colic dataset naive bayes significantly better cleve dna pima soybean large 
relative error naive bayes smaller 
artificial datasets equal significantly better datasets monk monk local naive bayes better corral 
summary feature subset selection wrapper approach significantly improves id naive bayes datasets tested 
real datasets wrapper approach clearly superior filter method 
surprising result naive bayes performs real datasets discretization feature subset selection done 
explanations apparently high accuracy naive bayes independence assumptions violated explained domingos pazzani 
see real world domains dna feature selection step important improve performance 
overfitting error argue front data 
find twisting round fit theories 
sherlock holmes adventure 
induction algorithm overfits dataset models data predictions poor 
example specialized hypothesis classifier lookup table features 
overfitting closely related bias variance tradeoff geman breiman algorithm fits data variance term large error increased 
accuracy estimation methods including cross validation evaluate predictive power hypothesis feature subset setting aside instances holdout sets shown induction algorithm assess predictive ability induced hypothesis 
search algorithm explores large portion space guided accuracy estimates choose bad feature subset subset high accuracy estimate poor predictive power 
overuse accuracy estimates feature subset selection may cause overfitting feature subset space 
feature subsets leads hypothesis high predictive accuracy holdout sets 
example overfitting shown information dataset rand features label completely random 
top graph shows estimated accuracy versus true accuracy best node search expanding nodes 
see especially small sample size estimate extremely poor optimistic indicative overfitting 
bottom graphs show overfitting small real world datasets 
machine learning researchers reported cross validation estimates guide search final estimate performance achieving overly optimistic results 
experiments cross validation guide search report accuracy selected feature subset separate test set holdout sets generated external loop cross validation feature subset selection process 
problem overfitting feature subset space previously raised machine learning community wolpert schaffer subject received attention statistics community cf 
miller 
theoretical problem exists experiments indicate overfitting mainly problem number instances small 
kohavi sommerfield reported searches feature subsets datasets containing instances searches optimistically biased standard deviations pessimistically biased standard deviations expected standard deviations 
problem clearly exists severe nodes rand forward selection accuracy nodes breast cancer forward selection accuracy nodes glass backward elimination accuracy overfitting feature subset selection 
top graph shows estimated true accuracies random dataset id 
solid line represents estimated accuracy training set instances thick grey line training set instances dotted line shows real accuracy 
bottom graphs graphs show accuracy real world datasets 
solid line estimated accuracy dotted line accuracy independent test set 
datasets examined datasets contained instances training set 
estimates biased algorithm may choose correct feature subsets relative accuracy matters 
subset selection search probabilistic estimates look problem feature subset selection search probabilistic estimates 
view generalizes problem believe lead new practical results problem solved different approach previous sections 
wrapper approach uses accuracy estimation evaluation heuristic function complicates common state space search paradigm 
fact accuracy estimation random variable implies uncertainty returned estimate 
way decrease variance run accuracy estimation fold cross validation average results done 
increasing number runs shrinks confidence interval mean requires time 
tradeoff accurate estimates extensive exploration search space referred exploration versus exploitation problem kaelbling 
exploit knowledge shrink confidence intervals explored nodes sure select right explore new nodes hope finding better nodes 
tradeoff leads search problem 
definition search probabilistic estimates state space operators states 

unbiased probabilistic evaluation function maps state real number indicating state number returned comes distribution mean actual unknown value state 
goal find state maximal value 
mapping definition feature subset selection problem follows 
states subsets operators common ones add delete compound 
evaluation function accuracy estimation accuracy 
accuracy estimation techniques cross validation biased viewed unbiased estimators different quantity example fold crossvalidation unbiased datasets size gamma furthermore model selection pessimism minor importance bias may cancel 
describe falls general framework search probabilistic estimators 
greiner described conduct hill climbing search evaluation function probabilistic 
algorithm stops node local optimum high probability chernoff bound 
yan analyzed algorithm simulated annealing showed find global optimum time 
maron moore approach similar greiner attempted shrink confidence interval accuracy set models model proven optimal high probability 
evaluation function single step leave cross validation algorithm trained randomly chosen gamma instances tested left 
induction algorithm instance learning leads extremely fast evaluation training necessary 
step leave merely test instance classified correctly nearest neighbor 
note returns zero 
instance correctly classified 
step repeated times get reasonable confidence bound 
general idea race competing models clear winner 
models drop race confidence interval accuracy overlap confidence interval accuracy best model analogous imposing higher lower bound estimation function algorithm berliner 
race ends winner steps cross validation executed 
confidence interval defined hoeffding formula hoeffding fi fi fif gamma fi fi fi ffl gamma average evaluations bounds possible spread point values 
confidence level determine ffl confidence interval formula 
maron moore discuss search heuristic assumes fixed set models external source 
moore lee describe algorithm feature subset selection ingredients problem search heuristic uses probabilistic estimates non trivial manner 
algorithm forward selection backward elimination estimating accuracy added deleted feature leave cross validation features added deleted raced parallel 
assuming distribution normal confidence intervals eliminate features race 
schemata search moore lee search variant allows account interactions features 
starting empty full set features search begins features marked unknown 
time feature chosen raced 
combinations unknown features equal probability feature win race correlated feature 
method uses probabilistic estimates bayesian setting basic search strategy simple hill climbing 
fong gives bounds sample complexity number samples needs collect termination armed bandit problem 
fl approach allows trading exploitation exploration generalizing kaelbling interval estimation strategy kaelbling 
cases worst case bound remains optimal tradeoff exploration exploitation empirically determined domain dependent 
related pattern recognition literature devijver kittler kittler ben statistics literature draper smith miller miller machine learning papers almuallim dietterich almuallim dietterich kira rendell kira rendell kononenko consist measures feature subset selection data 
measures pattern recognition statistics literature monotonic sequence nested feature subsets delta delta delta measure obeys delta delta delta 
notable selection measures satisfy monotonicity assumption residual sum squares rss adjusted residual mallows discriminant functions distance measures bhattacharyya distance divergence 
press measure prediction sum squares obey monotonicity 
monotonic functions branch bound techniques prune search space 
wilson show compute residual sum squares rss possible regressions features 
floating point operations regression furthermore technique combined branch bound algorithms described 
narendra fukunaga apparently rediscovered branch bound technique improved yu yuan 
machine learning induction algorithms obey monotonic restrictions type dynamic programming 
branch bound search usually exponential features suboptimal methods 
searching space feature subsets studied years 
sequential backward elimination called sequential backward selection introduced marill green 
kittler generalized different variants including forward methods stepwise methods plus take away 
cover showed multivariate normally distributed features hill climbing procedure uses monotonic measure selects feature time find best feature subset desired size algorithm adds best pair removes worst single feature fail 
papers attempt ai techniques beam search bidirectional search siedlecki sklansky best search xu yan chang genetic algorithms de jong de jong 
algorithms described deterministic evaluation function cases easily extended probabilistic estimates cross validation 
bala jong vafaie wechsler wrapper approach holdout accuracy estimation genetic algorithm search space 
langley reviewed feature subset selection methods machine learning contrasted wrapper filter approaches 
atkeson leave cross validation search multidimensional real valued space includes feature weights addition parameters local learning 
theory rough sets defines notions relevance closely related ones defined 
set strongly relevant features form core set features allow bayes classifier achieve highest possible accuracy forms reduct 
reduct contain strongly relevant weakly relevant features 
shows core intersection reducts reduct consists core features weakly relevant features 
wrote important fundamental notions rough sets philosophy need discover redundancy dependencies features lot feature subset selection coming rough sets community cf 

goal finding feature subset kohavi claimed relevance necessarily imply usefulness induction tasks see section 
concentrated feature selection relevant features alternative method weigh features giving degree relevance 
theoretical results shown multiplicative forest service really interested problem 
school forestry yale university wilson forest service 
think working tree pruning linear regression 
learning algorithms linear combinations features perceptrons littlestone warmuth 
concept weighted combination classifiers generalizes idea choosing features commonly referred problem combining expert advice littlestone freund haussler helmbold schapire warmuth 
skalak uses wrapper approach feature subset selection decreasing number prototypes stored instance methods 
shows prototypes suffice 
example choosing relevant instances opposed relevant features 
turney defines feature primary feature value probability class changes conditioned value 
primary feature informative class considered 
defines contextual feature non primary relevant feature 
feature contextual helps context 
contextual features harder find involve interactions 
definitions orthogonal feature may primary strongly weakly relevant contextual strongly weakly relevant 
wrapper approach john seen papers 
langley sage wrapper approach select features naive bayes discretization langley sage select features nearest neighbor algorithm 
pazzani wrapper approach select features join features create super features compound naive bayes showed finds correct combinations features interact 
singh provan provan singh wrapper approach select features bayesian networks showed significant improvements original algorithm 
street mangasarian wolberg wrapper context linear programming generalizer 
algorithms mentioned hill climbing search engine 
aha bankert wrapper identifying feature subsets cloud classification problem features instances concluded empirical results strongly support claim wrapper strategy superior filter methods 
aha bankert compare forward backward feature subset selection wrapper approach beam search engine conclude forward selection better 
applied wrapper approach parameter tuning specifically setting parameters maximal performance kohavi john 
mladeni independently extended wrappers feature subset selection parameter tuning 
developed method similar wrapper approach independently compared search engines feature subset selection aware fact independent test set final estimation accuracy estimation guide search see section 
variations extensions current possible 
examined hill climbing search engines 
approaches examined simulated annealing approaches evaluate better nodes times laarhoven aarts 
looking search seen general area search space explored heavily 
worthwhile introduce diversity search genetic algorithm genetic holland goldberg koza 
problem abstracted search probabilistic estimates section done experiments attempt understand tradeoff quality estimates search size exploration versus exploitation experiments 
search subset conducted large space 
started search empty set features full set features start initial node 
possibility estimate features strongly relevant start search subset compound operators partial answer problem 
possibility start random points conduct series hill climbing searches 
start set features suggested relieved ensure set explored wrapper point search 
longer discussion contextual features may turney definitions originally flawed mentioned turney 
results papers aha bankert mladeni interpreted cautiously cross validation accuracy search final estimated performance opposed independent test set external loop cross validation done 
wrapper approach slow 
larger datasets possible cheaper accuracy estimation methods holdout decrease number folds 
furthermore inducers allow incremental operations classifiers add delete instances leading possibility doing incremental crossvalidation suggested kohavi drastically reducing running time 
support incremental operations utgoff shown possible implemented fast version leave decision trees utgoff 
wrapper approach easy parallelize 
node expansion children evaluated parallel cut running time factor equal number attributes dna 
theory possible feature subset identifies different model problem viewed model selection statistics 
models case chooses induction algorithms estimate accuracy select highest accuracy schaffer find underlying theory help predict best dataset see brazdil gama attempt successful finding regularities statlog project 
smallest problems space possible feature subsets large brute force enumeration possibilities resort heuristic search 
aggregation techniques called stacking advocated people machine learning neural networks statistics wolpert breiman freund schapire schapire freund perrone krogh vedelsby buntine kwok carter :10.1.1.32.9399
possible build models different parameter setting different feature subset vote class 
aggregation techniques reduce variance models aggregating extremely hard interpret resulting classifier 
summary described feature subset selection problem supervised learning involves identifying relevant useful features dataset giving subset learning algorithm 
investigated relevance irrelevance features concluded weak strong relevance needed capture intuition better 
shown definitions mainly useful respect optimal rule bayes rule practice look optimal features respect specific learning algorithm training set hand 
optimal features necessarily correspond relevant features weak strong shown section 
optimal features depend specific biases heuristics learning algorithm wrapper approach naturally fits definition 
feature relevance helped motivate compound operators practice currently practical way conduct backward searches feature subsets wrapper approach datasets features 
wrapper approach requires search space operators search engine evaluation function 
evaluation function cross validation accuracy estimation technique results kohavi 
common search space add delete operators basis comparing search engines hill climbing best search 
defined compound operators information children expanded node just 
compound operators backward search starting full set features practical 
best search compound operators strong performer improves id naive bayes accuracy comprehensibility measured number features 
showed problems filter methods attempt define relevance independently learning algorithm 
problems include inability remove feature symmetric targets concepts removal feature improves performance section inability include irrelevant features may help performance example inability remove correlated features may hurt performance section 
theoretical reasons relevance defined relative algorithm conducted experiments comparing wrapper approach relieved filter approach feature subset selection 
comparisons include different families induction algorithms decision trees naive bayes 
significant performance improvement achieved 
dna dataset extensively compared statlog project wrapper approach naive bayes reduced error rate relative error reduction making best known induction algorithm problem 
surprising results naive bayes performed 
global comparison table naive bayes feature selection outperforms feature selection real datasets 
experiments done real artificial datasets 
cases results varied dramatically sets 
reason real datasets preprocessed include relevant features dna exception artificial ones included irrelevant features purpose 
artificial datasets noise free monk real ones contained noise 
artificial problems contained high order interactions harder hill climbing algorithms find optimal feature subset 
expect tougher problems containing interactions occur unprocessed datasets coming real world 
shown problems wrapper approach overfitting large amounts cpu time required defined search problem state space search probabilistic estimates formulation may capture general problems studied independently solve existing problems 
time issue important larger amounts data cross validation replaced holdout accuracy estimation immediate improvement time factor 
overfitting problem lesser importance occur small training sets data available training overfitting chance harder 
supervised classification learning question feature dataset relevant prediction task useful question feature relevant prediction task learning algorithm 
goal optimize accuracy ask set features optimal task learning algorithm training set 
different algorithms different biases feature may help algorithm may hurt 
similarly different training set sizes imply different set features optimal 
small training set may better reduce number features reduce algorithm variance instances features chosen reduce algorithm bias 
acknowledgments karl pfleger help formulating wrapper idea 
anonymous reviewers 
reviewer formulated example better original example 
pat langley nick littlestone nils nilsson peter turney gave helpful feedback ideas presentation 
dan sommerfield implemented large parts wrapper mlc kohavi experiments done mlc george john supported national science foundation graduate research fellowship 
aha 
tolerating noisy irrelevant novel attributes instance learning algorithms international journal man machine studies pp 

aha bankert 
feature selection case classification cloud types empirical comparison working notes aaai workshop case reasoning pp 

aha bankert 
comparative evaluation sequential feature selection algorithms fisher lenz eds proceedings fifth international workshop artificial intelligence statistics ft lauderdale fl pp 

aha kibler albert 
instance learning algorithms machine learning pp 

almuallim dietterich 
learning irrelevant features ninth national conference artificial intelligence mit press pp 

almuallim dietterich 
learning boolean concepts presence irrelevant features artificial intelligence pp 

anderson 
explorations incremental bayesian algorithm categorization machine learning pp 

atkeson 
locally weighted regression robot learning proceedings ieee international conference robotics automation pp 

bala jong vafaie wechsler 
hybrid learning genetic algorithms decision trees pattern classification mellish ed proceedings th international joint conference artificial intelligence morgan kaufmann publishers pp 

ben 
distance measures information measures error bounds feature evaluation krishnaiah kanal eds handbook statistics vol 
north holland publishing pp 

berliner 
tree search algorithm best proof procedure webber nilsson eds readings artificial intelligence morgan kaufmann publishers pp 

blum rivest 
training node neural network np complete neural networks pp 

boddy dean 
solving time dependent planning problems sridharan ed proceedings eleventh international joint conference artificial intelligence vol 
morgan kaufmann publishers pp 

brazdil gama 
characterizing applicability classification algorithms meta level learning bergadano raedt eds proceedings european conference machine learning 
breiman 
bagging predictors technical report statistics department university california berkeley 
breiman friedman olshen stone 
classification regression trees wadsworth international group 
buntine 
learning classification trees statistics computing june pp 

cardie 
decision trees improve case learning proceedings tenth international conference machine learning morgan kaufmann publishers pp 

caruana freitag 
greedy attribute selection cohen hirsh eds machine learning proceedings eleventh international conference morgan kaufmann publishers pp 

cesa bianchi freund haussler helmbold schapire warmuth 
expert advice technical report ucsc crl univ calif computer research lab santa cruz ca 
extended appeared stoc 
cestnik 
estimating probabilities crucial task machine learning aiello ed proceedings ninth european conference artificial intelligence pp 

cover 
possible orderings measurement selection problem ieee transactions systems man cybernetics smc pp 

dasarathy 
nearest neighbor nn norms nn pattern classification techniques ieee computer society press los alamitos california 
de 
distance attribute selection measure decision tree induction machine learning pp 

devijver kittler 
pattern recognition statistical approach prentice hall international 

evaluation feature selection methods application computer security technical report cse university california davis 
domingos pazzani 
independence conditions optimality simple bayesian classifier saitta ed machine learning proceedings thirteenth international conference morgan kaufmann publishers pp 

dougherty kohavi sahami 
supervised unsupervised discretization continuous features prieditis russell eds machine learning proceedings twelfth international conference morgan kaufmann pp 

draper smith 
applied regression analysis nd edition john wiley sons 
duda hart 
pattern classification scene analysis wiley 
fayyad 
induction decision trees multiple concept learning phd thesis eecs dept michigan university 
fayyad irani 
attribute selection problem decision tree generation tenth national conference artificial intelligence mit press pp 

fong 
quantitative study hypothesis selection prieditis russell eds machine learning proceedings twelfth international conference morgan kaufmann publishers pp 

freund 
boosting weak learning algorithm majority proceedings third annual workshop computational learning theory pp 

appear information computation 
freund schapire 
decision theoretic generalization line learning application boosting proceedings second european conference computational learning theory springer verlag pp 

wilson 
regression leaps bounds technometrics pp 

geman bienenstock doursat 
neural networks bias variance dilemma neural computation pp 

gennari langley fisher 
models incremental concept formation artificial intelligence pp 

ginsberg 
essentials artificial intelligence morgan kaufmann publishers goldberg 
genetic algorithms search optimization machine learning addison wesley publishing 
estimation probabilities essay modern bayesian methods press 
greiner 
probabilistic hill climbing theory applications glasgow hadley eds proceedings ninth canadian conference artificial intelligence morgan kaufmann publishers pp 

hancock 
difficulty finding small consistent decision trees unpublished manuscript harvard university 
hoeffding 
probability inequalities sums bounded random variables journal american statistical association pp 

holland 
adaptation natural artificial systems introductory analysis applications biology control artificial intelligence mit press 
rivest 
constructing optimal binary decision trees np complete information processing letters pp 

john kohavi pfleger 
irrelevant features subset selection problem machine learning proceedings eleventh international conference morgan kaufmann pp 

judd 
complexity loading shallow neural networks journal complexity pp 

kaelbling 
learning embedded systems mit press 
kira rendell 
feature selection problem traditional methods new algorithm tenth national conference artificial intelligence mit press pp 

kira rendell 
practical approach feature selection proceedings ninth international conference machine learning morgan kaufmann publishers kittler 
une en de quelques algorithms sous de recherche ensembles proc 
congr es reconnaissance des formes traitement des images 
kittler 
feature selection extraction academic press chapter pp 

kohavi 
feature subset selection search probabilistic estimates aaai fall symposium relevance pp 

kohavi 
power decision tables lavrac wrobel eds proceedings european conference machine learning lecture notes artificial intelligence springer verlag berlin heidelberg new york pp 

kohavi 
study cross validation bootstrap accuracy estimation model selection mellish ed proceedings th international joint conference artificial intelligence morgan kaufmann publishers pp 

kohavi 
wrappers performance enhancement oblivious decision graphs phd thesis stanford university computer science department 
stan cs tr ftp stanford edu pub ronnyk ps 
kohavi 
useful feature subsets rough set reducts third international workshop rough sets soft computing pp 

appeared soft computing lin 
kohavi john 
automatic parameter selection minimizing estimated error prieditis russell eds machine learning proceedings twelfth international conference morgan kaufmann publishers pp 

kohavi sommerfield 
feature subset selection wrapper model overfitting dynamic search space topology international conference knowledge discovery data mining pp 

kohavi wolpert 
bias plus variance decomposition zero loss functions saitta ed machine learning proceedings thirteenth international conference morgan kaufmann publishers available robotics stanford edu users ronnyk 
kohavi sommerfield dougherty 
data mining mlc machine learning library tools artificial intelligence ieee computer society press appear 
www sgi com technology mlc 
kononenko 
estimating attributes analysis extensions relief bergadano raedt eds proceedings european conference machine learning 
kononenko 
biases estimating multi valued attributes mellish ed proceedings th international joint conference artificial intelligence morgan kaufmann publishers pp 

koza 
genetic programming programming computers means natural selection mit press 
krogh vedelsby 
neural network ensembles cross validation active learning advances neural information processing systems vol 
mit press 
kwok carter 
multiple decision trees levitt kanal lemmer eds uncertainty artificial intelligence elsevier science publishers pp 

laarhoven aarts 
simulated annealing theory applications kluwer academic publishers 
langley 
selection relevant features machine learning aaai fall symposium relevance pp 

langley sage 
induction selective bayesian classifiers proceedings tenth conference uncertainty artificial intelligence morgan kaufmann publishers seattle wa pp 

langley sage 
oblivious decision trees cases working notes aaai workshop case reasoning aaai press seattle pp 

langley iba thompson 
analysis bayesian classifiers proceedings tenth national conference artificial intelligence aaai press mit press pp 


model selection john wiley sons 
littlestone 
learning quickly irrelevant attributes abound new linear threshold algorithm machine learning pp 

littlestone warmuth 
weighted majority algorithm information computation pp 

mallows 
comments technometrics pp 

marill green 
effectiveness receptors recognition systems ieee transactions information theory pp 

maron moore 
hoeffding races accelerating model selection search classification function approximation advances neural information processing systems vol 
morgan kaufmann publishers miller 
selection subsets regression variables royal statistical society pp 

miller 
subset selection regression chapman hall 
minsky papert 
perceptrons computational geometry mit press 
expanded ed 
mladeni 
automated model selection ecml workshop knowledge level modeling machine learning 

feature selection rough sets theory brazdil ed proceedings european conference machine learning springer pp 

moore lee 
efficient algorithms minimizing cross validation error cohen hirsh eds machine learning proceedings eleventh international conference morgan kaufmann publishers moret 
decision trees diagrams acm computing surveys pp 

murphy aha 
uci repository machine learning databases www ics uci edu mlearn mlrepository html 
murthy salzberg 
lookahead pathology decision tree induction mellish ed proceedings th international joint conference artificial intelligence morgan kaufmann publishers pp 

narendra fukunaga 
branch bound algorithm feature subset selection ieee transactions computers september pp 

wasserman 
applied linear statistical models rd edition irwin il 

rough sets kluwer academic publishers 

rough sets state foundations computing decision sciences pp 

pazzani 
searching dependencies bayesian classifiers fisher lenz eds proceedings fifth international workshop artificial intelligence statistics ft lauderdale fl 
perrone 
improving regression estimation averaging methods variance reduction extensions general convex measure optimization phd thesis brown university physics dept provan singh 
learning bayesian networks feature selection fisher lenz eds proceedings fifth international workshop artificial intelligence statistics ft lauderdale fl pp 

quinlan 
induction decision trees machine learning pp 

reprinted shavlik dietterich eds 
readings machine learning 
quinlan 
programs machine learning morgan kaufmann publishers los altos california 
quinlan 
oversearching layered search empirical learning mellish ed proceedings th international joint conference artificial intelligence morgan kaufmann publishers pp 

rendell 
learning hard concepts constructive induction framework rationale computational intelligence november pp 

rosenblatt 
perceptron probabilistic model information storage organization brain psychological review pp 

russell norvig 
artificial intelligence modern approach prentice hall englewood cliffs new jersey 
schaffer 
selecting classification method cross validation machine learning pp 

schapire 
strength weak learnability machine learning pp 

siedlecki sklansky 
automatic feature selection international journal pattern recognition artificial intelligence pp 

singh provan 
comparison induction algorithms selective non selective bayesian classifiers machine learning proceedings twelfth international conference pp 

skalak 
prototype feature selection sampling random mutation hill climbing algorithms cohen hirsh eds machine learning proceedings eleventh international conference morgan kaufmann publishers street mangasarian wolberg 
inductive learning approach prognostic prediction machine learning proceedings twelfth international conference 
taylor michie 
machine learning neural statistical classification paramount publishing international 
thrun 
monk problems performance comparison different learning algorithms technical report cmu cs carnegie mellon university 
turney 
exploiting context learning classify brazdil ed proceedings european conference machine learning ecml pp 

turney 
identification context sensitive features formal definition context concept learning kubat widmer eds proceedings workshop learning context sensitive domains pp 

available national research council canada technical report 
utgoff 
improved algorithm incremental induction decision trees machine learning proceedings eleventh international conference morgan kaufmann pp 

utgoff 
decision tree induction efficient tree restructuring technical report university massachusetts amherst 
de jong 
genetic algorithms tool feature selection machine learning fourth international conference tools artificial intelligence ieee computer society press pp 

de jong 
robust feature selection algorithms fifth international conference tools artificial intelligence ieee computer society press pp 

wolpert 
connection sample testing generalization error complex systems pp 

wolpert 
stacked generalization neural networks pp 

xu yan chang 
best strategy feature selection ninth international conference pattern recognition ieee computer society press pp 

yan 
stochastic discrete optimization siam control optimization pp 

yu yuan 
efficient branch bound algorithm feature selection pattern recognition pp 


discovery analysis representation data dependencies databases piatetsky shapiro frawley eds knowledge discovery databases mit press 

