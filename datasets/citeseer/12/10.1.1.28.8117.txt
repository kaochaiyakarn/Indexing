finite automata compact representation language models nlp jan daciuk van noord alfa informatica rijksuniversiteit groningen groningen netherlands mail fj daciuk rug nl april technique compact representation language models natural language processing 
brief review motivations compact representation language models shown nite state automata compactly represent language models 
technique seen application extension perfect hashing means nite state automata 
preliminary practical experiments indicate technique yields considerable important space savings practice 
important practical problem natural language processing nlp posed size knowledge sources employed 
nlp systems aim full parsing unrestricted texts example realistic electronic dictionaries contain information hundreds thousands words 
years perfect hashing techniques developed nite state automata enable compact representation large dictionaries sacri cing time required access dictionaries 
freely available implementation techniques provided experience context wide coverage grammar dutch established importance techniques :10.1.1.12.5797
www pg gda pl fsa html lexicon derived existing lexical resources 
contains stems give rise fully ected entries compiled dictionary runtime 
standard representation provided underlying programming language case prolog lexicon took megabytes 
prolog library constructed implemented interfaces prolog tools provided fsa package :10.1.1.12.5797
dictionary contains megabytes noticeable delay lexical lookup times 
dictionaries space consuming resources required current state art nlp systems 
particular language models containing statistical information occurrence words word meanings typically require space 
order illustrate point consider model described chapter uential dissertation nlp 
chapter describes statistical parser bases parsing decisions bigram lexical dependencies trained penn treebank 
collins reports tests sun mhz processor 
parser uses megabytes memory training sentences essentially extracting occurrence counts corpus takes minutes 
loading hash table bigram counts memory takes approximately minutes 
similar example described 
foster compares number linear models maximum entropy models parsing considering features feature represents occurrence particular pair words 
data intensive probabilistic models limited parsing 
instance describes method learn ordering prenominal adjectives english british national corpus purpose natural language generation system 
resulting model contains counts di erent pairs adjectives 
practice systems need capable bigram models trigram models considered 
instance order solve pp attachment ambiguities describe unsupervised method constructs model word newspaper corpus contains counts relevant hv hn trigrams preposition head verb phrase head noun phrase preceding preposition head noun phrase preposition 
speech recognition language models trigrams common 
illustration dutch newspaper corpus sentences contains word types bigram types trigram types 
addition order improve accuracy models larger text collections needed training 
experiments employed dutch newspaper corpus sentences 
corpus contains unigram types bigram types trigram types 
straightforward textual representation trigram counts corpus takes megabytes storage 
standard hash implementation provided gnu version standard library take megabytes storage run time 
initializing hash table takes minutes 
technique introduced size reduced megabytes loading line constructed compact language model takes half second 
examples illustrate size knowledge sources employed important practical problem nlp 
runtime memory requirements problematic cpu time required load required knowledge sources 
propose method represent huge language models compact way techniques 
loading compact models faster practice delay compact models observed 
formal preliminaries attempt generalize details speci statistical models employed nlp systems 
assume models composed various functions tuples strings tuples numbers 
language model function nite function 

word columns typically contain words word meanings names dependency relations part speech tags 
number columns typically contain counts log probabilities potential numerical information diversity 
language model function quite typical dictionaries may fact dictionary 
instance table bigram counts set rst words set second words 
technique introduced able take advan tage shared dictionaries require dictionaries di erent columns 
naturally space savings expected rst case 
compact representation language models language model function 
represented perfect hash nite automata table rows 
construct acyclic nite automaton words automaton additional information compiled implements perfect hashing 
perfect hash automaton converts word unique number jw write refer hash key assigned corresponding perfect hash automaton 
overlap words di erent columns prefer perfect hash automaton columns 
common situation grams statistical natural language processing 
construct table domain row table consisting note cells table contain numbers 
represent number bytes required largest number column 
representation compact machine independent implementation signi cant byte comes rst 
table sorted 
language model function represented table compressed numbers number perfect hash automata converting words corresponding hash keys 
access value involves converting words hash keys perfect hashing automata constructing query string hash keys compressing hash keys binary search query string table obtained de compressing values table 
special case language model functions 
words unique need store numbers table 
numbers just serve indices table 
access di erent general case 
obtain word number address numerical part tuple 
preliminary results performed number preliminary experiments 
results summarized table 
text method indicates size required straightforward textual representation 
old methods indicate size required straightforward prolog implementation long list facts standard implementation hashes 
noted hash require space text representation 
compared method hash map datastructure provided gnu implementation standard library original implementation knowledge sources bigram pos tagger referred table 
concat dict method indicates size required treat sequences strings words single dictionary represent means nite automaton 
great space savings achieved case nite automaton representation able compress pre xes suxes words words get long get concatenating multiple words automaton representation suitable 
nal new column indicates space required new method introduced 
compared di erent methods various inputs 
tuple contains tuples words part speech tags name dependency relation 
relates tuple tuple consisting numbers 
rows sents trigram refers arti cial test calculated trigram counts dutch newspaper corpus sentences 
sents rows similar case computed counts 
pos tagger row presents results visible markov model part speech tagger dutch tag set containing tags trained corpus sentences 
knowledge sources table bigrams tags containing entries table word tag pairs containing entries 
concluded results table new representation cases compact generally uses half space required textual format 
hashes practice purpose consistently require times space 
sizes reported table obtained unix command wc size hash 
store hashes disk sizes estimated increase memory size reported top 
results obtained bit architecture 
test set text old concat dict new prolog hash tuple na sents trigram sents trigram sents sents pos tagger na na table comparison various representations kbytes variations investigated additional methods compress speed representation language model functions variations mentioned pointers 
table hash key rst column rows 
trigrams example rst hash keys may identical rows table 
situation arise columns 
representing providing pointer remaining part doing recursively columns arrive structure called trie 
trie edges going root labeled hash keys rst column 
point vertices outgoing edges representing tuples words 
keeping copy hash keys rst columns hope storage space 
need additional memory pointers 
vertex represented vector edges edge consists items label hash key pointer 
method works best table dense columns 
construct trie columns representing words keep numerical columns intact 
dense tables may perceive trie nite automaton 
vertices states edges transitions 
reduce number states transition automaton minimizing 
process isomorphic subtrees automaton word columns replaced single copies 
means additional sharing space takes place 
need determine paths automaton lead sequences numbers numerical columns 
done means perfect hashing 
implies transition automaton contains label hash key pointer state number required construct hash key 
share transitions need space storing additional numbers 
perfect hash automaton sequences perfect hash keys faster 
look time table basic model described previous section determined binary search 
time look tuple proportional binary logarithm number tuples 
may possible improve access times interpolated search binary search 
automaton possible look time independent number tuples 
done sparse matrix representation applied nite state automata 
state represented set transitions big vector transitions automaton 
transitions occupy adjacent space indexed labels label transition number 
gaps labels gaps representation single state 
lled transitions belonging states provided states point transition vector 
possible ll gaps space wasted 
preliminary results alternative representation language model functions discouraging 
take word holding part table create automaton row converted string transitions labeled word numbers successive columns minimize automaton compare number transitions get reduction 
transition holds additional items usually size label means times big simple label 
trie representation don need numbering information transition twice big label automaton transitions 
sparse matrix representation introduces additional loss space 
rst method space transition vector lled 
loss due fact labels outgoing transitions state subset numbers 
sharp contrast natural language dictionaries instance size alphabet smaller 
experiments required method dismissed conclusively 
method placing states transition vector quite naive replace clever method yielding better space eciency 
divide vector columns corresponding columns table initial tuple representation 
width columns varied column need pointer eld width numbering information eld columns smaller initial ones 
improvements give compact representation tuples base alternative representation may preferred want trade space speed 
alternatively represent language model function dimensional array 
perfect hashing automata dictionaries query value index array array typically sparse stored sparse matrix representation 
noted approach give fast access space required represent big depending success sparse matrix representation size table constructed previous method 
acknowledgments research carried framework project algorithms linguistic processing funded nwo dutch organization scienti research university groningen 
bouma van noord robert malouf 
wide coverage computational analysis dutch 

submitted volume clin 
available www rug nl 
michael collins 
head driven statistical models natural language parsing 
phd thesis university pennsylvania 
jan daciuk 
experiments automata compression 
daley yu editors conference implementation application automata pages london ontario canada july 
university western ontario 
jan daciuk 
finite state tools natural language processing 
coling workshop tools architectures build nlp systems pages luxembourg august 
george foster 
maximum entropy minimum divergence translation model 
vijay shanker chang ning huang editors proceed ings th meeting association computational linguistics pages hong kong october 
frederick jelinek 
statistical methods speech recognition 
mit press 
claudio tomasz kowaltowski 
applications nite automata representing large vocabularies 
software practice experience jan 
robert malouf 
order prenominal adjectives natural language generation 
vijay shanker chang ning huang editors proceedings th meeting association computational linguistics pages hong kong october 
patrick pantel dekang lin 
unsupervised approach prepositional phrase attachment contextually similar words 
chang ning huang editors proceedings th meeting association computational linguistics pages hong kong october 
dominique 
ethodes algorithmes 
phd thesis institut blaise pascal paris france 

emmanuel roche 
finite state tools language processing 
acl 
association computational linguistics 
tutorial 
robert endre tarjan andrew chi chih yao 
storing sparse table 
communications acm november 

