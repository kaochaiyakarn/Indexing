learning bayes net structure sparse data sets kevin murphy february essentially kinds approaches learning structure bayesian networks bns data 
rst approach tries nd graph satis es constraints implied empirical conditional independencies measured data pv sgs shi 
second approach searches space models dags pdags uses scoring metric typically bayesian approximation bic mdl evaluate models ch hec hec kra typically returning highest scoring model 
main interest learning bn structure gene expression data mm sgs 
domains ratio number observations number variables low sparse data selecting threshold conditional independence ci tests tricky repeated tests lead inconsistencies dd 
bayesian scoring methods avoid problem exploit prior knowledge 
common approach called bayesian model selection returns single best model computed greedy local search algorithm 
little data probably won single best model 
re ect true uncertainty need compute posterior distribution models 
need distribution design experiments minimize structural uncertainty 
approach called bayesian model averaging 
bayesian model averaging estimate probability feature presence particular edge data follows jd gjd set dags xed size wish consider 
contains feature 
example detect presence certain edge way compute posterior probability possible edges display results shading arcs fully connected graph re ect con dence edge 
output constraint algorithms partially directed acyclic graph pdag called pattern essential graph 
pdag chain graph graph contains directed undirected edges directed cycles 
essential graph represents class markov equivalent dags 
graphs markov equivalent imply set conditional independencies 
example markov equivalent represent zjy general graphs markov equivalent structure ignoring arc directions structures vp 
structure consists converging directed edges node 
distinguish members equivalence class interventional experimental data pea cy 
gjd posterior probability graph bayes rule gjd djg djg prior probability graph structure djg marginal likelihood called evidence djg djg jg parameters model 
missing data hidden nodes need sum possible completions unobserved data assuming discrete data simplicity djg zjg jg diculties implementing equations de ne priors jg likelihood xjg 
integrate parameters 
marginalize hidden nodes 
sum graphs 
rst problems arise bayesian model selection hec tutorial discusses issues focus methods suitable small data sets 
problem unique bayesian model averaging 
general tutorials bma discuss bns detail 
tutorial aims ll gaps 
priors parameter priors common practice assume global parameter independence jg jpa parameters conditional probability distribution cpd node likelihood xjg jpa nodes share parameters 
parameter nodes shown inside dotted circles 
data observed parameter priors independent parameter posteriors independent sl 
means marginal likelihood decomposes product terms node djg score pag jd score jd ju represents value case value parents product cases follows assume training data 
integral tractable compute depends form cpd prior discuss 
widely cpd assumes nodes discrete unconstrained multinomial cpds represented tables 
conjugate dirichlet prior nodes observed global local parameter independence compute equation marginal likelihood closed form see appendix leaves problem set hyperparameters parameters prior possible graph structures consistent robust way 
consistent mean equivalent models equal priors robust mean tweaking priors radically ect outcome model selection 
discuss issues length case dirichlet distributions appendix 
multinomial simple exible distribution model combinatorial interactions parents requires lot data estimate binary node binary parents needs parameters specify cpd number possible combination parents 
genetic networks believed fan estimating just parameters observations gene 
possible solution parameter tying see 
encodes fact expect certain kinds regulatory interactions gates repeat di erent parts network 
uncertain parameters tie assume drawn mixture distribution soft weight sharing nh bis neural networks 
unfortunately choosing hyperparameters complex prior may prove formidable task 
addition hierarchical priors hard integrate analytically 
simpler solution restrictive cpds parameters 
simple example logistic regression applicable nodes binary 
de ne ju ij sigmoid logistic function ij weight arc bit vector representing parents values 
closely related model noisy function pea 
case child parents provided links parents broken 
de ne ij probability link fails 
failures assumed independent see mh ways lift restriction 
noisy de ned ju ij set ij ln ij rewrite ju ij form logistic cpd 
add dummy parent node called leak node represent causes corresponds adding set bias term ij 
logistic regression related models parent contributes independently ect child 
model combinatorial interaction simple model parameterize 
genetic networks prior knowledge sign connection excitatory inhibitory 
special case qualitative probabilistic network qpn wel 
encode prior knowledge diagonal gaussian prior weight vector mean excitatory links inhibitory links links unknown sign 
suggest modeling prior knowledge signs constrained dirichlet distributions requires numerical integration 
see wj ways imposing constraints parameter priors 
major disadvantage logistic related cpd inability compute marginal likelihood equation exactly 
possible variational approximation discussed jj accuracy small samples determined 
representation cpds discrete nodes variable complexity ranging parameters tree 
fg show allowing local structure enable learning denser global graph structures tting 
continuous valued nodes widely cpd linear gaussian xju 
corresponding conjugate prior normal wishart compute marginal likelihood closed form hg 
unfortunately assessing prior dicult 
non linear interactions consider generalized linear models neural networks 
data gene expression arrays real valued anticipate having conditional density functions accurately especially non linear ones 
propose discretizing binning data cpds discrete nodes 
reduce artifacts able perform discretization process simultaneously structure learning fg 
structure priors domains little prior knowledge common uniform prior possible models 
alternatively impose penalties number arcs size families discourage networks globally locally dense bun 
domains lot prior knowledge construct prior network penalize deviations initial model structural penalty strictly necessary sparse networks fewer free parameters larger marginal likelihood ockham factor gul 
matches intuition trust constrained correct 
model predict 
counting number mismatched edges hgc 
example itr constructed network databases listing known protein protein protein dna interactions 
time fuzzy prior knowledge 
propose novel approach lets model fact may con dence arcs 
suppose simplicity prior beliefs edge independent 
ij prior probability edge ji prior edge ij ji prior edge ij ji means unsure causes vice versa 
believe unfortunately represent joint constraints simple factored prior 
arbitrary dag ij contains edge ij contains edge ij contains edge ij multinomial random variable prior probabilities ij ij ij ji ij ij ji 
weight graph ijk ij 
de ne prior normalized weight 
understand behavior prior rst consider case entries means know true model 
case weight graph equal number edges gets correct correct means absent prior says absent agree prior orientation edge 
example suppose know true structure re ected prior possible dags variables 
graph highest weight correct correct missing correct suppose uncertain orientation edge addition sure edge uncertain orientation 
encoded weight matrix case graphs achieve maximal weight graphs correspond choice setting combined choice setting mcmc methods recall goal compute gjd djg djg normalizing constant djg intractable compute number graphs 
avoid intractability plan mcmc markov chain monte carlo techniques search large space possible models see grs mcmc 
speci cally plan metropolis hastings mh algorithm requires able compute posterior odds current candidate model proposed new model jd jd djg djg ratio evidences djg djg called bayes factor bayesian equivalent likelihood ratio test 
idea applying mh algorithm graphical models rst proposed called technique mc mcmc model composition 
basic idea construct markov chain state space set dags stationary distribution gjd 
achieve follows 
de ne transition matrix kernel jg 
constraints resulting chain irreducible aperiodic 
sample new state proposal distribution 
jg accept new state probability minf rg acceptance rate jd gjd jg kernel symmetric jg term cancels algorithm called metropolis algorithm 
idea sample chain long ensure reached stationary distribution called burn time throw samples away samples non independent samples true posterior gjd estimate quantities interest jd 
algorithm pseudo code 
comparison show pseudo code greedy hill climbing popular local search algorithm 
cases initial graph chosen ci techniques sm dd 
issue diagnosing convergence mcmc algorithms discussed grs 
number samples needed reaching convergence depends rapidly chain mixes moves posterior distribution 
get gure gc mc nd distribution dags binary nodes course markov equivalent fully observed dataset cases 
iterations burn converge iterations 
proposal distribution suggested kernel 
de ne neighborhood current state set dags di er edge generate considering single edge additions choose converged pick compute jd gjd jg sample unif minf rg pseudo code mc algorithm 
choose converged compute score djg arg maxg score score score converged true pseudo code hill climbing 
deletions reversals subject acyclicity constraint 
way quickly checking proposed dag acyclic ancestor matrix derived gc 
jg jg ndb jp djg jp djg main advantage proposal distribution ecient compute di er single edge assuming complete data see section 
advantage important greedy search need know score neighbors step scores change steps edge time 
single edge change proposal lead high acceptance rates slow mixing takes small steps space 
alternative propose large changes swapping substructures genetic programming 
sure proposal create mixture distribution 
weights mixture parameters tuned hand 
grg suggest kernel designed average acceptance rate 
natural way speed mixing reduce size search space 
suppose node restrict maximum number parents 
reasonable expect fan small 
reduces number parent sets need evaluate heuristics choosing set potential parents 
long give non zero probability possible edge changes proposal guaranteed get correct answer get graph single edge changes chain irreducible heuristics merely help reach right answer faster 
heuristics change time look observed dependencies explained current model 
adaptive proposal distributions cause theoretical problems convergence 
computing marginal likelihood discuss eciently compute posterior odds equation decide accept reject proposal 
typically evaluating ratio priors ecient problem reduces evaluating bayes factors eciently 
mentioned section data complete global parameter independence marginal likelihood product terms node 
graphs di er single link marginal likelihoods di er terms cancel 
example chain middle arc reversed djg djg score score score score score score score score score score score score score de ned equation 
general arc added deleted score needs evaluated arc reversed score score need evaluated 
partial observability approximating marginal partial observability compute marginal likelihood closed form discrete variables conjugate priors parameter posteriors longer independent 
marginalizing hidden variables induces mixture distribution 
straightforward approach approximate marginal likelihoods directly take ratios 
accurate way approximating marginal likelihood known candidate method chi raf follows 
pick arbitrary value map value compute djg dj jg jd computing jg trivial computing dj done bn inference algorithm 
denominator approximated gibbs sampling see ch details 
see snr related approach harmonic mean estimator 
various large sample laplace approximations marginal likelihood computationally cheaper 
compared ch 
cheeseman stutz approximation variation bic accurate case naive bayes mixture models 
beats computationally expensive laplace approximation 
cost approximation equal cost running em nd map value variational bayes technique att min 
accuracy techniques small samples suspect 
large samples approximations inaccurate estimating djg djg probable model second probable 
approximations useful model selection model averaging 
data augmentation alternative approach extend state space markov chain searches models values unobserved nodes 
simplify computation add parameters state space need sampled 
idea gibbs sampling alternate sampling new model current completed data set sampling completed data set current model 
basically extension ip algorithm data augmentation tw 
high level algorithm cycles steps observed data hidden data 
sample jg 
compute jy 
sample zjy avoid computing normalizing constant implicit rst step mh algorithm approximate bayes factor current completed dataset 
approximation expected complete data bayes factor bun raf jg jg zjg zjg jg jg second step performed closed form conjugate distributions see appendix 
sample predictive distribution zjy mean parameter values see appendix gibbs sampling 
algorithm sketched 
principle chain ergodic starting point matter practice initialisation important fast convergence 
choose initial sampling zjg gibbs sampling computing map estimate map arg max jg gibbs sampling zjg map 
dicult choose initial graph structure ci methods sgs latent variables 
hand choose say empty graph initial estimate poor 
assume initial guess domain knowledge 
structural em deterministic approximation data augmentation scheme called structural em proposed fri 
basic idea compute expected complete data marginal likelihood jg ez zjg jg bn inference algorithm applied current model current choose pick compute jg jg compute jp jp sample unif minf rg compute jy bayesian updating compute jy sample zjy gibbs sampling pseudo code mc algorithm modi ed handle missing data 
choose converged compute equation inference algorithm step compute score jg arg maxg score score score structural step arg max jg jg parametric step converged true pseudo code structural em 
map parameters case multinomials see appendix compute expected sucient statistics family candidate model follows ijk pag jjy sem designed model selection opposed model averaging formula embedded inside hill climbing algorithm resulting algorithm shown 
bayesian version proposed fri 
case map value approximately integrates 
note families junction tree algorithm jen inference means may compute joint probability distributions sets nodes clique slow 
searching orderings fk claim mcmc structures mix large models variables say 
mcmc search variable orderings 
total ordering likelihood decomposes product terms family parents node chosen independently global acyclicity constraint 
equation rst noted bun dj score pag jd score jd set graphs consistent ordering set legal parents node consistent 
bound fan number parents summation equation takes time compute equation takes time 
quantity averaged orderings produced markov chain 
fk claim posterior landscape orderings smoother models small changes ordering ect score small changes edge reversal model 
probably true space orderings size space dags disadvantages approach 
firstly natural specify prior model structures variable orderings 
secondly harder extend technique missing data case 
contrast searching directly model space allows apply suite known approximations 
searching essential graphs distinguish members markov equivalence class observational data sense search space essential graphs smaller space dags 
constraint algorithms sgs essential graphs 
bayesian scoring metric applied dags convert pdag dag evaluate chi 
mcmc methods nding high scoring pdags discussed mapv 
hybrid methods start constraint methods switch greedy search bayesian evaluation metric discussed sv sm dd 
methods applicable experimental interventional observational data 
reversible jump mcmc mention just completeness reversible jump mcmc algorithm gre 
necessary state space variable dimension occurs estimating parameters model structure number parameters varies structure 
application graphical models see 
data augmentation able integrate parameters avoid complexities 
interventions give hints ordering 
example knockout gene notice genes change wildtype state genes suggests ancestor heuristic set covering algorithm learn boolean networks interventional data 
multinomial distributions dirichlet priors discrete nodes common assume local cpds multinomial represented table form pr kj ijk number values node take number values node parents take 
parameters satisfy constraints ijk ijk 
common practice assumptions 
global parameter independence ijk parameters node second local parameter independence ij ij ijk parameters th row table parameters th instantiation parents 
factored prior complete data posterior parameters factored sl 
give form posterior 
note missing data parameter posterior longer factored 
assuming parameter independence equivalent assuming prior knowledge derived fully virtual database 
gh rg prove assumptions global local parameter independence plus additional assumption called likelihood equivalence imply prior dirichlet 
fortunately dirichlet prior conjugate prior multinomial ber analysis easier see 
reason dirichlet assumption likelihood equivalence violated 
note case binary nodes multinomial bernoulli distribution dirichlet beta 
global local independence cpd ju ij multinomial random variable possible values 
dirichlet prior ij ij de ned ij ij ijk ijk ij normalizing constant dimensional beta function 
gamma function positive integers 
hyperparameters ijk simple interpretation pseudo counts 
quantity ijk represents number imaginary cases event occured virtual prior database 
seeing database event occurs ijk times parameter posterior ij jd ij ij posterior mean ijk jd ijk ijk ijl ijl graph structures likelihood equivalent assign marginal likelihood data djg djg 
weaker assumption markov hypothesis equivalence says graphs equivalent encode set conditional independence assumptions 
hypothesis equivalence clearly violated adopt causal interpretation bns 
addition likelihood equivalence violated interventional data 
see hec discussion 
posterior mode map estimate arg maxp ijk jd ijk ijk ijl ijl computing marginal likelihood predictive distribution just xj ijk ijk ijk ijk ijk ijk ijk indicator function event occurs case 
compute marginal likelihood database cases sequential bayesian updating sl prior result updating compute batch form simply compute posterior means parameters equation plug expected values sample likelihood equation dj ijk ijk alternatively written follows ch ij ij ij ij ij ij ijk ijk ijk interventional data equation modi ed de ning ijk number times passively observed context shown cy 
intuition setting tell event occur chance counted 
addition need keep record variables clamped data case 
equations conditioned speci graph structure making explicit write unnormalized posterior graph djg djg equation 
hgc called bd bayesian dirichlet metric 
assessing dirichlet priors clearly impossible user specify parametric priors graph structures 
hgc show assumptions possible derive dirichlet parameters arbitrary graph single prior network plus con dence factor speci cally ijk jjg complete graph 
priors derived manner bd score called bde bd likelihood equivalence 
unfortunately dicult parameterize prior network especially counterintuitive conditioning see hgc discussion 
addition computing parameter priors arbitrary graph structure prior network requires running inference algorithm slow 
suggest similar way computing dirichlet priors prior network 
simpler alternative non informative prior 
natural choice ijk corresponds maximum likelihood 
binary case called prior 
improper prior 
importantly cause log likelihood explode case contains event seen training data 
set ijk encourage parameter values ijk near encoding distributions 
desirable domains 
bra explicitely encodes bias entropic prior form ij ij ijk ijk unfortunately entropic prior conjugate prior 
ch suggest uniform prior ijk 
non informative prior ect posterior ect marginal likelihood 
unfortunately entirely uninformative transformation invariant 
fully non informative prior called je rey prior 
special case binary root node node parents beta distribution je rey prior just ijk computing je rey prior arbitrary bn hard see kms 
bun suggests prior ijk equivalent sample size 
induces distribution kj ijk special case bde metric prior network assigns uniform distribution joint distribution hgc call bdeu metric 
sophisticated approach hierarchical prior place prior hyperparameters 
example ijk ij ij prior precision ij ij gamma distribution 
unfortunately longer compute assumptions global local parameter independence likelihood equivalence parameter modularity structural possibility 
parameter modularity says ij graph structures set parents 
structural possibility says complete fully connected graph structures possible priori 
marginal likelihood closed form hierarchical priors resort sampling df 
ecient method known empirical bayes maximum likelihood type ii estimate hyperparameters data see min details 
cpds exponential family cpds exponential family multinomial gaussian conjugate prior dirichlet normal wishart corresponding posterior marginal likelihood computed closedform provided complete data 
see bs bun discussion general case gh discussion gaussian case 
att attias 
variational bayesian framework graphical models 
nips 
ber berger 
statistical decision theory bayesian analysis 
springer verlag 
bis bishop 
neural networks pattern recognition 
clarendon press 
bra brand 
structure learning conditional probability models entropic prior parameter extinction 
neural computation 
bs bernardo smith 
bayesian theory 
john wiley 
bun buntine 
theory re bayesian networks 
uai 
bun buntine 
operations learning graphical models 
ai research pages 
ch cooper herskovits 
bayesian method induction probabilistic networks data 
machine learning 
ch chickering heckerman 
ecient approximations marginal likelihood incomplete data bayesian network 
machine learning 
chi chib 
marginal likelihood gibbs output 
jasa 
chi chickering 
learning equivalence classes bayesian network structures 
uai 
cy cooper yoo 
causal discovery mixture experimental observational data 
uai 
dd dash 
hybrid anytime construction causal models sparse data 
uai 
df forster 
markov chain monte carlo model determination hierarchical graphical log linear models 
biometrika 
appear 
van der gaag 
elicitation probabilities belief networks combining qualitative quantitative information 
uai 
fg friedman goldszmidt 
discretizing continuous attributes learning bayesian networks 
intl 
conf 
machine learning 
fg friedman goldszmidt 
learning bayesian networks local structure 
uai 
fk friedman koller 
bayesian network structure 
uai 
friedman nachman pe er 
bayesian networks analyze expression data 
computational biology 
friedman nachman peer 
learning bayesian network structure massive datasets sparse candidate algorithm 
uai 
fri friedman 
learning bayesian networks presence missing values hidden variables 
uai 
fri friedman 
bayesian structural em algorithm 
uai 
gc 
improving markov chain monte carlo model search data mining 
machine learning 
appear 
green 
ecient model determination discrete graphical models 
biometrika 
appear 
gh geiger heckerman 
learning gaussian networks 
uai volume pages 
gh geiger heckerman 
characterization distributions local global independence 
annals statistics 
gre green 
reversible jump markov chain monte carlo computation bayesian model determination 
biometrika 
grg gelman roberts gilks 
ecient metropolis jumping rules 
bernardo berger dawid smith editors bayesian statistics 
oxford 
grs gilks richardson spiegelhalter 
markov chain monte carlo practice 
chapman hall 
gul gull 
bayesian inductive inference maximum entropy 
erickson smith editors maximum entropy bayesian methods science engineering volume foundations pages 
dordrecht 
hec 
bayesian approach learning causal networks 
uai 
hec heckerman 
tutorial bayesian networks 
jordan editor learning graphical models 
mit press 
hg heckerman geiger 
learning bayesian networks uni cation discrete gaussian domains 
uai volume pages 
hgc heckerman geiger chickering 
learning bayesian networks combination knowledge statistical data 
machine learning 
gi ord jaakkola young 
graphical models genomic expression data statistically validate models genetic regulatory networks 
proc 
paci symp 
biocomputing 
madigan raftery 
bayesian model averaging tutorial 
statistical science 
karp 
discovery regulatory interactions perturbation inference experimental design 
proc 
paci symp 
biocomputing 
itr christmas buhler hood 
integrated genomic analysis systematically metabolic network 
science 
submitted 
jen jensen 
bayesian networks 
ucl press london england 
jj jaakkola jordan 
bayesian parameter estimation variational methods 
statistics computing 
kms kontkanen aki tirri gr 
comparison noninformative priors bayesian networks 
yearbook finnish statistical society pages 

kra krause 
learning probabilistic networks 
technical report philips research labs england 
mapv madigan anderson perlman 
bayesian model averaging model selection markov equivalence classes acyclic graphs 
technical report univ washington 
mh meek heckerman 
structure parameter learning causal independence causal interaction models 
uai pages 
min minka 
estimating dirichlet distribution 
technical report mit 
min minka 
variational bayes mixture models reversing em 
technical report mit 
mm murphy mian 
modelling gene expression data dynamic bayesian networks 
technical report berkeley dept comp 
sci 
madigan york 
bayesian graphical models discrete data 
intl 
statistical review 
nh nowlan hinton 
simplifying neural networks soft weight sharing 
neural computation 
pea pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
pea pearl 
causality models reasoning inference 
cambridge univ press 
pv pearl verma 
theory inferred causation 
knowledge representation pages 
raf raftery 
hypothesis testing model selection posterior simulation 
markov chain monte carlo practice 
chapman hall 
rg geiger 
parameter priors discrete dag models 
ai stats 
david spiegelhalter philip dawid ste en lauritzen robert cowell 
bayesian analysis expert systems 
statistical science 
sgs spirtes glymour scheines 
causation prediction search 
mit press 
nd edition 
sgs spirtes glymour scheines 
constructing bayesian network models gene expression networks microarray data 
proc 
atlantic symposium computational biology genome information systems technology 
shi 
cause correlation biology user guide path analysis structural equations causal inference 
cambridge 
sl spiegelhalter lauritzen 
sequential updating conditional probabilities directed graphical structures 
networks 
sm spirtes meek 
learning networks discrete variables data 
proc 
conf 
knowledge discovery data mining 
snr newton raftery 
easy estimation normalizing constants bayes factors posterior simulation stabilizing harmonic mean estimator 
technical report washington 
sv singh 
algorithm construction bayesian network structures data 
uai 
tw tanner wong 
calculation posterior distributions data augmentation 
jasa 
vp verma pearl 
equivalence synthesis causal models 
uai 
wel wellman 
fundamental concepts qualitative probabilistic networks 
arti cial intelligence 
wj wittig jameson 
exploiting qualitative knowledge learning conditional probabilities bayesian networks 
uai 
york madigan lie 
birth defects registered double sampling bayesian approach incorporating covariates model uncertainty 
applied statistics 

