lightly supervised acoustic model training lori lamel jean luc gauvain gilles adda spoken language processing group limsi cnrs bp orsay cedex france gauvain limsi fr tremendous progress speech recognition technology capability todays state art systems transcribe unrestricted continuous speech broadcast data systems rely availability large amounts manually transcribed acoustic training data 
obtaining data time consuming expensive requiring trained human annotators substantial amounts supervision 
describe experiments lightly supervised techniques acoustic model training order reduce system development cost 
strategy investigate uses speech recognizer transcribe unannotated broadcast news data optionally combines hypothesized transcription associated unaligned closed captions transcripts create labeled training 
show approach dramatically reduces cost building acoustic models 

decade witnessed substantial progress large vocabulary continuous speech recognition 
number sites state art systems transcribe unrestricted continuous speech unknown speakers taken american english television radio broadcasts word errors 
today technology adaptation recognition system new task language requires large amounts transcribed training data 
generating transcribed data expensive process terms manpower time 
certain sources radio television news broadcasts provide essentially unlimited supply acoustic training data 
vast majority audio data sources corresponding accurate word transcriptions 
sources particular main american television channels broadcast manually derived closed captions 
closed captions close exact transcription spoken coarsely time aligned audio signal 
manual transcripts available certain radio broadcasts 
describe experiments lightly partially financed european commission language engineering project le 
avoid confusion group types transcripts refer closed captions 
supervised acoustic model training 
basic idea speech recognizer automatically transcribe unannotated data generating labeled training data 
iteratively increasing amount training data accurate acoustic models obtained transcribe set unannotated data 
compare straightforward method training automatically annotated data closed captions transcripts filter hypothesized transcriptions removing words incorrect 
data train acoustic models reported 
bbn describes experiments completely unsupervised training conversational speech switchboard callhome corpora reports small improvements data addition hours annotated data compared training original hours 
results conjecture order magnitude data needed achieve comparable levels performance transcribed data 
kemp waibel report significant word error reductions data german broadcast news transcription source 
show comparable levels performance obtained twice data transcribed data hours versus hours 
authors give little information data train language models difficult assess level supervision 
section presents basic ideas lightly supervised training followed description corpora overview limsi broadcast news transcription system 
experimental results section 
lightly supervised training hmm training requires alignment audio signal phone models usually relies perfect orthographic transcription speech data phonetic lexicon 
general easier deal relatively short speech segments transcription errors propagate jeopardize alignment 
orthographic transcription usually considered ground truth training done closely supervised manner 
speech segment training algorithm provided exact orthographic transcription spoken isca asr paris lamel gauvain adda word sequence speech recognizer hypothesize confronted speech segment 
training acoustic models new corpus reflect change task language usually entails sequence operations audio data transcription files loaded ffl normalize transcriptions common format adjustment needed different text sources different conventions 
ffl produce word list transcriptions correct errors include typographical errors inconsistencies ffl produce phonemic transcription words master lexicon manually verified 
ffl align orthographic transcriptions signal existing models pronunciation lexicon bootstrap models task language 
procedure rejects substantial portion data particularly long segments 
ffl eventually correct transcription errors just ignore audio data available ffl run standard em training procedure 
procedure usually iterated times refine acoustic models 
general iteration recovers portion rejected data 
imagine training acoustic models supervised manner iterative procedure manual transcriptions alignment iteration word transcription current models information available audio sample 
approach fits em training framework suited missing data training problems 
completely unsupervised training procedure current best models produce orthographic transcription training data keeping words high confidence measure 
approach limited supervision provided confidence measure estimator 
estimator turn trained development data needs small keep approach interesting 
carefully annotated data detailed transcriptions provided ldc transcription wide spectrum possibilities 
really important cost producing associated annotations 
detailed annotation requires order times real time manual effort manual verification final transcriptions exempt errors 
orthographic transcriptions closed captions done times real time quite bit costly 
transcriptions advantage available television channels produced specifically training speech recognizers 
approach possible sources texts newswires summaries internet 
sources indirect correspondence audio data provide supervision 
problems faced dealing closed captions speech transcriptions 
addition providing exact word level transcription said detailed speech transcriptions provide wealth additional information available closed captions 
includes marking non speech events respiration throat clearing indication speaker turns speaker identities gender indication acoustic conditions presence background music noise transmission channel annotation non speech segments music 
closed captions true orthographic transcription speech 
repetitions marked may word insertions changes word order 
nist disagreement closed captions manual transcripts hour subset tdt data sdr evaluation order 
order closed captions training need automatically produce missing information audio segmentation speaker turns speaker identifiers identifying nonspeech segments acoustic conditions 
gaussian mixture models sex bandwidth identification trained small amount data required labeling costly 
word closed needs aligned audio signal allow transcription errors insertions deletions 
training procedure ffl train language model texts closed captions normalization ffl partition show homogeneous segments label acoustic attributes speaker gender bandwidth ffl train acoustic models small amount manually annotated data ffl automatically transcribe large amount training data ffl align closed captions automatic transcriptions standard dynamic programming algorithm ffl run standard acoustic model training procedure speech segments transcripts agreement ffl reiterate step 
easy see manual considerably reduced generating annotated corpus training procedure longer need deal new words word fragments data need correct transcription errors 
basic idea align automatically generated word transcriptions hours audio broadcasts spoken document task nist sdr 
isca asr paris lamel gauvain adda 
corpora unannotated audio data experiments taken darpa tdt corpus sdr sdr evaluations 
audio corpus sdr contains hours data shows sources cnn headline news minute shows abc world news tonight minute shows public radio international world hour shows voice america today world report hour shows 
data broadcast january june 
data comes associated closed captions commercial transcripts 
divided stories identifying story average duration min secs story 
hub acoustic training data releases ldc www ldc upenn edu contain total hours carefully annotated data variety sources abc world news world news tonight cnn early prime headline news prime news world today early edition prime time live washington journal public policy npr things considered marketplace 
addition word transcriptions annotations include speech fragments non speech events speaker turns identities markers overlapping portions non english speech 
language model training data hub task exception manual transcriptions acoustic training data word list selection language model estimation 
data include words newspaper newswire texts distributed ldc jan may hub tdt corpora words commercial broadcast news transcripts distributed ldc years bought directly years closed captions june distributed part tdt corpus 
testing purposes hub evaluation data comprised minute data sets selected nist 
set extracted hours data broadcast june second set set broadcasts recorded august september 

system description limsi broadcast news transcription system main components audio partitioner word recognizer 
data partitioning serves divide continuous stream acoustic data segments associating appropriate labels segments 
segmentation labeling procedure detects rejects non speech segments applies iterative maximum likelihood segmentation clustering procedure speech segments 
result partitioning process set speech segments cluster gender telephone wideband labels 
speech recognizer uses continuous density hmms gaussian mixture acoustic modeling gram statistics estimated large text corpora language modeling 
context dependent phone model tied state left right cd hmm gaussian mixture observation densities tied states obtained means decision tree 
word recognition performed steps initial hypothesis generation word graph generation final hypothesis generation 
initial hypotheses cluster acoustic model adaptation mllr technique prior word graph generation 
gram language model decoding passes 
final hypotheses generated gram language model acoustic models adapted hypotheses step 
baseline system darpa evaluation tests acoustic models trained hours audio data darpa hub broadcast news corpus ldc broadcast news speech collections 
august february releases ldc transcriptions 
overlapping speech portions detected transcriptions removed training data 
acoustic feature vector components comprised cepstrum log energy second order derivatives 
gender dependent acoustic models built map adaptation si seed models wideband telephone band speech 
computational reasons smaller sets acoustic models decoding pass 
position dependent crossword triphone models cover contexts tied states gaussians state 
second third decoding passes larger set position dependent cross word triphone models tied states approximately gaussians 
baseline language models obtained interpolation backoff gram language models trained different data sets bn transcriptions nab newspapers ap texts excluding test epochs transcriptions bn acoustic data 
baseline recognition vocabulary contains words phone transcriptions lexical coverage evaluation test sets years 
pronunciation graph associated word allow alternate pronunciations including optional phones 
pronunciations set phones set phone units represent silence filler words breath noises 
filler breath phones model events transcribing lexical entries 
lexicon contains compound words frequent word sequences word entries common acronyms providing easy way allow reduced pronunciations 
limsi system obtained word error darpa nist evaluation set combined scores fourth row table transcribe unrestricted broadcast data word error 
word error reduced system running xrt entry table 
isca asr paris lamel gauvain adda training conditions bn bn average lmc lmc lmc lmc lma lma table word error rate various conditions acoustic models trained hub training data detailed manual transcriptions 
runs done xrt run column 
designates set gender independant acoustic models designates sets gender bandwidth dependent acoustic models 
lma language model results interpolation lm trained detailed acoustic transcriptions trained text sources excluding tdt closed captions 
lmc trained texts tdt closed captions detailed acoustic transcriptions 

experimental results section summarize series experiments assess recognition performance function available acoustic language model training data 
recognition runs carried xrt stated 
mentioned usual procedure build language models bn data interpolate gram lms built sources texts large amounts newspaper newswire texts large amounts commercial bn transcriptions smaller amounts available detailed bn transcriptions 
aim investigate acoustic model training data detailed transcriptions built language models experiments replacing detailed transcriptions commercial transcriptions closed captions radio transcripts tdt data 
doing new word list selected word frequencies training data excluding detailed transcriptions 
including tdt closed captions language model training data provides supervision decoding process transcribing tdt audio data produce transcriptions training purposes 
language models result interpolation individual lms built text source 
language model interpolation coefficients chosen order minimize perplexity development set composed second set nov evaluation data portion tdt data jun included lm training data 
resulting interpolation coefficients commercial transcript lm newspaper lm tdt closed lm 
seen rows table word error rates original language model lma new lmc give comparable results eval test data acoustic models trained hours manually annotated data 
experiments run lmc language model set gender bandwidth independent acoustic models 
order bootstrap training procedure initial set acoustic models trained minutes manually transcribed data ldc hub corpus 
data consist shows abc cnn early prime npr things considered 
acoustic models quite small compared standard hub models 
pass models cover triphone contexts tied states gaussians second third pass models cover triphone contexts tied states gaussians respectively manually transcribed data bootstrap process building successive model sets 
small models transcribe broadcasts hours data 
methods investigated automatically transcribed data acoustic model training 
method hypothesized transcriptions aligned closed captions story story regions automatic transcripts agreed closed captions kept training purposes 
alignment hours speech data available training 
second method consists simply training aligned data trying filter recognition errors 
case hours data available 
cases closed story boundaries delimit audio segments automatic transcription 
labeled data train substantially larger acoustic models 
models transcribe additional shows 
shows processed hours data resulting hours aligned acoustic data prior filtering hours filtering 
data models sets close size baseline system built 
pass models cover triphones tied states gaussians third pass models cover triphones sharing states gaussians 
acoustic model sets trained subsets automatically transcribed data assess recognition performance function available data 
unfiltered model sets larger terms number triphone contexts covered total number gaussians built filtered data 
recognition results sets hub evaluation test shown table 
results compared rows table report results detailed manual transcriptions training data 
observations results 
expected training data word error rate decreases 
true filtered unfiltered training 
word error reduction saturate amount training data increases hope lower difference amounts data transcribed training due factors 
total duration includes non speech segments eliminated prior recognition partitioning 
secondly story boundaries closed captions eliminate irrelevant portions commercials 
thirdly remaining silence frames portion retained training 
isca asr paris lamel gauvain adda amount training data bn bn average unfiltered filtered unfiltered filtered unfiltered filtered unfiltered filtered table word error rate increasing quantities automatically label training data evaluation test sets gender bandwidth independent acoustic models language model lmc 
runs done xrt 
error rate continuing procedure 
filtering automatic transcripts closed captions reduces word error relative compared error rate obtained simply training available data 
including closed captions language model training data provide supervision ensure proper convergence training procedure 
best word error rate obtained procedure higher obtained training hours detailed annotated transcriptions versus models part difference may due fact different corpora training conditions believe essentially due difference transcription qualities 
differences arise errors procedure word boundary problems incorrect labeling non speech events breath noises supervision available 

summary discussion investigated low cost data train acoustic models broadcast news transcription supervision provided closed captions 
show recognition results obtained acoustic models trained large quantities automatically annotated data comparable relative increase word error results acoustic models trained large quantities data detailed manual annotations 
significantly higher cost detailed manual transcription substantially time consuming producing commercial transcripts expensive money considered closed captions commercial transcripts produced purposes interest explore methods requiring substantial computation time little manual effort 
advantage offered approach need extend pronunciation lexicon cover words word fragments occurring training data 
appears closed captions provide supervision language model sufficient small advantage filter system hypotheses 
believe effective ways closed captions improve models 
procedure bootstrapped training acoustic models hour manually transcribed data 
recognition error rate reached lower limit continuing procedure process tdt data 
adda gauvain language modeling broadcast news transcription proc 
esca eurospeech budapest hungary pp 
september 
barras wu mark liberman transcriber development tool assisting speech corpora production appear speech communication 
graff liberman tdt text speech corpus proc 
darpa broadcast news workshop herndon va 
see morph ldc upenn edu tdt 
gauvain adda lamel adda decker transcribing broadcast news limsi nov hub system proc 
arpa speech recognition workshop va pp 
february 
gauvain lamel fast decoding indexation broadcast data appear proc 
icslp beijing october 
gauvain lee maximum posteriori estimation multivariate mixture observation markov chains ieee trans 
sap pp 
april 
garofolo voorhees fisher trec spoken document retrieval track overview results proc 
th text retrieval conference trec november 
graff broadcast news speech corpus proc 
arpa speech recognition workshop va pp 
february 
kemp waibel unsupervised training speech recognizer experiments proc 
esca eurospeech budapest hungary pp 
september 
woodland maximum likelihood linear regression speaker adaptation continuous density hidden markov models computer speech language pp 

pallett fiscus przybocki broadcast news test results proc 
nist nsa speech transcription workshop college park maryland may 
waibel mayfield schultz multilinguality speech spoken language systems ieee special issue spoken language processing 
utilizing training data improve performance proc 
darpa transcription va pp 
february 
isca asr paris lamel gauvain adda 
