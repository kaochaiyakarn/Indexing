extraction relevant delays temporal modelling cyril goutte modelling temporal processes just pattern recognition selecting optimal number inputs central concern 
contribution take advantage specific features temporal modelling propose novel method extracting inputs yield best predictive performance 
method relies generalisation estimators assess performance model 
technique applied time series processing perform number experiments synthetic data real life dataset compare results benchmark physical method 
method extended system identification linear fir filter 
submitted ieee press possible publication 
corresponding address department mathematical modelling technical university denmark building dk lyngby denmark 
mail dtu dk overview time series prediction system identification try model behaviour system past plus possibly exogenous control signal 
just pattern recognition performance depends things design proper input regression vector 
inputs model sufficient information unable grasp inner workings system resulting large approximation error 
hand include irrelevant inputs top necessary ones model parameterised usually results poor prediction performance suggested curse dimensionality 
trade particular instance bias variance dilemma 
contribution focus mainly time series prediction address problem system identification 
context potential inputs past values words delays time series 
aim method select delays necessary model system discarding unnecessary delays prove harmful performance 
method related iterative feature selection techniques traditional statistics optimal 
technique builds specificities temporal processing provide original way selecting potential inputs 
relevance candidate delay assessed directly effect modelling performance measured generalisation estimators 
organisation article follows give general presentation feature selection section applied time series modelling statistical physical point view 
suggest estimators generalisation error evaluate quality subset features 
extraction relevant delays erd method described section principled alternative 
second part contains number experiments conducted different datasets 
time series predictions section addressed known artificial map real time series measuring mean monthly flow fraser river erd extended system identification section functional magnetic resonance imaging dataset 
conclude discussion method results 
feature selection consider standard time series modelling problem 
sequence measurements collected wish predict set past values gamma delays 
note setting sampling time length basic time delay difference imposed address problem obtaining best sampling frequency 
extracting relevant delays consists finding set delays dm input vector gamma gamma gamma dm allows best possible prediction 
physical approach physical approach consists trying estimate embedding dimension time series 
essentially equivalent finding set primary delays delays direct influence observed time series values 
example time series generated gamma primary delay 
estimated gamma gamma primary delay 
methods proposed estimate embedding dimension embedding space time series 
neural networks literature pi peterson introduced ffi test address issue :10.1.1.33.1578
system identification lipschitz quotients proposed order identify order non linear input output systems 
similar method applied time series name geometrical technique signal processing literature 
different practice methods rely common assumption continuity relative smoothness underlying mapping geometrical approach data 
continuity argument means mapping close inputs correspond close outputs 
accordingly long input space insufficient missing delays close inputs correspond arbitrarily distant outputs 
illustrated section practical example 
quantifying closeness done measuring empirical probabilities outputs close corresponding inputs close ffi test calculating ratio output input distances lipschitz quotients 
noted techniques mentioned nonparametric 
important side effect consider possible input output pairs require extensive calculations 
furthermore methods rely data need specify model contrary statistical approach 
turn disadvantage data set select set relevant delays regardless ability model implement underlying mapping 
flexible models neural networks uniform approximation capabilities balance limitation 
practice happen model hand estimation benefit inclusion secondary delays 
shown experiments discussed 
statistical approach statistical point view extraction relevant delays special case feature selection turn seen part general problem analysing structure data 
statistical approach relies specifying model try estimate input output mapping 
parametric non parametric model 
important assumption conventional variable selection necessary variables available sufficient subset inputs exists 
provided data sampled correctly assumption usually satisfied case time series 
breaks case long term delay needed ranges time period spanned data 
relevance long term prediction questionable data identify associated parameter 
note delays positive integers variables features past values time series gamma inputs vectors containing past values 
finding relevant delays equivalent finding input give optimal predictive performance 
feature selection relies different components 
selection method searching possible subsets variables 
evaluation criterion order assess quality subsets 
condition 
knowledge general optimal method selecting best features perform exhaustive search possible subsets variables 
usually impossible combinatorial reasons number possible subsets variables gamma 
time series factor fact maximum delay total number variables known advance 
typically accept probe quite far past 
evaluation criterion monotonous branch bound algorithm provides efficient alternative 
computational cost overwhelming cases predictive ability monotonous criterion 
sub optimal methods perform iterative search regularly increasing decreasing number features selected ffl forward selection start empty set variables add variables evaluation criteria condition reached 
ffl backward elimination start full set possible variables proceed deleting variable time evaluation criteria condition reached 
ffl stepwise regression start set usually variables alternate methods 
possibility perform backward elimination inclusion method chooses adding deleting variables evolution evaluation criterion 
note focus neural networks research example entirely backward elimination various pruning schemes 
sub optimal methods rely heavily evaluation criterion 
typical choices include example statistic extensions non linear models mutual information 
alternative choices 
generalisation approach goal time series modelling obtain model best prediction 
natural measure predictive abilities base evaluation criterion 
consider model providing mapping input vector containing delays gamma gamma dm output 
risk measures cost associated estimating output observed model 
assuming gaussian noise output lead choice quadratic risk gamma generalisation error expected risk associated model defined expectation risk unknown fixed joint input output distribution gamma ideally evaluation criterion subset generalisation error model subset input 
equation directly joint input output probability unknown 
shall estimated available data tmax split sample estimator obtained replacing joint distribution empirical estimator particular validation set ae gss jv gamma jv size cardinal set note validation set set estimated training set ae distinct order unbiased estimator 
cross validation estimator resamples validation training sets available data order increase reliability resulting estimator 
fold cross validation split split sample method referred cross validation neural networks literature 
inconsistent definition reserve term exclusively averaging method 
disjoint subsets roughly equivalent size split sample estimator calculated remaining data model estimation averaged subsets gcv gamma gamma gammat delta gammat model estimated excluding subset containing strictly speaking equation estimate generalisation error average possible datasets size sampled 
number analytical estimators average generalisation error proposed literature final prediction error fpe generalised prediction error gpe final prediction error regularised problems network information criterion nic 
settle expression similar gpe final prediction error number parameters replaced number efficient parameters gamma training error average risk dataset model estimated gamma calculation depends estimator regularisation method training see 
estimators generalisation error evaluation criterion order derive original delay extraction scheme 
extraction relevant delays statistical feature selection applies general regression problems 
particular application time series prediction extent system identification displays number characteristics worth mentioning 
features interest past values delays time series 
mentioned means possible variables available 
hand maximum delay unknown priori 
consequence potentially necessary features upper bound number order 
extraction relevant delays bad candidate backward elimination schemes 
chronological order gives natural way ordering selecting variables 
scheme takes advantage order select candidate delays natural chronological order 
rationale choice test primary delays secondary delays 
due temporal structure data secondary delays necessarily placed primary delay related gamma gamma 
secondary delay tested inclusion bring prediction model unable represent underlying mapping primary delays secondary delay primary delay heuristic justified fact traditional caveat sequential selection variables combinations variables confuse feature selection algorithm 
context time series prediction combination variables arise naturally past values series 
chronological selection ensures variable tested inclusion variables depends 
guarantees selection stays parsimonious 
stress fact claim heuristic choice optimal 
experiments suggest results appear close optimum 
proposed extraction relevant delays method forward selection method 
takes delays natural order adds candidate variable yields significant decrease generalisation error 
algorithm described follows gamma gamma primary delay secondary delay primary delay 

initialise oe time series variance input selected 

model add delay gamma selected inputs estimate generalisation error resulting model 

test significantly smaller keep delay gamma set discard delay gamma 
iterate go step condition reached 
significant decrease error 
candidate delay yields decrease estimated generalisation error step requires assess significance decrease 
requirement avoids include delay leads negligible decrease generalisation error happen chance 
take advantage fact generalisation estimators described averaging statistics test statistics associated generalisation estimators obtained different models statistically significantly distinct means 
inspection equations shows written generally ffl sum runs jv split sample cross validation fpe elements 
ffl residual expression depends estimator ss ffl gamma cv ffl gamma gammat fpe ffl gamma gamma wish test gm generalisation error model delays current best algorithm different gm generalisation error model delays augmented candidate delay 
done basis residuals ffl ffl average gm gm unbiased estimators gm gm respectively 
gaussian noise assumption generalisation error roughly distributed 
assuming difference residuals ffl ffl approximately normally distributed fact residuals calculated data assess significance observed decrease generalisation error tailed paired test residuals 
operational reasons amount data available models different 
typically candidate model additional long term delay meaning input output pairs formed 
order apply paired test necessary discard additional data examples smaller model 
waste couple data points counter balanced superior power paired test compared non paired version 
algorithm requires condition 
set heuristic way specifying maximum delay willing probe maximum period allowed delay selection 
roughly periodic data fraser river fmri datasets example reasonable decide want go back limited number periods 
time series prediction map consider time series generated known map gamma gamma gamma equation generate dataset containing data 
independently dataset elements generated order empirically test generalisation abilities 
non noisy noisy maps investigated additive gaussian noise variance oe 
experiments different models ffl linear model fpe generalisation estimator 
obviously bad choice modelling non linear map 
ffl kernel smoother leave cross validation fold generalisation estimator 
erd compared results obtained competitors 
embedding dimension estimated ffi test 
traditional statistical approach tested inclusion scheme forward method statistic 
scheme candidate delay included rm gamma rm rm gamma gamma ff rm rm average risk models delays delays respectively calculated data ff threshold ff confidence level gamma gamma degrees freedom 
interpretation candidate included leads significance improvement observed performance 
order test performance generalisation estimators generate third independent set order provide additional unbiased estimate generalisation error similar 
estimate model decide candidate delays included paired test 
note large validation set divine guidance method course impractical real data 
order check behaviour generalisation estimators erd 
figures show plot proposed investigate embedding dimension 
point corresponds pair data 
points displayed close inputs kx gamma small 
clearly delay input close inputs guarantee close outputs 
continuous mapping time series values delays 
hand delays points displayed show data pairs close inputs close outputs output distance close inputs delay plot output distance vs input input contains delay 
point represents pair data points smallest input distances included 
notice close inputs lead distant outputs indicating insufficient input space 
map noise noisy linear kernel linear kernel lvs erd inclusion ffi test table delays selected map dataset methods divine guidance method large independent validation set lvs proposed extraction relevant delays erd method section inclusion method equation ffi test 
suggests continuous mapping delays time series value implemented 
output distance close inputs delays plot output distance vs input input contains delay 
point represents pair data points smallest input distances included 
points output distance small indicating sufficient input space 
accordingly ffi test selects delays map dataset 
methods select additional delay shown table 
resulting models tested independent noise free data order check generalisation abilities 
surprisingly flexible kernel smoother provides better modelling performance linear model 
note noise free case selection methods extract delays kernel smoother able implement map perfectly available data 
second salient feature ffi test manages outperform statistical methods kernel smoother noisy data 
means imply method inefficient 
contrary manages efficiently extract true delays data 
argue address relevant goal prediction time series value suffers limitation 
close lvs erd inclusion delta test generalisation error results map kernel noise kernel noisy linear noise linear noisy results map models noise conditions selections schemes divine guidance method large validation set lvs proposed extraction relevant delays erd method section inclusion method equation ffi test 
agreement large validation set lvs erd results shows generalisation estimators erd algorithm perform duty efficiently 
behaviour inclusion scheme kernel smoother noisy data suggests method adapted non linear models especially flexible models 
due presence noise estimate favours fitting 
flexible model fits data overly small denominator small 
consequence statistic biased upwards leads overly optimistic selection delays case methods 
notice table erd favour parsimonious models delays linear model lvs keeping generalisation abilities 
interesting notice noisy data lvs erd methods select delay ffi test 
due fact try learn underlying relationship limited number data 
conditions additional delay theoretically necessary improves predictive performance significant 
fraser river turn attention real time series dataset 
data records mean monthly measurements flow fraser river hope british columbia canada march december 
dataset analysed publicly available 
contains values rough periodicity maximum months 
shows measurements recorded january january 
data set split half data model estimation delay extraction testing generalisation abilities resulting model 
experiments data centred 
selection method large validation set course sensible context extra data available 
accordingly consider selection schemes proposed extraction relevant delays ffi test 
applied models ffl linear model ffl kernel smoother ffl non linear neural network model multi layer perceptron hidden layer 
parameters linear neural network models estimated minimising mean squared error transformed data 
generalisation estimators fpe gpe respectively 
kernel smoother generalisation estimator leave estimator 
non parametric ffi test selects delays erd extracts delays depending model jan jan jan jan month fraser river time series mean monthly flow fraser river recorded hope british columbia january january solid line 
twelve month year moving averages shown dotted dashed lines respectively 
fraser river linear kernel neural net erd ffi test table delays extracted model methods proposed extraction relevant delays erd ffi test 
table 
surprisingly flexible models kernel smoother neural network delay linear model uses twice 
interesting notice ffi test erd extract number delays models delays coincide 
typically erd probes periods neural network linear model 
interesting artefact ffi test method tries consider pairs data close inputs 
number inputs grows linear kernel nn linear kernel nn training error results fraser river data full marks empty marks erd delta test result different models extraction schemes proposed extraction relevant delays erd full marks ffi test empty marks 
models linear kernel smoother non linear neural network nn 
fewer fewer pair meet requirement 
leads high variance estimated probabilities test unable select additional delays 
results schemes models measured calculating prediction error test set 
shows predictive performance plotted versus training performance methods 
points far dotted line indicate probable fitting 
surprisingly kernel smoother fit data quite severely 
models known suffer acutely curse dimensionality problems handling high dimensional inputs especially smoothing parameter 
surprising result best performance achieved combination erd linear model 
flexible neural network model trained combination weight decay optimal brain surgeon obs pruning better linear model training set worse test set 
noted performance reported depends mainly factors training algorithm extracted delays 
training algorithm neural network theory able outperform linear model set delays 
erd scheme predictably extracted delays models 
particular linear model uses long term delays neural network access 
results illustrate limitations physical approach estimation embedding dimension take account model predicting 
statistical approach model performance allows gather information past yields improvement performance impressive linear model 
system identification extend erd scheme extraction relevant delays simple system identification problem 
dataset acquired functional magnetic resonance imaging fmri experiment 
subject lies scanner asked lie motionless rest perform simple motor task activation 
experiment task left handed finger thumb opposition 
slices brain echo planar fmri images acquired seconds 
dimension voxel theta theta mm 
runs images acquired sequence images seconds rest images seconds activation images rest 
complete time series consists measurements minutes voxel 
experiments analyse slice focus theta area containing brain 
fmri signal measures response focal neuronal activation 
widely believed response brain characterised convolution stimulus signal representing activation linear filter 
experiment stimulus square wave positive values scans acquired subject performing motor task negative values rest 
previous approaches tried voxels image number time average fmri time series right brain map indicating activated voxels black analysed erd 
background anatomical obtained average activation 
left average time series activated voxels thick line stimulus thin black 
characterise response convolution filters poisson gamma fir filter fixed length 
try extend general fir filters extracting delays yield best modelling performance 
words looking set delays dm fi gamma models observed fmri signal possible 
extension erd algorithm section straightforward step algorithm delays added inputs delays stimulus time series 
modelling generalisation estimations independent particular choice inputs rest algorithm affected extension 
forming cross correlation time series signal isolate voxels display significant activation 
pre processing step isolates groups voxels covering primary motor area sma 
expected left handed opposition produces contra lateral activation right hemisphere 
delay voxel 
extracted relevant delays delays extracted voxel white squares 
gray line indicates average delay indexes 
erd algorithm run voxel order extract delays allow best modelling fmri signal signal 
runs corresponding data points delay extraction modelling remaining runs points test generalisation abilities resulting model 
condition set limiting extraction delays fir filter probe run past 
order loosen selection criterion agreement standard statistical practice subset selection significance level paired test set 
performance filter extracted delays compared fir filter fixed size 
average delay observed seconds images fir filter delays 
shows delays extracted voxel 
range selected voxels 
average delay estimated computing weighted mean delays indices absolute filter coefficients fi weights 
average value short delays long delays test error erd test error parameter fir filter versus test error fir filter extracted erd 
crosses indicate voxels average delay seconds 
circles indicate voxels longer term delays seconds 
voxel large test error cases discarded improve clarity 
activated voxels roughly divided categories time series average delay seconds roughly accepted range seconds 
remaining voxels average delays seconds suggests extracted delays sense biological point view manage efficiently improve modelling relationship signal fmri 
performance erd scheme contrasted fixed length fir filter delays 
plots test error fixed length fir filter test error fir filter delays extracted erd 
points dotted diagonal show test error fixed length filter higher modelling worse 
clearly voxels benefit extracted delays display small decrease performance 
noted erd filters yield better performance fewer parameter 
displays location average fmri activation image number time average fmri time series voxels voxels short term delay image number time average response average fmri time series voxels voxels long term delay voxels short term delays top seconds voxels long term delays bottom seconds biological explanation 
brain maps left show location analysed voxels black dots right plot shows average fmri signal voxels thick signal thin 
signal groups voxels 
voxels short term delays top long term delays bottom slightly different patterns activation especially runs 
small sample sizes respectively strict comparison difficult 
discussion main difference ffi test extraction relevant delays matter approach 
physical approach investigates true delays probable physical meaning 
called primary delays 
main advantage approach probably closest get terms interpretation selected delays 
hand statistical approach embodied erd method focuses performance 
extracts delays yield model best generalisation directly optimising relevant performance criterion 
resulting delays easily interpreted resulting predictive performance improved 
exemplified fmri example quarter filters apparent biological justification 
problems time series prediction system identification modelling performance usually especially model knowingly parametric model system 
raises related issue 
physical approach assumption underlying model intend predict time series system 
means delays need extracted wish compare models dataset 
hand output erd method totally dependent model 
means extraction relevant delays repeated model wish 
note erd scheme straightforward implement usually demanding computationally physical counterparts ffi test lipschitz quotients 
particular extraction relevant delays simple linear model extremely fast 
interesting side effect erd method takes naturally account limited dataset size 
estimation model parameters done limited number data leading possible variability model estimate 
similar effect known aic bic information criteria bic selects asymptotically optimal model size aic tends estimate 
aic effectively minimises average generalisation error finite datasets 
similarly erd method selects additional delays true delays help predict time series 
map methods generalisation es include additional delay leads sizeable improvement predictive performance 
example shows generalisation estimators erd pick right model size model size generalises best large validation set test set 
generalisation estimation crucial part erd method emphasised interested location minimum generalisation precise value error 
erd scheme accommodate estimator consistent bias 
extraction relevant delays method contribution essentially statistical forward selection method 
straightforward apply scheme backward elimination loss generalisation evaluation criterion 
argued section probably badly suited temporal modelling problems addressed 
hand straightforward extension design method similar stepwise regression alternate forward backward cycles 
models addressed model identification estimation procedure 
kernel smoothers adaptive metric handle irrelevant inputs 
linear models neural networks interesting option regularisation term pruning effect 
methods allow discard variables included input vector turn irrelevant point 
interesting prospect research extend erd scheme general system identification architecture 
autoregressive filter exogenous input arx natural candidate 
note case filter input consists time series values 
precise ordering candidate inputs way alternate delays system output control signal determined 
generalisation method extraction relevant delays temporal modelling focus time series prediction system identification 
method related traditional forward selection methods wellfounded 
easy implement requires little time addition model estimation 
furthermore accommodates efficiently tradeoff number selected delays model size model flexibility approximation error 
method provides interesting results number experiments performed different contexts 
fraser river dataset section publicly available statlib lib stat cmu edu datasets 
fmri dataset section acquired robert savoy general hospital boston author patrick gallinari jan larsen learning group imm dtu valuable discussions previous versions 
supported research fellowship technical university denmark 
akaike 
fitting autoregressive models prediction 
annals institute statistical mathematics 
akaike 
new look statistical model identification 
ieee transactions automatic control december 
zoran 
estimating embedding dimension 
physica 
peter eric wong james hyde 
processing strategies time course data sets functional mri human brain 
magnetic resonance medicine august 
battiti 
mutual information selecting features supervised neural net learning 
ieee transactions neural networks 
robert 
comparison stopping rules forward stepwise regression 
journal american statistical association march 
turner 
analysis functional mri time series 
human brain mapping 
stuart geman elie bienenstock ren doursat 
neural networks bias variance dilemma 
neural computation january 
cyril goutte 
pruning prior neural networks 
wilson editors neural networks signal processing vi proceedings ieee workshop number vi pages piscataway new jersey 
ieee 
ftp imm dtu dk dist goutte abs 
cyril goutte 
lag space estimation time series modelling 
proceedings icassp pages 
ieee 
ftp imm dtu dk dist goutte icassp ps gz 
cyril goutte lars kai hansen 
regularization pruning prior 
neural networks 
ftp imm dtu dk dist goutte regularization ps cyril goutte jan larsen 
adaptive metric kernel regression 
neural networks signal processing viii proceedings ieee workshop number viii page press piscataway new jersey 
ieee 
ftp imm dtu dk dist goutte html 

applied nonparametric regression 
number econometric society monographs 
cambridge university press 
hassibi stork 
second order derivatives network pruning optimal brain surgeon 
hanson cowan giles editors advances neural information processing systems volume nips pages 
morgan kaufmann 
asada 
new method identifying orders input output models nonlinear dynamic systems 
american conference control san francisco california 

comm 
math 
phys 
rev 

analysis selection variables linear regression 
biometrics march 
hornik stinchcombe white 
multilayer feedforward networks universal approximators 
neural networks 
nicholas lange scott zeger 
non linear fourier time series analysis human brain mapping functional magnetic resonance imaging 
journal royal statistical society series applied statistics 
larsen hansen 
generalized performance regularized neural networks models 
hwang wilson editors neural networks signal processing iv proceedings ieee workshop number iv pages piscataway new jersey 
ieee 
larsen hansen 
empirical generalization assessment neural network models 
girosi makhoul wilson editors neural networks signal processing proceedings ieee workshop number pages piscataway new jersey 
ieee 
philippe patrick gallinari 
feature selection neural networks 
technical report lip lip 
appear 
mcleod 
diagnostic checking periodic autoregression models application 
journal time series analysis 
christophe molina nick sampson william fitzgerald niranjan 
geometrical techniques finding embedding dimension time series 
neural networks signal processing vi proceedings ieee workshop pages 
moody 
note generalization regularization architecture selection nonlinear learning systems 
juang kung kamm editors proceedings ieee workshop neural networks signal processing number pages piscataway new jersey 
ieee 
murata amari 
network information criterion determining number hidden units artificial neural network model 
ieee transactions neural networks 
narendra fukunaga 
branch bound algorithm feature subset selection 
ieee transactions computers 
finn nielsen lars kai hansen peter cyril goutte niels claus robert savoy bruce rosen peter born 
comparison convolution models fmri time series 
holm lassen nowak editors third international conference functional mapping human brain neuroimage part page 
academic press may 
imm dtu dk 
hong pi carsten peterson 
finding embedding dimension variable dependences time series 
neural computation may 
press teukolsky vetterling flannery 
numerical recipes cambridge university press nd edition 
savit green 
time series dependent variables 
physica 
gideon schwartz 
estimating dimension model 
annals statistics 
stone 
cross choice assessment statistical predictions 
journal royal statistical society 
discussion 
toussaint 
bibliography estimation misclassification 
ieee transactions information theory july 
wand jones 
kernel smoothing 
number monographs statistics applied probability 
chapman hall london 

