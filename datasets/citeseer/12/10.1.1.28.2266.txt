lee giles steve lawrence tsoi noisy time series prediction recurrent neural network grammatical inference machine learning volume number july august pp 

noisy time series prediction recurrent neural network grammatical inference lee giles steve lawrence ah chung tsoi research nj nec com act edu au nec research institute independence way princeton nj faculty informatics university nsw australia financial forecasting example signal processing problem challenging due small sample sizes high noise non stationarity non linearity 
neural networks successful number signal processing applications 
discuss fundamental limitations inherent difficulties neural networks processing high noise small sample size signals 
introduce new intelligent signal processing method addresses difficulties 
method proposed uses conversion symbolic representation selforganizing map grammatical inference recurrent neural networks 
apply method prediction daily foreign exchange rates addressing difficulties non stationarity overfitting unequal priori class probabilities find significant predictability comprehensive experiments covering different foreign exchange rates 
method correctly predicts direction change day error rate 
error rate reduces rejecting examples system low confidence prediction 
show symbolic representation aids extraction symbolic knowledge trained recurrent neural networks form deterministic finite state automata 
automata explain operation system relatively simple 
automata rules related known behavior trend mean reversal extracted 
predicting noisy time series data prediction events noisy time series data commonly done various forms statistical models 
typical neural network models closely related statistical models lee giles institute advanced computer studies university maryland college park md 
estimate bayesian posteriori probabilities appropriately formulated problem 
neural networks successful number pattern recognition applications 
noisy time series prediction neural networks typically take delay embedding previous inputs mapped prediction 
examples models neural networks machine learning methods applied finance see 
high noise high non stationarity time series prediction fundamentally difficult models 
problem learning examples fundamentally ill posed infinitely models fit training data generalize 
order form accurate model desirable large training set possible 
case highly non stationary data increasing size training set results data statistics relevant task hand creation model 

high noise small datasets models prone overfitting 
random correlations inputs outputs great difficulty 
models typically explicitly address temporal relationship inputs distinguish correlations occur temporal order 
approach consider recurrent neural networks rnns 
recurrent neural networks employ feedback connections potential represent certain computational structures parsimonious fashion 
rnns address temporal relationship inputs maintaining internal state 
rnns biased learning patterns occur temporal order prone learning random correlations occur temporal order 
training rnns tends difficult high noise data tendency long term dependencies neglected experiments reported tendency recurrent networks take account short term dependencies long term dependencies network fall naive solution predicting common output 
address problem converting time series sequence symbols self organizing map som 
problem grammatical inference prediction quantity sequence symbols 
possible take advantage known capabilities recurrent neural networks learn grammars order capture predictability evolution series 
recurrent neural network significant reasons firstly temporal relationship series explicitly modeled internal states secondly possible extract rules trained recurrent networks form deterministic finite state automata symbolic conversion significant number reasons quantization effectively filters data reduces noise rnn training effective symbolic input facilitates example form inputs delay embedding previous values series 
rules extracted feedforward networks types recurrent neural networks recurrent network approach deterministic finite state automata extraction particularly suitable time series problem 
extraction rules trained networks 
furthermore argued instantiation rules predictors gives user confidence prediction model facilitates model lead deeper understanding process 
financial time series data typically contains high noise significant non stationarity 
noisy times series prediction problem considered prediction foreign exchange rates 
brief overview foreign exchange rates section 
foreign exchange rates foreign exchange market world largest market changing hands day 
important currencies market dollar acts currency japanese yen british pound german mark swiss franc 
foreign exchange rates exhibit high noise significant non stationarity 
financial institutions evaluate prediction algorithms percentage times algorithm predicts right trend today time 
considers prediction direction change foreign exchange rates business day 
remainder organized follows section describes data set section discusses fundamental issues ill posed problems curse dimensionality vector space assumptions section discusses efficient market hypothesis prediction models 
section details system prediction 
comprehensive test results contained section section considers extraction symbolic data 
section provides concluding remarks appendix provides full simulation details 
exchange rate data data publicly available www stern nyu edu time series santafe html 
weigend 
data consists daily closing bids currencies german mark dm japanese yen swiss franc british pound canadian dollar respect dollar monetary yearbook chicago exchange 
data points exchange rate covering period september may 
contrast weigend considers prediction exchange rates data prediction days week just 
fundamental issues section briefly discusses fundamental issues related learning example techniques multi layer perceptrons mlps ill posed problems curse dimensionality vector space assumptions 
help form motivation new method 
problem inferring underlying probability distribution finite set data fundamentally ill posed problem infinitely solutions infinitely functions fit training data exactly differ regions input space 
problem posed additional constraints 
example constraint limited size mlp imposed 
case may unique solution best fits data range solutions model represent 
implicit assumption constraint underlying target function smooth 
smoothness constraints commonly learning algorithms 
smoothness constraint reason expect model perform unseen inputs generalize 
learning example operates vector space function mapping approximated 
problem vector spaces representation reflect additional structure data 
measurements related neighboring pixels image neighboring time steps time series measurements relationship treated equally 
relationships inferred statistical manner training examples 
recurrent neural network mlp window time delayed inputs introduces assumption explicitly addressing temporal relationship inputs maintenance internal state 
seen similar hints help problem ill posed 
curse dimensionality refers exponential growth function dimensionality 
consider regression hypersurface arbitrarily complex unknown dense samples required approximate function accurately 
hard obtain dense samples high dimensions curse dimensionality 
relationship sampling density number points required dimensionality input space number points 
number points sampling density dimension order keep density dimensionality increased number points increase kolmogorov theorem shows continuous function dimensions completely characterized dimensional continuous function 
specifically kolmogorov theorem states continuous function xn gf universal constants depend fq universal transformations depend continuous dimensional function totally characterizes xn gf typically highly non smooth 
words continuous function arguments dimensional continuous function completely characterizes original function 
seen problem dimensionality complexity function curse dimensionality essentially says high dimensions fewer data points available target function simpler order learn accurately data 
suggested mlps suffer curse dimensionality 
true general mlps may cope better models 
apparent avoidance curse dimensionality due fact function spaces considered constrained input dimension increases 
similarly smoothness conditions satisfied results 
recurrent neural network important viewpoint curse dimensionality rnn take account greater history input 
trying take account greater history mlp increasing number delayed inputs results increase input dimension 
problematic small number data points available 
self organizing map symbolic conversion important viewpoint curse dimensionality 
seen topographical order som allows encoding dimensional som single input rnn 
efficient market hypothesis efficient market hypothesis emh developed fama broad acceptance financial community 
emh weak form asserts price asset reflects information obtained past prices asset movement price unpredictable 
best prediction price current price actual prices follow called random walk 
emh assumption news promptly incorporated prices news unpredictable definition prices unpredictable 
effort expended trying prove disprove emh 
current opinion theory disproved evidence suggests capital markets efficient 
emh true financial series modeled addition noise component step zero mean gaussian variable variance 
best estimation words series truly random walk best estimate time period equal current estimate 
assumed predictable component series possible zero mean gaussian variable variance 
non linear function arguments 
case best estimate prediction model problematic series contains trend 
example neural network trained section little chance generalizing test data section model trained data range covered section 

example difficulty model trained raw price series 
generalization section training data section difficult model trained inputs range covered section common solution model order differences equation raw time series zero mean gaussian variable variance 
case best estimate system details high level block diagram system shown 
raw time series values denote daily closing bids financial time series 
difference series taken follows produces 
order compress dynamic range series reduce effect outliers log transformation data sign log 
high level block diagram system 
resulting usual delay embedding series considered delay embedding dimension equal experiments reported 
state vector 
delay embedding forms input som 
som input vector values log transformed time series 
output som topographical location winning node 
node represents symbol resulting grammatical inference problem 
brief description self organizing map contained section 
som represented equation number symbols nodes som 
node som assigned integer index ranging number nodes 
elman recurrent neural network trained sequence outputs som 
elman network chosen suitable problem grammatical inference style problem shown perform comparison recurrent architectures see 
elman neural network feedback hidden nodes hidden nodes shown 
elman network az bu vector representing weights hidden layer output nodes number hidden nodes number output nodes constant bias vector elman refers topology network full backpropagation time opposed truncated version elman 
vector denoting outputs hidden layer neurons 
vector follows embedding dimension recurrent neural network matrices appropriate dimensions represent feedback weights hidden nodes hidden nodes weights input layer hidden layer respectively 
vector containing sigmoid functions 
vector denoting bias hidden layer neuron 
vector containing outputs network 

output trained predict probability positive change second output trained predict probability negative change 
complete system considered terms original series note static mapping due dependence hidden state recurrent neural network 
sections describe self organizing map recurrent network grammatical inference dealing non stationarity controlling overfitting priori class probabilities method comparison random walk 
self organizing map self organizing map som unsupervised learning process learns distribution set patterns class information 
pattern projected possibly high dimensional input space position map low dimensional display space display space divided grid intersection grid represented network neuron 
information encoded location activated neuron 
som classification clustering techniques attempts preserve topological ordering classes input space resulting display space words similarity measured metric input space som attempts preserve similarity display space 
pre processed delay embedded series converted symbols self organizing map 
elman neural network trained sequence symbols 
algorithm give brief description som algorithm details see 
som defines mapping input space topologically ordered set nodes usually lower dimensional space vector assigned node som 
assume nodes 
training input compared obtaining location closest match jjx jj min jjg 
input point mapped location som 
nodes som updated ci time learning ci neighborhood function smoothing kernel maximum usually ci jjr jj represent locations nodes som output space node closest weight vector input sample ranges nodes 
ci approaches jjr jj increases approaches 
widely applied neighborhood function ci exp jjr jj scalar valued learning rate defines width kernel 
generally monotonically decreasing time 
neighborhood function means nodes close som structure moved input pattern winning node 
creates smoothing effect leads global ordering map 
note reduced far map lose topographical order neighboring nodes updated closest node 
som considered nonlinear projection probability density input patterns typically smaller output space 
remarks nodes display space encode information contained input space nodes implies input pattern vectors transformed set symbols preserving original topological ordering original input patterns highly noisy quantization set symbols preserving original topological ordering understood form filtering 
amount filtering controlled large implies little reduction noise content resulting symbols 
hand small implies heavy filtering effect resulting small number symbols 
recurrent network prediction past years recurrent neural network architectures emerged grammatical inference 
induction relatively simple grammars addressed learning tomita grammars 
shown particular class recurrent networks turing equivalent 
set symbols output som linearly encoded single input elman network single input 
linear encoding important justified topographical order symbols 
order defined symbols separate input symbol resulting system dimensions increased difficulty due curse dimensionality 
order facilitate training recurrent network input window symbols input neurons recurrent neural network 
dealing non stationarity approach deal non stationarity signal build models short time period 
intuitively appealing approach expect inefficiencies market typically long time 
difficulty approach reduction small number training data points 
size training set controls noise vs non stationarity tradeoff 
training set small noise harder estimate appropriate mapping 
training set large nonstationarity data mean data statistics relevant task hand creating estimator 
ran tests segment data separate main tests chose models trained days data see appendix details 
training model tested days data 
entire training test set window moved forward days process repeated depicted 
real life situation desirable test day advance windows steps day 
days order reduce amount computation factor enable larger number tests performed times predictions number training runs increasing confidence results 
controlling overfitting highly noisy data addressed techniques weight decay weight elimination early stopping control overfitting 
problem early stopping difference stopping point chosen multiple tests separate segment data validation set normal manner 
tests showed validation set normal manner seriously affected performance 
surprising dilemma choosing validation set 
validation set chosen training data terms temporal order series non stationarity series means validation set may little predicting performance test set 
validation set chosen training data problem occurs non stationarity desirable validation set small possible alleviate problem set small 
depiction training test sets 
needs certain size order results statistically valid 
chose control overfitting setting training time appropriate stopping point priori 
simulations separate segment data contained main data see appendix details select training time epochs 
note separate segment data select training set size training time expected ideal solutions best choice parameters may change significantly time due non stationarity series 
priori data probabilities financial data sets possible obtain results simply predicting common change training data noticing change prices positive negative training set predicting positive test set 
predictability due data bias may worthwhile prevent discovery better solution 
shows simplified depiction problem naive solution better mean squared error random solution may hard find better solution may particularly problematic high noise data true nature error surface understood 
results reported models prevented acting bias ensuring number training examples positive negative cases equal 
training set remains unchanged temporal order disturbed training signal appropriate number positive negative examples starting series 
number positive negative examples typically approximately equal simple technique result loss data points 

depiction simplified error surface 
naive solution performs better solution may difficult find 
comparison random walk prediction corresponding random walk model equal current price change 
percentage times model predicts correct change apart change obtained random walk model 
possible models data corresponds random walk compare results obtained real data 
equal performance obtained random walk data compared real data significant predictability 
tests performed randomizing classification test set 
original test set randomized versions created 
randomized versions created repeatedly selecting random positions time swapping classification values input values remain unchanged 
additional benefit technique model showing predictability data bias discussed previous section results randomized test sets show predictability predicting positive change result performance original randomized test sets 
experimental results results previously described system alternative systems comparison 
cases elman networks contained hidden units som dimension 
details individual simulations appendix table shows average error rate number nodes self organizing map som increased 
results shown som embedding dimensions 
results shown original data randomized data comparison random walk 
results averaged exchange rates simulations exchange rate test points simulation total test points 
best performance obtained som size nodes embedding dimension error rate 
test indicates result significantly different null hypothesis randomized test sets indicating result significant 
considered null hypothesis corresponding model completely random predictions corresponding random walk result significantly different null hypothesis 
table shows average results som size individual exchange rates 
test error som size embedding dimension embedding dimension randomized embedding dimension randomized embedding dimension 
error rate number nodes som increased som embedding dimensions 
results represents average individual series simulations series simulations tested points individual classifications 
results randomized series averaged values randomized series simulation total classifications result data available due limited computational resources 
som embedding dimension som size error randomized error som embedding dimension som size error randomized error table 
results shown 
exchange british canadian german japanese swiss rate pound dollar mark yen frank embedding dimension embedding dimension randomized embedding dimension randomized embedding dimension table 
results shown 
bp cd dm jy sf test error exchange rate embedding dimension embedding dimension randomized embedding dimension randomized embedding dimension 
average results exchange rates real test data randomized test data 
som size results som embedding dimensions shown 
order gauge effectiveness symbolic encoding recurrent neural network investigated systems 
system symbolic encoding preprocessed data entered directly recurrent neural network som stage 

system recurrent network replaced standard mlp network 
tables show results systems 
performance systems seen performance null hypothesis random walk performance hybrid system indicating hybrid system result significant improvement 
error randomized error table 
results system symbolic encoding preprocessed data entered directly recurrent neural network som stage 
reject performance equation order calculate confidence measure prediction max max min max maximum output min minimum outputs outputs transformed softmax transformation exp exp som embedding dimension som size error randomized error table 
results system mlp rnn 
original outputs transformed outputs number outputs 
confidence measure gives indication confident network model classification 
fixed value prediction obtained particular algorithm divided sets set threshold set threshold 
reject points set computation classification accuracy 
shows classification performance system percentage correct sign predictions confidence threshold increased examples rejected classification 
shows graph randomized test sets 
seen significant increase performance approximately error obtained considering examples highest confidence 
benefit seen randomized test sets 
noted time test points rejected total number remaining test points 
reject percentage reject performance rnn test 
classification performance confidence threshold increased 
graph average individual results 
automata extraction common complaint neural networks models difficult interpret clear models arrive prediction classification input pattern 
number people considered extraction symbolic knowledge trained neural networks feedforward recurrent neural networks 
recurrent networks ordered triple discrete markov process input extracted form reject percentage reject performance rnn randomized test 
classification performance confidence threshold increased randomized test sets 
graph average individual randomized test sets 
equivalent deterministic finite state automata dfa 
done clustering activation values recurrent state neurons 
automata extracted process recognize regular grammars algorithm automata extraction described 
algorithm observation activations recurrent state neurons trained network tend cluster 
output state neurons divided intervals equal size yielding partitions space outputs state neurons 
starting initial state input symbols considered order 
input symbol causes transition new partition activation space new dfa state created corresponding transition appropriate symbol 
event different activation state vectors belong partition state vector reached partition chosen new initial state subsequent symbols 
extraction computationally infeasible partitions visited clusters corresponding dfa states cover small percentage input space 
sample deterministic finite state automata dfa extracted trained financial prediction networks quantization level seen 
dfas minimized standard minimization techniques 
dfas extracted networks som embedding dimension som size 
expected dfas extracted case relatively simple 
cases som symbols representing positive negative changes series transitions marked solid line correspond positive changes transitions marked dotted line correspond negative changes 
dfas state starting state 
double circled states correspond prediction positive change states double circle correspond prediction negative change 
seen corresponding mean reverting behavior change series negative dfa predicts change positive vice versa 
seen variations exceptions simple rules negative change corresponds positive prediction input changes negative positive prediction alternates successive positive changes 
contrast exhibit behavior related trend algorithms regular grammar tuple fs pg start symbol non terminal terminal symbols respectively represents productions form ab positive change corresponds positive prediction negative change corresponds negative prediction negative change positive change 
shows sample dfa som size complex behavior seen case 

sample deterministic finite state automata dfa extracted trained financial prediction networks quantization level 
traditional learning methods difficulty high noise high non stationarity time series prediction 
intelligent signal processing method uses combination symbolic processing recurrent neural networks deal fundamental difficulties 
recurrent neural network significant reasons temporal relationship series explicitly modeled internal states possible extract rules trained recurrent networks form deterministic finite state automata 
symbolic conversion significant number reasons quantization effectively filters data reduces noise rnn training effective symbolic input facilitates extraction rules trained networks 
considering foreign exchange rate prediction performed comprehensive set simulations covering different foreign exchange rates showed significant predictability exist error rate prediction direction change 
additionally error rate reduced rejecting examples system low confidence prediction 
method facilitates extraction rules explain operation system 
results 
sample dfa extracted network som size som embedding dimension 
som symbols represent positive solid lines negative dotted lines change change close zero gray line 
case rules complex simple rules previous 
problem considered positive show symbolic knowledge meaning humans extracted noisy time series data 
information useful important user technique 
shown particular recurrent network combined symbolic methods generate useful predictions rules predictions non stationary time series 
interesting compare approach neural networks time series analysis machine learning methods generate rules :10.1.1.16.2929:10.1.1.16.2929
interesting incorporate model trading policy 
appendix simulation details experiments reported 
rnn learning rate linearly reduced training period see regarding learning rate schedules initial value 
inputs normalized zero mean unit variance 
nodes included bias input part optimization process 
weights initialized shown haykin 
target outputs tanh output activation function quadratic cost function 
order initialize delays network rnn run training time steps prior start datasets 
number training examples positive negative cases equal training appropriate number positive negative examples starting training set temporal order series disturbed training signal number positive number negative examples starting series 
trained iterations learning rate schedule ti current iteration number ti total number iterations 
neighborhood size altered training floor ti 
data split follows points plus size validation set validation set experiments exchange rates selection training dataset size training time experimentation validation set 
points exchange rates main tests 
data successive simulation offset size test set points shown 
number points main tests exchange rate equal initialize delays training set size test sets size 
selection training dataset size training time number points exchange rate test sets size 
experiments validation set simulation number points size validation set exchange rates 
acknowledgments farmer christian andreas weigend anonymous referees helpful comments 
machine learning 
international journal intelligent systems accounting finance management 
special issue 
proceedings ieee conference computational intelligence financial engineering 
abu mostafa 
learning hints neural networks 
journal complexity 
alexander mozer 
template algorithms connectionist rule extraction 
tesauro touretzky leen editors advances neural information processing systems volume pages 
mit press 
martin anthony norman biggs 
computational learning theory view economic forecasting neural nets 
editor neural networks capital markets 
john wiley sons 
back tsoi 
fir iir synapses new neural network architecture time series modeling 
neural computation 
barron 
universal approximation bounds superpositions sigmoidal function 
ieee transactions information theory 
bellman 
adaptive control processes 
princeton university press princeton nj 
bengio 
neural networks speech sequence recognition 
thomson 
cleeremans servan schreiber mcclelland 
finite state automata simple recurrent networks 
neural computation 
darken moody 
note learning rate schedules stochastic optimization 
lippmann moody touretzky editors advances neural information processing systems volume pages 
morgan kaufmann san mateo ca 
denker schwartz solla howard jackel hopfield 
large automatic learning rule extraction generalization 
complex systems 
elman 
distributed representations simple recurrent networks grammatical structure 
machine learning 
fama 
behaviour stock market prices 
journal business january 
fama 
efficient capital markets review theory empirical 
journal finance may 
gabor lugosi 
strong universal consistency neural network classifiers 
ieee transactions information theory 
friedman 
overview predictive learning function approximation 
cherkassky friedman wechsler editors statistics neural networks theory pattern recognition applications volume nato asi series pages 
springer 
friedman 
computational learning statistical prediction 
tutorial neural information processing systems denver 
zoubin ghahramani michael jordan 
factorial hidden markov models 
machine learning 
lee giles horne lin 
learning class large finite state machines recurrent neural network 
neural networks 
lee giles miller chen chen sun lee 
learning extracting finite state automata second order recurrent neural networks 
neural computation 
lee giles miller chen sun chen lee 
extracting learning unknown grammar recurrent neural networks 
moody hanson lippmann editors advances neural information processing systems pages san mateo ca 
morgan kaufmann publishers 
lee giles sun chen lee chen 
higher order recurrent networks grammatical inference 
touretzky editor advances neural information processing systems pages san mateo ca 
morgan kaufmann publishers 
granger 
forecasting economic time series 
academic press san diego second edition 
hayashi 
neural expert system automated extraction fuzzy rules 
richard lippmann john moody david touretzky editors advances neural information processing systems volume pages 
morgan kaufmann publishers 
haykin 
neural networks comprehensive foundation 
macmillan new york ny 
hopcroft ullman 
automata theory languages computation 
addison wesley publishing reading ma 

statistical mechanics nonlinear nonequilibrium financial markets applications optimized trading 
mathematical computer modelling 
ishikawa 
rule extraction successive regularization 
international conference neural networks pages 
ieee press 
jordan 
neural networks 
tucker editor crc handbook computer science 
crc press boca raton fl 
kehagias petridis 
time series segmentation predictive modular neural networks 
neural computation 
kohonen 
self organizing maps 
springer verlag berlin germany 
kolmogorov 
representation continuous functions variables superpositions continuous functions variable addition 
dokl 

kolmogorov theorem relevant 
neural computation 

kolmogorov theorem 
michael arbib editor handbook brain theory neural networks pages 
mit press cambridge massachusetts 
steve lawrence lee giles fong 
natural language grammatical inference recurrent neural networks 
ieee transactions knowledge data engineering 
jean 
foreign currency dealing brief data set 
weigend gershenfeld editors time series prediction forecasting understanding past 
addison wesley 
andrew lo craig mackinlay 
non random walk wall street 
princeton university press 

efficient market hypothesis 
macmillan london 

random walk wall street 
norton new york ny 
mcmillan mozer smolensky 
rule induction integrated symbolic subsymbolic processing 
john moody steve hanson richard lippmann editors advances neural information processing systems volume pages 
morgan kaufmann publishers 
moody 
economic forecasting challenges neural network solutions 
proceedings international symposium artificial neural networks 
taiwan 
mukherjee osuna girosi 
nonlinear prediction chaotic time series support vector machines 
principe giles morgan wilson editors ieee workshop neural networks signal processing vii page 
ieee press 
giles 
extraction rules discrete time recurrent neural networks 
neural networks 
principe 
kuo 
dynamic modelling chaotic time series neural networks 
tesauro touretzky leen editors advances neural information processing systems pages 
mit press 
editor 
neural networks capital markets 
john wiley sons new york ny 
rogers 
multilayer perceptron approximation optimal bayes estimator 
ieee transactions neural networks 
saad wunsch ii 
comparative study stock trend prediction time delay recurrent probabilistic neural networks 
ieee transactions neural networks november 
setiono liu 
symbolic representation neural networks 
ieee computer 
siegelmann sontag 
computational power neural nets 
journal computer system sciences 
weiss indurkhya 
rule machine learning methods functional prediction 
journal artificial intelligence research 
takens 
detecting strange attractors turbulence 
rand 
young editors dynamical systems turbulence volume lecture notes mathematics pages berlin 
springer verlag 
taylor editor 
modelling financial time series 
wiley sons chichester 
andrews diederich 
truth come light directions challenges extracting knowledge embedded trained artificial neural networks 
ieee transactions neural networks november 
tomita 
dynamic construction finite state automata examples hill climbing 
proceedings fourth annual cognitive science conference pages ann arbor mi 
towell shavlik 
extraction refined rules knowledge neural networks 
machine learning 
geoffrey towell jude shavlik 
extracting refined rules knowledge neural networks 
machine learning 
george matthew 
testing efficient markets hypothesis gradient descent algorithms 
editor neural networks capital markets 
john wiley sons 
watrous kuhn 
induction finite state languages second order recurrent networks 
moody hanson lippmann editors advances neural information processing systems pages san mateo ca 
morgan kaufmann publishers 
watrous kuhn 
induction finite state languages second order recurrent networks 
neural computation 
weigend huberman rumelhart 
predicting sunspots exchange rates connectionist networks 
casdagli eubank editors nonlinear modeling forecasting sfi studies sciences complexity proceedings vol 
xii pages 
addison wesley 
yule 
method investigating periodicities disturbed series special sunspot numbers 
philosophical transactions royal society london series 
zeng goodman smyth 
discrete recurrent neural networks grammatical inference 
ieee transactions neural networks 

