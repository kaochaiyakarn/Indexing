application neural networks biological data mining case study protein sequence classification jason wang ma department computer information science new jersey institute technology newark nj usa cis njit edu dennis shasha courant institute mathematical sciences new york university new york ny usa shasha cs nyu edu cathy wu national biomedical research foundation pir georgetown univ medical ctr 
washington dc usa georgetown edu biological data mining aims extract significant information dna rna proteins 
significant information may refer motifs functional sites clustering classification rules 
presents example biological data mining classification protein sequences neural networks 
propose new techniques extract features protein data combination bayesian neural network classify protein sequences obtained pir protein database maintained national biomedical research foundation 
evaluate performance proposed approach compare protein classifiers built sequence alignment machine learning methods 
experimental results show high precision proposed classifier complementarity tools studied 
keywords bioinformatics biological data mining machine learning neural networks sequence alignment feature extraction protein data 
result human genome project related efforts dna rna protein data accumulate accelerating rate 
mining biological data extract useful knowledge essential genome processing 
subject gained significant attention data mining community 
case study subject application neural networks protein sequence classification 
problem studied stated formally follows 
unlabeled protein sequence known superfamily want determine belongs 
refer target class set sequences non target class 
general superfamily group proteins share similarity structure function 
unlabeled sequence detected belong infer structure function process important aspects computational biology 
example drug discovery sequence obtained disease determined belongs superfamily may try combination existing drugs treat disease feature extraction protein data dimensional point view protein sequence contains characters letter amino acid alphabet fa yg 
important issue applying neural networks protein sequence classification encode protein sequences represent protein sequences input neural networks 
sequences may best representation 
input representations easier neural networks recognize underlying regularities 
input representations crucial success neural network learning 
propose new encoding techniques entail extraction high level features protein sequences 
best high level features relevant 
relevant mean high mutual information features output neural networks mutual information measures average reduction uncertainty output neural networks values features 
way look features capture global similarity local similarity protein sequences 
global similarity refers similarity multiple sequences local similarity refers motifs frequently occurring substrings sequences 
sections elaborate find global local similarity protein sequences 
section presents classification algorithm employs bayesian neural network originated mackay 
section evaluates performance proposed classifier 
section compares approach protein classifiers 
section concludes 

calculate global similarity protein sequences adopt gram encoding method proposed 
gram encoding method extracts counts occurrences patterns consecutive amino acids residues protein sequence 
instance protein sequence gram amino acid encoding method gives result pv indicating pv occurs vk indicating vk occurs twice kt tn nv 
adopt letter exchange group fe represent protein sequence fh kg fd qg fcg fs gg fm vg ff wg 
exchange groups represent conservative replacements evolution 
example protein sequence represented gram exchange group encoding sequence protein sequence apply gram amino acid encoding gram exchange group encoding sequence 
theta theta possible gram patterns total 
gram patterns chosen neural network input features require weight parameters training data 
difficult train neural network phenomenon called curse dimensionality 
different methods proposed solve problem careful feature selection scaling input dimensionality 
propose select relevant features grams employing distance measure calculate relevance feature 
feature value 
denote class conditional density functions feature class represents target class class non target class 
denote distance function 
distance measure rule prefers feature feature easier distinguish class class observing feature feature feature gram pattern 
denote occurrence number feature sequence denote total number gram patterns len represent length len gamma 
define feature value gram pattern respect sequence len gamma 
example suppose 
value feature vk respect 
protein sequence may short random pairings large effect result 
approximated distance gamma respectively mean value standard deviation feature positive negative respectively training dataset 
mean value standard deviation feature set sequences defined gamma gamma value feature respect sequence total number sequences xng ng top ng features gram patterns largest values 
intuitively ng features occur frequently positive training dataset frequently negative training dataset 
protein sequence training test sequence examine ng features calculate feature values ng feature values input feature values bayesian neural network sequence compensate possible loss information due ignoring gram patterns linear correlation coefficient lcc values gram patterns respect protein sequence mean value gram patterns positive training dataset calculated input feature value specifically lcc defined lcc gamma gamma gamma mean value jth gram pattern positive training dataset feature value jth gram pattern respect 
local similarity protein sequences local similarity protein sequences refers frequently occurring motifs protein sequences 
fs positive training dataset 
previously developed sequence mining tool find regular expression motifs forms motif length len approximately matches mut mutations occur sequences mutation mismatch insertion deletion letter residue len mut occur userspecified parameters 
segments sequence substrings consecutive letters variable length don care vldc symbol 
length motif number non vldc letters motif 
matching motif sequence vldc symbol motif instantiated arbitrary number residues cost 
example matching motif sequence instantiated mn second instantiated kwk 
number mutations motif sequence representing cost inserting motif 
number motifs returned enormous 
useful develop measure evaluate significance motifs 
propose minimum description length mdl principle calculate significance motif 
mdl principle states best model motif case minimizes sum length bits description model length bits description data positive training sequences case encoded model 
experimental results show choosing ng yield reasonably performance provided sufficient training sequences 
evaluating significance motifs adopt information theory fundamental form measure significance different motifs 
theory takes account probability amino acid motif sequence calculating description length motif sequence 
specifically shannon showed length bits transmit symbol channel optimal coding gammalog px px probability symbol occurs 
probability distribution px alphabet sigma fb calculate description length string bk bk bk alphabet sigma gamma log px bk 
case alphabet sigma protein alphabet containing amino acids 
probability distribution calculated examining occurrence frequencies amino acids positive training dataset straightforward way describe encode sequences referred scheme encode sequence sequence separated delimiter 
dlen denote description length sequence dlen gamma na log na number occurrences example suppose sequence dlen gamma log log log log log log log log log dlen denote description length fs 
ignore description length delimiter description length dlen dlen method encode sequences referred scheme encode regular expression motif say encode sequences specifically sequence approximately match encode encode scheme 
example illustrate scheme 
consider example 
encode indicates mutation allowed matching delimiter signal motif 
denote alphabet fa amino acids 
denote probability distribution alphabet approximated reciprocal average length motifs 
gamma denotes number motif motif form motif form 
calculate description length motif follows 
dlen denote description length bits motif dlen gamma log log log 
instance consider 
dlen gamma log log log log log log log 
sequences approximately matched motif encoded aid motif 
example consider 
matches mutation representing cost inserting third position vldc symbol instantiated mn second vldc symbol instantiated kwk 
rewrite mn ffl ss ffl kwk ss ffl denotes concatenation strings 
encode oi 
delimiter indicates mutation occurs matching oi indicates mutation insertion adds letter third position general mutation operations involved positions observed approximate string matching algorithms 
description length encoded denoted dlen calculated easily 
suppose sequences sp sp positive training dataset approximately match motif weight significance denoted defined dlen sp gamma dlen dlen sp 
intuitively sequences approximately matching bits encode encode sequences larger weight 
find set regular expression motifs forms positive training dataset motifs satisfy user specified parameter values len mut occur 
choose top motifs largest weight 
denote set motifs 
suppose protein sequence training sequence test sequence approximately match mut mutations motifs motifs mm local similarity ls value denoted ls defined ls max ls 
ls value input feature value bayesian neural network sequence note max operator maximize discrimination 
general positive sequences large ls values high probabilities small ls values low probabilities 
hand negative sequences small ls values high probabilities large ls values low probabilities 

bayesian neural network classifier adopt bayesian neural network bnn originated mackay classify protein sequences 
ng input features including ng gram patterns lcc feature described section ls feature described section 
protein sequence represented vector ng real numbers 
bnn hidden layer containing multiple hidden units 
output layer output unit logistic activation function gammaa 
bnn fully connected adjacent layers 
fx tmg denote training dataset including positive negative training sequences 
input feature vector including ng input feature values tm binary target value output unit 
tm equals represents protein sequence target class 
denote input feature vector protein sequence training sequence test sequence 
architecture weights bnn output value uniquely determined put vector logistic activation function output unit output value interpreted jx probability represents protein sequence target class likelihood function data model calculated djw pi gamma gammat exp gammag djw 
djw cross entropy error function djw gamma tm log gamma tm log gamma 
djw objective function non bayesian neural network training process minimized 
process assumes possible weights equally 
weight decay avoid overfitting training data poor generalization test data adding term ff objective function ff weight decay parameter hyperparameter sum square weights neural network number weights 
objective function minimized penalize neural network weights large magnitudes 
penalizes complex model favors simple model 
precise way specify appropriate value ff tuned offline 
contrast bayesian neural network hyperparameter ff interpreted parameter model optimized online bayesian learning process 
adopt bayesian training neural networks described calculate maximize evidence ff djff 
training process employs iterative procedure iteration involves levels inference 
classifying unlabeled test sequence represented input feature vector output bnn jx probability belongs target class 
probability greater decision boundary assigned target class assigned non target class 

classifier carried series experiments evaluate performance proposed bnn classifier pentium ii pc running linux operating system 
data experiments obtained international protein sequence database release protein information resource pir maintained national biomedical research foundation pir georgetown university medical center 
database accessible www georgetown edu currently sequences 
positive datasets considered globin kinase ras superfamilies respectively pir protein database 
globin superfamily contained protein sequences lengths ranging residues residues 
kinase superfamily contained protein sequences lengths ranging residues residues 
ras superfamily contained protein sequences lengths ranging residues residues 
superfamily contained protein sequences lengths ranging residues residues 
negative dataset contained protein sequences taken pir protein database lengths ranging residues residues negative sequences belong positive superfamilies 
positive negative sequences divided training sequences test sequences size training dataset equaled size test dataset multiplied integer training data tested bnn models different numbers hidden units 
hidden units evidence obtained largest fixed number hidden units 
models hidden units require training time achieving roughly performance 
parameter values experiments follows 
ng number gram patterns bnn number motifs bnn len length motifs mut mutation number occur occurrence frequency motifs size ratio 
measure evaluate performance bnn classifier precision pr defined pr otal theta number test sequences classified correctly otal total number test sequences 
results globin superfamily results superfamilies similar 
results experiment considered gram patterns evaluated effect performance proposed bnn classifier 
performance improves initially ng increases 
reason gram patterns precisely represent protein sequences 
ng large training data insufficient performance degrades 
second experiment considered motifs studied effect performance classifier 
motifs lengths ranging residues residues 
motifs uses better performance achieves 
require time matching test sequence motifs 
experimented parameter values len mut occur 
results didn change parameters changed 
tested effects individual feature combinations 
best performance achieved features case pr bnn classifier 

comparison protein classifiers purpose section compare proposed bnn classifier commonly blast classifier built sequence alignment sam classifier built hidden markov models 
blast version number 
default values parameters blast 
sam version number internal weighting method suggested 
blast aligned unlabeled test sequence positive training sequences target class globin superfamily negative training sequences non target class tool 
score threshold value fixed undetermined 
assigned class containing sequence best matching sam employed program build hmm model positive training sequences 
calculated log odds scores training sequences program 
log odds scores negative real numbers scores positive training sequences generally smaller scores negative training sequences 
largest score sp positive training sequences smallest score sn negative training sequences recorded 
high max fsp sng blow min fsp sng 
calculated log odds scores test sequences program 
score unlabeled test sequence smaller blow classified member target class globin sequence 
score larger high classified member non target class 
score blow high undetermined 
addition basic classifiers bnn blast sam developed ensemble classifiers called combiner employs unweighted voter works follows 
bnn blast sam agree classification results result combiner results classifiers classifiers agree classification results result combiner results classifiers classifiers agrees classification results result combiner undetermined 
comparison blast sam bnn classifier faster yielding undetermined sequence 
combiner achieves highest precision 
classifiers superfamilies globin kinase ras 

bayesian neural network approach classifying protein sequences 
main contributions include development new algorithms extracting global similarity local similarity sequences input features bayesian neural network ii development new measures evaluating significance gram patterns frequently occurring motifs classifying sequences iii experimental studies compare performance proposed bnn classifier classifiers blast sam different superfamilies sequences pir protein database 
main findings include 
studied classifiers bnn sam blast complement combining yields better results classifiers individually 
training phase done learning classifiers bnn sam may take time 
classifiers trained run significantly faster alignment tool blast sequence classification 

acknowledgments supported part nsf iri iri pharmaceuticals 
sigkdd reviewers thoughtful comments 
dr david mackay sharing bayesian neural network software dr richard hughey providing sam software 

altschul madden schaffer zhang zhang miller lipman 
gapped blast psi blast new generation protein database search programs 
nucleic acids research 
barker hunt yeh pfeiffer 
pir international protein sequence database 
nucleic acids research 
ukkonen 
discovering patterns subfamilies 
proceedings fourth international conference intelligent systems molecular biology pages 
hughey 
weighting hidden markov models maximum discrimination 
bioinformatics 
mackay 
evidence framework applied classification networks 
neural computation 

novel method protein sequence classification frequency analysis application search functional sites domain localization 
computer applications biosciences 
wang chirn marr shapiro shasha zhang 
combinatorial pattern discovery scientific data preliminary results 
proceedings acm sigmod international conference management data pages 
wang shapiro shasha eds 
pattern discovery biomolecular data tools techniques applications 
oxford university press new york 
wu 
neural networks genome informatics 
elsevier science 
