actor critic algorithms vijay john tsitsiklis february 
propose analyze class actor critic algorithms 
time scale algorithms critic uses temporal di erence td learning linearly parameterized approximation architecture actor updated approximate gradient direction information provided critic 
show features critic ideally span subspace prescribed choice parameterization actor 
study algorithms markov decision processes general state action spaces 
state prove results regarding convergence 


problems nance communication networks operations research formulated dynamic programming dp problems 
formulations su er fact dimension state space large problem tractable 
underlying dynamics seldom known usually di cult identify 
reinforcement learning rl neuro dynamic programming ndp methods try overcome di culties combining simulation learning compact representations policies value functions :10.1.1.32.7692
vast majority methods fall categories 
actor methods parameterized family policies 
gradient performance respect actor parameters directly estimated simulation parameters updated direction improvement :10.1.1.129.8871
possible drawback methods gradient estimators may large variance 
furthermore policy changes new gradient estimated independently past estimates 
learning sense accumulation consolidation older information 

critic methods rely exclusively value function approximation aim learning approximate solution bellman equation hopefully prescribe near optimal policy 
methods indirect sense try optimize directly policy space 
method type may succeed constructing approximation value function lack reliable guarantees terms near optimality resulting policy 
actor critic methods aim combining strong points actor methods 
critic uses approximation architecture simulation learn value function update actor policy parameters direction performance improvement 
methods long may desirable convergence properties contrast critic methods convergence guaranteed limited settings 
hold promise delivering faster convergence due variance reduction compared actor methods 
hand theoretical understanding actor critic research partially supported nsf contract ecs afosr contract 
preliminary version nips appeared conference proceedings 
laboratory information decision systems massachusetts institute technology cambridge ma usa 
email mit edu mit edu 
methods limited case lookup table representations policies value functions 
propose actor critic algorithms critic uses linearly parameterized approximations value function provide convergence proof 
algorithms important observation number parameters actor update relatively small compared number states critic need attempt compute approximate exact value function high dimensional object 
fact show critic ideally compute certain projection value function low dimensional subspace spanned set basis functions completely determined parameterization actor 
key insight derived simultaneous independent included discussion certain actor critic algorithms :10.1.1.146.4070
outline follows 
section state formula gradient average cost markov decision process nite state action space 
provide new interpretation formula section derive algorithms 
section consider markov decision processes gradient average cost greater generality describe algorithms general setting 
sections provide analysis asymptotic behavior critic actor respectively 
appendix contains general result concerning tracking ability linear stochastic iterations section 
markov decision processes parameterized families randomized stationary policies 
consider markov decision process nite state space nite action space stage cost function 
xy denote probability state current state current action randomized stationary policy rsp mapping assigns state probability distribution action space consider set randomized stationary policies parameterized terms vector 
pair denotes probability action state encountered policy corresponding 
distinguish parameter rsp rsp 
say rsp refer rsp corresponding parameter vector 
note rsp sequence states fx sequence state action pairs fx markov decision process form markov chains state spaces respectively 
assumption family policies 
assumption 
finite case 

map 
di erentiable bounded derivative 
furthermore valued function 
ln bounded bounded rst derivatives xed 
function 
bounded derivative 

markov chains fx fx irreducible aperiodic stationary probabilities stand gradient respect vector 
respectively rsp 
positive integer state 
xx denotes transition matrix markov chain fx control policy 
notation xx denote entry matrix consider average cost function interested nding rsp minimizes 
natural way nd rsp start policy improve gradient descent 
rely formula shortly 
di erential cost function de ned solution poisson equation xy intuitively viewed disadvantage state expected excess cost top average cost incurred start state plays role similar played familiar value function arises total discounted cost markov decision problems 
de ne function xy recall result stated 
di erent versions result established 
theorem 
quantity formula interpreted expected excess cost incurred certain renewal period markov chain fxn ung rsp estimated means simulation leading algorithms 
provide alternative interpretation formula theorem inner product arrive di erent set algorithms 
de ne inner product 
real valued functions viewed vectors hq notation matrix valued functions 
notation rewrite formula hq stands ith component 
denote norm induced inner product denote span vectors ng 
set functions form scalars note gradient depends function vector possibly high dimensional space dependence inner products vectors learning function su ces learn projection low dimensional subspace 
projection operator de ned arg min kq qk hq know projection compute 

actor critic algorithms 
view actor critic algorithms stochastic gradient algorithms parameter space actor 
actor parameter vector job critic compute approximation projection actor update policy approximate gradient direction 
analysis shows precisely temporal di erence td learning algorithms try compute projection exact value function subspace spanned feature vectors 
allows implement critic td algorithm 
note types critics possible batch solution squares problems long aim computing projection 
note minor di erences common usage td 
context need projection functions value functions 
easily achieved replacing markov chain fx markov chain fx di erence assume control policy feature vectors xed 
algorithms control policy features need change actor updates parameters 
suggested results need pose problems long actor parameters updated slower time scale 
ready describe actor critic algorithms di er far critic updates concerned 
variants critic td algorithm linearly parameterized approximation architecture function form denotes parameter vector critic 
features critic dependent actor parameter vector chosen assumptions satis ed assumption 
critic features 
set feature vectors ug bounded 

exists kj 
span vectors denoted contains note formula holds rede ned projection long contains straightforward choice number critic parameters equal number actor parameters allow possibility properly contains critic uses features necessary 
added exibility may turn useful number ways 
possible certain values feature vectors close zero linearly dependent 
values operator ill conditioned negative ect performance algorithms 
avoided richer set features 
second algorithm propose td critic compute approximate exact projection 
additional features result reduction approximation error 
avoid rst possibility choose features critic assumption satis ed 
assumption 
exists jjr jj parameter vector critic stores auxiliary parameters scalar estimate average cost vector represents sutton eligibility trace :10.1.1.32.7692
actor critic updates take place course simulation single sample path controlled markov chain 
parameters critic parameter vector actor time state action pair time 
new state obtained action applied 
new action generated rsp corresponding actor parameter vector critic carries update similar average cost temporal di erence method positive step size parameter :10.1.1.146.4070
variants critic di erent ways updating td critic 
state appearing assumption 
td critic 
actor 
actor updates parameter vector follows 
scalar controls step size actor account current estimate critic 
note denote simulated processes algorithm 
hats denote simulated processes update parameters algorithm denote processes randomized stationary policy xed 
understand actor update recall formulas 
formulas projection subspace contains known current value estimate steady state expected value equal 
known natural place critic current estimate rk 
important critic estimate accurate asymptotically order scheme converges 
prove section critic estimate asymptotically accurate assumption step sizes 
assumption 
step sizes deterministic non increasing satisfy 
function 
assumed satisfy inequalities positive constants jrj jrj prove theorem convergence actor greater generality 
theorem 
assumptions 
actor critic algorithm td critic lim inf jr 
exists su ciently close algorithm td critic lim inf jr 
algorithms possible variations 
instance consider episodic problems starts initial state runs process random termination time time process reinitialized objective minimizing expected cost termination 
setting average cost estimate unnecessary removed critic update formula 
critic parameter reinitialized time entered obtain method closely related williams reinforce algorithm 
method involve value function learning observations episode ect critic parameter episode 
contrast approach observations past episodes ect current critic parameter sense critic learning 
advantageous long slowly changing observations episodes carry useful information function current policy 
discuss algorithms general spaces section close section examples parameterized families rsp 
provide example mdp general state action spaces verify assumptions case 
example 
dynamic programming problems guess engineering insight prove rigorously solution bellman equation satis es certain structural properties solution closely approximated quadratic cases usually starts parameterized family functions fq required structural properties tries nd critic algorithm best family true function 
hope performance greedy policy suggested best close optimal 
knowledge structure solution bellman equation di erent way context actor critic algorithms 
rst approximate set greedy policies suggested family fq set rsp exp exp note approximation greedy policies rsp depends positive temperature parameter temperature goes zero policy deterministic approaches greedy policy suggested function actor critic algorithms nd optimal rsp family 
example 
concrete example family rsp class threshold policies multiple service multiple resource systems 
systems state space subset set non negative integers 
th component state vector represents number customers system type need resources ip th component ij denotes amount resources type resource constrained system state space fx ax rg vector denotes available resources 
problem decide admit reject arriving customer 
note model decision problem mdp state consists vector type customer requesting service 
natural class control policies threshold policies parameterized matrix threshold vector customer type arrives resources available bx calculated current state 
th component bx compared th component customer admitted lesser 
class policies approximated class rsp 
denote decision admit decision reject threshold rsp described tanh temperature parameter 

algorithms general state control spaces 
section consider actor critic algorithms markov decision processes general state action spaces 
algorithms case nite state spaces repeated section 
restate assumptions general setting notation theory quite technical 
allow uncountable spaces appropriate terms controlled markov chain control space markov decision processes action space respectively 
abbreviation 
phrase probability 
denote norms real euclidean spaces 
norms hilbert spaces jj 
jj probability measure integrable function denote expectation respect 

preliminaries 
state space control space assumed polish spaces countably generated elds respectively 
non technical audience note practical spaces countable state spaces polish common intuition regarding probabilities holds spaces 
describe evolution controlled markov chain cmc general state control spaces need describe probability distribution state ected current state control 
done function 
probability measure map 
measurable 
function 
called transition kernel 
similarly describe randomized stationary policy need describe current state ects probability distribution generate control 
measurable map 


probability measure call map randomized stationary policy 
note cmc fx controlled xed rsp markov chain underlying probability space constructed follows 
canonical sample space outcome sequence state action pairs 


rsp unique probability measure satisfying sjx ajx denote expectation countable state control spaces parameterized family rsp represented parameterized family probability mass functions 
similarly cmc general state space need represent family parameterized rsp parameterized family probability density functions 
measure control space family positive measurable functions 
probability density function respect du parameterized family densities viewed parameterized family rsp du corresponding rsp 
denote probability law markov chain fx policy started state denote corresponding expectation 
probability measure dx denote law markov chain fx starting state distribution 
average cost de ned policy family impose assumptions parameterized family rsp assumption 
irreducibility process fx controlled rsp irreducible aperiodic 
notion irreducibility known discrete state spaces 
processes general state space usual notion irreducibility applicable 
generally markov chain irreducible relative notion mass size formalized measure 
formally markov chain fx said irreducible holds words markov chain fx irreducible sets positive mass reachable positive probability starting state 
note discrete state spaces assume counting measure irreducibility equivalent usual notion irreducibility countable state space markov chains 
tweedie show de nition irreducibility essentially independent measure relative de ned 
fx irreducible measure exists maximal irreducibility measure markov chain fx irreducible absolutely continuous respect markov chain fx irreducible aperiodic policy follows proposition exists set states positive integer constant probability measure xn assume condition holds uniformly 
assumption 
uniform geometric ergodicity 
exists positive integer set constant probability measure xn 
exists function constants bi 
indicator function set call function satisfying inequality stochastic lyapunov function 
uniform geometric ergodicity assumption restrictive assumptions 
nite case note assumption implies rst part assumption fx see implies second part consider rst hitting time state sequence values actor parameter consider time varying markov chain obtained policy time consider function sup assumption guarantees 
nite su ciently close 
matter simple algebraic calculations see satis es 
geometric ergodicity results shown assumption satis ed markov chains fx fx steady state distributions dx dx du dx du respectively 
steady state reached geometric rate 
words exists real valued measurable function satis es jf je cv nite spaces introduced certain boundedness assumptions maps 

derivatives 
general bounds may depend state action pair 
assume bounds form kq constant function satis es 

du see 
assumption ensures steady state distribution markov chain policy smooth policy parameter 
nite case validity assumption automatic consequence assumption 
assumption 
di erentiability 

map continuously di erentiable 

exists sup kq 
lim sup assumptions wish prove gradient formula similar valid ln immediate consequence assumption proofs outline proof theorem imitated show measurable jf map :10.1.1.146.4070
bounded bounded derivatives 
need properties average cost function 
assume stage cost function assumption 
exists jc kq 
jj 
jj denote inner product norm respectively denote function assigns value state action pairs 
assumption implies average cost function written dx du hc de ne operator dy say solution poisson equation parameter satis es proposition easily show solution poisson equation parameter exists unique constant 
solutions collinear 
obvious family solutions poisson equations convergence series consequence 
regenerative representations solutions poisson equation useful purposes analysis derivation algorithms 
theorem holds solution poisson equation parameter 
theorem 
assumptions proof 
fix assume family functions solution poisson equation parameter 
assume map 
di erentiable state action pair family functions 

bounded expectation respect du 
di erentiate sides equation respect obtain interchange di erentiation integration justi ed uniform integrability 
inner product sides equation facts fi fi obtain hq second equality follows fact necessarily collinear easily veri ed fact 
complete proof need show existence family functions shown imitating proofs glynn outline :10.1.1.146.4070
assumptions construct slightly enlarged probability space regeneration time sampled markov chain fx kn controlled policy splitting technique athreya ney 
regeneration time obtain representation average cost function 
solutions poisson equations 
ju furthermore positivity implies measures equivalent 
equations rewritten ju likelihood ratio dp dp jf conditional expectation radon nikodym derivative respect assumptions imply map 
di erentiable family functions jl uniformly integrable 
implies average cost function 
solutions poisson equation di erentiable di erentiation respect 
interchanged 
go algorithms general state control spaces illustrate assumptions veri ed context inventory control problem 
example 
consider facility amount stock th period negative stock representing unsatis ed backlogged demand 
denote random demand th period 
problem decide amount stock ordered th period current stock previous demands 
represents amount stock ordered th period cost incurred assumed max max pu price material unit cost incurred unit backlogged demand holding cost unit stock inventory 
evolution stock assume demands nonnegative nite mean known see optimal policy form max depending distribution approximation policies having form family randomized policies chosen random density sech 
constant picked prior knowledge upper bound parameter optimal policy 
de ne family density functions family policies du sum dirac measure lebesgue measure 
density functions tanh sech dynamics stock inventory controlled policy described max fs density independent demands stock easy see markov chain fx irreducible lebesgue measure prove markov chain aperiodic su ces show holds 
borel set consider max inf dy dt dy probability distribution rest follows observation integral strictly positive 
prove lyapunov condition assume exponentially decreasing tails 
words assume exists exp rst argue intuitively function exp jxj min candidate lyapunov function 
see note lyapunov inequality says lyapunov function decrease common factor outside set try set su ciently larger inventory starts stock larger stock ordered high probability stock decreases decreasing lyapunov function factor exp inventory starts large backlogged demand new stock ordered satisfy backlogged demand decreasing lyapunov function precise follows exp max exp exp exp exp exp js note third term bounded uniformly min 
rst term bounded negative second term bounded positive 
lyapunov function decreases factor exp decreases factor exp su ciently large 
rest assumptions easy verify jxj 
critic 
nite case feature vectors assumed bounded 
assumption seldom satis ed general state spaces 
reasonable impose bounds growth feature vectors assumption 
assumption 
exists kq 
exists kj jq 
exists jjr jj nal assumption relates features critic assumption 
subspace spanned features critic contains subspace spanned functions 
td critic 
td critic strengthen assumption 
assumption 
set consists single state note requirement single state hit positive probability quite strong satis ed practical situations involving queueing systems systems regenerative splitting techniques 
assumption expected value features zero restrictive feature vectors new feature vectors du satisfy assumption 
particular put assumption satis ed du du du 
actor 
assume vector assumption 
exists map 
di erentiable jr kq control time generated randomized policy corresponding parameter similarly fact state obtained simulation cmc transition kernel 
expressed formally sj 
convergence critic 
section analyze convergence critic algorithms described 
held constant value follow results critic parameters converge 
case changes slowly allow show converges zero 
establish fact cast update critic linear stochastic approximation driven markov noise speci cally form equation appendix show critic update satis es hypotheses theorem appendix desired result theorem follow 
start notation 
time deterministic constant purpose clear 
eld generated de ne lc zc understand role scale factor consider steady state expectation matrix 
required matrix positive de nite certain linear equations unique solutions 
need positive de nite block diagonal matrices diagonal shown positive de nite 
role scale factor reduce non diagonal terms matrix positive de nite 
value chosen su ciently large depending parameter dependence stated proofs lemmas 
jf matrix valued martingale di erence 
update critic written apply theorem update equation need prove satis es assumptions 
verify assumptions cases separately 
common lemmas proved rest subsections 
loss generality assume lemma 
sup 
proof 
assumption note xk bp result follows 
immediate consequence lemma equation assumption assumption moments bounded uniformly holder inequality easy verify assumption identifying assumption update term actor iteration 
assumption follows assumption stepsize sequence concentrate remaining assumptions sections 

td critic 
de ne process terms process fx section policy xed follows ifx gz indicator function 
note process fz depends parameter 
process inside expectation probability measure assume parameter process parameter probability expectation 
easy see markov chain bj borel set update satis es assumption 
stopping time de ned minfk jx de ne ju fact follows assumption assumption uniform ergodicity assumption 
de ne hq ht show steady state averages value actor parameter xed 
prove 

bounded assumption need lemma 
lemma 
exists 
jq du 
jt proof 
su cient prove results jensen inequality ifx assumption eq 
du jc du follows theorem ju jq jc je ju jc ju integrating sides required result 
proof second part similar 
lemma 


bounded 
proof 
assumptions lemma follows jj jj jj jj jj jjq jj bounded inner products bounded 
lemma 

hq 
proof 
similarly equality follows assumption equation 
lemma suggests regenerative representations 
de ne previous lemma little algebraic exercise verify 

satisfy assumption 
prove functions satisfy assumption need result 
lemma 
sup proof 
denote vector step size sequences deterministic forms time varying markov chain 
denote conditional law process yn de ne sequence stopping times process yn follows minfn xn de ne veri ed nite due uniform geometric ergodicity bounds 
easy see su cient prove sup show acts lyapunov function algorithm 
te te te ifx algebraic manipulations bounds 
verify ifx bounded 
constant rest follows 
result easy verify assumption 
verify assumption note 



ne expressed zf functions jf du holder inequality previous results see assumption satis ed 
proof theorem likelihood ratio methods verify assumptions 
lemma veri es assumption 
lemma 
exist jrj proof 
assumption rst inequality jjr jj min jrj jrj min jrj choose sup min possible bounded cf 
lemma 

td critic 
analyze td critic rede ne process easy see de nition equation holds td critic 
means assumption satis ed 
hp hp assumption 
written hp easy see cauchy schwartz inequality boundedness jj jj see proof lemma 

bounded assumption satis ed 
lemma 
exists 
je ck max 
je ck max proof 
prove part proof part similar 
je hp max second inequality follows assumptions 
previous lemma clear de ned easy check satisfy assumption 
veri cation assumptions tedious outlined 
trick write 

form ju show map ju lipschitz continuous lipschitz constant polynomial forgetting factor dominates polynomial sum lipschitz continuous 
verify assumption counterpart lemma 
lemma 
sup proof 
inequality follows jensen inequality 
rest follows fact sup verify assumption lemma 
lemma 
exists jrj proof 
recall de nition vectors due eq 
constant jjp jj jr 
jjp jj jrj hp jjr jj hp jjr jj jjr jj jjr jj jrj jjr jj jjr jj jj jj jrj jrj take rest similar proof lemma 
having veri ed hypotheses theorem conclude theorem 
td critic lim 
convergence actor 

recursion actor parameter written bounded 
satis es condition easy see satis es 
taylor series expansion see 


jh re ects bound lipschitz constant 
note quantities depend parameter critic 
lemma shows quantities depend lemma 
td critic 

exists sup constant 
proof 
de nition solution linear equation ri easy see projection part follows theorem equation assumption 
prove second part write de ned section show explicitly dependence 
easy see hp inequality follows geometric ergodicity eq 

similarly see hq solutions linear equations ri hq respectively 
implies rest follows observation lemma 
convergence noise terms 

converges 

lim 
jh 
proof 
proof part similar proof lemma 
proof parts follows theorem fact 
bounded 
proof part similar proof lemma jh jh theorem 
convergence actor critic algorithms 
td exists su ciently close lim inf jr 

td critic lim inf jr 
proof 
proof standard outline 
de ne sequence random variables min eq 
jr jr de ned 
jh lemma implies goes zero 
result follows easily 
appendix linear stochastic approximations driven slowly varying markov chains 
appendix state prove general theorem regarding tracking ability linear stochastic iterations driven slowly varying markov chain 
consider stochastic process fy values polish space 
fp parameterized family transition kernels consider iteration update vector iteration fh 

parameterized family vector valued matrix valued measurable functions measurable function denote measurable function 
eld generated assumptions 
assumption 
measurable set ajy 
step size sequence deterministic non increasing satis es 
random sequence parameters satis es nonnegative process fh bounded moments deterministic sequence 
matrix valued martingale di erence bounded second moments jf sup 
existence solutions poisson equation exist satisfy poisson equation 

boundedness constant max 
boundedness expectation exists sup jf 
represents functions 




lipschitz continuity constant max cj 
lipschitz continuity expectation exists measurable function 
max jf jp 
functions 



sup 
exists wish prove theorem theorem 
assumptions satis ed lim theorem special case theorem page 
changing albeit slowly need di erent techniques prove result 
proofs combination techniques 
subsection overview proof intuition 

overview proof 
techniques prove convergence stochastic approximations broadly classi ed categories martingale methods probabilistic ode methods deterministic 
martingale methods constructs super martingale super martingale uses martingale convergence theorems infer convergence super martingale turn implies convergence stochastic approximations 
ode methods views stochastic approximation update random perturbation deterministic iteration proves perturbation noise asymptotically negligible 
implies asymptotic behavior deterministic iteration asymptotic behavior stochastic approximations 
uses martingale convergence theorems prove perturbation noise asymptotically negligible rest proof essentially deterministic 
second ode method proofs 
particular note assumption implies markov chain transition kernel vector matrix steady state expected values respectively 
expect iteration behaves steady state changing slowly step size expect goes zero 
formalize intuition subsections 
major part proof involves proof boundedness iterates separately subsection prove claimed convergence subsequent subsection 
techniques essentially specialization general techniques developed 
facts useful proving boundedness convergence 
lemma 
fa non negative sequence satisfying non negative sequence 
sup sup 
proof 
follows sup lemma 
matrix gr jrj su ciently small rj jrj proof 
rj jrj jrj jrj jrj su ciently small result follows inequality 
proof boundedness 
de ne sequence nonnegative integers min de ne sequence max jr note adapted satis es iteration max jr max jr viewed perturbation noise 
wish show noise ne negligible 
di cult establish bounded 
circumvent problem positive constant de ne stopping time minfk cg suppress notation value important 
note arbitrarily large bounded cf 
assumption 
integers denote minimum lemma gives bounds ect perturbation noise stopping time lemma drop superscript convenience 
denote max jr lemma 
exists constant max proof 
gj consider ah 
satis es poisson equation consider note optional sampling theorem martingale inequality assumption see max pk pk ifl gj suitable constants 
assumption 
schwartz inequality easy see similarly second moment contribution pk pk pk inequalities follow fact non increasing schwartz inequality assumption 
max pk max lemma says long bounded perturbation noise remains negligible 
lemma prove sequence approximates simpler deterministic iteration 
de ne sequence fr max jr de ne stopping time minfk 

bounded lemma easy see sup max jr constant 
time gets ball origin radius deviated completely inside ball radius fix constants lemma drop constants notation convenience 
lemma 
lim max 
proof 
note 
bounded discrete inequality easy see max ct max lemma chebyshev inequality implies max cf 
lemma 
expression probability left hand side exactly max rest follows summability series borel cantelli lemma 
lemma 
sup jr 
proof 

bounded assumption lemma imply su ciently large jr jr max jr inequality jr jr max jr previous lemma implies jr max jr jr max jr max jr 
multiplying sides max jr fact jr jr jr ct follows lemma sup jr 
rest follows observation sup jr sup max jr max sup jr max jr max jr boundedness fr previous lemma 

proof theorem 
prove theorem consider sequence 
sequence evolves iteration lemma 
converges 
proof 
part proof similar proof lemma part outlined 
de ne sequence stopping times minfk jr jg stopped process decomposed lemma components say 
components martingales bounded second moments converge 
calculating expectation remaining components easily see absolutely convergent 
stopped process converges 
implies converges set outcomes 
boundedness fr implies result follows 
lemma 
lim 
proof 
assumptions imply jr fh bounded moments 
converges zero 
rest follows boundedness fr recall notation previous section 
de ne lemma 
lim max 
proof 
discrete inequality easy see max ct max ct sup ct sup rest follows previous lemmas 
theorem 
lim 
proof 
lemma assumption lemma implies 
rest follows lemma arguments similar closing arguments proof lemma 
athreya ney 
new approach limit theory recurrent markov chains 
trans 
amer 
math 
soc 
barto sutton anderson 
neuron elements solve di cult learning control problems 
ieee transactions systems man cybernetics 
benveniste 
adaptive algorithms stochastic approximations 
springer verlag berlin heidelberg 
bertsekas 
dynamic programming optimal control 
athena scienti belmont ma 
bertsekas tsitsiklis 
neuro dynamic programming 
athena scienti belmont ma 
borkar 
stochastic approximation time scales 
systems control letters 
borkar 
method convergence stochastic approximation reinforcement learning 
siam journal control optimization 
cao chen :10.1.1.146.4070
perturbation realization potentials sensitivity analysis markov processes 
ieee transactions automatic control 
glynn :10.1.1.32.7692
stochastic approximation monte carlo optimization 
proceedings winter simulation conference pages 
glynn :10.1.1.146.4070
likelihood ratio gradient estimation stochastic recursions 
advances applied probability 
singh jordan :10.1.1.27.9242
reinforcement learning algorithms partially observable markov decision problems 
advances neural information processing systems volume pages san francisco ca 
morgan kaufman 
jordan varaiya 
control multiple service multiple communication networks 
ieee transactions automatic control 
borkar 
actor critic learning algorithms markov decision processes 
siam journal control optimization 
tsitsiklis 
simulation optimization markov reward processes 
ieee transactions automatic control press 
tweedie 
markov chains stochastic stability 
springer verlag 

splitting technique harris recurrent chains 
verw 
geb 
sutton barto 
reinforcement learning 
mit press cambridge ma 
sutton mcallester singh mansour :10.1.1.146.4070
policy gradient methods reinforcement learning function approximation 
advances neural information processing systems volume pages 
tsitsiklis van roy 
analysis temporal di erence learning function approximation 
ieee transactions automatic control 
tsitsiklis van roy :10.1.1.146.4070
average cost temporal di erence learning 
automatica 
williams 
simple statistical gradient algorithms connectionist reinforcement learning 
machine learning 

