ma acta mathematics computing management engineering series data exploration self organizing maps samuel kaski helsinki university technology neural networks research centre fin espoo finland thesis degree doctor technology due permission public examination criticism helsinki university technology st march clock noon 
helsinki university technology department computer science engineering laboratory computer information science espoo kaski data exploration self organizing maps 
acta mathematics computing management engineering series espoo pp 
published finnish academy technology 
isbn 
issn 
keywords data mining exploratory data analysis multivariate analysis neural networks self organizing map som finding structures vast multidimensional data sets measurement data statistics textual documents dioecult time consuming 
interesting novel relations data items may hidden data 
selforganizing map som algorithm kohonen aid exploration structures data sets illustrated special map displays 
methodology soms exploratory data analysis data mining reviewed developed 
properties maps compared properties related methods intended visualizing highdimensional multivariate data sets 
set case studies som algorithm applied analyzing illustrating structures standard living world organizing full text document collections 
measures proposed evaluating quality dioeerent types maps representing data set measuring robustness illustrations maps produce 
measures may comparing knowledge dioeerent maps represent 
feature extraction general tailored application done case studies 
exists algorithm called self organizing map developed kohonen may help 
extracts invariant features automatically data set 
algorithm characterized terms objective function demonstrated able identify input patterns subject dioeerent transformations 
aid feature exploration kernels algorithm creates achieve invariance illustrated map displays similar illustrating data sets 
fl rights reserved 
part publication may reproduced stored retrieval system transmitted form means electronic mechanical photocopying recording prior written permission author 
preface carried neural networks research centre helsinki university technology 
completing thesis participated projects centre supervision academy professor teuvo kohonen 
wish guidance providing facilities nancial support necessary studies 
important numerous discussions fundamental eoeect mention contributions professor kohonen projects authoring majority publications 
wish workers projects included thesis timo honkela krista lagus rest personnel neural networks research centre laboratory computer information science 
special comments manuscript due timo honkela krista lagus sinkkonen professors teuvo kohonen erkki oja 
numerous people worked lived projects hopefully lifelong deserve 
acknowledgments relate completely dioeerent stories 
nancial support academy finland antti foundation gratefully acknowledged 
espoo february samuel kaski contents preface list publications author contribution list symbols abbreviations methods exploratory data analysis visualization high dimensional data items 
clustering methods 
projection methods 
linear projection methods 
nonlinear projection methods 
self organizing maps 
self organizing map algorithm 
properties useful exploring data 
mathematical characterizations 
variants 
notes statistical accuracy 
relations dioeerences som mds 
stages som exploratory data analysis preprocessing 
computation maps 
choosing maps 
interpretation evaluation maps 
case studies multichannel eeg signal 
statistical tables 
full text document collections 
developments 
developments feature exploration adaptive subspace som 
comparison knowledge areas 
appendix key country names list publications thesis consists publications 
kaski larsen 
self organizing map recognition topographic patterns eeg spectra 
ieee transactions biomedical engineering 

kaski kohonen 
exploratory data analysis selforganizing map structures welfare poverty world 

abu mostafa moody weigend editors neural networks financial engineering 
proceedings third international conference neural networks capital markets pages 
world singapore 

kaski honkela lagus kohonen 
creating order digital libraries self organizing maps 
proceedings world congress neural networks pages 
lawrence erlbaum inns press mahwah nj 

kohonen kaski lagus honkela 
large level som browsing newsgroups 
von der malsburg von seelen editors proceedings icann international conference articial neural networks lecture notes computer science vol 
pages 
springer berlin 

lagus honkela kaski kohonen 
self organizing maps document collections new approach interactive exploration 
simoudis han fayyad editors proceedings second international conference knowledge discovery data mining pages 
aaai press menlo park ca 

kaski 
computationally eoecient approximation probabilistic model document representation websom full text analysis method 
accepted neural processing letters 

kaski lagus 
comparing self organizing maps 
von der malsburg von seelen editors proceedings icann international conference articial neural networks lecture notes computer science vol 
pages 
springer berlin 

kohonen kaski self organized formation various invariant feature lters adaptive subspace som 
accepted neural computation 
author contribution publication deals monitoring exploration eeg waveforms 
authors primarily responsible medical aspects implications study 
methodology related preprocessing computation maps visualization results mainly responsibility author thesis 
publication idea computing taxonomies countries som socioeconomic data valuable guidance contributions writing article credited professor teuvo kohonen 
author thesis designed conducted practical part study 
studies websom full text document analysis method described publications 
basic system publication subsequently developed vast maps publication exploration interface suggested ways publication 
original idea stage som architecture organizing document collections due timo honkela methods producing huge soms eoeciently publication originated professor kohonen 
ideas details implementation experiments developed jointly team possible give full account dioeerences contributions team members 
ideas predominantly due author thesis eoecient encoding documents table lookups subsequent convolution analyzed detail publication entropy weighting words computing word category histograms described publication 
new procedures comparing self organized maps introduced publication 
realization potential importance comparing maps judging relative goodness due ms krista lagus 
professor kohonen credited valuable discussions related comparing soms 
idea goodness measure design measures experiments predominantly due author thesis 
publication assom architecture conceived professor kohonen kohonen kohonen kohonen kohonen described set experiments demonstrate capabilities reported detail 
theoretical aspects assom minor details notwithstanding due professor kohonen 
contributions author thesis joint development mathematical analysis part publication professor kohonen joint conduction experiments 
list symbols abbreviations assom adaptive subspace self organizing map dss decision support system eeg em expectation maximization gnp gross national product gtm generative topographic mapping ir information retrieval kdd knowledge discovery databases mds multidimensional scaling pca principal component analysis rbf radial basis function som self organizing map input vector data item kth input vector discrete time index number input vectors input space dimensional euclidean space ith cluster centroid ith model vector index centroid model vector closest number cluster centroids vectors projection distance distance em cost function metric mds method en cost function nonmetric mds method monotonically increasing function nonmetric mds cost function sammon mapping inherent dimensionality data ij neighborhood kernel som algorithm location ith map unit map grid probability density function voronoi region corresponding viz 
set consisting number data items centroid computational complexity order em modied cost function metric mds method decreasing weighting function relatively easy give answers specied questions statistical nature understood data set large aeroplane cockpit accommodate potential pilots example data may assumed normally distributed straightforward estimate threshold 
data available accurate answer 
hand data understood problem specied increase amount data may opposite eoeect 
holds multivariate data particular 
goal simply try sense data set generate sensible hypotheses nd interesting novel patterns paradoxically data available dioecult understand data set 
structures hidden large amounts multivariate data 
exploring data set new insights methods discover illustrate structures data help 
methods applied large data sets topic 
data driven search statistical insights models traditionally called exploratory data analysis jain dubes tukey statistical literature 
process making statistical inferences consists exploratory data driven phase followed phase reproducibility results investigated 
exists wealth applications data sets need summarized gain insight goal data set form easily understandable time preserves essential information data possible 
exploratory data analysis methods tools knowledge discovery databases kdd fayyad fayyad fayyad simoudis 
relatively established eld emphasis interactive process knowledge discovery discovery novel patterns structures data 
process consists multitude steps starting setting goals evaluating results possibly reformulating goals results 
data mining step discovery process step suitable tools disciplines including exploratory data analysis nd interesting patterns data 
depending goals data mining process essentially kinds pattern recognition devijver kittler fu fukunaga machine learning forsyth langley michalski multivariate analysis cooley hair jr kendall authors terminology data mining refer process synonym kdd 
algorithms may useful examples cf 
fayyad 

essential novelty eld lies emphasizing discovery previously unknown structures vast databases emphasizing importance considering process 
exploratory data analysis methods illustrate structures data sets applied large databases 
tool endeavor self organizing map som kohonen kohonen 
properties distinguish som data mining tools numerical symbolic nonparametric capable learning supervision 
numerical nature method enables treat numerical statistical data naturally represent graded relationships 
method require supervision nonparametric sense assumptions distribution data need may nd quite unexpected structures data 
thesis relation som data visualization clustering methods rst analyzed section 
recipes som exploratory data analysis section 
areas application treated publications introduced section nally developments methodology discussed section 
methods exploratory data analysis exist methods quickly producing visualizing simple summaries data sets tukey 
example called number summary consisting smallest largest data value median rst third quartiles visualized drawing number corresponds constituent altitude box 
simple methods useful summarizing low dimensional data sets dimensionality increases ability visualize relations soon degrades 
section methods illustrating structures multivariate relations data items high dimensional data sets discussed 
treatment restricted methods regard inputs metric vectors making assumptions distribution data 
assumed external information class labels available data items 
illustrations driven solely actual structures data assumptions class structure 
analysis unsupervised possible class labels may aid interpretation results structures 
vectors input data set denoted statistics customary call components data vectors observations recorded variables 
mathematical terminology preferred 
components may called features customary pattern recognition literature 
section emphasis methods illustrate structures data sets 
may useful note practical applications selection preprocessing data may important choice analysis method 
example changes relative scales features drastic eoeect results methods larger scale component component result 
dioecult give general guidelines application specic task preprocessing approaches case studies discussed section 
questions play central role applying method large high dimensional data sets kinds structures method able extract data set illustrate structures reduce dimensionality data reduce number data items 
visualization high dimensional data items graphical means proposed visualizing high dimensional data items directly letting dimension govern aspect visualization integrating results gure cf du toit jain dubes 
methods visualize kinds highdimensional data vectors data items vectors formed descriptors data set number summaries tukey 
simplest method visualize data set plot item dimensional graph dimensions enumerated axis corresponding values fig 

alternative scatterplot original dimensions data chosen portrayed location icon rest dimensions depicted properties icon 
example lengths rays emanating center icon may visualize values rest components fig 

familiar pie diagrams 
andrews curves andrews curve data item obtained components data vectors orthogonal sinusoids added pointwise fig 

faces famous visual displays 
dimension data determines size location shape component facial caricature fig 

example component associated width mouth separation eyes major drawback applies methods data mining setting reduce amount data 
data set large display consisting data items portrayed separately incomprehensible 
methods useful illustrating dimensional data item visualized dioeerent methods 
component values length ray emanating center illustrates component andrews curve facial caricature 
kinds summaries data set cluster centroids introduced vectors self organizing map 
clustering methods goal clustering reduce amount data categorizing grouping similar data items 
grouping pervasive way humans process information motivations clustering algorithms provide automated tools help constructing categories taxonomies jardine sibson sokal 
methods may minimize human factors process 
clustering methods anderberg hartigan jain dubes jardine sibson sokal bailey divided basic types hierarchical partitional clustering 
types exists wealth subtypes dioeerent algorithms nding clusters 
hierarchical clustering proceeds successively merging smaller clusters larger ones splitting larger clusters 
clustering methods rule decided small clusters merged large cluster split 
result algorithm tree clusters called dendrogram shows clusters related 
cutting dendrogram desired level clustering data items disjoint groups obtained 
partitional clustering hand attempts directly decompose data set set disjoint clusters 
criterion function clustering algorithm tries minimize may emphasize local structure data assigning clusters peaks probability density function global structure 
typically global criteria involve minimizing measure dissimilarity samples cluster maximizing dissimilarity dioeerent clusters 
commonly partitional clustering method means clustering macqueen discussed detail closely related som algorithm 
means clustering criterion function average squared distance data items nearest cluster centroids ek kx gamma index centroid closest possible algorithm minimizing cost function begins initializing set cluster centroids denoted positions adjusted iteratively rst assigning data samples nearest clusters recomputing centroids 
iteration stopped change markedly 
alternative algorithm randomly chosen sample considered succession nearest centroid updated 
equation describe objective related method vector quantization gersho gray makhoul 
vector quantization goal minimize average squared quantization error distance sample representation algorithm minimizing equation described straightforward generalization algorithm proposed lloyd minimizing average quantization error dimensional setting 
problem clustering methods interpretation clusters may dioecult 
clustering algorithms prefer certain cluster shapes algorithms assign data clusters shapes clusters data 
goal just compress data set inferences cluster structure essential analyze data set exhibits clustering tendency 
results cluster analysis need validated 
jain dubes methods purposes 
potential problem choice number clusters may critical quite dioeerent kinds clusters may emerge changed 
initialization cluster centroids may crucial clusters may left empty centroids lie initially far distribution data 
clustering reduce amount data induce categorization 
exploratory data analysis categories limited value 
clusters illustrated aid understanding 
example case means algorithm centroids represent clusters high dimensional additional illustration methods needed visualizing 
projection methods clustering reduces amount data items grouping 
exist methods reducing dimensionality data items 
methods called projection methods goal projection represent input data items lower dimensional space way certain properties structure data set preserved faithfully possible 
projection visualize data set suoeciently small output dimensionality chosen 
linear projection methods principal component analysis pca hotelling display data linear projection subspace original data space best preserves variance data 
standard method data analysis understood eoeective algorithms exist computing projection 
neural algorithms exist oja oja rubner cichocki unbehauen 
demonstration pca 
projection pursuit 
exploratory projection pursuit friedman friedman tukey data projected linearly time projection reveals non normally distributed structure data set possible sought 
done assigning numerical index possible projection maximizing index 
denition interestingness projected data deviates normally distributed data main body distribution 
neural implementation idea fyfe baddeley 
interesting projection structure projection interesting may removed data procedure ripley divides statistical data analysis methods clustering methods projection methods multidimensional scaling mds methods 
useful patterns represented vectors euclidean space mds methods essentially form nonlinear projections 
eth npl zar mdg sle mli nga ind ben ken pak gha idn egy bol sen civ dom gtm mar cmr ecu col tha tun jam tur pol pan cri chl bwa mus mys arg ven bra hun prt kor irl isr esp sgp bel gbr ita aus aut fra usa deu swe jpn fin dataset projected linearly dimensional subspace obtained pca 
dimensional data item describes dioeerent aspects welfare poverty country 
data set consisting countries publication picked world development report published world bank 
missing data values neglected computing principal components zeroed forming projections 
key abbreviated country names appendix 
restarted reveal structure data set 
nonlinear projection methods pca take account nonlinear structures structures consisting arbitrarily shaped clusters curved manifolds describes data terms linear subspace 
projection pursuit tries express nonlinearities data set high dimensional highly nonlinear may dioecult visualize linear projections low dimensional display chosen carefully 
approaches proposed reproducing nonlinear higherdimensional structures lower dimensional display 
common methods allocate representation data point lower dimensional space try optimize representations distances similar possible original distances corresponding data items 
methods dioeerent distances weighted representations optimized 
multidimensional scaling mds refers group methods widely especially behavioral econometric social sciences analyze subjective evaluations pairwise similarities entities commercial products market survey 
starting point mds matrix consisting pairwise dissimilarities entities 
thesis distances pattern vectors euclidean space considered mds dissimilarities need distances mathematically strict sense 
fact mds creating space entities represented vectors evaluation dissimilarities entities 
goal thesis merely create space represent relations data faithfully reduce dimensionality data set suoeciently small value allow visual inspection set 
mds methods goal 
exists multitude variants mds slightly dioeerent cost functions optimization algorithms 
rst mds metric data developed historical treatments introductions mds provided example kruskal wish de leeuw heiser wish carroll young generalized analyzing nonmetric data common structure dissimilarity matrices corresponding instance evaluations dioeerent individuals 
algorithms designed analyzing single dissimilarity matrix reducing dimensionality data set broadly divided basic types metric nonmetric mds 
original metric mds torgerson cf 
young householder distances data items conguration points give rise distances sought 
linear projection subspace obtained pca 
key idea method approximate original set distances distances corresponding conguration points euclidean space constructing nonlinear projection method 
item represented lower dimensional say dimensional data vector goal projection optimize representations distances items dimensional space close original distances possible 
distance denoted distance dimensional space metric mds tries approximate 
square error cost objective function minimized written em gamma perfect reproduction euclidean distances may best possible goal especially components data vectors expressed ordinal scale 
rank order distances vectors meaningful exact values 
projection try match rank order distances dimensional output space rank order original space 
best possible rank ordering conguration points guaranteed introducing monotonically increasing function acts original distances maps distances values best preserve rank order 
nonmetric mds kruskal shepard uses function kruskal wish normalized cost function en gamma conguration projected points chosen minimize equation 
nonmetric mds motivated need treating data course inputs pattern vectors euclidean space 
projection tries preserve order distances data vectors absolute values 
demonstration nonmetric mds applied dimension reduction task 
eth npl zar mdg sle mli nga ind ben ken pak gha idn egy bol sen civ dom gtm mar cmr ecu col tha tun jam tur pol pan cri chl bwa mus mys arg ven bra hun prt kor irl isr esp sgp bel gbr ita aus aut fra usa deu swe jpn fin nonlinear projection constructed nonmetric mds 
data set 
missing data values treated simple method demonstrated produce results pattern recognition context dixon 
computing distance pair data items squared dioeerences component values available computed 
rest dioeerences set average computed dioeerences 
nonlinear projection method sammon mapping sammon jr closely related metric mds version described 
tries optimize cost function describes pairwise distances data set preserved 
cost function sammon mapping omitting constant normalizing factor gamma dioeerence sammon mapping nonlinear metric mds eq 
errors distance preservation normalized distance original space 
normalization preservation small distances emphasized 
demonstration sammon mapping 
eth npl zar mdg sle mli nga ind ben ken pak gha idn egy bol sen civ dom gtm mar cmr ecu col tha tun jam tur pol pan cri chl bwa mus mys arg ven bra hun prt kor irl isr esp sgp bel gbr ita aus aut fra usa deu swe jpn fin sammon mapping data set projected pca nonmetric mds 
missing data values treated manner forming nonmetric mds 
principal curves 
pca generalized form nonlinear curves 
pca projection data set linear manifold constructed goal constructing principal curve project set nonlinear manifold 
principal curves hastie stuetzle smooth curves dened property point curve average data points project point closest point curve 
intuitively speaking curves pass data set 
principal curves generalizations principal components extracted pca sense linear principal curve principal component connections methods delineated carefully original article 
extracted structures called principal curves generalization surfaces relatively straightforward resulting algorithms computationally intensive 
conception continuous principal curves may aid understanding principal components sensibly generalized 
useful practical computations curves discretized 
turned cherkassky ritter discretized principal curves essentially equivalent soms introduced hastie stuetzle introduced principal curves 
conception principal curves useful providing possible viewpoint properties som algorithm 
methods 
problem nonlinear mds methods computationally intensive large data sets 
computational complexity reduced restricting attention subset distances data items 
placing point plane distance points plane set exactly 
property triangulation method lee 
points mapped sequentially plane distance new item nearest items mapped preserved 
alternatively distance nearest item point common items may preserved 
points mapped order nearest neighbor distances original space preserved 
triangulation computed quickly compared mds methods tries preserve small fraction distances projection may dioecult interpret large data sets 
method may useful connection sammon mapping biswas 
dimensionality data sets reduced aid autoassociative neural networks represent inputs smaller number variables dimensions input data 
networks try reconstruct inputs faithfully possible representation data items constructed network reduced dimensional expression data 
linear nonlinear associative memories introduced kohonen 
representations formed hidden layer algorithm hastie stuetzle proposed nding discretized principal curves resembles batch version kohonen som algorithm details dioeerent 
multilayer perceptron dimension reduction task demers cottrell garrido 
special version multilayer perceptrons replicator neural network hecht nielsen shown capable representing inputs terms 
occurs somewhat idealized model inherent dimensionality data increases 
natural coordinates correspond coordinates dimensional unit cube transformed elastically distribution data 
inherent dimensionality data course dioecult identify practice 
replicator neural networks possibly forming visualization data set choosing 
intriguing approach require separate study compare quality results computational requirements network having practical size 
learning multilayer perceptrons backpropagation algorithm cf rumelhart known slow haykin possible alternative learning algorithms feasible 
self organizing maps self organizing map som kohonen kohonen kohonen kohonen neural network algorithm wide variety applications engineering problems data analysis back carlson cheng mart del br serrano serrano ultsch ultsch vars zhang li 
comprehensive treatment topic provided kohonen aspects relevant data exploration aspects needed understanding relationships som algorithms 
section data mining tools divided categories clustering methods projection methods 
som special case time reduce amount data clustering projecting data nonlinearly lower dimensional display 
basic algorithm rst motivated discussion competitive learning section 
properties useful data analysis introduced section 
mathematical treatments algorithm relations algorithms discussed section 
especially algorithms means clustering principal curves closely related som 
mathematically oriented reader may wish concentrate section algorithm fully usable competitive learning 
self organizing map algorithm competitive learning adaptive process neurons neural network gradually sensitive dioeerent input categories sets samples specic domain input space amari grossberg kohonen kohonen nass cooper rez von der malsburg 
kind division labor emerges network dioeerent neurons specialize represent dioeerent types inputs 
specialization enforced competition neurons input arrives neuron best able represent wins competition allowed learn better described 
exists ordering neurons neurons located discrete lattice self organizing map competitive learning algorithm generalized winning neuron neighbors lattice allowed learn neighboring neurons gradually specialize represent similar inputs representations ordered map lattice 
essence som algorithm 
neurons represent inputs vectors components correspond synaptic weights 
vector associated neuron called unit setting 
unit indexed vector nearest input winner competition arg min gamma usually euclidean metric choices possible 
winning unit neighbors adapt represent input better modifying vectors current input 
amount units learn governed neighborhood kernel decreasing function distance units winning unit map lattice 
locations units map grid denoted dimensional vectors respectively ij kr gamma denotes time 
learning process time vectors changed iteratively adaptation rule input time index winning unit ci gamma practice neighborhood kernel chosen wide learning process guarantee global ordering map width height decrease slowly learning 
learning process consisting winner selection equation adaptation synaptic weights equation modeled neural network structure neurons coupled inhibitory connections kaski kohonen kohonen 
properties useful exploring data virtue learning algorithm som forms nonlinear regression ordered set vectors input space 
vectors form dimensional follows distribution data 
ordered display 
ordered nature regression map display data items 
items mapped units map closest vectors nearby units similar data items mapped 
ordered display data items facilitates understanding structures data set 
kohonen rst propose displays illustrate data set 
display displaying kinds information 
clear advantage display analysts grow familiar map interpret new information displayed faster easily 
example map display ordered groundwork original data variables components data vectors displayed natural order 
displays demonstrated publication 
variables smoothed locally display helps gaining insight distributions values data set 
displays illustrative instance raw linearly organized statistical tables 
useful display residuals average dioeerences variables smoothed values 
visualization clusters 
ordered display illustrating clustering density dioeerent regions data space 
density vectors organized map density input samples kohonen ritter 
clustered areas vectors close empty space clusters sparse 
cluster structure data set brought visible displaying distances vectors neighboring units ultsch ultsch 
cluster display may constructed follows 
distance pair vectors computed scaled distances minimum maximum value optionally removing outliers 
map display scaled distance value determines gray level color point middle corresponding map units 
gray level values points corresponding map units set average nearest distance values hexagonal grid average distances corner 
values set visualized display smoothed spatially 
resulting cluster diagram general sense needs assumed shapes clusters 
clustering algorithms prefer clusters certain shapes jain dubes 
demonstration display constructed som 
map display constructed som algorithm 
order countries correspond fairly closely sammon mapping data set fig 

prominent clustering structures visible displays 
details map constructed publication 
size map units 
missing data 
frequently occurring problem applying methods statistics missing data 
components data vectors available data items may applicable dened 
simple dixon complex dempster approaches proposed tackling problem clustering projection methods likewise 
case som problem missing data treated follows choosing winning unit equation input vector compared vectors components available note vector components missing 
small proportion components data vector missing result comparison statistically fairly accurate 
vectors adapted equation components available modied 
demonstrated better results obtained approach described discarding data items components missing samad harp 
data items majority indicators missing assume winner selection accurate 
reasonable compromise publication discard data items exceeding chosen proportion missing values learning process 
discarded samples tentatively displayed map organized 
note som explore incomplete data sets preprocessing methods may problems missing components input data items 
example normalization data vectors done straightforward manner 
normalization variance component separately contrast viable operation incomplete data sets 
outliers 
measurement data may exist outliers data items lying far main body data 
outliers may result instance measurement errors typing errors inserting statistics data base 
cases desirable outliers result analysis 
case map displays generated som algorithm outlier map unit neighborhood rest display may inspecting rest data 
furthermore outliers easily detected clustering display input space denition sparsely populated near outliers 
desired outliers discarded analysis continued rest data set 
possible outliers erroneous data items really strikingly dioeerent rest 
case map display reveals outliers discarded paid special attention 
mathematical characterizations rigorous mathematical treatment som algorithm turned extremely dioecult general reviews provided kangas kohonen 
case discrete data set xed neighborhood kernel exists potential function som kohonen ritter schulten ci kx gamma index depends vectors cf 
eq 

learning rule som equation corresponds gradient descent step minimizing sample function ci kx gamma obtained selecting randomly sample iteration learning rule corresponds step stochastic approximation minimum equation discussed kohonen 
note equation index function vectors implies may change gradient descent step taken 
locally index change gradient step valid 
relation means clustering 
cost function som equation closely resembles equation means clustering algorithm tries minimize 
dioeerence som distance input vectors just closest taken account weighted neighborhood kernel som functions conventional clustering algorithm width neighborhood kernel zero 
close relation som means clustering algorithm hints self organized map follows closely distribution data set input space known vector quantization density vectors approximates density input vectors high dimensional data kohonen zador means essentially equivalent vector quantization 
fact expression density vectors som derived dimensional case ritter limit wide neighborhood large number vectors density proportional probability density function input data 
note means clustering algorithm som closely related best ways data mining probably dioeerent 
means clustering algorithm number clusters chosen number clusters data som number vectors chosen larger irrespective number clusters 
cluster structures visible special displays discussed section 
relation principal curves 
som algorithm creates representation input data set follows data distribution 
representation data set organized 
possible view cf 
ritter organization provided mathematical characterization principal curves hastie stuetzle 
point principal curve average points project 
curve formed conditional expectations data 
discrete distributions sensible dene curves possible construct practical algorithms data spatially 
som similar smearing performed neighborhood kernel adaptation process cf 
eq 

known som literature som vector represents local conditional expectations data items batch map algorithm kohonen essentially manifestation idea 
principal curves manifolds essentially continuous counterparts som 
principal curves characterization previous discussion may source providing intuitive understanding som algorithm 
goodness curve representing data distribution measured example average squared distance data points curve goodness means algorithm measured average squared distance data points nearest cluster 
principal curves critical points measure extremal respect small smooth variations hastie stuetzle 
implies smooth curve corresponds minimum average distance data items principal curve 
decomposition cost function 
cost function som equation decomposed terms follows lampinen oja discrete version kx gamma ij kn gamma denotes number data items closest vector voronoi region corresponding vector deriving approximation assumed ij holds exactly toroidal maps kernel shape away borders non toroidal map kernel zero locally 
rst term equation corresponds cost function means clustering algorithm average distance data points nearest cluster centroid 
clusters dened terms centroids terms vectors rst term may interpreted way measuring accurately map follows distribution input data 
second term hand may interpreted governing ordering vectors 
considering second term may help note general close centroid cluster dened xed point som algorithm closer neighborhood kernel ci centered minimize second term units close map similar vectors value neighborhood kernel large 
units lie farther away map may hand quite dissimilar vectors neighborhood kernel small distance large contribution error 
variants rich variety versions basic som algorithm proposed cf 
kohonen 
variants aim improving preservation topology map structures xed grid blackmore miikkulainen sommer fritzke fritzke martinetz schulten martinetz schulten 
methods may useful purposes visualization easily regular grid 
variants aim reducing computational complexity som 
speed computation extremely important aspect data mining vast databases analyzed 
publication computational speedups developed kohonen studies basic som algorithm 
speedup methods proposed 
example dimensional map enlarged simply inserting vector pair neighbors luttrell 
search best matching unit speeded constructing tree structured som oja level tree consists separate progressively larger som 
search best match proceeds level level time restricting search subset units governed location best match previous smaller level 
map taught level time starting smallest level 
teaching best match search done quickly data set relatively small location best match previous level tabulated input sample 
subset units search winner needs performed single table lookup 
fast approach construct hierarchical tree soms soms bottom layer treat dioeerent subsets components input variables 
outputs soms bottom layer combined hierarchical manner nal som takes account input vector luttrell 
small soms receives small dimensional input vectors winning unit potentially sought single fast table lookup 
computational speedups potentially useful comparison methods require careful study 
truly eoeective speedups suboptimal sense approximate original som careful study needed guarantee results satisfactory application 
bishop 
introduced latent variable density model called generative topographic mapping gtm closely related som principal curves 
latent variable models assumed data set explained small set latent variables 
principle latent variable space continuous gtm discrete grid grid som computational reasons 
probability distribution latent grid postulated generate probability distribution input space parameterized model 
input data set goal gtm model maximum likelihood sense data set 
done expectation maximization em algorithm dempster iteratively estimating rst probability distribution latent grid maximizing likelihood input data distribution 
gtm claimed advantageous properties compared som signicant disadvantages bishop 
may useful study properties closely get deeper understanding relations relative merits methods 
gtm objective function may facilitate theoretical analyses method 
som objective function input data distribution continuous erwin 
practical applications data sets nite som local objective function equation 
methods gtm neighborhood function governs smoothness mapping som algorithm 
gtm smoothness governed widths certain basis functions generating probability distribution input space 
essential dioeerence regarding neighborhood som width neighborhood kernel decreasing time certain wellestablished schedules cf kohonen kohonen gtm basis functions remain xed 
possible way dening topology preservation require mapping lower dimensional output space input space smooth continuous 
denition continuous version gtm interpreted topology preserving 
smooth continuous mappings measure regularity mapping needed 
attempts measure topology preservation soms fact aimed measuring goodness regularity mapping approaches discussed section 
measures help choosing basis functions provide suitably regular mappings probably useful case gtm 
probably useful practical applications dioeerent mappings generated gtm algorithm compared comparing likelihoods 
likelihoods straightforward interpretation 
objective function som equation goodness measure publication interpreted manner 
methods enable selecting best mapping set candidate mappings main goal applications 
remains seen important advantages gtm practical applications 
addition advantages disadvantage computational complexity 
computation gtm requires twice time som bishop 
dioeerence probably enhanced speedup methods discussed section introduced may dioecult apply similar speedups gtm 
speedup methods reduce computational complexity best match search best matching units gtm units contribute directly representing input 
notes statistical accuracy question large som grid simple answer unlimited number learning samples available large available computational resources allow publication essentially case 
data set consists nite sample larger data set question statistical accuracy map considered correct positions vectors estimated accurately available data 
question especially important map displaying new data items 
new items assumed follow distribution items learning map statistically accurate new items displayed accurately old ones 
high dimensional spaces general dioecult achieve suoecient statistical accuracy samples necessarily sparse 
tting lower dimensional structures som grid data set unsupervised manner hope generalizability results 
possible rule thumb choosing suitable size map number free parameters say certain fraction number values estimating 
dioecult compute number free parameters som neighborhood kernel restricts placement vectors 
rule thumb increase map size brings resolution mapping 
map smoothness mapping controlled independently map size changing nal width learning neighborhood 
directly total number parameters som may overly conservative 
goal data analysis merely visualize data set exploratory purposes publication new samples need mapped map taught represent input data set suf cient statistical accuracy sampling replacement small data set computation map 
course guarantee map generalize new data 
som useful maps having units data items 
map passed data items ordered manner distances close data item pairs visualized map displays methods discussed section 
brief application som may serve illustrate point 
dimensional circular som eoeect nding approximate solution traveling salesman problem ang nd shortest route passes data items 
map forms path follows data distribution extends close data item 
dimensional map follows data similar manner 
forms follow data closely map units 
note regularity mapping controlled independently map size changing width neighborhood kernel 
setting number nodes approximately equal number input samples useful rule thumb applications data sets relatively small 
extensive empirical investigations done example cross validating results utilizing cost function eq 
measures publication 
relations dioeerences som mds relations mappings ideal case 
relative performance dioeerent algorithms reducing dimensionality input data studied articles bezdek pal mao jain 
studies measures performance related preservation distances points 
studies cost function som type eq 
considered example cost function sammon mapping eq 

may surprising sammon mapping method measure 

hand dene preservation neighborhoods preservation distance ordering goal nonmetric mds 
approach comparing pattern classication capability original mapped space valid measure feature extraction performance unfortunately unlabeled data sets 
methods pursue dioeerent related goals productive approach trying quantify relative performance may analyze dioeerences qualitatively 
short delineation differences som methods dioeerences cost functions 
relation som clustering methods hand nonlinear distance preserving projection methods best revealed decomposition som cost function equation 
som performs clustering rst term eq 
organizes clusters second term eq 
time reducing amount data projecting low dimensional display 
essential dioeerence projection formed som nonlinear distance preserving projection methods mds som tries form locally correct projection mds methods try directly preserve interpoint distances 
applies nonmetric mds tries preserve rank order distances treats distances equally 
sammon mapping emphasizes preservation local distances 
som try preserve distances forms kind homeomorphism preserving nonlinear map grid data set 
order determined neighborhood kernel 
kernel localized narrow order determined locally 
global order arises local interactions gradual narrowing neighborhood kernel aids avoiding 
general case high dimensional space course accurately mapped low dimensional method 
cases results som distance preserving projection methods may quite dioeerent 
possible preserve distances quite possible mds methods preserve distances approximately distances poorly 
especially metric mds long distances dominate shorter local ones 
som contrast localized neighborhood kernel determines local distances contribute error function 
terms exploratory data analysis dioeerence interpreted follows mds methods coarse global order projected data items probably accurate denition provide accurate view distances data items 
som contrast tries guarantee items projected nearby locations similar local order local clustering structures shown map display trustworthy possible 
global structures usually displayed local ones considered important 
dioeerence mds tries preserve metric case nonmetric mds global ordering relations original space som tries preserve topology local neighborhood relations 
sammon mapping lies middle tries preserve metric considers preservation local structures important mds methods 
may hard dene topology preservation rigorously nite discrete structures may possible construct viable practical measures neighborhood relations preserved 
measures discussed detail section publication 
dioeerence methods preserve distances methods preserve topology hypothetical experiment data set consisting curved dimensional surface dimensional space 
distance preserving methods require dimensions describe structure data adequately 
topology preserving methods som hand need dimensions 
map forms follow curved surface data space 
computational properties 
common mds methods construct explicit mapping function projections samples computed simultaneous optimization process 
new samples projected recomputing projection accurately 
case sammon mapping problem may alleviated somewhat computing projection multilayer perceptron type network learns minimize cost function 
done learning rule analogous error backpropagation algorithm mao jain 
method necessarily reduce computational complexity original algorithm backpropagation learning rule computationally intensive generalizes mapping new samples 
alternative approach radial basis function rbf network construct mapping webb promising alternative original mds methods 
optimization cost functions mds methods requires comparisons pairs vectors computational complexity order number data items 
computational complexity som number map units learning step requires computations achieve suoecient statistical accuracy number iterations multiple size map order number input samples publication computational complexities methods order magnitude 
reduction complexity needed resolution som reduced smaller map 
possible faster versions algorithm discussed section parallel implementation 
publications massively parallel neurocomputer perform part som computations 
proposed chang lee computational complexity sammon mapping reduced applying representative subset total data set 
rest data items mapped trying optimize distances xed subset 
method accurate original 
computational complexity existence local minima cost functions may cause problems 
basis empirical experience sammon mapping easily get stuck local optima cf 
mao jain ripley practice computation projection repeated times starting dioeerent initial 
som algorithm hand quite robust learning begun wide neighborhood shrinks gradually nal narrow form kohonen 
may dioeerences resulting maps depending initial state stochastic input sequence eoeect dioeerences actual maps empirically 
measures proposed publication help 
combinations methods 
dioeerent methods display dioeerent properties data set useful approach probably 
especially useful combination rst reduce amount data clustering som display vectors distance preserving projection method gain additional insight 
original sammon suggested clustering front mapping algorithm sammon jr 
som perform clustering dioeerent views data available certainly useful 
proposed combination vector quantization algorithm followed metric mds sammon mapping cost function modied slightly introducing decreasing weighting function em gamma function forces mapping concentrate local distances 
stages som exploratory data analysis section guidelines som practical applications case studies publications belonging thesis 
related treatments earlier kaski kohonen kohonen 
basic steps applying som quite standard usual exploratory data analysis care taken choosing inputs evaluating results avoid misleading 
steps analysis interactively modifying composition data set basis insight gained previous round 
rst step analysis consists choice data set step evident stressed 
matter methodology results fundamentally depend quality suitability data 
methods may aid process gaining insight nature data 
preprocessing feature extraction choice suitable representations data items key step analysis 
unsupervised methods merely illustrate structures data set structures ultimately determined features chosen represent data items 
usefulness dioeerent preprocessing methods depends strongly application 
comprehensive treatment immense task basic approaches case studies introduced 
data comes process exists priori knowledge information course choosing features 
case eeg signal publication known frequency content signal depends instance vigilance individual 
better features tailored requirements task better results 
general task tailoring requires considerable expertise application area data analysis methodology 
experiments method potentially useful automatic feature extraction stage reported publication 
sophisticated feature extraction methods required case mining symbolic information text evident automatically available semantic features exist 
similarity relations computed forms words convey information meaning words 
contextual information constructing useful similarity relationships textual data ritter kohonen 
system utilizes representations words represent words suitably processed word category histograms represent documents discussed publications 
choice features scaling chosen applying som algorithm 
exists knowledge relative importance components data items corresponding dimensions input space scaled information 
importance may applications estimated automatically example criterion publication 
criterion available variance components may scaled equal value done publication 
importance choosing set indicators describes phenomenon interest proper scaling indicators 
best analysis methods overcome mistakes stage 
computation maps detailed guidelines compute maps kohonen documentation public domain program package kohonen 
vectors rst initialized lie ordered conguration plane spanned principal eigenvectors data taught phase process 
learning starts wide neighborhood kernel covering map rst phase kernel quickly narrows close nal width time smaller peak amplitude 
second longer phase neighborhood kernel continues narrower form slowly shrinks nal width magnitude 
rst phase enforces global ordering map second phase nal accurate state map formed gradually 
nal neighborhood width determines map closely map follow local structures data 
choosing maps som learning process described section stochastic variation left learning results 
ensure quality maps computed best map chosen cost function equation 
cost function som specic size map topology map lattice dened neighborhood kernel 
value cost function generally decrease map size increases increase width neighborhood function increases 
compare maps dioeerent sizes neighborhood kernels auxiliary criteria needed 
measure map preserves topology input space suitable auxiliary criterion map goodness 
topology preservation turned quite dioecult dene sensibly discrete grid 
exist dioeerent approaches measuring degree topology preservation specic examples reviewed publication additional approaches proposed zrehen inen 
rst approach relations vectors relations corresponding units map lattice compared 
example measured line connecting vectors neighboring units voronoi region unit endpoints zrehen 
voronoi region unit dened set consisting points input space closer vector 
alternative approach measuring topology preservation input samples determine continuous mapping input space map grid vectors closest data item neighbors map lattice mapping continuous near item 
approaches takes account accuracy map representing data rst term decomposition cost function eq 

publication proposed goodness som measured sum accuracy suitably chosen index topology preservation cf 
fig 

continuous near discontinuous near method measuring goodness soms consists computing representative set input samples distance rst closest vector second closest vector map 
index goodness average distances 
mapping input space map grid continuous near distances generally smaller mapping discontinuous 
dots denote vectors dimensional map dimensional input space 
vectors neighboring map units connected thin lines 
thick line indicates distance computed 
dimensional maps distance computed shortest path map 
related measure proposed 
analyzing topological structure data space 
essential dioeerences measures rst combine qualitatively dioeerent measures linear combination 
combination may dioecult choose publication measures combined measure distances space 
second combine number vectors measure 
considered size map indirectly extent contributes preservation topology 
best way measure goodness choosing map assuming computational resources teach set maps choice map size neighborhood kernel 
map minimizes cost function eq 
chosen set 
resulting set best maps nal map chosen measure goodness publication 
additional notes space publication note sparse data 
data set sparse consisting samples map unit dioecult measure topology preservation 
judgment continuity mapping sum data samples may inaccurate 
may useful consider distance nearest second nearest vector done publication distance third closest vector weighted suitably 
note high dimensionality 
motivation index continuity mapping input space map grid measure goodness mapping stems fact lower dimensional som grid tends fold trying follow distribution higher dimensionality 
discontinuities mapping indicate presence folds 
motivation may misleading high dimensional data spaces necessarily sparse cf 
bellman 
may samples distinguishing non linear curve sample higher dimensional distribution instance 
goodness measure proposed publication may useful index regularity mapping cases local lower dimensional regions input space measure useful regions 
note computational complexity 
proposed measure fairly computationally intensive 
requires searching shortest path pair units map distances vectors data space 
measure computed dynamic programming cf sedgewick reduces complexity 
coarse estimate measure suitable large maps obtained computing distance vectors units lie shortest path map lattice computing distance map lattice 
interpretation evaluation maps interpretation 
interpretation ndings exploratory data analysis depends course application 
exist general methods may aid interpretation process 
caution due interpretation map metaphor may useful intuitively understanding kinds applications worthwhile necessarily hold way interpretation map 
road maps example basically scale distances som may transform locations data items highly nonlinear manner 
sensible try interpret vertical horizontal axes map general special cases publication may exist straightforward interpretations 
desired simple interpretations may sought displaying auxiliary information map display inspecting distribution 
publication longer axis map correlate economical welfare measured gnp gross national product capita 
som tries preserve local structures interpretation map predominantly done locally local relations data items map 
global structure useful 
dioeerent properties vectors data items visualized map display aid interpretation discussed section 
data items come time varying process possible visualize trajectory successive samples map monitor state process easily understandable visual display cf kangas kohonen 
trajectory displays publication 
method aids interpretation maps provided external information class labels available plot labels organized map 
distribution samples class plotted map density histogram may help interpretation process 
distributions samples containing dioeerent types background eeg activity displayed publication dioeerent discussion topics usenet newsgroups publication 
distributions known classes overlapping displays explore degree overlap dioeerent types samples may possible gain insight classes exist new kinds features added data items classes easily separable 
displays vectors may useful 
methods visualizing high dimensional data discussed section application specic visualization methods head shaped displays publication 
evaluation 
quality map display generally evaluated expert application area 
samples having known classes available potentially useful try classify samples map 
map unit labeled majority voting samples samples projected unit label 
classication accuracy indicates classes separated map classication accuracy new samples measures generalizability result 
classication accuracy manner publications 
generalizability results give indication quality mapping 
generalizability measured sensitivity map small variations input data caused adding articial noise cross validation 
purposes exploratory data analysis sensitivity visualization measured sensitivity vectors som 
publication possible sensitivity measure dioeerence maps represent visually relations pairs data items 
organized maps exploratory data analysis 
illustrations formed som tools gaining insight data set 
summarize data sets results explorative research decision support system dss cf 
serrano 
som facilitating exploration data set searching known kinds data ltering new incoming data visualization results 
som extracting clusters automatically lampinen oja murtagh card vars rule extraction ultsch 
case studies thesis consists case studies addition methodological developments 
case studies serve demonstrations som dioeerent kinds analysis tasks equally important motivation studies need actual applications 
multichannel eeg signal eeg lopes da silva lopes da silva consists set signals measured electrodes scalp 
pattern changes signals large scale brain activity example occurrence certain kinds oscillation patterns known correlated certain vigilance states subject 
addition brain activity eeg activation head eye movements interference nearby electric devices changing conductivity electrodes due movements subject reactions electrode sites 
activities directly related current cognitive processing subject collectively referred background activity 
eeg measurements provide plenty continuous valued time dependent data eeg specic feature extraction procedure needed revealing interesting activity patterns 
publication background eeg activity visualized som extracting frequency features short time spectra eeg signals channels 
dimensional data items correspond short time frequency content eeg multiple bands multiple locations scalp 
resulting map distinguished dioeerent kinds background activity 
dioeerent activity types predominantly projected dioeerent areas map 
areas connected samples subjects 
visualizing trajectory successive data items possible monitor changes background activity types activity underlying dioeerent map areas inspected visualizing vectors 
statistical tables standard living involves wealth dioeerent aspects ranging health education quality environment 
time consuming task acquire insight statistical table indicators represents numerous relevant aspects 
task precisely suitable application exploratory data analysis methods 
publication total indicators describing dioeerent aspects standard living chosen world development indicator set world bank display structures welfare poverty data set reveals set som 
dominant axis map correlated gnp capita included teaching corpus displaying distribution gnp capita values groundwork formed organized map 
interpretation ne structures welfare poverty done inspecting distributions values original indicators map groundwork 
study input data set nite components data items scaled equal variance dioecult determine dioeerences importance 
study demonstrated missing value problem treated 
full text document collections project aims constructing methods exploring full text document collections websom started timo honkela suggestion organizing semantic ritter kohonen preprocessing stage encoding documents 
documents organized preprocessing stage map way nearby locations contain similar documents exploration collection facilitated intuitive neighborhood relations 
structures collection visualized methods described section 
basic method described publication experiments large maps document collections publication browsing interface exploration examples publication 
partly supervised version method constructed honkela 
maps publications available exploration internet address websom hut fi websom 
advantages gained som feature extraction stage websom analyzed detail publication 
turned self organizing semantic map form computationally eoecient approximation probabilistic model takes account contextual information encoding documents 
developments turned quite simpler document encoding method publications produce better results 
standard practice information retrieval ir salton mcgill encode documents vectors component corresponds dioeerent word value component frequency occurrence word document 
word occurs times document dened equal unit vector corresponding kth vector component possible code document problem encoding method vocabulary large dimensionality vector high 
publications problem solved eliminating common rarest words clustering words word categories 
extended version equation probabilistic information similarity dioeerent words incorporated alternative approach reducing dimensionality simply reduce dimensionality vectors eoeect represent words 
simple method project randomly space lower dimensionality ritter kohonen 
turned experiments version equation contextual information results better separability dioeerent usenet discussion areas accuracy newsgroup separation current websom method accuracy method vectors estimated contextual information place gallant gallant accuracy 
experimental procedures described detail publication 
reduced dimensional version equation fast processing documents table lookups subsequent convolutions publications impossible 
preliminary results need thorough validation 
may case concluded methods utilizing contextual information improved 
developments feature exploration adaptive subspace som feature extraction dioecult task enterprise data exploration 
components data vectors selected preprocessed relations representations data items correspond meaningful relations items 
considerable expertise application area may required building suitable feature extractors 
adaptive subspace som assom kohonen kohonen kohonen kohonen step general purpose feature extractor extracts invariant features input features invariant particular transformation operated inputs 
assom act learning feature extraction stage produces invariant representations explored som feature exploration 
assom extracts invariant features lters formed automatically short sequences input samples learning process 
insight processes produced data set gained visualizing resulting lters 
visualization particularly easy representations assom ordered just basic som 
publication study assom algorithm continued theoretical treatment scope experimental simulations broadened 
demonstrated features invariant dioeerent kinds transformations extracted areas units specialized invariant transformations emerge transformations training data albeit dioeerent times 
input space map grid nonlinearity som taken account dening distance map units distance formed map input space 
left vectors dimensional map dimensional input space denoted dots 
neighboring vectors connected thin lines 
distance units drawn thick line 
depending values vectors path distance computed need shortest path map grid shown right 
comparison knowledge areas soms adopted large scale summarizing information various data sets able compare data sets indirectly comparing formed ordered sets vectors 
possible application areas include comparison organizational data idata making data organization available online retrieval nowadays quite popular cf 
fayyad comparison expertise dioeerent parties deciding learn 
comparisons dioeerent maps focus similarity representations knowledge maps form 
measure similarity maps represent relations data items publication 
relation distance representations data items map display computed account nonlinearity map distance pair neighboring map units rst dened distance corresponding vectors 
distance map units dened distance minimum path units map cf 

computation distance illustrated 
distance data items dened sum distances data items closest vector plus distance corresponding units map 
measure dis similarity maps metric soms constructed comparing relations pairs data items maps 
thesis methodologies established applying self organizing maps exploratory analysis large data sets 
methods demonstrated dioeerent kinds case studies continuous valued dense data eeg signals continuous valued sparse data indicators standard living dioeerent countries discrete valued data full text document collections self organizing maps illustrate structures data dioeerent manner example multidimensional scaling traditional multivariate data analysis methodology 
som algorithm concentrates preserving neighborhood relations data trying preserve distances data items 
comparisons methods having dioeerent goals eventually practical applicability 
som shown provide viable alternative 
inen 
process error detection self organizing feature maps 
kohonen simula kangas editors articial neural networks 
proceedings icann international conference articial neural networks volume ii pages 
north holland amsterdam 
amari 
topographic organization nerve elds 
bulletin mathematical biology 
anderberg 
cluster analysis applications 
academic press new york ny 
andrews 
plots high dimensional data 
biometrics 
ang de la le 

selforganizing feature maps traveling salesman problem 
neural networks 
back 
data mining accounting numbers self organizing maps 
honkela jakobsson editors proceedings step finnish articial intelligence conference pages 
finnish articial intelligence society finland 
bellman 
adaptive control processes guided tour 
princeton university press new jersey nj 
bezdek pal 
index topological preservation feature extraction 
pattern recognition 
bishop svens williams 

em optimization latent variable models 
touretzky mozer hasselmo editors advances neural information processing systems pages 
mit press cambridge ma 
bishop svens williams 

gtm principled alternative self organizing map 
von der malsburg von seelen editors proceedings icann international conference articial neural networks lecture notes computer science vol 
pages 
springer berlin 
biswas jain dubes 
evaluation projection algorithms 
ieee transactions pattern analysis machine intelligence 
blackmore miikkulainen 
incremental grid growing encoding high dimensional structure dimensional feature map 
proceedings icnn ieee international conference neural networks volume pages 
ieee service center piscataway nj 

algorithme de kohonen application analyse de donn es 
bulletin des des 
sommer 
dynamic cell structure learns perfectly topology preserving map 
neural computation 


self organizing neural network traveling salesman problem competitive simulated annealing 
neural computation 
carlson 
self organizing feature maps appraisal land value shore parcels 
kohonen simula kangas editors articial neural networks 
proceedings icann international conference articial neural networks volume ii pages northholland amsterdam 
chang lee 
heuristic relaxation method nonlinear mapping cluster analysis 
ieee transactions systems man cybernetics 
cheng liu wu 
interactive knowledge discovery self organizing feature maps 
proceedings world congress neural networks volume iv pages 
lawrence erlbaum hillsdale nj 

faces represent points dimensional space graphically 
journal american statistical association 
cichocki unbehauen 
neural networks optimization signal processing 
john wiley chichester england 
cooley 
multivariate data analysis 
wiley new york ny 
de leeuw heiser 
theory multidimensional scaling 
krishnaiah kanal editors handbook statistics volume pages 
north holland amsterdam 

analyse de donn es par de neurones auto data analysis self organized neural networks 
phd thesis institut national polytechnique de grenoble grenoble france 


curvilinear component analysis selforganizing neural network nonlinear mapping data sets 
ieee transactions neural networks 
demers cottrell 
non linear dimension reduction 
hanson cowan giles editors advances neural information processing systems pages morgan kaufmann san mateo ca 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
devijver kittler 
pattern recognition statistical approach 
prentice hall englewood nj 

simulation modeling distributed information processing frog visual system 
phd thesis stanford university 

model visuomotor mechanisms frog optic 
mathematical biosciences 
dixon 
pattern recognition partly missing data 
ieee transactions systems man cybernetics 
du toit 
graphical exploratory data analysis 
springer verlag new york ny 
erwin obermayer schulten 

self organizing maps ordering convergence properties energy functions 
biological cybernetics 
fayyad 

data mining knowledge discovery making sense data 
ieee expert october pages 
fayyad piatetsky shapiro smyth 
knowledge discovery data mining unifying framework 
simoudis han fayyad editors proceedings kdd second international conference knowledge discovery data mining pages 
aaai press menlo park ca 
fayyad piatetsky shapiro smyth editors advances knowledge discovery data mining 
aaai press mit press menlo park ca 
fayyad piatetsky shapiro smyth 
data mining knowledge discovery overview 
fayyad piatetsky shapiro smyth uthurusamy editors advances knowledge discovery data mining pages 
aaai press mit press menlo park ca 

limitations self organizing maps vector quantization multidimensional scaling 
appear mozer jordan petsche editors advances neural information processing systems 
forsyth editor machine learning principles techniques 
chapman hall london 
friedman 
exploratory projection pursuit 
journal american statistical association 
friedman tukey 
projection pursuit algorithm exploratory data analysis 
ieee transactions computers 
fritzke 
grow self organizing feature maps problem dependent cell structure 
kohonen simula kangas editors articial neural networks 
proceedings icann international conference articial neural networks volume pages north holland amsterdam 
fritzke 
growing cell self organizing network unsupervised supervised learning 
neural networks 
fu 
syntactic methods pattern recognition 
academic press new york ny 
fukunaga 
statistical pattern recognition 
academic press new york ny 
fyfe baddeley 
non linear data structure extraction simple hebbian networks 
biological cybernetics 
gallant 
methods generating revising context vectors plurality word stems 
patent number 
gallant caid carleton hecht nielsen pu qing 
hnc system 
acm sigir forum 

self organizing map applied macro micro analysis data dummy variables 
proceedings world congress neural networks pages 
lawrence erlbaum inns press hillsdale nj 
garrido serra 
multilayer feedforward neural nets display method multidimensional distributions 
international journal neural systems 
gersho 
asymptotically optimal block quantization 
ieee transactions information theory 
finch sejnowski 
quantifying neighborhood preservation topographic mappings 
technical report institute neural computation la jolla ca 
gray 
vector quantization 
ieee assp magazine april pages 
grossberg 
development feature detectors visual cortex applications learning reaction systems 
biological cybernetics 
hair jr anderson black 
multivariate data analysis readings th edition 
prentice hall englewood nj 
inen 
measure disorder self organizing map 
proceedings icnn ieee international conference neural networks volume ii pages 
ieee service center piscataway nj 
hartigan 
clustering algorithms 
wiley new york ny 
hastie stuetzle 
principal curves 
journal american statistical association 
haykin 
neural networks 
comprehensive foundation 
macmillan new york ny 
hecht nielsen 
replicator neural networks universal optimal source coding 
science 

exploratory data analysis 
kotz johnson read editors encyclopedia statistical sciences volume pages 
wiley new york 
honkela kaski lagus kohonen 
exploration fulltext databases self organizing maps 
proceedings icnn ieee international conference neural networks volume pages 
ieee service center piscataway nj 
hotelling 
analysis complex statistical variables principal components 
journal educational psychology 
kohonen kangas kaski 
visualizing clusters self organizing map 
carlsson rvi editors proceedings conference articial intelligence research finland pages 
finnish articial intelligence society helsinki finland 
jain dubes 
algorithms clustering data 
prentice hall englewood nj 
jardine sibson 
mathematical taxonomy 
wiley london 
kangas 
analysis pattern sequences self organizing maps 
phd thesis helsinki university technology espoo finland 
kaski kohonen 
winner take networks physiological models competitive learning 
neural networks 
kaski kohonen 
structures welfare poverty world discovered self organizing map 
technical report helsinki university technology laboratory computer information science espoo finland 
kangas simula 
process state monitoring self organizing maps 
aleksander taylor editors articial neural networks 
proceedings icann international conference articial neural networks pages north holland amsterdam 
kendall 
multivariate analysis 
charles london 
kohonen 
construction similarity diagrams phonemes selforganizing algorithm 
report helsinki university technology espoo finland 
kohonen 
self organized formation topologically correct feature maps 
biological cybernetics 
kohonen 
self organization associative memory 
rd edition 
springer berlin 
kohonen 
self organizing map 
proceedings ieee 
kohonen 
self organizing maps optimization approaches 
kohonen simula kangas editors articial neural networks 
proceedings icann international conference articial neural networks volume ii pages north holland amsterdam 
kohonen 
physiological interpretation self organizing map algorithm 
neural networks 
kohonen 
adaptive subspace som assom implementation invariant feature detection 
fogelman gallinari editors proceedings icann international conference articial neural networks volume pages 
ec cie paris 
kohonen 
emergence invariant feature detectors selforganization 
marks ii fogel fukuda editors computational intelligence 
dynamic system perspective pages 
ieee press new york ny 
kohonen 
self organizing maps 
springer berlin 
kohonen 
emergence invariant feature detectors self organizing map 
biological cybernetics 
kohonen kangas laaksonen 
self organizing map program package 
technical report helsinki university technology laboratory computer information science espoo finland 
kohonen oja simula visa kangas 

engineering applications self organizing map 
proceedings ieee 

progress tree structured self organizing map 
cohn editor proceedings ecai th european conference articial intelligence pages wiley chichester england 

fast deterministic self organizing maps 
gallinari editors proceedings icann international conference articial neural networks volume ii pages ec cie paris 
oja 
self organizing hierarchical feature maps 
proceedings ijcnn san diego international joint conference neural networks volume ii pages ieee service center piscataway nj 
mao jain 
non linear projection method kohonen topology preserving maps 
proceedings icpr th international conference pattern recognition pages ieee computer society press los alamitos ca 
mao jain 
nonlinear projection method kohonen topology preserving maps 
ieee transactions neural networks 
kruskal 
multidimensional scaling optimizing goodness nonmetric hypothesis 
psychometrika 
kruskal wish 
multidimensional scaling 
sage university series quantitative applications social sciences number 
sage publications park ca 
lampinen oja 
clustering properties hierarchical selforganizing maps 
journal mathematical imaging vision 
langley 
elements machine learning 
morgan kaufmann san francisco ca 
lee blum 
triangulation method sequential mapping points space space 
ieee transactions computers 
lloyd 

squares quantization pcm 
unpublished memorandum bell laboratories 
published ieee transactions information theory 
lopes da silva storm van leeuwen mond editors handbook clinical neurophysiology 
volume clinical applications computer analysis eeg neurophysiological signals 
elsevier amsterdam 
luttrell 

self organizing multilayer topographic mappings 
proceedings icnn ieee international conference neural networks volume pages 
ieee service center piscataway nj 
luttrell 

hierarchical vector quantization 
iee proceedings 
macqueen 

methods classication analysis multivariate observations 
le cam neyman editors proceedings fifth berkeley symposium mathematical statistics probability 
volume statistics pages 
university california press berkeley los angeles ca 
makhoul gish 
vector quantization speech coding 
proceedings ieee 
mao jain 
articial neural networks feature extraction multivariate data projection 
ieee transactions neural networks 
mart del br serrano 
self organizing neural networks analysis representation data nancial cases 
neural computing applications 
martinetz schulten 
network learns topologies 
kohonen simula kangas editors articial neural networks 
proceedings icann international conference articial neural networks volume pages 
north holland amsterdam 
martinetz schulten 
topology representing networks 
neural networks 

som statistical analysis supermarket customer 
sax editors proceedings symposium neural network research finland pages 
finnish articial intelligence society turku finland 
michalski carbonell mitchell editors machine learning articial intelligence approach 
tioga publishing palo alto ca 
ikeda nakayama 
topology analysis data space self organizing map 
proceedings icnn ieee international conference neural networks pages 
ieee service center piscataway nj 
cherkassky 
self organization iterative kernel smoothing process 
neural computation 
murtagh 
interpreting kohonen self organizing feature map contiguity constrained clustering 
pattern recognition letters 
nass cooper 
theory development feature detecting cells visual cortex 
biological cybernetics 
lopes da silva editors basic principles clinical applications related fields 
urban baltimore second edition 

electric fields brain 
eeg 
oxford university press new york ny 
oja 
subspace methods pattern recognition 
research studies press england 
oja 
principal components minor components linear neural networks 
neural networks 
card 
linguistic interpretation self organizing maps 
ieee international conference fuzzy systems pages 
ieee service center piscataway nj 
rez glass 
development cat visual cortex 
journal mathematical biology 
ripley 

pattern recognition neural networks 
cambridge university press cambridge great britain 
ritter 
asymptotic level density class vector quantization processes 
ieee transactions neural networks 
ritter kohonen 
self organizing semantic maps 
biological cybernetics 
ritter martinetz schulten 
neural computation selforganizing maps 
addison wesley reading ma 
ritter schulten 

kohonen self organizing maps exploring computational capabilities 
proceedings icnn ieee international conference neural networks volume pages 
ieee service center piscataway nj 
rubner 
self organizing network principal component analysis 
letters 
rumelhart hinton williams 
learning internal representations error propagation 
rumelhart mcclelland pdp research group editors distributed processing 
explorations microstructure cognition 
volume foundations pages 
mit press cambridge ma 
salton mcgill 
modern information retrieval 
mcgraw hill new york ny 
samad harp 
self organization partial data 
network computation neural systems 
sammon jr 
nonlinear mapping data structure analysis 
ieee transactions computers 

pattern recognition statistical structural neural approaches 
wiley new york ny 
sedgewick 
algorithms 
addison wesley reading ma nd edition 
serrano 
self organizing neural networks nancial diagnosis 
appear decision support systems 
shepard 
analysis proximities multidimensional scaling unknown distance function 
psychometrika 
simoudis 

reality check data mining 
ieee expert october pages 
sokal 
numerical taxonomy 
freeman san francisco ca 

model formation ocular dominance stripes 
proceedings royal society london 

decision estimation classication 
pattern recognition related topics 
wiley new york ny 
torgerson 
multidimensional scaling theory method 
psychometrika 

self organizing feature maps process control chemistry 
articial neural networks 
proceedings icann international conference articial neural networks volume pages north holland amsterdam 
bailey 
cluster analysis 
mcgraw hill new york ny 
tukey 
exploratory data analysis 
addison wesley reading ma 
ultsch 
knowledge extraction self organizing neural networks 
opitz lausen editors information classication pages 
springer verlag berlin 
ultsch 
self organizing neural networks visualization classi cation 
opitz lausen editors information classication pages 
springer verlag berlin 
ultsch 
kohonen self organizing feature maps exploratory data analysis 
proceedings icnn international neural network conference pages kluwer dordrecht 
vars 
clustering socio economic data kohonen maps 
neural network world 

applications basics computing exploratory data analysis 
duxbury press boston ma 
von der malsburg 
self organization orientation sensitive cells striate cortex 
kybernetik 
webb 

multidimensional scaling iterative majorization radial basis functions 
pattern recognition 
wish carroll 
multidimensional scaling applications 
krishnaiah kanal editors handbook statistics volume pages 
north holland amsterdam 
world bank world development report 
oxford university press new york ny 
young 
multidimensional scaling 
kotz johnson read editors encyclopedia statistical sciences volume pages 
wiley new york ny 
young householder 

discussion set points terms mutual distances 
psychometrika 
zador 

asymptotic quantization error continuous signals quantization dimension 
ieee transactions information theory 
zhang li 
self organizing map new method clustering data analysis 
proceedings ijcnn nagoya international joint conference neural networks pages 
ieee service center piscataway nj 
zrehen 
analyzing kohonen maps geometry 
kappen editors proceedings icann international conference articial neural networks pages springer london 
appendix key country names afg afghanistan ago alb united arg argentina aus australia aut austria bdi bel belgium ben bgr bulgaria bol bra brazil bur bwa caf central african rep canada che switzerland chl chile china civ cote cmr cog congo col colombia cri costa czechoslovakia deu germany denmark dom rep algeria ecu egy egypt rep esp spain eth fin finland fra france gab gbr united kingdom gha gin guinea greece gtm hong kong hnd hti hun hungary idn indonesia ind india irl ireland iran rep irq iraq isr israel ita italy jam jor jordan jpn japan ken kor korea rep kuwait lao lao pdr lbn lebanon lbr sri lso mar mdg mex mexico mli mali mng mrt mus mys malaysia nam ner niger nga nic netherlands norway npl new zealand taiwan china oman pak pakistan pan panama png new guinea pol poland prt portugal rom romania arabia sudan sen sgp singapore sle sierra leone el salvador som swe sweden syr rep tcd chad tha thailand tun tur turkey uga uruguay usa united states ven nam rep yugoslavia south africa zar 
