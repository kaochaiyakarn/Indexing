strategies parallel data mining skillicorn skill cs ca may external technical report issn department computing information science queen university kingston ontario canada document prepared may copyright fl skillicorn set cost measures applied parallel algorithms predict computation data access communication performance 
measures possible compare different possible parallel implementation strategies data mining techniques necessity benchmark 
give general cost expressions common parallelizing strategies show instantiate cost expressions particular technique neural networks 
keywords cost measurement parallelization complexity association rules neural networks genetic algorithms 
role parallel computing successful data mining open question 
data mining algorithms expensive high demands computation data access 
plausible sophisticated algorithm greater commercial advantage demand computation grow rate growth data stored online suggests demand data access grow 
parallel computing natural role play parallel computers large computations handling large volumes data 
hand data mining may turn hard looks 
example sampling large dataset provides results obtained entire dataset high performance computation data access may unnecessary 
furthermore bottleneck sequential data mining access disk storage 
parallel computers multiple striped disks advantage parallel computers scale performance disk access system fast computational power price 
cost benefit trade may favor parallel computing long run 
parallel computing considerable potential tool data mining completely clear represents data mining 
idea parallel computing data mining obtained examining uses world largest supercomputers put 
number industrial users top list increased nov nov 
names organizations systems suggest data mining data mining success stories tend publicized 
commercial applications top rank owner state farm charles schwab oracle ibm chase manhattan sears deutsche morgan lexis nexis bern designing implementing parallel programs expensive 
data mining algorithms parallelized different ways 
impractical test building implementations comparing 
fortunately practical complexity measures parallel programming rapidly maturing 
show assess different parallelization strategies cost measures 
search space effective parallel data mining algorithms greatly reduced high level cost analysis 
section presents different parallelization strategies discusses performance intuitively 
section introduces robust set cost measures predict performance algorithm platforms 
section review standard data mining algorithms 
section give sequential cost expression algorithms 
section describe cost different techniques parallelizing data mining algorithms 
section give detailed example techniques neural networks section explore double speedup phenomenon occurs algorithms 
strategies parallelizing data mining algorithms typical data mining application starts dataset describing interactions customers organization 
concreteness suppose dataset table rows columns 
column describes specific possible interaction buying apples row describes set interactions occurred visit supermarket 
output data mining algorithm information distilled data 
results may ffl set concepts predicates interactions customers buy apples visit 
algorithms produce concepts said transparent concepts usually intelligible organizational terms 
ffl set model parameters 
typically build classifiers example distinguish profitable unprofitable customers 
algorithms kind said opaque usually clear classifier computes answers 
speak results data mining concepts difference draw 
size datasets data mining large 
dataset contain information customer interactions involving attributes 
parallel data mining requires dividing processors useful progress solution fast possible 
question division labor 
clearly idea divide computation equally important divide accesses dataset minimize communication processors working 
data mining applications want minimize resources spent developing concepts valid locally limited amount data available processor valid globally 
technique balancing act local speculative computation may turn wasted expensive reassuring communication 
basic strategies parallelizing data mining algorithms 

independent search 
processor access dataset heads different part search space starting randomly chosen initial position 

parallelize sequential data mining algorithm 
common variants 
set concepts partitioned processors processor examines entire dataset determine local concepts globally correct 
generating new concepts usually requires knowing smaller simpler concepts correct processor regularly exchange information concepts 
second dataset partitioned columns processor computes partial concepts hold columns see 
regular exchanges needed determine partial concepts fitted globally correct concepts 

replicate sequential data mining algorithm 
processor works partition dataset rows executes sequential algorithm 
information sees partial builds entire concepts locally correct may globally correct 
call approximate concepts 
processors exchange approximate concepts facts check globally correct 
learns parts dataset see 
course possibilities combinations distinguish main lines attack 
independent search strategy desired output optimal solution works minimization problems 
parallelized approaches attracted attention applying parallel computing require parallel algorithm 
variants problems 
requires processor access entire dataset 
access terabyte datasets slow compared processor speeds growing relatively slower afford lot computation avoid accesses 
information exchanged processors tends large typically concepts 
second parallelized approaches suffers partial concepts extended complete concepts facts true attributes true globally hard winnow quickly 
course knew attributes belonged assign processor get results exactly want discover 
replicated approach particularly novel best way add high performance data mining application 
significant advantages dataset partitioned access cost spread processors data exchanged phases smaller concepts communication cheap 
possible generate local concepts hold globally fact internally consistent consistent local subset dataset means happen 
results data mining algorithm larger input dataset surely interesting results 
follows concept set extremely large intermediate stages algorithm 
happens replication approach perform requires processor store current concept set 
analysis intuitions inherent communication data access 
intuitive ideas formal define realistic tractable way measuring costs computation communication data access 
parallel complexity theory standard parallel complexity theory pram architecture single shared memory accessible constant time 
programmer responsibility ensure processors access location simultaneously 
real architectures pay penalty accessing locations distant physically shared memory distributed memory 
requirement programmers prevent interference strong practice architectures allow arbitrary access patterns appear programs pay overhead run time prevent conflicting accesses 
latency costs costs accessing distance conflict costs preventing simultaneous accesses cause large discrepancies theoretical costs pram model observed programs run real machines 
furthermore pram model act approximation foundation accurate cost model 
general possible tell pram model going error depends details memory access pattern target architecture network 
worse errors occur polynomial problem size constant factor errors sequential complexity theory 
parallel complexity measure correct constant factor needed 
important aspects choosing measure 
take account costs associated memory hierarchy 
issue arises sequential cost modelling relatively small subclass memory bound memory algorithms execution time dominated number instructions execute 
parallel setting greater proportion programs significant amounts memory getting right critical 
second measure accurately reflect costs communication explicitly message passing programs implicitly shared memory programs 
costing communication difficult highly non linear cost sending message empty network different cost sending heavily loaded network 
reason course congestion network busy message greater chance blocked traffic trip 
experimental studies show congestion parallel computers happens boundaries network middle 
words problem getting getting network 
multiple messages arrive processor extract time block back network 
accurate measures communication performance obtained assume network heavily loaded state 
worst non linearities occur intermediate loads 
message may encounter messages take long time may lucky travel straight 
network busy messages encounter messages effects individual delivery times average 
processor sends messages random destinations possible determine expected delivery times small variance 
forms basis robust measure communication performance valid long communication takes place network busy true processors interleave computation phases communication phases rough synchrony 
convenient capture network performance terms network permeability called units time byte delivered 
mentioned earlier link processor network actual bottleneck 
processors sending data processor sends receives data take longest finish 
processor sends receives bytes accurate estimate cost communication phase maximum 
assumes processors spend time communicating programs behavior 
data mining algorithms approximately 
structured number phases involves local computation followed exchange data processors 
cost phase described expression form cost max processors max processors number instructions executed processor cost model derived bsp fundamental accuracy verified wide range applications 
bsp enforces alternating computation communication structure programs works reduce effective value doesn affect basic point 
notice terms units time 
avoids need decide weight cost communication relative computation possible compare algorithms different mixes computation communication 
addressed issue compute bound versus memory bound computations 
hard situation model time taken computation close required memory access hard predict dominate 
detailed cost model required precise interactions instructions cache may critical 
data mining applications straightforward tell computation dominate memory access memory access pattern typically single pass dataset phase cost predictable 
cost model produce accurate estimates running times existing parallel computers 
comparing different algorithms absolute accuracy important relative accuracy 
high confidence model counting instructions executed bytes communicated 
data mining applications different parallelization strategies execute approximately number instructions 
aspect separates communication 
clearly algorithm communicates going expensive regardless details communication 
cost model depends high level properties algorithms applied algorithm 
compare different parallelization strategies having develop different implementations benchmarking provided costs clearly different 
particular strategies unproductive ruled cheaply 
data mining algorithms data mining techniques investigated 
concentrate ffl association rules ffl neural networks ffl genetic algorithms techniques decision trees inductive logic programming singular value decomposition especially text known latent semantic indexing :10.1.1.31.1630
association rules association rules earliest data mining algorithms 
dataset support confidence step algorithm find frequent sets subsets attributes appears rows dataset 
rules form computed frequent sets fa dg provided sufficient confidence support dg support cg algorithms insight set frequent subsets frequent 
frequent sets computed alternately generating candidate sets certain size checking candidates fact frequent pass dataset surviving candidates generate candidates size greater 
algorithm goes phases computing candidate set size pruning pass dataset 
output association rule algorithm set rules capturing information certain patterns attributes occur attributes customer transactions 
example transparent algorithm 
neural networks contrast neural network data mining algorithms opaque 
result training neural network black box capable answering interesting questions person candidate mortgage explaining gives answers 
neural net consists layers units sum inputs transmit output weighted sum exceeds threshold 
different possible arrangements assume general output node layer connected inputs nodes layer edge associated weight 
neural networks trained presenting row dataset inputs comparing resulting net output desired difference error propagated back network altering internal weights 
variants 
dataset usually fed network times called epoch 
genetic algorithms genetic algorithms find concepts computational analogy darwinian evolution 
initial population concepts generated randomly 
fitness concept evaluated describes dataset 
concepts survive sufficiently fit remain concept set replicated fitness allowed mutate randomly allowed crossover exchanging parts substructure 
new concept set evaluated fitness process repeats 
strength genetic algorithms dependent problem structure weakness hard know process 
practice algorithms little change concept set fixed number iterations 
costing sequential data mining algorithms algorithms described previous section property global structure loop building accurate detailed concepts previous iterations 
suppose loop executes times generates oe concepts 
describe sequential complexity algorithm formula cost step nm oe access nm step gives cost single iteration loop access cost accessing dataset 
step depends oe nm possible set derived concepts larger input 
parallel complexity construct similar cost expressions parallelization strategies discussed 
complexity independent search independent search strategy straightforward partition data execute algorithm times randomization technique direct different part search space concepts 
genetic algorithms independent search approach requires running multiple copies sequential algorithm different random starting chromosomes 
best description selected 
computational equivalent evolution fill equivalent niches 
cost independent search strategy algorithm form cost step nm oe access nm oe number iterations program step access costs algorithm data accesses cost sharing answers processors oe cost computing best solution 
clear approach sense reason expect characteristic searching single optimum 
complexity parallelized algorithms basic structure algorithms ffl partition initial concepts subsets 
ffl repeat execute special variant data mining algorithm entire dataset segment current partial set concepts derive new partial set concepts 
exchange partial concepts processors deleting concepts globally correct 
genetic algorithms partial concept parallelization approach divides chromosomes pieces assesses fitness piece relevant features columns dataset 
total fitness chromosome depends fitness pieces 
association rules partial concept parallelization approach called data distribution technique 
possible frequent sets partitioned processors entire dataset examined processor 
practical reasons dataset usually divided pieces circulated processor turn 
partitioned concept parallelization approach called candidate distribution investigated 
cost parallelized algorithm form cost special access exch res concept set partitioned cost special access exch res concepts partitioned 
special complexity single step special algorithm exch cost exchanging partial concepts res cost resolving partial concepts partial concept set consistent set 
complexity replicated algorithms basic structure algorithms ffl partition data subsets processor 
ffl repeat execute variant sequential algorithm subset 
exchange information processor learned 
frequent set part association rule computation means partition dataset processors compute candidate sets locally measure support local partition exchange support values compute total support candidate repeat 
notice processor keeps candidate sets replicates computation new candidate sets old volume data exchanged small integers 
genetic algorithm replicated implementation similar structure 
processor subset dataset full set current concepts 
measures local fitness concepts partition exchanges data processors compute global fitness 
generation concepts computed universally known fitness requiring small data exchanges crossover permitted processors 
cost replicated algorithm form cost step nm access nm rpg res rp number iterations required parallel algorithm size data approximate concepts generated processor rpg cost total exchange processors approximate concepts res rp computation cost approximations compute better approximations iteration 
reasonable assume step nm step nm access nm access nm get cost cost rpg res rp words get fold speedup overhead term provided comparable size 
exchanging results frequently improves rate concepts derived algorithms gives double speedup cost cost overhead discuss phenomenon section 
comparisons relative costs different performance improvement strategies depends ffl number iterations basic loop structure relative size ks ffl amount data accessed iteration ffl amount communication take place processors 
hard dogmatic number iterations required general 
algorithms case may choose parallelized replicated algorithms basis 
relative performance independent search sensitive problem structure minimization problems may smaller ks problems may data access independent search suffers drawback entire dataset accessed processor 
parallelized algorithms potential require access rows columns nm access replicated algorithms require 
parallelized algorithms known require data usually nm values 
parallelized algorithms expensive 
communication independent search clear winner single global communication required determine best solution 
parallelized replicated algorithms communicate phase 
parallelized algorithms tend transmit parts concepts replicated algorithms tend transmit facts concepts smaller 
reason resolution concepts required replicated algorithms typically parallelized algorithms 
possible say strategy best strong tendency replicated strategies better parallelized strategies 
high level cost expressions strategy easily instantiated particular data mining techniques give refined basis decision 
example strategies implemented computing association rules 
replicated approach called count distribution 
shown outperform techniques provided candidate sets kept memory data distribution candidate set partitioned dataset circulated processors candidate distribution candidate set dataset partitioned 
reasons exactly cost expressions plain poorer techniques require greater data access communication count distribution 
frequent set calculation cases intermediate results larger dataset frequent sets elements powerset attributes 
scalability issue 
count distribution drawback requires candidates stored processor 
section illustrate cost expressions doing detailed analysis relative costs neural net training different performance improvement techniques 
detailed example neural networks cost expressions concrete turn neural networks display detailed cost expressions compare 
clear replicated approaches effective data mining algorithms general characteristics 
consider neural network layers neurons layer full connections layer preceding layers 
number weights connection lm number examples dataset replicated approach learning exemplar parallelism processor trains identical initial network pth fraction examples 
epoch rows dataset processed processor processors exchange error vectors combine deterministic learning 
continues sufficient epochs ensure convergence 
cost single training epoch cep aw gamma constant depends particular training algorithm 
term cost computation constant number weight adjustments row dataset term size nw divided equally processors 
second term cost communication total exchange sets errors weight processors 
parallelized approaches processor responsible subset neurons 
natural starting place assume processor responsible rectangular block neurons 
simple analysis possibility clear blocks full width depth neural network save communication 
cost network divided layers layer parallelism lp gamma gamma aw mg single epoch 
network divided columns column parallelism cost ccp aw gamma gamma layer parallelism form concept set partitioning column parallelism direct concept partitioning processor responsible subset inputs 
notice computation term magnitude approaches exemplar parallelism added term arises initial filling emptying pipeline layers 
communication term contains factor size far biggest term data mining applications 
result techniques perform poorly quite small datasets 
alternative approach allocate neurons randomly processors 
surprisingly approach worse 
cost cnp gamma gamma aw argument information theoretic sensitive variations precise neural net training technique architectural variations 
exception architectures assumption communication costs hold 
example kumar shekhar amin give neural net parallelization exploits carefully scheduled point point communication mesh faster layer parallelism expression suggest 
techniques require special purpose hardware shelf hardware norm today 
proposals neural net parallelization finer partitioning including partitioning edge set processors 
clear formulae increase cost communication 
double speedup interesting phenomenon double speedup occurs exemplar parallel training neural nets 
processor learns condensed way processor learned data communication phases take place 
information effect accelerating learning convergence 
effect smaller turn leads double speedup 
described exemplar parallelism far deterministic learning error vector generated entire dataset seen processor 
alternative batch learning error vectors computed exchanged subset called batch entire dataset processed 
notice processor sees pth fraction batch 
batch sufficiently large error vectors computed processor approximations deterministic error vector required processing discover 
result processor progress processors processing batch 
number epochs required achieve level convergence function batch size shape shown batch size number epochs batch sizes get smaller total number epochs required convergence gets smaller processor gets error information frequently 
batch size reaches size bound error information accurate helpful effect disappears 
general behavior verified experimentally number datasets 
batch size number batches epoch 
total cost exemplar parallelism training epochs total ep gamma wg range convergence depends linearly write constant get total ep gamma wg minimizing cost requires minimizing computation term making large possible value denominator speedups significant small 
note follow application exhibits double speedup necessarily solvable sampling 
information shared processors improves completion may accurate need helpful 
character hint answer hints know reduce searches dramatically 
best way reap benefits performance parallel computers data mining problems obvious 
implementing different parallel variants data mining algorithm benchmarking expensive unattractive way decide approach best 
shown cost measures counting computations data accesses communication allow algorithms compared 
cost measures expected completely accurate expressive rule possibilities 
developed cost expressions general implementation strategies suggested replication simplest best performing 
illustrated general cost expressions instantiated particular data mining techniques example neural networks 
clear replicated implementation outperforms parallelized techniques 
agrawal shafer 
parallel mining association rules design implementation experience 
technical report rj ibm research report february 
berry dumais brien 
linear algebra intelligent information retrieval 
siam review 
bishop 
neural networks pattern recognition 
oxford university press 
goldberg 
genetic algorithms search optimization machine learning 
addison wesley 
jonathan hill stephen donaldson david skillicorn 
stability communication performance practice cray networks workstations 
technical report prg tr oxford university computing laboratory october 
kumar shekhar amin 
highly parallel formulation backpropagation hypercubes 
ieee transactions parallel distributed systems october 
muggleton :10.1.1.31.1630
inverse entailment progol 
new generation computing systems 
quinlan 
programs machine learning 
morgan kaufmann 
rogers 
data mining parallel neural networks bulk synchronous parallelism optimality batch learning 
master thesis department computing information science queen university 
rogers skillicorn 
bsp cost model optimal parallel neural network training 
generation computer systems 
skillicorn hill mccoll 
questions answers bsp 
scientific programming 
toivonen 
discovery frequent patterns large data collections 
technical report department computer science university helsinki 
top list 
supercomputing november 
www netlib org benchmark top html 

