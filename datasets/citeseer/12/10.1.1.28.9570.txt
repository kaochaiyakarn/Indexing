technical report ml tr department computer science rutgers university august effect class distribution classifier learning empirical study gary weiss att com labs road piscataway nj usa foster provost stern nyu edu new york university stern school business th st new york ny usa article analyze effect class distribution classifier learning 
describing different ways class distribution affects learning affects evaluation learned classifiers 
results comprehensive experimental studies 
study compares performance classifiers generated unbalanced data sets performance classifiers generated balanced versions data sets 
comparison allows isolate quantify effect training set class distribution learning contrast performance classifiers minority majority classes 
second study assesses distribution best training respect performance measures classification accuracy area roc curve auc 
tacit assumption research classifier induction class distribution training data match natural distribution data 
study shows naturally occurring class distribution best learning substantially better performance obtained different class distribution 
understanding classifier performance affected class distribution help practitioners choose training data real world situations number training examples limited due computational costs costs associated preparing data 

machine learning data mining methods acceptance methods advanced point commonly applied large real world problems 
addressing real world problems focused attention research problems rarely considered 
example research papers workshops directed problems learning data sets unbalanced class distributions costs misclassifying examples non uniform 
order acceptance methods grow research continue address practical concerns arise dealing real world data sets 
research motivated fact obtaining data form suitable learning costly learning large data sets may costly 
costs associated creating useful data set include cost obtaining data cleaning data transporting storing data labeling data transforming raw data form suitable learning 
costs associated learning data involve cost computer hard ware cost associated time takes learn data opportunity cost associated able learn extremely large data sets due limited com weiss provost putational resources 
comprehensive list costs associated inductive learning provided turney 
costs necessary limit size training set 
consequence common question asked start data mining project data records best class distribution 
minimize impact limited training set size classifier performance essential training data chosen carefully 
important choice proper class distribution training set 
tacit assumption machine learning research naturally occurring class distribution best learning 
assumption coming increased scrutiny partly data sets learned high degree class imbalance 
cases learning naturally occurring class distribution produces classifiers perform poorly minority class examples 
simple example illustrates choosing class distribution carefully important 
imagine exist examples training belong class belong class limit training set examples eliminate need clean store records reduce learning time dramatically 
limited training set contain class examples class examples matches natural distribution 
answer depends particular domain results show answer 
article describe analyze results comprehensive set experiments designed investigate effect class distribution training set classifier performance 
evaluate classifier performance performance measures show cases naturally occurring class distribution best learning consequently training set size needs restricted class distribution natural class distribution chosen 
characterize optimal class distribution relates naturally occurring class distribution answer number basic questions class distribution training set affects learning 

background section important background information 
section describes related relationship research 
section answers fundamental questions concerning effect class distribution learning 
section describes adjust classifier compensate training distribution naturally occurring class distribution 
related research investigates effect class distribution learning size training set limited due cost concerns 
investigate question training examples permitted best class distribution training 
little existing research addresses specific question 
studies catlett chan stolfo examined aspects relationship training class distribution classifier performance training set size held fixed quite limited scope 
studies analyzed data sets difficult draw general focused attention issues attempting provide thorough study effect class distribution classifier learning 
furthermore studies adjust induced classifiers take account changes training effect class distribution classifier learning empirical study set class distribution 
improperly biases classifier show significantly reduce accuracy classifier 
research analyze data sets adjust induced classifiers account changes class distribution 
little research investigates effect class distribution classifier performance training set size limited considerable research related topic build classifiers class distribution data highly unbalanced costly misclassify minority class examples japkowicz 
conditions classifiers optimize accuracy tend generate trivial models predict majority class 
research handling highly unbalanced data sets part focused modifying class distribution training set 
case question best class distribution form classifier performs performance measure accuracy 
clearly research overlap research learning unbalanced data sets deal changing class distribution training set goal improving classifier performance 
concerned situation costs associated training data learning training data important limit size training set 
research dealing unbalanced data sets motivation changing class distribution build effective classifier fixed set examples minimize costs associated acquiring learning data maximize performance respect reduced number examples 
basic methods dealing class imbalance altering class distribution data set sampling eliminates examples majority class oversampling replicates examples minority class breiman kubat matwin 
methods cause class distribution unbalanced 
methods known drawbacks 
sampling throws potentially useful data sampling increases size training set time build classifier 
furthermore sampling typically exact copies minority class examples overfitting occur leaves rules appear perform may induced cover replicated example 
research focused improving basic methods 
kubat matwin developed sampling strategy intelligently removes majority examples removing majority examples redundant border minority examples figuring may result noise 
chawla 
combine sampling oversampling methods sampling replicating minority class examples form new minority class examples interpolating minority class examples lie close 
avoid overfitting problem cause decision boundaries minority class spread majority class space 
clearly techniques especially sampling techniques reduce training set size minimizing impact classifier performance 
existing research reduces training set size sampling generally motivated desire reduce training set size 
example research kubat matwin motivates sampling adding examples majority class training set detrimental effect learner behavior 
chan stolfo take slightly different approach learning unbalanced data set 
run preliminary experiments determine best class distribution learning respect specific cost function generate multiple training sets class distribution 
accomplished cases including minority class weiss provost examples majority class examples training set 
run learning algorithm data sets combine generated classifiers form composite learner 
method ensures available training data example training sets 
changing class distribution way improve classifier performance learning unbalanced data sets 
data sets highly unbalanced class distribution cost misclassifying minority class positive example typically greater cost misclassifying majority class negative example 
consequently cost sensitive learning methods form classifiers minimize cost error rate handle unbalanced data sets factoring costs 
cost sensitive learning methods appear direct appropriate method dealing class imbalance artificially modifying class distribution sampling sampling cost acquiring learning data issue 
quote cost sensitive learning papers considers question data available produce tree throwing away information learning speed degraded due duplicate instances drummond holte page 
article analyze effect class distribution learning generally just unbalanced data sets 
analysis focus situation training set size limited 
experiments choose training set sizes small achieve desired class distributions solely removing examples belonging minority class majority class 
instance duplicate examples select examples criteria class value 
understanding role class distribution learning provide qualitative description class distribution influences learning answering basic questions classifiers perform worse minority class majority class training distribution better learning 
addressing question answer general question classifiers perform differently minority majority classes 
note article consider class problems minority class considered positive class 
classifiers perform worse minority class 
conventional wisdom classifiers tend perform worse minority class majority class 
experimental results section show conventional wisdom justified observations 
observation classification rules decision tree leaves predict minority class tend higher error rate predict majority class 
second observation test examples belonging minority class misclassified test examples belonging majority class 
note observations different fact opposite poor performing rule predicts minority class frequently classifies majority class test examples belonging minority class degrades classification performance test examples belonging majority class 
give reasons observed behavior 
reasons provide basic needed understanding class distribution affects classifier performance 
classification rules perform worse majority labeled counterparts part simple subtle overlooked reason test set contains majority class effect class distribution classifier learning empirical study examples minority class examples 
test distribution effect equal classification rules predict minority class perform worse predict majority class 
see imagine randomly generated randomly labeled decision tree evaluated test set classes class ratio test set 
situation leaves predicting minority class expected error rate leaves predicting majority class expected error rate 
second reason minority labeled rules perform poorly provided section show rules generally formed fewer training examples majority labeled rules 
classifiers tend perform worse classifying minority class test examples 
reason due fact class priors marginal probabilities classes natural training distribution biased strongly favor majority class 
learners explicitly factor class priors biases learning process causing majority class predicted 
results improved performance majority class test examples degraded performance minority class test examples 
example decision trees handle feature values observed training data possible leaf decision tree cover training examples 
case learner adopts strategy labeling leaf majority class performance classifier majority class examples improve expense minority class examples 
second reason classifiers perform worse minority class test examples equal classifier fully flesh boundaries minority concept concept space fewer examples class learn 
minority class test examples may classified belonging majority class 
additional examples minority class available training expect minority concept grow include additional regions concept space regions previously sampled 
training distribution better 
article determine empirically best training distributions large number data sets 
section provide insight distribution better 
accomplish extend notion learning curve minority majority classes show varying size training set affects accuracy classifier predicting class value test examples belong classes 
shows learning curves data sets study 
case data sets study learning curve minority class remains learning curve majority class minority class curve starts higher error rate shows rapid improvement 
rapid improvement surprising value training set contains fewer minority class majority class examples generally expects rapid improvement learning fewer examples learning curves steepest near 
weiss provost amount data training error rate minority class majority class learning curves letter vowel data set provides insight improve classifier performance modifying class distribution 
curves generated naturally occurring class distribution ratio majority minority examples class ratio training set size increases majority class examples added minority class example 
letter vowel data set shown class ratio 
assume current training set size corresponds size additional example added training set 
measure evaluate classifier performance equally weights error rates minority class majority class examples example belonging class chosen 
relative improvement error rate occur adding majority class versus minority class example determined comparing slopes curves point 
comparison slope majority class curve divided curves adding majority class examples minority class example 
slope curve clearly times greater slope minority class curve point minority class example added 
measure evaluate classifier performance undifferentiated classification accuracy decision reflect fact accuracy error rate test examples counts case times 
class example added determined simply comparing slopes fact majority class counts times accuracy offset fact majority class examples added minority class example 
accuracy add minority class example training set size corresponding slopes curves 
noted class distribution training set deviates significantly natural class distribution curves longer apply 
curves provide insight changing class distribution training set affect classifier performance 
majority class learning curve plateaus surprising value curve majority class minority class training examples 
initially profitable add majority class examples training effect class distribution classifier learning empirical study set size sufficiently large may profitable start adding examples 
expect optimal training set distribution vary training set size investigate 
correcting changes class distribution training set context research purpose modifying class distribution training set improve performance classifier 
preferentially sampling class expect classifier improve performance classifying test examples class 
want improvement result larger number examples available learning bias result distorted training set class distribution 
classifier induction algorithms assume class distribution test set match training set 
decision tree change class distribution training set result biased posterior class probability estimates leaves 
bias cause preferentially sampled class predicted 
bias improve performance classifier test examples belonging preferentially sampled class bias cause performance classifier suffer show appendix 
experiments decision tree learner 
leaves decision tree typically labeled estimating probability class occurring leaf assigning class leaf 
take difference training set distribution test set distribution account developing probability estimates sensitive differences 
computing new estimates scratch start existing error estimates correct account differences class distributions 
common probability estimates listed table 
represent number minority majority class examples leaf estimate probability seeing minority class example leaf uncorrected versions common assumption training test sets drawn population 
frequency estimate straightforward requires explanation 
estimate perform sample size small defined sample size 
reasons laplace estimate 
consider version laplace law succession 
probability estimate closer frequency estimate difference estimates minimal large sample sizes 
estimate name uncorrected corrected frequency cb laplace law succession cb table probability estimates seeing minority class example corrected versions estimates assume training test sets class distribution 
estimates account differences class distribution factoring ratio minority class majority class examples training set divided ratio naturally occurring class distribution 
example ratio minority majority examples training set test set assuming test set drawn naturally occurring distribution 
classifiers including decision tree learner quinlan article assign labels classification rules value fre weiss provost quency estimate associated rule probability threshold 
note necessary laplace estimate assign class labels move probability estimate threshold 
laplace estimate included table described generate roc curves 
account properly differences training test natural class distributions probability estimates corrected leaves decision tree relabeled corrected estimates probability threshold 
example suppose ratio minority class examples majority class examples naturally occurring class distribution class problem training distribution modified ratio 
case value 
leaf labeled minority class probability greater corrected frequency estimate 
leaf labeled minority class covers times number majority class examples minority class examples general leaf labeled minority class covers times number majority class examples 
note calculating class ratios fraction examples belonging minority class mistakenly example half divided sixth 
class ratios substantially simplifies formulas leads easily understood estimates 
results article corrected frequency estimate label leaves decision tree produced 
uncorrected frequency estimate label decision trees appendix results compared corrected estimate 
comparison shows class distribution training set modified corrected frequency estimate yields classifiers substantially outperform labeled uncorrected version estimate error rate reduced average 
consequently critical take differences class distributions account 
previous modifying class distribution training set catlett chan stolfo japkowicz taken differences account undoubtedly affected results 
code modified utilize corrected estimates 
corrected probabilities calculated leaf labels reassigned post processing step 
approach permit differences class distribution factored tree building process research shown necessary order build classifier 
drummond holte showed decision tree splitting criteria relatively insensitive data set class distribution changes distribution splitting criteria perform better methods factor differences 
differences class distribution factored pruning strategy attempts minimize error rate allowed execute prune false assumptions test distribution matches training distribution 
may negatively affect generated classifier affect study effect class distribution classifier performance results article pruning strategy disabled 
research indicates target misclassification costs class distributions unknown pruning avoided anyway zadrozny elkan bauer ko independent research elkan bayes rule derive formula estimates class membership probabilities changes class distribution training set 
corrected frequency estimate equivalent elkan formula expressed terms fractions 
effect class distribution classifier learning empirical study havi pruning strategy adapted take costs distributions account significantly improve performance classifier bradford 

experimental methodology experiments article program inducing decision trees labeled examples quinlan 
reasons just described run pruning strategy disabled 
order allow generalize results experiments run collection data sets described table 
data sets include data sets uci repository blake merz identified previously published researchers cohen singer 
data sets listed order decreasing class imbalance convention article 
order simplify presentation analysis results data sets classes mapped class problems 
accomplished designating original classes typically frequently occurring class minority class mapping remaining classes new class majority class 
data set originally contained classes identified asterisk table 
letter recognition data set original data set names 
letter data set created letter recognition data set assigning examples labeled letter minority class data set created assigning examples labeled vowel minority class 
minority dataset minority dataset dataset examples size dataset examples size letter network pendigits car abalone german sick euthyroid breast wisc connect optdigits weather solar flare bands letter vowel market crx adult kr vs kp splice junction move network coding yeast table description data sets experiments class distribution training set varied minority class accounts training data 
experimental run training test sets formed follows 
test set formed randomly selecting minority class examples majority class examples original data set replacement 
resulting test set adhere original class distribution 
remaining data available training 
ensure results different class distributions compared fairly training set size required training class distribution 
accomplished setting training set size equal total number minority class examples available training original number 
possible replicating examples generate class distri weiss provost bution training set size training set formed stratified random sampling replacement remaining data desired class distribution achieved 
data set selected study required contain minimum minority class examples order ensure reasonable amount training data 
results averages computed multiple runs 
runs experiments section runs section improve power statistical significance tests 
fixed amount training data different class distributions cause induction algorithm generate different classifiers 
class distribution yield best classifier 
order answer question performance measure chosen 
describe performance measures study 
class problems performance classifier described confusion matrix shown 
performance measures classification accuracy defined tp tn tp fp fn tn equivalently classification error rate defined minus accuracy 
article consistent previous research consider minority class positive class 
actual positive actual negative predict positive true positive tp false positive fp predict negative false negative fn true negative tn consider classification accuracy part common evaluation metric machine learning research 
accuracy performance measure assumes target marginal class distribution known unchanging importantly error costs costs false positive false negative equal 
assumptions unrealistic provost 
accuracy particularly suspect performance measure studying effect class distribution learning heavily biased favor majority class described section 
highly unbalanced problems generally highly non uniform error costs favor minority class class primary interest consider medical diagnosis fraud detection 
classifiers optimize accuracy problems questionable value rarely predict minority class 
alternative method evaluating classifier performance receiver operating characteristic roc analysis swets represents false positive rate axis graph true positive rate axis 
terminology introduced confusion matrix true positive rate defined tp tp fn false positive rate fp fp tn 
roc curves produced varying threshold classification model numeric output case varying threshold class probability estimate leaves decision tree 
laplace estimate shown yield consistent improvements roc curves provost domingos 
point roc curve may correspond case leaves decision tree labeled minority class probability example leaf belonging minority class point curve may correspond probability threshold 
roc analysis machine learning described detail bradley provost fawcett 
purpose primary advantage roc curves evaluate performance classifier independent naturally occurring class distribution error cost 
interesting consequence method described section correct modifications training set class distribution affect roc curves shown appendix affects accuracy substantially 
effect class distribution classifier learning empirical study false positive rate true positive rate minority minority natural minority minority minority auc roc curves letter vowel data set shows roc curves generated letter vowel data set number training examples different training class distribution 
roc analysis classifier better classifier located northwest roc space 
point corresponds strategy making positive minority prediction point predicting positive minority class 
observe different training distributions perform better different areas roc space 
specifically note classifier trained minority class examples performs substantially better classifier trained natural distribution high true positive rates 
knowledge differences performance class distribution shown convincingly analyzed 
important thing note curve generated balanced training set minority class examples outperforms curve associated natural distribution low false positive rates natural distribution performs slightly better 
assess quality classifier measure fraction total area falls roc curve equivalent statistical measures evaluating classification ranking models hand 
area roc curve auc effectively factors performance classifier costs distributions 
larger auc values indicate generally better classifier performance particular indicate better ability rank cases likelihood class membership 
includes auc values curves 
auc values see balanced class distribution generates best classifier maximizes auc 
kept mind specific cost class distributions best model may maximizes auc 
single dominating roc curve multiple classifiers combined form classifier performs optimally costs distributions provost fawcett 
measure classifier performance auc error rate prefer auc measure interested drawing variety data sets true misclassification costs unknown 
believe results auc measure important believe auc provides realistic assessment performance classifier real world problems 
weiss provost 
results learning unbalanced vs balanced data sets assess performance classifiers formed data sets natural class distributions assess classifiers formed balanced versions data sets 
order comparisons fair data set sizes balanced unbalanced versions data set forced equal methodology described section 
coding data set begins balanced class distribution class arbitrarily designated minority class 
analysis affected arbitrary decision average median values displayed tables data points shown figures exclude results data set 
learning unbalanced data sets classifiers generated evaluated data sets naturally occurring class distributions 
results summarized table 
column table identifies data set second column specifies natural class distribution displayed table 
third column specifies percentage total test errors result misclassifying test examples belonging minority class 
results show data sets majority errors come misclassifying minority class examples shown second column minority class typically accounts far fewer half examples 
fourth column specifies number leaves labeled minority majority classes 
results indicate fewer leaves labeled minority class majority class 
minority errors dataset examples min 
min 
maj min 
maj min 
maj min 
maj min 
maj letter pendigits abalone sick euthyroid connect optdigits solar flare letter vowel adult splice junction network yeast network car german breast wisc weather bands market crx kr vs kp move coding average median recall leaves coverage leaf er example er table comparative performance minority majority classes unbalanced data sets effect class distribution classifier learning empirical study fifth column coverage specifies average number training examples minority labeled majority labeled leaf classifies 
results show leaves labeled minority class formed far fewer training examples labeled majority class 
section stated reason minority labeled leaves higher error rate majority labeled leaves test distribution effect having majority class minority class test examples 
table suggests second reason related fact minority labeled leaves tend formed fewer training examples consequence having fewer minority class examples training set 
small disjuncts disjuncts classification rules decision tree leaves cover training examples typically higher error rate large disjuncts holte weiss hirsh 
main reason behavior small disjuncts formed fewer training examples large disjuncts confident probability estimate 
consequently expect rules leaves labeled minority class higher error rate suffer problem small disjuncts 
columns table provide additional class specific information classifiers 
leaf er column specifies error rates leaves labeled minority majority classes performance leaves classifying test examples 
example er column specifies error rates test examples belonging minority majority classes 
column specifies recall classes 
recall measurement commonly information retrieval case represents percentage total minority class majority class test examples respectively correctly classified 
information columns graphically 
minority class majority class leaf er example er recall comparison minority majority class measurements unbalanced data sets results table classifiers perform worse minority class test examples majority class test examples 
data sets coding excluded analysis average error rate minority class test examples majority class test examples 
see leaves labeled minority class average error rate leaves labeled majority class error rate 
average classifiers correctly classify minority class examples majority class examples 
weiss provost results appear contradictory reconciled 
leaves labeled minority class higher error rate leaves labeled majority class equal majority class test examples higher error rate minority class test examples 
described section high error rate minority labeled leaves means majority class test examples misclassified 
reason observe lower error rate majority class test examples majority class predicted far minority class cf 
recall 
expect observed differences error rate test set examples diminish classifier predicted minority class examples predicted majority class examples 
results section address balanced class distribution expect frequency predicting class balanced 
learning balanced data sets experimental results described section training test sets created selecting equal numbers minority class majority class examples 
data sets considered balanced versions ones section 
balancing classes essentially eliminate factors described section tend favor majority class 
note case minority class refers class occurs frequently natural original distribution 
classifiers generated balanced versions data sets described table 
data points black correspond classifiers built data sets majority class formed combining classes described section 
minority errors dataset examples min 
min 
maj min 
maj min 
maj min 
maj min 
maj letter pendigits abalone sick euthyroid connect optdigits solar flare letter vowel adult splice junction network yeast network car german breast wisc weather bands market crx kr vs kp move coding average median leaves leaf er example er recall coverage table comparative performance minority majority classes balanced data sets effect class distribution classifier learning empirical study minority class majority class leaf er example er recall comparison minority majority class error rates balanced data sets comparing table table see expect minority majority classes behave similarly classifiers built balanced data sets 
classifiers built balanced data sets show identical symmetric behavior classes consistent surprising differences 
balanced data sets minority class test examples higher error rate data sets majority class higher recall 
trend observed unbalanced data sets 
data sets leaves predicting minority class lower error rate consistent previous discussion 
table shows generally fewer leaves labeled minor ity class majority class examples occur equal numbers 
summarize results follows learning balanced versions unbalanced data sets induction algorithm generally produces fewer accurate classification rules minority class majority class 
believe differences exist balanced distributions derived naturally unbalanced distributions minority majority classes exhibit fundamental differences 
specifically believe minority classes tend homogeneous set entities majority class corresponds 
example letter data set minority class corresponds letter majority class corresponds letters 
equal numbers examples class expect homogeneous class better learned 
note differences attributed fact data sets majority class formed multiple distinct classes 
comparing black versus white data points see classifiers built naturally occurring class problems white points exhibit consistent differences majority minority classes 

results effect training set class distribution classifier performance goal assess fixed training set size extent different training set class distributions affect classifier performance classifiers generated weiss provost natural class distribution compare built class distributions 
order complete assessment vary training set class distributions data sets described section distribution examples 
particular evaluate twelve minority class distributions data sets 

data set additionally evaluate performance naturally occurring class distribution 
methodology determining optimum training class distribution sections experimental results measure classifier performance terms error rate auc respectively 
sections address issues affect ability determine optimal training set class distribution 
evaluate possible class distribution determine optimal class distribution limited determining best distributions sampled 
concern issues statistical significance generate classifiers training distributions issue multiple comparisons jensen cohen 
issues necessarily conclude training distribution yields best performing classifiers truly best training 
take steps address issues statistical significance multiple comparisons 
enhance ability identify true differences classifier performance respect changes class distribution experimental results section runs runs section 
perform statistical significance testing described shortly 
trying determine optimal class distribution adopt conservative approach try identify optimal range class distributions 
confident range includes optimal class distribution 
expect classifier performance unimodal respect changing class distribution 
specific induction algorithm data set training set size believe optimal class distribution exists move away optimal distribution direction classifier performance degrade progressively 
results show expectation met error rate performance measure generally true auc 
unimodal distribution allows confidently compare points best point chances slim unimodal distribution observed differences points weren truly significant 
remainder section describe determine optimal range class distributions 
identify data set class distribution yields classifiers perform best runs lowest average error rate greatest average auc 
perform tests compare performance classifiers classifiers generated twelve class distributions tests data points 
test yields probability conclude best distribution statistically different distribution confident conclude truly perform differently group distributions 
grouped distributions collectively form optimum range range class distribution values confident true optimum class distribution falls caveat respect evaluated distributions 
things interested optimum range includes natural distribution 
effect class distribution classifier learning empirical study effect class distribution error rate data sets class distributions previously mentioned formed classifiers built distributions 
results displayed table 
table warrants explanation data set listed letter example 
column specifies data set name 
columns error rate values class distributions evaluated 
columns presents error rate values natural distribution natural distribution listed table 
value corresponding lowest error rate data set indicated underlining displaying boldface 
relative position natural distribution range evaluated class distributions denoted vertical bar columns 
letter data set lowest error rate occurs training set contains minority class examples 
vertical bar indicates natural distribution falls distributions 
error rate values significantly different statistically best value yield test value shaded 
letter data set optimum range goes includes natural distribution note error rate shaded 
columns column specifies test probability value computed best distribution natural distribution 
value confident distributions truly perform differently case probability displayed bold indicating natural distribution significantly different best distribution part optimum range 
column measures relative improvement error rate occurs learning best distribution natural distribution 
compute values cases differences statistically significant test probability value specified previous column cases relative improvement displayed bold 
test relative prob 
improv 
dataset natural best nat 
best nat 
letter pendigits abalone sick euthyroid connect optdigits solar flare letter vowel adult splice junction network yeast network car german breast wisc weather bands market crx kr vs kp move coding error rate specified training distribution expressed minority table effect training set class distribution error rate weiss provost results table show data sets confident natural distribution range optimal class distributions 
furthermore data sets best distribution natural distribution yields remarkably large decrease error rate 
feel sufficient evidence conclude accuracy safe assume natural distribution training 
inspection error rate results table shows best distribution differ natural distribution consistent manner includes minority class examples optdigits car fewer connect solar flare adult german weather 
clear data sets substantial amount class imbalance ones top half table class distribution best class distribution training minimize undifferentiated error rate 
examined error rate values remaining data sets test results permit conclude best observed distribution truly outperforms natural distribution 
cases see error rate values training set class distributions form perfectly unimodal distribution 
suggests adjacent class distributions may produce classifiers truly perform differently statistical testing sufficiently powerful 
shows behavior learned classifiers adult car optdigits data sets visual form 
natural distribution denoted enlarged tick mark 
error rate point noted marker error rate best distribution specified corresponding data point 
range error rate values displayed limited consequently data points adult curve omitted 
minority class error rate optdigits car adult effect class distribution error rate select data sets note curves perfectly unimodal 
clear near distribution minimizes error rate changes class distribution yield modest changes error rate far dramatic changes occur 
evident data sets table 
convenient property common goal minimizing error rate 
prop effect class distribution classifier learning empirical study erty far evident correction described section performed classifiers induced class distributions deviating naturally occurring distribution improperly biased 
effect class distribution auc results analogous ones displayed table table auc 
recall auc larger values indicate improved performance 
relative improvement classifier performance specified column terms area roc curve auc terms area curve auc 
include may accurately reflect relative improvement just table specify relative improvement terms change error rate change accuracy 
general optimum ranges appear centered right class distribution class distribution contains minority class examples optimum range data sets 
data sets optimum range include natural distribution data sets natural distribution contains fewer minority class examples class distributions optimum range exception solar flare appears anomalous value 
conclude strongly cost sensitive classification ranking appropriate simply choose natural class distribution training 
test prob 
dataset natural best nat letter pendigits abalone sick euthyroid connect optdigits solar flare letter vowel adult splice junction network yeast network car german breast wisc weather bands market crx kr vs kp move coding expressed minority auc specified training distribution relative improvement table effect training set class distribution auc example change error rate represents reduction error rate equivalent increase accuracy just slightly greater 
weiss provost powerful results particularly surprising 
error rate auc affected test set class distribution factors classifier performance class distributions expect evenly balanced distribution gen outperform natural distribution 
results indicate optimal range generally shifted evenly balanced distribution data sets includes distribution comprising minority class examples 
optimum ranges shown table broader table 
changing class distribution generally improves classifier performance class generally degrades performance class 
auc factors performance class distributions costs believe susceptible changes class distribution 
costs improving performance class expense yield better classifier performance changes performance tend cancel yielding wider optimum ranges 
shows class distribution affects auc data sets 
natural distribution indicated large diamond tick mark corresponding auc value distribution displayed maximum auc value shown corresponding data point 
minority class training set auc letter adult car effect class distribution auc select data sets discussion section provided reasons training distribution may better learning 
apply reasoning explain results section 
impractical analyze data sets restrict attention data sets exhibit interesting different behavior 
tables rows optdigits data set show accuracy auc optimal training distribution contains far minority class examples majority class examples 
contrast rows data set show accuracy optimal distribution contains far fewer minority class examples auc contains minority class examples 
effect class distribution classifier learning empirical study learning curves data sets shown figures 
data sets data sets learning curve minority class top curve middle majority class learning curve bottom 
test examples belonging minority class higher error rate belonging majority class 
described section training set sizes experiments vary class distributions set equal number minority class examples sizes optdigits data sets correspond value frp vo avt hq avt uh ur minority class learning curve optdigits data set shows dramatic improvement data set shows slight improvement 
amount data training error rate minority class majority class learning curve optdigits dataset training error rate minority class majority class learning curve dramatic improvement minority class learning curve optdigits data set occur majority class learning curve explains error rate optimal training distribution minority includes far minority examples natural distribution minority 
data set improvement shown value slight smaller values appears majority class learning curve shows improvement 
consequently learning curves help explain optimal distribution minority includes far fewer minority examples natural distribution minority 
section described method correcting classifier compensate changes class distribution training set 
utilize correction previous research catlett chan stolfo japkowicz 
order demonstrate quantify importance correction error rates result correction appendix results show employing corrected frequency estimate error rate reduced remains data sets error rate reduced average 
data sets class ratio greater error rate reduced average 
conclude correction essential class distribution training set altered 

limitations article demonstrated performance measures error rate auc fixed number training examples naturally occurring class distribution produce best performing classifier 
furthermore shown auc weiss provost formance measure measure believe appropriate error rate optimal distribution generally contains minority class examples 
situation strategy allocating half training examples minority class yield optimal results generally lead results worse superior natural class distribution 
know true misclassification costs unwilling determine experimentally optimal training distribution suggest maximizing auc training set formed equal numbers examples class 
article presents empirical study effect class distribution classifier learning 
goal article find optimal class distribution efficiently 
practice cost effective data incrementally suggest progressive adaptive sampling strategy provost jensen oates developed incrementally requests new examples improvement classifier performance due added minority class majority class examples 
best class distribution estimated cross validation 
results decision tree learner 
believe hold learners believe effect training class distribution classifier performance specific decision tree learners 
particular believe reasons provided section classifiers perform differently minority class versus majority class apply learners 
faced complex learning problems involve highly unbalanced data sets practitioners modify class distribution training set 
modifications seldom done principled manner reasons changing distribution fully understood 
article provides deeper understanding class distribution affects learning discusses length issues involved class distribution modified 
particular provide explanations minority class generally higher error rate majority class compare performances classifiers generated balanced unbalanced data sets 
demonstrate changing class distribution training set change distribution errors minority class majority class 
areas addressed article learning algorithm modified account changes training distribution resulting classifier unduly biased 
quantify performance penalty occurs change class distribution properly accounted show penalty accuracy quite substantial 
correction suggest ignored research practice 
hope results help researchers practitioners understand better relationship training class distribution classifier performance learn effectively large data sets situations training set size limited 
acknowledgments brian davison chris matthew stone comments article haym hirsh comments feedback provided research 
ibm faculty partnership award 
effect class distribution classifier learning empirical study bauer kohavi 

empirical comparison voting classification algorithms bagging boosting variants 
machine learning 
breiman friedman olshen stone 

classification regression trees 
belmont ca wadsworth international group 
blake merz 

uci repository machine learning databases www ics uci edu mlearn mlrepository html department computer science university california 
bradley 

area roc curve evaluation machine learning algorithms 
pattern recognition 
bradford kunz kohavi brunk brodley 

pruning decision trees misclassification costs 
proceedings european conference machine learning pp 

catlett 

machine learning large databases 
ph thesis department computer science university sydney 
chan stolfo 

scalable learning non uniform class cost distributions case study credit card fraud detection 
proceedings fourth international conference knowledge discovery data mining pp 
menlo park ca aaai press 
chawla bowyer hall kegelmeyer 

smote synthetic minority sampling technique 
international conference knowledge computer systems 
cohen singer 

simple fast effective rule learner 
proceedings sixteenth national conference artificial intelligence pp 
menlo park ca aaai press 
drummond holte 

exploiting cost sensitivity decision tree splitting criteria 
proceedings seventeenth international conference machine learning pp 

elkan 

foundations cost sensitive learning 
proceedings seventeenth international joint conference artificial intelligence 
published 


estimation probabilities 
cambridge ma press 
hand 

construction assessment classification rules 
chichester uk john wiley sons 
holte acker porter 

concept learning problem small disjuncts 
proceedings eleventh international joint conference artificial intelligence pp 

san mateo ca morgan kaufmann 
japkowicz 

learning imbalanced data sets comparison various strategies 
papers aaai workshop learning imbalanced data sets 
tech 
rep ws menlo park ca aaai press 
weiss provost japkowicz holte ling matwin 
eds 

papers aaai workshop learning imbalanced data sets 
tech rep ws menlo park ca aaai press 
remove cite 
jensen cohen 

multiple comparisons induction algorithms 
machine learning 
kubat matwin 

addressing curse imbalanced training sets sided selection 
proceedings fourteenth international conference machine learning pp 

provost fawcett kohavi 

case accuracy estimation comparing classifiers 
proceedings fifteenth international conference machine learning 
san francisco ca morgan kaufmann 
provost jensen oates 

efficient progressive sampling 
proceedings fifth international conference knowledge discovery data mining 
acm press 
provost fawcett 
robust classification imprecise environments 
machine learning 
provost domingos 

trained pets improving probability estimation trees 
working stern school business new york university new york ny 
quinlan 

programs machine learning 
san mateo ca morgan kaufmann 
swets monahan 

better decisions science 
scientific american october 
turney 

types cost inductive learning 
workshop cost sensitive learning seventeenth international conference machine learning stanford ca 
weiss hirsh 

quantitative study small disjuncts proceedings seventeenth national conference artificial intelligence 
menlo park ca aaai press 
zadrozny elkan 

learning making decisions costs probabilities unknown 
tech 
rep cs department computer science engineering university california san diego 
effect class distribution classifier learning empirical study appendix importance adjusting classifier compensate changes class distribution training set article relied corrected frequency estimate described section label leaves induced decision tree tree biased alterations class distribution training set 
table compare decision trees labeled corrected frequency estimate ct fb uncorrected version fb 
comparison training distribution modified contain equal number minority class majority class examples test distribution uses natural distribution 
data sets listed order decreasing class imbalance 
table second column lists error rates uncorrected corrected frequency estimates lowest error rate data set underlined 
third column specifies relative improvement reduction error rate results corrected frequency estimate uncorrected version 
fourth specifies percentage leaves corrected estimate leads different class value uncorrected version 
column specifies percentage total errors contributed minority class test examples 
rel 
labels dataset fb ct fb improv 
changed fb ct fb letter pendigits abalone sick euthyroid connect optdigits solar flare letter vowel adult splice junction network yeast network car german breast wisc weather bands market crx kr vs kp move coding average median error rate min 
errs table impact training distribution correction error rate weiss provost table shows employing corrected frequency estimate uncorrected frequency estimate error rate classifiers reduced average 
furthermore note classifiers labeled corrected version frequency estimate equal lower error rate data sets 
correction tends yield larger reduction highly unbalanced data sets plays larger role causes greater percentage leaves change class value 
restrict data sets class ratio greater relative improvement data sets 
corrected frequency estimate reduce number leaves decision tree labeled minority class 
consequently column table demonstrates corrected version estimate cause errors come minority class test examples 
