copyright scott frederick kaplan compressed caching modern virtual memory simulation scott frederick kaplan dissertation faculty graduate school university texas austin partial fulfillment requirements degree doctor philosophy university texas austin december compressed caching modern virtual memory simulation approved dissertation committee acknowledgments paul wilson served advisor semester 
enthusiasm material taught great deal think analyze create 
treated peer time graduate student helping career 
number graduate students worked 
graduate experience background richer provided perspective helped goals 
yannis smaragdakis played significant role research effort intelligence energy invaluable 
important note chapters derived papers published paul wilson yannis smaragdakis 
significant hand developing 
mark johnstone kakkad doug van gold involved related research interaction kept interest high long process 
emery berger supportive included life showed enthusiasm managed accomplish 
implicitly reminded value interaction kind 
mike seen graduate process seen grow person critical years 
lucky friend 
department computer sciences come halt 
course am referring administrative staff 
particular katherine help finished dissertation 
want boyd patti spencer system administrators maintain daunting network 
want extend special bill pugh department computer science university maryland 
moved austin needed place continue surrounded interested 
kind provide access resources 
imagine position constant support love parents family 
taught value education ensured opportunity encouraged 
supported unsure showed pride done 
friends enjoyed sharing experience 
surprise ee 
encouraged love 
supported writing dissertation significant transitions 
best friend advocate 
enjoy am 
scott frederick kaplan university texas austin december vi compressed caching modern virtual memory simulation publication 
scott frederick kaplan ph university texas austin supervisors paul wilson donald cpu speeds increased quickly programs suffer lack cpu capacity 
programs domains computing hindered slowness memory level virtual memory hierarchy 
greatest pitfalls system performance virtual memory paging 
improving performance part virtual memory hierarchy essential 
understanding program behavior relation virtual memory design performance significantly advanced past years 
ideas improve virtual memories evaluated direct implementation part operating system kernel 
unfortunately approach allow controlled reproducible experiments help researcher understand programs memory 
trace driven simulation allows kinds controlled scientific experiments 
difficulties gathering storing processing large vii traces kept researchers adopting approach 
dissertation begins outlining approach reducing traces simulations virtual memory 
traces explore compressed caching insertion new compressed level ram memory hierarchy 
trace driven investigations compression memory data 
new principled approach making page replacement compressed caching adapt program behavior possible controlled simulations 
viii contents acknowledgments vii list tables xiv list figures xvi chapter motivation 
study virtual memory 
study compressed caching 
simulation 
virtual memory background 
brief history 
current 
trace driven simulation 
problem virtual memory simulation 
simulation 
new ideas 
behavior page replacement 
ix brief history 
new ideas 
compressed caching 
revisiting idea 
new ideas 
road map 
chapter concepts caching page replacement timescale relativity 
line caching prediction problem 
temporal spatial locality 
known replacement policies 
timescale relativity virtual memory time timescale relativity ii aggregate behavior 
timescale relativity iii adaptivity decay 
lru approximations real systems 
compression 
background 
modeling 
encoding 
compressibility vs caching 
chapter trace reduction 
background motivation 
overview related 
value techniques 
algorithms 
safely allowed drop sad 
optimal lru reduction olr 
trace manipulation issues 
experimental results 
reduction results 
clock segq simulations 
comparison glass cao technique 

chapter adaptive caching overview 
motivation related 
eelru algorithm 
general idea 
concrete algorithm 
eelru vs lru 
experimental assessment 
settings methodology 
locality analysis 
experiment memory intensive applications 
second experiment small scale patterns 
thoughts program behavior replacement algorithms 

chapter memory data compression motivation background 
compression compressed caching 
wilson kaplan wk compressors 
xi modeling encoding phase 
packing phase 
experimental evaluation 
experimental design 
results 
related 
original wk newton 
match 
zero removal 
parallel cooperative dictionary compression 
chapter compressed caching 
calculating costs benefits 
example 
generalization derivation 
calculations real programs 
fixed size compressed cache simulations 
experimental design 
results 
adaptively adjusting compressed cache size 
online cost benefit analysis 
adapting behavior 
size adaptive compressed cache simulations 
results detailed simulations 
technology trends 
related 

xii chapter timescale relativity adaptive caching 
trace reduction simulation 
compressed caching 
bibliography vita xiii list tables small reduction memories significant reduction factors achieved 
reduced traces highly compressible 
reduction traces gc 
ratios number events traces reduced olr reduction technique glass cao glca shown 
ratio means glca trace times larger 
reduction traces gc 
ratios trace file size traces reduced olr reduction technique glass cao glca shown 
ratio means glca trace times larger 
reduction traces gc 
ratios compressed trace file size traces reduced olr reduction technique glass cao glca shown 
ratio means glca trace times larger 
information traces gc 
xiv processing times seconds program test suite processor study 
memory available paging occurs times turnaround times 
xv list figures traditional virtual memory hierarchy pages fit main memory pages evicted backing store 
compressed caching hierarchy main memory divided uncompressed cache compressed cache 
pages fit uncompressed cache evicted compressed cache turn evicts pages backing store 
sample lru access histogram artificial data 
axis increasing lru position axis number lru queue position 
sample lru histogram artificial data 
axis memory size axis number misses suffered memory size 
decreasing shape access histogram common programs 
note decrease severe log scale needed 
lru access histogram program loops pages memory 
xvi lru histogram program loops pages memory 
lru prediction pages soon exactly wrong prediction 
note number misses decrease memory pages added entire loop held memory 
pages managed segmented queue consider segmented queue comprising fifo lru segments initial state shown 
page cause page taken lru segment inserted head fifo segment 
page extracted tail fifo segment inserted head lru queue 
resulting state segmented queues shown 
page fifo queue cause change segment segmented queue 
example page bring segmented queue state shown identical 
st order markov model symbol source alphabet corresponds node state machine 
weighted edge node represents probability symbol input 
assigning shorter bit representations heavily weighted edges smaller representation input corresponds model 
nd order markov model node represents seen input symbols 
higher order model means responding context means storage requirements significantly larger 
xvii binary tree represents huffman encoding alphabet symbol encoding described path root leaf contains symbol 
traversal edge left child encoded right child 
example path root left left right 
second eliminated lru distance third reduction memory size pages 
olr core accepts sequence fetched evicted pages page lru memory produces shortest trace cause behavior memory 
sad olr reduction factors reduction memory sizes cc 
sad olr reduction factors reduction memory sizes ghostscript 
sad olr reduction factors reduction memory sizes lindsay 
sad olr reduction factors reduction memory sizes acrobat reader 
sad olr reduction factors reduction memory sizes netscape navigator 
sad olr reduction factors reduction memory sizes microsoft word 
absolute percent error number faults introduced clock simulations grobner reduced traces sad sd 
notice reduction memory sizes chosen reduced trace approximately times smaller original 
xviii absolute percent error number faults introduced clock simulations go reduced traces sad sd 
absolute percent error number faults introduced clock simulations cc reduced traces sad sd 
absolute percent error number faults introduced clock simulations ghostscript reduced traces sad sd 
absolute percent error number faults introduced clock simulations acrobat reader reduced traces sad sd 
absolute percent error number faults introduced clock simulations netscape navigator reduced traces sad sd 
absolute percent error number faults introduced segq simulations acrobat reader reduced traces sad sd 
plot fifo segment size fixed axis shows total memory size obtained combining fifo lru segment 
absolute percent error number faults introduced segq simulations cc reduced traces sad sd 
absolute percent error number faults introduced segq simulations go reduced traces sad sd 
absolute percent error number faults introduced segq simulations ghostscript reduced traces sad sd 
absolute percent error number faults introduced segq simulations powerpoint reduced traces sad sd 
xix absolute percent error number faults introduced segq simulations microsoft word reduced traces sad sd 
general eelru scheme lru axis correspondence memory locations 
example recency distribution page touches 
lru pages memory 
evicting early pages stay memory 
pages common example distribution evicting early yields benefits eelru wfl fallback lru axis correspondence memory locations 
probability memory page recency 
recency graph wave 
recency graphs gnuplot 
recency graphs ksim 
recency graphs perl 
recency graphs gcc 
fault plot applu 
fault plot gnuplot 
note seq curve overlaps opt curve 
fault plot ijpeg 
fault plot ksim 
fault plot murphi 
note eelru curve overlaps lru curve 
fault plot perl 
fault plot 
fault plot wave 
xx different fault plot gnuplot 
second simulation extra early eviction point available eelru 
fault plot espresso 
fault plot gcc 
fault plot grobner 
fault plot ghostscript gs 
note eelru line overlaps lru line 
fault plot lindsay 
note eelru line overlaps lru line 
fault plot 
note eelru line overlaps lru line 
wk compression decompression algorithm 
compressor reads source page word time looking zeros exact matches dictionary partial matches dictionary 
stores modeling results simple ad hoc encoding centered bit tag values indicate kind match word 
decompressor reads tags plus necessary bits direct decompression 
maintaining dictionary fashion compressor reconstruct uncompressed page word time 
sixteen consecutive tag entries bits apiece represented pairs alphabetic characters 
uppercase letter bit tag lowercase letter second bit 
zero bits meaningless ones need eliminated packing 
group entries handled machine word 
means compressed sizes paging traffic different lru memory sizes kbyte pages espresso 
xxi standard deviations compressed sizes paging traffic different lru memory sizes kbyte pages espresso 
means compressed sizes paging traffic different lru memory sizes kbyte pages cc 
standard deviations compressed sizes paging traffic different lru memory sizes kbyte pages cc 
means compressed sizes paging traffic different lru memory sizes kbyte pages gnuplot 
standard deviations compressed sizes paging traffic different lru memory sizes kbyte pages gnuplot 
means compressed sizes paging traffic different lru memory sizes kbyte pages grobner 
standard deviations compressed sizes paging traffic different lru memory sizes kbyte pages grobner 
means compressed sizes paging traffic different lru memory sizes kbyte pages ghostscript 
standard deviations compressed sizes paging traffic different lru memory sizes kbyte pages ghostscript means compressed sizes paging traffic different lru memory sizes kbyte pages lindsay 
standard deviations compressed sizes paging traffic different lru memory sizes kbyte pages lindsay 
means compressed sizes paging traffic different lru memory sizes kbyte pages 
standard deviations compressed sizes paging traffic different lru memory sizes kbyte pages 
means compressed sizes paging traffic different lru memory sizes kbyte pages rscheme 
standard deviations compressed sizes paging traffic different lru memory sizes kbyte pages rscheme 
means compression times paging traffic different lru memory sizes kbyte pages grobner 
standard deviations compression times paging traffic different lru memory sizes kbyte pages grobner 
means decompression times paging traffic different lru memory sizes kbyte pages grobner 
standard deviations decompression times paging traffic different lru memory sizes kbyte pages grobner 
means compression times paging traffic different lru memory sizes kbyte pages ghostscript 
standard deviations compression times paging traffic different lru memory sizes kbyte pages ghostscript means decompression times paging traffic different lru memory sizes kbyte pages ghostscript 
standard deviations decompression times paging traffic different lru memory sizes kbyte pages ghostscript means compression times paging traffic different lru memory sizes kbyte pages lindsay 
standard deviations compression times paging traffic different lru memory sizes kbyte pages lindsay 
means decompression times paging traffic different lru memory sizes kbyte pages lindsay 
xxiii standard deviations decompression times paging traffic different lru memory sizes kbyte pages lindsay 
means compression times paging traffic different lru memory sizes kbyte pages rscheme 
standard deviations compression times paging traffic different lru memory sizes kbyte pages rscheme 
means decompression times paging traffic different lru memory sizes kbyte pages rscheme 
standard deviations decompression times paging traffic different lru memory sizes kbyte pages rscheme 
lru histogram provides needed information calculation cost benefit dividing amount ram hold uncompressed compressed cache 
lru histogram labeled derivation cost benefit formulae 
lru histogram pascal compiler 
lru cost benefit performance curve shows total runtime range compressed cache sizes page memory 
lru histogram netscape navigator popular web browser 
lru cost benefit performance curve netscape navigator shows total paging time range compressed cache sizes page memory 
lru histogram benchmark 
actual data provided part curve rest extrapolated lru cost benefit performance results benchmark shows total paging time range compressed cache sizes mbyte memory 
physical memory size paging time espresso compressed cache percentage physical memory 
comparison traditional vm time shows paging time required compressed cache 
physical memory size paging time cc compressed cache percentage physical memory 
comparison traditional vm time shows paging time required compressed cache 
physical memory size paging time grobner compressed cache percentage physical memory 
comparison traditional vm time shows paging time required compressed cache 
physical memory size paging time ghostscript compressed cache percentage physical memory 
comparison traditional vm time shows paging time required compressed cache 
physical memory size paging time lindsay compressed cache percentage physical memory 
comparison traditional vm time shows paging time required compressed cache 
physical memory size paging time compressed cache percentage physical memory 
comparison traditional vm time shows paging time required compressed cache 
results simulating compressed cache size adapted espresso wide range memory sizes 
note axis log scale 
results simulating compressed cache size adapted cc wide range memory sizes 
note axis log scale 
results simulating compressed cache size adapted gnuplot wide range memory sizes 
note axis log scale 
results simulating compressed cache size adapted ghostscript wide range memory sizes 
note axis log scale 
results simulating compressed cache size adapted wide range memory sizes 
note axis log scale 
results simulating compressed cache size adapted rscheme wide range memory sizes 
note axis log scale 
normalized paging times different compression algorithms espresso 
algorithms yield benefits significantly better 
normalized paging times different compression algorithms cc 
algorithms yield benefits significantly better 
normalized paging times different compression algorithms gnuplot 
algorithms yield benefits significantly better 
normalized paging times different compression algorithms ghostscript 
algorithms yield benefits significantly better 
normalized paging times different compression algorithms 
algorithms yield benefits significantly better 
normalized paging times different compression algorithms rscheme 
algorithms yield benefits significantly better 
normalized paging times compressed caching different processors cc wkdm 
normalized paging times compressed caching different processors cc lzo 
normalized paging times compressed caching different processors ghostscript wkdm 
normalized paging times compressed caching different processors ghostscript lzo 
normalized paging times compressed caching different processors rscheme wkdm 
normalized paging times compressed caching different processors rscheme lzo 
sensitivity analysis espresso disks various speeds 
plots interpreted faster disks cases slower cpus perfect prefetching larger page sizes 
sensitivity analysis cc disks various speeds 
plots interpreted faster disks cases slower cpus perfect prefetching larger page sizes 
sensitivity analysis gnuplot disks various speeds 
plots interpreted faster disks cases slower cpus perfect prefetching larger page sizes 
sensitivity analysis ghostscript disks various speeds 
plots interpreted faster disks cases slower cpus perfect prefetching larger page sizes 
sensitivity analysis disks various speeds 
plots interpreted faster disks cases slower cpus perfect prefetching larger page sizes 
sensitivity analysis rscheme disks various speeds 
plots interpreted faster disks cases slower cpus perfect prefetching larger page sizes chapter motivation study virtual memory 
cpu speeds increasing year number years 
memory speeds improving slower rate 
ram able provide data processor rate slow bandwidth bottleneck systems 
system designers inserted smaller faster memory hardware levels ram processor cache main memory data 
performance gap processors memory hardware widens ability manage memory increasingly important 
gap wider gap growing faster processor speed disk speed 
virtual memory systems disks slow large storage program data main memory full 
modern hardware ram provide data orders magnitude faster disk 
disk latency improving year disk store memory program data relatively expensive 
critical virtual memory systems manage program data 
addition new layers memory hardware programs tend written resources available 
virtual memory systems intelligent decisions data keep ram programs page disk storage ram full 
programs spend amount time waiting data fetched disk 
dissertation offers 
seek improve understanding programs memory understanding guide improvements main memory caching improvements scientifically evaluated 
investigate compressed caching approach ram effectively avoid paging 
evaluate idea trace driven simulation experiments controlled repeated 
experimentation help develop better understanding timescale relativity dictates virtual memory systems adapt program behavior 
simulation evaluate new approach page replacement virtual memory system adapts program uses memory 
concepts better defined described rest 
study compressed caching 
disk accesses expensive interested ideas improve utilization main memory 
compressed caching division main memory pools uncompressed cache pages natural uncompressed representation compressed cache pages compressed form 
compressing main memory effective memory size larger disk accesses avoided 
thorough description compressed cache fits memory hierarchy section 
compressed caching idea prematurely discarded 
early evaluations performed implementation real systems yielded ambiguous results 
misconception developed compressed caching useful machines disk palmtop computers 
believe compressed caching provide reduction paging broader range computing hardware workloads 
furthermore believe compressed caching smooth performance degradation occurs programs need memory physically available longer performance painfully poor slightly memory needed machine 
new developments compressed caching 
show compressed caching easily improve paging performance discrepancy processor disk speeds 
introduce new compression algorithms designed memory data part compressed caching system 
learned timescale relativity behavior compressed cache adapt size program needs important factor compressed cache performance 
simulation 
number ideas proposed improve virtual memory performance compressed caching prefetching network caching better page replacement name 
ideas evaluated implementation real systems 
complexity real systems kinds experiments difficult control variables reproduce results 
result ideas prematurely discarded understanding ideas remained rudimentary 
compressed caching idea understood early real system experiments reveal component implementation led poor performance 
simulation virtual memory system possible controlled experiments allow researchers develop understanding correlations 
simulations take input trace log memory locations program execution 
traces tend large particularly augmented extra information simulation rarely 
developed solutions problem trace length simulation desirable approach virtual memory experimentation 
new solutions trace reduction 
order simulation practical developed methods trace reduction specific virtual memory 
reduction methods possible examine complex virtual memory concepts usually handled simulation 
simulation address ideas designed reduce paging designed eelru new page replacement policy adapts program behavior 
second topic compressed caching developing new compression algorithms applying kind adaptivity eelru system adjust compressed cache size runtime 
virtual memory background brief history virtual memory 
concept virtual memory existed years den 
taken forms central concept virtual memory allow force programs refer data abstracted mechanism explicit commands hardware devices 
earliest forms virtual memory single defined idea 
devised central concepts common virtual memory systems 
automatic paging virtual memory implementations divide memory blocks known pages 
previously programmers explicitly controlled movement pages main memory core ram backing store drums disk 
system automatic paging assume complete control decisions 

unsegmented address space manner program refers data defines program address space 
previously memory divided segments requiring programs refer data dimensional address component segment name 
system programmers explicitly create manage segments 
unsegmented address space programs refer data dimensional numerical address 
early virtual memory systems ideas 
example influential multics operating system cv employed automatic paging provided segmented address space addressing abstraction 
today virtual memory systems provide unsegmented address spaces addition automatic paging 
early issues virtual memory 
automatic paging unsegmented address spaces met initially skepticism 
system designers believed programmer intelligence required ensure correct pieces data available main memory right times 
danger making management automatic time wasted disk transfers due poor management 
response skepticism research performed determine programs memory behavior 
behavior determine virtual memory systems provide referencing abstraction automatic management performed introducing unacceptable overhead 
basic performance results studies indicated virtual memory excellent idea 
cases allowing system manage program memory increased performance 
programmers poor decisions regard memory management 
important topics virtual memory management page replacement policies 
main memory full system determine page evicted disk order space needed page 
page replacement policy determines page evicted 
policy reduces number disk accesses choosing evict pages time keeping memory pages soon 
notice choosing requires policy guess page needed soon 
choice demands certain assumptions program pages making page replacement difficult problem 
page replacement approach improving provided section 
current intention virtual memory abstraction provide single simple view program storage runtime 
providing automatic paging programmers encouraged concentrate solutions details memory management 
program memory stored main memory system manage situation 
inevitably performance decrease decrease gradual 
today programmers aware memory management limitations paging 
disk access slow programs hand crafted avoid exceeding main memory capacity 
program page performance degrades drastically leaving user adjust program find program buy ram 
believe virtual memory systems effectively manage data restore intended power abstraction 
addressing performance gap caused increasingly slow disks reduce need programmers write code tailored fit common ram size 
trace driven simulation trace driven simulation log events taken real program execution allow simulator mimic behavior system 
simulator need implement system components studied implementation control simpler real system 
virtual memory simulator trace track movement pages memory level recording costs incurred fetch evict pages 
problem virtual memory simulation greatest obstacles virtual memory simulation size traces 
program executes seconds perform memory require gigabytes store sequence sequence comprises memory addresses 
trace needs store information data timing information values data trace larger 
length traces compressed compression utilities gzip traces tens hundreds megabytes 
worse compression utilities help reduce simulation time proportional length trace 
simulation slow infeasible ex meaningful range memory sizes hardware speeds virtual memory configurations 
simulation 
studies page replacement policies determine select page ram eviction disk memory full utilized trace driven simulation 
studies traces contain sequence pages augmented additional information page modified write operation timestamps 
simulator needs implement page replacement policy memory size performance estimated number fetch eviction operations replacement policy require 
simulation 
complex virtual memory concepts evaluated trace driven simulation 
approach avoided largely traces needed information large 
example studies compressed caching required traces contained actual copies kbyte page existed referenced 
order able gather traces information developed trace reduction methods specific virtual memory studies 
simulation positive improvements real virtual memory system observed provide evidence efficacy idea 
negative ambiguous results little concluded 
real systems complex implementation details may hinder performance experimental system obscuring evidence concept tested may valid 
worse real implementation experiments difficult control 
limiting changes real system isolate effects different variables formidable task 
simulations exactly reproducible small differences execution workload may change outcome significant ways 
hardware needed execute real experimental system may available time experiment available investigators 
simulated experiments virtual memory systems controlled repeated 
provide insight programs memory effect changes isolated components virtual memory system 
understand idea change virtual memory system investigation simulation 
simulations provide position results experimental concept understood verification simulations performed real implementation 
new ideas new methods safely allowed drop sad optimal lru reduction olr 
reduction methods discard information important virtual memory system decisions pages keep main memory evict 
policies discard information little effect virtual memory decisions account vast majority trace length 
traces reduced methods easily stored greatly increase speed simulations performed 
behavior page replacement brief history virtual memory initially greeted skepticism researchers analyzed behavior evaluated virtual memory performance 
stud ies helped develop important concepts principle locality den states small fraction program memory time 
locality exist vast majority programs virtual memory worked 
see locality compressed caching attractive idea 
known replacement policies 
early studies concentrated development comparison page replacement policies 
policies choice page evict evident policy name fifo lru frequently lfu name 
time apparent lru policy performed best practice 
policy performance fully described chapter 
today real systems replacement policies approximations lru 
part early replacement policies included line policy min bel replacement policy commonly known opt 
algorithm cheat examining avoiding poor decisions 
determine smallest possible number page faults sequence serves ideal replacement policies measured 
adapting behavior 
replacement policies extent adaptive act response order program pages 
levels page replacement policy parts virtual memory system react information gathered program behavior 
early example kind adaptivity loop detection 
program performs loop usually access amount memory cyclic pattern 
unfortunately pattern exhibit locality large portion memory equal frequency making difficult replacement policy reduce number main memory misses caching effectively 
response developers ict atlas machine bfh implemented heuristics attempted determine program performing loop 
heuristics perform concept adapting specific common types program behavior important 
understanding behavior virtual memory system successfully detect patterns loops adjust assumptions accordingly 
new ideas able develop concept timescale relativity idea view program behavior appears virtual memory system different depending memory size program 
program behavior kbyte cache quite different mbyte ram 
understanding memory size dictates relevance behavior virtual memory system ignore patterns occur quickly ignored notice patterns occur slowly exploited 
timescale relativity thoroughly defined discussed section 
better understanding concepts able develop recency adaptivity 
virtual memory policies form adaptivity take advantage timescale relativity information provided lru page replacement specifically pages successive page 
information virtual memory system respond features program behavior cause traditional virtual memories suffer performance breakdown 
information allowed detect virtual memory system evict page fetch main memory soon 
detecting situation system avoid 
applied form adaptivity develop early eviction lru eelru family page replacement policies performs lru replacement default perform non lru replacement patterns cause lru perform 
eelru rarely performs worse lru small constant factor 
outperform lru non lru replacement beneficial factor proportional memory size 
compressed caching compressed caching system ram divided levels shown 
ram store uncompressed cache pages normal encoding compressed cache pages stored compressed format 
uncompressed cache full pages evicted compressed cache 
similarly compressed cache full pages evicted disk 
revisiting idea idea initially evaluated implementation sprite operating system kernel dou 
ambiguous results programs exhibited reduction paging costs increase 
author noted results increasingly positive gap cpu disk grew perception years compressed caching flawed concept 
evaluation simulation shown concept valid performance trends soon allow compressed caching help real programs 
understanding compressed caching interacts program behavior developed 
information guide developments allow better real implementation 
standard hierarchy registers cache backing store main memory registers cache proposed hierarchy backing store compressed cache uncompressed cache traditional virtual memory hierarchy pages fit main memory pages evicted backing store 
compressed caching hierarchy main memory divided uncompressed cache compressed cache 
pages fit uncompressed cache evicted compressed cache turn evicts pages backing store 
new ideas poor performance early compressed caching implementations clearly due flawed concept 
evaluating compressed caching simulation able study address design individual components contribute compressed caching 
demands compression algorithm context different compression scheme designed file system data 
able develop wilson kaplan wk family algorithms specifically designed compression memory data held compressed cache 
able perform simulations compressed caching able apply adaptive caching concepts eelru dynamic resizing compressed cache 
ram divided uncompressed compressed pages critical size caches arbitrated carefully 
recency adaptivity possible resize dynamically compressed cache size 
experimental infrastructure concepts able develop results show compressed caching idea benefit real systems 
long real implementation designed avoid excess overhead compressed caching provide reduction paging costs rarely increasing paging costs 
reductions paging cost dramatic gap cpu disk speed increases 
studying virtual memory issues controlled reproducible way obtained positive results show widening gap need dominate performance machines 
road map number concepts surrounding virtual memory program behavior data compression serve background research 
concept pervades topics chapter dedicated entirely developing ideas 
chapter describe timescale relativity recency adaptivity detail show stem useful information lru provides program behavior 
concepts help understand page replacement algorithm minimum improve significantly lru 
chapter describe issues data compression 
area domains file systems human text quite mature thoroughly studied 
trace reduction memory data compression domains research done describe domains differ 
chapter focus topic trace reduction virtual memory studies 
safely allowed drop sad optimal lru reduction olr new methods reduce trace size affecting lru simulation results 
furthermore methods introduce significant error common lru policy simulations 
reduction techniques timescale relative ignore pages matter small memories 
ram sizes usually tens mbytes possible methods remove information irrelevant smaller common ram size 
see real sequences methods produce reduced traces smaller factors hundreds tens thousands 
address chapter methods virtual memory adaptivity developed 
methods keep kind recency information lru analyze information timescale relative way 
discuss eelru greater detail 
eelru performs lru replacement default lru exactly right policy important patterns 
lru fail common patterns fails severe manner 
eelru detects common patterns modifies replacement decisions 
information eelru needs provided lru information perform simple cost benefit function 
approach adaptivity serves model keep simple relevant information virtual memory systems adaptive 
chapter explore compression memory data 
particular heart compressed caching scheme actual compression decompression virtual memory pages 
address regularities commonly exist memory data desirable characteristics compression algorithm compressed caching 
wk compression algorithms design compressed caching context 
show results experimental evaluation new algorithms lzo lzrw fast lempel ziv algorithms compressed caching studies 
compressed caching facets chapter 
analytic approximation compressed cache perform trace average compression ratios speeds 
show simulations fixed size compressed caches approximation 
accuracy approximation supports understanding compressed caching problem valuable tool developing recency method adaptivity 
simulations approach dynamically adapting size compressed cache show understanding behavior compressed caching yield significant reductions paging time 
examine hardware trends directly influence increasing benefit gained compressed caching 
chapter concepts caching page replacement timescale relativity virtual memory abstraction provided system user program large unsegmented address space 
memory hardware machine single piece uniformly addressable space 
number different memory devices cpu registers chip chip caches ram disk store data machine 
task virtual memory system map single simple virtual address space pieces memory hardware 
virtual memory hierarchy 
note various pieces memory hardware listed vary size speed 
specifically larger memory device slower relationship size speed implies memory hierarchy 
top level hierarchy set cpu registers 
cpu operate piece data data moved top level 
program data fit registers data held registers stored larger slower chip cache 
data fits data stored larger slower chip cache 
hierarchy continues ram disk 
virtual memory system efficient try arrange data storage levels little time wasted accessing data stored larger slower levels memory 
specifically fast small level memory full replacement policy dictates data evicted memory level larger slower 
smaller memory level said cache larger level 
replacement policy cache data needed program fastest memory levels reducing time spent waiting slower memory levels 
weak link hierarchy 
levels virtual memory ram disk particular interest 
chip chip cache chip cache ram increase access cost order magnitude slower memory level 
disk approximately orders magnitude slower ram gap levels widening 
applicability levels 
research centers virtual memory management main memory disk 
concepts applicable levels memory hierarchy discuss caching terms management pages main memory accesses disk avoided 
important notice believe concepts apply level hierarchy 
purpose describing management behavior terms timescale relativity evaluate memory different levels hierarchy irrespective system specific configuration 
poor performance disks introduce virtual memory examine past discuss concepts surrounding page replacement policies 
particular line opt policy determine cheating examining smallest possible number disk accesses virtual memory system incur 
examine lru policy commonly approximated value fully appreciated 
attempt reveal causes poor performance frequently lfu policy 
develop faces timescale relativity analyze performance replacement policies terms concept 
allow see take billions memory information observe patterns program behavior 
replacement policies timescale relative perform better able respond patterns 
concepts background provide new perspective old ideas new ones chapters 
line caching prediction problem page replacement real systems done line policy decide page evict information collected previously pages 
policy assume information available real systems 
result page replacement policies available information past attempt choose page soon 
words page replacement policy past information predict 
policy successfully chooses evict page soon choice evict page cost virtual memory system time 
importantly page may needed sooner cached longer time available smaller cost chosen eviction 
line page replacement policies allowed examine deciding page evict 
replacement policies real systems information obtained runtime 
provide useful information quality predictions line policies 
temporal spatial locality chapter mentioned locality exhibited programs automatic paging aspect virtual memory practical 
basic idea locality data referenced close data referenced 
words data naturally clustered caching blocks pages data practical page unit data close 
notice term close interpreted ways commonly terms space time 
spatial temporal locality discussed analyzing behavior designing virtual memory systems 
pieces data temporally local accessed nearly time program execution 
pieces data spatially local near address space 
notice strict definition spatial locality refers data distributed address space 
spatial locality refer pieces data clustered set pages referenced nearly time program execution 
dividing memory pages useful strategy spatio temporal locality 
data temporally local tends stored spatially local way 
fetching page useful piece data page soon 
known replacement policies attempt understand timescale relativity relationship behavior replacement performance need background mechanical operations common page replacement policies 
section outline opt lru lfu best known studied page replacement policies 
replacement policies bear great resemblance 
problem page replacement understood examining responds common patterns 
opt optimal line replacement policy page replacement policy entire sequence page execution program page faults policy incur 
question belady algorithm min opt bel answers 
page replacement policy line 
provably yields smallest number page faults page replacement algorithm 
benchmark page replacement algorithms measured 
aid understanding line page replacement policies 
opt works 
page fault opt policy chooses evict resident page referenced furthest 
words opt examine sequence going occur 
page memory determine time 
page selected eviction 
number approaches implementing opt 
obvious brute force search forward trace page fault 
alternatives include schemes maintain window methods require pre processing trace augment timestamps 
line algorithm ability implement line relatively unimportant 
lru lru replacement policy line policy aptly named page evicted memory level policy chooses evict page 
implicit assertion drives policy page long time soon 
page reasonable choice eviction 
see section assumption programs poor 
lru works 
algorithm implement policy simple move front mtf algorithm keeps list pages known lru queue pages page moved front queue time page back queue 
implementing lru real system difficult 
mtf algorithm implementation lru simulators operations required remove referenced page middle list insert front inexpensive 
operations expensive perform real system memory 
result implementing lru policy real system done mtf 
fact real lru implementation require special hardware support feasible long time processor 
data structure commonly refered lru stack 
choose term stack believe data structure behave stack term misleading 
terms front back denote ends queue applied inconsistently 
insert items front queue remove back 
strong performance policy driven development policies approximate lru 
address section clock segq policies approximations real systems require hardware support lru 
lfu frequently frequently lfu policy line policy 
operates assertion superficially similar lru page frequently unused time page evict 
assumption may initially reasonable see perform poorly programs 
see frequency indicate 
works 
implementation lfu daunting lru 
statistics referenced page updated 
simulation table pages kept store statistics 
frequently page table important simulation 
real implementation lfu practical 
keeping statistics expensive 
approximation methods resemble lfu missing frequency information fundamentally changes behavior replacement algorithm 
see section lfu timescale relative fact largely responsible poor performance 
timescale relativity virtual memory time system time meanings particularly applied computations execution 
choice timescale study depends topic studied 
time considered real time measured stopwatch number cpu clock cycles pass 
computation proceeds rate hardware resources uses moment timescale speed resources may meaningful 
example measuring computational time terms number cpu instructions instructions require cpu clock cycles may meaningful real time 
concerned fetching evicting pages disk 
memory effect page replacement decisions events virtual memory system concerned dictate timescale 
virtual memory system time advance influence require replacement decisions 
size main memory significantly effects timescale memory system react 
consider kbyte hardware cache mbyte main memory 
program loops kbytes data going cause number significant events occur hardware cache cache block replacements performed 
main memory easily store data significant events occur 
concept obvious see sufficiently considered evaluating replacement policies 
predicting happen soon 
page replacement policy chooses page evicted main memory disk performing prediction 
reduce number main memory misses pages main memory replacement policy choose evict page referenced soon 
order determine quality prediction policy develop notion 
established soon page imply eviction poor choice 
page pages clearly going soon evicting mistake 
page clearly going soon eviction mistake 
middle ground page pages 
page going soon caching excess disk accesses needed 
page going soon page cached sooner incurring disk access 
forward backward page distance 
provide useful characterization provide definition forward page distance page number unique pages referenced current time page 
example moment program execution may pages 
forward page distance point execution pages referenced backward page distance analog forward page distance page number unique pages page current time 
metric interesting relationship lru queue 
backward page distance page exactly lru queue position occupy moment execution 
definition key observations means page soon ffl forward page distance page determine soon 
tempting think terms raw number 
consider situation pages referenced great times page interested 
considering number may page question occur soon page remain cached 
view terms number predicting long page go unused respect time 
concerned virtual memory time events influence replacement decisions advance time 
referencing just pages little influence page replacement virtual memory time advance page question occur soon respect virtual memory time 
ffl virtual memory time help determine pages sooner know page going soon 
characterization terms memory size 
page memory wise keep page forward page distance 
page memory caching page clearly wise 
memory size directly influences page soon remain cached 
order understand poorly line page replacement algorithms predict soon page touched explore opt respects memory size number pages referenced successive page 
opt descriptions algorithms implement opt policy number measure soon page 
opt concerned choosing resident page soon needs relative ranking page referenced 
number metric wrong timescale linear measure time provide needed ranking pages 
useful virtual memory time analyzing behavior opt 
opt chooses resident page largest forward page distance 
way opt simply choosing resident page soon 
consequently opt timescale relative selects page soon decision poor 
true timescale appropriate manner opt selects page eviction important opt responds events virtual memory timescale 
note possibility responding data collected different timescales 
possible perform different kinds pattern recognition timescale may relevant virtual memory behavior 
kind pattern recognition 
furthermore interested detecting simple regularities timescale virtual memory systems compare replacement policies perspective 
lru lru bases prediction pages kind mirror image past longer page gone unused longer going 
words page lately going soon 
page soon page evicted 
lru performs access pattern number pages fit available memory 
performs access patterns pages fit memory 
access patterns loop slightly pages fit memory lru performs poorly possible 
see lru performs badly cases 
show lru timescale relative cases performs timescale relative patterns poor decisions 
recency lru distance 
analyze behavior lru need define related concepts recency lru distance 
lru selects page orders pages lru queue 
lru queue provides information recency page time execution 
lru distance special case backward page distance measure number pages successive page 
name taken distance page fall lru queue ordering second successive 
gamma pages uses page page reside lru queue position say second occurs lru distance term provides specific recency information page 
lru performs cases 
order understand lru performs need examine manner real programs memory 
particular want view patterns respect recency lru determine lru prediction corresponds reality 
figures see views access sequence lru memory 
lru access histogram shows number pages lru queue position complete execution program 
second lru histogram shows memory size number misses suffered 
note second histogram derived lru common access histogram common access histogram lru sample lru access histogram artificial data 
axis increasing lru position axis number lru queue position 
number misses page memory lru heretofore lru memory sum accesses lru positions th position 
note pages lowest lru queue positions plot shown log scale axis 
notice access histogram strictly non increasing lru queue position increases general pages referenced 
plots demonstrate programs exhibit strong temporal locality automatic paging 
plot lru assumption pages misses memory size pages common histogram common histogram sample lru histogram artificial data 
axis memory size axis number misses suffered memory size 
shape access histogram common programs 
note decrease severe log scale needed 
referenced soon pages valid 
collection kind recency information reveal useful information access patterns programs 
lru base predictions strong regularity see replacement policy avoid evicting pages perform programs 
note vast majority lru queue positions 
real memory sizes require lru evict pages higher lru queue positions lower lru queue positions little effect 
example pages simply reorder recency pages 
don advance virtual memory time little influence replacement decisions 
words effect prediction page higher lru queue position soon memory don affect lru notion 
kinds access patterns lru timescale relative 
lru performs terribly cases 
lru prediction time page suggests time works quite programs easily imagine case fails 
real programs memory way longer page soon 
figures show lru access histograms program loops pages 
page referenced remains unused pages loop traversed 
note pages referenced 
pages referenced loop iteration may contain example code data part frequent calculation 
assumption lru exactly wrong case 
longer page referenced closer referenced loop 
lru eviction choice choose worst possible page lru position access histogram loop dominated program loopy access histogram lru lru access histogram program loops pages memory 
misses memory size pages histogram loop dominated program loopy histogram lru histogram program loops pages memory 
lru prediction pages soon exactly wrong prediction 
note number misses decrease memory pages added entire loop held memory 
memory 
words lru timescale relative kind access pattern 
lru policies exactly wrong assumption common case lru 
lru making wrong prediction mistake cause page faults loop larger available main memory 
histogram shown main memory hold pages pages retained lru mistaken assumption cost 
situation remember influence memory size timescale relativity 
absolute determination soon page may matter pages soon poor prediction doesn matter 
obvious case lru poor prediction costs little loop significantly larger main memory 
program loops pages data main memory hold pages data replacement policy yield performance 
opt require pages evicted fetched 
lru require pages evicted fetched iteration worse optimal 
memory size directly influences timescale 
situation notion going perform nearly possible simply situation hopeless 
lru ignores useful recency information 
lru replacement damages performance significantly pattern loop somewhat pages stored memory 
notice lru access histograms reveals pattern high number accesses th lru queue position indicate memory somewhat smaller pages program referencing pages just evicted 
discuss information detail chapter 
eelru designed examine recency information detect lru evicting pages soon fetching 
case detected eelru begins perform non lru page replacement address problem 
examine detail detect cases problem analyzing recency information line 
apply kind adaptivity compressed caching chapter 
lfu select page eviction lfu predicts resident page soon 
notice lfu base prediction time frequency 
lfu records information respect time virtual memory time 
kinds access histograms shown figures strong correlation recency frequency 
page soon page frequently 
phase behavior reveals lfu weakness 
correlation recency frequency hold common access patterns 
programs exhibit phase behavior time phase program set pages great deal 
program need different set pages favored new phase 
lfu adjust phase changes 
consider execution program system pages available main memory 
phase say program frequently uses set pages 
pages frequently pages lru lfu perform 
consider happens program changes phases begins different set say pages 
new set pages policies suffer compulsory misses 
note influence replacement decisions advance virtual memory time 
lru responds aging old set pages immediately considering new set soon 
lfu prefer cache pages old set require execution time pass new set frequently 
frequency statistics reflect change lfu page significantly set pages easily fit memory 
lfu gathers responds information respond timescale memory size 
information collected lfu indicates page resembles past soon page 
lfu timescale relative respect 
theoretical limitations 
despite intuitive appeal pages evicted lfu shown experimentally theoretically perform poorly 
analytically sleator tarjan showed st lru competitive opt 
page memory lru suffer times faults opt 
generally lru memory opt factor increase faults lru bounded ratio lru opt memory sizes 
contrast proof line algorithm improve competitive property 
lfu shown competitive incur unbounded factor faults opt 
timescale relativity ii aggregate behavior line replacement policies predict 
specifically choose pages soon eviction 
choices time page evicted result single page temptation design policies select page soon 
opt need aggregate information temptation reinforced behavior line opt policy choose page soon 
nature prediction suggests kind specificity hazardous 
line policy perform vast majority patterns reliably selects page soon 
little need predict exactly page soon 
replacement policy attempts specific predictions brittle 
small scale variations pattern confound predictive heuristic damage performance 
important remember opt need predict soon page calculate exact page 
line algorithms need balance robust prediction specific information 
line policies respond coarse grained aggregate behavior 
resulting decisions may optimal ones poor costly choices 
lru uses aggregate information performance lru predictions pages exact predictions policies 
contrary lru performs large number access patterns rely exact predictions 
lru choice pages eviction ways inexact 
information kept page lru try detect patterns page 
page considered information recency taken consideration 
may detectable patterns page recency reliable predictor page pattern recognition easily lead poor eviction decisions 

exact recency order pages critical 
nearly access patterns evicting pages memory yield approximately performance lru 
see real systems approximations lru policy property achieve performance maintaining exact recency information 
property implies small changes access behavior degrade paging performance significantly lru sensitive small behavior changes 
lru responds aggregate behavior 
maintain timescale relativity irrespective minor differences behavior parameters memory size 
policy sensitive adjust different timescales easily 
lfu uses page information order determine page frequently lfu policy maintain statistical information history page 
lfu predicts soon page specific page past 
keeping page information contributes lfu difficulty phase changes 
lfu considers past page past pages similarly pages different time execution 
statistics page may reflect patterns previous phases mislead policy regards current page 
lfu sensitive exact frequency ordering 
lru selection frequently pages yield approximately paging performance 
timescale relativity iii adaptivity decay programs exhibit phase behavior want virtual memory system able respond phase changes 
changes transient phases changes persistent phases 
adapt phases need sure adaptivity agile attempt respond transient behavior advantage gained respond quickly persistent behavior managed 
order ignore short term behavior notice appropriate longterm behavior virtual memory system operate appropriate timescale 
see opt adapts immediately perfectly lru adjust phase changes time analyze lfu failure adapt phase changes 
describe recency information provided lru histograms decayed allowing information timescale relative way 
opt decay opt keep information past 
line policy rely information 
note opt looks page determination 
occur near provide information needed distant need considered 
line policies predict past characteristic opt supports notion relevant ones 
useful notice quickly opt adapts new phases 
pages great deal program danger eviction moment new phase begins pages strong candidates eviction 
opt adapts quickly possible causes fetch opt uses opportunity evict page longer needs cached 
lru decay decaying information collected past behavior greater weight ones 
worthwhile note lru performs strict kind decay virtual memory timescale 
lru stores information page 
page knowledge previous page discarded 
lru decaying severe way old information past pages 
decay occurs occur 
importantly behavior lru pages lru queue positions near corresponding memory size affect decay 
pages referenced pages change lru queue positions information pages decayed virtual memory time advanced 
simple kind decay sufficient allow lru adapt quickly phase changes 
similarly opt new phase encountered pages old phase age quickly candidates eviction 
great deal previously pages 
phases change virtual memory time moves quickly lru responds 
lfu decay reasons statistics kept lfu timescale relative events past equal weight 
consequence phase change occurs lfu gives weight past pages weight page 
pages current phase take long fall favor evicted 
frequently lrfu policy lck uses combination recency frequency information select page eviction 
policy uses single tunable decay parameter 
decay performed policy performs lfu replacement 
decay maximum information page kept policy performs lru replacement 
surprisingly low decay factors yield poor paging performance 
performance improves decay performed quickly policy approaches lru behavior 
experiments performed researchers developed policy indicate fast decay quite fast performed lru provides best performance 
improvement lru minimal experiments usually 
bulk improvement came decaying lfu information quickly making replacement policy timescale relative 
adapting phases lru histogram decay recency information provided lru access histograms shown figures result information gathered entire program executions 
histograms provide interesting picture behavior entire program weigh referencing behavior heavily 
kind information line detect patterns pages just evicted cached needs timescale relative 
particular decay histogram statistics virtual time passes 
timescale statistics may decay quickly slowly 
overly rapid decay may lead adaptive heuristics respond transient behavior advantage gained 
may eliminate information needed detect long term persistent behavior 
slow decay may prevent heuristic adapting new phases 
order decay histogram information correct rate advance virtual memory time pages old recency terms near eviction evicted 
pages pages evicted long ago caching policy hold little effect virtual memory time 
advancing time fashion phase changes force time move quickly adaptive heuristics respond quickly 
phases time advance persistent behavior target adaptation 
form decay virtual memory information eelru chapter compressed caching chapter 
see form adaptivity allows management methods perform wide range memory sizes different patterns 
lru approximations real systems lru performs situations noted earlier difficult implement real system 
result replacement policies behavior lru developed 
policies designed little hardware support real systems 
perform kinds replacement policies modern operating systems today 
categories policies behave lru 
refer lru variants 
replacement policies keep number pages lru order policy rest resident pages 
examples policies include fbr rd flw seq gc eelru 
policies differ lru general behavior similar majority pages policies keep lru queue 
policies clock car segmented queue segq known level replacement hybrid fifo lru bf represent category lru approximations 
policies replacement policies real systems 
eviction selections recency heuristics record recency exact lru queue 
policies similar lru tend just timescale relative 
surprisingly tend suffer lru lack timescale relativity patterns loop slightly available memory 
kind solution propose chapter eelru equally applicable lru variants approximations 
segmented queue segq policy segq policy uses queues fifo queue lru queue 
fifo queue fixed size lru queue unbounded 
page fifo queue referenced inserted front fifo queue 
long page fifo queue effect fifo segment lru segment pages managed segmented queue consider segmented queue comprising fifo lru segments initial state shown 
page cause page taken lru segment inserted head fifo segment 
page extracted tail fifo segment inserted head lru queue 
resulting state segmented queues shown 
page fifo queue cause change segment segmented queue 
example page bring segmented queue state shown identical 
queue ordering free overhead information collection 
pages aren fifo queue force page back queue 
pages inserted front fifo queue page evicted 
page evicted fifo queue inserted front lru queue 
pages evicted fifo queue page pushed lru queue ordering 
page referenced removed middle lru queue queue inserted head fifo queue 
pages lru queue incur re ordering overhead 
graphically depicts movement pages referenced queues manner fifo queue feeds lru queue 
clock policy clock policy views resident pages circular queue numbers face wall clock 
moment clock hand points resident pages 
page needs selected eviction clock hand sweeps resident pages looking hasn 
searches page updates recency information pages passes aging 
implementations clock take advantage hardware bits tags set hardware page memory 
clock hand sweeps pages aging ones searching bits determine recency 
page bit set touched 
chosen eviction aged clearing bit 
page bit clear page selected eviction sweeping clock hand stops 
page fetched placed location clock face 
pages bit set 
page remain unreferenced time clock hand find clear bit select eviction 
notice multiple bits page 
hardware bit serves primary bit resident page minor bits stored software 
clock hand sweeps ages page shifting bits bit clearing primary bit 
hand encounters page bits set page selected eviction 
basic form clock policy suffers lack timescale relativity 
particular hardware bits set timescale clock hand decays information virtual memory timescale 
complex variants clock ones interrupts multiple clock hands bring tasks nearer timescale developed real systems 
compression compression data key component different ideas dissertation 
application data compression reduction space required store pages compressed cache chapter 
second obvious form data compression shown chapter new methods trace reduction 
presenting background compression issues terminology 
go details modeling process detecting patterns data encoding generation compressed data representation results modeling 
background data compression domain specific due pervasiveness availability common compression utilities theoretical results limited scope believe general purpose compression algorithms 
belief algorithms adapt detect regularities kind data 
done algorithm choose reduced representations regularities occur 
general purpose compression exist data exhibit regularities unbounded number ways 
aforementioned theoretical results bcw assumptions data compressed 
type data compression algorithms yield reduced representations sense fixed 
example common studied lempel ziv lz family compression algorithms shown optimal data generated markov chain 
see section detail markov models 
lz algorithms perceived general purpose uses limited 
furthermore believe algorithms universally applicable best algorithms nearly domain 
lz algorithms provide compression different kinds data kinds 
lz algorithms ideal algorithm domains algorithm designed specific kind data may provide better compression lower computational cost 
domain specific algorithm may applicable wide range data types compression applied narrow domain 
situations compression algorithms designed detect regularities expected data perform best 
compression algorithm look set regularities input data 
expectations data compression performed 
similarly expectations incorrect input compression gained 
fact bit strings length representation yield smallest average uncompressed representation enumerating possible bit strings 
representation may able store strings compressed encoding average length long usually longer 
compression algorithm strict order authorship algorithms known ziv lempel 
lz ordering commonly dissertation 
operate kind data shorten representation time 
lossless vs lossy compression compression broken general categories lossless lossy 
lossless compression scheme information maintained input compressed 
words mapping compressed decompressed representations result compressing decompressing piece data exact restoration original input 
lossy compression scheme discards information input compresses mapping uncompressed compressed representations 
discarding information presumed superfluous reduction greater 
key methods assumptions compression algorithm intended data 
example popular jpeg compression format lossy method graphic images 
assumptions visual acuity humans need accuracy image 
assumptions allow jpeg method discard information input image 
long image decompressed viewed people jpeg assumes image sufficient quality 
decompressed image purposes bit bit comparison original jpeg inappropriate compression scheme 
chapter examine compression algorithms inmemory data paged compressed cache 
compression algorithms manage program data lossless 
contrast new methods trace reduction chapter lossy methods discard information relevant lru lru replacement policies 
lossy methods reduced compressed traces purpose information missing reduced trace may affect results 
inputs memory data traces contexts compressed caching trace driven simulation forms compression different 
compression schemes provide significant reduction assuming regularities inputs 
modeling vs encoding order understand compression algorithm works address interacting separate tasks modeling encoding 
process examining input detecting regularities process modeling 
compression algorithm looking regularities building internal representation input relates algorithm repetitions predictable values 
input modeled compression algorithm encode output try emit representation data smaller original input 
phases interact choice encoding schemes kinds regularities modeling 
distinct tasks blended 
modeling compression algorithm implement model expects find data 
example order model english text compression algorithm hold dictionary common words 
time words encountered algorithm record occurrence 
input english text correspond model common english words detected algorithm greater potential compressed representation encoding performed 
compression algorithms restrict domain applicability narrowly 
weaker lower level expectations data assumed 
compression algorithms try find various kinds repetitions bytes input assume high level human language regularities 
rarely implemented directly markov models common constructs detect regularities wide variety inputs 
markov models markov model 
approaches modeling data 
markov models form probabilistic modeling received great deal attention compression research bcw 
alphabet symbols may appear input string markov model indicate symbols seen probabilities symbol may 
model represented finite state machine known th order markov model 
see example st order markov model subset english alphabet specifically letters node state machine character alphabet 
symbol taken input string state machine enters node symbol 
note strictly speaking start states entry exit state machine 
depicted example 
directed edges node weighted probability edge followed probability node destination directed edge contains symbol 
reading input symbol appropriate edge symbol corresponding node model predict symbol follow 
input modeled set probabilities 
st order markov model symbol source alphabet corresponds node state machine 
weighted edge node represents probability symbol input 
assigning shorter bit representations heavily weighted edges smaller representation input corresponds model 
see encodings chosen probable transitions represented fewer bits probable transitions 
markov model sufficiently representative input heavily weighted probable edges traversed compressed representation produced 
input correspond markov model weightings representation expand 
kind st order markov model state machines remembers symbol input 
model attempts accurate predictions little context succeed model considers past input 
accurate predictions predict accurately greater variety input strings want state machine respond larger context 
example nd order markov model shown 
node represents symbols seen input edges represent probabilities symbol may follow 
note number nodes edges required larger 
state machine contain nodes possible string length nodes edge symbol alphabet 
markov model storage problem 
notice significant weaknesses markov models compression real data space requirements 
alphabet th order markov model requires jj nodes jj edges 
exponential growth storage increasingly higher order markov models quickly prohibitive 
predictions just symbols string ascii characters requires nodes edges 
see popular lempel ziv compression algorithms address storage problem storing possible probable symbol combinations 
cc bb cb ca ba ab aa ac bc nd order markov model node represents seen input symbols 
higher order model means responding context means storage requirements significantly larger 
determining transition probabilities 
state machines model input provide compression largely dependent probability weights assigned edges 
weightings assigned statically dynamically 
assigned statically assumption sequences symbols occur 
set weightings devised english text application model kind input non english human text binary machine code yield tight compression different weighting yield 
computational costs static weighting low updates needs performed 
dynamic assignment weights requires computation 
source string read weights modified reflect actual frequency edges traversed 
sequences reasonably consistent input markov model eventually modified provide accurate predictions compression 
dynamic approach setting probabilities yield better compression larger set inputs ameliorate need higher order models capture complex regularities 
adapting probabilities small context lower order models 
higher order markov model longer input string probabilities reach useful equilibrium 
high order markov model applied practice smaller inputs significantly compressed 
markov models capture regularities 
types models mathematically tractable way theorems compression properties proven 
proofs mistaken evidence models capable order model high probabilities appropriately set capturing kind regularity 
weaknesses markov modeling approach incapable capturing high level regularities 
state machines respond literal sequences symbols 
modeling similarities sequences nodes edges state machine independent 
markov models respond exact literal sequences symbols general repeating patterns 
example consider modeling english text 
modeling method breaks input english words individual characters designed predict nouns followed verbs 
modeling ad hoc high level able provide compression words seen input 
markov model capture kind regularity irrespective order model size input 
literal occurrences noun verb combinations captured generalization occur 
lempel ziv family compression algorithms research modeling centered statistical models markov models symbols assigned probabilities 
compression achieved correctly predicting symbols occur 
dictionary modeling obtains compression encoding portions input entries dictionary known compressor decompressor 
example approach modeling table common english words 
words input output contain index table 
methods statistical dictionary modeling mutually exclusive 
statistical models generally considered computationally expensive theoretical value dictionary methods considered researchers believe hardware speeds increase computation inex practical particularly methods ziv lempel proposed popular influential dictionary compression algorithms popular compression algorithms 
total set lempel ziv algorithms non trivial scope dissertation discuss 
variants improvements algorithms developed studied 
concentrate original influential algorithms lz lz 
lz compression algorithm assumes literal repetitions symbols input string 
find portion input repetition model encode portion merely pointing previous occurrence 
decompression algorithm recreate input serial copy previous occurrence recreate repetitive portion input string 
note lz considered dictionary compression algorithm input matched commonly considered dictionary 
number previously encoded symbols stored serving sliding window past input string 
algorithm reads tries match symbols current input contents sliding window 
note lookahead buffer current input symbols limited size 
simple approach compression successful 
implemented efficient space time provides significant reductions 
commonly utilities gzip employ simple variants compression algorithm 
lz algorithm similar predecessor attempts match current input sequences past ones 
approach building statistical methods practical bcw 
spite incredible increases cpu speed happened 
structure matching quite different 
past input stored simple sliding window substrings 
dictionary observed character sequences built past input 
sequence input symbols matched dictionary entry repetition encoded index dictionary 
furthermore time sequence input symbols matched entry dictionary entry modified adding symbol current input sequence 
example characters input sequence dictionary entry past sequence acb dictionary entry updated 
algorithm respond increasingly long repetitions input 
dictionary grows larger predetermined bound discarded process building begins anew 
lz provides significant compression reasonable space time efficiency 
shown input generated ergodic source markov model algorithm achieve asymptotically optimal compression arbitrarily long string 
unfortunately real inputs may easily represented markov model 
real input going arbitrarily long 
practically lz achieves compression comparable lz compression schemes 
recency modeling lz compression algorithms maintain limited dictionary seen substrings source input 
literal recurrences substrings input regularity may exist 
symbols substrings repeat input commonly symbols substrings grouped 
words seen patterns recur seen ones 
recency modeling commonly uses move front mtf algorithm perform compression 
dictionary just arbitrarily ordered bag observed substrings ordered entry 
entry provides match substring input entry moved front dictionary 
see encodings chosen take advantage regularity modeled lru ordered dictionaries smaller bit representations assigned entries 
form modeling lend beneficial encoding helps maximize effectiveness dictionary 
extensive dictionaries past input large 
large dictionary increases space requirements algorithm increase time requirements operations required attempt match current input entry 
choosing discard entries dictionary bounded size entries kept yield matches 
idea mtf algorithm perform compression modeling proposed separate occasions bcw 
mtf compression algorithm implemented experimentally evaluated 
recency simple form modeling study demonstrated represents regularity exists kinds data results study promising 
note mtf recency compression implement lru page replacement 
similarities predicting contents data compressed predicting page 
address section limitations similarities 
exhibit simple regularity particular pattern observed occur soon 
page replacement mtf performs compression modeling forms predictions strong regularity data easily detected 
important note recency modeling isolation regularity detected combination sequential repetitions data making robust form modeling 
encoding regularities modeling input string output representation chosen 
process selecting representation emitting output representation known encoding 
compression possible effective modeling actual reduction representation size depends just encoding 
modeling phase detect regularities encoding phase choose reduced bit representations detected regularities final output smaller original input 
number approaches encoding 
shown exhibit minimal space certain assumptions encoding choice binary representation 
note obvious encoding bit input uncompressed representation possibilities enumerated bits 
uncompressed representation yields smallest possible mean number bits strings hope devise encoding smaller majority input strings encounter 
ad hoc encoding simple form encoding comprises set bit combinations reflect space efficient manner data collected modeling 
wilson kaplan compression algorithms chapter kind encoding 
kind binary representation usually represents directly model regularities decompressor reconstruct original representation 
disadvantage kind encoding result smallest compressed output necessarily close 
encodings exhibit regularities detected modeling scheme compressed 
exchange suboptimal reduction representation speed gained 
see context compressed caching simple ad hoc encoding implemented generate output quickly yield sufficient compression context 
simplicity allow decompressor parse quickly compressed representation 
example specialized form ad hoc encoding number contexts difference encoding 
approach data bits symbol 
encode symbol difference encodings represent single symbol represented difference single symbol 
example string integers encoded values calculated literal value 
difference encoding performed quickly simply applied specific kinds data 
data known comprise large values differ little difference encoding may applicable 
traces context spatial locality implies pages part address space 
sam method trace reduction lossless compression technique designed specifically traces significant difference encoding 
huffman encoding ad hoc encoding simple devise implement contexts demand better compression ratios increased time space required achieve 
huffman encoding shown optimal prefix coding encoding bits represent symbol prefix bits represent symbol 
note common uncompressed encoding data symbols represented fixed number bits 
huffman encoding variable length encoding different numbers bits may represent symbol 
number bits fixed valid variable length encoding prefix code 
words possible decompress variable length encoding ambiguity particular sequence bits encodes symbol just encoding symbol 
huffman encoding requires symbol input alphabet assigned probability occurrence 
probabilities assigned binary tree constructed follows 
insert symbols nodes probabilities set 

select remove set nodes lowest probabilities 

assign nodes left right child new node 

insert new node set 

repeat element remains 
proof property see clr intuitively term prefix free coding standard terminology 
binary tree represents huffman encoding alphabet symbol encoding described path root leaf contains symbol 
traversal edge left child encoded right child 
example path root left left right 
greedy algorithm construct binary tree seen 
edge tree labeled bit left child edges labeled right child edges labeled 
path root desired symbol sequence bits encodes symbol revealed sequence binary values traversed edge labeled 
note prefix code property guaranteed symbols reside leaf nodes 
huffman encoding merely encoding scheme really assignment bit representations th order markov model 
probabilities assigned symbol analogous probabilities assigned edges markov state machine vertex remember character observed 
note assignment probabilities performed input seen 
particular kind encoding known static huffman encoding 
forms dynamic huffman encoding exist 
probabilities variable length binary representations symbol modified input compressed 
simple approach updating encoding recalculate probabilities reconstruct encoding tree input symbol 
frequent extensive computation prohibitive schemes incremental updates practice 
arithmetic encoding note huffman encoding symbol input represented number bits output 
arithmetic coding toone mapping input symbols output bit strings 
input symbols represented partial numbers bits 
course total output provided integral number bits may fraction bit carry useful information 
arithmetic encoding yield encodings smaller created huffman encoding 
basic idea arithmetic encoding represent message single real valued number arbitrary precision 
huffman encoding th order markov model assumed kinds models applied 
probabilities assigned symbol statically fixed dynamically modified input processed 
sake simplicity describe arithmetic encoding static probabilities 
value real number output computed input symbol time follows 
initialize range output number fall 

divide current range probabilities symbols 
range currently probability symbol range symbol 

read symbol input 

restrict range output number fall range symbol 
example symbol read example restrict range 

repeat symbols remain input 
note input special string symbol 

output number final restricted range 
note special string symbol necessary decoding algorithm 
value encode aa aaa 
method encoding yield compressed representation computationally expensive ad hoc methods 
compressibility vs caching underlying concepts line caching compression similar 
require predictions patterns past behavior 
similarities try analyze domain terms 
measuring locality compressibility 
popular approach determining locality program compress trace 
trace program compression utility order see compressible trace assumption program exhibits locality yield compressible trace program poor locality generate poorly compressible trace 
assumption faulty 
cost incurred due poor prediction quite different domains 
compression model predict input symbol cost bits overhead compressed output 
caching poor prediction page referenced cause slowdown orders magnitude 
high compressibility imply locality 
illustrate important difference poor prediction cost consider example trace may contain repeating pattern page small number pages randomly chosen page 
trace extremely compressible 
repeated small number pages reduced bit 
randomly chosen pages may require expanded representation bits harm total compressibility little 
contrast number randomly chosen pages larger memory hold pages require page fetched slower memory 
modern processors fast issue hundreds millions memory second 
causes page fault processor generate tens thousands page faults second 
disk requires milliseconds fetch page 
disk service hundreds page faults second 
pages referenced easily cached randomly chosen uncached pages cause system thrash 
program causes hopeless level page faults irrespective replacement policy considered poor locality 
high compressibility necessarily imply locality 
low compressibility imply poor locality 
may able glean quality locality easily compressibility may possible rank locality exhibited programs metric 
example augmented demonstrate relative rank trace compressibility indicate relative rank locality 
consider second trace contains uniform number thousands pages 
trace compressible trace described require bits average represent compressed form 
systems store pages locality program excellent suffer page faults 
compressibility provides little information locality 
chapter trace reduction trace driven simulation common approach studying virtual memory systems 
trace sequence virtual memory addresses accessed executing program simulator imitate management virtual memory system 
traces experiments virtual memory management policies reproduced controlled environment 
unfortunately traces extremely large easily exceeding capacities modern storage devices traced executions lasting seconds 
size traces impedes storage processing 
trace reduction compression traces lossless lossy stored processed efficiently 
existing methods trace reduction 
methods undesirable characteristics virtual memory simulation discard information reduced trace introduces significant error simulation common page replacement policies 
methods difficult control information discarded size memories simulated accurately 
methods reduce storage costs reducing number time required process trace 
trace reduction methods safely allowed drop sad optimal lru reduction olr suffer deficiencies 
allow user control degree reduction specification reduction memory size 
sad simpler removes guaranteed affect lru opt behavior trace provided simulated memory sizes smaller reduction memory size 
assumption olr algorithm yields shortest possible trace exact lru simulations place original trace 
olr useful provides greater reduction sad output gives lower bound length reduced trace 
algorithms efficient practice significantly reduce storage processing costs 
guaranteeing accurate simulation lru opt may exciting 
trace policies simulation run results stored re 
approach effective simulations virtual memory replacement policies 
nearly replacement policies studied real workloads variants approximations lru weak sense sufficient trace reduction techniques 
similarity common page replacement policies hardly surprising replacement algorithms evict pages current 
lru variants approximations discussed section 
lru variants flw seq gc fbr rd eelru keep referenced pages memory pages memory accessed pure lru 
approach trace reduction applicable cases reduction memory size small values allow olr sad achieve reduction factors orders magnitude guaranteeing exact simulations 
sad olr useful studying lru approximations clock segq 
replacement policies ignore high frequency referencing information nearly replacement policy ignore sad olr discard traces 
information ignored lru approximations pure lru pages matter replacement timescale smaller memories managed virtual memory systems 
high frequency don influence replacement decisions hardware real systems support efficient collection information 
show error introduced sad olr clock segq replacement simulations small number faults cases 
compare sad olr stack deletion sd smi commonly known technique removing high frequency information virtual memory traces 
reduced traces comparable size created methods sad olr introduce error average clock segq simulations 
sd introduces error simulations traces times excess original research sd indicate 
additionally ability sad algorithm produce reduced traces valid exact opt simulations pleasant side effect means single trace experiments virtual memory study 
studies compare new algorithm lru opt 
background motivation importance trace reduction surprising wealth research reduction techniques 
impossible exhaustively address approaches 
section presents overview section positions method relative closely related techniques 
survey trace driven simulation uhlig mudge um 
overview related lossless reduction techniques straightforward approach lossless trace reduction apply standard data compression techniques trace 
simple lempel ziv compression algorithms provide reduction factors typical traces um 
higher degrees reduction achieved combining compression algorithms difference encoding techniques 
best known instances sam jh systems explore spatial locality trace encode differentially 
subsequently standard text compression techniques applied result reduction size 
useful note limitation general purpose compression algorithms believe lz compressors 
algorithms achieve significant reduction adapting regularities traces means effective compressors 
simple efficient compression algorithms domain specific compression algorithms achieve tighter compression lz algorithms 
regularities data domain known compression needed domain specific compression perform better adaptive general purpose compression algorithm 
lossless techniques reconstruct trace accurately purposes 
compression ratios achieved high possible lossy trace reduction 
importantly traces need uncompressed simulation performed 
reduction gains lossless compression translate simulation speedups 
lossy reduction techniques performing trace reduction usually knowledge uses trace 
lossy trace reduction techniques attempt exploit knowledge trace size reduced dramatically information maintained intended uses 
simplest lossy reduction technique blocking ah 
blocking replaces individual addresses memory pages 
subsequent addresses page reduced single 
reduction affect simulation time independent paging algorithms algorithms consider exact time making replacement decisions 
algorithms lru opt instance working set den 
blocking widely applicable practically assumed simulation 
remainder refer original trace referring blocked trace 
blocking interesting exploiting different kind regularity reduction techniques 
lossy concentrate temporal locality program trace blocking exploits spatial locality results extra significant factor reduction 
trace reduction methods kinds trace sampling trace stripping see 
better suited high speed hardware cache simulations introduce inaccuracy fully associative virtual memory policy simulations 
remaining types lossy trace reduction methods oriented virtual memory simulations 
techniques address concerns algorithms directly comparable 
section discusses related reduction techniques detail 
sad opt lossy techniques area relevant comparison new methods 
value techniques approach fills prominent gap spectrum trace reduction techniques 
existing techniques guarantee accurate simulations achieve high reduction factors method 
isolate approaches stand particularly related 
ffl smith stack deletion sd smi consists keeping cause pages fetched page lru memory 
sd directly comparable sad algorithm 
techniques simple similar preconditions reduction memory size chosen trace reduced determines minimum simulation memory size results accurate 
sad guarantees error introduced lru opt simulations sd guarantee exact results replacement policy 
smith experimentally demonstrated error introduced sd small 
error small reduction memory size smaller simulated memory typically size 
sad larger reduction memory yield greater reduction achieve exact results 
additionally show sd introduces larger error sad olr clock segq simulations reduced traces size 
sad olr safer introduce error effective yield smaller traces useful comparable purposes sd 
ffl technique coffman randell cr seen alternative sad olr lru simulations 
approach consists lru behavior sequence sequence pages fetched evicted page lru memory perform exact simulations lru memories smaller pages 
behavior sequence typically short small values biggest drawback coffman randell approach product reduction trace 
best case simulator tools trace browsers need change accept new format 
practical burden simulator implementors hard distribute traces compatible form 
main reason simple technique widespread 
olr algorithm complementary approach coffman randell offers efficient way turn behavior sequence format shortest possible trace exhibiting lru behavior 
advantages algorithms exist 
instance sad applicable opt simulations show sad olr introduce little error simulations clock segq 
ffl just techniques reduction method glass cao gc applicable exact virtual memory simulations policies 
coffman randell method glass cao technique suffers need modify simulator accept reduced trace format trace 
modifications far trivial hard reduced trace information simulations policies studied gc lru opt seq experimental replacement algorithm exactly simulated reduction method 
drawback technique lack control interesting memory ranges 
possible specify directly memory sizes simulation exact 
trace filter allows indirect control minimum memory sizes simulation valid worse minimum size determined trace gathered 
method efficient approach lru simulations 
access traces glass cao unreduced form able derive olr reduced form traces directly glass cao reduced traces 
olr reduced traces times shorter reduced form glass cao terms absolute size terms significant events 
detailed results comparison section 
applications algorithms possible 
optimality properties olr ideal purposes trace analysis 
provides estimate amount reordering done inside lru memory 
useful evaluating trace behave similarly lru lru approximations clock segq implementations 
possible application olr trace synthesis 
exact sequence fetched evicted pages lru memory olr produce minimum length trace cause fetches evictions 
provide alternative statistical trace synthesis techniques bab 
mention techniques complementary reduction algorithms exploit different principles 
output algorithms trace trace reduction techniques applied jh ah 
see simple file compression reduced traces gzip utility yields smaller files decreasing storage requirements 
algorithms safely allowed drop sad full traces commonly contain large number ignored virtual memory replacement policies 
account majority space required store trace consume majority time required perform virtual memory simulation 
safely allowed drop sad removes trace affect order fetches evictions lru memory user specified size 
show sad allows exact simulations lru opt 
show section introduces little error simulation lru approximations clock segq 
finding drop recall chapter defined lru distance number pages referenced page 
idea sad simple page trace lru distance third removing middle affect outcome lru opt simulations memories size greater section describes elimination middle effect lru opt 
sad application observation 
user specifies reduction memory size sad searches trace left right search triplets form page lru distance third middle triplets eliminated 
shows page lru distance second third 

second eliminated lru distance third reduction memory size pages 
third third distinct pages referenced third memory size chosen reduction safely drop second affecting results lru opt simulation 
nearly programs frequently pages 
due temporal locality eliminated sad constitute vast majority usual program traces small reduction memories 
sad algorithm implementation sad needs determine lru distances pairs page order find middle eliminated 
search proceeds left right allowing reduction performed single forward traversal original trace 
trace processed algorithm maintains lru queue requested size 
stores input original trace 
keeping lru queue history algorithm find groups page lru distance third reduction memory size 
information find middle eliminated 
necessary store find triplets number bounded 
necessary store order find lru distance third page 
specifically implementation needs store pages page lru queue plus third pages triplet 
triplet middle eliminated page stored 
triplets possibly pages need stored 
hash table help find pages performing reduction little augmented lru queue simulation executed efficiently 
exact simulation lru opt sad reduces trace page reduction memory reduced trace exact simulation lru opt memories pages 
exact lru simulation 
consider lru queue unbounded length contents unreduced reduced trace 
dropping sad allows pages drift away front lru queue page referenced 
pages guaranteed positions queue eliminated page lru distance previous 
pages adversely affected removing 
position lru queue closer top reduced trace original 
positions queue may different contents reduced traces ones results lru simulations memories size larger identical reduced unreduced trace 
illustrate argument examining 
memory size larger remain memory third middle second effect lru replacement dropped third ensure incorrectly evicted 
exact opt simulation 
consider 
opt choose page eviction selects resident page referenced furthest 
show case case removal second affect replacement decisions opt ffl opt processing removal second affect eviction choices opt determine evicted 
ffl opt processing third know fewer distinct pages referenced note page currently referenced memory caused replacement candidate eviction making number distinct referenced pages preceding third gamma 
memory size page referenced furthest third 
absence second affect replacement decision 
ffl opt processing follow third page affect decisions 
opt examines decisions missing second effect 
optimal lru reduction olr sad algorithm obtains significant reduction factors actual traces 
sad reduced traces necessarily smallest lru opt simulations exact 
instance consider sequence applying sad reduction memory pages trace yields reduction 
shorter trace exactly lru behavior original memory size larger 
recall lru behavior trace memory size sequence pairs pages fetched evicted memory trace applied 
lru behavior traces memory size ha nf hb nf hc nf hd bi hb ci special value nf denotes memory full insertion element cause eviction 
importance lru virtual memory systems motivated design olr algorithm computing optimally short sequences 
olr takes trace input outputs smallest trace lru behavior input memory size larger 
output olr function behavior different input traces exhibiting behavior lru memory size produce output 
step olr simulate input trace lru memory size derive behavior sequence 
sequence input olr core algorithm shown 
common terminology term block synonym page touch synonym 
explanation conventions followed algorithm description necessary input sequence behavior coffman randell lru behavior olr core behavior current fetch prev evict 
denotes empty lru size behavior current touch blocks behavior current evict done false behavior done behavior evict fetch done true prev evict behavior evict produce ref behavior evict touch touch prev evict behavior evict fetch fetch touch produce ref produce ref behavior current fetch fetch fetch current current current produce ref block touch block output block olr core accepts sequence fetched evicted pages page lru memory produces shortest trace cause behavior memory 
sequence represented array simplicity 
special value signals sequence 
olr core uses data structure lru queue augmented operations ffl blocks block returns set blocks touched block data structure distinct blocks touched 
block special value nf returned set empty useful uniform treatment boundary case structure filled 
ffl block block returns boolean value indicating block touched block block special value block special value nf false returned 
discuss algorithm intuitive level hope convey insights development 
proof correctness olr core algorithm sma 
recall olr core takes input sequence representing behavior trace lru memory size behavior sequence consists pairs fetched evicted pages lru memory 
sequence fetched pages subsequence output olr core causes fetch operation remain reduced trace sequence fetches preserved 
purpose olr core find minimum set extra need added sequence evictions preserved 
reduced trace causes fetch eviction specified behavior sequence causes reordering simulated lru queue fetch cause correct eviction 
point algorithm execution data structure reflects contents lru queue size output point applied 
iteration outer loop algorithm lines current index points successive elements input behavior sequence 
iteration outer loop causes fetch output line pages lru queue referenced corresponding expected evicted page touched 
set touch contains exactly pages line lines ensure get touched 
way causing fetch touched page lru queue corresponding evicted page indicated input 
describe essence olr core useful briefly review simple algorithmic problem lru queue desired recency ordering pages currently queue fewest page required reorganize queue desired ordering holds 
show simple algorithm produce minimal sequence examine desired ordering inverse order element 
find page order queue previous page desired ordering 
produce page subsequent pages desired ordering 
exactly algorithm implemented inner loop olr core algorithm lines 
lru queue modified desired ordering described evicted pages smallest subsequence behavior begins position current contains events page fetched lru queue page evicted 
condition detected fetch set algorithm contains pages fetched position current behavior sequence 
variable lookahead index elements behavior position current 
evicted part element fetch line inner loop ends 
short essence olr core algorithm page lru queue possible extra pages lru queue produced recency order queue matches expected eviction order point page evicted fetched 
intuitively reason condition necessary entire lru queue need reorganized page need touched points page fetched page evicted 
benefit achieved looking eviction pages fetched 
reordering done pages reordered referenced evicted 
note olr core efficient main component running time olr lru simulation performed input trace derive behavior sequence sma 
olr execution fast simple lru simulation input trace memory size algorithm performs just single forward pass bounded look ahead elements ideal online applications 
trace manipulation issues discussion sad olr simplified form traces containing address information page referenced 
real trace formats may need contain information operation required instruction read write operation exact instruction causing operation program counter timer information additionally trace may need re blocked experiments conducted different page sizes 
standard trace manipulation perfectly compatible sad olr 
instance ffl re blocking reduced trace reduction memory size larger page size simulations continue accurate memories size larger note size refers number pages actual minimum memory size kbytes simulations exact larger re blocking 
consequence stack algorithm properties lru opt 
ffl maintaining information virtual memory studies measure cost writing dirty pages backing store eviction 
studies require traces marked read write operation 
sad olr augmented tag appropriate operation 
order maintain information page reduced traces reduction methods notice pages modified write operation page lru memory 
methods maintain memory reduction implementation record page memory 
page reduction memory page evicted marked write operation 
simulation reduced trace mark page dirty evicted page larger memory 
ffl maintaining timing information timing information trivial maintain sad algorithm removes original trace 
olr reordering may occur sense keep time information causing page fetched memory 
guaranteed exactly order original trace 
experimental results applied trace reduction methods traces collected windows nt unix platforms 
windows nt traces include full set commercially distributed traces gathered utility etch 
include known windows nt applications acrobat reader netscape photoshop powerpoint word various programs cc compress go vortex 
unix traces espresso gcc grobner ghostscript lindsay gathered portable tracing tool user level page protection traces freely available web site 
windows nt traces blocked kbyte pages appropriate virtual memory simulations 
unix traces generated kbyte pages 
section show reduction factors achieved range reduction memory sizes 
reduced traces simulate clock segq replacement policies 
policies simulated exactly reduced traces show error introduced simulation small practice 
show error introduced significantly stack deletion smi known reduction method 
chose simulate clock segq replacement policies real systems 
approximations lru similar replacement policies discard information pages 
reduction results traces reduced sad olr range reduction memory sizes 
recall original traces blocked kbyte pages hundreds mbytes gbytes 
measured number bytes required store original trace reduced traces 
traces text representation virtual memory page number hexadecimal record comprises usually exactly bytes 
direct correspondence number bytes number records trace 
plots figures show reductions achieved sad olr fifteen original traces 
curves shown plot reduction ratio achieved function increasing reduction memory size 
chose show reduction results original traces platform remaining programs show similar increase reduction memory size equally high reduction factors 
note reduction factors increase quickly memory size grows 
reduction achieved particular reduction memory size direct result locality exhibited traced program 
vast majority pages small reduction memory yield large benefits 
note size olr reduced trace measure program locality smallest trace lru behavior original memory large reduction memory 
virtual memory systems simulate hundreds thousands pages traces hundreds times smaller appropriate experimental studies 
reduced trace allow researcher perform simulations quickly simulation time usually proportional length input trace 
note sad achieves reduction factors close olr 
sad simpler algorithm provides nearly optimal reduction allowing exact opt simulation exact lru simulation 
hard tell plots high reduction ratios achieved factor reduction original reduced reduction memory size pages gcc reduction sad olr sad olr reduction factors reduction memory sizes cc 
factor reduction original reduced reduction memory size pages ghostscript reduction sad olr sad olr reduction factors reduction memory sizes ghostscript 
factor reduction original reduced reduction memory size pages lindsay reduction sad olr sad olr reduction factors reduction memory sizes lindsay 
factor reduction original reduced reduction memory size pages acrobat reader reduction sad olr sad olr reduction factors reduction memory sizes acrobat reader 
factor reduction original reduced reduction memory size pages netscape reduction sad olr sad olr reduction factors reduction memory sizes netscape navigator 
factor reduction original reduced reduction memory size pages word reduction sad olr sad olr reduction factors reduction memory sizes microsoft word 
small reduction memory sizes 
show table sad olr perform small reduction memories pages windows nt plots pages unix plots windows nt programs larger footprints 
sizes chosen show significant reduction achieved reduction memory smaller smallest desired simulation memory 
trace reduction reduction memory ratio size sad olr cc compress go netscape photoshop powerpoint vortex espresso gcc grobner ghostscript lindsay table small reduction memories significant reduction factors achieved 
worth noting reduced traces compressed applying lossless trace reduction techniques instance jh sam 
experiment methods sad olr reduced traces highly compressible standard text compression tools 
table shows compression factors achieved unix gzip utility reduced traces ratios shown reduced trace size divided compressed reduced trace size 
results representative reduction memory sizes 
reduction memories larger reduced traces dramatically smaller compression factors shrink 
eventually compression ratios approach ratio largely artifact representing text 
traces thousands times smaller originals storage requirements negligible 
trace reduction gzip compression memory ratio size sad olr cc compress go netscape photoshop powerpoint vortex espresso gcc grobner ghostscript lindsay table reduced traces highly compressible 
clock segq simulations simulated clock segq replacement policies traces reduced sad olr smith sd method 
results simulations compared simulations original unreduced traces 
chose policies common similar page replacement policy practice 
approximations lru recency information tends excellent predictor patterns 
discard information referenced pages hardware available machines 
hardware bits supported clock 
segq designed machines hardware support allowed efficient recency page replacement 
clock simulator simulated single hand bit implementation 
interestingly reduced traces removed high frequency timescale information causes kind basic clock implementation suffer timescale relativity problems mentioned section 
simulated segq replacement policy original reduced traces 
simulations fixed size fifo queue examined results lru queues possible sizes 
error introduced sad olr sad olr perform exact simulations clock segq little error introduced simulation ratio simulation memory reduction memory sufficiently large 
error defined absolute value difference number page faults incurred unreduced reduced traces 
practice ratio yields uniformly low error 
ratio yields small error majority memory sizes introduced unacceptable error excess large lead erroneous due inaccurate results 
programs occupy small footprint yielded largest errors programs large footprint suffered smallest errors 
larger ratio better results 
summarize results simulations source traces observations 
graphical depictions error introduced selected reduction memory sizes figures methods compared sd 
observations valid simulations simulated memory reduction memory ratio paging events occur virtual memory study traces causing fewer faults 
ffl vast majority memory sizes error observed reduction methods 
ffl clock sad introduced error 
olr performed slightly worse sad average isolated cases exhibited nearly error 
ffl segq olr sad exhibited error 
olr performed sad average 
ffl reduced traces caused misses 
pattern bias reduction increase numbers 
ffl smaller footprint program larger observed error 
programs larger footprints hundreds pages error near zero memory sizes 
crucial note ratio simulation memory size reduction memory size large effect error introduced 
virtual memory studies simulated memories sizes hundreds thousands pages 
shown large reduction factors achieved reduction memories sizes tens pages 
possible produce significantly reduced traces ratio allow acceptably accurate simulations 
comparison stack deletion referenced form trace reduction smith stack deletion sd smi 
interesting compare sd reduction techniques value demonstrated exclusively experimental arguments 
sd guarantee exact simulations shown introduce small error simulation replacement policies lru opt clock 
compared sad olr sd suite fifteen traces techniques particularly sad consistently yield smaller error 
reduction method chose reduction memory size yield reduced trace times smaller original traces methods approximately size 
alternative reduction memory size method 
unfair sd keeps information olr sad 
clock results 
performed clock segq simulations reduced trace 
subset clock results shown figures 
plots show percent error difference number page faults introduced sad olr sd clock simulations 
traces go grobner chosen represent programs small footprint pages respectively 
programs gcc ghostscript occupy medium sized footprints pages 
acrobat reader netscape larger footprint programs pages respectively 
programs sad olr match exceed accuracy provided sd 
average olr introduces error sad sd 
sad provides smaller error sd memory size 
percent error number faults simulated memory size pages grobner clock stack deletion pages sad pages olr pages absolute percent error number faults introduced clock simulations grobner reduced traces sad sd 
notice reduction memory sizes chosen reduced trace approximately times smaller original 
percent error number faults simulated memory size pages go clock stack deletion pages sad pages olr pages absolute percent error number faults introduced clock simulations go reduced traces sad sd 
percent error number faults simulated memory size pages gcc clock stack deletion pages sad pages olr pages absolute percent error number faults introduced clock simulations cc reduced traces sad sd 
percent error number faults simulated memory size pages ghostscript clock stack deletion pages sad pages olr pages absolute percent error number faults introduced clock simulations ghostscript reduced traces sad sd 
percent error number faults simulated memory size pages acrobat reader clock stack deletion pages sad pages olr pages absolute percent error number faults introduced clock simulations acrobat reader reduced traces sad sd 
percent error number faults simulated memory size pages netscape clock stack deletion pages sad pages olr pages absolute percent error number faults introduced clock simulations netscape navigator reduced traces sad sd 
note leftmost portion plot contains large error simulations memories comparable reduction memory size inaccurate 
plots error drops significantly ratio simulated memory size reduction memory size 
cases sd reaches reasonable level error slightly smaller memory size sad olr smaller reduction memory size 
smith claimed ratio sufficient experimentation sd 
ratio significant error specific cases 
example sd introduces error simulation grobner ratio 
introduces error go ratio 
sad olr suffer unacceptably large error ratios isolated cases 
consequently recommend ratio uniformly negligible error 
segq results 
segq simulations results similar error introduced average reduction methods 
selected traces original fifteen results similar rest 
note error introduced irregular behavior segq dominated fifo stack algorithm produce unpredictably different results slightly different memory sizes 
plots shown size fifo segment fixed percent error shown increasing lru segment sizes 
note plots show results olr performed similarly sad inclusion noisy plots difficult read 
fifo segment size chosen plots approximately twice reduction size sad 
total simulated segmented queue memory twice large reduction memory reduction methods 
programs sad sd introduce sufficiently small error acceptable studies 
sad introduces error memory sizes percent error number faults simulated memory size pages acrobat reader segmented queue stack deletion pages sad pages absolute percent error number faults introduced segq simulations acrobat reader reduced traces sad sd 
plot fifo segment size fixed axis shows total memory size obtained combining fifo lru segment 
percent error number faults simulated memory size pages gcc segmented queue stack deletion pages sad pages absolute percent error number faults introduced segq simulations cc reduced traces sad sd 
percent error number faults simulated memory size pages go segmented queue stack deletion pages sad pages absolute percent error number faults introduced segq simulations go reduced traces sad sd 
percent error number faults simulated memory size pages ghostscript segmented queue stack deletion pages sad pages absolute percent error number faults introduced segq simulations ghostscript reduced traces sad sd 
percent error number faults simulated memory size pages powerpoint segmented queue stack deletion pages sad pages absolute percent error number faults introduced segq simulations powerpoint reduced traces sad sd 
percent error number faults simulated memory size pages word segmented queue stack deletion pages sad pages absolute percent error number faults introduced segq simulations microsoft word reduced traces sad sd 
memory sizes sd better method 
shown olr performed better segq clock performance comparable sad 
traces error introduced method significant 
sad olr preferable sd practice 
introduce small error simulations sd introduces slightly average sad consistently introduces 
sad olr allow exact simulation lru lru variants flw seq gc fbr rd eelru sad allows exact simulation opt 
comparison glass cao technique limited comparison reduction technique method glass cao 
access unreduced form traces gc possible reduce sad 
manage obtain olr reduced traces memory sizes threshold accurate lru simulation 
mentioned section reduction technique glass cao allow select memory range simulations exact 
sampling granularity reduction arbitrarily determine minimum memory size lru opt simulations possible see tables 
obtain olr reduced traces minimum lru simulatable size size greater simple source line change lru simulator glass cao caused behavior sequence lru memory output 
olr construct smallest trace exhibiting behavior memory size 
tables compare size olr reduced trace reduction memory size equal minimum simulatable lru size program min 
simulatable trace size ratio mem 
size kb events glca olr lru opt glca olr applu blizzard coral gcc gnuplot ijpeg ksim murphi perl swim turb vortex wave table reduction traces gc 
ratios number events traces reduced olr reduction technique glass cao glca shown 
ratio means glca trace times larger 
program min 
simulatable trace size ratio mem 
size kb kb glca olr lru opt glca olr applu blizzard coral gcc gnuplot ijpeg ksim murphi perl swim turb vortex wave table reduction traces gc 
ratios trace file size traces reduced olr reduction technique glass cao glca shown 
ratio means glca trace times larger 
program min 
simulatable trace size ratio mem 
size kb kb glca olr lru opt glca olr applu blizzard coral gcc gnuplot ijpeg ksim murphi perl swim turb vortex wave table reduction traces gc 
ratios compressed trace file size traces reduced olr reduction technique glass cao glca shown 
ratio means glca trace times larger 
reduced trace glass cao 
traces different format implementation olr uses simple text format traces glass cao memory images possible ways compare 
metric shown table events trace 
close possible apples apples comparison 
possible error favor glass cao technique olr events strictly smaller events glass cao representation contains information olr extra ordering information 
second metric table shows size reduced trace file kbytes 
useful see traces reduced olr smaller raw uncompressed representation metric probably informative 
choice representation significant effect file size reduced trace formats differ significantly respect 
order reduce effect differing representations compressed reduced trace files gzip utility 
results compressions shown table 
compressed encoding redundant information largely removed olr removed information trace providing simulations 
storing processing long memory traces costly 
proposed sad olr new methods drastically reducing traces alleviate storage processing requirements 
reduction methods designed eliminate information pages 
allow exact simulation lru memories minimum size chosen explicitly user 
sad allows exact simulation opt memories 
sad olr invaluable realistic virtual memory studies 
studied virtual memory policies variants approximations lru 
traces reduced sad olr provide accurate simulations lru variants memories larger user defined threshold 
additionally shown reduced traces introduce little error commonly lru approximations clock segq 
chapter adaptive caching overview decades lru dominant replacement policy approximated variety contexts 
lru shown empirically typically modest constant factor optimal number misses fixed memory size 
lru exhibits known failure mode shown section 
particular performance lru suffers regular access patterns larger size main memory 
lru performs badly loop access patterns 
kinds patterns common real programs 
instance consider roughly cyclic loop pattern accesses modestly pages fit memory 
cyclic patterns induced loop program execution runtime system garbage collector reuses memory 
pages touched cyclically fit main memory lru evict ones touched longest time exactly ones touched 
way look phenomenon lru keeps page memory long time keep long 
lru keeps pages memory long prediction incorrect 
lru assumes longer page unused longer 
case longer page unused sooner 
lru notion incorrect 
contrast optimal algorithm evict pages shortly touched iteration loop 
evicting pages early allows pages remain memory looped 
optimal algorithm large cyclic patterns unfair particularly noteworthy pages chosen early eviction relative lru 
pages simply sacrificed fairness disaster situation 
eelru deviates lru lru performs badly 
observations form intuition early eviction lru eelru 
eelru performs lru replacement default diverges lru evicting pages early notices pages touched roughly cyclic pattern larger main memory 
patterns reliably detected recency information 
kind information maintained lru eelru maintains resident non resident pages 
particular eelru detects lru performs fetched pages evicted lately 
bounds eelru vs lru 
behavior eelru provides interesting guarantee performance relative lru 
number misses incurred eelru constant factor higher lru constant factor depends parameters picked eelru 
reason bound eelru deviates lru incurs faults reverts back lru soon eelru starts incurring faults 
words eelru performing poorly quickly return lru behavior 
lru contrast perform worse eelru factor proportional size memory worst case factor usually thousands 
guaranteed property eelru interesting ubiquity lru approximations segmented queue tl bf commonality lru worst case pattern simple large loop practice 
eelru designed timescale relative 
additionally eelru firmly distinct principle program locality studies timescale relativity see section 
timescale relativity comes play real programs exhibit strong phase behavior 
eelru tries adapt phase changes assigning weight events past respect virtual memory time 
eelru designed track virtual memory time relevant events occur 
relevant events pages touched 
intuitively eelru ignores high frequency influence replacement decisions may dilute virtual memory time important regularities impossible distinguish 
timescale relativity time eelru advances rate inversely proportional memory size 
larger memories proportionally larger number relevant events occur algorithm decide adapt behavior 
reason choice timescale relevant caching decisions eviction decisions turn bad roughly time page spend memory touched evicted 
eviction replacement policy attempt choose page touched soon specifically pages touched pages fit memory 
timescale crucial adaptation online adaptive replacement policy detect program behaviors long matter choice caching strategies 
argue timescale relativity represents sound principle design replacement policy 
propose special kind plot termed recency graph appropriate studying program locality behavior section 
experimental evaluation 
validate eelru experimentally applied fourteen program traces studied performance 
traces memory intensive applications come experiments glass cao gc 
glass cao traces evaluate seq adaptive replacement algorithm attempts detect linear recency address terms faulting patterns 
set traces contains representatives trace categories identified gc traces large memory requirements clear memory access patterns small access patterns large access patterns 
extra traces collected representatives applications memory intensive may small scale patterns 
results evaluation encouraging 
eelru performed lru situations significantly better 
results fewer faults compared lru common wide range memory sizes applications large scale patterns 
comparison seq algorithm gc instructive seq detecting patterns address space eelru detects patterns recency distribution 
simulation quite conservative see section eelru managed obtain significant benefit traces seq 
hand seq aggressive algorithm performed better programs clear linear access patterns address space 
cases eelru captured large part available benefit 
eelru simple soundly motivated effective replacement algorithm 
representative approach studying program behavior recency timescale relativity proves quite promising 
motivation related main purpose section compare contrast approach taken eelru replacement policies 
help illustrate rationale design choices eelru 
eelru uses recency information distinguish pages 
standpoint see spi flw wfl dictates way differentiate pages examining past history regard information pages proximity address space 
view ensures looping patterns different kinds treated 
note access patterns cause lru page excessively necessarily correspond linear patterns memory address space 
instance loop may accessing records connected linked list binary tree 
case accesses regular repeated addresses pages touched may follow linear pattern 
interesting regularities necessarily appear memory arrangements pages touched past 
seq replacement algorithm gc bases decisions address information detecting sequential address patterns 
consequently lacking generality detect loops linked lists connecting interspersed pages 
section compares eelru seq extensively 
eelru principle timescale relativity helps detect adapt phase changes 
application timescale relativity eelru determining time advances slower rate larger memories equivalently length constitutes phase program behavior proportional memory size examined 
idea means new 
fact commonplace pieces theoretical paging st tor execution decomposed phases working sets size equal memory 
second application timescale relativity eelru dictates events matter replacement decisions count advance time 
past replacement policies ideas yielded results affected events wrong timescale 
example inter gap irg model developed pha 
irg attempts predict soon pages referenced looking time successive past 
simpler version idea known atlas loop detector bfh examines successive 
loop detector fails time measured number memory performed prediction time virtual memory time 
loop repeats significant variation iteration time successive may vary 
variance unusual loops may perform different numbers operations step different iterations instance case nested loop patterns 
atlas loop detector fail recognize regularity 
complex higher order irg models studied detect significantly regularities presence variation 
complexity prohibitive actual implementations 
contrast consider possible timescale relative approach loop detection time page iteration measured number pages 
metric detection loops simple matter pages aged amount recency terms 
combined response aggregate behavior specific distances page kind approach easily tolerate variation loop 
view recency timescale relativity applied literature 
replacement policies deals specific formal models program behavior 
eelru inspired lru stack model spi discuss section 
independent events model events pages identified recency number pages touched touch page 
optimal replacement algorithm wood fernandez lang wfl 
unfortunately programs modeled accurately independent recency events 
hand short program phases modeled closely 
line recency algorithm needs adaptive detect phase changes 
timescale relativity eelru crucial providing adaptivity reliably 
known models class markov models cv fg 
straightforward case th order markov model corresponds known independent model irm adu 
optimal replacement algorithm markov models kpr 
believe replacement algorithms markov models fail practice try solve far harder problem hand 
replacement algorithm trying select page soon discussed section 
markov models express probability occurrence specific sequences 
just trying predict page soon markov models trying predict order pages 
view markov models attempt predict program behavior timescale virtual memory timescale 
markov model replacement brittle actual realistic models offer accuracy large timescale memory replacement decisions 
eelru viewed way negate partially stated assertion limits actual eviction algorithms 
quoting tor stated way guaranteed performance deterministic line algorithm degrades sharply intrinsic working set size access sequence increases memory size performance optimal line algorithm degrades gracefully intrinsic working set size access sequence increases memory size 
assertion holds theoretical worst case analysis practice program sequences exhibit strong simple regularities line algorithm exploit 
eelru example approach adds lru ability gracefully degrade performance large working sets patterns roughly cyclic 
eelru algorithm general idea eelru algorithm structure 
early eviction lru eelru algorithm reasonably simple structure 
perform lru replacement past pages fetched just evicted 

past pages fetched just evicted apply fallback algorithm evict page evict th page pre determined recency position 
early eviction point main memory size eviction points potential mru page late region early region region lru memory general eelru scheme lru axis correspondence memory locations 
turn idea concrete algorithm need define notions highlighted exact fallback algorithm 
changing aspects obtain family eelru algorithms different characteristics 
discuss single fallback algorithm particularly simple sound theoretical motivation 
algorithm described section 
section describe basic flavor eelru approach remains regardless actual fallback algorithm 
recency information adaptivity 
presents main elements eelru schematically showing recency axis called lru axis potential eviction points 
recency axis discrete axis point represents th accessed page written 
see eelru distinguishes regions recency axis 
lru memory region consists blocks main memory 
note name may slightly misleading lru region holds blocks 
name comes fact part buffer handled regular lru queue 
position lru axis called early eviction point 
region early eviction point memory size called early region 
late region begins point extent determined fallback algorithm see section 
recall page fault time eelru evict page page point recency axis th page 
called early eviction purpose keep pages memory little longer hope soon referenced 
challenge eelru adapt changes program behavior decide reliably approaches best occasion 
eelru maintains queue referenced pages ordered recency way plain lru 
difference eelru queue contains records pages main memory evicted 
eelru keeps total number page early late regions recency queue 
information enables cost benefit analysis expected number misses fallback algorithm incur avoid 
essence algorithm assumption program recency behavior remain near compares page faults incur performed lru replacement incur evicted pages early fallback algorithm 
sketch cost benefit analysis performed means example 
consider shows recency distribution phase program behavior 
shows position recency axis hits pages position occurred lately 
distribution changes time remains fairly constant separate phases program behavior 
eelru adaptivity mechanism meant detect exactly phase changes 
distribution monotonically decreasing lru best choice replacement 
large loops cause distribution hits late region early region 
kind distribution encourages eelru sacrifice pages order allow evictions pages memory pages memory example recency distribution page touches 
lru pages memory 
evicting early pages stay memory 
pages common example distribution evicting early yields benefits 
stay memory longer 
eelru starts evicting pages early eventually hits late region pages cached 
timescale relative adaptivity phase behavior 
eelru algorithm attempt exploit recency information eviction decisions see flw 
strength adaptively succeeds detecting changes program phase behavior 
key detecting phase behavior cost benefit analysis eelru weight assigned events past 
discussed section decaying information provided past events virtual memory time critical part adapting timescale relative manner 
eelru advances virtual memory time detecting influence replacement decisions early late regions 
notice regions timescale appropriately determined memory size 
high frequency events hits referenced pages totally ignored eelru analysis 
reason allowing high frequency affect notion time information extent reliable analysis performed 
number memory may contain different numbers relevant events different phases program execution 
illustrate need adapt correct timescale consider happen allowed hits referenced pages affect statistics 
actual programs referenced pages pages lru distribution 
virtual memory time diluted extent cost benefit analysis completely unreliable 
phases frequent pages cause time advance quickly 
result statistics early late region occur decayed away 
adaptive mechanism attempts react early late region information quickly decays system may try react transient behavior doesn persist early late regions 
time advanced incorrect rate single constant bias bring timescale agreement virtual memory timescale patterns 
multiple early eviction points 
basic eelru idea straightforwardly generalized allowing instance scheme replacement policy 
words system eelru eviction policies choose best phase program behavior 
instance multiple early eviction points may exist events relevant point affect cost benefit analysis 
point yields highest expected benefit determine page replaced 
efficient implementation 
simplicity general eelru scheme allows efficient implementations 
provided kernel version eelru speculate quite feasible 
particular eelru approximated techniques identical standard kernel lru approximations clock segq segmented queue tl bf 
policies introduce overhead pages remaining pages incur traps kernel 
maintaining recency information pages require additional processor cycles trap 
eelru needs recency information pages pages soon evicted early 
traps occurring recency information maintained number pages information gathered reduced 
expensive keep recency information pages poor choice evict early anyway recency information need gathered 
architectures contain hardware support clock policy basic recency information recorded pages 
invoking clock algorithm periodically interrupts extra recency information adaptive mechanisms 
matter recency information exact eelru simulator keep exact recency information recency information accesses various early late regions stored providing coarse grained recency 
concrete algorithm step producing concrete instance eelru choosing reasonable fallback algorithm 
turn guide cost benefit analysis exact distribution information needs maintained 
obvious candidate algorithm evicts th page 
equivalent applying mru replacement early region clearly captures intention caching pages 
real programs exhibit strong phase behavior den causes mru unstable pages may touched cached indefinitely 
algorithm wood fernandez lang flw wfl henceforth called wfl simple modification mru eliminates problem 
wfl replacement algorithm specifies parameters representing early late eviction point lru axis 
evictions performed early point doing means page late eviction point cached 
algorithm written simply wfl algorithm called generalized lru creators 
avoid confusion acronym subsequently overloaded mean global lru 
potential eviction point potential eviction point early eviction point main memory size lru memory region late eviction point mru page eelru wfl fallback lru axis correspondence memory locations 
memory fault accessed page evict page evict page early late eviction points 
shows elements wfl algorithm schematically 
making wfl adaptive 
shown exist values wfl algorithm optimal lru stack model program behavior wfl spi 
program phase behavior welldefined long lasting phases cause algorithm perform 
surprising wfl adaptive algorithm 
presumes optimal early late points chosen advanced knowledge recency distribution 
adaptivity provided eelru crucial transforms wfl line replacement algorithm 
particularly true multiple pairs early late eviction points exist eelru chooses yielding benefit 
entire programs modeled accurately lru stack model shorter phases program behavior closely approximated 
assumptions model wfl algorithm additional advantage lru memory region mru page early eviction main memory size late eviction point point probability memory page recency 
simplifying cost benefit analysis 
properties wfl algorithm reaches steady state probability th accessed page page memory gamma gamma probability distribution shown 
applying cost benefit calculation 
simplified approach calculating cost benefit estimate number faults wfl incur steady state compare number faults lru incur 
call total number hits pages recency order 
similarly call early number hits pages eviction algorithm total delta gamma gamma early memory fault accessed page evict accessed page evict page consider obvious generalization algorithm instances wfl different values active parallel 
total early denote total early values th instance algorithm 
instance wfl decide page evicted maximizes expected benefit value total delta gamma gamma gamma early values negative plain lru eviction performed 
note case multiple early late eviction points eelru adaptivity performs dual role 
hand produces line estimates values algorithm performs optimally 
plain lru case values 
hand adaptivity detects phase transitions changing values accordingly 
case multiple early late eviction points modification basic wfl algorithm sense 
late eviction points equal possible th instance wfl called evict page page memory case algorithm evict pages guarantee steady state pages referenced memory 
note modification basic wfl algorithm affect steady state behavior consequently proof optimality lru stack model wfl 
change account final eviction algorithm benefit maximum values total delta gamma gamma gamma early index value occurs benefit page memory memory fault accessed page evict accessed page evict page form eelru experiments described 
eelru vs lru interesting property eelru robust respect lru worst case analysis 
particular eelru perform constant factor worse lru lru perform worse eelru factor proportional number memory pages 
exact values factors depend parameters chosen eelru algorithm number positions early late eviction points speed adaptation 
offer rigorous proof claim argument straightforward 
eelru diverges lru incurred faults past 
eelru reverts back lru detects lru performed better 
complete sequence lru better eelru lru incurred faults eelru choose fallback algorithm just faults cease occur lru 
words lru fault evicted pages bursts just rate required mislead eelru 
cases ratio rates algorithms worse constant 
conversely steady loop slightly larger memory late regions eelru cause lru suffer misses page eelru suffer constant number misses iteration 
illustrative example worst case analysis consider extreme case eelru single early eviction point position recency axis may evict early page corresponding late eviction point position memory size 
case steady loop pages cause lru suffer faults eelru suffer single fault iteration 
time eelru outperformed lru factor pattern 
decides diverge lru may incur faults lru fault causes divergence 
experimental assessment settings methodology test suite 
assess performance eelru fourteen program traces covering wide range memory access characteristics 
traces memory intensive applications experiments glass cao gc 
traces collected individually programs smaller footprints described fully section 
second set traces chosen show approach adaptivity applicable different memory sizes timescales tuning parameters 
limitations glass cao reduced traces 
traces gc half traces study 
rest experiments reproduced reduced trace format glass cao omitted information necessary accurate eelru simulation 
specifically consider behavior eelru point early evictions performed making algorithm replace page point lru axis 
trace information determine th accessed page 
trace sufficiently accurate lru simulation memory size reduced traces glass cao impose limitations minimum memory size lru simulation performed 
minimum simulatable memory size eelru reduced traces dictated earliest early point eelru consider 
traces trace limitation prevented simulating eelru memory ranges tested gc 
example cc trace glass cao study lru simulation memories larger pages see gc 
early eviction points simulator minimum eelru simulatable memory size pages outside memory range experiment 
trace causes faults memories larger pages 
reproduce experiments possible picked early eviction points memory managed strict lru 
limitation simulations quite conservative means eelru perform early non lru evictions 
mentioned earlier simulations traces meaningful choice points simulated memory ranges overlap significantly experiments performed glass cao 
table contains information traces 
worth noting set traces contains representatives program categories identified glass cao 
programs clear patterns murphi ksim programs small scale patterns perl traces gc es satisfy restrictions outlined available 
program description min 
simulatable lru memory kb pages applu solve coupled parabolic elliptic pdes gnuplot postscript graph generation ijpeg image conversion jpeg format ksim microprocessor cycle level simulator murphi protocol verifier perl interpreted scripting language tridiagonal matrix calculation wave plasma simulation table information traces gc programs large scale patterns applu gnuplot ijpeg wave 
traces gathered supply data points 
traces executions consume memory 
applications traced espresso circuit simulator gcc compiler ghostscript postscript engine grobner formula rewrite program lindsay communications simulator hypercube computer pascal translator 
simulation parameters 
simulations performed twelve pairs early late eviction points 
early points memory size memory constituted lru region respectively leaving early region 
late points chosen probability took values 
parameter affects simulation results significantly 
recall re placement decisions guided program behavior past 
give weight virtual memory events recency information needs decayed 
rate decay tied directly memory size values statistics multiplied weight factor progressively th memory size times weight 
algorithm sensitive respect value 
values yield identical results 
decaying information exponentially correct timescale obtain desired adaptivity large range constant decay factors 
locality analysis show memory characteristics traces plotted graphs 
graphs scatter plots map page corresponding position recency axis 
pages ignored resulting graphs maintain right amount information relevant timescale clear picture program locality 
instance consider recency graph wave trace plotted 
graph produced ignoring accessed pages 
taken account patterns see squeezed just small portion resulting plot high frequency information dominated patterns interest difficult impossible see 
graph observations 
wave exhibits strong large scale looping behavior 
loops accessing pages 
horizontal lines represent regular loops pages accessed order access touching number pages 
note steep upwards diagonals commonly represent pages lru position recency wave recency graph wave 
re accessed opposite order access 
advantages recency graphs 
lot information kind recency graphs giving graphs advantages usual space time plots see plots pha gc program locality analysis 
name ffl information relevant 
depicting page gets accessed recency graphs show page accessed touched 
ffl high frequency information hits accessed pages time space time graphs 
common interesting behavior respect faulting occurs small region space time graph 
information affect recency graphs easily filtered depending memory size timescale interest 
ffl time information may dominate space time graph allocations large structures 
information irrelevant paging analysis appear recency graph 
seeing behavior recency graphs 
figures recency graphs representative traces 
plots lead observations behavior programs ffl programs exhibit strong phase behavior recency terms 
access patterns exhibit clearly identified features commonly persist 
comparing values horizontal vertical axes gives estimate long features persist 
plots features commonly size pages 
lru position recency gnuplot recency graphs gnuplot 
lru position recency ksim recency graphs ksim 
lru position recency perl recency graphs perl 
lru position recency gcc recency graphs gcc 
ffl gnuplot graph exhibits strong large loops restricted narrow recency region 
gnuplot trace accessed pages filtered plot pages mark recency axis 
kind access pattern lru perform terribly pages memory available 
ffl ksim graph initially displays large loop note short horizontal line followed smoke feature area distinctive patterns 
initial loop simple linear loop little pages 
note lasts approximately number indicating just loop iterations 
smoke pattern characteristic sets pages accessed random uniform order 
pages set get touched pages recency axis touched move upward 
gradually accesses concentrate higher higher points recency axis size set limit 
feature part ksim graph represents random accesses large structure 
nature application speculate heavily hash table 
ffl perl graph exhibits smoke features steep diagonals recall represent pages re accessed opposite order 
recency graphs similar ones shown 
traces espresso gcc grobner ghostscript lindsay behave gcc having distinctive patterns 
murphi displays random ksim 
rest traces regular patterns 
case patterns clear linear patterns gnuplot 
cases ijpeg applu patterns look perl wave 
patterns eelru provides benefit 
recency graphs identify areas eelru exhibit clear benefit 
graph displays high intensity dark areas right light areas eelru able evict pages early keep memory needed soon 
comparing graphs results simulations sections shows case memory sizes eelru particularly successful near intensity boundaries 
experiment memory intensive applications plots figures show page faults incurred eelru lru opt seq memory intensive traces 
seq algorithm glass cao gc traces originally tested 
detailed analysis behavior lru relative opt traces gc 
restrict attention eelru 
eelru consistently performs better lru traces murphi eelru essentially performs lru replacement 
large part available benefit delineated opt curve captured applications exhibit clear patterns 
comparison seq algorithm quite instructive 
seq compares eelru 
idea seq detect access patterns observing linear faulting sequences linearity address space 
seq detecting address space patterns eelru detecting coarse grained recency space patterns 
approach offer distinct benefits results seq obtained running simulator glass cao traces 
testing seq traces require significant re engineering simulator replacement logic tied trace reduction format 
memory size kb pages applu lru opt eelru seq fault plot applu 
memory size kb pages gnuplot lru opt eelru seq fault plot gnuplot 
note seq curve overlaps opt curve 
memory size kb pages ijpeg lru opt eelru seq fault plot ijpeg 
memory size kb pages ksim lru opt eelru seq fault plot ksim 
memory size kb pages murphi lru opt eelru seq fault plot murphi 
note eelru curve overlaps lru curve 
memory size kb pages perl lru opt eelru seq fault plot perl 
memory size kb pages lru opt eelru seq fault plot 
memory size kb pages wave lru opt eelru seq fault plot wave 
ffl eelru capable detecting regularities seq capture 
instance linked list traversal may necessarily access pages address order clearly exhibit strong looping behavior 
traversals easily revealed recency information maintained eelru 
ffl seq detect linear patterns quickly eelru get possible benefit cases 
reason recency information available page re accessed second iteration loop address information immediately available 
observation consequences robustness algorithms eelru fairly conservative diverges lru case persistent patterns 
seq hand risky guesses traversals encountered presumes sequential scans pages larger main memory 
expect eelru cause harm programs large scale looping patterns 
results experiments agree analysis 
eelru outperformed seq traces applu ijpeg perl seq outperformed eelru gnuplot wave ksim murphi difference small eelru slightly better case slightly worse 
note seq performed better programs clear linear access patterns perform early loop detection 
programs eelru captured available benefit 
traces recency patterns exploited eelru consistently yielded improvements lru seq results indicate opportunities existed ijpeg perl 
memory size kb pages gnuplot lru opt eelru seq different fault plot gnuplot 
second simulation extra early eviction point available eelru 
conservative eelru simulation 
recall mentioned section simulation eelru traces comparison seq conservative memory buffer dedicated lru region 
limitation caused loss effectiveness especially traces clear access patterns 
confirm effect limitation gnuplot trace information enable meaningful simulations lru regions memory size 
memory dedicated early region early eviction point early recency terms 
added extra early eviction point result shown 
see performance eelru improved compared conservative simulations shown 
eelru approaches performance seq opt expected 
gnuplot trace exhibits regular behavior fact dominated big loop 
memory sizes small contain data needed loop eelru evict pages early pages remain cached long 
eelru desirable approach graceful performance degradation 
believe recency approach eelru simpler intuitively appealing general applicability address approaches seq 
generality conjecture course proven extensive experiments widely accepted representative workloads preliminary results experiments confirm 
belady anomaly 
eelru exhibit known belady anomaly increasing size physical memory may increase number page faults 
eelru suffers anomaly result adaptive behavior 
takes risks tries predict program behavior past patterns 
example decision diverge lru depends memory size 
larger memory size eelru may decide worth evicting pages early order capture loop significantly larger memory size 
loop persist long eelru incur faults larger size smaller memory size adaptive heuristic attempt evict pages early 
example risks taken eelru recency patterns steady recall diagonal lines plots section 
right early eviction point may immediately evident eelru starts diverging lru 
error eelru estimate different results different memory sizes depending positions late eviction points relative shape recency patterns 
second experiment small scale patterns second experiment applied eelru traces smaller memory requirements 
traces programs real virtual memory system page help demonstrate approach stable handles wide range available memories 
additionally traces confirm patterns eelru recognizes unique programs written paging mind 
ability adapt large scale small scale patterns useful algorithm employed replacement algorithm multi process environment 
short discussion potential eelru replacement algorithm time sharing systems section 
traces programs smaller footprints programs exhibit distinctive patterns see gcc recency plot expect eelru behave similarly lru 
performance confirms robustness algorithm eelru perform worse lru regular patterns exist 
seen fault plots memory size kb pages espresso lru opt eelru fault plot espresso 
memory size kb pages gcc lru opt eelru fault plot gcc 
memory size kb pages grobner lru opt eelru fault plot grobner 
memory size kb pages gs lru opt eelru fault plot ghostscript gs 
note eelru line overlaps lru line 
memory size kb pages lindsay lru opt eelru fault plot lindsay 
note eelru line overlaps lru line 
memory size kb pages lru opt eelru fault plot 
note eelru line overlaps lru line 
figures performance eelru lru nearly identical 
note traces eelru manages get small benefit compared lru faulting average 
particular eelru capable detecting exploiting small scale patterns 
examination gcc recency plot quite interesting 
see small regions high intensity dark areas directly low intensity light areas 
eelru exploiting exactly small scale patterns exhibits benefit memory size pages boundary dark light areas plot 
thoughts program behavior replacement algorithms lru serves point understanding performance replacement algorithms 
roughly divide program behavior classes 
lru friendly cases lru excellent replacement policy resemble lru 
specifically prefer cache touched pages vast majority time lru 
programs seldom touch pages fit memory fairly long periods time lru friendly programs 
regularity sufficiently strong lru nearly optimal replacement policies random fifo 
common case exhibits lru greatest strength 

lru unfriendly lru evicts pages shortly touched excellent replacement policy evict pages early attempt cache pages mru 
program behavior consistently exhibits kind regularity mru behavior nearly optimal 
relatively common case exhibits lru greatest weakness addressing properly main contribution research demonstrate simple deviation lru replacement sufficient substantially improve lru 

unfriendly programs touch pages touched long time pages evicted long ago lru unfriendly replacement policy opt 
lru default comparable opt case algorithm 

mixed catch category covers programs touch pages soon evicted lru pages lru evicted 
aggregate recency distribution particularly indicator replacement policy 
area 
notice mixed behavior nearly optimal replacement policy evict pages preference 
sufficient uniformly fair lru uniformly unfair mru 
finer distinctions recognizing subtle aspects program behavior address 
conversely notice cases fine distinctions unnecessary performance 
fine distinctions may harmful regularities strong little benefit gained attempting fine distinctions crucial information primarily aggregate behavior large numbers pages details page histories 
replacement algorithms important components operating system design affect system performance significantly 
eelru adaptive variant lru uses recency information pages memory replacement decisions 
believe eelru valuable replacement algorithm 
simple soundly motivated intuitively appealing general 
eelru addresses common lru failure modes insufficiently large memories remaining robust 
performance worse lru constant factor 
simulation results confirm belief value algorithm 
main axes experimentation examine eelru replacement algorithm time sharing systems cost performance kernel approximation eelru 
generally demonstrated recency information reveal behavior virtual memory system adapt 
information timescale relative adaptation respond patterns persist 
show chapter type calculation recency information adapt size compressed cache 
words collection timescale relative decay recency information provides system useful information number purposes 
interestingly possibilities recency approach replacement algorithm multi processing systems 
thing eelru useful global algorithm managing pages regardless process belong 
recency information kind maintained eelru help memory partitioning 
process incurs lot faults evicted pages replacement algorithm allocate memory process expense process smaller memory space cause faults 
ideas open possibilities quite sophisticated recency replacement algorithms 
eelru served demonstrate sound principles timescale relative adaptivity algorithms 
chapter memory data compression motivation background compression data relies ability find regularities data process known modeling bcw nel 
modeling performed encoding chosen uses fewer bits represent regularities detected 
modeling encoding phases contribute compression problem simpler 
understood approaches encoding problem provide reduced representation input data 
modeling input find regularities significant challenge data set 
modeling method may correspond data domain failing find regularities domain 
computational expense modeling method may quite acceptable domains 
popular lempel ziv lz compression algorithms designed find regularities byte streams specifically human text 
compressed caching need able compress small virtual memory pages large byte streams 
lz variants specifically lzo lzrw wil applied compressed caching possible devise compression algorithms better suited domain 
memory data exhibits different regularities byte streams 
regularities strong describe simple form modeling fundamentally different lz modeling effective virtual memory pages 
compressed caching places unique demands performance characteristics compression algorithms met lz algorithms address 
memory data vs text file regularities 
data stored memory program execution different byte streams stored file systems lossless compression research performed 
compression algorithms find repetitions literal byte sequences encode small number bits 
assumption models sequences bytes exact repetitions previously seen sequences bytes 
memory data may comprise byte sequences 
order model compress memory data effectively characterize virtual memory page hold 
list properties common data held virtual memory pages 
properties hold find hold suggest simple effective model memory data compression 
memory data ffl word double word aligned faster handling processor unaligned loads stores slower 
ffl comprises large portion integers pointers word sized 
ffl contains integers cover range smaller integer type store majority information low bits 
ffl contains pointers region memory majority information low bits 
ffl contains repeating common values particularly zero 
regularities ones exist memory data 
program stores complex structures stored consistent format struct class sequential patterns data 
memory data sequence bytes example program may significant input output buffers 
program may may exhibit certain regularities different inputs 
set regularities listed applies programs inputs 
see simple regularities provide significant compression reducing virtual memory pages original size average 
quality compression achieved lz compressors suggests regularities may just easy capture domain specific compressor 
compression compressed caching design compression algorithm depends regularities data context compression algorithm 
existing lossless compression centered compression byte streams data file system 
file systems read operations performed write operations 
result computationally expensive compression algorithm acceptable operations performed data needs written file system 
decompression required read operations efficiency decompression important 
asymmetry compression decompression efficiencies common vast majority lz algorithms 
virtual memory system compressed cache read write operations occur nearly frequency 
order service fault page compressed incoming page decompressed page outgoing page compressed 
list desirable properties compression algorithm compressed caching ffl reduction space needed store page pages stored compressed cache space 
tighter compression greatly expand effective memory size requires ram taken uncompressed cache reducing number uncompressed cache misses 
ffl compression decompression speed time required compress page decompress overhead introduced keeping resident pages compressed smaller 
costly take ram compressed cache order expand effective main memory size 
note compression decompression speed matter performed nearly frequency 
ffl small space overhead purpose compressed cache effective ram 
compression algorithms require kbytes commonly kbytes mbyte perform compression 
space compression algorithm needs ram store uncompressed compressed caches 
note page evicted disk clean discarded disk contain valid copy disks large hold redundant copies pages cached ram 
compressed cache hold redundant copies pages held uncompressed cache valuable ram wasted 
avoid redundancy copy page held ram 
consequently page evicted uncompressed cache compressed cache compressed irrespective 
douglis study dou redundant copies pages stored uncompressed compressed caches lzrw algorithm study compressed slowly 
believe redundancy significantly reduced effectiveness compressed cache partially responsible ambiguous results 
wilson kaplan wk compressors simple regularities memory data provided combined desirable properties compression algorithm compressed caching designed family wilson kaplan wk compressors 
algorithms designed detect regularities exist virtual memory pages 
provide reasonable trade effective reduction computational time small space overhead 
note believe current wk variants represent ideal approach compression domain 
believe improvements modeling encoding memory data investigated 
current wk compressors interest spite unsophisticated modeling encoding techniques provided best performance compressed caching results see chapter 
wk compression algorithms recency dictionary compressors 
small number seen symbols stored dictionary 
new input symbol compared dictionary hopes symbols match resemble current input symbol 
comparison successful algorithm captured regularity data encoded small number bits 
compression decompression speeds matter modeling encoding components wk compressors interleaved 
specifically internal representation store results modeling exactly set bits encoding 
entire input modeled encoded follows packing phase gathers encoding bits emitted fast space efficient manner 
describe phases implementation 
describe wk variants names derived manner organizes dictionary seen words 
specifically uses fully associative dictionary wk uses way set associative dictionary elements set wkdm uses direct mapped dictionary 
find associativity dictionary moderate effect compression tightness significant effect compression time 
modeling encoding phase wk compressors dictionary match input symbols seen symbols 
doing compressor models similarity values spatially local 
input symbol compared dictionary compressor record results modeling 
speed execution important wk compressors result modeling symbol immediately recorded set bits compressed encoding 
tasks modeling encoding interleaved 
reading input 
memory data comprises words pieces data aligned word double word boundaries wk compressors take input word time 
input larger chunks lz implementa tions read bytes wk compressors reduce number input symbols read reduce computational time 
reading words difficult wk compressors find byte regularities 
comparing input expected values 
regularities inmemory data wk compressor compare word expected values 
sequence comparisons comparison succeed expensive comparison perform 
expected value special case zero 
special case exists common 
making special case dictionary entry need wasted store zero seen value 
value zero word compared entry dictionary search exact match 
match desirable allows small encoding index dictionary needed 
matches occur programs small set identical values clustered pages 
asserted pointer integer values memory data similar information residing low bits 
order capture redundant similarity high bits compare input word dictionary entries time search partial match 
partial match word read input data word dictionary achieved upper bits match lower bits differ 
numbers chosen empirical evidence evidence showed compressibility change radically different split high low bits 
long information low bits separated redundancy high bits deal integer pointer regularity choice low bits high bits bit machine word size 
numbers change bit architecture 
captured 
dictionary searched 
note specified order current input word compared dictionary entries 
different variants wk algorithms implemented arrange word dictionary difference ways 
describe variants arranges dictionary performs updates 
ffl word dictionary fully associative 
entries kept lru order seen word earlier appears dictionary 
input word compared dictionary entries comparisons ordered seen word 
entry may comparison variant fully associative 
ffl wk dictionary way set associative cache words 
set comprises entries entries set maintained lru order 
current input word compared dictionary entries word mapped sets hash function 
fully associative version comparisons ordered seen words 
fully associative version words appropriate set comparison 
ffl wkdm variant hashing function maps current input word exactly dictionary entries 
exact partial matches attempted entry entry considered match 
entry updated seen input word hashed 
algorithm implemented straightforward 
achieved reasonable compression slower desired 
execution time spent attempts match input words destined match entry single word dictionary 
additionally cost implementing move front mtf algorithm maintained lru order required handful operations 
order reduce time spent searching dictionary matches idea set associativity common hardware caches created wk 
surprisingly execution time significantly reduced result fewer futile comparison attempts 
expected set associative dictionary yielded greater reductions page size 
instrumenting accumulate number matches dictionary entry vast majority matches seen words 
twelve entries rarely yielded match 
input words failed dictionary matches wk comparisons seen words hashed set 
rarely useful entries stored served pollute cache 
making set associative pollution reduced greater number matches possible 
wk faster compressed tightly wasn faster best lz implementations 
order eliminate unsuccessful matches remove mtf algorithm dictionary update process wkdm created 
input word match dictionary entry size reduction provided new algorithm wk 
tightness compression quite speed increase significant 
surprisingly see chapter applied compressed cache simulations wkdm provides best caching performance 
storing modeling results encoding representation 
results modeling word need stored compression algorithm 
sake execution speed internal representation data exactly set bits output encoding 
steps separated intermediate representation directly recording bits actual emission output fast packing phase 
tag values 
word compressed bit tag indicates kind match occurred word compared expected values 
tag values decompressor determine reconstruct decompressed word 
depending kind match occurs modeling bits may need output decompressor needed information 
list tags correspond kind match modeling phase bits output tag 

zero zeros common words zero valued bits special case algorithms 
input word zero tag value dictionary comparisons performed 
bits 
exact match input word compared entries dictionary 
input word equal word dictionary exact match 
bits ffl bit dictionary index index dictionary entry contains exactly input word 

partial match input word matches dictionary entry top high bits differs low bits partial match 
bits ffl bit dictionary index index dictionary entry contains word upper bits match upper bits input word 
ffl bit difference pattern low bits input word differed low bits dictionary entry 

match input word zero matches dictionary entry exactly partially expected value 
case representation input word expand 
bits ffl bit literal pattern entire input word literally represented 
simple representation allows results modeling recorded quickly 
simple easily quickly parsed decompressor 
complete interleaved modeling encoding 
know wk algorithms read input words compare input word seen words dictionary provide concrete description algorithm 
shows algorithm employed wk compressor algorithm wk decompressor 
note details dictionary lookup update aren shown different wk variant 
wk compress uncompressed page dictionary initialize current word uncompressed page current word record zero tag continue entry dictionary exact match current word entry null record exact match tag record entry continue entry dictionary partial match current word entry null record partial match tag record entry low bits extract low bits current word record low bits continue record tag record current word wk compression decompression algorithm 
compressor reads source page word time looking zeros exact matches dictionary partial matches dictionary 
stores modeling results simple ad hoc encoding centered bit tag values indicate kind match word 
wk decompress compressed page dictionary initialize tag compressed page tag zero tag output continue tag exact match tag entry read index word dictionary entry output word continue tag partial match tag entry read index low bits read low bits word assign low bits dictionary entry low bits output word continue assert tag tag word read word output word decompressor reads tags plus necessary bits direct decompression 
maintaining dictionary fashion compressor reconstruct uncompressed page word time 
choosing order matches 
compression algorithm search regularities proceeds specific general 
special case zero comparison 
comparison fails input word compared dictionary entries exact matching partial matching 
comparisons fail modeling predictions failed input word considered 
reason input word matched specific case compared general cases 
proceeding specific general common approach different ordering may efficient 
example partial matches common exact matches rare execution faster partial matches attempted exact matches 
misses frequent partial exact matches attempting partial match allows algorithm skip attempt exact match 
useful note changing order comparisons affect decompression algorithm 
decompressor guided sequence tags decompressor reconstruct original words exactly sequence compressor modeled 
wk implementations took advantage property allow compressor adapt order comparisons depending commonly cases 
simple algorithm effectively captures regularities described earlier 
word sized aligned values identical upper bits differing lower bits easily detected simple recency dictionary 
small dictionary provide significant compression allowing low space overhead fast dictionary updates 
simple algorithm significantly improved 
larger dictionary sliding window lz provide matches 
fast huffman encoding implementation yield significantly greater reduction 
model detect sequences words repetition words improve compression 
simple algorithm performs demonstrating power domain specific compression 
packing phase earliest implementations bit values record results modeling output stream word input corresponding bits created modeling encoding emitted 
approach required bit stream accumulated word emitted bits time performing number conditional checks bit shifts bit wise operations 
expensive instructions performed tag dictionary entry index low bit string full word compressed representation 
word word approach emitting encoding expensive 
worse parsing bit stream decompressor slower creation compressor group bits added output stream performing shifts bit wise operations decompressor read individually item bit string sequence bit manipulations 
fast approach emitting encoding 
initial hopelessly slow compared efficient lz algorithms 
developed time efficient method outputting encoding items 
new method reduced time required construct encoded bits output order magnitude operations 
different types items comprise wk compressed encoding bit tags bit dictionary entries bit partial match patterns bit match patterns arrays allocated compression kind encoding item 
modeling performed word encoding item recorded respective array 
efficiency encoding item written array entry 
recording modeling results fast entry arrays contains number bits store information 
encoded bits arrays packed leaving meaningless unused bits array entry 
key details understanding packing process efficiency 
shifting meaningful bits entry correspond meaningless bits entry entries combined bitwise xor operation 
result combined entries meaningless bits left 

entry array may byte half word 
combining just pairs entries xor operation machine words array combined 
result entries combined parallel reducing time required pack array 
example consider sixteen consecutive entries array tags packed single bit word 
array allocated array bytes 
byte array store single bit tag value 
array entry contains meaningful low bits meaningless high bits 
contiguous group sixteen entries considered words tag entries word 
combining operation new entries added final accumulated word 
combining words tags interleaved deterministic manner easily quickly unpacked decoder 
number entries combined number bits shifted depends entirely size entry 
consider analogous packing algorithm initial tag entries stored array word word value aa bb cc dd ee ff gg hh ii jj kk ll mm nn pp qq shift word left bits combine word aa bb cc dd ee ff gg hh accum shift word left bits combine accumulated word accum ii jj kk ll accum shift word left bits combine accumulated word accum mm nn pp qq accum sixteen consecutive tag entries bits apiece represented pairs alphabetic characters 
uppercase letter bit tag lowercase letter second bit 
zero bits meaningless ones need eliminated packing 
group entries handled machine word 
dictionary index entries 
indices bits apiece stored entry array bytes 
new entries accumulated xor operation compared new entries xor operation achieved tags 
low bit patterns generated partial match bits apiece real parallel packing occur 
entries combined straightforward manner 
efficiency bits packed word low bit patterns left meaningless bits 
note full bit pattern generated misses written directly output modeling phase packing performed item 
space requirements fast packing 
approach packing requires extra space arrays space required arrays approximately kilobytes equivalent virtual memory pages 
speed increase provided approach significant packing unpacking total compression decompression costs 
total space required wk algorithms significantly smaller lz compressors 
pentium execution times 
comparing execution times different wk implementations packing schemes pentium cycle counter timing mechanisms 
reasons execution time double placement order lines source code compressors 
specifically modeling encoding phase distinct block code packing phase 
executed modeling encoding phase execution require approximately cycles 
executed packing phase cycles needed 
executed sequence cycles required 
rewriting block codes odd effect disappeared 
experimental evaluation simple kind compressed cache single compression algorithm pages 
priori obvious compression algorithm best average 
clear compressibility pages varies program input amount memory allocated program execution 
order answer questions collected statistics performance algorithms designed speed 
see mean compressibility pages vary significantly program 
see compressibility pages vary drastically examine paging traffic different memory sizes 
find standard deviation compressibility pages hundreds bytes program execution significant variance pages compresses 
experimental design compression algorithms evaluated 
selected compression algorithms 
designed speed suitable candidates compressed caching 

wkdm recency compressor operates machine words uses direct mapped word dictionary fast packing implementation 

wk recency compressor operates machine words 
word dictionary arranged way set associative cache seen words set stores elements 
implementation fast packing method 

lzo specifically lzo carefully coded lempel ziv implementation designed fast particularly decompression tasks 
suited compressing small blocks data small codes dictionary small 
compressors study written speed optimized implementation intel instruction set 

lzrw fast lempel ziv implementation 
algorithm douglis dou fastest lz time 
perform lzo wanted demonstrate algorithm provides compression low execution time 
see current hardware algorithm support beneficial compressed caching system 
input corpus 
order obtain meaningful statistics experimental input data representative data need managed compressed cache 
order obtain representative input collected traces specifically page image traces contained sequence pages numbers copies page images 
important note data pages traced code text pages 
code compression addressed efficient reduction difficult achieve similar domain specific compressor 
topic addressed section 
page image traces input lru simulator able capture paging traffic including contents pages lru memory size 
describe process trace gathering specifically section 
paging traffic lru memory approximation input compression algorithm see compressed caching system uncompressed cache lru memory size 
measuring performance compression algorithms actual paging traffic gathered real program executions obtain approximation compression decompression costs compressed cache 
importance representative input 
studies compression algorithms memory data paging traffic input 
area discussed detail section sampling contents memory program execution 
snapshots working set taken number applications 
study swap space different machines sampled input compression algorithms evaluations 
forms input represents workload compression algorithm compressed caching 
sample address space fail capture information frequency pages moved uncompressed cache 
pages may evicted reasonably sized uncompressed cache may paged times 
techniques studies give equal weight kinds pages 
earliest evaluations implementation core files program executing memory deposited file 
input believed reduce pages approximately original size 
paging traffic discover reduction original size realistic 
choosing inappropriate input greatly influence results kind study 
clear description provided working set set resident pages set allocated pages subset program pages 
results measured compression algorithm reduce page 
measured quickly compression decompression algorithms executed page 
measurements taken paging traffic wide range memory sizes memory small amount paging overwhelming memory large paging occurs 
see general compressibility traffic different memory sizes vary greatly 
number interesting differences programs memory sizes compression algorithms 
differences guide development better compression algorithms 
results shown see variance compressibility different points program execution 
expect programs may different kinds data phase compressibility may vary time 
results indicate real programs data incompressible average 
ability adapt changes compressibility addressed section 
reduction performance figures see means standard deviations sizes compressed pages program test suite 
axis varies memory size 
axis mean standard deviation compressed size paging traffic lru memory size 
plot curves compression algorithm 
common features 
gross feature curves group generally little change mean standard deviation memory size changes 
patterns manner values change mean compressed size bytes memory size pages mean compressed size espresso wkdm wk lzo lzrw means compressed sizes paging traffic different lru memory sizes kbyte pages espresso 
standard deviation compressed size bytes memory size pages standard deviation compressed size espresso wkdm wk lzo lzrw standard deviations compressed sizes paging traffic different lru memory sizes kbyte pages espresso 
mean compressed size bytes memory size pages mean compressed size cc wkdm wk lzo lzrw means compressed sizes paging traffic different lru memory sizes kbyte pages cc 
standard deviation compressed size bytes memory size pages standard deviation compressed size cc wkdm wk lzo lzrw standard deviations compressed sizes paging traffic different lru memory sizes kbyte pages cc 
mean compressed size bytes memory size pages mean compressed size gnuplot wkdm wk lzo lzrw means compressed sizes paging traffic different lru memory sizes kbyte pages gnuplot 
standard deviation compressed size bytes memory size pages standard deviation compressed size gnuplot wkdm wk lzo lzrw standard deviations compressed sizes paging traffic different lru memory sizes kbyte pages gnuplot 
mean compressed size bytes memory size pages mean compressed size grobner wkdm wk lzo lzrw means compressed sizes paging traffic different lru memory sizes kbyte pages grobner 
standard deviation compressed size bytes memory size pages standard deviation compressed size grobner wkdm wk lzo lzrw standard deviations compressed sizes paging traffic different lru memory sizes kbyte pages grobner 
mean compressed size bytes memory size pages mean compressed size ghostscript wkdm wk lzo lzrw means compressed sizes paging traffic different lru memory sizes kbyte pages ghostscript 
standard deviation compressed size bytes memory size pages standard deviation compressed size ghostscript wkdm wk lzo lzrw standard deviations compressed sizes paging traffic different lru memory sizes kbyte pages ghostscript 
mean compressed size bytes memory size pages mean compressed size lindsay wkdm wk lzo lzrw means compressed sizes paging traffic different lru memory sizes kbyte pages lindsay 
standard deviation compressed size bytes memory size pages standard deviation compressed size lindsay wkdm wk lzo lzrw standard deviations compressed sizes paging traffic different lru memory sizes kbyte pages lindsay 
mean compressed size bytes memory size pages mean compressed size wkdm wk lzo lzrw means compressed sizes paging traffic different lru memory sizes kbyte pages 
standard deviation compressed size bytes memory size pages standard deviation compressed size wkdm wk lzo lzrw standard deviations compressed sizes paging traffic different lru memory sizes kbyte pages 
mean compressed size bytes memory size pages mean compressed size rscheme wkdm wk lzo lzrw means compressed sizes paging traffic different lru memory sizes kbyte pages rscheme 
standard deviation compressed size bytes memory size pages standard deviation compressed size rscheme wkdm wk lzo lzrw standard deviations compressed sizes paging traffic different lru memory sizes kbyte pages rscheme 
program programs pages roughly compressible original size deviation bytes 
exceptions small large memories compressibility better worse 
extreme memory sizes uncompressed cache 
feature common plots means standard deviations compressibility change identically programs memory sizes irrespective compression algorithm 
note exceptions pattern moment interesting see lz wk approaches modeling find kinds regularities compressibility identically affected changes 
notable exception espresso shown 
memory size varies changes compressibility wk algorithms fundamentally different lz algorithms 
exception traffic incurred pages performance similar shapes curves wk lz algorithms dissimilar 
larger memory sizes lz algorithms compress significantly better 
modeling input bytes words keeping extensive dictionary lz compressors capture regularities wk algorithms 
see capturing regularities comes expense slower lz execution 
differences algorithms 
algorithms compress data better 
programs wk provides better compression wkdm lzo provides better compression lzrw 
cc gnuplot wk algorithms provide slightly better compression lz algorithms superior programs 
effectiveness modeling depends program data stores 
interesting see cases relative rank algorithms doesn change memory size changes 
data stored program sufficiently similar pages best choice algorithm tightest compression change 
exceptions ffl cc ghostscript lzrw wkdm change relative rank depending memory size 
changes performance similar shapes curves 
ffl gnuplot smallest memory sizes compressibility different 
program uses pages data frequently rest approximately pages part large loop simple integer data 
traffic small memory sizes quite different content memory sizes 
ffl similarly lindsay compressibility relative rank compression algorithms changes memories smaller pages 
pages data referenced quite remaining pages part loop 
memories smaller pages wk algorithms perform best 
traffic larger memory sizes lzo performs best lzrw continues provide compression 
note compressibility algorithms memory sizes pages excellent reduced representations original size 
sophisticated approach encoding lzo accounts superior performance 
differences programs 
significant variation compressibility data program 
average reduction roughly original size average particular program may quite different 
see programs pages compressed poorly compressible pages raise mean 
ffl poorly compressible data grobner small memory sizes pages compressed roughly original size 
sizes near footprint program pages reduced size lzo size wkdm 
grobner stores byte floating point values wk algorithms model lz algorithms model modestly better 
rscheme compressibility program pages varies greatly compression algorithm 
wkdm pages compress original size 
algorithms reduction 
poor compressibility wkdm undesirable see fastest algorithm provides best compressed cache performance 
ffl compressible data espresso program pages compress wk wkdm respectively original size small physical memories 
pages compress roughly algorithms larger memory sizes 
lz algorithms perform better reducing pages larger memory sizes approximately original size 
cc compiler pages compress original size depending algorithm 
notice compressibility doesn change memory size 
pages compress approximately original kbytes 
cc compressibility varies drastically memory size 
programs build kinds data structures execution procedure compiled pages contain kind data 
ghostscript compressibility program pages vary memory size 
smaller memory sizes pages compress uncompressed size larger memory sizes compress worst algorithms lzrw best lzo 
compressibility results imply memory data compression applied postscript printers 
compressible data gnuplot data stored program simple integers creating scatter plot 
surprising simple wk compressors provide excellent reduction 
pages nearly memory sizes reduced lz algorithms wk algorithms 
lindsay program pages compressible 
smallest memory sizes pages reduced lzrw wk 
majority memory sizes roughly pages available pages compressible original size lzrw 
pages contain nearly zeros tightness encoding scheme determines compressed size algorithm 
programs lindsay store empty pages scientific applications sparse data 
clear correlation compressibility footprint size 
intuition programs large footprints tailored tight data representations avoid paging 
representations tend programming complicated solution program implements 
desirable programmers write clearly direct simple data representations focus problem solution 
compressed caching allows writing special tight data representations complicate code 
relationship means standard deviations 
see figures plots standard deviation memory sizes similar plots means 
examine standard deviation important high 
deviation exceedingly large difficult maintain compressed cache 
unpredictable size compressed pages difficult determine compressed caching provide benefit 
fortunately memory data compressed efficiently significantly compressed consistently 
programs standard deviation hundreds bytes pages rarely compress larger original size smaller original size 
ghostscript exhibited standard deviation greater kbyte 
compressibility program pages high deviation ruin benefits gain compressed caching 
variation memory size little gross effect deviation 
program relative rank compression algorithms usually doesn change irrespective memory size 
curiously lzrw exhibits consistently lower deviation lzo 
consistency may due lzrw conservative encoding com note extreme changes compressibility largest memory sizes 
large variations result statistics small sample size paging events large memory sizes result fundamental change compressibility pages 
data lzrw achieves moderate reduction compared lzo 
consequently compressed sizes lzrw consistently near mean 
wk exhibits consistently lower deviation wkdm wk extra computational complexity allows achieve reduction wider range inputs 
wkdm easily fail model input may suffer larger number pages compress poorly 
examining pairs mean standard deviation plots program observe surprising correlations statistics obvious relationship programs 
programs espresso gnuplot ghostscript rscheme somewhat directly related mean smaller standard deviation 
shapes curves roughly similar 
programs cc grobner lindsay relationship holds entire range memory sizes 
consistent relationship mean standard deviation significant changes occur memory sizes 
programs mean drastically increases decreases particular memory size standard deviation changes size 
change paging traffic pages may compressible consistent compressibility 
compression decompression times compression decompression speeds algorithms consistent memory sizes programs 
fluctuation times set operations required highly compressible page different poorly compressible page 
variation isn large tends affect slower algorithms faster ones 
results consistent program program fig mean compression time nanoseconds memory size pages mean compression time grobner wkdm wk lzo lzrw means compression times paging traffic different lru memory sizes kbyte pages grobner 
ures show mean standard deviation statistics compression decompression times programs 
results programs similar shown 
compression decompression times gathered mhz ultrasparc representative current processor speeds 
mean compression speeds 
designed compress quickly wk algorithms consistently provided fastest compression 
wkdm easily fastest algorithm programs nearly memory sizes wk commonly slower 
lz algorithms twice standard deviation compression time nanoseconds memory size pages standard deviation compression time grobner wkdm wk lzo lzrw standard deviations compression times paging traffic different lru memory sizes kbyte pages grobner 
mean decompression time nanoseconds memory size pages mean decompression time grobner wkdm wk lzo lzrw means decompression times paging traffic different lru memory sizes kbyte pages grobner 
standard deviation decompression time nanoseconds memory size pages standard deviation decompression time grobner wkdm wk lzo lzrw standard deviations decompression times paging traffic different lru memory sizes kbyte pages grobner 
mean compression time nanoseconds memory size pages mean compression time ghostscript wkdm wk lzo lzrw means compression times paging traffic different lru memory sizes kbyte pages ghostscript 
standard deviation compression time nanoseconds memory size pages standard deviation compression time ghostscript wkdm wk lzo lzrw standard deviations compression times paging traffic different lru memory sizes kbyte pages ghostscript 
mean decompression time nanoseconds memory size pages mean decompression time ghostscript wkdm wk lzo lzrw means decompression times paging traffic different lru memory sizes kbyte pages ghostscript 
standard deviation decompression time nanoseconds memory size pages standard deviation decompression time ghostscript wkdm wk lzo lzrw standard deviations decompression times paging traffic different lru memory sizes kbyte pages ghostscript 
mean compression time nanoseconds memory size pages mean compression time lindsay wkdm wk lzo lzrw means compression times paging traffic different lru memory sizes kbyte pages lindsay 
standard deviation compression time nanoseconds memory size pages standard deviation compression time lindsay wkdm wk lzo lzrw standard deviations compression times paging traffic different lru memory sizes kbyte pages lindsay 
mean decompression time nanoseconds memory size pages mean decompression time lindsay wkdm wk lzo lzrw means decompression times paging traffic different lru memory sizes kbyte pages lindsay 
standard deviation decompression time nanoseconds memory size pages standard deviation decompression time lindsay wkdm wk lzo lzrw standard deviations decompression times paging traffic different lru memory sizes kbyte pages lindsay 
mean compression time nanoseconds memory size pages mean compression time rscheme wkdm wk lzo lzrw means compression times paging traffic different lru memory sizes kbyte pages rscheme 
standard deviation compression time nanoseconds memory size pages standard deviation compression time rscheme wkdm wk lzo lzrw standard deviations compression times paging traffic different lru memory sizes kbyte pages rscheme 
mean decompression time nanoseconds memory size pages mean decompression time rscheme wkdm wk lzo lzrw means decompression times paging traffic different lru memory sizes kbyte pages rscheme 
standard deviation decompression time nanoseconds memory size pages standard deviation decompression time rscheme wkdm wk lzo lzrw standard deviations decompression times paging traffic different lru memory sizes kbyte pages rscheme 
slow wkdm 
interestingly lzrw compressed faster lzo 
mhz processor wkdm able compress page approximately milliseconds processor cycles 
wk slightly slower requiring ms cycles task 
lzrw lzo significantly slower compressing page mean time ms cycles 
extreme case grobner shown 
lz algorithms achieve better compression wk algorithms larger memory sizes reduction requires time ms cycles compress page 
important note ultrasparc processor timing performance implementation lzo hand written assembly implementation available intel architecture 
careful implementation lzo approximately faster 
difference change relative rank lzrw outperform wk compressors 
algorithms benefit careful assembly implementation relative performance algorithms accurately reflected implementations 
standard deviations compression speed 
standard deviations share general rankings means 
wk algorithms lower deviations lz compressors factor 
deviations particularly high highest ms cycles 
wkdm particular exhibits extremely low deviation commonly ms cycles 
compression time consistent wkdm deviation doesn change compressibility page 
wkdm employs simple modeling technique time required model compress ible page different time needed model poorly compressible page 
exceptions due high compressibility 
figures see compression performance qualitatively different data extremely compressible 
smallest memory sizes means standard deviations compression times programs 
memories larger pages paging traffic comprises pages zeros 
result compression time algorithm lower paging traffic 
lz algorithms slower wk algorithms 
lzo lzrw compress kbyte page approximately ms cycles twice fast usually paging traffic 
interestingly wk compressed pages ms cycles reduction mean compression time wkdm improves ms cycles 
implementations wk checks special case zero wkdm performs comparison second comparing input word dictionary entry 
wk compress zero fewer steps wkdm 
mean decompression speeds 
striking feature plots mean decompression speed lack sensitivity memory size 
decompression effected contents compressibility page performs number operations case 
ranking algorithms changes respect decompression times 
wkdm fastest algorithm requiring roughly ms cycles decompress page 
ms cycles lzo slower 
lzo designed fast decompression surprise nearly fast wkdm 
wk suffers result slow approach emitting encoding described 
performs operations parse component compressed page decompression time worst algorithms 
value fast packing wkdm evident decompression compression 
unusual case lindsay traffic memory sizes pages comprises zeros see significant change mean decompression time 
lzrw performs slowly traffic traffic memory sizes pages 
rest algorithms compress faster due change paging traffic 
standard deviations decompression speed 
consistency mean decompression time surprising corresponding standard deviations low compression algorithms ms cycles algorithms greater variance ms cycles wkdm lzo cases 
related compression algorithms designed memory data compression 
implemented software hardware implemented 
algorithms resemble quite different may capture meaningful regularities combined modeled wk compressors 
original wk newton precursor wk algorithms successfully apple newton personal digital assistant palmtop backing store sw 
algorithm early version algorithm 
successfully increase effective memory size machines 
match match algorithm similar wk algorithms 
match examines input word word 
keeps dictionary observed words dictionary grows new word encountered 
dictionary ordered mtf algorithm 
output created huffman encoding 
partial matching match algorithm 
wk algorithms rely similarity high bits words match attempts match bytes word dictionary entries 
find dictionary entry shares byte values input word considered partial match 
rl variant algorithm capable detecting contiguous sequences zero words run length encoding sequences 
memory data zeros run length encoding capability may provide real benefit 
match designed hardware implementation mind 
researchers implemented algorithm hardware tested applicability memory data 
hardware implementation provide faster execution 
attempts memory data compressors capture known regularities data 
regular personal communications paul wilson walter smith 
ities efficiently captured research required determine efficacy new techniques 
known compression domain may premature commit solutions completely hardware 
reached gooch jones similar strong simple regularities memory data captured straightforward models 
memory pages efficiently compressed original size compression performed nearly quickly decompression 
zero removal extremely common occurrence zero words led rizzo develop zero removal zr compression algorithm memory data 
compressor uses simple encoding 
zero words encoded emitting single bit words encoded emitting bit literal bits non zero word 
approach compression amenable fast implementation 
programs deal sparse data compression may acceptable 
experiments compression pages evicted backing store real machines rizzo zr reduce pages approximately original size 
swap space real machine sample traffic compressed cache see choice 
pages sampling occur correct frequency contents swap partition real disk may largely unused 
importantly compression lost modeling zeros significant loss reduction 
rizzo asserts detecting common word values zero re rizzo name algorithm providing name convenience 
quire computational time compressed caching context 
performance wk algorithms match algorithms demonstrate finding regularities non zero values possible essential effective compressed caching 
parallel cooperative dictionary compression part research project ibm robinson thomas developed new approaches compression small blocks frt 
intent develop hardware compressor compressed caching system possibly domains 
novel ideas propose 
ideas specific memory data contribute development fast effective compressor 

input divided set sub blocks concurrently compressed 
exploiting available concurrency nearly linear time speedup achieved serial compression 

thread compress sub block concurrent compression scheme need independent dictionaries 
researchers describe maintenance shared cooperative dictionary compressing threads 
sharing improve effectiveness modeling yield better compression 

dictionary compressors lz store pointers previously observed repeated portions input allowing shorter representation 
storing pointers previous input sequences guaranteed unambiguous decompression 
researchers note pointers previous input sufficient necessary 
describe approach matching current portions input portions guaranteeing unambiguous decompression 
finding forward pointers cooperative dictionary built threads compressing different sub blocks provides matches 
implement compression algorithm software simulate concurrent execution 
compare performance text file calgary corpus bcw huffman encoding pointers 
find parallel compression scheme reduce input approximately original size 
find cooperative dictionary parallel compression nearly effective serial compression provided lz variant 
exploiting available parallelism data compression approach improving speed 
research consider concurrent approach taken compression techniques lempel ziv 
compression techniques designed find efficiently regularities common memory data 
algorithms fast 
experiments fail provide useful information performance algorithms representative data 
compression text file experiments fails demonstrate algorithm compress real inmemory data pages 
virtual memory pages small important asses compression performance algorithm particularly lz algorithms may need larger blocks accumulate useful dictionary contents 
chapter compressed caching stressed gap processor disk speeds large growing 
programs run entirely ram benefit improvements cpu speeds runtime programs page dominated disk accesses may run times slowly cpu bound programs 
desirable virtual memory performance improve increases processor speed 
desirable performance decrease due paging gradual small medium amounts paging tolerable 
wil wil wilson proposed storing pages compressed form main memory compressed cache increase effective memory size reducing disk paging 
appel promoted idea evaluated empirically number studies douglis dou rc ccb 
study performed douglis influential years 
unfortunately study kernel implementation sprite operating system showed reductions paging time programs increase 
study russinovich mixed pc workload showed slight potential benefit 
result studies widespread belief compressed caching attractive machines fast local disk diskless handheld computers network computers laptops slow disks 
douglis pointed compressed caching attractive cpus continue get faster relative disk speeds 
studies ccb performed concurrently show compressed caching provide benefit current hardware configurations 
significantly studies methods adaptively adjusting size compressed cache 
douglis virtual memory systems compressed cache size effective method adaptivity employed 
unfortunately method adaptivity respond behavior timescale relative manner 
aim show discouraging results studies primarily due machines quite slow current standards 
current fast disk machines compressed caching offers substantial performance improvements advantages increase processors get faster 
study trends memory disk bandwidths 
compressed caching increasingly attractive regardless os improvements sophisticated prefetching policies reduce average cost disk seeks log structured file systems reduce cost writes disk 
concrete points analysis come simulations programs covering variety memory requirements locality characteristics 
simulation allowed try easily ideas controlled environment 
noted simulation parameters chosen perfectly realistic possible conservative realism difficult achieve 
instance conservatively assume fast disk perfect dirty page cleaning experiments 
measure great accuracy costs compressions actual paging traffic incurred real compressed caching system 
main value simulation results estimating exact benefit compressed caching substantial 
important result possible detect reliably memory compressed phase program execution 
form recency adaptivity applied eelru chapter applied result compressed caching policy adapts program behavior 
exact amount compressed memory crucially affects program performance compressing memory needed detrimental compressing little memory slightly prevent memory faults 
simulations fixed fraction memory compressed cache adaptive scheme yields uniformly high benefits test programs wide range memory sizes 
chapter demonstrating calculate costs benefits keeping compressed cache particular size 
show access patterns workloads cost benefit calculations indicate compressed caching provide benefit 
show results simulating fixed size compressed caches 
simulations reveal adapting compressed cache size improve performance difficult adaptivity method responds simple recency information 
discuss perform adaptivity analyze results simulating size adaptive compressed cache 
calculating costs benefits recall recency information reveals useful access patterns eelru responded 
cost benefit analysis recency information lru queue reveals program 
examining lru histogram indicates number misses range memory sizes determine cost incurred benefit provided compressed cache 
compressed cache introduced memory hierarchy cost associated moving pages new level 
compressed cache reduce number disk accesses benefit 
order determine compressed caching improves system efficiency benefit outweigh cost 
example shown specific artificial lru histogram assumed system parameters 
illustrate calculate cost benefit particular histogram 
generalize calculation derive set values 
calculation real histograms different workloads show compressed caching provide benefit real systems 
example shows artificially constructed lru histogram 
consider system pages available ram 
program incur disk accesses traditional virtual memory system compressed cache 
assume half ram compressed cache pages uncompressed cache pages dedicated compressed cache 
assume compression ratio reduction compressed pages 
page compressed cache store compressed number misses memory size lru histogram provides needed information calculation cost benefit dividing amount ram hold uncompressed compressed cache 
pages 
total pages stored ram compressed cache effectively expanded memory size 
observe uncompressed cache 
missed require decompression referenced page compression page evicted uncompressed cache compressed cache 
assume ms required compress page decompress 
servicing misses requires ms value cost compressed cache 
effective main memory size larger compression system needs access disk 
disk accesses avoided 
assuming disk access time ms ms disk access time saved 
benefit compressed cache ms compressed cache saved ms disk access time exchange ms compression decompression time 
benefit greater cost ms program benefit compressed cache 
note numbers chosen example simple real values involved quite different 
accounted cost writing dirty pages disk 
compressibility pages program data dependent histogram 
program kind calculation approximate effect compressed cache 
generalization derivation shows lru histogram similar labeled symbols may derive general cost benefit formulae 
assume real amount main memory available executing program 
fraction comp real comp store compressed cache 
fraction real dedicated uncompressed cache remaining portion main memory gamma comp values calculate amount ram comp dedicated compressed cache amount ram uncompressed cache comp real theta comp real theta differences lru histograms programs significant 
cost benefit analysis depends monotonically non increasing nature lru histogram 
comp reads number misses misses disk memory size real eff lru histogram labeled derivation cost benefit formulae 
set pages stored compressed cache refer mean compression ratio symbol ratio 
calculate eff effective memory size achieved compressed cache follows eff comp theta ratio disk accessed memory size eff assuming compression achieved ratio number disk accesses eff greater usually fewer real specifically misses number misses memory size number fetches disk saved due increased effective memory size drawn mean compression ratio commonly achieved memory data ratio 
delta disk reads misses real gamma misses eff similarly misses misses uncompressed cache require compression decompression 
note pages disk compressed form store pages evicted compressed cache 
misses eff misses effective memory size require disk access compression decompression 
misses eff compression decompression tasks counted misses misses uncompressed cache 
lack redundancy uncompressed compressed caches 
counted number pages read memory level counting number written 
disks large store redundant copies pages ram 
pages modified ram pages dirty written disk eviction 
compressed caches store redundant copies pages uncompressed cache redundancy reduces effective memory size causes disk faults misses misses uncompressed cache require compression operation irrespective evicted page 
dirty number dirty pages evicted memory size number disk write operations saved expanding effective memory size compressed cache delta disk writes dirty real gamma dirty eff past experiments compressed caches store redundant copies 
redundancy may contributed significantly negative results 
service times needed calculate cost benefit service cc read time required decompress page fetched compressed cache service cc write time required compress page evicted compressed cache 
similarly service disk read service disk write times required read page write page disk respectively 
calculate cost benefit 
cost denoted symbol cost time required service compressions required compressed cache cost service cc read service cc write theta misses note redundant copies pages stored compressed uncompressed cache uncompressed cache require eviction compressed cache irrespective page evicted 
benefit denoted symbol time required service extra disk faults compressed cache service disk read theta delta disk reads service disk write theta delta disk writes cost time saved avoiding disk faults greater time required service compressed cache faults compressed cache reduce program paging time 
number misses smaller memory usually higher larger number compressed cache faults larger number disk faults 
compression decompression significantly faster disk access 
calculate faster algebraically transform cost service disk read service disk write theta delta disk writes delta disk reads service cc read service cc write misses delta disk reads factor delta disk writes delta disk reads fraction disk writes saved disk reads saved 
disk reads saved may required corresponding write operation 
range fraction delta disk writes delta disk reads 
disk writes saved compressed cache delta disk writes delta disk reads 
disk write operations queued written batch assuming fraction zero allows conservatively assume perfect dirty page cleaning 
inequality ratio disk speed combined compression decompression speeds large ratio compressed cache accesses created disk accesses saved 
queuing disk writes dirty page cleaning perfect compression decompression slower relative disk satisfying inequality 
calculations real programs apply cost benefit calculation derived lru histograms taken real programs 
need assume reasonable parameters cpu speeds disk access times 
chosen values real machines experiments specifically mhz pentium pro 
chosen parameters 
assume disk latency ms common average seek time disks fast workstations 
machines slower disks benefit compressed caching greater 
page small unit data disk read write time read write page dominated latency actual transfer time negligible 
systems perform effective prefetching systems larger page size average latency page may somewhat lower compressed caching offer benefit 
denote time read write page service disk read service disk write ms 
assume mean time performing compression sion 
numbers approximately correct wkdm compression algorithm pentium pro mhz processor performed simulations sections 
decompression requires service cc read ms page compression requires service cc write ms page 
lru histograms show misses writing dirty pages 
calculate worst case compressed cache assuming dirty page cleaning perfect disk accesses saved write operations delta disk writes delta disk reads 
result calculation reflected section service disk read service cc read service cc write misses delta disk reads important notice lru histograms taken representative workloads executed real computing environments 
compiled lru histograms different workloads having collected different platforms 
note cost benefit calculation provide convincing case compressed caching 
trying show real histograms different workloads indicate compressed caching real contexts merit investigation 
unix workload 
data set taken group noninteractive programs run linux intel machines 
set programs simulations fully described section 
chosen histogram pascal compiler representative example 
lru histogram plotted provides number misses memory level 
numbers assumed hardware parameters allow calculate execution time possible compressed cache size 
results calculations shown 
non paging runtime total execution time memory page 
paging runtime total execution time compressed cache pages main memory available 
page faults memory size pages lru rate histogram misses lru histogram pascal compiler 
results different programs different memory sizes shape results shown 
notice large range compressed cache sizes improve paging performance pages 
finding corresponds simulation results calculation determine roughly extent fixed size compressed cache reduce paging time 
total execution time seconds compressed cache physical memory ratio percentage physical pages slow processor compressed cache runtime non paging runtime paging runtime lru cost benefit performance curve shows total runtime range compressed cache sizes page memory 
windows nt workload batch interactive programs 
researchers university washington etch tracing tool collect traces interactive non interactive programs running windows nt 
lru histogram popular web browser netscape navigator 
histogram calculate time application spend paging compressed cache 
results shown 
navigator interactive program similarly shaped lru histograms 
histograms similar compressed cache benefit programs 
navigator runs pages ram notice program spend seconds paging page faults memory size pages lru rate histogram netscape misses lru histogram netscape navigator popular web browser 
compressed cache need seconds paging 
benchmark 
examine lru histogram entire benchmark 
histogram taken rc experiment compressed caching windows operating system 
histogram data provided misses mbyte memories mbyte intervals 
extrapolated histogram results shown 
similar histograms shown extrapolated histogram reaches zero misses real histograms memory large hold entire footprint 
mbytes available run programs traditional virtual total paging time seconds compressed cache physical memory ratio percentage netscape navigator physical pages slow processor paging compressed cache paging compressed cache lru cost benefit performance curve netscape navigator shows total paging time range compressed cache sizes page memory 
memory system benchmark suffer seconds paging time 
slightly mbytes store compressed pages total paging time reduced seconds paging reduction 
different kinds programs gathered different platforms yield similar results 
may workloads compressed caching provide benefit real programs exhibit behavior compressed caching helpful 
page faults memory size megabytes lru rate histogram benchmark real misses extrapolated misses lru histogram benchmark 
actual data provided part curve rest extrapolated 
fixed size compressed cache simulations set real traces unix workload mentioned described section simulated compressed caches sizes fixed 
particular program simulated range physical memory sizes physical memory size simulated range compressed cache sizes 
group simulations verified cost benefit calculation reasonable helped reveal relationship physical memory sizes compressed cache sizes 
simulations demonstrated total paging time seconds compressed cache physical memory ratio percentage benchmark physical megabytes fast processor paging compressed cache paging compressed cache lru cost benefit performance results benchmark shows total paging time range compressed cache sizes mbyte memory 
compressibility compression speed affect ideal compressed cache size different ways characteristic important 
experimental design section describe methodology results detailed simulations compressed caching 
captured traces varied unix programs contain sequence pages touched contents 
page image traces measure performance various memory data compressors section simulate compressed caching detail 
note traces contain executable code pages 
focus data pages main interest compressing memory data 
explain section compressing code equally difficult 
techniques complementary proposed compressing code 
data rc indicate code pages exhibit locality properties data pages 
test suite 
simulations traced programs intel architecture linux operating system uses page size kbytes 
programs chosen represent number different footprint sizes know patterns small programs fundamentally different large programs 
wanted seek patterns behavior compressed cache performance scale memory size 
ffl espresso circuit simulator optimize programmable logic arrays 
input file provided ben zorn part study dynamic allocation behavior joh 
ffl cc gnu compiler version heart compiler performs compilation 
input file compilation performed pre processed combine largest source code file gcc distribution 
ffl ghostscript version interpreter postscript page rendering language 
input file page manual self system 
pages contain mixture graphics text require space rendered 
ffl grobner program rewrites formula grobner basis functions 
ffl lindsay hyper communications simulator hypercube network processors 
builds internal model hypercube network simulates transmission random messaging 
model hypercube stored single large array allocated early execution lives termination 
messages represented small dynamically allocated blocks memory live short time 
ffl translates pascal source code source code 
test input results code file approximately kbytes long 
data footprints programs vary widely cc ghostscript lindsay larger pages espresso grobner small pages 
compression algorithms experiments experiments memory data compression wkdm wk lzo lzrw 
described fully section 
different cpus 
additional dimension experiments recorded results compression decompression times different architectures cpu speeds 
results allowed demonstrate effect improvements cpus occur year applicability different architectures 
processors 
pentium pro mhz processor represented average desktop computer year ago 
compressed caching fast machines 

ultrasparc mhz faster processors available soon average processor 
compressed caching works better faster processor 

ultrasparc mhz slower sparc machine provides interesting comparison pentium pro due different architecture faster memory subsystem 
gathering traces 
simulator takes input trace pages program touches augmented information compressibility page time required perform compression decompression 
results needed compression algorithm 
create trace keep trace size manageable steps tracing filtering tools 
traced program portable tracing tool 
added module emit complete copy page referenced 
mechanism capture page image traces 
record page image trace slightly kbytes assuming kbyte page size 
traces contain tens hundreds millions page image traces quickly exceed available storage 
stored infeasible traces input simulator 
page image traces reduced coffman randell cr technique create behavior sequences augmented page images 
technique described fully section record actual effectiveness time cost compressing page image created set compressibility traces 
combination compression algorithm cpu created trace augmented time required compress decompress pages page image trace reduction achieved compression 
test programs compression algorithms cpu total compressibility traces 
sad olr techniques developed gathered traces reinvented coffman randell behavior sequence technique 
tool creates compressibility traces linked particular compressor corresponding decompressor 
consumes page image trace record input tool compresses decompresses page image outputs trace record 
record contains page number times compressing decompressing page contents moment resulting compressed size page 
page image compressed decompressed times median times recorded 
timing precise solaris high resolution timer compression timings done solaris operating system 
avoid favorable hardware caching effects caches filled unrelated data compression decompression 
approach conservative burstiness page faults usually mean relevant memory cached second level cache real system 
dynamic allocation compressibility 
programs linked doug lea memory allocator minimize fragmentation wjnb 
allocator fragmented memory badly pages contain wasted space compress unfairly compressed cache 
exception cc 
uses special allocator particularly suited stack allocation patterns compilation 
range physical memory sizes 
programs chose interesting memory sizes simulate 
loose definition interesting memory sizes paging required execute program paging program slow run 
program footprint pages interesting simulate execution page memory 
paging occur specifically real implementation consumes reduced page image lru behavior sequence 
events consequential virtual memory study arise 
similarly interesting simulate program execution page memory page total runtime orders magnitude longer 
specifically chose memory size mhz pentium pro test processors ms disk spend total running time paging remaining executing cpu 
refer memory size point largest memory size perform simulations 
note point memory size paging occurred 
range chose point memory size small execution time spent paging executing program cpu 
words program total runtime nearly times longer point 
believe people interested running program times slower due paging 
estimates 
simulation estimate costs reading page disk writing disk 
conservatively assumed writing dirty pages disk incurs cost 
additionally assumed disk uniform seek latency ms results understanding plots 
results simulations shown figures 
program selected memory sizes incur moderate amount paging traditional virtual memory system 
moderate mean program execute twice slowly times slower ram available avoid total paging time seconds compressed cache physical memory ratio percentage total paging time espresso physical pages wkdm wk lzo lzrw traditional vm physical memory size paging time espresso compressed cache percentage physical memory 
comparison traditional vm time shows paging time required compressed cache 
paging 
note plots chosen show relationship compressed cache size compression algorithm performance 
results physical memory sizes quite similar shown 
convincing argument applicability compressed caching nearly physical memory sizes section 
note curves shown plots truncated right 
traces reduced limit information available simulator larger compressed caches 
words simulation requires total paging time seconds compressed cache physical memory ratio percentage total paging time cc physical pages wkdm wk lzo lzrw traditional vm physical memory size paging time cc compressed cache percentage physical memory 
comparison traditional vm time shows paging time required compressed cache 
total paging time seconds compressed cache physical memory ratio percentage total paging time grobner physical pages wkdm wk lzo lzrw traditional vm physical memory size paging time grobner compressed cache percentage physical memory 
comparison traditional vm time shows paging time required compressed cache 
total paging time seconds compressed cache physical memory ratio percentage total paging time ghostscript physical pages wkdm wk lzo lzrw traditional vm physical memory size paging time ghostscript compressed cache percentage physical memory 
comparison traditional vm time shows paging time required compressed cache 
total paging time seconds compressed cache physical memory ratio percentage total paging time lindsay physical pages wkdm wk lzo lzrw traditional vm physical memory size paging time lindsay compressed cache percentage physical memory 
comparison traditional vm time shows paging time required compressed cache 
total paging time seconds compressed cache physical memory ratio percentage total paging time physical pages wkdm wk lzo lzrw traditional vm physical memory size paging time compressed cache percentage physical memory 
comparison traditional vm time shows paging time required compressed cache 
knowledge pages fetched evicted uncompressed cache 
simulation performed uncompressed cache large memory size specified reduction 
compressed cache sizes 
chosen compressed cache size significant reduction paging costs possible 
main memory pages dedicated compressed cache time spent compressing decompressing beneficial expansion effective memory size going small 
similarly memory compressed cache time needed perform compressions benefit provided 
program lindsay unusual case 
loop sufficiently strict memories approximately pages little paging time required traditional virtual memory system 
see nearly time eliminated compressed caching 
smaller memory easily able compress decompress quickly maintain beneficial compressed cache size 
size compressed cache depends greatly locality referencing behavior program 
regularities results 
plots shown shown physical memory sizes exhibit regular shape 
curve single minimum 
minimum corresponds compressed cache size yield paging time particular physical memory size compression algorithm 
note slight exception espresso plot shown wkdm wk exhibit perfect shape 
approximately pages compressed cache see slight change concavity curve 
point unique experiments shows possible curves multiple local minima 
compressibility data generally uniform see cases non shape 
uniformity plots provides useful information problems maintaining compressed cache ffl algorithms capable providing benefit current hardware 
ffl slower wk algorithm effective lzrw algorithm best algorithms 
ffl algorithms compress tightly execute slowly lzo perform best smaller compressed cache algorithms compress tightly execute quickly wkdm 
provide roughly equal performance appropriate compressed cache size clear chosen 
ffl ideal compressed cache sizes compression algorithm differ tends range sizes yield nearly optimal performance 
adaptivity methods need sensitive compression algorithm order choose compressed cache size 
ffl compressed cache total memory size 
regularities indicate simple heuristic able find ideal compressed cache size 
smoothness curves indicate heuristic somewhat inaccurate choosing compressed cache size quite ideal provide significant portion available benefit 
curves resemble closely generated cost benefit analysis 
application simple calculation sufficient compressed cache adapt size 
adaptively adjusting compressed cache size perform compressed caching system adapt behavior programs caches 
program data fits comfortably ram pages pages kept compressed overwhelming majority pages kept uncompressed form accessed penalty 
program data larger available ram compressing pages allow kept ram pages compressed working set captured 
douglis observed experiments different programs needed compressed caches different sizes 
implemented adaptive cache sizing scheme varied split uncompressed compressed ram dynamically 
adaptive caching system results inconsistent programs ran faster ran slower 
believe douglis adaptive caching strategy may partly fault 
fairly simple scheme compressed cache competed uncompressed cache file system cache main memory space 
resizing performed space away cache held globally page 
uncompressed cache holds touched pages compressed cache scheme requires bias ensure compressed cache memory 
believe form adaptivity single bias commensurate timescales uncompressed cache compressed cache timescales determined varying behavior memory size 
words single bias value appropriate strength bias depend locality behavior program 
online cost benefit analysis adaptive cache sizing mechanism addresses issue adaptation performing online cost benefit analysis recency information decayed timescale relative manner 
calculation similar outlined section approach keeping statistics nearly identical eelru section 
assuming behavior relatively near resemble behavior relatively past mechanism keeps track aspects program behavior bear directly performance compressed caching different cache sizes compresses fewer pages improve performance 
general recency information allows estimate cost benefit compressed cache size regardless current size compressed cache regardless pages currently memory 
analysis find current split memory uncompressed compressed caches better 
simply count number touches pages different regions lru queue interpret hits misses relative different sizes uncompressed cache corresponding sizes compressed cache 
multiple target sizes 
generalize scheme different target compressed cache sizes just multiple potential early eviction points eelru 
adaptive component system computes costs benefits target sizes counts touches regions lru ordering chooses target size lowest cost 
compressed cache size adjusted demand driven way memory compressed uncompressed access compressed page compressed ram disk occurs 
system chooses target uncompressed cache size corresponding effective cache size computed number page frames left compressed cache multiplied estimate compression ratio compressed pages 
means statistics kept adaptivity mechanism exact past information may contain hits different recency region indicated current compressibility estimate 
coarse grained approach matter simulations approximate statistics pages touched touched quite sufficient 
system need sensitive details replacement policy uncompressed cache normal lru approximation sufficient 
adapting behavior adapt program behavior statistics decayed exponentially virtual memory time 
events advance time pages affect cost benefit analysis hits compressed memory target compression sizes currently suggests 
additionally decay factor inversely proportional size memory total number page frames time typically advances slowly larger memories small ones small memories usually shorter replacement cycle need decay statistics faster rate larger ones 
extensive simulation results show strategy works intended adaptivity ensures memory size cache responds changes behavior program relatively quickly benefit reasonably persistent program behavior 
respond quickly continually distracted short duration behaviors 
single setting decay factor relativized automatically memory size works variety programs wide ranges memory sizes 
attempts different decay factors demonstrated adaptive heuristic particularly sensitive choice factor 
correct timescale critical rate decay constant factor 
size adaptive compressed cache simulations implemented approach adaptivity simulator 
test suite traces gathered test suite superset fixed boundary compressed cache simulations new programs added 
performed experiments set compression algorithms wkdm wk lzo lzrw 
processors specifically mhz pentium pro mhz ultrasparc mhz ultrasparc 
simulating large range physical memory sizes program adaptive method incurs little overhead vast majority memory sizes chooses compressed cache size reduces paging time 
programs test suite 
note experiments added test suite programs larger footprints ffl gnuplot plotting program create dimensional plots functions data files 
input provided mbyte file dimensional data generate simple scatter plot 
input file provided gideon glass experiments page replacement algorithm seq gc 
program pro sparc sparc name mhz mhz mhz gnuplot rscheme espresso gcc ghostscript grobner lindsay table processing times seconds program test suite processor study 
memory available paging occurs times turnaround times 
ffl rscheme bytecode interpreter scheme programming language 
input provided code performed number lattice manipulation functions 
performance dominated runtime generational garbage collector 
excluding compression algorithm 
results wk algorithm similar wkdm slowness yielded consistently lesser performance 
believe version algorithm fast packing technique perform better wkdm cases 
runtimes test suite 
results terms time spent paging helpful know processing time required execute program test suite 
table shows number seconds required execute programs processors paging occurs 
times added paging time information obtain total runtime time architecture memory size virtual memory configuration 
simulation parameters 
different target compression sizes values equal simulated memory size 
persistent phases program behavior system time adapt memory pages holding compressed data 
chose range sizes observed fixed size compressed caching simulations section 
specifically compressed cache small rarely provide significant benefit larger require compressions reduce paging time 
limiting number target compression sizes guarantees cost benefit analysis incurs low overhead 
small number target sizes demonstrates adaptive heuristic sensitive specific parameters coarse grained approximation perform 
form conservatism programs benefit larger compressed caches 
decay factor th event size memory weight equal event 
results particularly sensitive exact value decay factor choice decay factor worked memory size experimented 
section examine effect faster disk seek time ms 
allocation fragmentation 
simulator consider space lost fragmentation result variable sized compressed pages 
believe fragmentation kept low significantly reduce benefits shown simulator 
compressed pages need stored contiguously 
held linked list similar data structure 
overhead introduced data structure small 
furthermore allocation compressed pages handled differently allocation objects 
practice fragmentation shown low real programs wjnb usually percent allocation compressed pages different 
alternative periodically compact space store compressed pages 
periodic compaction space eliminate fragmentation done time required single disk fault hundreds millions processor cycles pass 
space lost fragmentation compressed cache kept low significantly reduce benefits having 
results detailed simulations wide range results test programs chose wide range memory sizes simulate 
plots section show entire simulated range program 
subsequent sections concentrate interesting region memory sizes 
range begins size program spends time paging time executing cpu ends size program causes little paging 
range begins memory sizes small runtime hundreds thousands times slower due paging ram hold program data 
figures show log scale plots paging time programs function memory size 
line plot represents results simulating compressed cache particular algorithm sparc mhz machine 
paging time regular lru memory system compressed cache shown comparison 
see compressed caching yields benefits wide range memory sizes indicating adaptivity mechanism reliably detects locality patterns different sizes 
note com total paging time seconds memory size pages total paging time espresso sparc traditional vm lzo lzrw wkdm results simulating compressed cache size adapted espresso wide range memory sizes 
note axis log scale 
total paging time seconds memory size pages total paging time gcc sparc traditional vm lzo lzrw wkdm results simulating compressed cache size adapted cc wide range memory sizes 
note axis log scale 
total paging time seconds memory size pages total paging time gnuplot sparc traditional vm lzo lzrw wkdm results simulating compressed cache size adapted gnuplot wide range memory sizes 
note axis log scale 
total paging time seconds memory size pages total paging time ghostscript sparc traditional vm lzo lzrw wkdm results simulating compressed cache size adapted ghostscript wide range memory sizes 
note axis log scale 
total paging time seconds memory size pages total paging time sparc traditional vm lzo lzrw wkdm results simulating compressed cache size adapted wide range memory sizes 
note axis log scale 
total paging time seconds memory size pages total paging time rscheme sparc traditional vm lzo lzrw wkdm results simulating compressed cache size adapted rscheme wide range memory sizes 
note axis log scale 
pression algorithms exhibit benefits definite differences performance 
figures aim convey general idea outcome experiments 
results analyzed detail subsequent sections isolate interesting memory regions algorithms architectures trends 
normalized benefits effect compression algorithms quantify benefits obtained compressed caching identify effect different compression algorithms system performance 
hard see effect plots log scale axis indicate compression algorithms obtain similar results 
detailed plot reveals significant variations algorithm performance 
figures show plots normalized paging times different algorithms interesting region 
recall usually begins size program spends time paging time executing cpu ends size program causes little paging 
normalized paging time mean ratio paging time compressed caching paging time regular lru replacement policy line represents paging time traditional virtual memory system 
algorithms obtain significant benefit traditional virtual memory systems interesting ranges memory sizes 
benefits common large parts plots shown 
time losses rare exhibited gnuplot small 
gnuplot maximum loss paging performance 
roughly half loss due conservative assumption disk traffic compressed main memory allocated compressed cache 
half loss attributed adaptivity mechanism attempted allocate pages compressed cache ratio cvm traditional vm paging time percentage memory size pages normalized paging time espresso sparc traditional vm lzo lzrw wkdm normalized paging times different compression algorithms espresso 
algorithms yield benefits significantly better 
ratio cvm traditional vm paging time percentage memory size pages normalized paging time gcc sparc traditional vm lzo lzrw wkdm normalized paging times different compression algorithms cc 
algorithms yield benefits significantly better 
ratio cvm traditional vm paging time percentage memory size pages normalized paging time gnuplot sparc traditional vm lzo lzrw wkdm normalized paging times different compression algorithms gnuplot 
algorithms yield benefits significantly better 
ratio cvm traditional vm paging time percentage memory size pages normalized paging time ghostscript sparc traditional vm lzo lzrw wkdm normalized paging times different compression algorithms ghostscript 
algorithms yield benefits significantly better 
ratio cvm traditional vm paging time percentage memory size pages normalized paging time sparc traditional vm lzo lzrw wkdm normalized paging times different compression algorithms 
algorithms yield benefits significantly better 
ratio cvm traditional vm paging time percentage memory size pages normalized paging time rscheme sparc traditional vm lzo lzrw wkdm normalized paging times different compression algorithms rscheme 
algorithms yield benefits significantly better 
discover help 
cost eliminated refining adaptivity implementation 
note losses diminish faster compression algorithms 
adaptivity introduce overhead cost reduced having fast compression algorithm direct function performing unnecessary compressions 
gnuplot interesting program study closely 
program stores highly compressible data exhibiting ratio average 
way adaptive heuristic consider large effective memory sizes expecting compress pages required data remains main memory 
gnuplot running time dominated large loop iterating twice lot data 
small memory sizes behavior compressed caching policy tries exploit ends benefits seen 
choice possible uncompressed cache sizes limits compressed cache available ram 
program larger compressed cache capture entire working set small total memory 
larger memory sizes benefit substantial reaching 
shown plots performance difference compressed caching different compression algorithms 
wkdm algorithm achieves best performance vast majority data points due speed comparable reduction rates lzo 
lzrw algorithm douglis consistently yields worst results 
fact combined slow machine current standards partially responsible disappointing results douglis observed 
ratio cvm traditional vm paging time percentage memory size pages normalized paging time gcc wkdm traditional vm pro sparc sparc normalized paging times compressed caching different processors cc wkdm 
implementation architecture effects past sections showed results sparc mhz machine 
expected faster sparc mhz machine lower compression decompression overhead perform better 
pentium pro mhz machine usually slower sparc machines compressing pages unexpectedly older architecture see remarks memory bandwidth 
figures show test programs simulated wkdm lzo architectures 
wkdm performance displayed ratio cvm traditional vm paging time percentage memory size pages normalized paging time gcc lzo traditional vm pro sparc sparc normalized paging times compressed caching different processors cc lzo 
ratio cvm traditional vm paging time percentage memory size pages normalized paging time ghostscript wkdm traditional vm pro sparc sparc normalized paging times compressed caching different processors ghostscript wkdm 
ratio cvm traditional vm paging time percentage memory size pages normalized paging time ghostscript lzo traditional vm pro sparc sparc normalized paging times compressed caching different processors ghostscript lzo 
ratio cvm traditional vm paging time percentage memory size pages normalized paging time rscheme wkdm traditional vm pro sparc sparc normalized paging times compressed caching different processors rscheme wkdm 
ratio cvm traditional vm paging time percentage memory size pages normalized paging time rscheme lzo traditional vm pro sparc sparc normalized paging times compressed caching different processors rscheme lzo 
agrees observations machine speeds 
performance lzo significantly better pentium pro mhz machine expect processor speed 
lzo performs relatively better particular processor pointed earlier hand optimized speed intel assembly language 
surprisingly effect optimization quite significant seen 
ghostscript instance pentium pro faster sparc mhz lzo 
technology trends memory bandwidth problem 
compressed caching benefits increases cpu speed relative disk latency 
different factor comes play disk memory bandwidths taken account 
observation moving data memory takes third execution time wkdm compression algorithm 
ratio true pentium pro mhz machine slow memory subsystem sparc mhz fast processor 
significantly better sparc mhz machine 
memory bandwidth limiting factor near 
importantly faster memory architectures rambus soon widespread compression algorithms fully benefit need read contiguous data 
trend favorable 
memory bandwidths historically grown disk bandwidths latencies grown rates 
sensitivity analysis cost benefits compressed caching dependent relative costs compressing decompressing page vs fetching page disk 
compression insufficiently fast relative disk paging compressed caching virtual ratio cvm traditional vm paging time percentage memory size pages normalized paging time espresso wkdm sparc traditional vm ms ms ms ms ms sensitivity analysis espresso disks various speeds 
plots interpreted faster disks cases slower cpus perfect prefetching larger page sizes 
memory worthwhile 
hand cpu speeds continue increase far faster disk speeds compressed virtual memory increasingly effective increasingly attractive 
decade cpu speeds increased year disk latency bandwidth improved year 
works increase cpu speeds relative disk speeds third year doubling half years years 
figures show plots simulated performance adap ratio cvm traditional vm paging time percentage memory size pages normalized paging time gcc wkdm sparc traditional vm ms ms ms ms ms sensitivity analysis cc disks various speeds 
plots interpreted faster disks cases slower cpus perfect prefetching larger page sizes 
ratio cvm traditional vm paging time percentage memory size pages normalized paging time gnuplot wkdm sparc traditional vm ms ms ms ms ms sensitivity analysis gnuplot disks various speeds 
plots interpreted faster disks cases slower cpus perfect prefetching larger page sizes 
ratio cvm traditional vm paging time percentage memory size pages normalized paging time ghostscript wkdm sparc traditional vm ms ms ms ms ms sensitivity analysis ghostscript disks various speeds 
plots interpreted faster disks cases slower cpus perfect prefetching larger page sizes 
ratio cvm traditional vm paging time percentage memory size pages normalized paging time wkdm sparc traditional vm ms ms ms ms ms sensitivity analysis disks various speeds 
plots interpreted faster disks cases slower cpus perfect prefetching larger page sizes 
ratio cvm traditional vm paging time percentage memory size pages normalized paging time rscheme wkdm sparc traditional vm ms ms ms ms ms sensitivity analysis rscheme disks various speeds 
plots interpreted faster disks cases slower cpus perfect prefetching larger page sizes 
tive caching system page compression timings measured mhz ultrasparc 
line represents paging costs simulations disk fault cost 
costs normalized performance conventional lru memory disk page access time curve represents speedup slowdown comes compressed caching 
middle line plot regarded performance machine speed mhz ultrasparc average page fetch cost kbyte pages ms half average disk seek time fast disk 
note normalized performance terms assuming disk twice fast exactly equivalent assuming cpu twice slow 
time studying case fast disk conservatively covers case perfect prefetching multiple pages disk twice fast equivalent prefetching page seek 
turn conservatively covers case larger page sizes 
sensitivity analysis account fast disks subsumes scenarios 
looking middle line plot see disk page access cost ms programs show reduction paging times percent averaged interesting range memory sizes 
compressed caching clear win disk access cost ms kbyte page 
line middle taken represent system cpu speed disk costs factor lower ms kbyte page 
performance system significantly worse speedup obtained 
top line represents system disk page accesses cost ms kbyte page 
programs degrades performance point compressed caching clearly worthwhile 
different perspective look lower line see performance system disk speed processor twice fast 
programs doubling cpu speed offers significant additional speedup typically decreasing remaining paging costs percent 
related addressed compression machine code shown possible compress machine code factor specially tuned version conventional compressor yu factor compressor understands instruction set eef 
believe similar techniques fast achieve compression ratio similar ratios get data 
ratio size reduction russinovich extremely fast simple untuned general purpose compression algorithm code data pages rc 
significant previous study douglis mention earlier performed implementing compressed caching scheme virtual memory system sprite operating system 
evaluated decstation roughly times slower machines experiments 
douglis results mixed compressed caching beneficial programs detrimental 
apparent discussion performance modeling believe primarily due slow hardware today standards 
supported sensitivity analysis showed machine times slower mhz ultrasparc yield mixed results better compression algorithms available douglis 
discussed earlier russinovich study rc showed simple compressed cache achieve significant benefits pc application workload studied 
results ac reflect trade offs involved 
hand reported compression overheads unrealistically low ms compression intel dx improbable process speeds memory bandwidth limitations 
single factor responsible results high overhead handling page fault incurred ms overhead containing actual seek time 
overhead certainly result slow processor possibly artifact os windows implementation 
study compressed caching performed published done gooch jones 
simulations demonstrate efficacy compressed caching 
additionally addressed problem memory management variable size compressed pages 
experiments lzrw compression algorithm software showed programs kinds reduction paging costs observed 
benefits greater hardware implementation algorithm 
gooch jones address issue adaptively resizing compressed cache response behavior 
assumed beneficial compress pages avoid disk faults 
clearly true pages compressed memory accesses may suffer decompression overhead disk faults may avoided 
purpose adaptive mechanism determine trade beneficial compression performed 
gooch jones acknowledge compressed cache sizes damage performance 
results strongly suggest need adaptivity test programs exhibit performance deterioration software compression memory sizes 
study area performed cortes ccb 
group inserted cache compressed pages uncompressed cache disk 
consider cache compressed cache pages fetched disk decompressed directly uncompressed cache copy kept compressed cache 
matter terminology disagree approach managing fetching decompression disk faults identical constitutes compressed cache ram cache compressed pages 
experiments researchers implemented compressed cache linux operating system architecture 
compression lzo algorithm 
studies paging traffic compressible pages reduced original size 
test suite observed slight loss performance overhead attempting compressed caching great improvement factor improvement runtime 
average runtime reduced 
gooch jones study clearly addresses need adaptivity 
experimented cache sizes varied kbytes mbytes 
choice size yielded beneficial results note system sensitive choice 
compressed caching appears quite attractive current machines offering improvement tens percent virtual memory system performance 
improvement largely due increases cpu speeds relative disk speeds substantial additional gains come better compression algorithms successful adaptivity program behavior 
programs examined currently available hardware virtual memory system uses compressed caching incur significantly paging cost 
memory sizes running program suffers tolerable amounts paging compressed caching eliminates paging cost average savings approximately 
gap processor speed disk speed increases benefit continue improve 
recency approach adaptively resizing compression cache provides substantial benefit nearly memory size kinds programs 
tests adaptive resizing provided benefit wide range memory sizes program paging little 
adaptivity perfect small cost may incurred due failed attempts resize cache performs vast majority programs 
capable providing benefit small medium large footprint programs 
chapter cpu speeds increase year disk latencies decrease number computational cycles wasted virtual memory paging grow 
initial studies virtual memory paging conducted performance cost due poor caching decisions nearly high 
memory management critical issue performance kinds computation paging expensive memory management operations 
virtual memory paging known drastic performance decrease causes 
widespread belief predictive tasks required line caching policies provide graceful performance degradation line management techniques 
line technique able match omniscience line policies shown graceful degradation possible 
considering concept timescale relativity able develop better understanding performance virtual memory systems different patterns 
turn allowed develop simple effective methods recognize various patterns runtime 
applied methods eelru size adaptive compressed caching adapt behavior timescale relative way 
form adaptivity yielded promising results improve paging performance provide graceful degradation line paging needs 
potential advances lost new virtual memory idea evaluated real implementation results unduly influenced uncontrolled aspects particular system 
compressed caching example provided little benefit idea valid implementation hardware blame 
furthermore new ideas yield benefit real implementation rarely examined depth cause benefit understood 
response problems evaluation real techniques developed understood trace driven simulation 
timescale relativity adaptive caching dissertation tied analyses developments concept timescale relativity 
time virtual memory system depends behavior memory size 
advance quickly slowly depending phases program execution 
relationships virtual memory time time real time vary greatly single program execution 
timescales commensurate need study virtual memory events virtual memory timescale 
idea studying phenomena appropriate timescale trivial rarely understand paging behavior 

page replacement heart virtual memory system 
order understand replacement policy performs crucial stand predictions making pages 
replacement policy predicts fashion soon believes page 
order understand prediction means appropriate notion 
determine notion employed policy 
timescale events recorded policy responds dictates soon means 
describing notion employed line line policies understand performance policy different patterns 
aggregate behavior 
viewing behavior recency information aggregate view patterns 
may coarse grained approach past patterns inferior finegrained specific information aggregation strength 
deal variance programs repeat patterns 
seeking predict region recency axis accessed respond generally different kinds access patterns 
specific predictions page access histories tend brittle perform poorly underlying assumptions incorrect 
concentrating aggregate behavior virtual memory system robust 
furthermore aggregate behavior easier identify coarsegrained patterns 
examining higher chunked level characterize patterns policies adapt patterns 
decay adaption phases 
important developments ability decay information timescale relative manner 
policies give equal weight past lfu fail notice changes phase past phases dominate available statistics 
methods give greater weight virtual memory events past adapt easily phase changes 
eelru size adaptive compressed caching scheme take advantage kind informational decay 
information collected lru queue resident non resident pages form adaptivity detect patterns adapt 
recency information decayed proper rate form adaptivity jump respond transient behavior fail respond persistent behavior 
decay tied virtual memory timescale 
observed constant perform exponential decay nearly important virtual memory time determine quickly information age 
form adaptivity applied simple fashion 
recency information perform cost benefit analyses allow virtual memory scheme choose beneficial management approach 
aging information recency information reflects patterns causes cost benefit analysis predict reliably patterns persist near 
form timescale relative adaptivity played large role making graceful decay paging performance possible 
recency information reveal patterns cause disastrous amounts paging 
decaying information system detect phases exhibit patterns 
eelru compressed caching improve paging performance programs memory available adapting phases 
uses kind adaptivity addressed 
kind analysis aid line adaptivity variable space management 
kind recency information better decisions effect space increase decrease process 
early eviction compression provide interesting alternatives addition subtraction space process 
topics advance sophistication scheduling variable space management policies time decades 
trace reduction simulation ideas evaluated trace driven simulation 
order determine effects different variables virtual memory system wanted control experiments 
wanted experiments reproducible scientific experiment 
wanted experiment wide range configurations 
traces difficult collect share size 
length traces needed collect large reasons processors perform tens hundreds millions memory second simple traces blocked pages grow gigabytes length seconds execution 
second compressed caching study wanted traces augmented images page referenced multiplying length simple trace approximately 
trace reduction methods statistically sampling designed hardware caches virtual memory systems introduce greater error desirable 
existing reduction methods designed virtual memory studies limited accuracy smith stack deletion output trace coffman randell behavior sequences glass cao technique lossless reduce simulation time lempel ziv compressors 
developed sad olr overcome limitations 
gather traces yield exact results lru memories user chosen minimal size 
traces reduced sad simulate opt policy exactly memories smaller minimal size 
lru opt nearly paging study lru policies dominate real page replacement opt provides standard line policies measured 
sad olr introduce error simulation policies sd cited decades 
significant reductions sad olr traces simulation ideas compressed caching possible simulation requires traces store extensive information page images storage simulation time 
compressed caching compressed caching discarded idea deemed useful machines slow non existent backing stores 
consideration increasing gap processor disk speeds improvements compression memory data led re evaluate compressed caching 
simulations shown significant portion program paging time eliminated 
memory data domain specific compression 
improvements compression memory data contributed deal positive results 
wk algorithms demonstrate important ideas strong simple regularities memory data captured computational expense 
number improvements current wk algorithms implementations attempts provided better compression algorithm programs 
second domain specificity wk demonstrates general purpose compressors may widely applicable necessarily provide tightest compression fastest execution 
importance compression tailored specific domain lost proofs optimality assumptions input data assumptions applicable highest level models domains 
adapting compressed cache size 
timescale relative adaptation eelru detect patterns recency applied problem choosing compressed cache size 
adaptivity introduced small overhead selecting compressed cache sizes led reduced paging different programs large range memory sizes 
advantages provided compressed caching improve gap processor speed disk speed grows 
compression decompression expensive 
greater number compressed cache hits accommodated adaptivity method attempt aggressive mistakes costly 
extra processor cycles compress tightly providing larger effective main memory sizes 
study effects compressed caching multiprogrammed environments 
mentioned earlier scheduling variable space allocation change drastically information provided recency adaptivity options afforded compressed caching 
compressed caching longer idea relegated small poorly connected machines applicable different kinds workloads hardware configurations 
bibliography adu alfred aho peter denning jeffrey ullman 
principles optimal page replacement 
journal acm january 
ah agarwal huffman 
blocking exploiting spatial locality trace compaction 
proceedings acm sigmetrics pages 
andrew appel kai li 
virtual memory primitives user programs 
fourth international conference architectural support programming languages operating systems asplos iv pages santa clara california april 
bab babaoglu 
efficient generation memory strings lru stack model program behaviour 
proceedings performance pages 
bcw timothy bell john cleary ian witten 
text compression 
prentice hall englewood cliffs new jersey 
bel belady 
study replacement algorithms virtual storage 
ibm systems journal pages 
bf babaoglu domenico ferrari 
level replacement decisions paging stores 
ieee transactions computers december 
bfh fletcher 
paging studies atlas computer 
information processing ifip congress booklet 
jon louis bentley daniel sleator robert tarjan victor wei 
locally adaptive data compression scheme 
communications acm april 
car richard carr 
virtual memory management 
research press ann arbor mich 
ccb cortes 
improving application performance swap compression 
proceedings freenix track usenix annual technical conference pages 
usenix june 
clr thomas cormen charles leiserson ronald rivest 
algorithms 
mcgraw hill mit press 
cr coffman randell 
performance predictions extended paged memories 
acta informatica 
cv 
overview multics system 
afips conf proc pages 
cv courtois 
model program paging behavior 
acta informatica 
den peter denning 
virtual memory 
computing surveys september 
den peter denning 
working set model program behavior 
communications acm 
den peter denning 
working sets past 
ieee transactions software engineering se january 
dou fred douglis 
compression cache line compression extend physical memory 
proceedings winter usenix conference pages san diego california january 
eef jens ernst william evans christopher fraser steven lucco todd proebsting 
code compression 
proceedings sigplan conference programming language design implementation las vega nevada june 
acm press 
fg franklin gupta 
computation pf probabilities program transition diagrams 
communications acm 
flw fernandez lang wood 
effect replacement algorithms paged buffer database system 
ibm systems journal 
frt peter john robinson joy thomas 
parallel compression cooperative dictionary construction 
proceedings data compression conference pages 
ieee computer society press 
gc gideon glass pei cao 
adaptive page replacement memory behavior 
sigmetrics acm sigmetrics international conference measurement modeling computer systems volume pages 
acm press june 
jh johnson ha 
lossless address trace compression reducing file size access time 
proceedings ieee international conference computers communications pages 
joh mark johnstone 
non compacting memory allocation garbage collection 
phd thesis department computer sciences university texas austin december 
morten gooch jones 
main memory hardware data compression 
nd euromicro conference pages 
ieee computer society press september 
morten gooch jones 
empirical study characteristics compressibility 
iee proceedings comput 
digit 
tech volume pages 
iee january 
gooch jones 
performance evaluation computer architectures main memory data compression 
journal systems architecture 
kpr anna karlin phillips raghavan 
markov paging 
ieee symposium foundations computer science pages 
ieee computer society press 
lee crowley baer anderson bershad 
execution characteristics desktop applications windows nt 
th annual international symposium computer architecture 
ieee computer society press 
lck lee choi jong hun kim sam noh sang min cho chong sang kim 
existence spectrum policies subsumes lru frequently lfu policies 
sigmetrics sig pages 
mattson traiger 
evaluation techniques storage hierarchies 
ibm systems journal 
nel mark nelson 
data compression book nd ed 
books 
markus 
lzo real time data compression library 
pha 
modeling managing program memory hierarchy 
phd thesis rutgers university 
thomas 
analysis cache replacement algorithms 
phd thesis university massachussetts dept electrical computer engineering february 
rc mark russinovich bryce 
ram compression analysis february 
reilly online publishing report available ftp uni mannheim de info windows win update model html 
rd robertson 
data cache management frequency replacement 
sigmetrics 
luigi rizzo 
fast algorithm ram compression 
operating systems review pages 
sam samples 
loss trace compaction 
acm sigmetrics pages may 
sig sigmetrics acm sigmetrics international conference measurement modeling computer systems 
acm press june 
yannis smaragdakis scott kaplan paul wilson 
eelru simple efficient adaptive page replacement 
sigmetrics acm sigmetrics international conference measurement modeling computer systems sig 
sma yannis smaragdakis 
trace reduction lru simulations 
technical report university texas austin 
smi alan smith 
methods efficient analysis address trace data 
ieee transactions software engineering se january 
spi 
distance string models program behavior 
ieee computer 
st daniel dominic sleator robert endre tarjan 
amortized efficiency list update paging rules 
communications acm 
sw walter smith robert 
model address oriented software hardware 
th hawaii international conference systems sciences january 
tl turner levy 
segmented fifo page replacement 
sigmetrics acm sigmetrics international conference measurement modeling computer systems 
acm press 
tor 
unified analysis paging caching 
algorithmica 
um richard uhlig trevor mudge 
trace driven memory simulation survey 
computing surveys 
wfl wood fernandez lang 
minimization demand paging lru stack model program behavior 
information processing letters pages 
wil paul wilson 
issues strategies heap management memory hierarchies 
oopsla ecoop workshop garbage collection object oriented systems october 
appears sigplan notices march 
wil ross williams 
extremely fast ziv lempel compression algorithm 
data compression conference pages april 
wil paul wilson 
operating system support small objects 
international workshop object orientation operating systems pages palo alto california october 
ieee press 
wjnb paul wilson mark johnstone michael neely david boles 
dynamic storage allocation survey critical review 
international workshop memory management scotland uk 
springer verlag lncs 
paul wilson scott kaplan 
virtual memory tracing user level access protections 
preparation 
paul wilson kakkad mukherjee 
anomalies adaptation analysis development policies 
journal systems software 
yu tong lai yu 
data compression pc software distribution 
software practice experience november 
vita scott frederick kaplan born new haven connecticut april fourth son harold barbara kaplan 
graduating hopkins school new haven attended amherst college 
received bachelor arts computer science school began ph program department computer sciences university texas austin 
permanent address department mathematics computer science amherst college box amherst ma dissertation typeset author 
extension collection macros trademark american mathematical society 
macros formatting dissertation written dinesh das department computer sciences university texas austin 

