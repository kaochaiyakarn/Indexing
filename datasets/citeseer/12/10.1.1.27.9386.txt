leonid peshkin ai mit edu mit ai laboratory technology square cambridge ma learning cooperate policy search kee kim kek cs brown edu computer science dept brown university box providence ri cooperative games agents share payoff structure 
reinforcement learning algorithms variants learning applied learning cooperative games apply game state completely observable agents 
policy search methods reasonable alternative value methods partially observable environments 
provide gradient distributed method cooperative games compare notion local optimum nash equilibrium 
demonstrate effectiveness method experimentally small partially observable simulated soccer domain 
interaction decision makers share environment traditionally studied game theory economics 
game theoretic formalism general analyzes problem terms solution concepts nash equilibrium usually works assumption environment perfectly known agents 
reinforcement learning explicit model environment assumed learning happens trial error 
interest applying reinforcement learning algorithms multi agent environments 
example littman describes analyzes learning algorithm finding optimal policies framework zero sum markov games players strictly opposite interests 
hu wellman propose different multi agent learning algorithm general sum games argue converges nash equilibrium :10.1.1.138.2589
simpler interesting case multiple agents share objectives 
study behavior agents nicolas meuleau nm ai mit edu mit ai laboratory technology square cambridge ma leslie pack kaelbling ai mit edu mit ai laboratory technology square cambridge ma employing learning individually claus boutilier focusing influence game structure exploration strategies convergence nash equilibria 
boutilier extension value iteration developed allows agent reason explicitly state coordination 
research assumes agents ability completely reliably observe state environment reward received system 
schneider investigate case distributed reinforcement learning agents complete reliable state observation receive local reinforcement signal 
investigate rules allow individual agents share reinforcement neighbors 
investigate complementary problem agents receive shared reward signal incomplete unreliable generally different perceptions world state 
environments methods generally inappropriate causing turn policy search methods applied previously single agent partially observable domains :10.1.1.102.8052:10.1.1.14.5408
describe gradient descent policy search algorithm cooperative multi agent domains 
setting agent performs action observation individual strategy receive payoff 
objective find learning algorithm agent independently find strategy enables group agents receive optimal payoff 
possible general distributed algorithm finds local optima space agents policies 
rest organized follows 
section give formal definition cooperative multi agent environment 
section review gradient descent algorithm policy search develop multiagent setting 
section discuss different notions optimality strategies 
empirical results section 
identical payoff games identical payoff stochastic game describes interaction set agents markov environment receive payoffs tuple discrete state space probability distribution initial state collection agents agent tuple ai oi bi discrete action space ai discrete observation space oi observation function bi oi mapping states environment actions agents probability distributions states environment andr payoff function ai joint action space agents 
agents identity observation function game completely observable 
partially observable 
time step agent observes oi corresponding bi selects action ak strategy compound action 
am joint action space performed inducing state transition environment identical reward received agents 
objective agent choose strategy maximizes value game 
discount factor set strategies distribution initial state value game 
general case strategy agent mapping history observations game current action 
limit consideration cases agent actions may depend current observation agent finite internal memory 
actions depend current observation policy called memoryless reactive policy 
dependence probabilistic call stochastic reactive policy deterministic reactive policy 
note completely observable reactive policies sufficient implement best possible joint strategy 
follows directly fact mdp optimal deterministic reactive policy 
mdp product action space ai corresponding completely observable representable deterministic reactive policies agent 
called stochastic games markov games multi agent markov decision processes :10.1.1.138.2589:10.1.1.138.2589
denotes set probability distributions defined space 
influence diagram agents fscs 
shown partially observable environments best reactive policy arbitrarily worse best policy memory 
statement easily extended 
possibilities constructing policies memory 
finite state controller fsc agent 
detailed description fscs derivation algorithms learning may previous simply state definition 
finite state controller fsc agent action space observation space tuple finite set internal controller states probability distribution initial internal state internal state transition function maps internal state observation probability distribution internal states action function maps internal state probability distribution actions 
depicts influence diagram agents controlled fscs 
note partially observable environments agents controlled fscs memory represent optimal policy general require infinite memory partially observable markov decision process pomdp 
concentrate problem finding locally optimal controller class fscs fixed size memory 
better understand consider example boutilier illustrated 
agents choice actions states 
transitions deterministic labeled joint action corresponds transition 
instance joint action corresponds agent performing action second agent performing action refers action taken corresponding agent 
coordination problem completely observable identical payoff game 
starting state agent decides move environment state performing action state performing action state matter agents step receive reward state risk free 
state agents choice cooperating choosing action reward state choosing different actions getting state 
represent joint policy parameters agent state denoting probability agent perform action corresponding state 
parameters important outcome 
optimal joint policies deterministic reactive policies 
gradient descent policy search section introduce general method gradient descent policy spaces show applied multi agent problems 
basic algorithm williams introduced notion policy search reinforcement learning reinforce algorithm generalized broader class error criteria baird moore 
start considering case single agent interacting pomdp 
agent policy assumed depend internal state values finite set commitment details policy architecture long defines probability action past history continuous differentiable function set parameters establish notation 
denote ht set possible experience sequences length order specify element part history time write example th reward action history denote prefix sequence def ht truncated time 
thevalue defined equation rewritten ht pr 
assume policy expressed parametrically terms vector weights wm 
calculate derivative wk possible exact gradient descent value making updates wk compute deriva wk tive weight wk wk ht ht pr wk pr ln wk pr spirit reinforcement learning assume knowledge world model allow calculate pr retreat stochastic gradient descent 
sample distribution histories interacting environment calculate trial estimate gradient accumulating quantities ln pr wk particular policy architecture readily translated gradient descent algorithm guaranteed converge local optimum central control factored actions consider case action factored meaning action consists components am 
consider kinds controllers joint controller policy mapping observations complete joint distribution controller independent sub policies ai ai possibly dependence individual internal state action component 
factored controllers represent subset policies represented joint controllers 
obviously product policies factored controller ai represented joint controller pr pr ai 
stochastic joint controllers represented factored controller require coordination probabilistic choice action components illustrate example 
action component controls liquid component meal milk second controls solid cereal 
sake argument assume sticking combination mixed strategy meaning healthy diet want eat milk cereal times 
optimal policy randomized say time time milk cereal 
components controlled independently represent policy 
randomization forced drink cereal milk occasions 
interested individual agents learning independent policies concentrate learning best factored controller domain suboptimal global sense 
requiring controller factored simply puts constraints class policies distributions represented 
stochastic gradient descent techniques previous section applied directly case find local optima controller space 
call method joint gradient descent 
distributed control factored actions step learn choose action components centrally distributed control multiple agents 
obvious strategy agent perform gradient descent algorithm parallel adapt parameters local policy ai 
surprisingly distributed gradient descent dgd method effective 
theorem factored controllers distributed gradient descent equivalent joint gradient descent 
proof show controllers algorithm stepwise starting point search space data sequence algorithms converge locally optimal parameter setting 
factored controller described nm om nm am corresponding history individual agent hi ni oi ni ai clear collection hm individual histories agent specifies joint history joint gradient descent algorithm requires draw sample histories pr gradient descent sample gradient time step history equal factored controller executed single agent implemented agents individually executing policies ai parallel joint histories generated distribution pr am 
distributed algorithm sampling correct distribution 
show weight updates distributed algorithm joint 
wk set parameters controlling action component ak 
ln pr action probabilities agent independent parameters agents policies 
mind factored controllers derivative expression ln pr ln pr ai ln pr ai ln pr ak 
weight updates performed dgd joint gradient descent factored controller 
theorem shows policy learning control component actions distributed independent agents aware choice actions 
important requirement agents perform simultaneous learning naturally synchronized coming rewards 
relating local optima nash equilibria game theory nash equilibrium common solution concept 
gradient descent methods guaranteed converge local optima policy space useful understand points related nash equilibria 
limit discussion case results generalizable agents 
nash equilibrium pair strategies deviation agent strategy assuming agent strategy fixed improve performance 
formally equilibrium point pair strategies inequalities strict called strict nash equilibrium 
discounted stochastic game nash equilibrium point :10.1.1.138.2589
shown certain convexity assumptions shape payoff functions gradient descent process converges equilibrium point 
clear optimal nash equilibrium point nash equilibrium highest value possible point convergence gradient descent algorithm global optimum policy space 
return game described 
optimal strict nash equilibria 
set sub optimal nash equilibria take value interval take value interval 
sub optimal nash equilibria represent situations agent chooses bottom branch second agent acts moderately randomly state 
cases strictly better agent stay bottom branch expected value 
second agent payoff matter behaves incentive commit particular action state necessary upper branch preferred 
problem nash equilibria local optima gradient descent algorithm 
unfortunately equivalence holds direction general case 
state precisely theorems 
theorem strict nash equilibrium local optimum gradient descent space parameters factored controller 
proof assume agents denote strategy point strict nash equilibrium encoded parameter vector simplicity assume boundary parameter space weight locally relevant weight changes policy changes 
definition nash equilibrium change value parameters agent change agent parameters results decrease value words wj equilibrium point 
implies singular point furthermore value decreases direction maximum 
case locally irrelevant parameter ridge direction 
points ridge singular strict local optima essentially local optima gradient descent 
problem nash equilibria boundary parameter space interesting 
convergence points depends details method keep gradient descent boundary 
particular problem comes equilibrium point occurs parameters infinite value uncommon shall see section 
cases equilibrium reached usually approached closely practical purposes 
theorem local optima gradient descent space parameters factored controller nash equilibria 
proof consider situation agent policy single parameter wi policy space described construct value function modes 
assume global maxima 
local optimum nash equilibrium 
experiments established benchmark problems multiagent learning 
illustrate method empirical results problems simple coordination problem small multi agent soccer domain 
simple coordination problem originally discussed policies problem weights corresponding agent state probabilities 
force gradient descent respect bounds simplex standard boltzmann encoding agent state weights wi wi action 
action probability coded function weights wi sa sa 
sb ran dgd learning rate discount factor results shown 
payoff average sample run trials average payoff payoff sample run distributed gradient descent simple problem 
graph sample run illustrates agents typically initially move sub optimal policy 
policy agent takes action second agent acts fairly randomly nash equilibrium saw section 
policy exactly representable boltzmann parameterization requires weights infinite drive probability 
algorithm moves policy reaches exactly 
means advantage second agent drive parameter resulting eventual convergence global optimum note parameterization optima reached exactly 
average runs shows algorithm converges pair policies value close maximum value 
soccer conducted experiments small soccer domain adapted littman 
game played grid shown 
learning agents team single opponent fixed strategy 
time game begins learning agents randomly placed right half field opponent left half field 
cell grid contains player 
player field including opponent equal chance initially possessing ball 
time step player execute actions north south east west stay pass 
agent passes ball transferred agent team time step 
players selected actions executed random order 
player executes action move cell goal goal soccer field 
represent learning agents represents opponent 
occupied player possession ball goes stationary player move occur 
ball falls goals game ends reward 
partially observable version domain test effectiveness dgd agent obtain information player possesses ball status cells north south east west location 
possible observations cell open field occupied 
compare learning curves dgd learning central controller completely observable partially observable cases 
show learning curves dgd action pass order measure learned policies 
graphs summarize simulations game different fixed strategy opponents random executes actions uniformly random 
greedy moves player possessing ball stays 
ball goal 
defensive front goal stays moves random leaves goal area 
show average performance runs error bars standard deviation 
learning rate dgd learning discount factor experiments 
agent dgd team learned reactive policy 
policy parameters initialized drawing uniformly random appropriate domains 
greedy exploration learning 
performance graph reported evaluating greedy policy derived table 
completely observable case domain mdp opponent strategy fixed really adversarial game learning expected learn optimal joint policy 
interesting note slow convergence completely observable learning random opponent 
conjecture average discounted reward average discounted reward learning completely observable learning partially observable dgd partially observable dgd pass partially observable trials thousands learning completely observable learning partially observable dgd partially observable dgd pass partially observable trials thousands average discounted reward average discounted reward learning completely observable learning partially observable dgd partially observable dgd pass partially observable trials thousands learning partially observable dgd partially observable trials thousands learning curves dgd policy search learning defensive opponent top left greedy opponent top right random opponent bottom left team agents different fixed strategies bottom right 
random opponent larger part state space visited 
table value function offers opportunity generalization requires great deal experience converge 
soon observability restricted learning longer reliably converges best strategy 
joint learner input local observations individual players 
behaves quite extremely high variance converges policy bad 
unreliable behavior attributed known problems value function approaches especially learning pomdps 
individual dgd agents stochasticity action choices gives representational abilities unavailable learner 
tried additional experiments dgd agents state fsc performance improve appreciably 
expect experiments larger field players important agents internal state 
despite fact learn independently combination policy search plus different policy class allows gain considerably improved performance 
tell close performance optimal performance partial observability computationally impractical solve exactly 
bernstein show finite horizon case agent solve pomdps worst case complexity sense 
important see dgd agents learned cooperate sense algorithm run domain pass action allows agent give ball teammate performance deteriorates significantly defensive greedy opponents 
agents don know respect goal probably choose pass faced opponent 
completely random opponent strategies equally 
probably sufficient case simply run straight goal cooperation necessary 
performed additional experiments domain opponent behaved greedily 
domain completely observable state space large difficult store table populate reasonable val ues 
just compare state dgd agents limited view centrally controlled learning algorithm 
surprisingly find dgd agents considerably successful 
performed informal experiments increasing number opponents 
opponent team defensive agent increasing number greedy agents 
cases opponent team greedy agents dgd strategy time agents rushed front goal stayed forever 
algorithm distributed learning cooperative multi agent domains 
guaranteed find local optima space factored policies 
show converges nash equilibrium local optima policy space nash equilibria 
algorithm performed small simulated soccer domain 
important apply algorithm complex domains see gradient remains strong drive search effectively see local optima problematic 
interesting extension allow agents perform explicit communication actions see exploited improve performance domain 
addition may interesting connections establish game theory especially relation solution concepts nash equilibrium may appropriate cooperative games 
craig boutilier michael schwarz relevant ideas comments 
leonid peshkin supported nsf ntt nicolas meuleau part research ntt kee kim part afosr rlf leslie pack kaelbling part ntt part darpa contract dabt 
arrow hurwicz 
stability gradient process person games 
journal society industrial applied mathematics 
baird 
reinforcement learning gradient descent 
phd thesis carnegie mellon university pittsburgh pa 
baird moore 
gradient descent general reinforcement learning 
advances neural information processing systems 
mit press 
bernstein zilberstein immerman 
decentralized control markov decision processes 
proceedings sixteenth conference uncertainty artificial intelligence 
boutilier 
sequential optimality coordination multiagent systems 
proceedings sixteenth international joint conference artificial intelligence 
claus boutilier 
dynamics reinforcement learning cooperative multiagent systems 
proceedings tenth innovative applications artificial intelligence conference pages madison wisconsin usa july 
hu wellman :10.1.1.138.2589
multiagent reinforcement learning theoretical framework algorithm 
proceedings fifteenth international conference machine learning pages 
kaelbling littman moore 
reinforcement learning survey 
journal artificial intelligence research 
littman 
memoryless policies theoretical limitations practical results 
animals animats brighton uk 
meuleau peshkin 
kim kaelbling 
learning finite state controllers partially observable environments 
proceedings fifteenth conference uncertainty artificial intelligence pages 
morgan kaufmann 
narendra 
learning automata 
prentice hall 
osborne rubinstein 
course game theory 
mit press 
peshkin meuleau kaelbling 
learning policies external memory 
bratko dzeroski editors proceedings sixteenth international conference machine learning pages san francisco ca 
morgan kaufmann 
puterman 
markov decision processes 
john wiley sons new york 
schneider 
wong moore riedmiller 
distributed value functions 
bratko dzeroski editors proceedings sixteenth international conference machine learning pages san francisco ca 
morgan kaufmann 
singh jaakkola jordan 
learning state estimation partially observable markovian decision processes 
proceedings eleventh international conference machine learning 
sondik 
optimal control partially observable markov processes infinite horizon discounted costs 
operations research 
sutton barto 
reinforcement learning 
mit press cambridge massachusetts 
williams 
class gradient estimating algorithms reinforcement learning neural networks 
proceedings ieee international conference neural networks san diego california 
williams 
simple statistical gradient algorithms connectionist reinforcement learning 
machine learning 
