understanding fault tolerant distributed systems cristian computer science engineering university california san diego la jolla ca cs ucsd edu may propose small number basic concepts explain architecture fault tolerant distributed systems discuss list architectural issues find useful consider designing examining systems 
issue known solutions design alternatives discuss relative merits give examples systems adopt approach 
aim introduce order complex discipline designing understanding fault tolerant distributed systems 
computing systems consist multitude hardware software components bound fail eventually 
systems component failures lead unanticipated potentially disruptive failure behavior service unavailability 
systems designed fault tolerant exhibit defined failure behavior components fail mask component failures users continue provide specified standard service despite occurrence component failures 
users temporary errant system failure behavior service unavailability acceptable 
growing number user communities cost unpredictable potentially hazardous failures system service unavailability significant 
examples include line transaction processing process control computer communications user communities 
minimize losses due unpredictable failure behavior service unavailability users rely fault tolerant systems 
increasing dependence placed computing services number users demand fault tolerance increase 
task designing understanding fault tolerant distributed system architectures notoriously difficult stay control standard system activities components complex situations occur components fail 
difficulty task exacerbated lack clear structuring concepts confusing terminology 
presently quite common see different people different names concept term different concepts 
example person calls failure second person calls fault third person call error 
term fault tolerant ambiguously designate distinct system properties system defined failure behavior system masks component failures 
attempts introduce discipline order understanding faulttolerance issues distributed system architectures 
section propose small number basic architectural concepts 
sections concepts formulate list key hardware software issues arise designing examining architecture fault tolerant distributed systems 
search satisfactory answers issues matter current research experimentation examine various proposals discuss relative merits illustrate existing commercial fault tolerant systems 
useful design guide list issues provides basis classifying existing fault tolerant system architectures 
section comments adequacy proposed concepts 
basic architectural concepts achieve fault tolerance distributed system architecture incorporates redundant processing components 
discuss issues underlie faulttolerance redundancy management systems need introduce basic architectural building blocks classify failures basic blocks experience 
services servers depends relation notions feel provide best means explain computing system architectures concepts service server depends relation servers 
computing service specifies collection operations execution triggered inputs service users passage time 
operation executions may result outputs users service state changes 
example ibm raw processor service consists operations defined processor manual db database service consists relational query update operations clients database 
operations defined service specification performed server service 
server implements service exposing users internal service state representation operation implementation details 
details hidden users need know externally specified service behavior 
servers hardware software implemented 
example raw processor service typically implemented hardware server see service emulated software 
db service typically implemented software conceivable implement service hardware database machine 
servers implement service services implemented servers 
server depends server correctness behavior depends correctness behavior 
server called user client called resource resources turn depend resources provide service atomic resources system wish analyze 
user client resource server names relative depends relation resource server certain level abstraction client user level abstraction 
relation represented acyclic graph nodes denote servers arrows represent depends relation 
customary represent graphically user resource said level abstraction higher 
example file server uses services provided disk space allocation server disk server provide file creation access update deletion service depends see 
implement file service designer assumes allocation services provided certain properties 
specifications imply properties correctly implemented behave correctly 
servers depend processor service provided underlying operating system execute interpreted 
written high level language depend compilers link editors translated correctly machine language 
software servers discussion depend translation processor services customary omit representing fact depends graph 
depends relation note static depends relation defined relates correctness service implementation differs dynamic call flow control interprets executes relations exist run time servers situated different abstraction levels 
example file server typically synchronous blocking calls ask allocation server free storage asynchronous non blocking calls ask server initiate disk parallel 
completed server typically notify file server call cl interrupt processor interprets programs depend executes 
distributed system consists software servers depend processor communication services 
processor service typically provided concurrently software servers multi user operating system unix mvs 
operating systems turn depend raw processor service provided physical processors turn depend lower level hardware resources cpus memories controllers disks displays keyboards 
communication services implemented distributed communication servers implement communication protocols tcp ip sna depending lower level hardware networking services 
customary designate union processor communication service operations provided application servers distributed operating system service 
failure classification server designed provide certain service correct response inputs behaves manner consistent service specification 
assume specification prescribes server response initial server state input real time interval response occur 
server response mean outputs deliver users state transition undergo 
server failure occurs server behave manner specified 
omission failure occurs server omits respond input 
timing failure occurs server response functionally correct response occurs outside real time interval specified 
timing failures early timing failures late timing failures performance failures 
response failure occurs server responds incorrectly value output incorrect value failure state transition takes place incorrect state transition failure 
omission produce output server omits produce output subsequent inputs restart server said suffer crash failure 
depending server state restart distinguish kinds crash failure behaviors 
amnesia crash occurs server restarts predefined initial state depend inputs seen crash 
partial amnesia crash occurs restart part state crash rest state reset predefined initial state 
pause crash occurs server restarts state crash 
halting crash occurs crashed server restarts 
note crashes state servers pause crashes halting crash behaviors subsets omission failure behaviors general partial total amnesia crash behaviors subset omission failure behaviors 
follows follow accepted practice term crash ambiguously designate kinds crash failure behaviors meaning intended clear way state restart operation server consideration defined 
operating system crash followed re boot predefined initial system state database server crash followed recovery database state reflects transactions committed crash examples crash failures 
communication service occasionally loses delay messages example service suffers omission failures 
excessive message transmission message processing delay due overload affecting set communication servers example communication performance failure 
action taken processor soon timer runs fast speak early timing failure 
search procedure finds key inserted table alteration message communication link subject random noise examples server response failures 
server failure semantics programming recovery actions server failure important know failure behaviors server exhibit 
example illustrates point 
consider client sends service request sr communication link server maximum time needed transport sr maximum time needed receive process reply sr designer knows communication affected omission performance failures reply sr received time units receive reply sr handle resend new service request sr maintain local data allow distinguish answers current service requests sr answers old service requests sr hand designer knows suffer performance failures reply sr received time units maintain local data example sequence number allow discard late answer sr recovery actions invoked detection server failure depend failure behaviors server fault tolerant system extend standard specification servers include addition familiar failure free semantics set failure free behaviors failure behaviors failure semantics 
specification server prescribes failure behaviors observed users class say failure semantics discussion mean deferred section 
term failure semantics failure mode semantics widely accepted term characterizing behaviors absence failures logical reason dissimilar words semantics mode label notion allowable server behaviors 
example communication service allowed lose messages probability delays corrupts messages negligible say omission failure semantics discuss negligible means section 
service allowed lose delay messages corrupts messages say omission performance failure semantics 
similarly processor suffer crash failures memory suffer omission failures response read requests parity errors say processor memory crash read omission failure semantics respectively 
general failure specification server allows exhibit behaviors union failure classes say failure semantics 
server failure semantics experience failure behaviors server failure semantics say weaker restrictive failure semantics equivalently stronger restrictive failure behavior allowed server failure semantics specified weakest possible say arbitrary failure semantics 
class arbitrary failure behaviors includes failure classes defined previously 
responsibility server designer ensure properly implements specified failure semantics 
example ensure local area network service omission performance failure semantics standard practice error detecting codes detect high probability message corruption 
ensure local area network omission failure semantics typically uses network access mechanisms guarantee bounded access delays real time executives guarantee upper bounds message transmission processing delays ll 
implement raw hardware processor service crash failure semantics duplication matching physically independent processors execute parallel sequence instructions compare results instruction execution crash occurs disagreement processor outputs detected tw 
general stronger specified failure semantics expensive complex build server implements 
examples illustrate general rule fault tolerant computing 
processor achieves crash failure semantics duplication matching discussed tw expensive build elementary processor form redundancy prevent users seeing arbitrary failure behaviors 
storage system guarantees update completely performed performed failure occurs expensive build storage system restart inconsistent state allows updates partially completed failures occur 
design effort required build real time operating system provides software servers processor service crash failure semantics build standard multi user operating system unix provides processor service crash performance failure semantics ll 
hierarchical failure propagation masking failure behavior classified respect certain server specification certain level abstraction 
server depends lower level servers provide correctly service failure certain type lower level abstraction result failure different type higher level abstraction 
example consider value failure physical transmission layer network causes bits message corrupted 
data link layer physical layer uses bit error detecting codes detect message corruption discards corrupted messages failure propagated omission failure data link layer 
example consider clock affected crash failure displays time 
clock higher level communication server specified associate different timestamps different messages sends different real times communication server may classed experiencing arbitrary failure 
illustrated failure propagation servers situated different abstraction levels depends hierarchy complex phenomenon 
general server depends resource arbitrary failure semantics arbitrary failure semantics means check correctness results provided task checking correctness results provided lower level servers cumbersome fault tolerant system designers prefer possible servers failure semantics stronger arbitrary crash omission performance 
hierarchical systems relying servers exception handling provides convenient way propagate information failure detections abstraction levels mask low level failures higher level servers 
pattern follows 
levels abstraction server depends service implemented lower level calls server information failure propagates means exceptional return time event signalling timely return 
server depends calls lower level servers implement service needs knowledge timing call events able detect lower level server failures 
example server expects interrupt sensor server milliseconds missing sensor data update detected time 
server provide service despite failure say masks failure 
examples masking actions perform calls redundant servers repeated calls suffer transient omission failures 
masking attempts succeed consistent state recovered information failure propagated level abstraction masking attempts take place 
way information failure lower level server hidden human users successful masking attempt propagated human users failure higher level service requested 
programming masking consistent state recovery actions client usually simpler designer knows change state provide standard service 
servers initial state input provide standard service signal exception changing state termed atomic respect exceptions simplify fault tolerant programming provide users simple understand omission failure semantics 
illustrate hierarchical failure masking pattern described ibm mvs operating system example running processor cpus examples hierarchical masking 
attempt reading cpu register results parity check exception detection automatic cpu retry saved cpu state 
masking attempt succeeds data original failure logged human operator notified original transient omission cpu failure occurrence masked mvs operating system software servers 
observed parity exception followed unsuccessful cpu retry reported interrupt crash failure cpu server mvs system turn may attempt mask failure re executing program caused cpu register parity exception previously saved checkpoint alternate cpu 
masking attempt succeeds failure cpu masked higher levels abstraction software servers run application programs 
alternate cpus masking attempts initiated mvs system fail crash failure mvs system occurs 
failure masking server groups ensure service remains available clients despite server failures implement service group redundant physically independent servers fail remaining ones provide service 
say group masks failure member group responds specified users despite failure hierarchical masking discussed previous section requires users implement resource failure masking attempts exception handling code group masking individual member failures entirely hidden users group management mechanisms 
group output function outputs individual group members 
example group output output generated fastest member group output generated distinguished member group result majority vote group member outputs 
phrase group failure semantics shorthand failures observable users class 
server group able mask clients concurrent member failures termed fault tolerant group called single fault tolerant greater group called multiple fault tolerant 
example members server group crash performance failure semantics group output defined output fastest member group mask concurrent member failures provide crash performance failure semantics clients 
similarly primary standby group servers crash performance failure semantics members ranked primary backup second backup 
th backup mask concurrent member failures provide crash performance failure semantics 
group members arbitrary failure semantics output result majority vote outputs computed parallel members mask minority member failures 
majority members fail arbitrary way entire group fail arbitrary way 
hierarchical group masking points continuum failure masking techniques 
practice sees approaches combine elements 
example user primary backup server group sends service requests directly primary detect primary server failure transient service failure explicitly attempt mask failure re sending service request 
service request automatically resent underlying group communication mechanism matches service requests replies automatically detects missing answers contains exception handling code deal situation entire primary backup group fails 
specific mechanisms needed managing redundant server groups way masks member failures time group behavior functionally indistinguishable single servers depend critically failure semantics specified group members communication services 
stronger failure semantics group members communication simpler efficient group management mechanisms 
conversely weaker failure semantics members communication complex expensive group management mechanisms 
illustrate general rule fault tolerant computing consider tolerant storage service elementary storage servers build read omission failure semantics error detecting codes ensure probability read value failures caused bit corruptions negligible implement follows identical physically independent elementary servers interpret write writes interpret read read read results omission failure read 
elementary storage servers suffer omission read value failures possible response read value returned value returned different written elementary physically independent servers needed implementing write results writes servers results elementary reads servers majority vote elementary results returned 
majority exists result read majority value read 
majority exists read results omission failure 
service implemented voting complex expensive service implemented slower 
illustrations rule group management cost increases failure semantics group members communication services weaker es families solutions group communication problem studied increasingly weaker group member communication failure semantics assumptions 
statistical measurements run time overhead practical systems confirm general rule cost group management mechanisms higher failure semantics group members weaker run time cost managing server pair groups crash performance failure semantics low ba cost managing groups arbitrary failure semantics high total throughput system pb 
expensive build servers stronger failure semantics cheaper handle failure behavior servers higher levels abstraction key issue designing multi layered fault tolerant systems balance amounts failure detection recovery masking redundancy various abstraction levels system obtain best possible cost performance dependability results 
example case single fault tolerant storage service described combined cost incorporating effective error correcting codes elementary storage servers implementing tolerant service servers general lower cost storage servers weaker failure semantics voting 
small investment lower level abstraction ensuring lower level servers stronger failure semantics contribute substantial cost savings speed improvements higher levels abstraction result lower cost 
hand deciding redundancy especially masking redundancy lower levels abstraction system wasteful cost effectiveness point view low level redundancy duplicate masking redundancy higher levels abstraction anyway satisfy dependability requirements 
similar cost effectiveness arguments layered implementations fault tolerant communication services discussed src 
choosing right failure semantics probability server suffers failures outside failure class small considered negligible 
terms justified assume failure behaviors class 
answer questions depends stochastic requirements placed system part 
specification server consist functional requirements prescribe server standard failure semantics stochastic specification 
stochastic requirements prescribe minimum probability standard behavior observed run time maximum probability potentially catastrophic failure different specified failure behavior observed 
higher level server depends built critical design decisions depend verification design satisfies standard failure functional specifications relies 
check satisfies stochastic specifications designer rely stochastic modelling simulation testing techniques ensure probability observing behaviors run time probability observing unspecified potentially catastrophic behaviors outside smaller small allow demonstrating design meets stochastic requirements failure semantics appropriate system significant demonstration impossible designer settle failure semantics weaker re design new redundancy management techniques appropriate 
illustrate point consider single fault tolerant storage service specified follows standard functional specification requires read write returns value written failure specification states writes fail admissible read failures omission failures reading value different previously written considered potentially catastrophic consequences stochastic specification requires probability observing specified standard behavior gamma probability observing read value failure gamma assume build decides duplex design physically independent storage servers bit error detecting codes 
specification elementary storage servers follows read returns value previously written probability gamma write succeeds probability read returns value different written gamma read failures value failure read value failure occurs value read different form value written value storage word values different simple stochastic calculation shows duplex design described section satisfies requirements probability read omission failure order gamma probability read value failure order gamma omission failure semantics assumed duplex design appropriate build neglect probability gamma failure assumption violated run time 
requirement design reliable storage service specification probability read value failure gamma duplex design omission failure hypothesis memories bit error detecting codes longer adequate 
design probability gamma read returns corrupted value passed user unacceptably high 
simple stochastic calculation shows design voting described section memories bit error detecting codes satisfies requirements simplicity assume perfect voting 
design ensures probability read omission failure order gamma probability read value failure order gamma goal build storage system longer appropriate designer assume memories bit error detection code omission failure semantics 
probability observing read value failure non negligible designer adopt weaker failure hypothesis memories bit error correcting codes omission value failure semantics 
examples choose server failure semantics highly dependable systems discussed po 
remainder assumes preliminary stochastic analyses practical statistics helped settle issue adequacy choosing certain failure semantics context systems described 
hardware architectural issues provide processor service application software servers operating system needs hardware resources cpus memory controllers communication controllers 
hardware architectures package basic resources single replaceable units 
architectures resources replaceable unit 
replaceable hardware unit mean physical unit failure replacement growth unit fails independently units removed cabinet affecting units added system augment performance capacity availability 
ultimate goal hardware replaceable units enable physically removed failure preventive maintenance horizontal growth inserted back system disrupting activity software servers running higher levels abstraction 
expensive impossible achieve 
goal ensure service provided hardware servers replaceable unit nice failure semantics crash omission higher software levels abstraction provide low overhead hardware failure masking relying stronger failure semantics 
depending qualified field engineer needed remove install replaceable unit customary classify field replaceable need intervention field engineer customer replaceable need 
replaceable units 
grouped connected 
depending granularity replaceable hardware units processor architecture possible distinguish coarse granularity architectures fine granularity architectures 
coarse granularity architecture replaceable units package elementary hardware servers cpus memory controllers communication controllers 
fine granularity architecture elementary hardware server replaceable unit 
examples commercially successful coarse granularity architectures tandem dec vax cluster kls ibm mvs ibm 
examples fine granularity architectures tw sequoia 
examples fault tolerant architectures la 
bus control cpu memory channel disc 
contr 
terminal contr 
tandem processor architecture tandem processor architecture packages cpu memory bus controller servers single replaceable units illustrated 
units communicate dual bus called 
disk tape communication terminal controller servers field replaceable units 
high tandem systems cpu memory units field replaceable newer low systems customer replaceable 
key ideas tandem architecture follows ensure existence disjoint access paths terminal controller physical servers needed interpreting user commands cpu memory disk tape group servers failure masking server pair groups 
operating system implements pair management algorithms decides resources play active role interpreting user commands resources play backup role 
operating system uses combination hierarchical group masking techniques mask hardware resource failures users active hardware resources active path fails operating system uses path continue provide service affected users 
example software server interpreting user commands uses disk certain disk controller controller fails operating system masks failure re directing disk accesses disk controller 
operating system ensure disk bus bus controller failures masked higher level application processes 
cpu memory replaceable unit fails software server executing unit fails 
hardware architecture ensures cpu memory unit access resources needed failed servers exists 
failed server primary group implementing operating system service disk failure automatically masked higher level user servers promotion backup role primary 
user level application servers generally implemented process pairs avoid complications associated programming periodic check pointing failure user application servers visible human users wait servers re started 
duplication hardware resources needed software servers tandem architecture single fault tolerant single hardware replaceable unit failure tolerated provided second replaceable unit failure occurs 
reported server pair groups implement operating system services enables masking significant fraction operating system server failures caused residual software design faults 
single fault tolerance mean impossible mask concurrent hardware replaceable unit failures 
example simultaneous failure disk controller servers attached distinct disks masked higher level software servers 
single fault tolerance means architecture guarantees masking single hardware replaceable unit failure guarantee concurrent replaceable unit failures masked 
exist double failures masked 
example cpu memory disk controller replaceable units attached disk fail service depends disk unavailable 
commercial system architectures discuss single fault tolerant refrain mentioning point 
terminal contr 
terminal contr 
vax vax vax storage contr 
storage contr 
comp 
interconnect lan ethernet 
vax cluster processor architecture vax cluster processor architecture field replaceable units entire containing cpu main storage lan controllers entire storage controllers containing microprocessors specific device drivers lan controllers entire terminal controllers 
kinds local area networks vax cluster high speed custom designed computer interconnect connects storage controllers low speed ethernet 
vax cluster single fault tolerant dual lans needed connecting processors storage controllers 
manner similar tandem architecture idea ensure existence disjoint access paths terminal controller physical resources cpu memory disk tape needed software server interprets user commands 
hardware replaceable unit failures masked combination hierarchical group masking techniques 
disks dually ported different storage controllers controller fails operating system re direct alternate controller 
computer interconnect permanent failure similarly masked operating system level re directing remaining traffic alternate computer interconnect 
vax fails software servers running fail 
vax capacity cluster active running servers explicitly re started case user servers tandem system 
tandem vax cluster architectures single failures cpu memory replaceable units result failure higher level application servers failures visible human users 
system data sets system data sets surveillance journal active system backup system data bases replicated ibm architecture ibm architecture replaceable hardware units entire high ibm processors 
general previously existing processor architectures integrated fault tolerant system local area network granularity resulting architecture naturally coarse 
system provides continuous ims database service group masking group ims servers running distinct high processors connected pointto point local area network provide ims service 
servers primary interprets user requests date view application state server backup lags knowledge current application state 
backup maintains delayed knowledge reading log generated primary 
arrangement allows hardware operating system failures masked users ims service 
communication bus controller controller lan cpu memory disc 
contr 
processor architecture processor architecture introduce systematic cpu communication controller hardware servers implemented paired microprocessors execute identical instruction streams continuously compare results 
implementation technique ensures servers crash failure semantics high probability 
characteristic architecture fine granularity elementary hardware servers customer replaceable units power supply 
substantially reduce maintenance costs ease horizontal growth 
third idea group masking hardware level pairing elementary hardware servers crash omission failure semantics 
enables hardware server failures masked higher software levels 
example order provide single fault tolerant processor service software servers cpu servers crash failure semantics implemented pair microprocessors executing lock step continuously compare results organized masking group 
group member receives sequence inputs 
absence member failures output member accepted 
member suffers crash failure detected disagreement microprocessors implement output taken member 
similarly memory banks read omission failure semantics implemented bit detection bit correction codes obtain single fault tolerant storage service read omission failure semantics 
elementary hardware server failures masked directly hardware level 
example disk failures hierarchically masked operating system level lower hardware levels state information enable pure hardware group masking take place 
dual bus called enables replaceable units processor communicate despite single bus failure dual lan named enables processors communicate despite single failure 
terminals optionally connected processor pair communication buses redundantly connected communication controllers attached processor 
enables system mask single communication controller bus failures requiring user move terminal 
double hardware failure operating system failure cause operating system crash 
case user servers services provided operating system fail 
shows sequoia multi processor architecture example fine granularity architecture 
cpu controller services implemented paired microprocessors execute instruction stream lock step continuously compare results architecture 
ensures cpu controller hardware services operating system servers crash failure semantics 
memory servers rely error detecting correcting codes implement omission failure semantics high probability 
individual cpu memory controller servers packaged customer replaceable units 
fault tolerant communication replaceable units provided dual system bus 
bus composed processor segment connects cpus memory segment connects memory elements elements global segment connects previously mentioned buses master mi slave si interfaces 
master slave interfaces provide electrical isolation adjacent bus segments error confinement reasons 
operating system uses hierarchical masking techniques hide single hardware server failure software processes running 
example cpu server crashes operating system automatically attempts re start software server executed failed cpu cpu server previously saved check point server state 
ensure server state updates atomic respect cpu crashes shadowing processor processor si si mi mi cpu cpu cpu slave interfaces global bus master interfaces global bus local bus processor memory local bus global bus 
memory element memory element sequoia multiprocessor architecture technique adopted 
requires operating system maintains copies state software server different memory units 
mask memory read omission failures writable data pages duplicated different physical memory elements read executable pages stored disks 
memory element fails data page read recovered memory element disk depending writable readable executable page 
operating system mask single disk failures duplicating disk pages dual ported disks attached different controllers 
way operating system mask single hardware replaceable unit failure long cpu controller memory servers correctly failures occur fully recovered second failure occurs 
number correctly working hardware servers drops thresholds operating system continue provide processor service crash performance failure semantics long cpu controller memory server continue correctly 
multiple concurrent failures occur cpu memory controller service longer available cpu memory controllers failed operating system higher level software servers fail 
processor service failures course occur residual design faults operating system 
failure assumptions hardware replaceable units 
developers operating system software typically assume cpu communication controller servers crash failure semantics memory elements read omission failure semantics disks seek performance failure semantics read write omission failure semantics communication buses communication lines omission omission performance failure semantics 
available field statistics indicate commercial line transaction processing systems characterized order minutes hours year failure assumptions adequate 
previously mentioned systems assumptions 
strong hardware failure semantics enable system designers known hierarchical masking techniques mask hardware server failures storage mask loss data replicated memory disk servers read write omission failure semantics ls virtual circuits mask omission performance communication failures time outs sequence numbers retries 
masking possible strong hardware server failure semantics omission crash enable system programmers ensure operating system communication services implement strong failure semantics 
example cpu disk controllers crash failure semantics disks read write omission failure semantics enable implementation higher level stable storage service ls write operations atomic respect crashes stable storage write interrupted crash carried completion performed 
stable storage service higher level servers implement atomic transactions relying known database recovery techniques write ahead logging phase commit 
similarly lower level data gram communication services omission performance failure semantics enable implementation higher level virtual circuits crash failure semantics 
restrictive hypotheses failure semantics basic hardware replaceable units complexity cost detecting diagnosing recovering elementary hardware server failures increases levels regard unacceptable commercial applications line transaction processing telecommunications 
examples processor architectures designed highly critical application areas ultra high stochastic dependability requirements designers assume elementary hardware servers arbitrary failure semantics ha hld wa 
specified hardware failure semantics implemented order detect failures buses communication lines memory disk servers previously discussed architectures error detecting codes 
hardware failure detection technique understood 
rich literature specializing error detecting codes subject reached fairly mature state pw 
detect failures hardware servers cpu communication controllers systems ibm vax high tandem error detecting codes newer systems sequoia tandem dec lock step duplication comparison 
error detecting codes remain choice method detecting failures storage communication hardware servers memories disks buses communication lines give way duplication matching complex circuits cpus device communication controllers shelf microprocessors 
reason trend duplication matching provide better approximation crash failure semantics complex servers error detecting codes 
cpus controllers error detecting codes possibility data written bus storage cycles failure detection erroneous duplication matching self checking comparator circuits virtually eliminates possibility damage 
cost excellent failure detection capability physical hardware servers plus comparison logic needed elementary server augmented error detecting circuitry 
providing better guarantee crash failure semantics complex servers cpu controllers duplication matching number attractive characteristics 
absence error detecting circuitry elementary physical servers reduces complexity leading increased reliability reduced design testing costs 
elimination error detecting circuitry servers faster 
reason lock step duplication availability cheap fast microprocessors error detection circuitry 
pairing shelf components adding comparator cheaper developing proprietary processor designs elaborate error detecting capabilities 
pairing shelf processors enables computer system manufacturers ride wave rapid improvements chip speed update product lines promptly new chips available market 
advantage lockstep duplication worth mentioning improved software quality growth 
elementary hardware server failure promptly detected disagreement data damage occurs easier disambiguate failures caused software design faults failures caused physical faults 
operating system failure occurs disagreement hardware processors observed high probability failure due software design fault 
sharp contrast occurs traditional processor architectures error correcting codes 
architectures hardware failures cpu servers result undetectable damage data due latencies failure detection large class system failures attributable software fact caused hardware 
knowledge hardware failures significant probability masquerade software design faults create serious difficulties proper diagnosis system failures 
case operating system developers may blame hardware malfunctioning system failures diagnose hardware developers may blame operating system failures hardware apparently worked properly 
replaceable hardware unit failures masked 
possible implement redundancy management mechanisms mask hardware server failures directly hardware example group masking techniques physical hardware servers arbitrary failure semantics voting ha hardware servers crash failure semantics turn implemented server pairs duplication matching tw 
allows single processor failures masked higher software levels abstraction increases mean time failures raw processor service 
note hardware eliminate need handling application software levels processor service failures way service implemented non redundant processor 
failures occur frequently occur handled 
example transactions implemented database service level atomic respect processor service crashes code implementing transaction failure semantics written possibly relying logging recovery facilities provided underlying operating system processor service implemented simple cpu cpu arrangement 
note hardware help providing fault tolerance operating system application level 
systems attempt mask hardware server failures operating system level application software servers continue run interruption 
example sequoia mvs operating systems combination hierarchical group masking methods hide single cpu failures higher level software servers restarting server ran failed cpu manner transparent server 
similar masking cpu bus disk controller failures done operating systems tandem vax cluster ibm architectures 
choice masking hardware server failures operating system level provide tolerance hardware operating system server failures solve problem ensure fault tolerance application services 
redundant application software server groups allows hardware operating system software server failures masked highest level abstraction application level 
idea implement service available users despite hardware software failures group redundant software servers run distinct hardware processor hosts maintain redundant information global service state 
group member fails lower level hardware software service failure residual design fault program surviving group members service state information continue provide service 
approach masking server failures highest application level powerful mask hardware software server failures successful commercial systems discussed previously tandem ibm redundant software server groups 
software architectural issues software servers analogous hardware replaceable units 
basic units failure replacement growth software 
hardware replaceable units ultimate goal enable software servers removed system due failures upgrades horizontal growth disrupting activity users 
impossible achieve cost complexity reasons number active servers providing service drops threshold goal ensure software servers nice failure semantics crash omission performance 
allow possibly human users recover failures relying simple masking protocols log wait time try 
similarity goals issues dealt software architecture level similar ones discussed section 
methods solving issues quite different implementation details 
failure semantics specified software servers 
depending state service persistent require application servers implement service provide atomic transaction failure semantics simply amnesia partial amnesia crash failure semantics 
atomic transaction sequence operations persistent data usually stored databases change consistent database state consistent database state change database state lower level processor service crash occurs ls 
software servers low level communication controllers typically persistent state usually content amnesia crash failure semantics failure servers properly reinitialize accept new service requests 
implement atomic transaction simply crash failure semantics commercial systems described previously assume programs implement operations exported software servers totally partially correct 
program totally correct behaves specified response input long services uses fail 
partially correct program may suffer crash performance failure certain inputs services uses fail 
way server delivers result performs state transition response service request result state transition correct result may timely delivered 
partial correctness basic hypothesis failure semantics software servers appealing reasons 
easily achievable total correctness 
second achievable practice undergoing extensive reviews tests software available commercially occasionally crashes slow output bad results 
third partially correct server definition crash performance failure semantics lower level software hardware services depends fail maintains crash performance failure semantics lower level services fail provided lower level service failures crash omission performance failures 
levels abstraction system hardware operating system applications servers crash omission performance failure semantics system crash omission performance failure semantics 
yields simple understand homogeneous quality assurance goal abstraction levels hardware crash omission failure semantics software totally partially correct 
special purpose systems assume component servers nice failure semantics crash omission performance aa ha hld wa 
assuming group management services clock synchronization atomic broadcast voting correctly systems mask minority server failures application groups built members arbitrary failure semantics 
depending assumptions faults cause failures systems classified classes attempt tolerate physical faults ha hld wa attempt tolerate design faults aa 
class system replicates application servers different physically independent processors 
physical faults tend occur independently independent processors faults cause minority application group failures 
experience systems ha hld wa confirms effectively mask consequences physical faults 
diverse software design pursues goal producing diverse software design methodology ensure groups application servers running diverse programs suffer majority failures despite existence residual design faults programs 
accepted techniques estimate confidence increase reliability results diverse programming voting design diversity generated considerable controversy date ka 
specified software server failure semantics implemented 
problems related implementation atomic transactions persistent data despite processors crash performance failure semantics disks omission failure semantics communications omission performance communication failures investigated intensively years researchers area database systems 
monographs books bhg treat subject great detail 
separate concerns monographs assume programs implementing transaction atomicity partially correct 
problems need solved implementing totally partially correct programs object intensive study decades software engineering community 
years ideas emerged proven effective helping software designers prevent tion design faults programs 
mention pursuit simplicity clarity design hierarchical design methods information hiding data types rigorous design specification verification techniques systematic identification detection handling exception occurrences modern inspection testing methods 
papers describing commercial systems discussed earlier deal issue reasonable assume extensively modern software engineering methods attempting ensure software totally partially correct 
systems discussed provide concurrency control recovery support implementing application servers atomic transaction failure semantics 
goal ensure concurrently executing transactions serializable execution equivalent serial execution sequences changes stable storage performed completion aborted 
servers rely techniques locking logging disk mirroring atomic commit bhg ls 
software server failures masked 
commercial systems discussed earlier hierarchical masking techniques mask hardware certain software server failures 
techniques effective masking crash omission performance server failures long underlying processor service needed servers fail needs group masking technique tolerate software server failures caused failures underlying processor service 
software group masking techniques members crash performance failure semantics commercially successful systems discussed previously tandem ibm 
tandem systems processes pairs implementing fault tolerant basic operating system services disk service spool service logging commit service system pairs primary backup database communication servers implement fault tolerant database communication services 
prerequisite implementation service software server group capable masking processor service failures existence multiple host processors access physical resources service 
example disk containing database accessed different processors processors host database servers database service 
member database server group organized mask concurrent processor failures 
generally replication resources needed service prerequisite making service available despite individual resource failures 
example disks omission failure semantics store database enables implement database service mask single disk failure 
software server groups raises number novel issues understood 
group members running different processors maintain consistency local states presence member failures member joins communication failures 
second server groups communicate 
third automatically ensured required number members maintained server groups despite operating system server communication failures 
difficulty solving problems main reasons software server groups widespread commercial applications 
second reason failed elementary hardware server cpu memory bank repair take hours days software server crashes repair restart take minutes 
elementary hardware server failures heavily contribute user visible service downtime manufacturers prefer attack problem 
third reason software server groups unpopular additional complexity associated group management mechanisms inherent run time overhead 
fourth reason priori clear low overhead groups members crash performance failure semantics provide effective tolerance residual software design faults 
statistics show software server group management mechanism designed servers crash performance failure semantics realistically effective masking server failures caused hardware faults residual design faults left production quality software extensive reviews testing 
example reported sample set tandem systems representing system hours indicates primary backup process group tandem distributed operating system failures affecting group double server failures group failures 
remaining failures detected group single member failures left member group running correctly 
plausible explanation phenomenon physical faults affecting physically independent processors occur independently cause affected software servers crash independently 
similarly residual software design faults manifest intermittently rare exceptional conditions result time dependent synchronization errors slow accumulation resources acquired temporary released 
errors residues accumulate different rates different group members eventually lead individual group members crash different times 
local states group members synchronized 
service server group synchronization policy prescribes degree local state synchronization exist servers implementing service 
close synchronization called masking active redundancy prescribes local member states closely synchronized letting members execute service requests parallel go sequence state transitions 
group output depends failure semantics assumed members 
group members fail arbitrary ways majority voting common method group answer output majority members agree 
group organization masks minority member failures price slowing group output time time needed majority members compute agreeing answers voting process take place 
majority members fail concurrently group output incorrect 
group members crash omission performance failure semantics output computed member sent users 
members send outputs parallel group output understood output computed set fastest members 
advantage output sending technique group perform correctly long group member stays correct 
drawback high communication overhead 
reduce overhead group members ranked respect communication ranked output sending restricted highest functioning ranked member 
cost increased output delay highest ranked member fails due need detect failure agree new ranking surviving members 
closely synchronized groups software servers systems attempt tolerate arbitrary server failures ha hld wa 
examples closely synchronized groups members crash performance failure semantics described cdd 
number rules transforming non fault tolerant services implemented nonredundant application programs fault tolerant services implemented closely synchronized server groups proposed discussed 
contrast close synchronization loose synchronization called dynamic standby redundancy ranks group members respect closely synchronized current service state understood application operations requested clients service initialization initial service state 
distinguish ranking ranking introduced call ranking 
loose synchronization requires highest ranking group member usually called primary server process service requests record current service state local state 
prerequisite form group synchronization group members crash crash performance failure semantics 
highest ranking member usually sends group output users know rankings coincide 
backup servers lower ranks log service requests receive periodically state checkpoints primary 
arrangement local state backup server lags state primary backup state reflect execution service requests primary server 
primary fails highest ranking backup server recover state existing primary failure re executing service requests logged state check point obtained primary 
particular case loose synchronization situation group size primary server active backups 
case server checkpointing logging 
failure new primary started reads state check point recovers state existed failure executing logged service requests 
way failure masked users experience delay getting response 
variants check pointing rollback masking technique developed case primary server implemented set distributed processes check point log service requests independently jz kt sy 
groups composed servers performance failure semantics main advantage loose close synchronization primary servers full share required replicated service resources backups reduced 
allows servers coexist amount computing power 
main drawback delays seen clients group members fail longer 
call maximum sequence service requests processed primary successive state check points primary backup processing lag worst case delay answering client request primary failure composed time needed detect reach agreement primary failure time needed new primary absorb primary backup processing lag 
line transaction processing environments delays considered tolerable 
real time applications response time required smaller time needed detect member failure absorb primary backup processing lag close synchronization 
examples loosely synchronized server groups discussed ba bj cdd ibm ol 
close loose synchronization described just points continuum synchronization policies 
imagine intermediate synchronization policies share advantages drawbacks points various degrees 
members server group 
server group replication policy prescribes number members group expected 
servers group greater availability capacity servicing service requests parallel 
hand members group higher cost communication synchronization group members 
certain required service availability stochastic modelling simulation methods determining optimum number members account individual server failure rates server failure semantics service request arrival rates message communication costs 
note degree software server redundancy necessary providing certain level service availability influences host processor redundancy degree service 
example stochastic modelling simulation studies show achieve certain radar surveillance tracking service availability level necessary physically independent closely synchronized servers host processors necessary running servers parallel shown 
group communication protocols 
communication server groups different point point communication services offered traditional protocols sna tcp ip 
group state replicated members members need update replicas special group protocols ensure replica consistency presence process communication failures 
exist significant number protocols proposed group communication asc ba bd bj ca cm cri cz gs kt lg lls ol 
area research presently active 
large variety approaches proposed reflects fact properties required group communication protocols depend protocol goals failure semantics radar radar radar processor processor processor triple modular redundancy group members failure semantics lower level communication services 
related communication issue naming rules clients servers define service names names translated lower level message routing information 
fault tolerant systems servers move host processor convenient require clients know location servers provide service 
goal location transparent naming services mask clients effects individual server failures group 
examples fault tolerant location transparent name services implemented real systems ca cdd kls 
survey issues involved naming cp 
enforce group availability policies automatically 
synchronization replication policies defined service implemented server group constitute availability policy service group 
possible approach enforcing certain group availability policy illustrated implement group member mechanisms reaching agreement rankings group mechanisms detecting member failures procedures handling new member joins promotions higher group loosely synchronized 
drawback approach results substantial code duplication lack modularity group management mechanisms services highly available basically 
alternative approach adopted distributed fault tolerant system built air traffic control cdd requires group members implement application specific get state check point case group ranked procedures needed local member state synchronization join check point times 
tasks detecting server failures coordinating promotions enforcing replication policies service availability management service 
availability presence failures distributed system services dependent availability service availability management implemented group servers call service availability managers 
assume servers crash performance failure semantics 
group service availability manager group group service availability management objective ensure service system operator enables system provide server group availability policy defined effectively enforced human intervention 
arrow means enforces availability policy depends attempts illustrate goal 
simplify presentation availability management service assume service servers run distinct processors started order ignore issues related existence rankings groups 
assume hosts designated various services computing power support services mention issues related load shed load balancing 
degree server replication denote set host processors members denote processor membership set correctly functioning processors agree communicate order collectively provide clients services enabled system 
goal service availability management service ensure enabled service min servers 
servers long number correctly functioning hosts greater equal number correctly functioning hosts drops servers correctly functioning hosts 
state server availability manager consists set members processors function correctly system set services enabled system set servers functioning correctly mappings tables relate sets mapping servers members set functioning processor members gives set servers servers running mapping service server gives service implemented state server group availability manager changed events service enabled system operator service disabled system operator server implementing service fails processor server implementing service started processor processor fails processor starts 
state transitions availability manager undergo response events specified follows 
service enabled servers started processors members members servers started processors members added set servers started added set server started mappings servers service updated accordingly 
service disabled removed existing servers shut state variables servers service updated accordingly 
server service fails server started free host members state variables servers service updated accordingly 
successful start server reflected appropriate update server service state variables 
failure processor interpreted failure servers running see interpretation server failures sketched plus removal members state variable 
start processor result start servers services hosted server population replication threshold specified appropriate updates state variables members servers service assuming problems server processor failure detection solved difficult build centralized service availability manager achieves state transitions runs processor system 
solution satisfactory processor server group availability policy enforced 
mentioned objective specified service availability policy enforced host processor works system server availability managers replicated processors system 
results need replicate global state variables members service servers working processors members 
maintain consistency replicated global state variables processors presence random communication delays failures occurrence global system state update broadcast correctly functioning processors way processors see sequence state updates 
different availability managers see different sequences updates local views global system state diverge 
lead violations specified server group availability policy 
illustrate point consider service members fm sees sequence events enabled disabled sees sequence disabled enabled state variable different variable 
start local server cause violation availability policy specified achieve agreement global states 
random communication delays performance failures cause messages broadcast updating state replicas lost received order 
ensure correct servers agree sequence global state updates solve major problems 
achieve agreement sequence processor joins failures point time correctly functioning processor knows group correct processors communication possible 
second achieve agreement order messages broadcast processors group 
protocol ensures agreement unique temporal sequence successive processor group group memberships members members members termed processor membership protocol 
protocol ensures broadcasts originated processors group members received order correct processors members termed atomic broadcast protocol 
requirements membership atomic broadcast consist number safety timeliness properties 
safety properties invariants true points real time 
example membership protocol ensure processors joined group agree group membership atomic broadcast protocol ensure correct processors group agree order broadcast messages 
timeliness properties require upper bounds time takes propagate information group members 
example membership protocol require surviving members detect member failure bounded time atomic broadcast protocol require correct members group learn atomic broadcast bounded time 
assume existence bound message delays existing membership atomic broadcast protocols divided synchronous protocols cri asynchronous protocols bj ca cm 
general synchronous protocols assume processor clocks synchronized constant maximum deviation epsilon asynchronous protocols require clocks synchronized 
synchronous protocols strong timeliness properties guarantee information propagates bounded times group members failures member joins occur concurrently propagation 
known synchronous protocols propagation speed depends ffl 
assumption actual message delays shorter crucial synchronous approach delays experienced protocol messages exceed servers communicate messages transiently partitioned synchronous protocol violate safety requirements 
example transient partition occurrences servers disagree order messages broadcast group membership 
protocols detecting transient partitions reconciling diverging server views investigated 
posteriori partition detection recovery protocols need compensate damage done disagreement existed group members 
contrast synchronous protocols build asynchronous protocols violate safety requirements communication delays unbounded communication partitions occur 
strong safety guarantees come price 
timeliness properties asynchronous protocols weak guarantee priori known upper bound amount time elapse instant processor learns event processor learns event 
example member joins failures continue occur asynchronous membership protocol need unbounded amount time detect changes set correctly working processors 
similarly message atomically broadcast group need unbounded amount time reach group members 
second asynchronous protocols activities membership computation broadcast interfere broadcasts occur membership change aborted bj ca cm 
third asynchronous protocols specifically designed tolerate partitions require correctly working processors system form quorum done cm kls 
requirement needed prevent divergence processors distinct partitions affects adversely system availability quorum processors needs functioning done 
quorum correct processors possibility ask human operator permission go ahead kls 
possibility attempt modify quorums dynamically 
designers distributed fault tolerant systems faced choices attempt ensure existence upper bound message delays accept unbounded message delays 
alternative requires advance knowledge maximum system load real time operating systems massive communication hardware redundancy ensure negligible probability network partitions occur 
upper bound achievable bound allows satisfactory recovery speed events processor communication failures adopt synchronous approach guarantees strong timeliness properties enables system continue autonomously long exists correct processor 
bound achievable account specified peak load real time operating system scheduling algorithms small allow system react sufficiently fast component failures example measured minutes required recovery time failures measured seconds bound realistically options exist 
adoption timeout delay unsatisfactory bound exists timeout delay definition smaller 
synchronous approach provides sufficient recovery speed cost compensating actions taken transient partitions smaller cost meeting recovery deadlines adopt synchronous approach 
ensure strong timeliness properties eliminate need worry quorums 
sufficiently large compared median message delay messages delays exceed time units cr transient partitions rare 
hand cost compensating inconsistent actions taken consequence transient partition occurrences higher cost missing recovery deadlines adopt asynchronous approach timeout delay 
cost weak timeliness properties need worry quorums 
timeout approaches choice timeout delay delicate matter 
larger failure detection recovery times larger frequency transient partition detections decreases 
conversely smaller failure detection recovery times smaller frequency transient partition detections increases 
choice balance cost recovering transient partitions cost detecting permanent failures fast 
understanding technical area complex fault tolerant distributed computing requires identifying fundamental concepts naming unambiguously 
proposed small number concepts believe fundamental designing understanding fault tolerant distributed systems 
concepts notions service server depends relation fundamental kind system 
notions failure semantics hierarchical failure masking group failure masking specific fault tolerant systems 
concepts capture goals fault tolerant computing trade offs mask component failures possible masking possible economical ensure system clearly specified failure semantics 
demonstrate adequacy concepts capturing essential architectural aspects formulate list key hardware software issues arise fault tolerant distributed systems design describe various design choices alternatives known issue comment relative merits drawbacks alternatives illustrate issues addressed existing practical system architectures 
fact objectives achieved solely basic notions introduced section indicates power express crucial architectural issues fault tolerant distributed systems 
clear concepts terminology help entirely solve problem design fault tolerant distributed system uses right amount redundancy various abstraction layers achieve optimum function dependability cost result 
little complexity theory fault tolerant computing exists today guide designer choosing multitude possible redundancy management solutions hardware operating system application levels 
choices difficult lack analytical experimental cost information various redundancy management techniques lack published data kind failure behaviors various system components exhibit failure distributions associated components 
stochastic aspects inherent fault tolerance discussed detail add dimension complexity design 
available needed theories experimental cost failure data number choices consider immense systematic search optimality happen 
reasons building distributed fault tolerant systems remain art foreseeable 
thing certain increasing dependence placed computing systems availability computing communication services presence component failures hardware software changes horizontal growth important 
achieve high levels availability systems need fault tolerant 
motivation recording snapshot understanding basic concepts issues fault tolerant distributed systems provided invitations give overviews fault tolerant computing conferences annual conf 
canadian information processing society edmonton brazilian conf 
fault tolerant computing rio de janeiro 
program committees conferences particular professors tamer ozsu julius leite inviting phil bernstein cacm reviewers helpful comments suggestions 
anderson lee fault tolerance principles practice prentice hall 
avizienis software fault tolerance ifip computer congress san francisco august 
aa avizienis kelly traverse tso ucla system distributed testbed multi version software th int 
conf 
fault tolerant computing ann arbor mi 
asc abbadi skeen cristian efficient fault tolerant protocol replicated data management th acm conf 
principles database systems 
bartlett nonstop kernel th symp 
operating system principles dec 
bernstein sequoia fault tolerant tightly coupled multiprocessor transaction processing ieee computer february 
ba borg blau herrmann oberle fault tolerance unix acm trans 
computer systems vol 
feb 
bd babaoglu streets network architectures fast reliable broadcast ieee tr 
software engineering vol 
se 
barbara garcia molina increasing availability mutual exclusion constraints dynamic vote reassignment acm trans 
computer systems vol 
nov 
bhg bernstein hadzilacos goodman concurrency control recovery database systems addison wesley 
bj birman joseph reliable communication presence failures acm trans 
computer systems vol 
february 
cristian rigorous approach fault tolerant programming ieee tr 
software eng vol 
se 
cristian agreeing absent synchronous distributed system th int conf fault tolerant computing tokyo june 
cristian exception handling dependability resilient computers anderson ed blackwell scientific publications oxford 
ca carr tandem global update protocol tandem systems review vol 
june 
cristian strong dolev atomic broadcast simple diffusion byzantine agreement th int 
conf 
fault tolerant computing ann arbor mi 
cdd cristian fault tolerance advanced automation system th int 
conf 
fault tolerant computing newcastle tyne england june 
cl clark structuring systems calls th acm symp 
operating systems principles 
cm chang reliable broadcast protocols acm tr 
computer systems vol 
august 
cooper replicated distributed programs phd thesis uc berkeley 
cp comer peterson understanding naming distributed systems distributed computing vol 
pp 

cr cristian probabilistic clock synchronization distributed computing vol 
pp 

cri cristian synchronous atomic broadcast redundant broadcast channels ibm research report rj dec 
cz cheriton zwaenepoel distributed process groups kernel acm tr 
comp 
systems vol 
may 
dijkstra hierarchical ordering sequential processes acta informatica vol pp 

es shrivastava characterization faults systems th symp 
reliability dist softw 
database systems los angeles january 
gray notes database operating systems operating systems advanced course lecture notes computer science springer verlag vol 
gray computers done 
th symp 
reliability dist softw 
database systems los angeles january 
gs garcia molina message ordering multicast environment th int 
conf 
distributed systems newport beach california june 
ha hopkins smith highly reliable fault tolerant multiprocessor aircraft proceedings ieee vol 
oct 
hld harper fault tolerant parallel processor architecture overview th int conf fault tolerant computing tokyo june 
ibm ibm international technical support centers ims vs extended recovery facility technical 
jz johnson zwaenepoel sender message logging th int conf fault tolerant computing tokyo june 
ka knight amann issues influencing version programming proceedings ifip congress san francisco august 
kt kaashoek tanenbaum fault tolerance group communication th acm sigops european workshop bologna sept 
kopetz fault tolerant membership synchronous real time system ifip working conference dependable computing critical applications santa barbara august 
kls levy strecker closely coupled distributed system acm transactions computer systems vol 

kt koo toueg check pointing rollback recovery distributed systems ieee transactions software engineering vol 
se 
lamport time time outs fault tolerant systems acm trans programming languages systems vol 

lamport part time parliament dec src report sept 
la laprie dependability unifying concept reliable computing fault tolerance anderson ed blackwell scientific publications oxford 
la laprie definition analysis hardware software fault tolerant architectures ieee computer july 
lg gligor fault tolerant protocol atomic broadcast th int conf distributed computing systems paris may 
ll le critical issues distributed real time computing proceedings workshop communication networks distributed operating systems space environment european space agency report oct 
lls ladin liskov shrira lazy replication method managing replicated data th annual acm symposium principles distributed comput ing august 
ls lampson sturgis atomic transactions distributed systems advanced course lecture notes computer science vol 
springer verlag 
fault tolerant systems technical report csl stanford university 
melliar smith moser agrawala broadcast protocols distributed systems ieee tr parallel distributed systems vol 
jan 
ol oki liskov replication new primary copy method support highly available distributed systems th acm symp 
principles distributed computing august 
parnas designing software ease extension contraction ieee tr 
software engineering vol 
se march 
pa powell la tolerance aux dans les systemes les hypotheses leur importance laas research report september 
pb butler measurement sift operating system overhead nasa technical memo 
pw peterson error correcting codes mit press massachusetts 
randell system structure software fault tolerance ieee trans 
software eng vol 
se 
siewiorek fault tolerance commercial computers ieee computer july 
schneider state machine approach tutorial tr cornell univ 
schmuck efficient broadcast protocols asynchronous distributed systems phd thesis tr cornell univ 
src saltzer reed clark arguments system design acm trans 
computer systems vol 
nov 
strong skeen cristian handshake protocols th int 
conf 
distributed computing systems berlin september 
sy strom yemini optimistic recovery distributed systems acm transactions computer systems vol 

tanenbaum computer networks prentice hall englewood cliffs nj 
trivedi probability statistics reliability queuing computer science applications prentice hall 
tw taylor wilson system architecture dependability resilient computers anderson ed blackwell scientific publications oxford 
rodrigues amp highly parallel atomic multicast protocol proceedings acm austin texas sept 
error detecting codes self checking circuits applications elsevier north holland new york 
wa lamport goldberg green levitt melliar smith shostak weinstock sift design analysis fault tolerant computer aircraft control proc ieee vol 
oct 
wulf reliable hardware software architecture int 
conf 
reliable software sigplan 
general terms design reliability additional key words phrases exception handling failure failure classification failure masking failure semantics fault tolerant system group communication redundancy server group system architecture 

