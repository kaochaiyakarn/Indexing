evolution beats temporal difference learning backgammon linear architecture non linear architecture paul darwen cognitive science research group department computer science electrical engineering university queensland brisbane queensland australia email darwen ieee org free lunch theorems show algorithm suit problem 
answer novice question problem algorithm 
compares evolutionary learning temporal difference learning game backgammon real world tasks element random uncertainty 
unfortunately fully evaluate single strategy undirected sampling board positions random dice rolls requires great deal computation 
evolution replacement entire solutions needs accurate evaluation relatively rare board positions needed train certain level 
temporal difference learning incremental changes approach 
results relevance variety real world tasks uncertainty schedule optimization 
free lunch theorems show solve problem requires algorithm suited problem 
begs question particular problem type algorithm best suits 
currently little help novice expert matter asks question 
real world problems possess element random uncertainty 
example research schedule optimization criticized solving static benchmark problems weeks supercomputer time real factory scheduling needs change day hour hour due changes machine breakdowns new orders staff 
backgammon provides suitable proxy realworld problems uncertainty 
chess backgammon uses random dice 
conveniently learning method known temporal difference learning td works backgammon provides useful benchmark 
compare evolutionary learning td learning 
traditional way represent game strategy move evaluator function current board position legal moves evaluated function move highest value taken 
turns linear node function represent strategy evolution beats td learning 
multi node neural network allows nonlinearity evolution doesn find nonlinear solution td learning better 
due sampling sparse evolution requires accurate evaluation strategy lifetime needing exposure sufficiently games see rare board positions train nonlinear features 
evolution live die approach contrasts incremental changes td learning matter hill climbing 
trial strategy large population play sufficiently vast number games sample rare board positions accurately measure trial strategy ability 
insufficient sampling fitness function gives insufficient feedback 
backgammon possible board positions sufficient sampling individual generation needs cpu time td learning 
background backgammon move evaluator problem list alternative choices choice take 
move evaluation function puts value legal move 
move highest value taken 
chess checkers backgammon number possible alternatives branching factor large brute force lookahead search possible board positions dice give possible rolls offers average moves page 
quality player move evaluator function recognize board positions desirable undesirable 
generalization problem move evaluator classifier exposure board positions judge unseen board position move bad 
incidentally larger branching factor surprisingly difficult 
example checkers possible move openings 
don correct move eventually lose game moves 
chances losing small average player master player know exploit 
know sequence don basically 
backgammon larger branching factor hit correct sequence 
recognizing nonlinearity neural networks tesauro reminds backgammon problems nonlinear function works better page 
study hill climbing apparently find nonlinear solution backgammon 
worth reviewing nonlinearity context neural networks 
consider feed forward neural network single hidden node linear function 
input vector vector input weights initial bias linear function look sigmoid 
node function sets linear decision boundary straight line divides input space 
side line function returns positive values side returns negative values 
adjusting weights moves decision boundary better classify points input space 
location linear decision boundary determined page 
normal vector distance jjwjj decision boundary neural network single sigmoid node sets linear decision boundary location depends value initial bias vector input weights sharpness linear decision boundary depends magnitudes weights 
multiplying values positive constant move decision boundary merely sharper illustrated 
convenient measure sharpness node decision boundary square root sum squares input weights including initial bias having neurons adding weighted outputs allows guarantee nonlinear behavior 
hidden node output weight potentially nonlinear multi node function looks sigmoid 
sigmoid sigmoid input second input output neural network decision boundary location sharpness 
node functions linear decision boundary multiplying weights positive constant boundary 
possibility nonlinearity comes sigmoids 
nonlinear function approximate linear node function ways 
input weights small sigmoid equation simply sum corresponding weights get linear function 
inputs weights small equation approximate linear function utilize nonlinear potential 
second nodes input weights large cause effect nodes output weights equation small negligible neural network approximate linear node function 
ways recognizing absence nonlinear solution demonstrate evolution learn nonlinear behavior backgammon impractically large number games 
crossover label permutation difficulty evolving artificial neural networks label permutation problem known competing label convention problem section 
toone mapping genotype phenotype 
feed forward neural network single layer hidden nodes offers 
different ways write network permuting hidden nodes sigmoid function odd symmetric tanh function flipping signs page 
shows example 
result functionally similar neural networks label nodes different permutation cause crossover disrupt offspring section 
researchers find tasks crossover help 
cases insightful researchers take parsimonious approach doing crossover 
works hidden nodes inputs output label permutation problem neural networks 
pair node feed forward neural networks functionally identical 
equivalent nodes different places crossover disruptive 
applications including evolving neural networks play game checkers 
hand free lunch theorem says tasks benefits crossover outweigh neural networks 
task alas uncertain 
researchers try invent crossover operators cause disruption neural networks 
course crossover helps hinders depends choices operators representation attributes problem 
uses uniform crossover elements thierens reduce disruption crossover causes neural networks 
section show disruption due crossover appear major obstacle nonlinear solution 
learning games evolution usually human expert writes evaluation function judge trial solutions population 
unsuited tasks learning play game requires priori expertise 
example imagine trying evolve art evaluation function programmed picasso 
nice pictures result due machine learning picasso human skill 
artificial intelligence just editing tool 
interesting problem get picasso quality paintings human picasso 
evolution trial solution population judged performs solutions evolving population population evolving tandem 
evolution discover solutions problems prior knowledge human expert 
evolution worked games checkers iterated prisoner dilemma non game tasks scheduling creating sorting algorithm 
section describes evolutionary system learn backgammon human teacher 
experimental setup backgammon player stored vector weights feed forward neural network fixed architecture 
population strategies play 
generation strategy plays player fixed number games assuming ties oneself 
individual fitness number wins games 
strategy population move evaluator function 
move pair pseudo random dice rolled legal move generator lists board position reached dice rolls move look ahead search 
move evaluation function individual population returns value possible move 
move highest value taken 
move evaluator inputs directly board position hand written feature detectors indicate pieces bar prison captured pieces 
indicate pieces completed journey board 
board positions inputs position occupied opponent pieces counting pieces 
linear function section exactly architecture unseen benchmark player pubeval 
pubeval node feed forward neural network created 
output sum products input corresponding weight hidden nodes 
evolution directly manipulates values weights backpropagation 
multi node architecture sections conventional single layer hidden nodes input weights plus bias feeds output weights single output 
nodes comes weights 
final output bias equation move evaluator add final bias moves change move highest value 
weights represented real numbers bit strings 
initial values weights random gaussian distribution mean zero standard deviation 
elitism best population copied unchanged generation 
selection linear ranking stochastic universal selection page best individual expects offspring worst expects 
allele chance gaussian mutation mean standard deviation giving small changes reduce major disruptions 
incidentally previous evolving checkers players self adapting mutation step sizes 
author self adapting step size significant difference performance may due unforeseen interactions choices representation parameters 
backgammon move position occupied opposing piece 
legal move detector 
crossover basically uniform crossover changes try reduce crossover disruption 
details enlightening suffice say relies thierens 
particular nodes negative final output weights multiplied 
doing output weights positive change behavior sigmoid function hyperbolic tangent function negative symmetric tanh tanh 
half new offspring mutated half crossed suffers 
results linear function random dice evolution better consider player linear node function 
uses exactly architecture unseen benchmark player pubeval allow direct comparison td learning evolution 
note evolution learns playing population outside opponent merely shows performance pubeval learning completed 
best population plays pubeval games generation evolution node random dice td learning node random dice td learning nodes random dice population size representing backgammon move evaluator linear single node function unseen benchmark player pubeval 
typical run shows external opponent evolution learn better linear move evaluator td learning 
shows proportion games best individual generation achieves unseen benchmark pubeval created 
evolution outperforms pubeval 
fact td learning nodes wins time pubeval table 
node evolution covers half improvement 
evolution finds substantially different linear solution td learning 
cosine players weight vectors vector inner product page shows significant difference angle linear decision boundary 
players generation typical player inner product pubeval st player table individuals generation linear single node architecture 
evolution finds different solution td learning pubeval architecture 
normalized inner product weight vectors gives cosine linear decision boundaries page 
population members clustered fairly close different pubeval 
run table shows normalized inner product pubeval weight vector weight vector population individual 
shows population members similar pubeval 
indicates evolutionary solution noticeable difference td learning solution 
far promising 
random dice rolls coevolutionary learning better td learning single node move evaluator 
previous shown difference statistically significant 
td learning multi node architecture allows nonlinearity gave world best backgammon program excitement sections consider multiple nodes 
nodes canned dice evolution nonlinear hill climbing node neural network allow nonlinearity apparently failed find nonlinear solution 
section checks underlying evolutionary machinery fact capable finding nonlinear solution 
simplified generalization problem section uses dice rolls set games 
member population plays games 
fitness generation number wins nearly games 
different sequences dice rolls get recycled set games pairwise interaction specific games players take turns move pair games uses dice rolls 
game player going set dice rolls second game player goes set dice rolls 
games difference sequence dice rolls 
dice roll sequences game pairwise interaction 
aim reduce generalization problem searching possible board positions searches board positions possible canned dice rolls 
naturally causes brittle specialization particular canned dice rolls 
interesting finds nonlinear solution despite supposedly disruptive effect crossover 
shows evolution find nonlinear solution 
axis shows node output weight fraction nodes output weights equation 
node axis value indicates final output weight close zero node essentially ignored 
axis shows sharpness node decision boundary 
precisely excluding initial bias weight input weights squared summed square root sum gives axis value 
measures sharp transition decision boundary node shown 
nodes small input weight values little effect neural network approximate linear single node function described section 
displays hidden nodes players reveals sharp decision boundaries non trivial final output weights 
indicates merely approximating linear function ways outlined section utilizing hidden nodes go nonlinear 
share final output weight node sharpness magnitude node canned dice rolls games goes nonlinear canned dice goes nonlinear 
shows generation individuals population nodes sharp decision boundary big final output weight node 
nodes random dice linear section showed random dice rolls linear single node architecture evolution beat td learning 
section showed evolution find nonlinear solution canned dice rolls spite crossover supposed disruption neural networks 
far 
exciting question evolution beat td learning random dice rolls multi node architecture 
shows typical evolutionary run members population neural networks hidden nodes weights 
plays games oneself 
fitness number wins nearly games random dice rolls 
best population plays pubeval games generation evolution nodes random dice td learning node random dice td learning nodes random dice learning random dice rolls multiple hidden nodes gives better performance linear single node representation 
results better single node 
back evolution node beat td learning node 
nodes beat td learning node time table 
expect evolution nodes beat 
doesn 
evolution nodes looks node 
suggests finding nonlinear solution nodes 
checking nonlinear solutions mentioned section convenient measure sharpness node decision boundary square root sum squares input weights excluding initial bias 
axis shows measure node typical run final population 
axis shows normalized final output weight node 
nodes small input weights sharp 
sum approximate linear function 
bunched left hand side 
nodes big weights nonlinear output weights small negligible impact 
scattered bottom 
node network approximating linear function 
says hidden nodes coevolution discover nonlinear behavior 
nodes small input weights low values small output weights low values 
ap share final output node sharpness magnitude node nodes generation multiple hidden nodes allows neural network decision boundary nonlinear 
evolution doesn utilize potential 
shows nodes population evolving generation 
nodes sharp approximate linear function sharp negligible output weight 
proximate linear function 
evolution nodes gives better performance node simply learning nonlinearity 
adding node time possible find nonlinear solution section uses separate run additional node 
successful single node linear solution section kept constant entire run second node evolved 
nodes kept constant entire run third node optimized 
removes disruption crossover due label convention problem section 
importantly runs strict limits final output weight node sharpness decision boundary avoid going small 
kept limits avoid approximating linear solution happened 
runs get population strategies nodes 
unfortunately performance pubeval shown better linear solution 
apparently achieving nonlinear solution 
turns extra nodes approximating single node novel expedient saturating node 
generation run nodes optimized allowed run board position evaluations node output evaluations recorded sorted 
shows distribution node output 
node saturated 
evolution merely discovered new way node output board position evaluations node second node third node fourth node fifth node extra nodes saturate adding nodes time new run additional node strict limits final output weight sharpness node decision boundary 
random dice rolls saturates nodes approximating linear solution 
nonlinear architecture approximate linear 
nonlinearity keeping output weights low making decision boundary sharpness gentle specifically banned section approximates node linear solution novel expedient saturating extra nodes 
due crossover disruption due label permutation problem additional node optimized separately 
discussion canned dice rolls evolution find nonlinear solution 
learning random dice rolls evolution approximates linear solution 
demonstrates supposed disruption crossover problem canned dice rolls linear random dice rolls case 
adding nodes time inter node disruption crossover remains linear 
doesn sampling search space size strategy population played strategy games 
generation strategies evaluated games 

feature solution rewarded lifetime games feature selected 
hundreds generations population may exposed millions games integrate accumulate 
evolution payoff sins wins ancestors 
features judged single lifetime games 
games td learning need 
table shows games td learning required final strategy table 
measure ability average number points won lost game points game expert humans 
table shows learning nonlinearity requires exposure large number board positions generated hundreds thousands games games 
program 
training performance name nodes games points game table table 
td learning incremental changes accumulated large number runs 
fully evaluate trial solution evolving population mere games sample rarer board positions expert level play learned 
tesauro results table suggest sample rare board positions need hundreds thousands games 
modest population worked plays requires pairwise interactions 
number games pairwise interaction increased games player participates games generation comes massive games generation 
generations run needs games 
larger architecture strategy weights bigger population needs longer run take underestimate 
sufficient sampling evolution beat td learning linear architecture 
nonlinear architecture billions games needed beat method requires games billions games contributing learning 
chess backgammon self play methods don chess 
apparently players learn respond narrow range possibilities peers 
large range different board positions players specialize brittle 
backgammon dice hand force wide sampling different board positions 
players identical random dice sample different board positions 
prevents brittle specialization 
goes far search space big sampling sparse undirected scattered 
evolution needs evaluate trial solution lifetime generation 
individual performance reflect ancestors offspring 
rarer board positions occur lifetime provide feedback 
player doesn hit right sequence lifetime feedback won get selected 
demands large number games generation adds staggering amount computation run 
real world parallelism rescheduling getting back original question particular problem algorithm choose 
provide rigorous iron guarantees general hints drawn noisy problems 
section showed long individual solutions accurately evaluated evolution beats td learning 
suits evolution death survival approach dealing trial solutions 
hand sampling time prohibitive td learning cumulative evaluation incremental changes works better integrating extra information sample 
evolution integrate information sample individual evaluation added peers offspring 
evolutionary computation ec big advantage easily parallelized 
processors parallel machine ec times samples produce better solution time 
cheap parallel supercomputers popular ec hunger cpu time easily fed rescheduling making rapid near optimal changes existing schedule response changes commercial problem great importance 
papers attempt evolve robust schedules 
evolution cope problem depend size space possible changes accurately trial schedule evaluated prohibitively large number random changes sample 
attempts partially answer question free lunch theorems particular search optimization problem algorithm gives answer reasonable time 
samples accurately evaluate trial solution evolutionary learning beats td learning backgammon 
nonlinear architectures require large number games sample board positions accurately evaluate strategy 
td learning incremental changes lighter cpu time evolution death survival approach 
consistent evolution performance chess checkers 
evolutionary computation ec easily parallelized td learning 
affordable parallel supercomputers ec attractive 
tasks backgammon require samples 
real world tasks random uncertainty rescheduling evolution long trial solution accurately evaluated reasonable time 
large search spaces sampling timeconsuming evolution may find better solution cpu time may worth 
cases td learning economical choice 
acknowledgments chris pascoe david reeves adrian lee gave invaluable technical support 
pga pack mpich software courtesy argonne national laboratory mississippi state university 
gerald tesauro available backgammon strategy pubeval 
research supported australian research council 
bibliography robert axelrod 
evolution strategies iterated prisoner dilemma 
genetic algorithms simulated annealing chapter pages 
morgan kaufmann 
james baker 
reducing bias inefficiency selection algorithm 
proceedings second international conference genetic algorithms pages 
lawrence erlbaum associates 
christopher bishop 
neural networks pattern recognition 
clarendon oxford england 
kumar chellapilla david fogel 
evolution neural networks games intelligence 
proceedings ieee september 
kumar chellapilla david fogel 
beats case study competing evolved checkers program commercially available software 
congress evolutionary computation pages 
ieee press 
paul darwen 
computationally intensive noisy tasks evolutionary learning temporal difference learning backgammon 
congress evolutionary computation pages 
ieee press 
paul darwen 
black magic interdependence prevents principled parameter setting self adapting costs computation 
applied complexity neural nets managed landscapes pages 
institute food crop research christchurch new zealand 
paul darwen xin yao 
speciation automatic categorical modularization 
ieee transactions evolutionary computation july 
peter hancock 
genetic algorithms permutation problems comparison recombination operators neural net structure specification 
proceedings international workshop combinations genetic algorithms neural networks pages 
ieee computer society press 
emma hart peter ross jeremy nelson 
producing robust schedules artificial immune system 
proceedings ieee international conference evolutionary computation pages 
jeffrey herrmann 
genetic algorithm minimax optimization problems 
congress evolutionary computation pages 
ieee press 
daniel hillis 
evolving parasites improve simulated evolution optimization procedure 
artificial life pages 
addison wesley 
philip husbands frank mill 
simulated coevolution mechanism emergent planning scheduling 
proceedings fourth international conference genetic algorithms pages 
morgan kaufmann 
hugues jordan pollack 
evolving intertwined spirals 
proceedings fifth annual conference evolutionary programming pages 
mit press 
helmut mayer roland 
evolutionary coevolutionary approaches time series prediction generalized multi layer perceptrons 
congress evolutionary computation pages 
ieee press july 
jordan pollack alan blair 
evolution successful learning backgammon strategy 
machine learning 
mitchell potter kenneth de jong john grefenstette 
coevolutionary approach learning sequential decision rules 
proceedings sixth international conference genetic algorithms pages 
morgan kaufmann 
gerald tesauro 
temporal difference learning 
communications acm march 
gerald tesauro 
comments evolution successful learning backgammon strategy 
machine learning 
dirk thierens 
non redundant genetic coding neural networks 
proceedings ieee international conference evolutionary computation pages 
ieee press 
david wolpert william macready 
free lunch theorems optimization 
ieee transactions evolutionary computation april 
xin yao 
evolving artificial neural networks 
proceedings ieee september 
