appear proc 
icassp variable length category gram language model niesler woodland cambridge university engineering department street cambridge cb pz england 
language model word category grams ambiguous category membership increased selectively trade compactness performance 
categories leads intrinsically compact model ability generalise unseen word sequences diminishes training data making larger feasible 
language model implicitly involves statistical tagging operation may explicitly assign category untagged text 
experiments lob corpus show optimal model building strategy yield improved results respect conventional gram methods tagger model seen perform relation standard benchmark 

word category grams generalisation word counterparts category opposed word tuples 
allows intrinsic generalisation unseen word sequences smaller number parameters reduces training set sparseness 
furthermore larger feasible statistical storage viewpoint factor seen marked impact language model quality 
language model category grams ambiguous category membership increased selectively trade compactness performance developed 
consequence stochastic category membership allows model employed statistical tagger valuable processing untagged corpora 

language model structure denote sequence temporally consecutive events gamma fz gamma subscripts identify individual members thereof alphabet size implies fz zk gamma denote sequences words symbol word categories word category relationship described gamma number different word categories 
word history classified particular equivalence class defined gram categories fv delta delta delta delta delta delta nhc gamma nhc number history equivalence classes 
word may belong categories general may map multiple history equivalence classes 
assuming wholly determined jw gamma jv delta vjw gamma assuming furthermore probability witnessing depends category gram context right hand side may decomposed vjw gamma gamma delta gamma subsequent sections treat estimation component probability individually 

estimating js compact storage category grams employ tree data structure associating node particular word category paths originating root correspond category grams definition implies node represents distinct history equivalence class sm associated conditional probability density function 
restricting length individual paths tree contexts arbitrary depth catered 
example corresponds trigram context gamma gamma fv 
level unigram context level bigram contexts level trigram contexts null root illustration language model tree set nodes constitutes set possible equivalence classes 
probabilities estimated application katz back conjunction nonlinear discounting 
model building proceeds level level tree growing strategy retains contexts improve language model quality criterion allows model compactness maintained employing longer grams benefit performance :10.1.1.28.4818

initialisation gamma 

grow add level level gamma adding grams occurring training set grams exist tree 

prune newly created leaf level apply quality criterion discard leaf fails 

termination nonzero number leaves remaining level goto step 
quality criterion checks improvement leaving cross validation training set likelihood addition leaf 
particular training set likelihood may written sum contributions nodes tree gamma omega tot delta nn gamma ll sn cum ll sn cum vk delta log gamma gamma vk omega rt delta delta nn number nodes tree log probability training corpus omega tot ll sn cum log probability associated events occurring sn vk number times vk seen sn omega tot number different categories gamma vk omega rt delta probability vk occurring sn retained part training set omega rt formed vk heldout part 
sn cum denote change ll sn cum caused addition leaf pruning criterion sn cum gamma delta gamma omega tot delta requires addition new node lead likelihood increase threshold defined fraction total likelihood choice threshold fairly problem independent 
may calculate perplexity indicating confidence tree predicts category 
language model evaluation referred category perplexity 

estimating jv assume category sufficiently large membership allow application relative frequency estimate jv jv employed avoid overfitting training data 
language model hypothesise categories vocabulary oov words probability occur category estimated 
accordingly entry uw added category count estimated leaving uw jv uw jv delta gamma uw jv number words seen exactly training set total number corresponding estimated count uw small constant introduced heuristically ensure denominator :10.1.1.28.4818
effect significant categories small sparsely trained 
effect performance seen empirically weak yields satisfactory results lob corpus :10.1.1.28.4818

estimating jw gamma set contexts particular word history gamma may belong probabilities associated may calculated means recursive approach 
define hyp possible classification word categories termed hypothesis 
sm tree history equivalence class corresponding deepest match gram tree 
nh number hypotheses 
expressions gamma hyp delta desired probability history equivalence class may sm tree gamma hyp delta sm gamma hyp delta explicit maintenance hypotheses opposed history equivalence classes necessary due varying lengths grams 
set existing hypotheses fv hyp gamma set new hypotheses fv hyp gamma nh gamma gamma gamma number different pos categories 
consider particular postulate hyp fv hyp gamma prime index indicating general fixed relation ordering sets hypotheses 
recalling assumptions equation follows gamma hyp delta gamma hyp delta gamma hyp delta delta gamma gamma hyp gamma delta gram model structure gamma hyp delta gamma hyp tree gamma hyp gamma delta delta gamma hyp tree gamma hyp gamma delta delta deltap gamma hyp gamma delta hyp gamma single initial null hypothesis tree gamma hyp gamma delta associated unigram context gamma hyp gamma delta 
follows bayes rule gamma hyp delta gamma jv hyp delta delta gamma hyp jf tree gamma hyp gamma delta delta delta gamma gamma hyp gamma delta instant postulate gamma hyp jw delta maximum 
nh extremely large increases practice necessary restrict storage max candidates 
letting hyp refer qth hypothesis gamma hyp delta gamma hyp delta pn max gamma hyp delta summation denominator max hypotheses effectively proportionally distributes probability mass associated discarded hypotheses retained ensuring max gamma hyp jw delta demanded language model 
denominator common new hypotheses choice max best candidates may considering joint probabilities conditional probabilities 
procedure maintains fixed maximum number hypotheses significant number low associated probabilities 
computational efficiency important issue may discarded means beam pruning :10.1.1.28.4818

statistical tagging language model maintains set hypotheses input text string gamma jw gamma highest implicitly maintains set probable category assignments word gamma 
categories part speech pos classifications allows statistical tagging unlabelled text 
tagging large quantities text example 

experimental results 
building gram trees lob corpus employing method section gram language model trees categories corresponding pos word classes constructed various pruning thresholds lob corpus remaining forming test set 
tree complexities category perplexities shown point labelled corresponding threshold value 
addition perplexities obtained pruning simply thresholding total number occurrences event training set shown various choices threshold termed count threshold ct 
technique commonly employed making gram models compact 
number parameters tree pos perplexity test set perplexity training set perplexity test set count threshold prune ct ct ct ct ct ct language models lob corpus shows model complexity increases test set perplexity moves minimum 
initial decrease due underfitting subsequent increase overfitting data 
overfitting occur leaving cross validation models test set approximately reduced significantly comparison count thresholds 
optimal model gamma significantly lower perplexity tree comparable size obtained count threshold pruning 

word perplexities lob corpus trees constructed gamma language model section 
bigram trigram structures obtained stopping growth levels respectively 
third obtained allowing tree growing algorithm execute completion referred 
table shows word perplexities obtained model various max word perplexities decrease monotonically max increases demonstrating history equivalence class total number parameters tree taken measure complexity 
max bigram trigram table word perplexities lob corpus ambiguity significant effect language model performance 
results indicate max yields near optimal results 
furthermore longer contexts lead drop perplexity respect bigram trigram structures 
word trigram language model corpus achieves perplexity contains parameters 
decrease perplexity accompanied fold increase complexity 

tagging accuracies language model trained lob training set tag test set 
tagging accuracy oov words significantly lower words vocabulary effect augmenting lexicon words additional sources investigated 
sources consisted oxford advanced learner dictionary available electronically frequent names surnames oov genitive forms words baseforms vocabulary 
effect existing category grams called generalise new words 
possible word grams new statistics gathered new entry 
table shows tagging accuracies ta vg model augmented ua lexica 
corresponding figures employing training test set tagger provided benchmark 
performance taggers similar exhibits improvement considerable reduction tagging errors oov words 
differences attributed longer gram contexts method calculate unknown word probabilities 
lexicon augmentation halves oov rate improves tagging accuracy 
vg ua vg oov words ta ta non oov ta oov words table lob corpus tagging accuracies 
word perplexities switchboard corpus switchboard corpus consists words recorded telephone speech focus research conversational speech recognition 
language model constructed corpus pruning threshold word vocabulary closed respect test set 
corpus annotated part speech information tagged built lob corpus augmented lexicon described previous section 
table shows performance resulting baseline word bigram trigram models switchboard dev test set words sentences 
compared trigram achieves higher perplexity contains parameters 
lower perplexities larger difference performance word category models compared section may ascribed greater homogeneity style topic switchboard corpus larger amount training data 
word bigram word trigram parameters perplexity table word perplexities switchboard corpus 
category language model capable doubling statistical tagger employing grams varying lengths described 
model building procedure optimising compactness respect performance experiments lob corpus show language models constructed way outperform conventional gram approaches 
model effective dealing corpora sparse due small size heterogeneous composition intrinsic ability generalise unseen word sequences maximum benefit 

bahl brown de souza mercer treebased statistical language model natural language speech recognition ieee trans 
assp vol 
july 
duda hart pattern classification scene analysis wiley new york 
tagger suite user manual may 
johansson atwell garside leech tagged lob corpus user manual norwegian computing centre humanities bergen 
katz estimation probabilities sparse data language model component speech recogniser ieee trans 
assp vol 
march pp 

ney essen kneser structuring probabilistic dependencies stochastic language modelling computer speech language vol 
pp 

niesler woodland variable length grams language modelling tech :10.1.1.28.4818
report cued infeng tr dept engineering university cambridge april 
shannon communication theory exposition fundamentals ire trans 
inf 
th feb 
