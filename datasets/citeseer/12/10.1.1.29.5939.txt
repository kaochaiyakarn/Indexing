thesis fr ed eric gruau front ecole normale superieure de lyon universite claude bernard lyon de doctorat specialty computer science neural network synthesis cellular encoding genetic algorithm 
date defense january th composition jury pr maurice nivat reviewers jean meyer jacques michel cosnard jacques pierre whitley phd thesis prepared centre etude nucl de grenoble epartement de recherche ere ee ecole normale sup erieure de lyon laboratoire de informatique du parall lip imag unit de recherche associ ee au cnrs acknowledgments am grateful michel cosnard thesis advisor 
gave trust thesis 
efficiency michel cosnard equation minutes michel cosnard week money 
michel cosnard gave research communication visiting laboratories 
michel cosnard france interested genetic neural networks 
believe subject going trunc connectionism 
michel cosnard ideas thesis put name publications 
pierre responsible front cea 
helped lot 
corrected publication 
discover new world statistical physics 
encouraged freedom 
ideal conditions 
gordon talks 
great cooperation whitley colorado state university fort collins combination learning gas 
whitley open discussions helped lot cellular encoding 
written chapter thesis 
heinz laboratory month 
heinz gave advices 
john koza generously told results ideas book years publication 
things 
artificial neural networks considered machine learns small modifications internal parameters 
changing 
learning method allow generate big neural networks solving real world problems 
thesis defends points ffl key word go dead modularity 
ffl tool generate modular neural networks cellular encoding 
ffl optimization algorithm adapted search cellular codes genetic algorithm 
point common idea 
modular neural network means neural network sub networks arranged hierarchical way 
example subnetwork repeated 
thesis encompasses parts 
part demonstrates second point 
cellular encoding machine language neural networks theoretical basis parallel graph grammar checks number properties compiler high level language 
second part thesis shows third point 
application genetic algorithm synthesis neural networks cellular encoding new technology 
technology solve problems unsolved neural networks 
automatically dynamically decompose problem hierarchy sub problems generate neural network solution problem 
structure network hierarchy sub networks reflects structure problem 
technology allows experience new scientific domains interaction learning evolution set learning algorithms suit ga contents genetic neural networks genetic algorithms genetic programming neural networks genetic neural networks overview presentation cellular encoding basic cellular encoding coding comparison kitano scheme properties cellular encoding architecture encoding neural network encoding simulation turing machine neural compiler stage compilation kind neurons sigmoid dynamic new program symbols principles neural compiler structure compiled neural network compilation pascal instruction compilation pascal program macro program symbols kernel pascal procedures functions arrays enhanced pascal results compilation compilation standard pascal program callgen instruction 
instruction application compiler neural network design tool design hybrid systems automatic parallelization genetic synthesis neural networks learning genetic programming fitness simulation parity simulation symmetry interaction learning evolution adding learning cellular development baldwin effect developmental learning comparative goals implementation hebbian learning operationalizing developmental learning simulation different possible combinations strategic mutation parity symmetry switch learning specification learning algorithm switch learning algorithm simulation basic switch learning algorithm extension switch learning algorithm simulation extended switch learning algorithm simulation genetic algorithm mixed parallel ga state art parallel genetic algorithms mixed parallel ga super linear speed modularity adding modularity cellular encoding experiments modularity ga ce genetic programming neural networks hierarchy genetic language cellular encoding versus lisp evolving genetic language successful marriage bibliography reviewed articles patent conference published proceedings national conference published proceedings research report conferences proceedings cellular encoding graph grammar cellular encoding graph grammar encoding grammar set trees equivalence algebraic approach graph grammar technical complements small example compilation macro program symbols registers cell syntax micro coding cellular operators list program symbols microcode syntax parse trees 
arithmetic operators 
reading writing array recursive macro program symbol rules rewriting nodes parse tree library functions handling arrays 
trained neural networks extended switch learning chapter genetic neural networks genetic algorithms genetic algorithms ga aim finding minimum mapping defined fixed length bit strings 
ga population bit strings evolve generations sufficiently solution allocated time elapsed 
bit strings called chromosomes 
evolution population determined steps 
ffl selection step individuals reproduced higher probability image small respect individuals 
ffl cross step individuals mated exchange pieces bit string 
ffl mutation step individual changes bit string small probability 
theorem called theorem ensures global improvement population generation provided hypothesis called building block hypothesis verified 
genetic algorithms robust applied optimization problem 
requirement able encode solutions bit strings 
easy parallelize 
parallel ga super linear speed 
basic ga dates back john holland father ga edited book called adaptation natural artificial environment 
great amount done subject usa germany strategies 
research mathematical analysis experiments various kind cross mutation selection explores operators inspired biology coding methods bit string 
section presents case labeled trees bit strings 
genetic programming john koza genetic algorithms evolve lisp programs 
lisp language programming functions 
lisp program expression rooted labeled tree ordered branches 
labels symbol lisp functions 
leaves labeled constants program inputs 
output program value computed expression 
recombination mother father tree sub tree cut father tree replaces sub tree mother tree 
koza mutation 
thinks mutation implemented recombination sub trees exchanged leaves 
john koza uses ga mutation step 
applies ga labeled trees bit strings 
search space space possible labeled trees 
koza wrote book shows genetic programming allows automatically generate programs solve great number problems various domains 
wrote second book lisp expressions allow automatic definition function reusability 
showed functions allows solve complex problems 
coding labeled trees 
see method generating neural networks proposed thesis named genetic programming neural networks ga handles data structure labeled trees 
neural networks genetic algorithms understood model learning 
model information distributed individuals 
neural networks represent model learning different flavor 
neural network oriented graph cells 
cell computes activity activities neighbors connected input 
cell propagates activity neighbors connected output 
computation done cell apply function called sigmoid weighted sum inputs 
weights represent information contained neural network 
define mapping computed neural network 
definition differs particular model neural network 
aim learning neural net compute particular function 
done progressive weights 
iteration weights slightly modified adding small quantity minimize gradient error 
research aim optimizing topology neural network 
research progressive 
iteration neuron connection added suppressed 
chang wrote book type methods called constructivist method 
constructivist algorithm optimize architecture progressive starting initial solution 
constructivist algorithm generate neural networks modular structure reflects structure problem solved 
generate neural network composed different functional units 
genetic neural networks relatively large number papers deal various combinations genetic algorithms ga neural networks 
survey schaffer whitley eshelman proceeding workshop combinations genetic algorithms neural networks 
genetic algorithms weight training supervised learning reinforcement learning applications 
genetic algorithms select training data interpret output behavior neural networks 
gas applied problem finding neural network architectures 
problem interested 
architecture specification indicates hidden units network units connected 
order apply ga problem finding neural network architectures problem solve code architecture structures called chromosomes manipulated ga classes methods exist genetic neural networks direct encoding graph data structure encoded 
connectivity matrix network directly encoded 
gives long chromosomes size network including neurons 
scalability problem small networks hidden units 
making cross directly hardware connections produce non functional offsprings 
known structural functional problem 
solution encode chromosome collection features provide building blocks 
abstraction representation 
parametrized encoding list parameters encoded 
parameters describe number layers size layers layers interconnected 
method 
provide big nets small chromosomes restrictive range architectures layered architectures 
find modular architectures modularity defined section 
grammatical encoding rewriting grammar encoded 
grammar interpreted recursive manner allows generate family related neural networks 
kitano begins implement scalability abstraction modularity 
method chromosome encodes matrix grammar 
rule grammar rewrites theta matrix theta matrix 
application rules follows starts theta matrix denoted single character axiom grammar 
step rule rewrites axiom matrix transformed theta matrix characters denoted second step character rewritten theta matrix 
yields theta matrix process goes theta matrix greater symbol refers desired number units target neural network 
extracting theta matrix mapping character get theta matrix elements 
matrix connectivity matrix encoded neural net 
grammar encoding code big nets short chromosomes imposing architecture particular shape 
method clearly improves previous 
optimizing grammars generate network architectures directly optimizing architectures method hopes achieve better scalability sense reusability network architectures 
words goal method find rules generating networks useful defining architectures general class problems 
particular allow developers define grammars smaller problems theses grammars building blocks attacking larger problems 
certain problems kitano representation scheme 
kitano uses matrix grammar 
theta matrix developed network neurons smallest power bigger order get acyclic graph feed forward neural network consider upper right triangle decreases efficiency encoding 
defines recursive equation matrix computes family integer matrices family weighted neural nets uses matrix grammar sense 
approach similar kitano 
kitano restricted matrices size clear range possible sizes 
size neural network determined advance 
overview kitano schemes matrix grammar grammar replace element matrix theta matrix 
thesis propose encoding graph grammar 
called cellular encoding 
cellular encoding directly develops family neural nets avoids need go matrix representation 
matrix object grammatical rules applied cell network 
cell copy code 
cell reads code different position 
depending reads cell divide change internal parameters neuron 
natural efficient act development level cells elements connection matrix 
resulting language describe networks clear compact way representation readily recombined ga part thesis demonstrate interest cellular encoding coding scheme 
cellular encoding show parallel graph grammar demonstrates list properties 
ga despite fundamental importance encoding attempts establish description theoretical properties encoding particular case neural network optimization 
show possible define verifiable properties neural network encodings 
representation chromosome compact 
chromosome develop valid phenotype 
developmental process gives modular interpretable architectures powerful scalability property 
theoretical properties show cellular encoding suits ga describe pascal compiler produces cellular code neural network computes specified pascal program 
show cellular encoding truly neural network machine language 
compiler presents interests 
considered tool design huge neural networks 
automatic programs written divide conquer strategy 
compile naive hybrid systems 
second part thesis demonstrate efficiency cellular encoding respect ga simulation hard problem solved reported 
ga find boolean weights architecture neural networks learn boolean functions 
ga find solution neural networks need backpropagation optimize weights time saved 
propose learning enhance method goal learning way combined 
ga find architectures adapted particular learning algorithm 
learning speed ga compare speed obtained different possible ways combining learning ga genetic algorithm allows global search search space find set sub neural networks neural networks solution problem include 
learning tunes draft neural networks produced ga speed process 
show new combination ga learning uses baldwin effect efficient lamarckian way learning 
combination mode called developmental learning 
propose learning method called switch learning adapted ga fast learning changes boolean weights deep neural networks correct 
parallel ga programmed reported 
example modular search done findings preceding chapters developmental learning switch learning parallel ga 
genetic programming uses ga evolve lisp computer programs 
compare genetic synthesis neural networks cellular encoding genetic programming 
part cellular encoding part thesis cellular encoding study properties 
chapter presentation cellular encoding basic cellular encoding cellular encoding method encoding families similarly structured neural networks 
chapter basic cellular encoding 
chapters extend alphabet code needed 
basic cellular encoding genetic algorithm synthesize neural networks second part thesis 
chapters consider extensions cellular encoding purpose ga cellular code represented grammar tree ordered branches nodes labeled name program symbols 
reader confusion grammar tree tree grammar 
grammar tree means grammar encoded tree tree grammar means grammar rewrites trees 
cell node oriented network graph ordered connections 
cell carries duplicate copy cellular code grammar tree internal reading head reads grammar tree 
typically cell reads grammar tree different position 
character symbols represent instructions cell development act cell connections cell 
step development process cell executes instruction referenced symbol reads moves reading head tree 
draw analogy cell turing machine 
cell reads tree tape cell capable duplicating execute instructions moving reading head manner dictated symbol read 
section shall refer grammar tree program character program symbol 
cell manages set internal registers development determine weights thresholds final neural net 
link register refer possibly fan connections links cell 
consider problem finding neural net exclusive xor function 
neurons thresholds 
connections weighted gamma 
weighted sum input strictly greater threshold neuron outputs outputs 
inputs neural net 
development neural net starts single cell called ancestor cell connected input pointer cell output pointer cell 
consider starting network right half cellular encoding depicted left half 
starting step reading head ancestor cell positioned root tree shown arrow connecting 
registers initialized default values 
example threshold set 
cell repeatedly divides gives birth cells eventually neuron neural network 
cell said neuron looses reading head 
input output pointer cells ancestor linked indicated boxes execute program symbol 
development process upper pointer cell connected set input units lower pointer cell connected set output units 
input output units created development added independently 
development complete pointer cells deleted 
example final decoded neural net input units labeled output unit labeled 
ffl division program symbol creates cells 
sequential division denoted seq child cell inherits input links second child cell inherits output links parent cell 
child connects second weight 
link oriented child second child 
child cells division program symbol label nodes arity 
child moves reading head left subtree second child moves reading head right subtree 
illustrated steps 
ffl parallel division denoted par second kind division program symbol 
child cells inherit input output links parent cell step step 
cell divides values internal registers parent cell copied child cells 
ffl program symbol denoted causes cell lose reading head finished neuron 
cell read subtree current node subtree node labeled 
labels leaves grammar tree nodes arity 
ffl value program symbol modifies value internal register cell 
incbias increments decrements threshold cell 
symbol inclr increments decrements value link register points specific fan link connection 
changing value link register causes point different fan connection 
link register default initial value pointing leftmost fan link 
operations connections accomplished resetting value link register 
program symbol denoted val sets weight input link pointed link register val sets weight gamma see step 
program symbols val val explicitly indicate fan connection corresponding instructions applied 
val val executed applied link pointed link register 
ffl unary program symbol cut cuts link pointed link register 
operator modifies topology removing link link register modify topology 
operators inclr cut illustrated required development neural net xor problem 
sequence cells execute program symbols determined follows cell executed program symbol enters fifo queue 
cell execute head fifo queue 
cell divides child reads left subtree enters fifo queue 
order execution tries model happen cells active parallel 
ensures cell active twice cell active 
cases final configuration network presentation cellular encoding depends order cells execute corresponding instructions 
example development xor performing step step produce neural net output unit having negative weights desired 
waiting program symbol denoted wait cell wait rewriting step 
wait necessary cases development process controlled generating appropriate delays 
output input incbias seq par seq par step development neural net exclusive xor function right half 
development starts single ancestor cell labeled shown circle 
inside circle indicates threshold ancestor cell 
ancestor cell connected neural net input pointer cell box labeled input neural net output pointer cell box labeled output cellular encoding neural net xor shown left half 
arrow ancestor cell symbol seq graph grammar represents position reading head ancestor cell 
continuous line indicates weight heavy line weight gamma 
default weights 
coding point description grammar tree recursion 
note recurrence grammar imply recurrence resulting neural network 
able develop single neural network grammar tree 
develop family neural networks share structure computing family similarly structured problem 
allow get property scalability 
purpose introduce recurrent program symbol denoted allows fixed number loops cell reads executes algorithm life life life reading head root tree reading head subtree current node life register cell initialized ancestor cell 
grammar develops family neural networks parametrized symbol illustrated 
cellular code cellular code xor network 
difference program symbol incbias seq par seq par seq par seq incbias input output output input left execution step sequential division seq pointed ancestor cell preceding causes ancestor cell divide cells 
cell feeds cell weight 
reading head cell points left sub tree cellular encoding left box par reading head new cell points right subtree box second level seq 
right execution parallel division par step causes creation cell 
cell inherit input feeding 
cells output cell place sent output 
reading head cell points left sub tree reading head new cell points right sub tree 
incbias seq par seq par incbias seq par seq par output input output input left execution step sequential division seq pointed cell preceding causes cell divide cells 
cell feeds cell weight 
reading head cell points left sub tree box third level par reading head new cell points right subtree box third level val 
right execution program symbols 
causes cells lose reading head finished neurons 
takes time steps time step presentation cellular encoding output input input output seq par seq incbias seq par seq incbias left execution parallel division step causes creation cell 
cell inherit input cell feeding 
cells send output cell place sent output 
reading head cell points left sub tree incbias reading head new cell points right sub tree 
right cell executes value program symbol set 
link register default value points left link 
action set set weight link 
dashed line indicate weight 
execution set cell goes read program symbol located fourth level tree 
replaced recurrent program symbol rec 
resulting cellular code able develop neural net parity function arbitrary large number inputs assembling copies xor sub network 
network parity inputs shown 
implementation recurrence allows precise control growth process 
development stopped network size reaches predetermined limit code read exactly times 
number structure neural networks 
comparison kitano scheme kitano scheme ce encodes grammar 
kitano encoded matrix grammar 
ce encodes graph grammar 
rewriting matrix grammar ce rewrites neuronal cells 
fact rewriting cells defining neural network machine language 
language describe network architectures weight patterns natural clear way 
developmental process global rewriting connectivity matrix node graph processed separately 
final product rewriting exactly encoded neural network need additional process 
contrary kitano developmental process produces raw matrix characters needs processing artificial conventions yielding neural network 
stress advantage ce kitano scheme 
ffl kitano scheme piece code represents subset rules repeatedly decoding phase produce pattern connections different places neural net 
repeated patterns connectivity output input val par seq par seq incbias output input left neuron executes value program symbol incbias 
action incbias increase threshold cell 
execution threshold cell reading head cell points fifth level tree 
right tree steps consist executing program symbols 
cause neural cells lose reading head finished neurons 
finished neurons development finished 
final neural net input units output unit 
neuron input unit link input pointer cell output link input pointer cell 
neuron second input unit link input pointer cell second output link input pointer cell matrix repeated subnetworks appear kitano experimental results 
theoretical property modularity proved ce 
network generated ga clear modular structure 
ffl kitano characterizes scalability ability ga find big neural networks parametrized problem big size 
goodness tested atypical case degenerated version encoder decoder problem number neurons hidden layer bigger number input output neurons 
original problem number hidden neurons logarithm number input neurons point problem challenging 
ce theoretical property scalability proved 
simulation ce show fixed size code develop arbitrary large neural network solve boolean problem arbitrary large size 
ffl kitano encoding scheme requires artificial conventions decrease compactness code 
theta matrix developed network neurons smallest power bigger worst case extracting theta matrix uses fraction gamma bits approximately bits large 
say neuron numbers limited power 
range matrix size limited power 
computation shows large matrix developed small part 
order get acyclic graph feed forward network consider upper right triangle 
presentation cellular encoding output input incbias seq par seq par rec incbias seq par seq par rec input output recurrent developmental program neural network computes parity binary inputs 
life ancestor cell 
steps shown preceding figures 
fourth step cell executes recurrent program symbol reading head points label seq root grammar tree back development step 
life decremented equal 
cell give birth second xor network outputs sent child cells generated cell 
output input step final decoded neural network computes parity binary inputs 
life ancestor cell 
total number steps tree executed times 
clearly see copies xor network 
initial life developed neural network input parity problem copies xor network 
ffl kitano scheme grammar encoded set rules 
grammar produced ga clean 
may rules rewrite character rules 
small percentage coded rules efficiency poor 
ce grammar encoded rooted point labeled tree ordered branches labels refer program symbol 
ensures ga produces deterministic grammars rewriting rules 
fig 
shows trees compact rules 
ffl cellular encoding special attributes allow encode recursive grammars produce infinite networks 
stopping development network size connectivity matrix reaches bound simple mechanism allows apply grammar rules fixed number loops grammar develop family neural networks parametrized chapter properties cellular encoding recognized key point successful application genetic algorithms ga optimization problem scheme encode solutions structures manipulated ga despite fundamental importance encoding attempt establish description theoretical properties encoding particular case neural network optimization 
show possible define verifiable properties neural network encodings 
previous approach focussed particular aspect 
encoding tried simulation reported intuitive explanation results 
find complete set theoretical properties 
chapter theoretical verifiable properties rate encoding neural networks defined 
properties completeness compactness closure modularity scalability power expression abstraction 
cellular encoding ce shown check properties 
shown cellular encoding graph grammar 
exist single ce plenty 
features needed define ce initial graph cells set program symbols 
describes initial graph cell development starts 
contains single ancestor cell pointer cells 
chapter kinds initial graphs cells ancestor cell initial graph recurrent link output input 
recurrent link develop recurrent networks 
initial network graph recurrent link noted recurrent link cyc 
particular ce program symbols determined symbols 
symbol refers initial network graph symbols refer program symbols 
notation symbol slanted 
sections shown increasing number program symbol allows achieve growing set properties 
reintroduce program symbols defined chapter give precise definition 
appendix 
gives list program symbols thesis 
architecture encoding section program symbols develop architecture studied 
architecture directed graph 
vertex stands neuron 
architecture includes ordered set neurons 
lists specify input output neurons 
minimal set program symbols encoding architecture 
parallel division 
execution execution case recurrent link 
weight represented weight program symbol parallel division noted par 
ary program symbol 
parallel division child cells inherit input output links father cell 
parent cell recurrent link children connect 
fig 
shows links reordered new cell added 
second noted cut cuts input link 
integer argument specifies number input link cut 
third program symbol noted cell lose reading head terminal neuron 
cut cut par par wait cut cut cut cut cut par par par cut cut cut cut cut cut cut cut encoding arbitrary recurrent network 
target connectivity matrix code uses program symbols par cut code uses program symbols par cut wait 
fig 
shows encode arbitrary graph 
leaves leaves labeled program symbol represented figures 
development graph having nodes done stages 
stage fully connected graph nodes developed dlog dxe ceiling function 
done binary tree depth dlog labeled par 
second stage properties cellular encoding cell cuts input links 
links gamma cells cut 
final graph nodes connected desired 
property convenient formalization property completeness encoding scheme complete respect set architectures resp 
neural networks element set encoded 
preceding construction yields proposition cyc par cut complete respect set architectures 
program symbol cut topology final network depends cell rewritten 
preceding construction cells start cut input links growth net finished 
net size exact power developed 
development avoided means unary waiting program symbol wait cell wait rewriting step 
program symbol wait necessary rewriting order encoded generating appropriate delays 
fig 
shows wait encode neural network shorter chromosome 
need create cells final number cells 
cell starts cut time 
cells finished division earlier just delayed program symbol wait 
clearly construction code shorter 
define property compactness follows property compactness architecture encoding scheme said topologically compact encoding architecture encoded code size encoded code size 
schemes encodes weights said functionally compact neural network encoded code size exists neural network encoded code size implements function associated par wait par par wait inclr clip clip par inclr clip clip compact encoding arbitrary recurrent network 
target connectivity matrix compactness respect compactness respect 
size code number bits necessary store 
program symbols bits suffice encode program symbol plus null character 
size code needed store tree ff nodes ff 
want show compactness respect classical representations graphs 
consider direct encoding connectivity matrix 
recall produces code size network neurons 
provide reading cell internal register called link register contains link number 
define unary program symbol inclr incrementing link register unary program symbol clip cutting link pointed link register 
shown fig 
cell fully connected graph select subset connections execution clip alternated inclr 
fully connected graph cells size total code 
holds proposition cyc clip inclr par wait topologically compact 
program symbol cut implemented clip sake clarity figures 
clip denoted cut arguments 
second consider representation list connections encoded 
neuron numbering encoded log bits 
connection numbers connected neurons specified 
produces encoding size log network connections 
encoding interesting connections 
assume link states 
cells links registers 
state stored register link called state 
link duplicated cell division register link duplicated 
rewriting links states removed 
define program symbol turns state link specified integer argument counterpart 
fig 
shows encode arbitrary network program symbols 
default value recurrent link initial graph 
labeling root grammar tree sets state recurrent link 
program symbols par develop fully connected graph links 
cell builds set connections applying program symbols numbers desired neighbors 
numbers coded log bits size total code log 
holds proposition cyc par wait topologically compact 
clone clone seq seq par wait par wait encode efficiently layered neural network 
sequential division sequential division code layer cells clone program symbol 
weight represented weight consider scheme encodes layered neural networks complete interconnections layers 
specifies number neurons layer 
define division properties cellular encoding program symbols 
sequential division noted seq child cell inherit input links father cell second child inherits output links child connect second link weight state 
example shown fig 

clone division denoted clone par child cells place reading head node grammar tree 
subtree nodes labeled clone program symbol unary 
called clone child cells execute exactly code give birth subnetwork neurons 
neural network layers developed follows apply gamma times seq produce string cells 
stage cell stands layer 
rank cell string corresponds layer number 
cell standing layer execute times par number cells th layer 
shown fig 
layer neurons developed combination log par clone program symbols 
total size code log 
times size code produced 
proposition cyc par seq clone wait topologically compact 
define property 
property closure architecture encoding scheme closed respect set architectures possible code develops architecture set 
program symbols ce closed respect set architecture 
means code generated ga develop architecture 
ensures ga generates chromosome ga able develop evaluate network receive feed back 
mrg cut par trunk leave leave wait cut par wait seq wait mrg mrg seq seq seq merge program symbol 
application program symbol application code arbitrary acyclic graph 
suppose architecture looking known belong particular set 
possible ga priori knowledge 
design encoding scheme closed respect set ga automatically place individuals set 
example require acyclic architectures order able backpropagation 
starts rewrite acyclic network graph remains acyclic par seq cut inclr wait closed respect set acyclic architectures complete respect set 
order obtain completeness program symbol merge denoted mrg introduced 
program symbol mrg integer argument rewrites cell manner input link number neighbor cell connected program symbol replaces list input links shown fig 

operation output links suppressed 
fig 
shows build code arbitrary acyclic graph 
acyclic graph possible number nodes node connected nodes having greater number 
proposed code trunk leaves number nodes 
intermediate step cells current network graph connected cell reads trunk 
periodically cell reads trunk divides creates child reads leaves represents node number stage cells represent nodes number connected cell read th leave prune input links 
corresponding cutting program symbols located inside dashed line fig 

proposition par seq cut inclr mrg wait closed complete respect set acyclic architectures 
adding clone yields property compactness respect 
neuron want path input unit neuron output neuron having path constant 
require exists path neuron output unit neuron computation neural net 
architecture meets requirements meaningful architecture 
put way architecture meaningful remains connected having suppressed input pointer cell output pointer cell 
show closure property respect set meaningful architectures acyclic 
semantic program symbols prune links slightly modified manner reading cell apply program symbols cut mrg link state number input links state strictly greater 
take initial network graph 
try suppress input pointer cell output pointer cell 
pruned graph connected graph node 
case analysis shows application program symbols defined keeps network graph connected 
final network graph connected 
final graph obtained starting minus input pointer output pointer cell 
characterization meaningful deduce final architecture meaningful proposition par seq cut inclr mrg wait closed respect set meaningful acyclic architectures 
consider case meaningful recurrent architectures 
construction uses fact meaningful recurrent architecture contains meaningful acyclic architecture 
take initial network graph cyc 
define order cells network graph rules ffl cell division child reads left subtree reads right subtree 
ffl cell cell descendant including descendant including 
properties cellular encoding provide cell input sites link connected cells link connected cells greater equal cell division takes place input sites managed respect definition 
order develop meaningful recurrent architecture respect preceding restriction application pruning program symbols input site connected inferior cells 
clone derive property compactness respect 
construction difficult new semantics allow set links 
step cell input link 
hopefully ensured increasing size code desired complexity 
input output imposing input units output units 
cell labeled ancestor cell 
wishes impose number input units output units take kind initial network graph shown fig 

neural network encoding section describe encode neural network 
architecture weights kind sigmoids neuron encoded 
simple way encoding weights provide unary program symbol val weights link pointed link register cell value specified argument 
argument sigma integer real depending desired neural network 
consider encoding scheme defined preceding section 
encoding scheme list weight values provided connectivity matrix 
holds 
result proposition cyc par seq cut inclr mrg wait val complete respect set neural networks units sigmoid 
functionally compact 
show properties stage development 
architecture developed weights set 
ga point view encoding val program symbol presents drawbacks 
ga forms initial population random individuals 
described koza program symbol val needs argument random value generated specified range 
mutation case koza scheme possible value argument generations limited set values represented initial population 
case program symbol val means possible values weights restricted finite set 
particular weight needed available current population ga able provide 
program symbol val possibility find values progressive refinement formation building blocks increasing size 
solution problem program symbols mult multiplies weight argument add adds argument weight 
possible go direction set program symbols dec mult div respectively increments decrements multiplies divides weight input link pointed link register 
program symbols interesting arguments 
set val formation weight values done subtle combination program symbols 
precision weights coded fixed priori 
encoding number takes bits fixed point format encoding number built program symbols chosen dec mult div allows property compactness respect weights coded fixed point format 
similar technique remove argument program symbols 
example argument link number program symbol value link register 
order keep property compactness needs program symbol mult lr multiplies value link register set value greater number input links 
purpose clarity figures program symbols integer argument 
max sigmoids 
sign function bounded linear function 
constant max parameter model 
neural network entirely determined weighted architecture 
neuron may features different sigmoid different learning procedure adjust weights 
encode internal register cell attributed features program symbols modify value registers introduced 
example assume choice sigmoids plotted fig 

register stores type sigmoid 
program symbol sets value register argument 
cell terminal neuron register contains number sigmoid 
simulation turing machine section build encoding neural network simulates turing machine 
transition function neurons sign function 
turns enable obtain interesting properties 
step want achieve property 
property modularity consider network includes different places copy subnetwork code modular includes code single time 
properties cellular encoding property tries formalize intuitive idea modularity 
modularity help reduce apparent complexity initial network decomposing set complex subnetworks connected simple structure 
process repetitive subnetworks may decomposed 
modularity connected reusability problem aim transform neural network design construction box activity networks built combination existing standard elements 
module separately understood human reader architectures produced easily understandable 
order include subnetwork provide program symbol reading head cell jump subtree defining method advocated ray called addressing template inspired molecular biology 
ray writes machine codes ip jumps piece code exact numeric address target code specified machine code 
consider biological system contrast order protein molecule interact protein molecule specify exact coordinate located 
molecule presents template surface complementary surface diffusion brings complementary conformations allow interact 
addressing template illustrated unary program symbol jmp 
jmp followed sequence nop operation program symbols kinds nop nop 
suppose piece code instructions order jmp nop nop nop 
cell reading jmp search tree nearest occurrence complementary pattern nop nop nop 
nearest defined natural topology graph distance vertex length shortest path connects 
pattern jmp program symbol ignored 
pattern cell positions reading head complementary pattern 
branching loop 
order avoid infinite growth network cell provided life register 
life register reading cell initial graph initialized value complementary pattern jmp program symbol decrements life register removes reading head life register positive 
algorithm number loops bounded code develops family neural networks parametrized size network increases order fulfill property modularity provide possibility encode separately subnetwork main network purpose ary program symbol def defined 
cell executes def places reading head right subtree 
order left subtree contains labels nop program symbols 
left subtree encodes subnetwork right subtree corresponds main network 
proposition cellular encoding scheme includes program symbols jmp nop nop def modular 
order able connect included subnetwork rest network functional way program symbol division called splitting division denoted split required 
cell executes split division follows assume number input links equal number output links 
cell splits children number links 
inheritance links shown fig 

child cell index inherits input link output link numbered child cell places reading head node grammar tree 
split split splitting division 
division duplication links input site order total number links equal creation neurons 
input links output links input links duplicated homogeneous manner equal number input output links apply splitting 
cell output links input links output links likewise duplicated 
ce design neural network particular mapping 
example fig 
shows neural network simulates turing machine 
design method build networks higher complexity specifying connect subnetworks smaller complexity 
representation scheme uses decomposition 
subnetwork neurons named represented separately 
represent subnetwork subtree encodes represents graph cells obtained manner 
take initial network graph reading cell reading head placed root neighbors set labeled cells 
labeled cells represents neighbors cells include development complete neural network 
initial network graph develop conventions 
ffl cell reads jmp program symbol freeze development cell introduce new subnetwork code subtree node labeled jmp write name included subnetwork followed name circle representing frozen cell 
ffl cell reads split program symbol freeze development introduce new subnetwork code subtree node labeled split write split followed name circle representing frozen cell 
subtree node labeled jmp split node necessary introduce new virtual subnetwork 
final network graph labeled cells indicate subnetwork connected rest network 
recall final network depends order execution 
order finish representation order execution frozen cells specified needed 
case turing machine specified cells frozen split program symbol execute program symbol neighbors neurons 
final encoding preceded certain number wait program symbols correctly delayed 
note number linear function initial value life register 
wait program symbols encoded means subnetwork includes wait 
fig 
shows neural net recurrent layers processing layer mixing layer 
layers connect manner split properties cellular encoding split split split mix rule rule ruler rules rules rules process mix par par par turing process mix mix par par par par par par par par par par rules states characters right move left move move mix mix mix encoding turing machine 
inserted layers 
time data flows neural net transition turing machine simulated 
fig 
shows processing layer applies parallel subnetworks neurons called rule tape registers 
slight difference leftmost rule called rightmost ruler 
subnetwork rule encodes transition rules turing machine 
group rule corresponds register turing machine tape 
register character state encoded 
reading head positioned register state encoded state turing machine state 
input th rule applied binary code state character corresponding register 
example bits state bits character 
states alphabet characters number input units rule dlog dlog number output units rule rows cells 
row represents possible moves reading head turing machine 
subnetworks ruler units 
handle register extremity tape moves possible 
reading head turing machine register state code concatenation dlog output rule dlog units middle row activities dlog input units 
accounts fact character tape remains unchanged 
reading head register moves direction left stand right 
corresponding row output neurons indicate state character write 
rows output zero values 
hidden unit rule implements rule 
action mixing layer 
mixing layer compute state character tape registers total output process layer 
activities recombined activities 
fig 
shows done 
fig 
show done 
il activities computed logical applied il activities 
define precisely meant simulating program machine arbitrary big memory parametrized neural net finite size 
define possible functions implemented neural net follows definition neural net implements function input output units vector bits clamped input units activities initialized relaxed synchronous dynamic stabilization activities output units consider machine memory bits program computations bit strings 
run particular input bit strings program memory time yielding binary vector 
definition particular input bit strings form input set function computed output function properties cellular encoding definition neural network parametrized integer simulates program type machine memory bits implements function computed 
property power expression parametrized neural network encoding power expression greater programming language program written language exists code parametrized neural net simulates program size code number bits needed store source code case turing machine program just list transition rules 
preceding construction produce code parametrized neural network simulates arbitrary turing machine program adjustment input units processing layer encode state connected input pointer cell 
initial activity zero register tape initially contain reading head turing machine 
initially state encoded register initial state turing machine evaluate size codes 
number bits needed encode transition rules log log size code par log log 
size code respectively rule ruler 
size codes subnetworks constant 
total size 
proposition cyc par seq split cut inclr mrg wait val nop nop jmp def power expression greater turing machine language 
start process tape code start computation state ipc opc input code neural network simulates turing machine simulates neural network outline neural network modification process layer subnetwork start 
fact ce power expression greater turing machine results sort maximal property compactness proposition cyc par seq split cut inclr mrg wait val nop nop jmp def functionally compact neural network encoding 
order prove consider arbitrary neural network encoding scheme code particular neural network encoded scheme 
call size exists program input builds neural network encoded runs network input returns output network 
program executed turing machine initial tape concatenation output output neural network 
neural network simulates size string neurons added string registers tape automatically initialized fig 
shows final neural network looks 
depicts neural network simulates turing machine develops neural network encoded simulates 
simulated neural net input units 
simplification neurons corresponding characters tape represented 
encoding final neural network specify string neurons bit complex encoding fig 

fig 
shows part code process layer 
subnetwork code develop subnetwork type rule 
th subnetwork connected input pointer cell 
connected subnetwork start intermediate neuron weight depending value th bit role subnetwork start send zero values 
subnetwork tape develops tape registers 
mixing layer slightly modified order mix registers size total encoding linear function 
total size 
close analysis shows slope linear function depend 
interesting property scalability 
consider problem variable size parametrized neural network encoding scheme 
suppose code solve problem size problems big sizes different parameter 
ga find code network solves problem small size harder ga find network problem size corresponding code 
ability address problems big size called scalability 
preceding allows define scalability theoretical property encoding scheme property scalability family functions finite domain integer index 
parameterized encoding scalable respect exists code neural net encoded parameter computes function consider definition definition family functions finite domain said recursive exists turing machine computes initially written input tape 
recursive tm space minimal number registers needed compute value holds proposition cyc par seq split cut inclr mrg wait val nop nop jmp def scalable respect set recursive family functions tm space smaller certain order prove proposition provide encoding arbitrary turing machine network developed size parameter simulates turing machine finite tape registers registers done 
encoding similar proposed 
technical matter properties cellular encoding 
family languages recognized turing machine registers certain called pspace 
pspace big includes known languages 
ce scalable day families functions 
cellular automaton simulated construction similar case turing machine 
time mixing layer precede processing layer task reproduce location concatenation states cell neighbors th cell 
proposition cyc par seq split cut inclr mrg wait val nop nop jmp def power expression greater cellular automata 
turing machine cellular automata universal computing devices 
cellular automata particularly powerful 
general algorithm efficiently programmed cellular automaton 
possible transform set transition rules cellular automaton ce code neural net efficient way 
consider efficient computing device cellular automata 
property step forward 
property abstraction encoding power expression greater high level programming language 
high level programming language uses procedure array data structure 
encoding scheme allows reduce problem genetically finding neural network problem genetically finding program computes target mapping 
exists small program computes target mapping space program search smaller space neural networks bounded size contains solution network 
encoding better ga mapping learn computed short program 
property abstraction hard demonstrate take chapter purpose 
fact proof compiler transform pascal program ce length proportional length pascal program 
property property coding scheme grammatical object encoded translated grammar rewriting system 
proposition cellular encoding grammatical encodes graph grammar proof appendix grammatical property genetic search neural networks cellular encoding appears grammatical inference process language implicitly specified explicitly positive negative examples 
results new perspectives ee particular method search cellular encoding grammar second part thesis 
encode grammar set trees evolve sets trees genetic algorithm 
method infer types grammars 
conversely results research grammatical inference field improve cellular encoding 
third study cellular encoding grammar helpful classify cellular encoding complexity classes exist rewriting grammars 
cellular encoding context sensitive parallel graph grammar 
applying genetic algorithm ga neural network synthesis recognized key point scheme encode neural networks structure manipulated ga theoretic properties rate efficiency neural network encoding proposed 
neural network encoding desired properties studied 
encoding called cellular encoding ce uses alphabet alleles 
allele corresponds slightly modifies topology labels graph describing network 
combination program symbols develops initial graph cells ancestor cell plus pointer neurons neural network 
program symbols par clone seq split mrg cut dec mult div inclr mult lr wait nop nop jmp def 
program symbols integer arguments 
ce generates neurons single type sigmoid sign function 
easy add program symbols types sigmoid needed 
properties ce follows completeness neural network encoded ga reach solution neural network exists 
compactness ce topologically compact classical representations neural networks functionally compact representation 
ensures codes manipulated ga minimal size 
shorter codes shorter search space effort ga 
closure ce develops meaningful architectures 
produces acyclic recurrent architectures depending initial graph 
closure properties ensure chromosomes produced ga give interesting neural networks 
ga throw away codes 
increases efficiency 
modularity network decomposed copies subnetworks code network concatenation subnetworks code code describes connect subnetworks 
facilitates formation building blocks 
resulting regularities final architecture clear interpretable 
code compact 
scalability fixed size code encodes family neural networks 
reasonable family problems exists code th neural network solves th problem 
parametrized problem arbitrary size say easy solve problem size code corresponding neural net size parameter changes 
power expression ce seen neural network machine language 
power expression greater turing machines cellular automata 
proof extreme compactness coding 
abstraction possible compile pascal program code neural network simulates computation program 
wide search space neural network transformed smaller search space programs 
helps ga find neural networks problems solved short program 
grammars cellular encoding encodes graph grammar property fact root properties 
chapter neural compiler encoding scheme called cellular encoding described preceding chapter 
cellular encoding seen machine language neural networks able describe turing machine cellular automaton 
cellular encoding tool designing neural networks simulate turing machine cellular automaton 
known neural network simulate turing machine infinite number neurons 
see variations theme relation cellular automata 
turing machine simulated neural networks having finite number neurons 
order store infinite amount data uses neuron activities 
activities unbounded precision 
results theoretical interest practical 
usual uneasy program turing machine cellular automaton 
chapter propose method compiling program written high level language neural net 
proof program simulated neural net compilation practical method building neural network solves problem 
see compilation interests design huge neural networks automatic parallel compilation compilation naive hybrid systems 
compiled networks finite number neurons activities finite precision finite memory 
possible compile instruction dynamic memory allocation 
recursive procedures compiled 
run program depth recursion smaller constant 
performance compiler measured term size code generated 
result confirms efficiency compiler allows demonstrate property abstraction number defined preceding chapter 
stage compilation neural compiler programmed 
software called jannet just automatic neural network translator 
aim software solely prove validity compilation measure performance product number neurons generated 
jannet encompasses stages shown 
input compiler program written enhanced pascal 
output neural net computes specified pascal program 
pascal said enhanced proposes supplementary instructions compared standard pascal 
simulation neural computer neural net cellular code parse tree pascal program stages neural compilation 
parsing pascal program 
rewriting parse tree cellular code tree grammar 
decoding cellular code neural network 
scheduling mapping neural network physical machine 
stage done 
stage parsing program building parse tree 
standard compilation language 
parse tree somewhat unusual form 
appendix tree grammar describes kind parse tree 
third stage uses cellular encoding 
simply decoding cellular code generated second step 
decoding uses development network graph seen preceding chapters 
second stage heart compiler 
stage rewriting trees 
rewriting means replacing symbol group symbols 
simple case word rewriting replace letter sequence letters 
starting initial word rewrites new word new word final word 
rewriting tree simple case consists replacing node tree sub tree 
added subtree specify glue sub trees node rewritten 
second stage consists replacing node parse tree sub tree labeled program symbols cellular encoding 
program symbols sub tree executed cell local graph transformation replacing cell cells connected neighbors cell node parse tree associated local transformation network graph cells 
node parse tree corresponds macro program symbol local graph transformation 
macro program symbol transformation bigger done program symbol cellular encoding scheme 
cases program symbol creates cell modifies register 
uses single integer parameter 
macro program symbol create lot cells needs parameter 
needs program symbols encode macro program symbol name macro program symbol 
presentation compilation method done cellular encoding 
consider process level macro program symbols 
macro program symbols implemented sub trees program symbols described appendix 
chapter read independently preceding ones 
release jannet go 
produce instructions executed particular neural computer 
produces neural network format suits neural computer processors dedicated neuron simulation 
presently neural network generated jannet simulated sequential machine 
fourth stage shown consists mapping neural net physical architecture neural computer 
step take account things memory size neuron communications processors arithmetic precision computation 
neural compiler kind neurons sigmoid neuron determined computation inputs 
ordinary neuron sum inputs weighted weights connections applies function called sigmoid 
different kind sigmoid 
number associated 
cell register called stored number sigmoid neuron 
sigmoids listed 
different kind sigmoids 
add numbers 
compare numbers test equality numbers 
non classical neurons product inputs divide input second input 
multiply divide numbers 
possible simulate multiplication division classical neurons cost lot neurons 
dynamic neuron computing arithmetic operation operands wait operands come starting compute activity 
particular dynamic 
neurons decide start computation rules ffl neuron computes activity soon received activities input neighbors 
ffl input units initialized input vector 
ffl neuron sends activity output neighbors initialized just computed activity 
special threshold units encode threshold 
moment neurons compute activity 
dynamic denoted half way sequential dynamic parallel dynamic 
need global entity control flow activity 
dynamics compiler 
dynamic concerns neurons having exactly inputs 
case neuron waits received activity neighbors 
input strictly positive neuron send activity second neighbor 
neuron propagate activity 
dynamic allows block flow activities 
eand neuron going see dynamic 
third dynamic called neurons dynamic compute activity activity received input neighbors 
activity activity particular neighbor 
order correctly neuron received activities time 
dynamic aor neuron 
allows gather neuron activities coming different parts 
program symbol adl 
add link argument 
program symbol adds cell link number 
microcode sm 
division adl division children 
segment analysed 
segment analysed segment analysed 
difference microcode microcode 
cell division cell divides labeled input neighbor labeled 
input link duplicated 
input link duplicated change ordering links cell child labeled second child labeled 
preceding chapter concerned theoretical properties interested practical implementation 
order compiler produce compact cellular code introduce division program symbols program symbols modify cell registers local topological transformations influence order execution 
new program symbol may integer argument specifies particular link particular register favors compact representation 
implementation purpose 
microcode composed parts 
part specifies category program symbol 
uses capital letters 
div indicates division top modification topology cel sit lnk modification respectively cell register site register link register 
cell possesses ordered set input sub sites links fan ordered neural compiler set output sub sites links fan 
exe bra indicates program symbol manage order execution hom refers division children aff denotes program symbols graph display 
program symbols listed appendix 
second part microcode operand 
cel sit lnk operand name register value modified argument 
arithmetic sign operator applies arithmetic operation actual content register argument places result register 
simply sets register argument 
exe operand name particular order execution 
bra program symbol tests condition registers cells topology true reading head placed left sub tree placed right sub tree 
operand name condition 
div operand list segments 
segment composed letter possibly empty list arithmetic signs 
cell divides child cell inherits input links second child cell inherits output links 
segments analysed reversed order specify movements duplications links child cells 
gives example div microcode analysis 
letter capital links duplicated moved output site second child cell output site child cell 
letter small letter links duplicated moved input site child cell input site second child cell 
ffl letter means move set links ffl letter means duplicate set links 
ffl letter added links ordered way neighboring cell 
explains difference 
ffl letter means connect output site child input site second child needs arithmetic sign 
arithmetic signs specify set links moved duplicated 
specifies links lower equal greater argument specifies links 
refer link refers links site neighboring cell gamma necessary count links decreasing order increasing order usual gamma placed capital letter exchanges role input site output site replace argument value link register 
replace argument number input links divided 
similar micro coding operands top locally modifies topology cutting reordering links 
case single child cell inherits output links mother cell analysis segments indicates movements links input site mother cell input site child cell 
appendix number program symbols labeling cells 
development network graph software programmed indicate program symbols read cell 
neuron keep reading head cellular code program symbol read 
dummy program symbols label leaves code 
enables see computer screen function neuron performs 
new program symbols section describe program symbols compilation pascal programs 
program symbols described microcode explain separately 
program symbol bloc allows block development cell 
cell reads bloc waits input neighbors cells finished neurons 
program symbol compiler avoid particular piece cellular code developed early 
compiler generates trees cellular code tree main program tree function 
trees called grammar trees number 
number jump program symbol 
program symbol jump arguments defined section 
compiler find efficient program symbol integer argument 
argument specifies number grammar tree 
reading cell executes program symbol jmp places reading head root grammar tree number split par propagation sub site flag 
sub site indicated small black disk 
parallel division cell labeled par parallel division see sub site created neighbor register input site neighbor contains value 
execution program symbol split 
result splitting division 
intermediate stage 
need handle separately groups link 
distribute links groups links 
group contained sub site 
sub sites created complicated way 
technical details follow may skipped problems understanding rest chapter 
sub sites coded boolean flag sub site link 
flag link indicates new sub site begins particular link 
flag link 
link encoded elements input site output site 
element inserted linked list 
position element linked list determines number link input site output site 
element contains different flag sub site 
necessary connection may start new sub site input site starting sub site output site 
input output site register called 
cell division takes place link duplicated 
rules determines setting flag sub site elements connection duplicated 
consider element belong cell neighbor cell divides 
register site corresponding element connection value flag duplicated 
value flag copied element smaller number element flag initialized value 
rules means neural compiler creation sub site 
old sub sites kept 
contrary sub site divides sub sites 
gives example sub site creation 
second update flag element connection belongs cells created division microcode div cell links modified microcode top 
consider case division 
value register mother cell duplicated 
seen preceding chapter div list connections created chaining segments connections 
value register sub site marks inserted sub site created segment 
new mark inserted number sub sites kept constant 
distribution links group links respect split program symbol 
program symbol particular behavior cell sub sites 
particular links duplicated number links sub site input output sub site 
call number links sub site 
cell splits child cells 
child cell input links input sub sites mother cell 
similarly child cell output links output sub sites mother cell 
value registers duplicated register child cell 
register child cell sub sites links sub site link 
register child cell sub sites 
principles neural compiler structure compiled neural network order simulate pascal program neural network 
shows simple example compilation 
variable pascal program contains value 
variable initialized program 
changes assignments 
variable corresponds neurons changes value 
neurons represent values taken run program 
step run value value contained neuron represents neuron connected input neighbors contain values variables compute neuron connected output neighbor contain values variables 
input neighbors value compute new value variable represent 
environment compilation means set variables accessed point program 
execution instruction environment represented row neurons 
row contains neurons variables 
neurons contains value variable represents particular moment 
instruction going modify environment modifying value variables 
instruction compiled neural layer computes modified values old values stored row neurons 
new values stored row neurons 
pascal program compiled sequence rows alternated neural layers compute new values 
neural layers pascal instructions 
compilation pascal instruction idea compilation method translate word pascal language modification network graph neurons 
coarse granularity compilation program result compilation pascal program var integer 
dotted line corresponds compilation instruction 
neurons simple addition inputs weights sigmoid identity 
represents initial environment 
encompasses values stored neurons line value changed contained new neuron computes sum 
order sum neuron connected neuron value changes line value changes 
see instructions executed parallel neurons example variable represented neurons run program 
decomposed compilation instruction 
instruction translated neural layer laid row neurons contains environment modified instruction 
compilation construction process 
compiler builds row neurons contains initial environment translating part parse tree variables declared 
compiler lays consecutively neural layers compiling program instruction instruction 
idea progressive building compiled neural network applied granularity smaller pascal instruction 
pascal instruction decomposed words pascal language words organized tree data structure called parse tree see 
associate word pascal language local modification network graph cells combined effect small modifications transforms single cell neural layer computes specified pascal instruction 
done system similar cellular encoding 
neural network developed steps 
certain neurons copy parse tree reading head reads particular node parse tree 
called reading cell neuron role computation corresponding neuron divide create neural layer associated pascal instruction 
cell reads parse tree different position 
labels parse tree represent instructions cell development locally act cell connections cell 
see instructions decomposed sequence program symbols cellular encoding scheme 
called macro program symbols 
step development cell executes macro program symbol read neural compiler reading head moves reading head leaves parse tree 
assign bin op idf lect idf lect idf aff parse tree instruction 
link side parse tree labeled words pascal program 
right labels parts part specifies kind node second part attribute stores function node 
example idf aff indicates node contains name variable modified idf lec name variable read bin op name binary operator addition assign indicates doing assignment left sub tree contain name variable modify right sub tree contains expression new value 
nodes idf aff idf lec attribute contains name variable modify read 
bin op attribute contains name particular binary operator 
assign node particular attributes 
cell manages set internal registers 
development determine weights thresholds final neurons 
consider problem development neural net compiling pascal instruction 
development instruction causes creation cells development cell left 
development begins single cell called ancestor cell connected neurons contain initial values environment 
consider network described right 
reading head ancestor cell placed root parse tree pascal instruction indicated arrow connecting 
registers initialized default values 
cell divide times executing macro program symbols associated parse tree 
give birth neural network associated parse tree 
cells loose reading head finished neurons 
finished neurons obtained desired neural network 
example describes compilation pascal instruction 
important keep mind figures input output connections ordered 
position connection circle represents cell codes connection number 
compilation pascal program preceding sub section shown parse tree pascal instruction interpreted tree labeled macro program symbols 
program symbols executed cells develop neural layer computes specified pascal instruction 
method generalized pascal program 
total program represented parse tree 
represents parse tree pascal program program var integer read write 
nodes parse tree assign bin op idf lect idf lect idf aff development neural network corresponding pascal instruction right side 
development begins ancestor cell labeled connected neurons labeled 
left side represent parse tree pascal instruction 
arrow ancestor cell symbol assign parse tree represents reading head ancestor cell 
input output connections ordered 
position arrow circle cell codes connection number 
example link number 
assign bin op idf lect idf lect idf aff step execution macro program symbol assign read ancestor cell preceding 
cell divides child cells 
child cells inherit input links mother cell 
child connected child 
reading head cell points node idf aff cell points node bin op assign bin op idf lect idf lect idf aff step execution macro program symbol idf aff read cell 
cell deletes link moves fourth link back position 
looses reading head neuron 
neural compiler assign bin op idf lect idf lect idf aff step execution macro program symbol binop read cell 
cell divides cells 
cells inherit input links mother cell connected output cell 
cell inherits output links mother cell looses reading head neuron 
sign inside circle indicates neuron addition inputs 
idf aff idf lect idf lect bin op assign idf aff idf lect idf lect bin op idf aff idf lect idf lect bin op assign idf aff idf lect idf lect bin op assign step execution macro program symbol idf lec read cell 
cell disappears having connected input neighbor output neighbor 
step execution macro program symbol idf lec read cell 
cell disappears having connected second input neighbor output neighbor 
reading cells compilation pascal instruction finished 
ancestor cell connected neurons contain values modified environment 
start develop instruction 
declare variables 
create layer neurons contain default values variables 
nodes parse tree correspond instructions 
create layers neurons computations associated instructions 
consider starting neural network right half 
input pointer cell output pointer cell ancestor cell linked execute macro program symbols 
development input pointer cell points input units network output pointer cell points output units 
development network reported appendix 
graph cells reaches size cells neurons left 
idf idf lec write type simple semi col read program parse tree pascal program program var integer read write 
parse tree usual form 
contains nodes declaration variables labels organized standard way 
particular form helps implementation neural compiler 
appendix contains bnf grammar describes form parse trees 
output input decl idf lec idf aff program read semi col type simple write development neural network simulates pascal program program var integer write left half 
development begins ancestor cell labeled represented circle 
ancestor cell connected input pointer cell box labeled input output pointer cell box labeled output 
left represent parse tree pascal program 
arrow ancestor cell parse tree represents reading head ancestor cell 
macro program symbols seen preceding section method neural compilation illustrative example compilation small pascal program 
method association label parse tree macro program symbol 
call macro program symbol name associated label 
macro program symbol replaces cell executes graph reading cells neurons 
graph connected neighbors term neighbor refers cells connected direct connection indirectly special neurons called pointer neurons 
graph specify reading cells going read parse tree 
fact reading cell read time child node node labeled node read exactly cell number reading cells equal number child nodes 
happen certain reading cells blocked finish development 
case graph contains reading cells child nodes 
parse tree described tree grammar appendix 
tree grammar neural compiler details list macro program symbols combined 
chapter refer non terminal grammar 
classify kinds macro program symbols macro program symbols type expression correspond labels generated non terminal expr produce neurons compute values 
macro program symbols type modification environment manage environment 
section going macro program symbols illustrated exemple shown preceding section 
put forward invariant pattern rewriting context 
cell rewritten macro program symbol neighbor cells type number 
environment contains variables cell input links output links 
input resp 
output link points input resp 
output pointer cell 
second input link connected cell called start cell input links point neurons contain values variables 
second output links points cell labeled 
cell role connect graph generated macro program symbol rest network graph 
macro program symbol type expression cell neuron reading cell 
macro program symbol executed compilation program macro program symbol program 
macro program symbol creates invariant pattern 
recurrence invariant kept reading cells generated macro program symbol neighbors verify invariant pattern 
associate macro program symbol label parse tree 
check invariant pattern try minimize number neurons created sure macro program symbol desired job 
case macro program symbol type expression generated sub network compute right value case macro program symbol type modification environment check modification correct 
time time cellular encoding description 
practical implementation macro program symbols decomposed program symbols 
macro program symbol decompositions reported appendix 
program symbols cellular encoding implement small modifications graph 
example create cells split single parameter 
contrary macro program symbols create lot cells connected complex manner 
decomposition macro program symbols program symbols efficient way implementing macro program symbols 
simple quick way 
allows macro program symbols pages expression parenthesis appendix 
compilation happens cells execution macro program symbol block development input neighbors neurons 
unblock go execute piece cellular encoding order certain graph transformation 
kernel pascal consider sub section programs written reduced instruction set kernel pascal allows write simple programs 
consider moment scalar variables see arrays 
macro program symbols decl string 
effect decl macro program symbols add input link ancestor cell 
link connected cell connected start cell weight 
connection ensures neurons corresponding variable activated start network 
suppose variable initialized value 
sequential execution implemented string reading cells associated list instructions 
cell string position reads sub parse tree instruction number list 
input neighbors cell neurons contain values environment 
cell position single input neighbor cell number cell number delay development cell number finished develop subnetwork modifies environment instruction 
cell blocks development input neighbors cell neurons 
assignment simple example instruction 
order understand recall variable corresponds input connection number 
storage value variable done connecting neuron contains value ancestor cell 
connection right number order store value right variable 
management connection number done macro program symbol idf aff 
similarly order read content variable know link number connect neuron contains value variable neuron needs value 
input units neural net correspond values read execution pascal program 
pascal program instruction read appears input unit created 
setting initial activity input unit variable initialized output units correspond values written execution pascal program 
instruction write appears output unit created 
time instruction write executed value output unit value neuron input unit connected input pointer cell 
output unit connected output pointer cell 
macro program symbol read adds connection input pointer cell neuron represents variable macro program symbol write adds connection neuron represents variable output pointer cell 
binary arithmetic operator translated tree neurons neuron implements particular arithmetic operation particular sigmoid 
unary operators implemented way cells created 
cell develop sub network corresponding sub expression unary operator applied 
second place reading head cellular code corresponds unary operator 
cellular code describes develop sub network computes unary operator 
macro program symbols rewriting cell cells 
going new kind macro program symbols 
rewrites cell graph cells size proportional size environment 
represents step complexity rewriting 
new class macro program symbols implemented sub sites split program symbol 
pascal line translated boolean formula neuronal formula eand aor eand 
true value coded false coded 
logical realised single neuron gamma weight 
see layer neurons eand divided sub layers neurons 
sub layers allow direct flow data environment sub network computes body sub network computes body 
recall neuron eand extended special dynamic 
integers inputs boolean integer 
boolean false neuron eand outputs 
boolean true outputs integer 
neuron aor arithmetic retrieves output environment sub network output environment subnetwork transmits instruction follows 
aor special dynamic 
update neural compiler activity soon input neighbor changed activity 
dynamics aor eand described section 
constant needed computation constant stored bias neuron sigmoid identity 
neuron linked start cell control activity say time times neuron sends constant neurons need 
instruction condition 
body 
corresponds recurrent neural network infinite loop computation take place finite memory 
memory allocation done inside body loop 
flow activity enters loop layer aor neurons 
layer buffer 
flow activity goes network compute condition 
value condition flow activities goes loop sent back body loop 
case goes buffer neuron aor having passed layer neurons act synchronizer 

synchronization done avoid neuron body loop updates activity times mean time neuron updated 
case encapsulated loops neurons corresponding inner loop come back initial state computations inner loop finished 
ready start loop outer loop commands 
neuron start useful especially body loop 
considered local variable copy buffer neurons aor 
connected neurons contain constants body loop activate neurons loop iteration 
essential neural network computations 
compilation repeat similar 
computation condition done 
procedures functions going explain compile procedures functions 
example pascal program order illustrate pass parameters call return procedure function 
program program var glob integer global variable procedure proc integer formal parameter var loc integer variable local procedure proc procedure proc integer formal parameter var loc integer variable local procedure proc proc parameter passed value proc body main program proc parameter 
passed value proc slightly modified invariant pattern environment contains variables 
variable global second parameter passed procedure third local variable procedure 
figures output link cell points output pointer cell second link points cell 
inverse represented purpose clarity representation 
allows planar graph 
imagine context certain point parse tree moment proc calling proc 
environment neuronal side encompasses global variable input pointer cell start cell plus pascal variable glob 
environment contains variables local proc loc 
want translate calling procedure proc 
thing suppress local variables proc environment 
done macro program symbol call described 
develop parameters passed value proc proc macro program symbol comma 
ancestor cell develop body proc 
body proc begins macro program symbol procedure inserts cell 
body procedure proc translated cell pop local variables proc cellular code pop 
cell inserted call executes cellular code restore allows recover local variables proc 
obligatory put aside cut restore local variables proc 
interest variable assigned fixed connection number 
macro program symbols call call pop parameters parameter global number global variables environment 
parameter local number local variables 
macro program symbol call simpler call 
need create cell restore environment calling procedure function return environment simply value 
macro program symbol function effect 
necessary create cell pop environment function 
macro program symbol return null effect 
arrays arrays important data structure programming language pascal 
special kind neurons order able handle arrays 
neurons called pointer neurons 
record array data structure neural tree 
pointer neurons nodes neural tree 
data leaves neural tree 
pointer neurons ensures ancestor cell possesses exactly input link variable 
allows handle variables separately want read modify 
pointer neurons represent array dimensions 
invariant extended 
contains pointer neurons 
uses example extended invariant represents array elements 
uses neural tree depth represent array dimensions 
tree depth represent array dimensions 
layers pointer neurons storing array dimensions necessary want able handle column separately dimension 
array variable declared structure neural tree created time 
purpose type variable coded parse tree string macro program symbols type array see 
macro program symbol associated dimension array creates layer pointer neurons neural tree 
macro neural compiler program symbol type array single parameter number columns dimension corresponding macro program symbol 
string type array ended macro program symbol type simple described 
macro program symbols handle neural trees 
recurrent exploration neural trees 
example aor sync macro program symbol re build neurons containing current environment 
know depth tree way process tree recursively 
problem cellular encoding allows recursive development 
program symbol bpn tests neighbor number id pointer neuron 
depending result test different cellular code executed 
case recursive call done case terminal processing done 
reading array translated unusual parse tree 
parse tree indicated 
reading array interpreted result function lect index written cellular code inputs array dimensions 
inputs indices plus name array 
function lect index returns value read array 
writing array example translated parse tree kind case reading 
indicated 
writing array dimensions interpreted result function affect index written cellular code inputs output array 
inputs indices value assign name array 
function affect index returns modified array writing 
see figures neural networks developed reading writing big 
number neurons proportional number elements array 
indices read write array known compilation time complexity developed neural net reduced 
added neural net fixed size depend number elements array 
enhanced pascal sub section describe compilation new instruction added standard pascal 
instructions exploit interest neural compiler 
added instruction callgen allows call function defined cellular code 
pascal program instruction include file name 
indicates name file cellular codes functions called stored 
syntax callgen idf 
callgen code name idf 
result call value assigned variable idf 
code name indicates particular cellular code called function 
idf 
parameter passed function 
macro program symbol callgen effect call body called function specified parse tree cellular code defined compilation takes place 
callgen includes neural layer computes called function 
neurons contain values idf idf connected input included neural network 
output included neural network connected neurons representing variable idf 
executing body callgen neurons contain value environment suppressed 
philosophy callgen define number functions various context include neural building blocks trained 
neural networks developed callgen need global variables 
simpler suppress calling systematic way 
exemple predefined functions appendix ffl function returns left part array ffl function returns right part array ffl function concat allows merge arrays 
ffl function int array adds dimension array ffl function array int suppresses dimension array ffl function random initialize array random values gamma main goal compiler automatic parallelization program written divide conquer strategy 
strategy problem size divided sub problems size sub problems size problem size 
uses array function order divide data array equal part 
share columns dimension sets columns equal size 
number columns odd sub array column 
need division process size problem 
test development neural net number columns array 
depending result test different part parse tree developed 
case part parse tree corresponding decomposition developed 
case part parse tree corresponding problem size developed 
static 
syntax normal replaced 
condition expression 
expression written array 
macro program symbols test array column dimension 
case ancestor cell goes read left sub tree goes read right sub tree 
results compilation section propose examples compilation illustrate principles compiler interest 
order run compiled network initialize input unit value read execution pascal program 
neural net computation dynamic halfway parallel sequential 
network stable computation finished 
output units contain values written pascal program 
compilation standard pascal program shows network compiled program program type tab array integer var tab max aux integer read neural compiler neural net sorting numbers neural net computing fibonacci number developed depth 
max max max fi aux max max aux write 
input units start cell plus values sorted 
output unit correspond instruction write 
recurrent connections drawn segments 
clearly see encapsulated loops mapped recursive sets connections 
shows neural net compiled program program fibonacci hybrid neural network simulates behavior animal var integer integer function fib integer integer fib fib fi return read write fib 
example illustrates fact program compiled uses recursive calls depth recursion bounded compilation begins 
depth recursion bounded 
net compute fib 
fact number value initialize life register ancestor cell 
callgen instruction 
added pascal language instructions callgen include allow include final network smaller networks weights learning algorithm 
code nets stored file name indicated instruction include 
show example pascal program uses callgen instruction 
compiled net shown 
include animal program animal type tab array integer tab array integer var pixel tab feature tab position bad move integer function opposite position integer integer return position neural compiler neural net parallel sorting integers 
read pixel feature callgen pixel position callgen feature bad callgen predator feature bad move opposite position move fi write callgen motor move 
file animal stored cellular codes different layered neural networks 
network inputs pixels outputs list relevant features 
network position object determines features position object object lies visual field 
neural net predator determines object predator animal 
neural net motor commands muscles legs 
compiled neural net simulates behavior animal front predator 
input units pixels output units neurons leg muscles 
neural net stored library order included bigger neural network 
example illustrates easy interface pascal language predefined neural nets 
compiled neural net encompasses neural layers plus part purely symbolic 
part corresponds instruction 
compiler links various neural networks allows symbolic computation outputs neural building blocks manage input output 
instruction added pascal language instruction allows test condition compile time 
tested condition form 
test column array column 
instruction allows automatic parallelization program written divide conquer strategy 
strategy problem size decomposed sub problems size problems size 
instruction predefined functions enrich array data structure 
exemple extract left part right part array int array array int add suppress dimension 
report example pascal program uses instruction compiled net shown include array program merge sort const type tab array integer var tab function merge tab integer tab integer tab var tab integer callgen concat fi fi fi return function sort tab integer tab return return merge sort callgen div div sort callgen div div fi read write sort 
function fusion written standard pascal 
function sort uses enhanced pascal order implement divide conquer strategy 
file array contains library cellular codes allow compute function manipulating arrays 
compiled neural net inputs start cells plus integers sort 
outputs show sorted list integers 
structure net understood 
layer recursive neural nets outputs sorted lists integers 
second layer outputs sorted lists integers 
recursive function sort compiled finite graph applied finite array integers 
divide conquer strategy network sorts integers time neuron computations parallel 
compiled programs divide conquer strategy 
convolution matrix product maximum integer list programs compiled neural net able job time ln 
application compiler jannet process new paradigm automatic building neural network algorithmic description problem solved 
paradigm exact opposite existing trend neural networks machine learns 
going show jannet reproduce improve kinds compilation 
jannet process able cumulate interest compilations 
neural compiler neural network design interest language describing neural networks sesame facilitate design large complex neural networks 
languages propose define building block neural networks 
put order build complex neural networks 
composed 
hierarchical design practical 
instruction callgen allows achieve effect jannet 
compiles neural net jannet produces cellular code 
cellular code stored file called callgen included bigger neural network 
allows modular compilation compilation point view 
permit hierarchical design neural network design point view 
jannet goes language description sesame 
jannet gives possibility combine neural building blocks mentioning physical connections network 
specification logical soft level 
human understand better soft description describe logical step computation set connections 
jannet allows design huge neural networks 
near machines millions neurons available advantages logical description obvious 
jannet compiler produces graphical representation neural net takes account regularities graph 
knowledge software 
tool design hybrid systems jannet process compile pascal program neural network 
compile base rules expert systems artificial intelligence neural network just need write base rules pascal inference motor 
jannet includes compiler base rules called hybrid systems goal combine artificial intelligence neural networks 
jannet design hybrid system different way 
compiler put model neural networks algorithmic knowledge knowledge compiled fuzzy knowledge learned fixed size neural networks 
compiler build naive hybrid systems encompass layers 
layer learned sub symbolic 
computes symbols grounding fuzzy distributed data 
second layer compiled computation symbols 
resulting hybrid system said naive symbols linked 
automatic parallelization numerical examples shows jannet automatically parallelize problems sorting matrix vector product matrix matrix product vector vector product 
jannet parallelize algorithms written divide conquer strategy 
kind algorithms divide problem sub problems half size 
sub problems divided sub subproblems 
problem size reached solved constant time 
problems size independent solved time 
lies potential parallelism 
actual version jannet considered automatic produce instructions particular parallel machine 
produces neural network representation adapted parallel computer neural network intrinsic parallel model 
final step consists mapping network architecture parallel machine 
step take account size memory processor communications processors arithmetic precision computation 
claim jannet full automatic 
neural networks produced jannet shape 
dimension structure represents time 
possible project network dimension array processors map neural net array processors 
jannet full program written divide conquer strategy 
algorithm programmed way advantage compared traditional approach described user need add program indications processor store data 
part ii cellular encoding genetic algorithm second part thesis study neural network synthesis genetic algorithm ga cellular encoding ce 
suppose reader knows cellular encoding 
chapter second part describes simulation done ga learning introduced chapters 
chapter genetic synthesis neural networks learning genetic programming thesis ga optimization procedure manipulates codes called chromosomes 
basic case chromosomes fixed length bit strings 
possible kind structures chromosomes provided operators mutation recombination defined structures 
recombination creates new offspring chromosome mixing information coming parent chromosomes 
mutation slightly modifies chromosome 
recombination allows exchange information chromosomes mutation performs kind random hill climbing 
actual trend ga community put possible information structure chromosome operators ensures chromosomes generated ga valid syntax correct evaluated structure chromosome contain syntactical information ga bother syntax 
koza shown tree data structure efficient encoding effectively recombined genetic operators 
genetic programming defined koza way evolving lisp computer programs genetic algorithm 
koza genetic programming paradigm individuals population lisp expressions depicted graphically rooted point labeled trees ordered branches 
chromosomes grammar trees exactly structure approach generating grammar trees 
set alleles set cell program symbols label tree search space hyperspace possible labeled trees 
crossover sub tree cut parent tree replaces subtree parent tree result offspring tree 
subtrees exchanged recombination randomly selected 
gives example cross 
genetic programming algorithm differences koza 
ga number generations 
elapsed time greater variable mutation rate allele probability mutated allele arity 
mutation consistently improves genetic search 
koza considers depth individuals initial population 
take account number nodes generate initial population chromosomes having exactly number nodes exchange sub tree done cross size tree allowed increase max designed sequential genetic algorithm uses local selection mating geographical lay individuals 
individuals output input input input output output incbias seq par seq par rec seq par rec seq par seq incbias illustration recombination trees 
second order neural network developed resulting offspring life parameter ancestor cell initialized 
father tree encode neural net 
selected subtree cut encircled 
mother tree encodes xor neural net 
dotted line indicates subtree pruned replaced 
resulting offspring encodes general solution parity problem 
reside grid mate best chromosome random walk neighborhood 
implements isolation distance different local regions grid may display local trends terms evolutionary development 
individuals deleted approximation scheme genitor 
select randomly individuals population delete worse individual sample deleting worse individual population 
population size number generations fixed 
ga deduces function yields expected number generations population size simulation total allocated cpu time 
ga steady state model uses strategy decide create new individual delete individual current population 
ga evaluates average time taken evaluation chromosome 
computes number generations processed keeping current population size higher creates individual deletes individual 
strategy allotted computation time elapsed final population size number generation 
genetic synthesis neural networks learning time new individual created evaluated time average evaluation time updated avg avg 
average evaluation time increases generations number individuals population decreases 
population size dynamically adapt change amount time required fitness evaluation 
early generations chromosomes quickly evaluated large number solutions explored 
generations evaluation see expensive chromosomes stay competition 
fitness important task genetic neural networks fitness evaluate neural network 
section describes fitness evaluate neural network 
define tar boolean function computed neural network nn boolean function computed neural network 
ga produces tree shaped grammars develop valid phenotype connected graph neurons 
input output units specified 
fig 
shows initial graph 
input output pointer cells pointers define set input output cells 
rewriting steps ordered output links input pointer cell define order input units 
similarly ordered input links output pointer cell define order output units 
number input output units encoded chromosome 
advantage code self consistent decoded knowing dimension input output space target mapping 
self consistency code building block concatenated code include corresponding neural net purpose modularity 
number input output units expected boolean function nn computed neural network defined follows visible units input output units neurons deleted decreasing order lower part input vector entered upper undefined components output vector set 
task define measure close nn target function tar usual mean squared error metric information theory 
reason fitness information 
information measure smoother relation natural topology labeled trees defined follow neighbors tree removing adding changing node 
give concrete short example kind smoothness induced information theory 
consider tree develops neural net computes complement tar suppose output space 
order get right mapping add single neuron logical 
adding branch tree possible build tree adds neuron 
respect preceding topology near 
fitness mean squared error give lowest mark highest lot useful information hope reproduce information lost 
fitness information gives mark short definition information mutual information 
consider finite set probability density random variables probability density pd qd information log mutual information omega holds omega show information theory order define fitness 
assume output space 
consider random vector input space uniform probability density 
fitness measure defined fl nn tar nn tar tar nn tar seen information brought nn tar divide tar order get fitness 
mathematical analysis shows fitness reaches value neural net produces right output produces complement 
output space apply preceding approach components average resulting fitnesses 
consider family functions assumed share type algorithm 
call tar member family indexes size problem stored initial value life register 
max neural net nl develops computes approximation tar target nn total fitness gamma max lmax fl nn tar experiments show individuals bad generations 
neural network poor fitness neural networks nl fitness 
necessary develop neural networks nl early generations 
rule decides exactly necessary develop neural network developing rule nl developed continue develop nl nl fitness greater threshold max development 
threshold parameter fix particular application 
rule early generations networks developed 
generations max neural networks developed time saved 
size nl times bigger size lmax evaluation fitness lmax lmax times longer 
max parity ratio 
developing rule interest term sum lies total sum lies max developing rule terms max terms summed 
threshold near developing rule says algorithm continue develop network current right mapping 
fitness lies means networks right mapping th correct 
fitness clearly interpreted tell far problem solved 
genetic synthesis neural networks learning simulation parity objective find ce neural network family computes parity arbitrary large size terminal set function set par seq wait val rec fitness cases random patterns bit input neural net nl number fitness cases determined ga raw fitness mutual information function computed neural net neural net target function standardized fitness raw fitness divided information target function neural net ranges standardized fitness sum standardized fitness developed neural net 
ranges parameters tm max success predicate standardized fitness equals 
networks developed fitness maximum table tableau genetic search neural network family parity problem inputs table summarizes key features problem evolving encoding generates family neural network computing parity function arbitrary large size 
ga tries find chromosome develops family neural networks nl nl computes parity function inputs 
parity binary inputs sum inputs modulo 
threshold defined developing rule 
develop max neural networks 
solutions tested parity problem inputs 
ga generates solutions parity arbitrary large size recurrence learned 
program symbol par seq wait val rec described chapter 
inclr link register modified value remains default value 
program symbol val sets weight input link value 
program symbol set threshold neuron respectively 
highly reduced set program symbols experiment 
wanted ga search easy possible 
population size number generations specified 
variable specifies time allocate ga ga deduces function yields expected number generations population size simulation total allocated cpu time ga computes evaluating average time taken evaluation chromosome 
average changes generations ga update deductions permanently 
evaluation time increases number individuals decreases fig 

population size dynamically adapt change time fitness evaluation 
early generations chromosomes quickly evaluated large number solutions explored select different peaks fitness landscape 
generations evaluation heavy times longer noticed people selected peak stay competition 
number patterns necessary evaluate fitness precision specified 
ga determines number comparing accuracy actual fitness variance population 
fig 
shows better individuals patterns allocated 
ga turns produce huge networks introduce parameter save time ratio number cells network graph respect number visible units exceeded threshold development stopped fitness neural net generation average valuation time population size time left plot average evaluation time milli seconds population size cpu time available seconds versus generations 
set zero 
experiments seconds sun workstation ga finds solution parity function size inputs 
fig 
vertical axis corresponds fitness space interpreted graduation information learned parity 
generation ga solves parity size xor generation reaches size 
analysis solution stage shows best chromosome generates family neural nets nl solves parity problem nl computes parity inputs 
number units neural nets nl proportional number visible units proportional ratio number goes infinite overcomes threshold neural net nl null fitness 
fitness go 
generation ga produces family nl neural nets having units solves parity problem 
ga learned recurrence 
fitness fact infinite remains computer finite memory 
fig 
shows chromosome ga corresponding th neural network solves parity inputs gamma hidden units connections 
clearly combination neuronal groups performing xor 
despite low weight complexity usually quadratic number input units hidden units removed damages 
solution fault tolerant 
designed software allows see development network graph final encoded neural network 
shows picture generated software 
represents neural net solves parity inputs 
dashed lines denote weight straight lines weight black disk represents threshold circle represents threshold 
raw solution input units produces right neural network modulo elimination input units process described section 
genetic synthesis neural networks learning generation average number pattern average fitness best individual plot average number patterns evaluate fitnesses average best fitness current population 
simulation symmetry close analysis solutions ga parity problem shows problem particularly easy solve compared boolean problems 
reasons 
possible encode family neural networks solves parity nodes 
corresponding code shown fig 

second reason obvious 
fig 
show chromosome develops network 
network computes xor 
xor inputs parity problem size ga tries solve preceding set experiments 
chromosome develops right neural network family nl 
fitness greater 
fig 
develop network graph encoded code convention reading cell reads recurrent operator rec freezes development 
means keeps reading head node labeled rec rewrite 
network graph input unit reading cell give birth subnetwork computing parity lower order 
network graph clear recursive process 
expresses recursive rule order compute parity bits compute xor bit result parity bits 
chromosomes differ position wait rec 
chromosome problem size chromosome problem arbitrary size easy ga search 
ga neural net look order solve problem size placed mutation ga introduces single recurrent operator rec allows go achieve recurrence 
express way clearly ce develop neural network recurrence 
involves solving problem size problem recurrence find way building network problem size combining network problem size supplementary neurons 
case wait seq par seq par seq rec seq wait par par solving parity chromosome ga neural net 
box dotted line indicates repeated neuronal group 
weight weight ffl threshold ffi threshold parity problem size problem recurrence xor neural network 
order tackle boolean problems particular feature extend basic ce 
introduce ary branching operator denoted 
cell reads executes algorithm life reading head left subtree current node reading head right subtree current node operator gives account fact computation problem size computation recurrence 
describe experiment problem size problem recurrence symmetry problem 
table summarizes key features problem evolving ce generates family neural network computing symmetry function arbitrary large size 
ga tries find code family neural network nl nl computes symmetry inputs 
input symmetric neural net output 
solutions tested symmetry problem inputs 
program symbol par seq wait val val inclr incbias cut described chapter 
unsuccessful trials noticed time raw neural nets produced ga right number input output units 
evaluated modulo convention described section 
added term fitness rewards neural nets having correct number visible units 
doing hoped kinds individuals coexist individuals developing neural nets mapping right number visible units individuals developing architectures having right number genetic synthesis neural networks learning wait seq par par rec par par wait seq wait seq chromosome xor corresponding neural net chromosome parity function corresponding neural net developement recursion frozen 
weight weight ffl threshold ffi threshold objective find ce neural network family computes symmetry arbitrary large size terminal set function set par seq wait val val inclr incbias cut rec fitness cases number fitness cases determined ga developed neural net neural net inputs input space tested equal number symmetric non symmetric random patterns bits tested 
raw fitness mutual information function computed neural net neural net target function standardized fitness raw fitness divided information target function neural net ranges standardized fitness sum standardized fitness developed neural net 
ranges parameters tm max success predicate standardized fitness equals 
networks developed fitness maximum table tableau genetic search neural network family symmetry problem inputs ce visible units badly weighted bad mapping 
recombination kind individuals produce interesting offsprings 
happens apparently minor modification fitness brought ga success previously constantly failing 
precisely new fitness weighted sum old plus term number input resp 
output unit right adds simulation new fitness old fitness 
new fitness lies 
symmetry problem difficult parity recurrence requires different computation problem size 
problem size xor inputs 
recurrence example network shown fig 
development cell reading rec operator frozen 
network fig 
expresses rec symmetry boolean function neural network problem size network graph recurrence problem sketch grammar tree recurrence vector bits symmetric middle gamma neurons symmetric neuron xor configuration 
fig 
network graph clearly different network graph problem size different recurrence problem 
ga learn distinct tasks 
recall generation neural net developed 
ga faces problem size 
generations chromosomes networks nl developed ga faces recurrence problem 
ga faces changing environment 
difficult problem population converge prematurely solution fitting environment 
reason left sun workstation nodes ipsc 
node machine approximately power half sun 
previous ga run node 
births ga exchanges individual neighbors 
individual best random subset population 
asynchronous communication simulated communication time negligible 
parallel ga studied chapter 
recall ga deduces population size number generations function yields expected number generations population size 
take simulation 
experiments hours ga finds solution trials symmetry function size inputs 
report successful trial 
population size decreases 
average number patterns allocated increases 
fig 
vertical axis corresponds fitness space graduation information learned symmetry 
generation ga solves symmetry size xor generation solves symmetry size approximation recurrence 
generation ga produces family nl neural nets having units solves symmetry problem 
ga learned recurrence 
chromosome ga val gamma seq seq par wait par rec val val wait val inclr val wait val gamma inclr par inclr incbias inclr val gamma rec val wait val inclr gamma chromosome lot information useful 
keeping genes modify network possible simplify 
fig 
shows simplified chromosome resulting neural network convention draw network fig 

simplified chromosome genes shorter solution hand 
design solution hand natural way put operator root tree encode xor subnetwork genetic synthesis neural networks learning generation average fitness best individual plot average best fitness current population versus generations rec inclr rec wait wait seq seq par par incbias par solving symmetry chromosome ga neural net 
box lines indicates repeated neuronal group right branch subnetwork doing mapping fig 
recurrence left branch 
gives genes chromosome seq par par rec seq par inclr incbias val gamma wait val gamma seq par seq par incbias wait val gamma ga able concentrate information trick produced genes piece code encircled fig 
common xor subnetwork subnetwork recurrence uses operator twice positions xor subnetwork subnetwork recurrence differ 
correspond ing th neural network solves symmetry inputs gamma hidden units gamma connections 
believe optimal solution 
clearly combination subnetworks 
similar network graph hand depicted fig 

represents neural net solves symmetry inputs 
chapter shows genetic algorithm cellular encoding able automatically dynamically decompose problem sub problems kind divide conquer strategy generate sub neural network solves sub problem rebuild neural network problem copies sub network 
clearly possible solve problem input parity gradient learning backpropagation kind learning scale size problem points 
chapter clearly show superiority ga ce technology particular case boolean functions exhibits certain amount regularity 
neural network parity input units 
genetic synthesis neural networks learning neural network symmetry input units 
chapter interaction learning evolution difficulty combining search neural network lead ga gradient learning high cost evaluation 
run back propagation algorithm faster improved form gradient descent evaluation neural network number evaluations needed find improved networks architectures quickly computationally prohibitive 
computation cost typically high genetic algorithms impractical optimizing small topologies 
example evaluation function modest sized neural network architecture complex problem involves hour computation time unrealistically low hard problem requires year architecture evaluations genetic search 
architecture complex connections evaluations inadequate genetic search 
research chapter focuses addition learning development process evolution grammar trees 
explore alternative directions approach described 
propose directions need lighter computation cost 
genetic algorithm find architectures adapted particular learning algorithm 
learning speed genetic algorithm 
compare speed obtained different possible ways combining learning genetic algorithm 
particular modes learning explored basic genetic algorithm learning simple learning merely changes fitness function making weight changes simple networks fitness function measures raw network performance called baldwin learning 
complex form learning changes weights simple networks reuses weight changes development larger nets called developmental learning form lamarkian evolution learned behavior coded back chromosome individual passed offspring genetic recombination called lamarkian learning 
feasible test lamarkian learning baldwin learning developmental learning cellular encoding encode neural nets 
features cellular encoding necessary ce encodes architecture weights ce possible code back weight change grammar tree 
ce recursive 
point allows study baldwin learning second point lamarkian learning third point possible study developmental learning 
section discusses role learning evolution various forms learning combined learning evolutionary search strategies experiments 
adding learning cellular development way speed search functional networks add form learning developmental process 
general effective learning process costly reproducing testing new individual cost learning approaches exceeds reproducing testing new individual learning provide greater chance resulting improved behavior 
numerous ways learning added developmental process problems considered 
working boolean networks 
reasons learning algorithm ideally result boolean weights 
reason learn boolean weights learned information coded back grammar 
coding learned weights back grammar useful form lamarkian evolution employed 
lamarkian evolution mean chromosomes grammar trees produced recombination improved learning improvements encoded back chromosome 
improved chromosomes placed genetic population allowed compete reproductive opportunities 
obvious criticism lamarkian evolution darwinian evolution way scientists believe evolution works 
criticism wish preserve schema processing capabilities genetic algorithm lamarkian learning 
consider simple case chromosomes bit strings grammar trees 
changing coding offspring bit string alters statistical information hyperplane implicitly contained population 
theoretically applying local optimization improve offspring undermines genetic algorithm ability search hyperplane sampling 
objection local optimization changing information offspring inherited parents results loss inherited schemata loss hyperplane information 
selection changes sampling rate schemata representing hyperplanes 
hybrid algorithms incorporate local optimizations typically result greater reliance hill climbing emphasis hyperplane sampling 
result global exploration search space hyperplane sampling gives genetic algorithm global search ability 
despite theoretical objection hybrid genetic algorithms typically optimization tasks reasons 
hybrid genetic algorithm hill climbing multiple points search space 
objective function severely multimodal may strings offsprings basin attraction global solution 
second hybrid strategy impairs hyperplane sampling disrupt entirely 
example learning local optimization improve initial population strings biases initial hyperplane samples interfere subsequent hyperplane sampling 
third general hill climbing may find small number significant improvements may dramatically change offspring 
case effects schemata hyperplane sampling may minimal 
baldwin effect ways evolution learning interact lamarkian nature 
method received new attention baldwin effect 
implementational point view exploitation baldwin effect requires individuals employ interaction learning evolution form learning acquired behavior coded back genetic encoding lamarkian learning 
learned behavior affects objective function words fitness function inherited behavior plus learned behavior 
inherited behavior refers features directly encoded genes 
point view learning changes fitness landscape 
particular genetic encoding precisely phenotypic behavior associated particular genetic encoding significantly improved minor changes form learning corrects inherited behavior changes fitness landscape associated objective function reward solutions close highly fit behavioral solution reached learning 
idea learned behavior influence evolution creating selective pressure genetically encoding genetic learned behavior proposed baldwin years ago 
hinton nolan offer simple example illustrate baldwin effect 
assume fitness landscape flat single spike representing target solution 
assume individuals recognize close solution alter behavior exploit target solution 
pointed hinton nowlan important individuals able recognize improved behavior 
closer individual target solution individual learn target solution 
learning situation builds basin attraction spike fitness landscape transformed difficult optimization problem 
hinton nolan acknowledge example extreme unrealistic serve illustrate principles baldwin effect 
illustrate baldwin effect genetic algorithm simple random learning process develops simple neural network 
genetic encoding specifies neural network topology indicating potential connections connect set artificial neurons 
objective function learning correct network results increased fitness networks inferior fitness 
creates spike correct solution flat fitness landscape 
genetic encoding uses character alphabet specifies existence connection absence connection connection unspecified connection unspecified set randomly set learning 
learning case merely random search possible assignments unspecified connections 
interesting results reported hinton nolan 
genetic search learning fails find solution surprising solution needle haystack 
second final solutions genetic search learning left somewhat underspecified 
words small number unspecified 
characters remain genetic encoding solution close target solution reliably learning 
theory second phenomenon happen selective pressure coupled recombination continue drive underspecified alleles 
underspecification hinton nolan occurred search prematurely terminated 
hinton nolan illustrates baldwin effect provides little insight baldwin effect intentionally exploited accelerate evolutionary learning 
belew offers critique assumptions underlying hinton nolan model 
particular number learning trials allocated individual sufficient occasionally locate spike remains needle haystack 
current context exploit form learning correct specification network grammar tree corresponding particular cellular encoding 
wish test hypothesis learning change value returned fitness function alter genetic code result genetic search quickly finds networks solve parity symmetry problems 
developmental learning way learning current context intermediate lamarkian learning directly learning weights 
refer intermediate form learning developmental learning 
form learning exploits unique characteristic recursive encoding cellular encoding method 
encoding allows recursive moves grammar tree learning network occurs exploited development networks nl values greater results iteration development recursion 
words learning actively take place iteration accomplished relatively minimal cost network small due short development 
learning modifies weights weight experiments 
learned information coded back grammar modified grammar generates modified weights 
modified grammar develop larger nets nl 
may help hypothesis regularity states incbias seq par seq par par seq par incbias wait seq algorithm developmental learning chromosome produced ga development neural net life training weight output unit modified dashed line 
back coding chromosome development neural network modified chromosome 
grammar develops right develop nl right 
hypothesis valid function nl compute regular parity symmetry 
illustrates grammar successfully register weight modification replicate times 
network represented weights set 
example network nl weights set 
method back coding explain section 
way developed nets require computational effort evaluate may important biologist point vue 
collect statistics reason lower cost clear size nl grows recurrent program symbol rec interaction learning evolution size grow case size grow linearly 
evaluation time grows neural network size 
developmental learning defined lamarkian sense learning coded back chromosome developmental purposes subsequent offsprings produced individual carry acquired information encodings 
happening genetic code store learned information lifetime individual development different networks occurs learned information directly passed reproduction 
refer developmental learning 
claiming biological plausibility method assume biological evidence recursive cellular encodings exact way 
species code learned information dna way storing learned information storage local impact reproduction 
pointed developmental learning works works exploiting baldwin effect 
words developmental learning just effective way learning effective way exploiting behaviors acquired lifetime development individual 
resulting behavior passed offspring 
comparative goals goals examine lamarkian learning impact baldwin effect learning developmental learning method 
lamarkian learning developmental learning necessary recoding learned behavior back chromosome 
cellular encoding specifies boolean network learning algorithm result boolean network 
addition learning process inexpensive 
case exploiting baldwin effect analogy suggested hinton nolan suggests fast relatively minimal form learning learning provide information tells close solution 
specifically ideal learning algorithm set experiments outlined pass learning algorithm returns minor changes network potentially lead improve performance 
changes tested 
requirements outlined chose hebbian learning 
means learning somewhat restricted way 
implementation hebbian learning assume boolean neural network hidden layer known layer correct set weights hidden units correct thresholds 
clearly implies problem reduced learning linearly separable function respect hidden units 
perceptron learning point algorithm result boolean net pass learning algorithm 
limit learning single pass look activations hidden nodes output nodes collect information connection inhibitory excitatory particular hidden unit output unit 
training pattern pair units indicate weight positive hidden unit output unit indicate weight negative hidden unit output unit 
consider cases hidden unit cases effect activation output unit feed forward network 
recall weights neural nets boolean sigma 
parity symmetry applications neural nets single output unit 
algorithm learn new weights links feed output units neural net 
training pattern input units 
activation level hidden neuron computed hidden units processed 
desired output value clamped output unit 
link feeding output unit hebbian information computed 
link variable activities units linked incremented decremented 
variable correlational measure activities neurons linked learning variables initialized zero 
note somewhat different perceptron learning collect statistical information training patterns just result errors 
learning similar local learning boltzman machines experiments concerned escaping local minima need simulated annealing component boltzman machine achieve global search weight space 
processing patterns variable determine flip weight 
consider subset links opposite sign recall sigma 
modify weights links subset 
links weight different correlation aim hebbian learning precisely set weights equal correlation 
reason claim hebbian learning 
links jd considered measure badness weight amount change order bring back right sign 
sort links subset quantity jd link biggest jd flipped probability 
second link flipped probability 
general th link flipped probability gamma learning algorithm ensures link flipped time 
requirement link opposite sign 
condition average number flipped link lies gamma 
weight changes process training data 
accept set weight changes fitness improved 
epoch training 
links flipped average 
sufficient create baldwin effect learning build shoulders minima fitness landscape associated raw evaluation function learning 
experiments bias terms threshold values learned genetic search 
experiments show learning simple sufficient flip correct weight exclusive xor network weights hidden units hidden unit computes logical computes 
order get xor set weight unit output unit gamma output units outputs results unit unit activated case outputs 
case xor fact parity problems xor function example limited form hebbian learning exploit fortuitous feature problem 
show fortuitous features need learning strategy effective 
famous done hinton ackley boltzmann machine shown hebbian learning tries optimize mutual information function computed neural net target function 
fitness mutual information ga learning tries optimize thing 
interaction learning evolution operationalizing developmental learning advantages recursive encoding scheme evaluation done incremental fashion 
parity symmetry network evaluated respect ability solve input case 
solution input case recursive iteration development carried input case evaluated 
way evaluation fast early evolutionary process expensive networks proficient handling lower order input cases 
analogously learning neural network iteration recursive development process 
neural net smallest family networks generated chromosome smallest term number units number weights little time expended learning 
neural nets benefit learning process 
encode weight changes occurred learning neural net back chromosome combination appropriate program symbols develop entire family associated networks exploit learning modified chromosome 
development network cell intermediate network graph executes new program symbols added learning set weights accordingly reproducing subnetwork contained neural network learning 
requirement back coding learning chromosome new code allow correctly develop iteration neural network including modifications learning 
simple 
normally cell neuron executes program symbol looses reading head longer rewrite 
order back coding keep reading head 
reading head points node particular subtree program symbols inserted order accomplish change weight 
concrete example shown helps illustrate problem 
consider value link register time cell neuron 
value learning changes weight second input link gamma generate subtree inclr val 
program symbol inclr sets link register program symbol val sets second input link gamma 
node pointed reading head neuron 
node arity previously transformed cell neuron 
order insert new subtree transform leaf node interface node arity 
replace program symbol unary program symbol wait wait insert subtree adds learned weight 
cell executes program symbol positions head subtree waits rewriting step 
sure type simple change grammar tree exactly recode learned behavior 
new encoding produce learned behavior conditions satisfied ffl learning done neural net family 
ffl development modified code cell intermediate neural network starts read inserted subtree cells fan connections feed cell lost reading heads 
ensures new changes affecting interact development cells fan connections feed neural net family feature neuron reads leaf grammar tree 
case higher order neural nets 
condition seq input output output input par seq par seq par incbias wait incbias seq par back coding grammar tree builds code xor network 
illustration net back coding 
illustration grammar tree modified corresponding weight change shown 
modified network computes xor 
met distinct neuron read leaf try concatenate distinct subtree corresponding weight modification leaf 
possible insert distinct subtrees single leaf 
subtrees encode contradictory modifications weights 
second condition satisfied change introduced added subtree remain local 
may produce unexpected side effects 
example assume program symbol incbias grammar tree replaced par corresponding node black split changing threshold fan connections output node 
assume wish change second fanin connection negative weight third connection 
inserted weight change executes parallel division second third fan connection negative 
weight change executes parallel division second weight negative 
order ensure proper sequence development add waiting wait key positions new inserted section tree executes proper sequence 
result increase amount time needed develop higher order neural nets critical likelihood difficult lamarkian learning chromosome quickly big 
means encoding longer quite compact recombination process deeper deeper grammar trees 
note strategy adding long wait delays impact developmental learning expanded chromosomes enters genetic population 
problem adding wait delays ensure development processes terminated effecting learned weight changes 
current experiments settled compromise 
replacing program symbol unary program symbol wait attaching subtree produces weight change add wait delay program symbol hope way sufficiently increase probability second condition verified 
reasons interaction learning evolution consistency approach lamarkian developmental learning strategies 
guarantee encoding learned behavior unexpectedly interact developmental process empirically find cases 
simulation different possible combinations describe result experiments compare different ways adding learning 
basic ga refers genetic algorithm learning 
described encoding chapter 
remind reader encoding encodes architecture weights 
possible run ga learning 
neural net developed perform hebbian learning 
improved fitness trained network obtained life case recursion call baldwin learning 
code learned information back chromosome develop higher order networks developmental learning 
improved chromosomes placed back genetic population allowed compete reproductive opportunities result lamarkian learning 
experiments compare learning modes different tasks increasing complexity 
strategic mutation course experiments noticed difficult problems faced genetic algorithm correctly place recurrent program symbol rec 
case parity problem correct estimation put program symbol rec solve recurrence indicated 
hand incorrect placement prevents genetic algorithm correctly developing networks family networks 
feature causes lack smoothness fitness landscape placing rec program symbol difficult 
order overcome problem strategic form mutation rec program symbol 
mutating rec times program symbols rec program symbols quickly removed general allele rec times represented program symbol arity 
order offset bias rec program symbol arity probability mut mutate rec program symbol mut mutation rate alleles 
combined effect mutations strategies explore large number different placements rec parity set experiments ga run seconds node parallel machine ipsc 
solve problem genetic algorithm find grammar encoding develops family neural networks nl nl computes parity function inputs 
parity function returns sum inputs modulo 
refer table description experiments parameters 
parameter changes seconds 
order produce valid comparison ran trials computed mean standard deviation variables time required find solution total number neural nets evaluations 
order obtain estimation standard deviation result divide standard deviation variable 
report standard deviation variable show high think information gives insight genetic search 
parity problem little special respect developmental learning 
network developed life implements xor 
xor network weights equal learning mode time time sd evaluations evaluation sd success basic ga baldwin developmental lamarkian table initial experiments parity problem ran seconds node parallel machine ipsc say single population 
success measure times solution parity problem 
number trial 
time measured seconds chapter weight links output unit shown 
train network weights set default learning need change weights feed output unit 
line table reports experiments learning learning 
runs fall classes runs quickly find solution take long time 
explains high standard deviation observed experiments 
ga frequently get trapped suboptimal solution 
baldwin learning improve computation time needs neural network evaluations average reach success basic ga needs 
means average evaluation time higher baldwin learning 
average extra time needed compute hebbian algorithm stays millisecond average time required develop evaluate neural network starts milliseconds significantly increases subsequent generations reach milliseconds 
hebbian learning algorithm takes percent time taken develop neural net evaluate 
average time required baldwin learning higher scheme evaluate fitness code family nl networks 
basic ga typically develop neural networks nl early generations 
experiments developing rule reported preceding chapter decides exactly necessary develop neural network max hebbian learning neural net quickly 
example baldwin learning output unit average time taken find neural net seconds 
basic ga takes seconds 
baldwin learning ga starts develop network second generation side effect making evaluation expensive 
lamarkian developmental learning better baldwin basic ga lamarkian better developmental 
order solve problem runs successfully find solutions bigger population parallel ga parallel ga described chapter 
number experiments 
experiment ga ran hours nodes parallel machine ipsc 
population size ranges node 
total hours computation deciding success failure 
results reported terms computations processor table 
processors experiments computation times evaluations multiplied factor 
ga successful runs 
standard deviation smaller population large reliably sample critical subprograms grammar trees 
baldwin learning produces speed basic ga developmental lamarkian learning produces interaction learning evolution learning mode time time sd evaluations evaluation sd success basic ga baldwin developmental lamarkian table second set experiments parity problem ran hours nodes parallel machine ipsc 
learning mode time time sd evaluations evaluation sd basic ga baldwin developmental lamarkian table experiments symmetry problem ran seconds nodes parallel machine ipsc 
better results 
effectiveness developmental lamarkian learning similar 
symmetry problem find encoding develops family neural networks nl nl computes symmetry function inputs 
symmetry binary inputs returns input symmetric middle bit 
function fairly compact cellular encoding difficult learn genetic algorithm 
genetic algorithm runs seconds nodes ipsc 
population size ranged individuals node combined set processors 
table reports experiment learning hebbian learning output unit 
results experiments run finds solution 
indicate computation time taken processor 
multiply find total computation effort 
developmental learning slightly better lamarckian 
addition learning search process produced interesting results 
notion developmental learning introduced 
allowing learning affect subsequent development early learning effectively exploited high cost learning larger networks developed life grammar tree avoided 
current experiments developmental learning provides fairer comparison lamarkian evolution darwinian evolution exploits baldwin effect 
developmental learning allows entire family networks developed cellular encoding exploit learning 
true lamarkian strategy offsprings inherit learned behavior develop subsequent networks just correspond networks developed single iteration grammar tree 
second main finding chapter developmental learning appears competitive lamarkian learning problem finding family boolean neural networks 
especially true symmetry problem 
results parity problem mixed parity problem unusual learning connections fan output unit sufficient find correct set weights 
learning local optimization change fitness landscape lamarkian learning strategies general interest part genetic algorithm community concerned general function optimization 
reported show cellular encoding may unique testbed lamarckian developmental learning 
hebbian learning simple 
forms learning need considered 
chapter describe learning tailored neural networks generated ga cellular encoding 
chapter switch learning chapter proposes learning especially fits networks generated genetic algorithm cellular encoding 
describe features learning 
propose learning algorithm learns sigma weights 
extension learns weights gamma efficient 
report kinds experiments demonstrate proposed learning fits requirements 
kind uses learning fixed neural architecture study convergence speed convergence second kind uses learning combination ga specification learning algorithm proposed learning algorithm designed train networks generated ga cellular encoding 
specific requirements 
ga produces networks integer biases boolean weights 
learning find boolean weight order able learned information cellular encoding 
required developmental learning lamarkian learning seen chapter 
learn bias cellular encoding bias compact 
program symbol incbias need specify link number case want encode weight change 
ga needs effort encode right biases right weights 
neuron sigmoid step function 
neuron computes net input weighted sum neighbor activities 
net input lower equal zero neuron activity set set 
ga generates thousands networks learning needs successful network 
hand learning fast time ga run learning time taken neural net multiplied thousands 
purpose learning refine neural net produced ga change correctly weights order increase performance network 
may find global optimum neural net training close 
networks generated ga cellular encoding particularities 
neurons sparsely connected usually fan 
number layers high standard back propagation fail 
weights weights 
weights set default negative weight values encoded program symbol val 
learning algorithm take advantage particularities 
switch learning algorithm currently processed neuron neuron fans link try modify activity compute new net input switch comp new net input old net input case small old net input rn rn rc wn wn wc case rn rn wc case wn wn wc activity null try flip weight compute new net input switch comp new net input old net input case small old net input rl rl rc wl wl wc case rl rl wc case wl wl wc try flip weight modify activity compute new net input switch comp new net input old net input case small old net input rn rn rc rl rl rc wn wn wc wl wl wc case rn rn wc rl rl wc case wn wn wc wl wl wc learning algorithm 
subscript refers currently processed unit subscript refers neighbor unit subscript refers link 
function comp returns net input change sign returns sign difference absolute value second argument 
function small returns argument returns 
describe learning algorithm epoch patterns learn forwarded network 
pattern passed value variables computed neuron link letter stands right stands wrong 
output unit activity desired output set set set set 
hidden unit variables computed hint correctness computed activity 
high modification activity decrease performance net currently processed pattern 
high modification activity increase performance net currently processed pattern 
similarly link variables computed order hint correctness current weight 
computation variables performed processing neurons backwards starting output units backpropagation 
algorithm sketched shows process neuron 
currently processed neuron 
possible trials modify activity ffl flip incoming weight link switch learning ffl modify activity incoming neighbor ffl time 
learning algorithm successively tries trial analyses effect modification neuron suppose effect desirable 
increases modified activity flipped weight 
suppose effect bad 
increases modified activity flipped weight 
possible effects ffl net input changed sign sign 
activity changes 
wanted activity change strength register fact trial worth strength modified activity add flipped weight add wanted activity change strength want register fact trial bad strength add depending time poorest stability net input trial 
net input possible cases effect counter balanced setting activity 
ffl net input increases absolute value 
wanted activity change effect bad change harder 
add depending ffl net input decreases absolute value 
wanted activity change effect desirable change easier 
satisfied case change completely done adding add ffi parameter lying strictly smaller ffi decrease absolute value net input 
ffi depends weight flip 
involve ffi register fact progress net input correct value better 
trial allows register information relevant weight flips flip change weight incoming link flip upstream incoming neuron change activity hand information computed relevant weight change 
information computed trial precise multiply backpropagated quantities patterns processed corresponding pattern summed 
compute fitness link equal gamma log positive coefficients parametrize learning 
experiments get results set coefficients 
variable refers depth link length minimum path link output unit 
links network lower fitness selected 
links chosen flipping 
link poorest fitness flipped probability higher 
performance neural net flipped weight computed 
performance total number correct outputs divided number patterns divided number output units 
performance increases weight change accepted accepted probability exp gamma delta 
temperature delta decay performance 
epoch trained neural net symmetry parity decoder 
circles represent neurons 
threshold circle empty circle filled black 
continuous line indicates weight dashed line weight gamma decreased order simulated annealing 
change accepted link selected change frozen epochs positive parameters 
due term link near output units frozen longer time 
counter balanced fact flipped due term expression fitness 
algorithm starts modifying weights units near output units better information relevant 
carries modifying weights deep units 
simulation basic switch learning algorithm describe sets experiments try learn respectively symmetry parity decoder 
target function consider cellular encoding ce hand produces family neural networks correct architectures thresholds 
tested switch learning networks family 
networks relevant benchmark switch learning algorithm designed combined ce 
represent fourth neural network correct weight 
modes weight initialization 
mode set weights mode set weights random value sigma 
want show influence parameter set algorithm uses trials computes information relevant single weight flip 
strictly positive algorithm computes information relevant weight flips 
fitness link indicates neural net increase performance flip link flip particular link 
set experiments tried value 
parameters 
initial temperature set divide epoch 
parameter set works experiments considered 
run trials experiments compute average standard deviation sd number epoch needed reach solution 
standard deviation average obtained dividing sd 
want learning fast require success 
allow learning run epochs 
switch learning table report order value mode weight initialization network number number successful trials average number epochs successful trials standard deviation number epochs 
weights random random target symmetry success epoch sd target parity success epoch sd target decoder success epoch sd table experimental results basic switch algorithm set experiments try learn symmetry function 
network nl computes symmetry inputs connections layers weights flipped learning initial weights 
symmetry bits input returns input symmetric middle bit 
function studied parity 
network nl input units layers connections weight flip weights initialized 
parity returns sum inputs modulo 
decoder function nl input unit output units layer gamma connections gamma weights flipped 
input decoder returns output unit number encoded input units set 
result fourth decoder neural network random weight initialization poor probably network weights 
learning flip average weights case 
experiments show starting initial weights help learning 
lot gamma weights neuron activities greater probability 
activities learning useful information 
experiment show setting enhance performance case random weight initialization 
case extra information brought setting precious little information available 
learning algorithm aim refine neural networks produced ga ce 
learning suits particular class neural nets generated ce 
neural nets small number connections 
epoch learning guesses best weight flip flips 
produces sigma weights possible 
guessing difficult redundant connections connections weights flip epochs 
neural nets generated ce great number weights learning works better networks 
lot layers experiment learning cope 
learning fulfils special requirements imposed ga quick need successful 
experiments show success rate achieved average number epoch networks having hidden units 
extension switch learning algorithm previous sections describe learning fits networks generated genetic algorithm cellular encoding 
learning algorithm looks boolean weights sigma 
choosing weights gamma restrictive 
section extend learning choose weights gamma 
setting weight amounts prune link 
possible 
extended switch learning algorithm similar basic 
epoch patterns learned forwarded network 
pattern passed update positive variables neuron positive variables link gamma fact consider value current weight 
letter stands right stands wrong 
output unit activity desired output set set set set 
hidden unit variables computed give hint correctness computed activity 
high activity right modification activity decrease performance net currently processed pattern 
high activity wrong modification activity increase performance net 
similarly link resp 
high setting weight decrease resp 
increase performance net currently processed pattern 
order compute value variables process neurons backwards starting output units back propagation 
algorithm similar algorithm sigma basic switch algorithm 
sketched appendix shows process neuron 
currently processed neuron 
possible trials modify activity change weight link fans trial modify activity neighbor fans trial trial time trial 
neighbor fans link learning algorithm successively tries depending activity trial analyses effect modification neuron effect desirable increases modified activity set weight effect bad increases modified activity set weight kind effects change net input weighted sum neuron activity fans minus threshold 
ffl net input changes sign sign 
activity changes 
wanted activity change strength register fact trial worth strength add depending want activity change strength register fact bad strength add depending ffl net input increases absolute value 
wanted activity change effect bad change harder 
add depending ffl net input decreases absolute value 
wanted activity change effect desirable change easier 
add depending switch learning want activity change effect bad activity stable easily changed 
add depending switch algorithm looks standard back propagation 
difference 
quantities back propagated multiplied coefficient rank relative importance corresponding change weight value activity value 
basic switch learning algorithm uses coefficients 
extended switch learning consider cases 
coefficients produced parameters 
parameter corresponds particular heuristic 
parameter promotes change activity neuron having small stability 
kind strategy 
theoretical experimental foundations 
parameters rank importance effect cases prefer try learn new patterns consolidate old patterns 
heuristic implemented choosing 
input unit activity fixed 
better evaluate effect weight change link fans input unit 
parameter register fact 
trial allows register information relevant weight changes change weight link fans changes weight upstream neuron connected order modify activity 
hand information computed relevant weight change 
information computed trial precise multiply back propagated quantities effect important change net input 
multiply backpropagated quantities ffi ffi absolute value difference net input trial net input trial 
algorithm appendix implemented multiplications additions conditional branching processing link computing coefficients advance possible net input weight activity 
patterns processed corresponding pattern summed 
fitness computed link gamma fv refers current weight 
formula gamma gamma positive coefficients parametrize learning 
fitness measures interesting set weight value variable refers depth link length maximum path link output unit 
fitness link maximum links network higher fitness selected 
links chosen weight change 
highest fitness chosen probability higher 
weight selected link set maximum fitness 
weight change performance neural net modified weight computed 
performance increases accept weight change accept probability exp gamma delta 
temperature delta decrease performance 
epoch decreased order simulated annealing 
weight change accepted weight link selected change frozen epochs positive parameters 
due term expression links near output units frozen longer time 
counter balanced fact links selected due term gammad appears computation fitness simulation extended switch learning algorithm describe sets experiments try learn boolean functions parity symmetry 
target function consider genetic code hand third learned network parity symmetry 
circles represent neurons 
threshold circle empty circle filled black 
continuous line indicates weight dashed line weight gamma dotted line null weight 
produces family neural networks links pruned learning order find solution 
tested switch algorithm neural networks family 
networks developed cellular encoding relevant benchmark learning algorithm designed combined cellular encoding 
represent third network family set correct weights learning fig 
modes weight initialization mode set initial weights mode set weights random value sigma 
symmetry set parameters 
keep parameters parity 
initial temperature set divide epoch 
run trials experiment compute average standard deviation 
problem difficult studied basic switch learning weights possible values weights allow epochs stating success failure 
table rows report order target application mode weight initialization network number number successful trials average number epochs successful trials standard deviation number epochs 
target parity symmetry weights random random success epoch sd set experiments target parity function 
parity binary inputs returns sum inputs modulo 
code hand develops network family input units hidden units connections layers weights modified learning weights initialized 
set experiments try learn symmetry function 
symmetry binary inputs returns input symmetric middle bit 
choose code develops network family features network input units hidden units connections layers 
weights modified learning weights initialized 
switch learning learning mode time evaluations success rate number trials basic ga hebbian learning output unit baldwin developmental lamarkian switch learning hidden layers baldwin developmental lamarkian table parity experiment experiment shows success rate achieved average number epochs networks having hidden units 
basic switch learning 
complexity problem studied important 
start weights initialized 
times weights modify case parity twice case symmetry 
choices new value weights 
case basic switch learning experiments show starting initial weights helps switch algorithm 
simulation genetic algorithm set experiments combine genetic algorithm basic switch learning basic switch learning algorithm searches sigma weights experiments preceding sections indicate extended version weights chosen inf gamma efficient particular target problems want solve 
parallel ga described chapter run nodes ipsc mimd parallel machine 
compared results obtained hebbian learning output unit basic switch learning units 
consider problems solve parity symmetry problem arbitrary large size 
possible combinations learning evolution described preceding chapter investigated 
baldwin learning developmental learning lamarkian learning 
table reports parity experiments 
population size fixed ga runs seconds stating success failure 
time failures considered average time give 
second table reports symmetry experiments 
population ga runs seconds stating success failure 
experiments clearly show switch learning allows get better speed hebbian learning especially symmetry problem 
surprising learning output unit exploits special property parity problem recall network parity problem inputs weights need negative weights feeding output unit 
cellular encoding weights positive default encodings positive weights compact encodings equivalent networks negative weights 
hand symmetry problem learning modify weights hidden units 
switch learning allows learn weights links feeding hidden units hebbian learning 
interest switch learning shows learning mode time evaluations success rate number trials basic ga hebbian learning output unit baldwin developmental lamarkian switch learning hidden layers baldwin developmental lamarkian table symmetry experiment symmetry problem 
problem developmental learning better lamarkian learning switch algorithm designed refine neural networks produced genetic algorithm cellular encoding 
epoch switch algorithm guesses best weight change best value change 
value sigma basic switch algorithm gamma extended case 
learning suits particular class neural nets generated ga neural nets units having small fan number connections small 
great number weights layers 
switch algorithm quick computer time spent learning comparable time spent ga genetic search development neural networks 
kinds experiments measure efficiency switch learning 
measure speed convergence fixed neural net correct architecture biases 
second measure speed brought ga kind experiments basic switch learning shows success rate achieved average number epochs networks having hidden units 
combined ga symmetry problem basic switch learning allows get speed hebbian learning output unit brings speed 
chapter mixed parallel ga state art parallel genetic algorithms genetic algorithm search technique principles population genetics 
shortly describes section 
ga handles population solutions problem 
order build new solution ga selects solutions population mating build offspring mutates offspring 
iterative process selection recombination mutation population solutions evolve better solutions 
process stopped sufficiently solution 
refer book gas 
basis parallel implementation ga multiprocessor system divide population subpopulations allocate subpopulation processor 
processors send best solutions 
communications take place respect spatial structure population 
different models parallel gas investigated 
different spatial structures 
classifies models 
island model population subdivided set randomly distributed islands migration random 
stepping stone model migration takes place neighboring islands 
isolating distance model treats case continuous distributions 
typically model individuals placed grid mate neighborhood 
mixed parallel ga chapter introduce new model called mixed model combines model isolating distance model 
advantages models combined 
ffl local mating allows achieve high degree mates 
degree measure similar mates average 
mates different combination produce unfit offsprings 
high degree favors creation interesting individuals 
ffl isolated islands help maintain genetic diversity 
individuals island relatively isolated individuals island 
sub population island exploring different part input space 
avoids premature convergence allows thorough search input space 
sending individual send candidate selection inserted point received individual selection mates island offspring recombination spatial structure population mixed parallel ga torus grid 
small square represents site dot square represents individual occupying site dot indicates individual best random walk 
grey squares represent site randomly chosen initiate random walk thick squares site individual inserted mixed parallel ga individuals distributed islands 
islands form torus 
island mapped processor mimd machine 
processor send individuals processors store neighbor islands 
inside island individuals arranged grid 
spatial structure torus grids represented fig 
sites grid occupied 
density population kept 
means site occupied probability 
mating done follows site randomly chosen grid 
site successive random walks performed 
best individuals random walks mated 
verification done ensure mates different 
offspring placed site site occupied individual placed randomly chosen neighboring site occupied mixed parallel ga placed randomly chosen neighboring site empty site 
scheme simulates isolating distance model sequentially processor 
tanese introduces migration parameters migration interval number generations consecutive migrations migration rate number individuals selected migration 
approach usually followed 
take original approach requires parameter 
ga steady state 
time new individual created built recombination individuals mean exchange 
exchange chosen probability called exchange rate 
processor exchanges individual site selected border grid random walk starts site best individual sent processor border 
sent coordinate received coordinates read individual placed exactly site opposite exchange preserves topological structure 
super linear speed apply ga find boolean neural net weight sigma computes parity inputs 
encode neural nets cellular encoding 
encoding allows develop family neural networks th network computes regular boolean function size parity inputs 
promotes individual hill climbing 
individual generated ga try enhance hill climbing 
case neural network optimization learning procedure hill climber 
basic switch learning 
learning applied chromosome solution produced ga neural network family 
learned information coded back chromosome develop networks family 
method called developmental learning studied chapter 
mimd parallel machine ipsc processors 
node machine mips roughly sparc 
communications processors asynchronous overlapped computations 
communication cost null 
theoretical prediction confirmed experimental measures 
ga failed find solution seconds restarted 
time find solution includes time spent failure 
optimal size population changes number processors 
study speed considered different population size 
shows optimal population size depends number processors 
authors claim super linear speed parallel ga results biased choose population size favors great number processors 
known performance ga parallel ga depends critically population size 
believe computation speed try optimize population size number processors little interest 
shows super linear speed processors 
possible parallel ga algorithm basic ga speed drops processors communication cost remains null roughly individuals solve target problem mixed pga works sub populations individuals difficult problem greater population size needed correctly sample search space super linear speed greater number processors achieved 
theta theta theta theta theta theta horizontal axis indicates total population size summed processors power 
vertical axis indicates time seconds taken mixed parallel ga find network parity inputs 
average experiments depending initial population 
tics vertical axis indicate best time number processors speed linear speed horizontal axis indicates number nodes vertical shows speed chapter modularity chapter describes incorporate modularity cellular encoding 
show add modularity basic cellular encoding genetic algorithm encompasses enhancements brought previous chapters find part modular code yields architecture sigma weights specifying particular neural network solving decoder boolean function inputs outputs 
result suggests ga exploit modularity order find architectures complex range 
adding modularity cellular encoding section shown program symbol jmp obtain cellular encoding checks property modularity 
tested operator success 
choose way implement modularity 
cellular code extended ordered list labeled trees single tree 
tree encodes grammar called grammar tree 
grammar tree encodes sub neural network included final neural network 
ga automatically define sub neural networks include copies final network 
koza describes ga automatically define functions insert calls functions 
koza calls automatic function definition 
shall call adsn automatic definition sub networks 
grammar tree name convenience single lower case letter axiom grammar tree name 
rewriting ancestor cell positioned root axiom grammar tree 
exists program symbol denoted includes sub network encoded grammar tree 
operator similar program symbol jmp defined section 
difference encode program symbol single letter arguments 
macro operators grammar trees 
notation coherent genetic algorithm 
cell reads program symbol positions reading head root tree name grammar tree includes development enters infinite loop 
amounts program symbol recursion defined section particular case cell executes algorithm moving reading head counts number loops life life life reading head root current tree adsn adsn adsn adsn fin fin main input decoder network 
development sub network starting initial network graph containing subnetwork 
development sub network 
steps development input decoder network step network graph similar initial network graph steps deduced 
graphic representation usual circle filled gray represents threshold 
abbreviated notation program symbols 
par seq wait incbias val val inclr cut sep adsn adsn replaced modularity loose reading head neuron 
variable life register cell 
initialized ancestor cell 
grammar develops family neural networks parametrized number structure neural network 
introduce branching program symbol denoted labels nodes arity 
cell reads executes algorithm life reading head right subtree current node reading head left subtree current node shows modular cellular code development network decoder 
complex refer reader chapter simple example development explanation cellular encoding 
complex encoding purposes 
shows power cellular encoding 
presents complex boolean function decoder modular decomposition needed encode corresponding network family 
allows better understand ga find solution decoder problem 
shows decoder network consists sub networks sequentially connected 
encoded adsn develops input units unit constantly outputs 
includes size 
second encoded adsn 
uses neurons generated sub network adsn 
includes twice size 
decoder network developed 
development done arbitrary large integer code modular definition section 
general cellular encoding modular grammar tree sub network 
new division program symbols 
gradual division denoted child cell inherits input links value link register second child cell inherits input links output links child connects second weight step fig 

separation division denoted sep inherit input links child inherits half output links second child inherits second half step fig 

program symbol sep microcode listed appendix 
experiments modularity try find network family nl nl computes decoder size nl input units output units 
binary code integer clamped input units th output set 
problem complex parity symmetry respect cellular encoding 
preceding section encoding decoder network needs grammar trees parity symmetry networks encoded single grammar tree 
enhancement brought previous chapters solve complex problem developmental learning basic switch learning parallel ga processors ipsc parallel machine 
parallel ga start develop neural networks objective find cellular encoding neural network family computes decoder arbitrary large size terminal set adsn adsn const function set par seq wait val val inclr incbias cut sep fitness cases random patterns bit input neural net nl number fitness cases determined ga hits fitness cases neural net produces right output raw fitness number hits neural net neural net standardized fitness raw fitness divided total number fitness cases 
neural net ranges standardized fitness sum standardized fitness developed neural net 
ranges parameters tm lmax success predicate standardized fitness equals 
networks developed score maximum number hits table tableau genetic search neural network family decoder boolean problem 
time report size population fixed experiments developing rule section 
min varies regularly processors 
processors min directly start develop min start different fitness functions represents different feature problem 
processor develops population view find building block corresponding particular feature final solution recombination exchanged genomes processors 
genetic diversity maintained code develops right 
code develop nl right recurrence learned 
genetic diversity kept general solution 
grammar tree ordered 
mutation generation random genomes implemented respect hierarchy grammar tree genome 
grammar tree include grammar tree prevents looping grammar trees 
fitness neural networks longer information theory simply number hits 
consistent respect switch learning tries maximize number hits 
genome recombined ga encompassed grammar trees named adsn adsn 
axiom grammar tree predefined seq adsn wait wait adsn 
tree indicates general structure network 
sub networks adsn sequentially connected 
development delayed times needs neurons produced 
predefined grammar tree named const val cut encodes sub network consists single unit outputs constant performs identity depending value life register 
table describes features problem generating neural network decoder 
figures shows genome ga corresponding th neural network solves decoder inputs gamma hidden units gamma gamma links 
clearly combination gamma sub networks 
tested nl 
formal proof obvious nl solves decoder inputs arbitrary large ga learns recurrence 
ga quite powerful 
sub population size processors fixed total population size processors 
number generations averaged processors 
total number genomes processed modularity incbias par val inclr wait val wait par val const inclr adsn wait sep val inclr wait val val wait inclr wait incbias val adsn inclr const adsn adsn adsn genome ga 
neural net 

total cpu time taken hour amounts roughly days sun sparc workstation 
chapter precise add modularity cellular encoding ga 
improvement allows encode neural networks modular fashion 
show develop boolean neural network decoder boolean function sub networks 
experiments report ga simultaneously evolve sub networks generate neural network computes decoder boolean function inputs outputs 
ga helped pieces code provided hand 
describes include sub networks searched ga encodes sub network ga building block 
way adding information general interesting 
decoder boolean function belongs complex class compared studied parity symmetry respect cellular encoding 
corresponding architecture regular elaborate 
parity symmetry architecture described layered fan bounded complexity linear 
decoder network links arranged structure trees fan unbounded complexity exponential 
decoder hard design improved ga efficiently preserves genetic diversity includes enhancements described preceding chapter 
neural network ga decoder input units output units 
chapter ga ce genetic programming neural networks thesis studied cellular encoding method encoding families similarly structured boolean neural networks compute scalable boolean functions 
object encoded parallel graph grammar 
grammar encoded set trees called grammar trees set rules 
cellular encoding describe networks elegant compact way 
representation readily recombined genetic algorithm 
various properties cellular encoding formalized proved 
ga search space grammar trees 
ga find cellular encoding yields architecture sigma weights specifying particular neural network solving parity symmetry decoder boolean function furthermore grammar trees recursive encodings generate families networks compute parity symmetry decoder 
way small parity symmetry decoder problems solved grammars typically generate solutions parity symmetry decoder problems arbitrary large size 
koza shown tree data structure efficient encoding effectively recombined genetic operators 
genetic programming defined koza way evolving computer programs ga 
koza genetic programming paradigm individuals population lisp expressions depicted graphically rooted point labeled trees ordered branches 
cellular code exactly structure approach generating cellular codes ga thesis show genetic search space cellular encodings really genetic programming approach genetic representation uses tree data structure 
introduce contribution 
cellular encoding considered low level programming language neural networks exists compiler jannet just automatic neural network translator 
inputs pascal program outputs cellular code neural net computes pascal program step allows put cellular encoding lisp level compare 
concept genetic programming language defined 
lisp cellular encoding distinct genetic programming languages 
criterion classify genetic languages increasing complexities introduced 
criterion lisp turns complex cellular encoding 
propose hierarchy genetic languages starts simple micro programming language increases continuously complexity reach high level language lisp equipped libraries 
address question genetic language better 
show combination genetic programming neural networks cellular encoding totally different encapsulation operator previously proposed koza rice lisp 
advantages cellular encoding put forward 
results koza lisp better task involves manipulation symbols 
hand case task synthesize neural networks 
scheme evolution genetic language briefly sketched 
scheme ideal genetic language appears hierarchy building blocks 
section new paradigm underlying genetic programming neural networks 
hierarchy genetic language section definition genetic language criterion classify genetic languages list genetic languages increasing complexity 
definition genetic programming language set symbols different arities 
genetic program language ordered set trees labeled symbols node number child nodes arity labeling symbol 
syntactic constraints limit range possible genetic programs 
koza distinguished symbols arity calls terminals symbols arity greater calls functions 
distinction misleading function symbol correspond real function 
definition genetic program gives theoretical skeleton 
goal capture useful property programming modularity 
tree represents piece code particular function code called trees number associated cellular encoding calling function amounts inclusion neural network computes calling implemented program symbol called jmp defined section 
program symbol argument 
cell reads program symbol places reading head root grammar tree number argument 
tree number refer trees possible pattern matching process koza chapter spontaneous self replication 
section describes done 
referring system interesting genetic point view general structure genetic program decomposed set trees 
different implementation 
consider special kind grammar defined follows definition gen grammar tree grammar rewrite genetic program written language genetic program written language rule grammar check condition left member symbol arity right member tree labeled symbols special symbols right member contains exactly occurrence special symbol 
rewritten symbols specify insert subtrees node labeled gen grammar simple 
fact describes morphism trees 
rewriting genetic program node just replaced subtree symbol size new program number nodes bounded constant times size old program taken maximum size right members grammatical rules 
chapter compiler called jannet inputs pascal program outputs neural net computes pascal program specifies 
compiler starts computing parse tree pascal program 
parse tree complex usual parse tree compiler includes nodes definition variables 
result major differences parse tree lisp program modulo reduction lisp instruction set 
second step compiler rewrite parse tree cellular encoding gen grammar 
third step compiler develop cellular encoding having chosen initial value life produce neural net computes pascal program specifies provided encapsulated recursive calls done execution 
apart development module separate program software compiler merely rewriting rules gen grammar rewrites parse tree cellular encoding 
propose classification genetic language ordering relation 
definition genetic language higher level exists gen grammar able rewrite genetic program written genetic program written computation 
existence neural compiler jannet proves pascal parse tree cellular encoding 
lisp reduced instruction set lisp cellular encoding pascal parse trees similar lisp expressions include definition variables 
propose genetic languages classify section show program symbols micro coded 
micro coding done gen grammar 
left member rule program symbol name right member microcode encoded tree 
software development jannet defined grammar trees neural building blocks called compiled code 
grammar trees form library considered macros written cellular encoding 
subsection lists cellular codes macros 
genetic language includes cellular encoding plus library called cellular macro encoding 
possible derive cellular encoding macro cellular encoding gen grammar 
left member name macro right member code macro 
uses lisp library resulting language called macro lisp 
possible derive lisp program macro lisp program gen grammar 
left symbol name lisp function library 
right member lisp code function 
results formula macro lisp lisp macro ce ce micro ce 
concrete example illustrating hierarchy 
consider pascal program program var integer write 
variables set zero default program compiled single output neuron bias outputs 
program parse tree program decl type simple write idf lec translation macro cellular encoding done compiler follows 
name variables replaced number encoding rank appearance 
parse tree contain variable number variable names 
implementation technical detail jannet put question hierarchy 
possible modify cellular encoding order keep variable names start seq switch var lr val bloc par seq cuto chop bpn mrg pn bloc write macro cut cuto bloc mrg cuto translation cellular encoding involves replacing call macro cellular code macro produces start seq switch var lr val bloc par seq cuto chop bpn mrg pn bloc label bpn mrg split jmp cut cuto bloc mrg cuto took simple program tree increase lot macro cellular encoding cellular encoding 
translation micro cellular encoding done replacing program symbol microcode produces div exe div top top div aff var cell lnk cel top exe div top div top top top exe exe top cel exe exe exe exe top hom exe top top exe top top observe size final tree bigger necessary number labels diminished 
possible code tree different labels 
cellular encoding versus lisp koza successfully lisp language genetic language broad range problems 
got impressive results mainly problems involve manipulation symbols 
mean mathematical symbols entities high level meaning 
success may due fact lisp language adapted symbolic manipulation 
hand koza rice attempt search space neural nets lisp results impressive 
neural net xor bit adder 
combination genetic programming neural network cellular encoding totally different proposed koza rice 
program simulates neural network neural networks behave program manner shown program compiled neural net 
modulo new kind combination genetic programming neural networks allows combine symbolic search led ga connectionist search obtained learning 
combined genetic search learning called switch learning particular kind combination mode called developmental learning 
natural solution fundamental problem building neural networks process symbols explicit mapping symbols neurons neuron states 
method exploits magic genetic programming builds different sort program explicit mapping target problem algorithm 
interest method incorporate modularity neural networks 
modularity property programming 
considered neural network encoding defined section 
preceding chapter shows cellular encoding property modularity conducts experiment modularity solve difficult problem 
advantage cellular encoding lisp cellular encoding infer recurrence number inputs grows 
example ga find genetic code neural network family computes parity inputs arbitrarily large 
uneasy lisp natural way inputs specified terminals finite number 
book john koza interesting point gp provide method automatic problem decomposition 
set grammar trees grammar tree encodes subnetwork subnetworks includes hierarchical manner koza uses list lisp expression expression encodes function function call hierarchical way 
koza writes variety different problem variety different fields automatic function definition enables genetic programming automatically dynamically decompose problem subproblems automatically dynamically discover solution sub problems automatically dynamically discover way assemble solution subproblems solution problem 
write variety different boolean problems cellular encoding enables genetic programming automatically dynamically decompose problem subproblems automatically dynamically discover sub neural networks solve sub problems automatically dynamically discover way assemble sub neural networks neural networks solves problem 
order automatic function definition koza define run ga number functions number arguments functions allele set functions 
cellular encoding need 
chapter grammar trees automatically defined subnetworks 
gp parameters explicitly passed function number arguments known advance 
ce parameters passed implicitly neurons connecting cell start develop sub neural network included 
need define number arguments subnetwork 
need define particular set alleles grammar trees help ga general section lisp better genetic language symbol manipulations cellular encoding adapted neural network synthesis 
believe neural network synthesis fundamental problem symbol manipulation 
root problem artificial intelligence symbol grounding problem 
hard bridge symbol real world 
gap symbols real world noisy fuzzy large 
genetic search level symbol solve symbol grounding problem 
genetic search cellular encoding allows search lower level complexities shown previous section eventually define structures retina cortical column macro cellular encoding 
structures touch real world directly connected symbols 
concrete example neural network beer gallagher solves leg locomotion problem 
encoded large network bits symmetries 
structure represented symmetries symmetries captured cellular encoding modular code 
final aim breed complex cognitive structures brain problem solving symbol believe better start lower level language lisp 
evolving genetic language cellular encoding method encoding families similarly structured boolean neural networks compute scalable boolean functions 
demonstrate genetic search neural networks cellular encoding equivalent genetic micro programming cellular encoding micro programming language neural networks 
order build theoretical frame allows generalization propose definition genetic programming language criterion classify range complexity 
criterion uses special kind simple tree grammars called gen grammars 
genetic languages classified macro lisp lisp macro cellular encoding cellular encoding micro cellular encoding 
classification shows decomposition possible high level description efficient programming language simple microcode specifies simple local topological modifications graph 
modification duplication movement set contiguous links 
proof languages appropriate genetic search 
argue high level genetic languages lisp adapted symbolic processing low level genetic language cellular encoding neural network synthesis 
fact right genetic language genetic search 
initial genetic programs built simple genetic language micro cellular encoding 
building block piece code average increases fitness structure 
symbol language level piece code level gamma intrinsic meaning may increase fitness average 
mind view symbol language level building block respect language level gamma 
scheme evolution process efficient genetic languages higher complexity acquired stored gen grammar 
rules gen grammar building blocks coded trees chromosome ordered set trees 
scheme needs addressing mechanism mechanism decide increase set trees mechanism initializing component 
mechanism addressing template complementary template searched trees lower order calling tree 
second mechanism increase number trees change environment occurs reflected change average fitness population 
initialization component simply random 
genetic language method hierarchy building blocks 
initial structures built higher level languages micro cellular encoding scheme model evolution modular neural networks neural building block level gamma symbol level successful marriage constantly oppose artificial neural networks classical computer turing machine 
considered model computation different 
classical computer programmed deterministic algorithm 
knows time memory computer needs 
neural network programmed learns 
know advance succeed learn number neurons necessary 
thesis marry approaches ga marriage principles modularity possible generate automatically big neural net solve complex problems giving modular hierarchical structure reflect regularities problem solved 
encoding modular hierarchical defined respect encoding scheme 
code optimization optimization done indirectly space codes 
programming neural nets encoding modular hierarchical structures programming language neural network description 
neural network labeled graph data structure point view 
graph rewriting encoding graph grammar powerful tool able describe graph structure knowledge 
different possible granularities encoding may different possible scaling granularities depending underlying graph grammar 
possible granularities ffl node graph rewritten nodes ffl node graph rewritten nodes ffl list links duplicated moved specification learning classical learning neural networks operates fixed size architecture gradient descent 
classical learning converge global optimum 
considered tool accelerating research done space code 
thesis new paradigms illustrated follows modularity experiments shows possible automatically generate huge neural networks modular solving arbitrary large boolean problems 
encoding formal precise definition modularity 
shown cellular encoding modular 
code optimization experimental results show genetic algorithm efficient optimization procedure 
building block hypothesis ensures success ga interpreted sub neural network encoded sub tree cellular code 
sub tree building block sub neural network interesting features 
sub neural net included bigger neural net fitness neural net average 
programming neural networks properties coding existence neural compiler jannet show cellular encoding kind programming language neural network description 
graph rewriting cellular encoding encodes parallel graph rewriting system 
different granularities macro program symbols introduced neural compiler program symbols cellular encoding micro coding illustrate different possible granularities 
learning defined new learning called switch learning new way combine learning research led genetic algorithm called developmental 
switch learning combined developmental mode allows achieve speed 
marriage programming neural network schemes new light 
examples longer obliged neurons traditional sigmoid neurons product inputs 
genetic algorithm generate type neurons 
learning weights design neural net part neural net learned part compiled directly generated cellular encoding 
specification learning changed converge global optimum aim learning accelerate genetic search 
important parallelize learning neural net 
parallelism natural powerful level genetic search 
genetic algorithm super linear speed 
fitness code costs lot time evaluated 
fitnesses evaluated parallel 
idea optimize neural networks indirectly code deep change paradigm 
certain sooner new principle change way people looking neural networks 
real problem artificial intelligence connectionism scaling problem 
methods successful small scale exponential complexity take computations size problem big 
way solve scaling problem provide algorithm automatically decompose problem subproblems solve subproblem assemble solutions sub problems general solution problem 
solution problem solved algorithm automatically modular hierarchical structure reflects decomposition 
john koza demonstrate number examples genetic programming achieve automatic decomposition recomposition 
koza uses calls automatic function definition 
adf ga automatically decomposes problem sub problems generate functions adf solve sub problems times function calls solution problem 
book john koza real breakthrough 
koza think concept automatic decomposition recomposition applied neural nets 
koza writes conceivably subtask performed neuron useful neural network 
existing paradigms neural networks provide way re set weights discovered part network part network similar subtask performed different set inputs precisely ga combined cellular encoding 
reuse single neuron sub network likewise genetic programming reuse procedure 
variety different problems cellular encoding allowed ga automatically dynamically decompose problem sub problems build sub network solving sub problem discover way assemble multiple copies subnetwork order build network global problem 
experimental results parity symmetry decoder neural net structure exactly reflects problem structure 
huge done order apply method proposed thesis problems boolean functions 
koza shown automatic function definition interesting problem solved amount regularities break point 
neural network application neural net certain amount regularities interesting application cellular encoding 
particular propositions beer gallagher generated ga neural network locomotion leg robot 
symmetries problem order obtain compact representation 
neural net composed copies sub neural network corresponds leg 
connections sub networks shaped identical 
bibliography beer gallagher encoded chromosome single sub network connections highly compact representation problem solved 
cellular encoding automatically exploit symmetries problem provide compact encoding 
part code represents part specifies assemble copies order build network 
cellular encoding ga automatically decompose problem identical sub problems generate solution adapted sub problem assemble copies solution 
problem locomotion legs robot solved automatically knowledge symmetries problem 
application concerns neural nets translation invariant low level visual processing 
sub network repeated times pixels 
represents building block encoded separately procedure 
thesis presents novel ideas need parameters order implemented computer 
admit remember set parameters 
usual question computer science shall go faster studied systematic way 
going faster constant motivation simulation done 
wanted improve performance genetic algorithm order solve problems increasing complexities 
looked optimal learning goal mind 
chapter second part chapter ga takes seconds solve parity problem 
time mentioned publication cellular encoding 
time reduced seconds chapter switch learning combined developmental mode mixed parallel ga nodes ipsc computer 
speed 
convinced research modern parallel computers possible go times faster doing 
race game need 
hour generate train test neural nets having tens neurons decoder experiments 
going faster way obtain robust method automatic generation huge modular neural networks 
bibliography ackley littman 
interactions learning evolution 
nd conf 
artificial life 
addison wesley 
randall beer john gallagher 
evolving dynamical neural networks adaptive behavior 
adaptive behavior 
belew 
individuals populations search adding simple learning genetic algorithm 
schaffer editor th intern 
conf 
genetic algorithms 
morgan kaufmann 
collins jefferson 
selection massively parallel genetic algorithm 
th intern 
conf 
genetic algorithms 
hinton ackley sejnowski 
learning algorithm boltzman machines 
cognitive science 
ehrig korf lowe 
tutorial algebraic approach graph grammars double single pushouts 
lncs 
max garzon stan franklin 
neural computability ii extended 
ijcnn 
gordon 
learning algorithms perceptrons statistical physics 
journal de physique january 
harp samad guha 
genetic synthesis neural networks 
schaffer editor rd intern 
conf 
genetic algorithms pages 
hinton nowlan 
learning guide evolution 
complex systems 
john holland 
adaptation natural artificial systems 
mit press 
john hopcroft jeffrey ullman 
automata theory languages computation 
addison wesley 
baldwin 
new factor evolution 
american naturalist 
kitano 
designing neural network genetic algorithm graph generation system 
complex systems 
koza rice 
genetic generation architecture neural network 
ijcnn international joint conference neural networks pages 
john koza 
genetic programming programming computers mean natural selection 
mit press 
john koza 
genetic programming ii automatic discovery reusable subprograms 
mit press 
appear 
tsu chang lee 
structure level adaptation artificial neural networks 
kluwer academics 
alexander linden christoph 
combining multiple neural network paradigms applications sesame 
international joint conference baltimore 

mathematical models cellular interaction development 
journal theoretical biology 
lucas 
algebraic approach learning syntactic neural networks 
ijcnn 
eric david sharp bradley alpert 
scaling machine learning genetic neural nets 
la ur los alamos national laboratory 

limitations multi layer perceptrons networks steps genetic neural networks 
parallel computing 

darwin continent cycle theory simulation dilemma 
complex system pages 
born 
parallel genetic algorithm function optimizer 
parallel computing 
hans zima peter barbara chapman jan 
automatic parallelization distributed memory systems experiences current research 
european informatics congress systems architecture 
thomas ray 
alive ga richard belew editor th intern 
conf 
genetic algorithms pages 
schaffer whitley combination genetic algorithms neural networks 
ieee computer society press 
eduardo sontag 
neural nets universal computing devices 
university 
tanese 
distributed genetic algorithm 
schaffer editor rd intern 
conf 
genetic algorithms pages 
whitley 
genitor algorithm selection pressure rank allocation reproductive trials best 
schaffer editor rd intern 
conf 
genetic algorithms pages 
whitley 
genetic algorithms neural networks optimizing connection connectivity 
parallel computing 
appendix bibliography reviewed articles ffl adding learning cellular developmental process comparative study 
computation 
pp 
collaboration whitley ffl genetic neural networks contribution au advance genetic programing edited kinnear mit press appear 
ffl genetic programing neural networks theory practice contribution au intelligent hybrid systems edited john appear 
patent patent entitled de en december process translation conception neural networks logical description target problem 
conference published proceedings ffl toroidal systolic array knapsack problem algorithms parallel vlsi architectures ii 
elsevier collaboration ffl modular systolic torus general knapsack problem 
application specific array processors iee computer society press 
collaboration ffl genetic synthesis boolean neural networks cell rewriting developmental process 
combination genetic algorithms neural networks 
ieee computer society press ffl genetic synthesis modular neural networks 
th international conference genetic algorithm 
morgan kaufman publisher ffl grammatical inference genetic search cellular encoding 
grammatical inference theory applications alternatives 
digest ffl learning pruning algorithm genetic neural networks 
european symposium artificial neural network 
ffl mixed parallel genetic algorithm 
parallel computing 
national conference published proceedings un pour combiner approche 
emergence des dans les mod eles de la cognition 
editor bernard amy alain grumbach jean jacques collaboration jean yves gilles 
research report ffl cellular encoding genetic neural network 
research report laboratoire de informatique du parall ecole normale sup erieure de lyon 
ffl adding learning cellular developmental process comparative study 
research report rr laboratoire de informatique du parall ecole normale sup erieure de lyon 
collaboration whitley 
ffl pascal de de neurones rapport de stage de projet de fin ann ee ecole nationale sup erieure informatique de math ematiques appliqu ee de grenoble collaboration jean yves gilles 
conferences proceedings ffl neural compiler graph grammars 
oral presentation schloss dagstuhl 
algebraic syntactical methods computer science ffl genetic algorithm designing neural network 
mod cognition pole alpes de 
ffl equivalence cellular encoding graph grammar 
oral presentation bordeaux 
algebraic syntactical methods computer science ffl parallel ga applied neural network synthesis 
mod cognition paris 
ffl parallel genetic algorithm 
oral presentation journ ees capa grenoble 
conception analyse des algorithmes parall eles ffl participation summer school mathematical methods artificial neural networks kurt bosch institute sion switzerland 
appendix cellular encoding graph grammar chapter cellular encoding discussed relation graph grammars 
cellular encoding specified system rewriting rules 
objects rewritten cells cell node directed graph ordered connections 
rewriting rules interpreted recursively times develop th neural net family compute parity inputs parity target problem 
order unify cellular encoding concept usual graph grammar shown cellular encoding translated graph grammar kind berlin algebraic approach 
process encoding grammar set trees described 
trees interesting data structure genetic algorithm efficiently recombine mutate 
experimental results obtained symmetry parity decoder boolean function second part thesis support claim 
searching cellular encoding ga grammatical inference process 
differs usual grammatical inference approach searched grammar generate language specified positive negative examples 
language specified indirectly 
language neural networks th neural net computes mapping target problem size reasonable size 
approach different syntactic neural network lucas language learning task known positive negative examples 
lucas approach isomorphism grammar syntactic neural network grammar mapped neural network 
syntactic neural networks type architectures cellular encoding impose prior constraint architecture structural constraint complexity constraint 
cellular encoding graph grammar cellular encoding seen system rewriting rules 
rules operate cells 
cell node directed network graph ordered connections 
cell labeled symbol grammar alphabet terminal non terminal 
single terminal labeled terminal cells finished neurons 
cells connections attributes thresholds weights final neural net rewriting rules 
example link register refer possibly fan connections cell 
life attribute bound number recursive interpretations grammar 
rule denoted ff rewrites cell cells lenght string ff 
non terminal symbol label parent cell applied ff string symbols terminal non terminal label child cells created rule refers program symbol computing links attributes child cells 
consider case 
number cells network graph augments 
parent cell said divide child cells 
case attributes child cells inherited parent cell 
rewriting rule modifies attributes parent cell 
cell deleted 
axiom grammar network graph consists single cell called ancestor cell connected input pointer cell output pointer cell 
consider starting network encoding depicted 
starting step attributes ancestor cell initialized default values 
threshold link register life want develop th network 
cell repeatedly divides gives birth cells eventually neural network 
input pointer cell output pointer cell rewritten 
point input output unit network 
example xor neural net input units labeled output unit labeled 
kinds cell division cell modification depending particular compute attributes links child cells 
illustrate example 
grammar seq ab par seq cd par en incbias gamma ng 
program symbol seq par incbias val defined preceding chapter 
sequence cells rewritten determined follows cell rewritten enters fifo queue 
cell rewritten head fifo queue 
cell divides child reads letter right member enters fifo queue 
order execution tries model happen cells rewritten parallel 
ensures invariant invariant cell rewritten twice cell rewritten 
imposing rewriting order ensure grammar produces single network initial life value handling done follows cell manages stack characters record history rewriting 
time rewriting rule seq ab applied cell cell executes steps algorithm 
call symbol left member 
cell checks appears stack 
cell decrements life number empties stack 
second cell pushes stack 
third cell checks life strictly positive 
rule seq ab rule wait cell finished neuron 
encoding grammar set trees section described encode clean grammar set trees 
clean grammar means deterministic grammar symbols reached productions 
cellular encoding graph grammar output output output output input input input input output output output output output output output input input input input input input input step step step step step step step step step step derivation parity network 
box labeled input represents input pointer cell box labeled output represents output pointer cell circles represent cells 
threshold circle empty threshold circle filled black 
continuous line indicates weight dashed line weight gamma 
network graphs show derivation network computes xor 
second sequence derivation starting network inputs parity derived 
directed graph represents grammar built constructive manner 
initially contains node non terminal labeled name corresponding non terminal 
rule ff ff includes non terminal edge drawn node node ff includes terminal new node labeled created edge drawn node node 
final graph nodes labeled occurrences right members rules 
grammar clean possible embed tree root tree labeled axiom contains nodes want decompose graph set trees 
branches pruned 
tree pruned name lower case 
initial tree particular branch named symbol 
pruning branches done processing bread manner 
time node input links links redirected newly created node arity 
possible cases labeling created nodes 
root nodes labeled name branch pruned name added nodes labeled case node replaced arity node labeled fully processed pruning procedure recursively applied pruned branches 
recursive time applied number nodes branches processed strictly diminishes 
process ended obtain set branches trees 
resulting set trees non zero arity node labeled non terminal 
replace non terminal name program symbol rule rewrites 
way final set trees interpreted initial grammar 
lower case interpreted jmp program symbol seen chapter 
cell executes program symbol just places reading head root grammar tree name program symbol rec section equivalent program symbol name tree read cell 
register life cell decremented case rec program symbol prevent looping sub tree call 
reason cell manages stack characters record history rewriting 
letter interpreted program symbol 
shows transform grammar parity network set tree 
grammar seq ab par par bbg translated set trees 
grammar derives layers network input units output units 
input unit connected output units 
need code names non terminal encoding grammar set trees compact trees 
case parity grammar encoding set rules needs bits encoding tree need bits half 
encoding grammar single tree ensures grammar generated clean 
closure property application ga people ga community tends put information structures manipulated ga ensure structures similarly kept clean 
equivalence algebraic approach graph grammar section show cellular encoding grammar translated graph grammar kind proposed ehrig lowe called berlin algebraic approach 
write rewriting graph rule done deleting part del adding new part add resulting new derived graph rule shall applicable additionally contains context kl essentially kept 
rewriting means replace part del kl kr add simplified model purpose 
graph del single node kr kl neighbor nodes representation production pair easy understand 
add time include nodes 
network graph cellular encoding links ordered 
network graph represented ordinary directed graph links ordered indicated 
cell corresponds node represented circle link connecting cell cell corresponds squares oval 
cell resp 
uses square resp 
encode link considered output link resp 
input link 
oval cellular encoding graph grammar jmp jmp rec rec rec rec incbias val par seq par seq seq par par transforming grammar set trees 
grammar parity grammar layers network input units output units 
shows graph tree 
shows results pruning 
presents final set trees labeled names program symbols 
ci si represent network graph directed graph 
link register cell labeled points th input link 
connects squares representing list input links output links cell organized torus 
circle representing cell connected particular square input list 
square corresponds link pointed link register 
fan fan directed graph bounded 
edges fan fan node need ordered case network graph representation 
circles labeled non terminal grammar 
ovals squares need labeled 
edges may labeled arrows indicate node rewritten explained 
production grammar translated corresponding production associated directed graph called sequential division easily translated effect remains local parallel division obvious 
write rule assuming cells inputs outputs 
case linked list squares squares special cases 
write rules special cases similar rules 
rule duplicates circle node representing cell 
rules duplicate squares corresponding links parent cell th rule describes duplicate ovals 
rules show duplicate squares corresponding torus squares neighbors 
neighbor cell done parallel division case oval replicated specified th rule 
oval rewritten knowing cells linked oval duplicate 
edges labeled arrows purpose 
arrows circulate torus squares round tours simulation rewriting step 
rule account middle tour 
tour edges connecting ovals labeled arrows oriented ovals 
rules need case parallel division arrow edge connecting oval set square duplications 
rules sequential division rules 
shown rule rule oval rewritten connections labeled incoming arrows 
ensures effect neighbor division taken account 
rules specify middle second tour 
second circulation arrows ring squares updated ready start simulation new rewriting 
cell apply rewriting rule list squares fans fans updated 
case arrows oriented circle representing cell 
circulation arrows interest 
ensures rewriting order original grammar graph grammar allowing check invariant 
invariant exactly invariant ensures grammars derive network 
invariant cell rewritten twice neighbor cell rewritten 
translate rules modify cell attributes 
difficult increment decrement link register 
just need rotate ring squares square forward square backward 
rule describes increment link register 
rule specifies set weight link pointed link register 
rule shows cut link pointed link register 
rule oval disappear 
rule simplify neighbor linked list squares 
rules handle flow arrows 
perfectly model previous order execution develop cellular encoding 
order match new rules order execution changed sequential division 
new definition cell divides sequential way child reads letter right member enters fifo queue 
comes fact cell modify input links output links 
consider cells linked link assume parallel division link duplicate links numbered 
cut modify weight translate cellular encoding graph grammar 
network graph rewritten graph grammar 
links cut weight modified 
modification brought sequential division order rewriting cell closure order induced directed cellular encoding graph grammar acyclic graph cells cell linked 
translated managing life attribute 
possible fit classical frame rewriting system copying approach proposed system 
systems rewriting rules applied parallel 
apply rewriting rule grammar resulting translation cellular encoding parallel 
write cellular encoding ordered set trees 
assume tree numbered refer trees numbered add number waiting operator number rewriting steps needed go different trees 
derivation value reproduced parallel application rules 
rewriting stopped time tree needed handle life attribute 
cellular encoding method encoding neural network families set named trees 
chapter shows cellular encoding encodes graph grammar 
precisely precise translate cellular encoding set graph grammar rewriting rules kind berlin algebraic approach graph rewriting 
translation done restricted alphabet cellular encoding code need done alphabet listed appendix 
believe involve conceptual difficulties 
equivalence cellular encoding graph grammar brings new light genetic neural network 
genetic search neural networks cellular encoding appears grammatical inference process language implicitly specified explicitly positive negative examples 
results new perspectives method search cellular encoding second part thesis encode grammar set trees evolve sets trees genetic algorithm 
chapter shows method infer kind grammars 
conversely result research grammatical inference field improve cellular encoding 
study cellular encoding grammar helpful classify cellular encoding complexity classes exist rewriting grammars 
cellular encoding context sensitive parallel graph grammar 
par seq bc bc translation rule cell division 
sequential division parallel division managing edge labels arrows 
cellular encoding graph grammar cut translation rules cell modification appendix technical complements small example compilation start output input decl idf lec idf aff program read semi col type simple write execution step macro program symbol program 
ancestor cell gives birth cells 
cell labeled start labeled 
start cell start neural net manage propagation activities 
cell finished development 
waits neighbors finished neurons 
lost reading head 
ancestor cell reads decl node 
output var start input decl idf lec idf aff program read semi col type simple write execution step macro program symbol decl 
ancestor cell gives birth cell labeled var 
cell represents variable 
input output connections ancestor cell complete represent topological invariant 
invariant checked cell cell executes macro program symbol 
input link points input pointer cell 
second input link points start cell rest input links point neurons contains value variables environment 
output link points output pointer cell 
second output link points cell establish connections rest neural net 
decl idf lec idf aff program read semi col type simple write output var start input execution step macro program symbol type simple 
effect set value weight connection neuron represents variable 
decl idf lec idf aff program read semi col type simple write output var start input execution step macro program symbol semi col macro program symbol compose instructions 
cell gives birth cell labeled 
cell goes read macro program symbol corresponds instruction read cell blocked instruction write technical complements var decl idf lec idf aff program read semi col type simple write output var start input execution step macro program symbol read neuron labeled var created 
contains new value variable 
neuron connected input pointer cell 
means value taken outside 
decl idf lec idf aff program read semi col type simple write output start input decl idf lec idf aff program read semi col type simple write output start input var decl idf lec idf aff program read semi col type simple write output start input execution step macro program symbol idf aff 
neuron var contains old value deleted output links 
cell unblocked carries development 
decl idf lec idf aff program read semi col type simple write start output input decl idf lec idf aff program read semi col type simple write output start input var decl idf lec idf aff program read semi col type simple write output start input unblocking step cell 
start compilation write cell executes small piece cellular code 
merges input links cell 
cell disappears 
invariant restored 
cell start compilation instruction 
decl idf lec idf aff program read semi col type simple write start output input decl idf lec idf aff program read semi col type simple write output start input var decl idf lec idf aff program read semi col type simple write output start input execution step macro program symbol write 
cell gives birth cell connected output pointer cell 
cell possesses input connections cell 
cell neuron 
decl idf lec idf aff program read semi col type simple write start output input decl idf lec idf aff program read semi col type simple write output start input var decl idf lec idf aff program read semi col type simple write output start input execution step macro program symbol idf lect 
cell select input corresponds variable connect input output pointer cell 
cell disappears 
decl idf lec idf aff program read semi col type simple write start input decl idf lec idf aff program read semi col type simple write start input var decl idf lec idf aff program read semi col type simple write start input cell unblocks disappears 
deletion leads suppression neuron 
final neural net encompass input neurons start cell input variable output neuron variable 
neural net reads write 
translates specified pascal program 
technical complements macro program symbols section describe macro program symbols illustrated preceding section simple example compilation 
input output var start var inst expr invariant pattern 
show network graph cells declaration scalar variables 
see clearly corresponding neurons labeled var 
neurons var neuron start constitute layer final neural network 
graph invariant pattern taken initial configuration macro program symbols 
avoids repetition stresses importance invariant pattern design macro program symbol 
cell called ancestor cell 
aor output input var start var eand cond inst inst execution macro program symbols ancestor cell invariant pattern 
sake clarity represented combined action 
reading cells layer neurons eand created 
cell develop sub neural net allows compute value condition 
true coded value false coded value 
logical operation realised weight dashed line 
depending value condition flow values contained environment sent left right 
neurons eand switching flow 
reading cell develop respectively body body 
cell possess input sub sites represented small black disks 
blocked piece cellular code called aor 
gather output lines coming sub network subnetwork 
technical complements aor execution cellular code aor cell preceding 
cell input sub sites allows distribute input links lists 
cellular code allows handle lists separately 
produces layer aor neurons 
layer retrieves output environment body output environment body transmits instruction 
int cst input output var start var execution macro program symbol int cst ancestor cell invariant pattern 
macro program symbol parameter value constant 
macro program symbol int cst effect sets bias ancestor cell value deletes links link start neuron link neuron 
sigmoid ancestor cell set identity ancestor cell neuron labeled 
constant needed neuron start activate neuron delivers constant coded bias 
sync var start var input inst output eand aor cond execution macro program symbol ancestor cell invariant pattern 
cells created reading cells aor neurons eand neurons neuron labeled forward environment 
layer aor neurons buffer stores environment iterations loop 
aor dynamic flow values comes top network iteration bottom iterations comes top bottom time 
reading cell develops neural net computes condition 
cell develops body 
case eand neurons switching flow activities 
condition true flow values forwarded body loop flow values turns right exits loop 
cell develops intermediate layer loop synchronize flow activities avoid collisions activities 
sync env env aor aor execution cellular code sync cell preceding 
cell gives birth layer neurons plus neuron dynamic normal dynamic 
layer inserted layer neuron stores output environment body buffer layer aor loop 
neuron connected input site neurons contains values environment 
neuron active environment available 
neuron linked neurons layer output site weights 
connections block neurons active 
neurons layer unblocked time flow values synchronized 
technical complements var attrb param list call input var start var output restore loc glob attrb procedure execution macro program symbol call ancestor cell invariant pattern 
environment variables 
call parameter attrb name called procedure 
cells created 
cell develops different sub networks sub network parameter pass called procedure 
cell develops body called procedure 
reading head placed root parse tree defines called procedure 
stress cell connected neurons contain local variables calling procedure loc 
variables accessed proc 
cell blocked cellular code restore 
restore environment calling procedure translation called procedure finished 
cell keeps connections local variables loc 
var var output output var start var input input var start var loc loc glob glob execution cellular code restore cell preceding 
cell unblocked body proc developed 
cell linked cell linked global variables may modified called procedure 
execution restore brings modified global variables unchanged local variables proc 
output var start var input list param comma expr execution macro program symbol comma ancestor cell invariant pattern 
macro program symbol creates cells 
cell develops sub network computing value parameter cell continues read parameter list 
output var start var input execution macro program symbol param ancestor cell invariant pattern 
assume cell preceding ready develop parameter parameter list reached 
list marked param simply eliminates cell 
procedure inst list pop output var start var input glob execution macro program symbol procedure ancestor cell invariant pattern 
macro program symbol adds intermediate cell blocked pop cellular code 
cell unblocked body loop developed 
role cut connections local variables called procedure 
similarly sub routine returns local variables stack 
cell neuron loc environment 
neuron generated macro program symbols associated declaration proc 
technical complements var var output pop var start var input output input var start var loc loc glob glob execution cellular code pop cell preceding 
macro program symbol allows retrieve part environment global 
local variables longer connected neuron execution pop 
macro program symbol equivalent pop instruction machine language return sub routines 
var param list call attrb input var start var output loc glob function attrb execution macro program symbol call ancestor cell invariant pattern 
macro program symbol effect call create cell necessary restore environment 
var return expr output var start var input loc glob execution macro program symbol return ancestor cell invariant pattern 
macro program symbol null effect 
var input output var start var pn extended invariant pattern environment contains scalar variable array elements 
pn pn representation array neural tree pointer neurons depth 
layers pointer neurons marked pn 
layer neurons marked contains values array 
type attrb type array type array attrb type start start pn execution macro program symbol type array 
macro program symbol allows develop neural tree pointer neurons required encode array 
need macro program symbols order represent array dimensions 
type array parameter number columns dimension corresponding macro program symbol 
type array macro program symbol different cells go read macro program symbol 
technical complements execution cellular code aor cell describes macro program symbol 
result production layer aor neurons duplication neural tree gives structure data 
layer aor neuron retrieves output environment body body transmits instruction 
param comma comma comma eqb eqb lect index aor eand callgen reading array 
index index 
layers sigmoid test equality zero 
output net input output 
eqb neurons different thresholds indicated 
third layer neurons computes logical ands 
layer single neuron outputs corresponds element read 
neurons layer output 
fourth layer neurons layer eand neurons 
neuron activated 
propagates element array read 
neuron aor gathers output lines retrieves element read 
instruction callgen call links global variables deleted 
callgen described section param comma comma comma comma affect index callgen eqb eqb eand aor pn pn writing array 
index index 
layers preceding 
fourth layer contains twice eand neurons 
neurons eand distributed pairs 
pair corresponds element array 
pair neuron activated activated 
left neuron activated single case element corresponds place want write 
layer layer aor neurons 
aor neuron corresponds element modified array 
retrieves element old array value written element corresponds place writing 
neural tree pointer neurons preserves structure array 
technical complements var input start var output call gen loc glob function attrb param list attrb execution macro program symbol callgen ancestor cell invariant pattern 
callgen effect call cell number execute body function written cellular code connections global variables pascal program 
var var pn output start var input inst inst attrb execution macro program symbol ancestor cell invariant pattern 
cells created 
cell goes read macro program symbol idf lec select array variable concerned test 
cell neuron cell restore environment test finished 
cell execute part condition encoded macro program symbol registers cell name role reading head reads particular position cellular code life counts number recursive iterations bias stores bias neuron stores sigmoid dyn stores dynamic nu lien points particular link specifies level dx dy specifies cell neuron reading cell table registers cell links registers weight weight state state mark sub site sub site 
sites single register called 
technical complements input var start output var var pn attrb inst inst var var pn output start var input inst inst attrb execution macro operator cell number preceding 
execution macro program symbol idf lec cell unblocked merges links input neighbor 
situation described top 
macro program symbol tests number input links neighbor 
number array elements 
number greater cell places reading head right subtree case 
goes read left subtree 
left sub tree contains body right sub tree contains body 
syntax micro coding cellular operators grammar generates microcode operators microcode 
top 
div 
reg 
exe 
aff 
top 
top segments 
top segments 
div 
div 
div 
div 
div segments 
div segments 
segments 
segment segments 
segment 
segment 
operator operand 
operator operand 
operator 
operand 
div 
hom number 
reg 
cell character 
sit character 
lnk character 
exe 
exe character 
bra character 
aff 
aff character 
non terminal number 
rewritten number non terminal character 
rewritten character 
non terminal cel sit lnk abbreviated name cell register site link exe reminds kind movements reading head 
bra specifies kind condition tested aff indicated kind neuron got 
list program symbols microcode letter indicates program symbol uses integer argument 
number input links number output links value link register 
local topological transformation 
cuts input links 
cuts right input links starting link cut cuts input link clip cuts input link cuto cuts output link mrg merges input link input neighbour mg merges input link input copies output output cell cuts input links link cuts output links link 
cuts input links cuts input links puts link position puts link position switch link permutation link link kill top deletes cyc tops creates recurrent link cell division execute separate code par parallel division seq sequential division gradual division sep 
separation division adl sm 
adds cell link technical complements ad sm 
adds cell link sd duplicates link add cell link duplicates inputs duplicates inputs duplicates input starting link duplicates input starting link cell division execute code clone olc clones argument split hom splits computed topology tab hom split child bias set hom split sub sites merged modification register sets bias argument sets bias sets bias incbias cel increments bias cel decrements bias sets type sigmoid sets sets level lr cell sets link register inclr cel increments link register cel decrements link register site sets sub sites argument sets input sub sites argument sets output sub sites argument merges input sites output sites merges input sites merges output sites creates input site input link val sets value input weight sets value output weight val sets value input weight val sets value input weight lnk increments value weight input link dec lnk decrements value weight input link mult lnk doubles value weight input link div lnk halves value weight input link rand lnk 
sets weight random value sets state link sets state link managing execution order wait waits steps wait waits step arguments supplied jmp includes subnetwork jmp uses pattern matching mechanism arguments supplied rec moves reading head back root currently read subtree finished neuron bloc waits loose reading head nop label nop label def starts new branch encodes subnetwork tests value life register tests value life register initial value bun tests input links bpn tests neighbour pointer neuron blr tests value link register equals argument enhancing display graph pn draws pointer neuron var draws neuron contains value variable operators labeling indicates insertion right sub tree indicates insertion left sub tree pn pointer neuron var contains initial value variable eand dynamic eand aor dynamic aor inf compares inputs sup compares inputs compares inputs compares inputs plus adds inputs input second mult multiplies inputs quo divides input second inputs logical logical logical neq tests inputs different eq tests inputs equal eqb tests input equal bias start start neuron syntax parse trees 
grammar generates parenthesized expressions interpreted correct parse trees 
parse tree parse tree main program parse tree procedure parse tree function 
compiler generates list trees tree main program tree function procedure 
parenthesized representation bit unusual 
put parenthesis single subtree put blanc 
sub trees consider right sub tree trunc put left subtree parenthesis 
special representation allows write simple grammar 
capital letters parenthesis terminals grammar 
non terminals written small letters brackets 

program 
procedure 
function 
program 
program 

procedure 
procedure 

function 
function 


decl attrb type 
decl attrb type 
type 
attrb type 

semicolon inst 
inst 

semicolon inst 

return expr 
inst 
write 
read 
assign 



repeat 

callgen 
assign 
assign 
expr 
read 
read 

attrb write 
write expr 
technical complements 
expr 









expr 

repeat 
repeat expr 


attrb 

coma expr 
callgen 
callgen attrb 
expr 



const 
binop 
unop 


callgen attrb 

callgen attrb 

attrb const 
attrb binop 
binop attrb expr expr 
binop 
unop attrb expr 

attrb 
grammar generates parse trees nodes having attributes 
indicates attribute represents 
name function attribute decl name declared variable type array dimension array idf aff name variable assign idf lec name variable read int cst value integer constant call name procedure call call name function call call gen attrb name pre encoded solution call gen attrb name procedure reading array call gen attrb name procedure writing array binop name binary operator unop name unary table attributes parse tree elementary cellular codes times 
defined separately order reduce size cellular code generated neural compiler 
arithmetic operators 
enumerate elementary codes implement arithmetic operators neurons aor eand dynamic 
sigmoids increasing order identity pi units stair step stair step equality zero div unit stair step stair step neg neg lr val lr val stair step lr val stair step inf inf lr val stair step inf sup sup lr val stair step sup eq eq lr val equality zero eq neq neq seq jmp eq jmp neg plus plus identity plus lr val identity mult pi unit mult quo div unit quo eor aor aor eand eand eand reading writing array cellular code function allow read write multi array 
function needs elementary piece code call 
affect index site cuto seq adl site jmp creat eqb mrg split jmp affect suit site bloc pn creat eqb tab val eqb affect suit bun jmp affect suit seq site adl jmp split jmp mrg split jmp affect suit site bloc pn affect suit bpn site jmp affect suit mrg mrg seq split jmp site bloc pn affect suit seq par cut lr val jmp eand cut jmp eand jmp aor index lec site cuto seq adl site jmp creat eqb mrg split jmp index suit bloc jmp index suit index suit bun jmp super eand site adl jmp creat eqb split jmp mrg split jmp index suit index suit split bpn jmp aor seq lr jmp super mrg site bloc pn recursive macro program symbol enumerate elementary codes places cellular code generated compiler 
code apply recursively operation environment 
macro super mrg blr mg jmp super mrg jmp index suit macro super eand super eand split bpn jmp eand mrg seq jmp super eand site bloc pn technical complements macro super aor super aor seq mrg mrg split bpn jmp aor jmp site bloc pn macro super dyn super dyn split bpn seq mrg jmp site bloc pn macro synchro synchro val site bloc macro split bpn jmp eand mrg jmp macro split bpn mrg jmp macro read suit read suit bpn cut lr val seq mrg split jmp site bloc pn macro write suit write suit bpn mrg split jmp write suit rules rewriting nodes parse tree rules non terminal grammar 
site site seq site cut par wait site jmp super eand lr val site jmp super eand cut lins bloc cuto jmp super aor par cut bloc cut bloc lins repeat site cyc site site jmp super dyn bloc bloc mrg site site lins par cuto cuto lr val seq wait jmp jmp synchro seq jmp super eand site bloc site cyc site site jmp super dyn bloc site site lins par cuto cut jmp super eand site cut bloc seq bloc site cuto seq mrg wait jmp jmp synchro lr val seq cut jmp super eand site bloc program start seq switch lins bloc mrg cuto inst cuto cut decl bloc lins type array seq clone attr lins bloc pn type simple var lr val lins bloc mrg assign bloc lins idf aff switch attr cut cuto idf lec cuto chop attr bpn mrg pn comma par lins unop seq lins cuto jmp attr binop seq par lins cuto jmp attr int cst chop cuto attr lr val call global bloc jmp attr cuto chop lr val param kill function ou return lins call local global bloc jmp attr bloc mrg cuto procedure seq lins bloc mrg global cuto read chop attr mrg site jmp read suit bloc write par seq bloc jmp write suit cut cuto seq par cuto bloc mrg lins bun attr chop mrg chop mrg lins callgen bloc jmp attr cuto chop lr val library functions handling arrays 
cuto mrg pn cuto mrg pn concat cuto mrg mrg pn int array cuto pn array int cuto mrg bpn mrg pn cuto jmp rand rand bpn lr val rand seq mrg split jmp rand pn trained neural networks cellular code correspond neural networks included pascal program animal cuto seq mrg split seq clo bloc pn position object cuto seq seq mrg clo clo predator cuto seq seq mrg clo clo motor cuto seq seq clo seq clo pn extended switch learning neuron currently processed oni old net input neuron fans link weight null try modify activity new net input abs oni switch comp oni case small oni rn rc rn rc wn wn wc case rn wc case wn wc rn rc input unit activity null equal initial weight technical complements try set weight new net input abs oni switch comp oni case small oni rl rc rl rc wl wc case rl wc case wl wc rl rc elseif input unit equal initial weight try set weight activity new net input abs oni switch comp oni case small oni rn rc rl rc rn rc rl rc wn wc wl wc case rn wc rl wc case wn wc wl wc rn rc rl rc algorithm describes computes value gamma starting output unit going backwards input units backpropagation 
subscript refers unit currently processed 
subscript refers link 
values select weight modify new value modify 
function comp returns net input change sign returns sign difference absolute value second argument 
function small returns argument returns operator increments variable left side quantity right side 
positive integers parametrize learning algorithm 
