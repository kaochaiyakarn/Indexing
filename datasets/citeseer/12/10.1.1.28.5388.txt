decision tree bigrams accurate predictor word sense ted pedersen department computer science university minnesota mn usa umn edu presents corpus approach word sense disambiguation decision tree assigns sense ambiguous word bigrams occur nearby 
approach evaluated sense tagged corpora senseval word sense disambiguation exercise 
accurate average results reported words accurate best results words 
word sense disambiguation process selecting appropriate meaning word context occurs 
purposes assumed set possible meanings sense inventory determined 
example suppose bill set possible meanings piece currency pending legislation bird jaw 
context senate bill consideration human reader immediately understands bill legislative sense 
computer program attempting perform task faces dicult problem bene innate common sense linguistic knowledge 
attempting provide computer programs real world knowledge comparable humans natural language processing turned corpus methods 
approaches techniques statistics machine learning induce models language usage large samples text 
models trained perform particular tasks usually supervised learning 
describes approach decision tree learned number sentences instance ambiguous word manually annotated sense tag denotes appropriate sense context 
prior learning sense tagged corpus converted regular form suitable automatic processing 
sense tagged occurrence ambiguous word converted feature vector feature represents property surrounding text considered relevant disambiguation process 
exibility complexity human language potentially nite set features utilized 
corpus approaches features usually consist information readily identi ed text relying extensive external knowledge sources 
typically include part speech surrounding words presence certain key words window context various syntactic properties sentence ambiguous word 
approach relies feature set bigrams word sequences occur text 
context ambiguous word occurs represented number binary features indicate particular bigram occurred approximately words left right word disambiguated 
take approach surface lexical features bigrams collocations occurrences contribute great deal disambiguation accuracy 
clear disambiguation accuracy improved features identi ed complex pre processing part speech tagging parsing anaphora resolution 
objectives establish clear upper bounds accuracy disambiguation feature sets impose substantial pre processing requirements 
continues discussion methods identifying bigrams included feature set learning 
decision tree learning algorithm described benchmark learning algorithms included purposes comparison 
experimental data discussed empirical results 
close analysis ndings discussion related 
building feature set bigrams developed approach word sense disambiguation represents text entirely terms occurrence bigrams de ne cat cat totals big big totals representation bigram counts consecutive words occur text 
distributional characteristics bigrams fairly consistent corpora majority occur time 
sparse skewed nature data statistical methods select interesting bigrams carefully chosen 
explore alternatives power divergence family goodness statistics dice coecient information theoretic measure related pointwise mutual information 
summarizes notation word bigram counts way contingency table 
value shows times bigram big cat occurs corpus 
value shows bigrams occur big rst word cat second 
counts indicate words big cat occur rst second words bigram corpus 
total number bigrams corpus represented power divergence family cressie read introduce power divergence family goodness statistics 
number known statistics belong family including likelihood ratio statistic pearson statistic 
measure divergence observed ij expected ij bigram counts ij estimated assumption component words bigram occur strictly chance ij value calculated ij log ij ij ij ij ij dunning argues favor especially dealing sparse skewed data distributions 
cressie read suggest cases pearson statistic reliable likelihood ratio test preferred 
light pedersen presents fisher exact test alternative rely distributional assumptions underly pearson test likelihood ratio 
unfortunately usually clear test appropriate particular sample data 
take approach observation tests assign approximately measure statistical signi cance bigram counts contingency table violate distributional assumptions underly goodness statistics 
perform tests fisher exact test bigram 
resulting measures statistical signi cance di er distribution bigram counts causing tests unreliable 
occurs rely value fisher exact test fewer assumptions underlying distribution data 
experiments identi ed top ranked bigrams occur times training corpus associated word 
cases rankings produced fisher exact test disagreed altogether surprising low frequency bigrams excluded 
statistics produced rankings distinction simply refer generically power divergence statistic 
dice coecient dice coecient descriptive statistic provides measure association words corpus 
similar pointwise mutual information widely measure rst introduced identifying lexical relationships church hanks 
pointwise mutual information de ned follows mi log represent words bigram 
pointwise mutual information quanti es words occur bigram numerator relative occur corpus denominator 
curious limitation pointwise mutual information 
bigram occurs times corpus component words occur part bigram result increasingly strong measures association value decreases 
maximum pointwise mutual information corpus assigned bigrams occur time component words occur outside bigram 
usually bigrams prove useful disambiguation dominate ranked list determined pointwise mutual information 
dice coecient overcomes limitation de ned follows dice value dice values value marginal totals typical case rankings produced dice coe cient similar mutual information 
relationship pointwise mutual information dice coecient discussed smadja 
developed bigram statistics package produce ranked lists bigrams range tests 
software written perl freely available www umn edu 
learning decision trees decision trees widely machine learning algorithms 
perform general speci search feature space adding informative features tree structure search proceeds 
objective select minimal set features eciently partitions feature space classes observations assemble tree 
case observations manually sense tagged examples ambiguous word context partitions correspond di erent possible senses 
feature selected search process represented node learned decision tree 
node represents choice point number di erent possible values feature 
learning continues training examples accounted decision tree 
general tree overly speci training data generalize new examples 
learning followed pruning step nodes eliminated reorganized produce tree generalize new circumstances 
test instances disambiguated nding path learned decision tree root leaf node corresponds observed features 
instance ambiguous word disambiguated passing series tests test asks particular bigram occurs available window context 
include benchmark learning algorithms study majority classi er decision stump naive bayesian classi er 
majority classi er assigns common sense training data instance test data 
decision stump node decision tree holte created stopping decision tree learner single informative feature added tree 
naive bayesian classi er duda hart certain blanket assumptions interactions features corpus 
search feature space performed build representative model case decision trees 
features included classi er assumed relevant task hand 
assumption feature conditionally independent features sense ambiguous word 
bag words feature set word training sample represented binary feature indicates occurs window context surrounding ambiguous word 
weka witten frank implementations decision tree learner known decision stump naive bayesian classi er 
weka written java freely available www cs waikato ac nz ml 
experimental data empirical study utilizes training test data senseval evaluation word sense disambiguation systems 
teams participated supervised learning portion event 
additional details exercise including data results referred senseval web site www itri ac uk events senseval palmer 
included tasks senseval training test data provided 
task requires occurrences particular word test data disambiguated model learned sense tagged instances training data 
words multiple tasks di erent parts speech 
example tasks associated bet noun verb 
tasks involving disambiguation di erent words 
words part speech associated task shown table column 
note parts speech encoded noun adjective verb words part speech provided 
number test training instances task shown columns 
instance consists sentence ambiguous word occurs surrounding sentences 
general total context available ambiguous word surrounding words 
number distinct senses test data task shown column 
experimental method process repeated task 
capitalization punctuation removed training test data 
feature sets selected training data top ranked bigrams power divergence statistic dice coecient 
bigram occurred times included feature 
step lters large number possible bigrams allows decision tree learner focus small number candidate bigrams helpful disambiguation process 
training test data converted feature vectors feature represents occurrence bigrams belong feature set 
representation training data actual input learning algorithms 
decision tree decision stump learning performed twice feature set determined power divergence statistic feature set identi ed dice coecient 
majority classi er simply determines frequent sense training data assigns instances test data 
naive bayesian classi er feature set word occurs times training data included feature 
learned models disambiguate test data 
test data kept separate stage 
employ ne grained scoring method word counted correctly disambiguated assigned sense tag exactly matches true sense tag 
partial credit assigned near misses 
experimental results accuracy attained learning algorithms shown table 
column reports accuracy majority classi er columns show best average accuracy reported participating senseval teams 
evaluation senseval precision recall converted scores accuracy product 
best precision recall may come di erent teams best accuracy shown column may higher single participating senseval system 
average accuracy column product average precision recall reported participating senseval teams 
column shows accuracy decision tree learning algorithm features identi ed power divergence statistic 
column shows accuracy decision tree dice coecient selects features 
columns show accuracy decision stump power divergence statistic dice coecient respectively 
column shows accuracy naive bayesian classi er bag words feature set 
accurate method decision tree feature set determined power divergence statistic 
line table shows win tie loss score decision tree power divergence method relative method 
win shows accurate method column loss means accurate tie means equally accurate 
decision tree power divergence method accurate best reported senseval results tasks accurate tasks compared average reported accuracy 
decision stumps fared proving accurate best senseval results tasks 
general feature sets selected power divergence statistic result accurate decision trees selected dice coecient 
power divergence tests prove reliable account possible events surrounding words occur bigram occurs bigram bigram consists 
dice coecient strictly event occur bigram 
tasks decision tree power divergence approach accurate senseval average promise scrap shirt bitter sanction 
dramatic difference occurred amaze senseval average decision tree accuracy 
unusual task instance test data belonged single sense minority sense training data 
analysis experimental results characteristics decision trees decision stumps learned word shown table 
column shows word part speech 
columns feature set selected power divergence statistic columns dice coe cient 
columns show node selected serve decision stump 
columns show number leaf nodes learned decision tree relative number total nodes 
columns show number bigram features selected table experimental results senses stump stump naive word pos test test train maj best avg pow pow dice dice bayes accident behaviour bet excess oat giant knee onion promise sack scrap shirt amaze bet bother bury calculate consume derive oat invade promise sack scrap seize brilliant oating generous giant modest slight wooden band bitter sanction shake win tie loss pow vs represent training data 
table shows little di erence decision stump nodes selected feature sets determined power divergence statistics versus dice coecient 
expected top ranked bigrams measure consistent decision stump node generally chosen 
di erences feature sets selected power divergence statistics dice coecient 
re ected different sized trees learned feature sets 
number leaf nodes total number nodes learned tree shown columns 
number internal nodes simply di erence total nodes leaf nodes 
leaf node represents path decision tree sense distinction 
bigram feature appear decision tree number inter table decision tree stump characteristics power divergence dice coecient word pos stump node leaf total features stump node leaf total features accident accident accident behaviour best behaviour best behaviour bet betting shop betting shop excess excess excess oat oat oat giant knee knee injury knee injury onion promise promise promising sack sack sack scrap scrap scrap shirt shirt shirt amaze bet bet bet bother bothered bothered bury buried buried calculate calculated calculated consume derive derived derived oat invade invade invade promise promise promise sack return return scrap seize seize seize brilliant brilliant brilliant oating generous generous generous giant giant giant modest modest modest slight slightest slightest wooden wooden spoon wooden spoon band band band bitter bitter bitter sanction south africa south africa shake head head nal nodes represents number bigram features selected decision tree learner 
original hypotheses accurate decision trees bigrams include relatively small number features 
motivated success decision stumps performing disambiguation single bigram feature 
experiments decision trees bigram features identi ed ltering step words decision tree learner went eliminate candidate features 
seen comparing number internal nodes number candidate features shown columns 
noteworthy bigrams ultimately selected decision tree learner inclusion tree include bigrams ranked highly power divergence statistic dice coecient 
expected selection bigrams raw text mea words top ranked bigrams form set candidate features decision tree learner 
ties top rankings may features fewer bigrams occurred times bigrams included feature set 
association words decision tree seeks bigrams partition instances ambiguous word distinct senses 
particular decision tree learner decisions bigram include nodes tree gain ratio measure mutual information bigram particular word sense 
note smallest decision trees functionally equivalent benchmark methods 
decision tree leaf node internal nodes acts majority classi er 
decision tree leaf nodes internal node structure decision stump 
discussion long term objectives identify core set features useful disambiguating wide class words supervised unsupervised methodologies 
ensemble approach word sense disambiguation pedersen multiple naive bayesian classi ers occurrence features varying sized windows context shown perform widely studied nouns interest line 
accuracy approach previously published results learned models complex dicult interpret ect acting accurate black boxes 
experience variations learning algorithms far signi cant contributors disambiguation accuracy variations feature set 
words informative feature set result accurate disambiguation wide range learning algorithms learning algorithm perform uninformative misleading set features 
focus developing discovering feature sets distinctions word senses 
learning algorithms produce accurate models shed new light relationships features allow continue re ning understanding feature sets 
believe decision trees meet criteria 
wide range implementations available known robust accurate range domains 
important structure easy interpret may provide insights relationships exist features general rules disambiguation 
related bigrams features word sense disambiguation particularly form collocations ambiguous word component bigram bruce wiebe ng lee yarowsky 
bigrams identify collocations include word disambiguated requirement case 
decision trees supervised learning approaches word sense disambiguation fared number comparative studies mooney pedersen bruce 
bag word feature sets mixed feature set included part speech neighboring words collocations morphology ambiguous word 
believe approach rst time decision trees strictly bigram features employed 
decision list closely related approach applied word sense disambiguation yarowsky wilks stevenson yarowsky 
building traversing tree perform disambiguation list employed 
general case decision list may suffer fragmentation learning decision trees practical matter means decision list trained 
believe fragmentation re ects feature set learning 
consists approximately binary features 
results relatively small feature space su er fragmentation larger spaces 
number immediate extensions 
rst ease requirement bigrams consecutive words 
search bigrams component words may separated words text 
second eliminate ltering step candidate bigrams selected power divergence statistic 
decision tree learner consider possible bigrams 
despite increasing danger fragmentation interesting issue bigrams judged informative decision tree learner ranked highly ltering step 
particular determine ltering process eliminates bigrams signi cant sources disambiguation information 
longer term hope adapt approach unsupervised learning disambiguation performed bene sense tagged text 
optimistic viable bigram features easy identify raw text 
shows combination simple feature set bigrams standard decision tree learning algorithm results accurate word sense disambiguation 
results approach compared senseval word sense disambiguation exercise show bigram decision tree approach accurate best senseval results words 
acknowledgments bigram statistics package implemented banerjee supported aid research scholarship oce vice president research dean graduate school university minnesota 
senseval organizers making data results event freely available 
comments anonymous reviewers helpful preparing nal version 
preliminary version appears pedersen 
bruce wiebe 

word sense disambiguation decomposable models 
proceedings nd annual meeting association computational linguistics pages 
church hanks 

word association norms mutual information lexicography 
proceedings th annual meeting association computational linguistics pages 
cressie read 

multinomial goodness tests 
journal royal statistics society series 
duda hart 

pattern classi cation scene analysis 
wiley new york ny 
dunning 

accurate methods statistics surprise coincidence 
computational linguistics 
holte 

simple classi cation rules perform commonly datasets 
machine learning 
palmer 

special issue senseval evaluating word sense disambiguation programs 
computers humanities 
mooney 

comparative experiments disambiguating word senses illustration role bias machine learning 
proceedings conference empirical methods natural language processing pages may ng lee 

integrating multiple knowledge sources disambiguate word sense exemplar approach 
proceedings th annual meeting association computational linguistics pages 
pedersen bruce 

new supervised learning algorithm word sense disambiguation 
proceedings fourteenth national conference arti cial intelligence pages providence ri july 
pedersen 

fishing exactness 
proceedings south central sas user group conference pages austin tx october 
pedersen 

simple approach building ensembles naive bayesian classi ers word sense disambiguation 
proceedings annual meeting north american chapter association computational linguistics pages seattle wa may pedersen 

lexical semantic ambiguity resolution bigram decision trees 
proceedings second international conference intelligent text processing computational linguistics pages mexico city february 
smadja mckeown hatzivassiloglou 

translating collocations bilingual lexicons statistical approach 
computational linguistics 
wilks stevenson 

word sense disambiguation optimised combinations knowledge sources 
proceedings coling acl 
witten frank 

data mining practical machine learning tools techniques java implementations 
morgan kaufmann san francisco ca 
yarowsky 

decision lists lexical resolution application accent spanish french 
proceedings nd annual meeting association computational linguistics 
yarowsky 

unsupervised word sense disambiguation rivaling supervised methods 
proceedings rd annual meeting association computational linguistics pages cambridge ma 
yarowsky 

hierarchical decision lists word sense disambiguation 
computers humanities 
