greedy function approximation gradient boosting machine jerome friedman ims lecture february modi ed march function approximation viewed perspective numerical optimization function space parameter space 
connection stagewise additive expansions steepest descent minimization 
general gradient descent boosting paradigm developed additive expansions tting criterion 
speci algorithms squares absolute deviation huber loss functions regression multi class logistic likelihood classi cation 
special enhancements derived particular case individual additive components regression trees tools interpreting treeboost models 
gradient boosting regression trees produces competitive highly robust interpretable procedures regression classi cation especially appropriate mining clean data 
connections approach boosting methods freund shapire friedman hastie tibshirani discussed 
function estimation function estimation predictive learning problem system consisting random output response variable set random input explanatory variables fx 
xn training sample fy known values goal obtain estimate function mapping minimizes expected value speci ed loss function joint distribution values arg min arg min ex frequently employed loss functions include squared error absolute error jy regression negative binomial log likelihood log yf classi cation 
common procedure restrict member parameterized class functions fp 
nite set parameters joint values identify individual class members 
focus additive expansions form am mh am generic function usually simple parameterized function input variables characterized parameters fa 
individual terms di er joint sequoia hall stanford university stanford ca stat stanford edu values am chosen parameters 
expansions heart function approximation methods neural networks rumelhart hinton williams radial basis functions powell mars friedman wavelets donoho support vector machines vapnik 
special interest case functions am small regression tree produced cart tm breiman friedman olshen stone 
regression tree parameters am splitting variables split locations terminal node means individual trees 
numerical optimization general choosing parameterized model changes function optimization problem parameter optimization arg min numerical optimization methods applied solve 
involves expressing solution parameters form pm initial guess fpm successive increments steps boosts sequence preceding steps 
prescription computing step pm de ned optimization method 
steepest descent steepest descent simplest frequently numerical minimization methods 
de nes increments fpm follows 
current gradient gm computed gm fg jm pm pm step taken pm arg min pm gm negative gradient gm said de ne steepest descent direction called line search direction 
numerical optimization function space take nonparametric approach apply numerical optimization function space 
consider evaluated point parameter seek minimize ex equivalently individual directly respect 
function space nite number parameters data sets discussed nite number ff involved 
numerical optimization paradigm take solution fm initial guess ffm incremental functions steps boosts de ned optimization method 
steepest descent fm gm fm fm fm assuming sucient regularity interchange di erentiation integration fm multiplier line search arg min fm gm finite data nonparametric approach breaks joint distribution estimated nite data sample fy case 
estimated accurately data value estimate values training sample points 
strength borrowed nearby data points imposing smoothness solution 
way assume parameterized form parameter optimization discussed section minimize corresponding data estimate expected loss am arg min situations infeasible try greedy stagewise approach 

am arg min fm fm fm mh am note stagewise strategy di erent stepwise approaches previously entered terms new ones added 
signal processing stagewise strategy called matching pursuit mallat zhang squared error loss fh am called basis functions usually taken complete wavelet dictionary 
machine learning called boosting exponential loss criterion yf freund schapire schapire singer negative binomial log likelihood friedman hastie tibshirani fht 
function called weak learner base learner usually regression classi cation tree 
suppose particular loss base learner solution dicult obtain 
approximator fm function mh am viewed best greedy step data estimate constraint step direction am member parameterized class functions 
regarded steepest descent step constraint 
construction data analogue unconstrained negative gradient fm gives best steepest descent step direction gm dimensional data space fm 
possibility choose member parameterized class am produces hm fh am parallel gm highly correlated data distribution 
obtained solution am arg min gm constrained negative gradient am place unconstrained steepest descent strategy 
speci cally line search performed arg min fm am approximation updated fm fm mh am basically obtaining solution smoothness constraint constraint applied unconstrained rough solution 
permits replacement dicult function minimization problem squares function minimization followed single parameter optimization original criterion 
feasible squares algorithm exists solving approach minimize loss conjunction forward stagewise additive modeling 
leads generic algorithm steepest descent 
algorithm gradient boost arg min fm am arg min arg min fm am fm fm mh am endfor algorithm note tting criterion estimates conditional expectation principle estimate smoothed negative gradient line algorithm 
squares natural choice owing superior computational properties squares algorithms 
special case loss function depends product yf analogy boosting steepest descent minimization noted machine learning literature breiman ratsch onoda muller 
du helmbold elegantly exploit analogy motivate procedures 
quantity yf called margin steepest descent performed space margin values space function values approach permits application general loss functions notion margins apparent 
drucker employs di erent strategy casting regression framework classi cation context adaboost algorithm freund schapire 
applications additive modeling section gradient boosting strategy applied popular loss criteria squares ls absolute deviation lad huber logistic binomial log likelihood 
rst serves reality check lead new boosting algorithms 
squares regression 
pseudo response line algorithm fm 
line simply ts current residuals line search line produces result minimizing line 
gradient boosting squared error loss produces usual stagewise approach iteratively tting current residuals algorithm ls boost fm am arg min fm fm mh am endfor algorithm absolute deviation lad regression loss function jy fm sign fm implies sign current residuals line algorithm 
line search line arg min jy fm am arg min jh am 
fm am fm am jh am 
weighted median weights inserting results algorithm yields algorithm absolute deviation boosting base learner 
regression trees consider special case base learner terminal node regression tree breiman friedman olshen stone 
regression tree model additive form fb fr disjoint regions collectively cover space joint values predictor variables regions represented terminal nodes corresponding tree 
indicator function 
value argument true zero 
parameters base learner coecients fb quantities de ne boundaries regions fr splitting variables values variables represent splits nonterminal nodes tree 
regions disjoint equivalent prediction rule regression tree update line algorithm fm fm lm rlm regions de ned terminal nodes tree mth iteration 
constructed predict pseudo responses line squares line 
fb lm corresponding squares coecients lm rlm scaling factor solution line search line 
update alternatively expressed fm fm lm lm lm lm view adding separate basis functions step rlm single additive 
optimal coecients separate basis functions solution lm arg min fm lm owing disjoint nature regions produced regression trees reduces lm arg min rlm fm just optimal constant update terminal node region loss function current approximation fm 
case lad regression lm median rlm fy fm simply median current residuals lth terminal node mth iteration 
iteration regression tree built best predict sign current residuals fm squares criterion 
approximation updated adding median residuals derived terminal nodes 
algorithm lad treeboost sign fm terminal node tree lm rlm fy fm fm fm lm rlm endfor algorithm algorithm highly robust 
trees order information individual input variables pseudo responses values 
terminal node updates medians 
alternative approach build tree directly minimize loss criterion tree arg min node tree jy fm tree fm fm tree algorithm faster uses squares induce trees 
squared error loss rapidly updated mean absolute deviation searching splits tree building process 
regression regression techniques attempt resistance long tailed error distributions outliers maintaining high eciency normally distributed errors 
consider huber loss function huber jy jy jy pseudo response fm fm jy fm 
sign fm jy fm line search arg min fm am 
solution obtained standard iterative methods see huber 
value transition point de nes residual values considered outliers subject absolute squared error loss 
optimal value depend distribution true target function 
common practice choose value quantile distribution jy controls break point procedure 
break point fraction observations arbitrarily modi ed seriously degrading quality result 
unknown uses current estimate fm approximation mth iteration 
distribution jy fm estimated current residuals leading quantile fjy fm jg regression trees base learners strategy section separate update terminal node lm huber loss solution approximated single step standard iterative procedure huber starting median lm rlm fr fr current residuals fm approximation lm lm nlm rlm sign lm 
min abs lm nlm number observations lth terminal node 
gives algorithm boosting regression trees huber loss algorithm treeboost fm quantile fjr jg jr 
sign jr terminal node tree lm median rlm fr lm lm nlm rlm sign lm 
min abs lm fm fm lm lm endfor algorithm motivations underlying robust regression algorithm properties similar squares boosting algorithm normally distributed errors similar absolute deviation regression algorithm long tailed distributions 
error distributions moderately long tails performance superior see section 
class logistic regression classi cation loss function negative binomial log likelihood fht log exp yf log pr pr pseudo response fm exp fm line search arg min log exp fm am regression trees base learners strategy section separate updates terminal node rlm lm arg min rlm log exp fm closed form solution 
fht approximate single newton raphson step 
turns lm rlm rlm 
gives algorithm likelihood gradient boosting regression trees algorithm treeboost log exp fm terminal node tree lm rlm rlm fm fm lm rlm endfor algorithm nal approximation fm related log odds 
inverted yield probability estimates pr fm pr fm turn classi cation 
cost associated predicting truth uence trimming empirical loss function class logistic regression problem mth iteration log exp fm 
exp fm large dependence small moderate values near zero 
implies ith observation uence loss function solution am arg min suggests observations fm relatively large deleted computations mth iteration having substantial ect result 
exp fm viewed measure uence weight ith observation estimate mh am 
generally nonparametric function space perspective section parameters observation function values ff uence estimate changes parameter value holding parameters xed gauged second derivative loss function respect parameter 
second derivative mth iteration 
measure uence weight ith observation estimate mh am mth iteration uence trimming deletes observations values solution fw weights fw arranged ascending order 
typical values 
note uence trimming identical weight trimming strategy employed real adaboost equivalent logitboost fht 
seen observations deleted sacri cing accuracy estimates uence measure 
results corresponding reduction computation factors 
multi class logistic regression classi cation develop gradient descent boosting algorithm class problem 
loss function fy log class pr 
fht symmetric multiple logistic transform log log equivalently exp exp substituting rst derivatives ik fy ij ff fj ik derived 
trees induced iteration predict corresponding current residuals class probability scale 
trees terminal nodes corresponding regions fr klm model updates klm corresponding regions solution klm arg min kl ik kl rlm log related 
closed form solution 
regions corresponding di erent class trees overlap solution reduce separate calculation region tree analogy 
fht approximate solution single newton raphson step diagonal approximation hessian 
decomposes problem separate calculation terminal node tree 
result klm ik ik ik leads algorithm class logistic gradient boosting algorithm lk treeboost exp exp ik ik fr klm terminal node tree ik klm klm ik klm ik ik km klm klm endfor endfor algorithm nal estimates ff km obtain corresponding probability estimates fp km 
turn classi cation arg min cost associated predicting kth class truth note algorithm equivalent algorithm 
algorithm bears close similarity class logitboost procedure fht newton raphson gradient descent function space 
algorithm trees induced corresponding pseudo responses ik ik weight applied observation ik 
terminal node updates klm ik equivalent 
di erence algorithms splitting criterion induce trees terminal regions fr klm squares improvement criterion evaluate potential splits currently terminal region subregions left right daughter response means respectively corresponding sums weights 
split unit weights weights give values weight sums di erent 
unit weights lk favor splits symmetric number observations daughter node logitboost favors splits sums currently estimated response variances var ik equal 
lk treeboost implementation advantage numerical stability 
logitboost numerically unstable value close zero observation happens quite frequently 
consequence diculty newton raphson vanishing second derivatives 
performance strongly ected way problem handled see fht pg 
lk treeboost diculties close zero observations terminal node 
happens frequently easier deal happen 
uence trimming multi class procedure implemented way class case outlined section associated observation ik uence ik ik ik deleting observations inducing kth tree current iteration regularization prediction problems tting training data closely counterproductive 
reducing expected loss training data point causes population expected loss decreasing start increase 
regularization methods attempt prevent tting constraining tting procedure 
additive expansions natural regularization parameter number components analogous stepwise regression fh am considered explanatory variables sequentially entered 
controlling value regulates degree expected loss training data minimized 
best value estimated model selection method independent test set cross validation 
goal prediction opposed compression regularization shrinkage provides superior results obtained restricting number components 
context additive models constructed forward stage wise manner simple shrinkage strategy replace line generic algorithm algorithm fm fm 
mh am making corresponding equivalent changes speci algorithms algorithms 
update simply scaled value learning rate parameter 
introducing shrinkage gradient boosting manner provides regularization parameters learning rate number components control degree ect best value 
decreasing value increases best value ideally estimate optimal values minimizing model selection criterion jointly respect values parameters 
computational considerations increasing size produces proportionate increase computation 
illustrate trade simulation study 
training sample consists observations fy target function randomly generated described section 
noise generated normal distribution zero mean variance adjusted ej ex jf median giving signal noise ratio 
illustration base learner taken terminal node regression tree induced best rst manner fht 
general discussion tree size choice appears section 
shows lack lof ls treeboost lad treeboost treeboost function number terms iterations values shrinkage parameter 
rst methods lof measured average absolute error estimate fm relative optimal constant solution fm jf fm jf logistic regression values obtained thresholding median distribution values values greater median assigned median assigned 
bayes error rate zero decision boundary fairly complicated 
lof measures treeboost minus twice log likelihood deviance misclassi cation error rate sign fm 
values lof measures computed independent validation data set observations 
seen fig 
smaller values shrinkage parameter shrinkage seen result better performance diminishing return smallest values 
larger values behavior characteristic tting observed performance reaches optimum value diminishes increases point 
ect pronounced lad treeboost error rate criterion treeboost 
smaller values tting expected 
dicult see misclassi cation error rate lower right panel continues decrease logistic likelihood reached optimum lower left panel 
degrading likelihood tting improves misclassi cation error rate 
counter intuitive contradiction likelihood error rate measure di erent aspects quality 
error rate depends sign fm likelihood ected sign magnitude 
apparently tting degrades quality iterations absolute error ls treeboost iterations absolute error lad treeboost iterations log likelihood treeboost iterations error rate treeboost performance gradient boosting algorithms function number iterations curves correspond shrinkage parameter values order top bottom extreme right plot 
magnitude estimate ecting improving sign 
misclassi cation error sensitive tting 
table summarizes simulation results values including shown fig 

shown value row iteration number minimum lof achieved corresponding minimizing value pairs columns 
table iteration number giving best best value shrinkage parameter values boosting methods 
ls fm lad fm log error rate trade clearly evident smaller values give rise larger optimal values 
provide higher accuracy diminishing return 
misclassi cation error rate optimal values unstable 
illustrated just target function base learner terminal node tree qualitative nature results fairly universal 
target functions tree sizes shown give rise behavior 
suggests best value depends number iterations large computationally convenient feasible 
value adjusted lof achieves minimum close value chosen lof decreasing iteration value number iterations increased preferably 
sequential nature algorithm easily restarted nished previously computation need repeated 
lof function iteration number conveniently estimated left test sample 
illustrated decreasing learning rate clearly improves performance usually dramatically 
reason clear 
shrinking model update iteration produces complex ect direct proportional shrinkage entire model 
fm fm model induced shrinkage 
update mh am iteration depends speci sequence updates previous iterations 
incremental shrinkage produces di erent models global shrinkage 
empirical evidence shown indicates global shrinkage provides best marginal improvement shrinkage far dramatic ect incremental shrinkage 
mystery underlying success incremental shrinkage currently investigation 
simulation studies performance function estimation method depends particular problem applied 
important characteristics problems ect performance include training sample size true underlying target function distribution departures 
problem known distribution know example binary bernoulli 
general real valued variable distribution seldom known 
nearly cases nature unknown 
order gauge value estimation method necessary accurately evaluate performance di erent situations 
conveniently accomplished monte carlo simulation data generated wide variety prescriptions resulting performance accurately calculated 
section studies attempt understand properties various gradient treeboost procedures developed previous sections 
study far thorough evaluating methods just selected examples real simulated results large study regarded suggestive 
random function generator important characteristics problem ecting performance true underlying target function 
method particular targets appropriate 
nature target function vary greatly di erent problems seldom known compare merits regression tree gradient boosting algorithms variety di erent randomly generated targets 
takes form coecients fa randomly generated uniform distribution 
function randomly selected subset size input variables speci cally fx separate random permutation integers 
ng 
size subset taken random rc drawn exponential distribution mean 
expected number input variables 
fewer somewhat 
re ects bias strong high order interaction ects 
realized chance functions involve higher order interactions 
case function nearly input variables 
dimensional gaussian function exp mean vectors randomly generated distribution input variables covariance matrix randomly generated 
speci cally random orthonormal matrix uniform haar measure 
square roots eigenvalues randomly generated uniform distribution jl limits depend distribution input variables studies number input variables taken joint distribution taken standard normal 
eigenvalue limits 
tails normal distribution shorter data encountered practice realistic uniformly distributed inputs simulation studies 
regression trees immune ects long tailed input variable distributions shorter tails gives relative advantage competitors comparisons 
simulation studies target functions randomly generated prescription 
performance evaluated terms distribution approximation inaccuracy relative approximation error misclassi cation risk di erent targets 
approach allows wide variety quite di erent target functions generated terms shapes contours dimensional input space 
lower order interactions favored functions especially suited additive regression trees 
decision trees produce tensor product basis functions components targets tensor product functions 
input variables target function 
data mining applications inputs 
relevant dimensionalities intrinsic dimensionality input space number inputs uence output response variable problems input variables usually high degrees collinearity number roughly independent variables approximate intrinsic dimensionality smaller 
target functions strongly depend small subset inputs 
error distribution section ls treeboost lad treeboost treeboost compared terms performance target functions di erent error distributions 
best rst regression trees terminal nodes algorithms 
break parameter treeboost set default value 
data sets fy generated represents target functions randomly generated described section 
rst study errors generated normal distribution zero mean variance adjusted ej ex jf giving signal noise ratio 
second study errors generated slash distribution 

scale factor adjusted give signal noise ratio 
slash distribution thick tails extreme test robustness 
training sample size taken training left test sample estimate optimal number components trials additional validation sample observations generated error evaluate approximation inaccuracy trial 
shrinkage parameter set default value 
left panels fig 
show boxplots distribution approximation inaccuracy targets error distributions methods 
shaded area boxplot shows interquartile range distribution enclosed white bar median 
outer hinges represent points closest plus minus interquartile range units upper lower quartiles 
isolated bars represent individual points outside range outliers 
plots allow comparison distributions give information concerning relative performance individual target functions 
right panels fig 
attempt provide summary 
show distributions error ratios errors 
target function method error method target divided smallest error obtained target methods compared 
trials best method receives value receive larger value 
particular method best smallest error target functions resulting distribution boxplot point mass value 
note logarithm ratio plotted lower right panel 
ls lad error normal ls lad error min error normal ls lad error slash ls lad log error min error slash distribution absolute approximation error left panels error relative best right panels ls treeboost lad treeboost treeboost normal slash error distributions 
ls treeboost performs best normal error distribution 
lad treeboost treeboost perform slash errors 
treeboost close best error distributions 
note logarithmic scale lower right panel 
left panels fig 
sees targets represent fairly wide spectrum diculty methods approximation errors vary factor 
normally distributed errors ls treeboost superior performer expected 
smallest error trials treeboost best times 
average ls treeboost worse best treeboost worse lad treeboost worse best 
slash distributed errors things reversed 
average approximation error ls treeboost explaining target variation 
individual trials better worse 
performance lad treeboost treeboost better comparable 
lad treeboost best times treeboost times 
average lad treeboost worse best treeboost worse ls treeboost worse best targets 
results suggest treeboost method choice 
extreme cases behaved normal badly behaved slash errors performance close best 
comparison lad treeboost su ered somewhat normal errors ls treeboost disastrous slash errors 
ls treeboost versus mars gradient treeboost algorithms produce piecewise constant approximations 
number pieces generally larger produced single tree aspect approximating function fm expected represent disadvantage respect methods provide continuous approximations especially true underlying target continuous fairly smooth 
randomly generated target functions continuous smooth 
section investigate extent piecewise constant disadvantage comparing accuracy gradient treeboost mars friedman targets 
treeboost mars produces tensor product approximation 
uses continuous functions product factors producing continuous approximation 
uses involved stepwise strategy induce tensor products 
mars squares tting compare ls treeboost normally distributed errors signal noise ratio 
experimental setup section 
interesting note performance mars considerably enhanced test set model selection default gcv criterion friedman 
top left panel fig 
compares distribution mars average absolute approximation errors randomly generated target functions ls treeboost fig 

mars distribution seen broader varying factor 
targets mars considerably better ls treeboost substantially worse 
illustrates fact nature target function strongly uences relative performance di erent methods 
top right panel fig 
shows distribution errors relative best target 
methods exhibit similar performance average absolute error 
number targets substantially outperformed 
bottom panels fig 
show corresponding plots root mean squared error 
gives proportionally weight larger errors assessing lack performance 
ls treeboost error measures close values targets 
mars root mean squared error typically higher average absolute error 
indicates mars predictions tend close far target 
errors ls treeboost evenly distributed 
tends fewer large errors small errors 
may consequence piecewise constant nature approximation dicult get arbitrarily close smoothly varying targets approximations nite size 
fig 
illustrates relative performance quite sensitive criterion measure 
results indicate piecewise constant aspect treeboost approximations serious disadvantage 
environment normal errors normal input variable distributions competitive mars 
advantage piecewise constant approach robustness speci cally provides immunity adverse ects wide tails outliers distribution input variables methods produce continuous approximations mars extremely sensitive problems 
shown section treeboost algorithm nearly accurate ls treeboost normal errors addition highly resistant output outliers 
data mining applications data assured outliers may relatively high accuracy consistent performance robustness treeboost may represent substantial advantage 
lk treeboost versus class logitboost adaboost mh section performance lk treeboost compared class logitboost fht adaboost mh schapire singer randomly generated targets section 
classes generated thresholding target quantiles distribution input values 
training observations trial class divided training model selection number iterations 
independently generated validation sample observations estimate error rate target 
bayes error rate zero targets induced decision boundaries quite complicated depending nature individual target function 
regression trees terminal nodes method 
ls treeboost mars error abs error ls treeboost mars error min error abs error ls treeboost mars error rms error ls treeboost mars error min error rms error distribution approximation error left panels error relative best right panels ls treeboost mars 
top panels average absolute error bottom ones root mean squared error 
absolute error mars distribution wider indicating frequent better worse performance ls treeboost 
mars performance measured root mean squared error worse indicating tends frequently larger smaller errors ls treeboost 
lk treeboost logitboost adaboost error rate lk treeboost logitboost adaboost rel 
error rate distribution error rate class problem left panel error rate relative best right panel lk treeboost logitboost adaboost mh 
lk treeboost exhibits superior performance 
shows distribution error rate left panel ratio smallest right panel target functions methods 
error rate methods seen vary substantially targets 
lk treeboost seen generally superior performer 
smallest error trials average error rate higher best trial 
logitboost best targets tie 
error rate higher best average 
adaboost mh best performer average worse best 
shows corresponding comparison logitboost adaboost mh procedures modi ed incorporate incremental shrinkage shrinkage parameter set default value lk treeboost 
sees somewhat di erent picture 
logitboost adaboost mh bene substantially shrinkage 
performance procedures nearly logitboost having slight advantage 
average error rate worse best corresponding values lk treeboost adaboost mh respectively 
results suggest relative performance methods dependent aggressiveness parameterized learning rate structural di erences 
logitboost additional internal shrinkage associated stabilizing pseudo response denominator close zero fht pg 
may account slight superiority comparison 
fact increased shrinkage applied lk treeboost performance improves identical logitboost shown fig 

shrinkage parameter carefully tuned methods little performance di erential 
tree boosting procedure algorithm primary meta parameters number iterations learning rate parameter 
discussed section 
lk treeboost logitboost adaboost error rate lk treeboost logitboost adaboost rel 
error rate distribution error rate class problem left panel error rate relative best right panel lk treeboost proportional shrinkage applied logitboost 
performance thee methods similar 
addition meta parameters associated procedure estimate base learner 
primary focus best rst induced regression trees xed number terminal nodes primary meta parameter base learner 
best choice value depends strongly nature target function highest order dominant interactions variables 
consider anova expansion function jk jkl 
rst sum called main ects component 
consists sum functions depend input variable 
particular functions ff provide closest approximation additive constraint 
referred additive model contributions add contributions 
di erent restrictive de nition additive 
second sum consists functions pairs input variables 
called variable interaction ects 
chosen main ects provide closest approximation limitation variable interactions 
third sum represents variable interaction ects 
highest interaction order possible limited number input variables especially large target functions encountered practice closely approximated anova decompositions lower order 
rst terms required capture dominant variation 
fact considerable success achieved additive component hastie tibshirani 
purely additive approximations produced naive bayes method warner toronto stephenson highly successful classi cation 
considerations motivated bias lower order interactions randomly generated target functions abs 
error rel 
error distribution absolute approximation error left panel error relative best right panel ls treeboost di erent sized trees measured number terminal nodes distribution smallest trees wider indicating frequent better worse performance larger trees similar performance 
section simulation studies 
goal function estimation produce approximation closely matches target 
usually requires dominant interaction order similar 
boosting regression trees interaction order controlled limiting size individual trees induced iteration 
tree terminal nodes produces function interaction order min 
boosting process additive interaction order entire approximation larger largest individual components 
treeboost procedures best tree size governed ective interaction order target 
usually unknown meta parameter procedure estimated model selection criterion cross validation left subsample training 
discussed large trees necessary desirable 
illustrates ect tree size approximation accuracy randomly generated functions section simulation studies 
experimental setup section 
shown distribution absolute errors left panel errors relative lowest target right panel 
rst value produces additive main ects components produces additive variable interaction terms 
terminal node tree produce interaction levels maximum min typical values especially seen fig 
smallest trees produce lower accuracy average distributions considerably wider 
means produce accurate inaccurate approximations 
smaller trees restricted low order interactions better able take advantage targets happen low interaction level 
quite badly trying approximate high order inter action targets 
larger trees consistent 
sacri ce accuracy low order interaction targets better higher order functions 
little performance di erence larger trees slight deterioration 
trees produced accurate approximation times corresponding numbers respectively 
average trees errors larger lowest target corresponding values respectively 
higher accuracy obtained best tree size individually estimated target 
practice accomplished evaluating di erent tree sizes independent test data set illustrated section 
interpretation applications useful able interpret derived approximation 
involves gaining understanding particular input variables uential contributing variation nature dependence uential inputs 
extent qualitatively re ects nature target function tools provide information concerning underlying relationship inputs output variable section tools interpreting treeboost approximations 
interpreting single decision trees tend ective context boosting especially small trees 
interpretative tools described section illustrated section context real data examples 
relative importance input variables useful descriptions approximation relative uences individual inputs variation joint input variable distribution 
measure 
var piecewise constant approximations produced decision trees strictly exist approximated surrogate measure re ects properties 
breiman proposed summation non terminal nodes terminal node tree splitting variable associated node corresponding empirical improvement squared error result split 
right hand side associated squared uence units correspond 
breiman directly measure uence squared uence 
collection decision trees obtained boosting generalized average trees tm sequence 
motivation purely heuristic arguments 
partial justi cation show produces expected results applied simplest context 
consider linear target function covariance matrix inputs multiple identity ex ci case uence measure produces ja table shows results small simulation study similar section taken linear coecients signal noise ratio 
shown mean standard deviation values random samples 
uence estimated uential variable arbitrarily assigned value estimated values scaled accordingly 
estimated importance ranking input variables correct trials 
seen table estimated relative uence values consistent 
table estimated mean standard deviation input variable relative uence linear target function 
var 
mean std 
breiman uence measure augmented strategy involving surrogate splits intended uncover masking uential variables highly associated 
strategy helpful single decision trees opportunity variables participate splitting limited size tree 
context boosting number splitting opportunities vastly increased surrogate correspondingly essential 
class logistic regression classi cation section logistic regression functions ff km described sequence trees 
case generalizes jk km km tree induced kth class iteration quantity jk interpreted relevance predictor variable separating class classes 
relevance obtained averaging classes jk individual jk quite useful 
case di erent subsets variables highly relevant di erent subsets classes 
detailed knowledge lead insights obtainable examining relevance 
partial dependence plots visualization powerful tools 
graphical renderings value function arguments provides comprehensive summary dependence joint values input variables 
unfortunately visualization limited low dimensional arguments 
functions single real valued variable plotted graph values corresponding value functions single categorical variable represented bar plot bar representing values bar height value function 
functions real valued variables pictured contour perspective mesh plots 
functions categorical variable variable real categorical best summarized sequence trellis plots showing dependence second variable conditioned respective values rst variable becker cleveland 
viewing functions higher dimensional arguments dicult 
useful able view partial dependence approximation selected small subsets input variables 
collection plots seldom provide comprehensive depiction approximation produce helpful clues especially dominated low order interactions section 
chosen target subset size input variables fz 
fx 
xn nl complement subset nl approximation principle depends variables subsets nl conditions speci values variables nl considered function variables chosen subset nl nl general functional form nl depend particular values chosen nl dependence strong average function nl nl nl nl dz nl represent useful summary partial dependence chosen variable subset nl nl marginal probability density nl nl nl dz joint density inputs complement marginal density estimated training data nl special cases dependence additive nl nl multiplicative 
nl nl form nl depend joint values compliment variables nl provides complete description nature variation chosen input variable subset alternative way summarizing dependence subset directly model function training data nl dz nl averaging conditional density marginal density causes re ect dependence selected variable subset addition apparent dependencies induced solely associations complement variables nl example contribution happens additive multiplicative evaluate corresponding term factor joint density happened product 
nl nl partial dependence functions help interpret models produced black box prediction method neural networks support vector machines nearest neighbors radial basis functions large number predictor variables useful measure relevance section reduce potentially large number variables variable combinations considered 
pass data required evaluate set joint values argument 
time consuming large data sets subsampling help somewhat 
regression trees single variable splits partial dependence speci ed target variable subset straightforward evaluate tree data 
speci set values variables weighted traversal tree performed 
root tree weight value assigned 
non terminal node visited split variable target subset appropriate left right daughter node visited weight modi ed 
node split variable member complement subset nl daughters visited current weight multiplied fraction training observations went left right respectively node 
terminal node visited traversal assigned current value weight 
tree traversal complete value corresponding weighted average values terminal nodes visited tree traversal 
collection regression trees obtained boosting results individual trees simply averaged 
purposes interpretation graphical displays input variable subsets low cardinality useful 
informative subsets comprised input variables deemed uential contributing variation 
illustrations provided section 
closer dependence subset additive multiplicative completely partial dependence function captures nature uence variables derived approximation 
subsets group uential inputs complex interactions provide partial dependence plots 
diagnostic nl separately computed candidate subsets 
value multiple correlation training data nl nl 
nl nl gauge degree additivity respect chosen subset additional diagnostic nl computed small number nl values randomly selected training data 
resulting functions compared judge variability partial dependence respect changing values nl class logistic regression classi cation section logistic regression functions ff logarithmically related pr 
larger values imply higher probability observing class partial dependence plots variable subsets relevant class provide information input variables uence respective class probabilities 
real data section treeboost regression algorithms illustrated moderate sized data sets 
results section suggest properties classi cation algorithm lk treeboost similar logitboost extensively applied data fht 
rst scienti data set consists chemical concentration measurements rock samples second demographic sample survey questionnaire data 
data sets partitioned learning sample consisting thirds data remaining data test sample choosing model size number iterations 
shrinkage parameter set 
garnet data data set consists sample collected world grin 
garnet complex ca mg fe cr commonly occurs minor phase rocks making earth mantle 
variables associated garnet concentrations various chemicals plate setting rock collected tio cr feo mno cao zn ga sr zr tec 
rst eleven variables representing concentrations real valued 
variable tec takes categorical values ancient stable shields shield areas young belts 
missing values data distribution variables tend highly skewed larger values outliers 
purpose exercise estimate concentration titanium tio function joint concentrations chemicals plate index 
table average absolute error ls treeboost lad treeboost treeboost garnet data varying numbers terminal nodes individual trees 
term 
nodes ls lad table shows average absolute error predicting output variable relative optimal constant prediction jy median test sample ls treeboost lad treeboost treeboost values size number terminal nodes constituent trees 
note prediction error measure includes additive irreducible error associated unknown underlying target function 
irreducible error adds amount entries table 
di erences entries re ect proportionally greater improvement approximation error target function 
methods additive approximation distinctly inferior larger trees indicating presence interaction ects section input variables 
terminal node trees seen adequate terminal node trees seen provide accuracy best 
errors lad treeboost treeboost smaller ls treeboost similar treeboost having slight edge 
results consistent obtained simulation studies shown fig 
fig 

shows relative importance input variables predicting tio concentration treeboost approximation terminal node trees 
results similar models table similar errors 
ga zr seen uential mno somewhat important 
top panels fig 
show partial dependence approximation uential variables 
piecewise constant nature approximation evident dramatic 
bottom panels show partial dependence pairings variables 
strong interaction ect ga zr clearly evident 
little dependence variable takes smallest values 
value increased dependence correspondingly ampli ed 
somewhat smaller interaction ect seen mno zr 
demographic data data set consists questionnaires lled shopping mall customers san francisco bay area impact resources columbus oh 
answers rst questions relating demographics illustration 
questions listed table 
data seen consist mixture real categorical variables small numbers distinct values 
missing values 
ga zr mno zn cao feo cr tec sr relative importance relative uence eleven input variables target variation garnet data 
ga zr uential 
table variables demographic data 
var demographic values type sex cat martial status cat age real education real occupation cat income real years ba real dual incomes cat number household real number household householder status cat type home cat ethnic classi cation cat language home cat illustrate treeboost data modeling income function variables 
table shows average absolute error predicting income relative best constant predictor regression treeboost algorithms 
tio tio tio tio tio tio ga zr mno ga zr ga mno mno zr partial dependence plots uential input variables garnet data 
note di erent vertical scales plot 
strong interaction ect zr ga somewhat weaker zr mno 
occ mar age edu hme eth lan num sex relative importance relative uence input variables target variation demographic data 
small group variables dominate 
table average absolute error ls treeboost lad treeboost treeboost demographic data varying numbers terminal nodes individual trees 
term 
nodes ls lad little di erence performance methods 
owing highly discrete nature data outliers long tailed distributions real valued inputs output little reduction error constituent tree size increased indicating lack interactions input variables approximation additive individual input variables adequate 
shows relative importance input variables predicting income ls treeboost approximation 
small subset dominates 
shows partial dependence plots uential variables 
categorical variables represented bar plots plots centered zero mean data 
approximation consists main ects rst sum plots completely describe corresponding contributions inputs 
occupation prof manag 
sales clerical student military retired unemployed household status rent live family marital status married live divorced sep single age education type home house apartment mobile home partial dependence plots uential input variables demographic data 
note di erent vertical scales plot 
abscissa values age education codes representing consecutive equal intervals 
dependence income age nonmonotonic reaching maximum value representing interval years old 
appear surprising results fig 

dependencies part con rm prior suspicions suggest approximation intuitively reasonable 
data mining shelf tools predictive data mining treeboost procedures attractive properties 
inherit favorable characteristics regression trees mitigating unfavorable ones 
favorable robustness 
treeboost procedures invariant strictly monotone transformations individual input variables 
example log jth input variable yields result 
need considering input variable transformations eliminated 
consequence invariance sensitivity long tailed distributions outliers eliminated 
addition lad treeboost completely robust outliers output variable 
treeboost enjoys fair measure robustness output outliers 
advantage regression tree induction internal feature selection 
trees tend quite robust addition irrelevant input variables 
treeboost clearly inherits property 
principal disadvantage regression trees inaccuracy 
consequence coarse nature piecewise constant approximations especially smaller trees instability especially larger trees fact involve predominately high order interactions 
mitigated boosting 
treeboost procedures produce piecewise constant approximations illustrated fig 
granularity ner 
treeboost enhances stability small trees ect averaging 
interaction level treeboost approximations ectively controlled limiting size individual constituent trees 
purported biggest advantages single regression trees interpretability boosted trees thought lack feature 
small trees easily interpreted due instability interpretations treated caution 
interpretability larger trees questionable ripley 
treeboost approximations interpreted partial dependence plots conjunction input variable relative importance measure illustrated section 
providing complete description er insight nature input output relationship 
tools approximation method special characteristics tree models allow rapid calculation 
partial dependence plots single regression trees noted caution required owing greater instability 
sorting input variables computation regression treeboost procedures ls lad treeboost scales linearly number observations number input variables number iterations scales roughly logarithm size constituent trees addition classi cation algorithm lk treeboost scales linearly number classes scales highly sub linearly number iterations uence trimming section employed 
point applying treeboost garnet data section required minute mh pentium ii computer 
seen section boosting iterations required obtain optimal treeboost approximations small values shrinkage parameter 
somewhat mitigated small size trees induced iteration 
illustrated fig 
improvement tends rapid initially levels slower increments 
nearly optimal approximations achieved quite early correspondingly computation 
near optimal approximations initial exploration provide indication nal approximation sucient accuracy warrant continuation 
lack improves little rst iterations say dramatic improvement 
continuation judged warranted procedure restarted left previously computational investment lost 
larger values shrinkage parameter speed initial improvement purpose 
seen fig 
provided accuracy optimal solution iterations 
case boosting restarted smaller shrinkage parameter value subsequently employed 
ability treeboost procedures give quick indication potential predictability coupled extreme robustness useful preprocessing tool applied imperfect data 
sucient predictability indicated data cleaning invested render suitable sophisticated robust modeling procedures 
data available modeling complete boosting continued new data starting previous solution 
usually improves accuracy provided independent test data set monitor improvement prevent tting new data 
accuracy increase generally obtained redoing entire analysis combined data considerable computation saved 
boosting successive subsets data insucient random access main memory store entire data set 
boosting applied data breiman sequentially read main memory time starting current solution recycling previous subsets time permits 
crucial independent test set training individual subset point estimated accuracy combined approximation starts diminish 
acknowledgments helpful discussions trevor hastie bogdan popescu robert tibshirani gratefully acknowledged 
partially supported csiro mathematical information sciences australia department energy contract de ac sf dms national science foundation 
becker cleveland 
design control trellis display 
comput 
statist 
graphics 
breiman 

prediction games arcing algorithms 
univ calif berkeley dept statistics technical report 
submitted neural computing 
breiman 

pasting bites prediction large data sets line 
univ calif berkeley dept statistics technical report 
breiman friedman olshen stone 

classi cation regression trees 
wadsworth 


regression prediction shrinkage discussion 
statist 
soc 

donoho 

nonlinear methods recovery signals densities spectra indirect noisy data 
di erent perspectives wavelets proc 
symp 
applied mathematics daubechies ed 
amer 
math 
soc providence 
drucker 

improving regressors boosting techniques 
proceedings fourteenth international conference machine learning 
ed 
fisher jr pp 

morgan kaufmann 
du helmbold 

geometric approach leveraging weak learners 
university california santa cruz technical report appear eurocolt springer verlag 
freund schapire 

experiments new boosting algorithm 
machine learning proceedings thirteenth international conference 
friedman 

multivariate adaptive regression splines discussion 
annals statistics 
friedman hastie tibshirani 

additive logistic regression statistical view boosting discussion 
annals statistics appear 
grin fisher friedman ryan reilly 

cr mantle 
appear 
hastie tibshirani 
generalized additive models 
chapman hall 
huber 

robust estimation location parameter 
annals mathematical statistics 
mallat zhang 
matching pursuits time frequency dictionaries 
ieee transactions signal processing 
powell 

radial basis functions multivariate interpolation review 
algorithms approximation eds 
mason cox pp 
oxford clarendon press 
ratsch onoda muller 

soft margins adaboost 
technical report nc tr neurocolt 
ripley 

pattern recognition neural networks 
cambridge university press 
rumelhart hinton williams 

learning representations back propagating errors 
nature 
schapire singer 

improved boosting algorithms con dence rated predictions 
proceedings eleventh annual conference computational learning theory 
vapnik 

nature statistical learning theory 
springer 
warner toronto stephenson 

mathematical model medical diagnosis application heart disease 
amer 
med 
assoc 


