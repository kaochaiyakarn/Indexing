logical computation fractal neural substrate simon levy jordan pollack brandeis university computer science department waltham ma usa levy pollack cs brandeis edu attempts neural networks model recursive symbolic processes logic met success faced serious hurdles caused limitations standard connectionist coding schemes 
contribution ort presents nite raam iraam new connectionist uni cation model fusion recurrent neural networks fractal geometry 
logical programming language modeling domain show approach solves problems faced earlier connectionist models supporting arbitrarily large sets logical expressions 
logical computation represented programming languages prolog classic example recursive symbolic system 
atoms primitives fred wilma loves combined form propositions loves fred wilma turn combine propositions knows barney loves fred wilma ad 
attempts build connectionist models systems generally followed approaches 
rst exempli ed dispenses entirely traditional representations data structures rules algorithms structures favor letting network learn patterns data modeled known back propagation algorithm similar training method 
approach subject harsh criticism disparity strength claims actual results reported apparent inability systems handle systematic compositional aspects meaning recursive symbol systems 
second sort connectionist approach goes rules representations view directly heart computing means showing recurrent neural network perform operations turing machine 
proofs may hold deal theoretical interest address degree particular computational paradigm connectionism suited particular real world task logic 
arguing merits connectionism model particular domain interest knowing turing equivalence help choosing macintosh pentium pc 
third approach proponents described representations rules wish take 
approach acknowledges need systematic compositional structure rejects traditional recursive rules favor exible computation orded connectionist representations 
proponents view course responsible showing representations support kinds processes traditionally viewed rules 
remainder show behavior neural network called nite raam corresponds directly process uni cation supporting systematic compositional model logical computation recursive symbol systems 
uni cation uni cation pattern matching algorithm popularized robinson basis automated core logical programming languages prolog 
basic uni cation algorithm introductory ai textbooks summarized recursively follows variable uni ed literal 
literals uni ed initial predicate symbols arguments uni ed 
example prolog database containing assertion male albert meaning albert male perform query male asking male uni cation algorithm rst attempt unify male albert male succeed matching predicate symbol male rule 
algorithm recur attempting unify variable atomic literal albert succeed rule terminate result bound albert answering query 
course real programming language applications require uni cation algorithms complicated illustrated simple example example suces goals 
raam describing nite raam model suited performing uni cation historical background model necessary 
recursive auto associative memory raam method storing tree structures xed width vectors repeated compression 
architecture consists separate networks encoder network construct xed dimensional code combining nodes symbolic tree bottom decoder network decompresses code components 
decoder applied recursively terminates symbols reconstructing tree 
networks simultaneously trained timevarying inputs 
training successful result bottom encoding coincide top decoding 
shows example raam storing binary trees bits representation input output 
solid lines depict encoder weights dashed lines decoder weights 
note real valued representation tree hidden layer fed back encoder build representation trees restricting network bits symbol allows straightforward visualization hidden layer dynamics plot 
raams real world tasks bits symbol 
output hidden input bias raam encoding decoding tree raam iterated function system consider raam decoder shown 
consists neurons receive input 
output portion network divided right left pair neurons 
operation decoder output pair neurons recursively reapplied network 
bar top gure gate determines left right output reapplied 
raam interpretation recursion implies branching node binary tree represented decoder initial starting point 
network recurrence evaluated context dynamical systems 
network form iterated function system ifs consisting transforms iteratively applied points dimensional space 
detail raam decoder typical ifs transforms linear equations form vectors matrix 
iterated part term ifs comes fact starting initial transforms applied iteratively output output transforms 
choice transform apply deterministically non deterministic probabilities associated transform 
transforms contractive meaning decrease distance input vectors limit process number iterations approaches nity yields attractor stable set ifs 
ifs research focussed systems attractor fractal meaning exhibits self similarity scales 
transforms raam decoder form familiar squashing function 
typical connectionist models matrix ranges entire set real numbers necessarily contractive 
squashing function provides pseudo contractive property yields attractor decoder 
context raams main interesting property pseudo contractive lies trajectories points space 
space divided sets points 
rst set consists points located underlying fractal attractor ifs 
second set complement rst points attractor 
trajectories points second set characterized gravitation attractor follows iteration produces set left right copies points previous iteration 
finite multiple iterations transforms ect bringing set copies arbitrarily close attractor 
dividing space way allowed solve vexing problem behavior decoder encoder feedback terminates exhausted set trees input decoder way knowing decoded terminal representation output arbitrarily close zero 
standard threshold solved problem extent led problems nite loops premature termination limited scalability raam model 
membership attractor terminal test decoder completely solves problems allows model represent extremely large sets trees small xed dimensional neural codes 
attractor fractal generated arbitrary pixel resolution 
interpretation possible tree described single point equivalence class initial points sharing tree shaped trajectories fractal attractor 
attractor terminal test allows natural formulation assigning labels terminals 
barnsley noted point attractor associated address simply sequence indices transforms arrive point points attractor 
address essentially nite sequence digits 
achieve labeling speci alphabet need consider sucient number signi cant digits address 
ideas encapsulated shows galaxy attractor obtained iterative blind watchmaker selection visually appealing shape sample derivations trees 
gure attractor points reachable attractor left transform colored dark gray points address reachable right transform light gray 
left transients attractor shown dashed lines right transients solid lines 
galaxy attractor trees nite raam attractor terminal test able hill climbing train raam decoder generate strings set 
simplest example non regular contextfree formal language target set number recurrent network research projects serves benchmark formal power model raam 
analysis decoder weights raam revealed pattern able generalize formal constructive proof deriving set weights generate language arbitrarily large values function pixel resolution 
proof hand felt justi ed term nite raam iraam refer decoder networks 
traditional approach problems logic recursive symbol systems held suciently generative models neural networks seen mere nite capacity implementations formally proven existence set pure weights provided evidence neural network serve nitely generative model dynamical systems interpretation network behavior 
uni cation iraam fundamental problem exists general case investigating capacity iraam decoder discrete sampling space tree equivalence classes 
transients attractor potentially entire unit space coming rest attractor potential depth trees encoded low resolution sampling quite large 
number possible trees grows depth trees discrete sampling method doomed nd nitesimal portion trees iraam encoding 
solving problem requires knowing precisely trees search nd 
limit number trees sucient limit number ifs iterations 
sampling limiting iterations produces approximation actual nite attractor 
zero iterations entire space attractor approximate tree encoded terminal may refer generically iteration point attractor goes attractor iteration tree encoded 
iterations trees encoded iterations 
solves rst part problem 
solving second part problem locating trees space requires switching top approach bottom approach 
longer start point attractor decode tree point path attractor 
start point set points attractor ask point point uni ed encoder term uni cation iraam 
perform uni cation rst compute attractor take image left right inverses transforms 
uni cations trees located precisely intersections inverses 
interpretation asking representations uni ed means asking inverses non empty intersection 
example determine locations binary trees depth rst iterate ifs twice producing attractor approximate encodes terminal tree process depicted 
gure shows ifs thought kind broken copy machine produces distorted copies left right transforms input 
union copies input machine iteration 
rst iteration copies unit square attractor approximate shown 
union copies ii attractor approximate fed back machine copies iii 
union copies attractor approximate shown iv 
ii iii iv iterations galaxy ifs attractor iv region encoding terminal tree intersecting left right inverses region gives region encoding tree 
encode tree take left inverse region intersect right inverse attractor 
swapping left right operations done obtain tree 
tree encoded region encoding trees 
labeling terminals discussion hill climbing decoder described scheme labeling points attractor terminal set means fractal addresses 
method involved approximating attractor pixel resolution labeling attractor point transform point reachable points attractor 
scheme implemented model attractor approximated iteration points reachable current attractor approximate lie approximates 
points part reachable outside scheme label terminals trees de nition transients attractor points outside 
illustrates approximated attractor connected set points lying bounded region space 
possible treat labeling task partitioning problem label corresponds distinct sub region attractor 
interpretation deriving raam set logical propositions requires coming set decoder weights set partitions attractor induced treating weights ifs transforms uni cations trees partitions yield exactly propositions 
rst step solving problem took xed set decoder weights hill climbing obtain set attractor partitions yielding small set propositions borrowed modi cation introductory tutorial standard prolog textbook propositions put pre form correspond equivalent binary trees female alice female alice 
parent predicate changed argument argument predicate encoding parent 
male albert 
male edward 
female alice 
female victoria 
parent victoria 
parent albert 
terminals male female parent albert edward victoria alice represented distinct circular region attractor 
initial locations centers circles chosen fall portion attractor reachable appropriate transform male female parent part attractor reachable unit square left transform albert alice edward victoria part attractor reachable right transform 
hill climbing iteration center radius circle mutated addition small amount random noise 
noise scaled error center point de ned average distance point inverse inverses center points labels point needed unify 
added noise moved inverses closer mutated center radius new center radius label 
number di erent initial conditions hillclimbing able discover set circular labeled regions small set propositions iterations 
solution shown displays part attractor labels ended 
predicates left side gure arguments part gure albert alice edward victoria male female parent 
trees uni cations intersections inverses appear elliptical regions top gure 
having obtained labels see conjunction weights answer logical queries database 
consider example prolog query female corresponding english question female 
load proposition database prolog interpreter pose query interpreter answer alice victoria 
answer question iraam model locate region circle corresponding predicate female take left inverse transform region take right forward transform inverse 
regions intersected resulting shape contain answer circle convex shape easy determine intersections parametrically 
attractor partitions obtained hill climbing query 
attractor labels shown regions circles labeled alice victoria 
small hill climbing experiment reported provides minimal proof concept applying uni cation raam problem logical computation 
particular problem terribly interesting solution essentially venn diagram lacks complexity depth real propositional tree structures 
inherent raam model partitioning algorithm restricts shallow examples currently working extending algorithm trees depth 
attempt integrate discovery network weights partitions evolutionary paradigm sort described 
fractal representation structured information neural networks relatively new eld test model substantial empirical data 
encouraged success related fractal encoding grammars see contributing ort 
hope serve foundation principled uni cation connectionist approaches traditional symbolic models alternative hybrid methods 
ackley hinton sejnowski 
learning algorithm boltzmann machines 
cognitive science 
barnsley 
fractals 
academic press new york 
clocksin mellish 
programming prolog 
springer verlag berlin 
dawkins 
blind watchmaker evidence evolution reveals universe design 
norton new york 
fodor pylyshyn 
connectionism cognitive architecture critical analysis 
cognition 
hillis 
evolving parasites improves simulated evolution optimization procedure 
langton taylor farmer editors arti cial life ii pages 
addison wesley 
horgan 
representations rules 
philosophical topics xvii 
melnik levy pollack 
raam nite context free language 
ijcnn 
international joint conference neural networks ieee 
pinker prince 
language connectionism analysis parallel distributed processing model language acquisition 
cognition 
pollack 
recursive distributed representations 
arti cal intelligence 
rich knight 
arti cial intelligence 
mcgraw hill new york 
robinson 
machine oriented logic resolution principle 
journal acm 
rodriguez wiles elman 
recurrent neural network learns count 
connection science 
rumelhart hinton williams 
learning internal representation error propagation 
rumelhart mcclelland editors parallel distributed processing explorations microstructure cognition volume 
mit 
rumelhart mcclelland 
learning past tenses english verbs 
rumelhart mcclelland editors op 
cit volume 

siegelmann 
computation turing limit 
science 

fractal encoding context free grammars connectionist networks 
expert systems international journal knowledge engineering neural networks 
williams zipser 
learning algorithm continually running fully recurrent neural networks 
neural computation 
