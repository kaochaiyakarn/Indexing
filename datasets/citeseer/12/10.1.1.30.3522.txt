feature subset selection genetic algorithm yang honavar artificial intelligence research group department computer science hall iowa state university ames ia cs iastate edu practical pattern classification knowledge discovery problems require selection subset attributes features larger set represent patterns classified 
due fact performance classifier usually induced learning algorithm cost classification sensitive choice features construct classifier 
exhaustive evaluation possible feature subsets usually infeasible practice large amount computational effort required 
genetic algorithms belong class randomized heuristic search techniques offer attractive approach find near optimal solutions optimization problems 
presents approach feature subset selection genetic algorithm 
advantages approach include ability accommodate multiple criteria accuracy cost classification feature selection process find feature subsets perform particular choices inductive learning algorithm construct pattern classifier 
experiments benchmark real world pattern classification problems demonstrate feasibility approach feature subset selection automated design neural networks pattern classification knowledge discovery 
practical pattern classification tasks medical diagnosis require learning appropriate classification function assigns input pattern typically represented vector attribute feature values finite set classes 
choice features attributes measurements represent patterns classifier affect things accuracy classification function learned inductive learning algorithm decision tree induction algorithm neural network learning algorithm features describe patterns implicitly define pattern language 
language expressive fail capture information necessary classification regardless learning algorithm accuracy classification function learned limited lack information 
time needed learning sufficiently accurate classification function representation classification function features describe patterns implicitly determine search space needs explored learning algorithm 
dance irrelevant features unnecessarily increase size search space time needed learning sufficiently accurate classification function 
number examples needed learning sufficiently accurate classification function things equal larger number features describe patterns domain interest larger number examples needed learn classification function desired accuracy langley mitchell 
cost performing classification learned classification function practical applications medical diagnosis patterns described observable symptoms results diagnostic tests 
different diagnostic tests different costs risks associated 
instance invasive exploratory surgery expensive risky say blood test 
comprehensibility knowledge acquired learning primary task inductive learning algorithm extract knowledge form classification rules training data 
presence large number features especially irrelevant misleading knowledge difficult comprehend humans 
conversely learned rules small number relevant features concise easier understand humans 
presents feature subset selection problem automated design pattern classifiers 
feature subset selection problem refers task identifying selecting useful subset features represent patterns larger set mutually redundant possibly irrelevant features different associated measurement costs risks 
example scenario significant practical interest task selecting subset clinical tests different financial cost diagnostic value associated risk performed part medical diagnosis task 
examples feature subset selection problem include large scale data mining applications power system control zhou construction user interest profiles text classification yang sensor subset selection design autonomous robots balakrishnan honavar 
rest organized follows section summarizes various approaches feature subset selection 
section describes approach uses genetic algorithm neural network pattern classifiers 
section explains implementation details experiments 
section presents results various experiments designed evaluate performance approach benchmark classification problems document classification task 
section concludes summary discussion directions research 
related number approaches feature subset selection proposed literature 
see siedlecki sklansky langley dash liu surveys 
approaches involve searching optimal subset features criteria interest 
feature subset selection problem viewed special case feature weighting problem 
involves assigning real valued weight feature 
weight associated feature measures relevance significance classification task cost salzberg punch wettschereck 
restrict weights binary valued feature weighting problem reduces feature subset selection problem 
focus feature subset selection 
performance measure evaluate feature subset respect criteria interest cost accuracy resulting classifier 
feature subset selection problem essentially optimization problem involves searching space possible feature subsets identify optimal near optimal respect feature subset selection algorithms broadly classified categories characteristics search strategy employed 
feature subset selection genetic algorithm feature subset selection exhaustive search approach candidate feature subsets evaluated respect performance measure optimal feature subset exhaustive search 
focus algorithm almuallim dietterich employs breadth search algorithm find minimal combination features sufficient construct hypothesis consistent training examples 
algorithm proposed uses minimum description length criterion rissanen select optimal feature subset exhaustive enumeration evaluation candidate feature subsets 
exhaustive search computationally infeasible practice rare instances total number features quite small 
feature subset selection heuristic search exhaustive search possible subsets feature set computationally feasible practice number authors explored heuristics feature subset selection conjunction branch bound search technique known combinatorial optimization cormen artificial intelligence russell norvig 
forward selection backward elimination common sequential branch bound search algorithms feature subset selection narendra fukunaga devijver sklansky fukunaga 
forward selection starts empty feature set adds feature time stage choosing addition increases backward elimination starts entire feature set step drops feature absence decreases forward backward selection procedures optimal stage unable anticipate complex interactions features affect performance classifier 
related approach called exchange strategy starts initial feature subset forward selection backward elimination tries exchange feature selected subset features outside 
find feature subset guaranteed best size feature subset considering possible subsets branch bound search narendra fukunaga assume monotone 
adding features guaranteed decrease worth pointing practical pattern classification scenarios monotonicity assumption satisfied 
example addition irrelevant features social security numbers medical records diagnosis task significantly worsen generalization accuracy decision tree classifier quinlan 
furthermore feature subset selection techniques rely monotonicity performance criterion appear reasonably linear classifiers exhibit poor performance non linear classifiers neural networks ripley 
systematic search find feature subset consistent training data forward selection reliability measure reported schlimmer 
greedy hillclimbing procedures different sequential search methods obtaining generalization decision tree construction algorithms id quinlan proposed caruana freitag 
related john forward selection backward elimination minimize cross validation error decision tree classifiers quinlan kohavi kohavi hillclimbing best search feature subset selection decision tree classifiers 
koller koller sahami koller sahami forward selection backward elimination select feature subsumed remaining features determined markov blanket set features render selected feature conditionally independent remaining features constructing naive bayesian duda hart mitchell decision tree classifiers quinlan :10.1.1.155.2293
preset algorithm employs rough set theory select feature subset rank ordering features generate minimal decision tree 
class techniques feature subset selection probability error correlation features reported 
feature subset selection randomized search randomized algorithms motwani raghavan randomized probabilistic opposed deterministic steps sampling processes 
researchers explored algorithms feature subset selection 
relief algorithm kira rendell assigns weights features estimated effectiveness classification randomly sampled instances 
features weights exceed user determined threshold selected designing classifier 
extensions relief introduced handle noisy missing features multi category classification kononenko 
randomized hillclimbing search feature subset selection nearest neighbor classifiers cover hart diday dasarathy proposed skalak 
lvf lvw algorithms liu setiono liu setiono randomized algorithms generate random feature subsets pick number patterns space defined feature subset lvf lowest error decision tree classifier lvw giving preference smaller feature subsets 
patterns said feature values different class labels 
authors explored randomized population heuristic search techniques genetic algorithms ga feature subset selection decision tree nearest neighbor classifiers siedlecki sklansky brill punch rule induction systems vafaie de jong 
related approach lateral feedback networks guo evaluate feature subsets guo uhrig 
feature subset selection techniques employ genetic algorithms require restrictive monotonicity assumption 
readily lend multiple selection criteria classification accuracy feature measurement cost 
particularly attractive design pattern classifiers practical scenarios 
filter wrapper approaches feature subset selection feature subset selection algorithms classified categories feature selection done independently learning algorithm construct classifier 
feature selection performed independently learning algorithm technique said follow filter approach 
said follow wrapper approach john 
filter approach generally computationally efficient wrapper approach major drawback optimal selection features may independent inductive representational biases learning algorithm construct classifier 
wrapper approach hand involves computational overhead evaluating candidate feature subsets executing selected learning algorithm dataset represented feature subset consideration 
feasible learning algorithm train classifier relatively fast 
summarizes filter wrapper approaches 
approach feature subset selection proposed instance wrapper approach 
utilizes genetic algorithm feature subset selection 
feature subsets evaluated computing generalization accuracy optionally cost features neural network classifier constructed computationally efficient neural network learning algorithm called distal yang 
feature selection genetic algorithm neural network pattern classifiers feature subset selection context practical problems diagnosis presents instance multi criteria optimization problem 
multiple criteria optimized include accuracy classification cost risk associated classification turn depends selection features describe patterns 
genetic algorithms offer particularly attractive approach multi criteria optimization 
neural networks offer attractive framework design trainable pattern classifiers real world real time pattern classification tasks account potential parallelism feature subset selection genetic algorithm feature subset selection features wrapper approach algorithm learning feature subset features generation features evaluation feature subset selection filter approach performance algorithm learning feature subset optimal feature subset optimal performance approaches feature subset selection incorporation learning algorithm 
features selected independently learning algorithm filter approach feature subsets generated evaluated learning algorithm wrapper approach 
fault noise tolerance gallant honavar ripley mitchell honavar honavar 
genetic algorithms generally quite effective rapid global search large search spaces difficult optimization problems neural networks offer particularly attractive approach promising solutions identified 
attractive explore combinations global local search techniques solution difficult design optimization problems mitchell 
background genetic algorithms feature subset selection design neural network pattern classifiers clearly interest 
explores wrapper multi criteria approach feature subset selection genetic algorithm conjunction relatively fast inter pattern distance neural network learning algorithm called distal 
general approach inductive learning algorithm 
interested reader referred honavar langley mitchell honavar honavar surveys different approaches inductive learning 
genetic algorithms evolutionary algorithms goldberg holland koza fogel michalewicz mitchell include class related randomized population heuristic search techniques include genetic algorithms goldberg holland mitchell genetic programming koza evolutionary programming fogel variety related approaches michalewicz mitchell 
inspired processes modeled biological evolution 
central evolutionary systems idea population potential solutions individuals corresponds members high dimensional search space 
individuals represent candidate solutions optimization problem solved 
wide range genetic representations bit vectors lisp programs matrices encode individuals depending space solutions needs searched 
genetic algorithms goldberg michalewicz mitchell individuals typically represented bit binary vectors 
resulting search space corresponds dimensional boolean space 
feature subset selection problem individual represent feature subset 
assumed quality candidate solution fitness individual population evaluated fitness function 
feature subset selection problem fitness function evaluate selected features respect criteria interest cost resulting classifier classification accuracy classifier 
case essentially function defined earlier 
evolutionary algorithms form fitness dependent probabilistic selection individuals current population produce individuals generation 
variety selection techniques explored literature 
common ones fitness proportionate selection rank selection tournament selection goldberg michalewicz mitchell 
selected individuals subjected action genetic operators obtain new individuals constitute generation 
genetic operators usually designed exploit known properties genetic representation search space optimization problem solved 
genetic operators enable algorithm explore space candidate solutions 
see balakrishnan honavar discussion desirable properties genetic representations operators 
mutation crossover commonly operators genetic algorithms represent individuals binary strings 
mutation operates single string generally changes bit random 
string may consequence random mutation get changed 
crossover hand operates parent strings produce offspring 
randomly chosen crossover position strings yield offspring result crossover 
genetic representations matrices lisp programs require appropriately designed genetic operators michalewicz mitchell 
process fitness dependent selection application genetic operators generate successive generations individuals repeated times satisfactory solution search fails 
shown evolutionary algorithms sort outlined simulate highly opportunistic exploitative randomized search explores high dimensional search spaces effectively certain conditions holland 
practice performance evolutionary algorithms depends number factors including choice genetic representation operators fitness function details fitness dependent selection procedure various user determined parameters population size probability application different genetic operators specific choices experiments reported summarized section 
neural networks neural networks densely connected massively parallel serial networks relatively simple computing elements neurons gallant honavar ripley mitchell honavar honavar 
neuron computes relatively simple function inputs transmits outputs neurons connected output links 
variety neuron functions practice 
neuron associated set parameters modifiable learning 
commonly parameters called weights 
computational capabilities pattern classification abilities neural network depend architecture connectivity functions computed individual neurons setting parameters weights 
known multi layer networks non linear computing elements threshold neurons realize classification function oe oe finite set classes finite number discrete real valued attributes set real numbers finite set discrete values 
attributes non numeric nominal mapped numeric values appropriate coding scheme 
function computed neural network determined topology computations performed individual neurons designing neural network particular pattern classification task reduces determination network architecture number neurons connectivity types neurons linear sigmoid threshold feature subset selection genetic algorithm parameter weight values 
typically accomplished combination design priori knowledge guesswork inductive learning may modify things weights network architecture gallant honavar uhr honavar parekh honavar 
genetic algorithm wrapper approach feature subset selection neural network pattern classifiers practical considerations genetic algorithms offer attractive technique feature subset selection neural network pattern classifiers reasons mentioned 
faced difficulties approach practice 
traditional neural network learning algorithms backpropagation perform error gradient guided search suitable setting weights weight space determined user specified network architecture 
ad hoc choice network architecture inappropriately constrains search appropriate setting weights 
example network fewer neurons necessary learning algorithm fail find desired classification function 
network far neurons necessary result overfitting training data leading poor generalization 
case difficult evaluate usefulness feature subset employed describe represent training patterns train neural network 
gradient learning algorithms mathematically founded unimodal search spaces get caught local minima error function 
complicate evaluation feature subset employed represent training patterns train neural networks 
due fact poor performance classifier due failure learning algorithm feature subset 
fortunately constructive neural network learning algorithms gallant honavar uhr honavar eliminate need ad hoc inappropriate priori choices network architectures potentially discover near minimal networks size commensurate complexity classification task implicitly specified training data 
new provably convergent relatively efficient constructive learning algorithms multi category real discrete valued pattern classification tasks begun appear literature yang parekh parekh yang honavar 
algorithms demonstrated performance terms reduced network size learning time generalization number experiments artificial fairly large real world datasets 
honavar uhr parekh yang 
exception distal yang time consuming iterative training algorithms setting weights neurons 
genetic algorithms feature subset selection design neural network pattern classifiers involves running genetic algorithm generations 
generation evaluation individual feature subset requires training corresponding neural network computing accuracy cost 
evaluation performed individuals population 
feasible computationally expensive iterative weight update algorithms training neural network classifiers evaluating candidate feature subsets 
background distal offers attractive approach training neural networks 
distal fast algorithm constructing neural network pattern classifiers distal yang simple relatively fast constructive neural network learning algorithm pattern classification 
results experiments neural networks constructed distal 
key idea distal add hidden neurons time greedy strategy ensures hidden neuron correctly classifies maximal subset training patterns belonging single class 
correctly classified examples eliminated consideration 
process terminates pattern set empty network correctly classifies entire training set 
happens training set linearly separable transformed space defined hidden neurons 
fact possible set weights hidden output neuron generate pool apply initial population genetic operators evaluate feature candidate subsets feature new pool rank selection individual best candidate subsets values fitness distal feature subset selection genetic algorithm distal 
starting initial population candidates having different feature subsets new populations generated repeatedly previous ones applying genetic operators crossover mutation selected parents evaluating fitness values offsprings distal ranking fitness values 
best individual obtained generation 
connections going iterative process 
straightforward show distal guaranteed converge classification accuracy finite training set time polynomial number training patterns yang 
experiments reported yang show distal despite simplicity yields classifiers compare quite favorably generated sophisticated substantially computationally demanding learning algorithms 
distal attractive choice experimenting evolutionary approaches feature subset selection neural network pattern classifiers 
key steps approach shown 
implementation details explained earlier genetic algorithm search optimization problem requires choice representation encoding candidate solutions manipulated genetic algorithm definition fitness function evaluate candidate solutions definition selection scheme fitness proportionate selection definition suitable genetic operators transform candidate solutions explore search space setting user controlled parameters probability applying particular genetic operator size population experiments run genetic algorithm goldberg mitchell rank selection strategy 
probability selection highest ranked individual user specified parameter second highest ranked individual gamma third highest ranked individual gamma ranked individual gamma sum probabilities selection individuals 
rank selection strategy gives non zero probability selection individual mitchell 
experiments parameter settings population size number generation probability crossover probability mutation probability selection highest ranked individual feature subset selection genetic algorithm parameter settings results preliminary runs 
comparable typical values mentioned literature mitchell 
individual population represents candidate solution feature subset selection problem 
total number features available choose represent patterns classified 
medical diagnosis task observable symptoms set possible diagnostic tests performed patient 
represented binary vector dimension total number features 
bit means corresponding feature selected 
value indicates corresponding feature selected 
fitness individual determined evaluating neural network constructed distal training set patterns represented selected subset features 
individual bits turned corresponding neural network input nodes 
fitness function combine different criteria accuracy classification function realized neural network cost performing classification 
accuracy classification function estimated calculating percentage patterns test set correctly classified neural network question 
number different measures cost classification suggest cost measuring value particular feature needed classification cost performing necessary test medical diagnosis application risk involved keep things simple chose criteria fitness function defined follows fitness accuracy gamma cost accuracy cost max fitness fitness feature subset represented accuracy test accuracy neural network classifier trained distal feature subset represented cost sum measurement costs feature subset represented cost max upper bound costs candidate solutions 
case simply sum costs associated features 
clearly somewhat ad hoc choice 
discourage trivial solutions zero cost solution low accuracy selected reasonable solutions yield high accuracy moderate cost 
ensures fitness cost max 
practice defining suitable tradeoffs multiple objectives knowledge domain 
general non trivial task combine multiple optimization criteria single fitness function 
wide variety approaches examined utility theory literature keeney raiffa 
experiments description datasets experiments reported wide range real world datasets machine learning data repository university california irvine murphy aha carefully constructed artificial dataset bit parity explore feasibility genetic algorithms feature subset selection neural network classifiers 
feature subset selection distal applied document classification problem journal abstracts news articles 
bit parity dataset 
dataset constructed explore effectiveness genetic algorithm selecting appropriate subset relevant features presence redundant features minimize cost maximize accuracy resulting neural network pattern classifier 
modified training set constructed follows original features replicated introduce redundancy doubling number features 
additional set irrelevant features generated assigned random boolean values 
bit random vectors generated augmented bit vectors corresponding original bits plus identical set bits 
feature resulting dataset assigned random cost 
performance considering random costs addition accuracy see equation compared obtained considering accuracy 
datasets uci repository 
experiments real world datasets objective compare neural networks built feature subsets selected genetic algorithm table datasets experiments 
size number patterns dataset features number input features class number output classes 
dataset size features feature type class bit parity problem numeric annealing database annealing numeric nominal audiology database audiology nominal pittsburgh bridges bridges numeric nominal breast cancer cancer numeric credit screening crx numeric nominal flag database flag numeric nominal glass identification glass numeric heart disease heart numeric nominal heart disease cleveland numeric nominal heart disease hungarian numeric nominal heart disease long beach numeric nominal heart disease swiss numeric nominal hepatitis domain hepatitis numeric nominal horse colic horse numeric nominal ionosphere structure ionosphere numeric pima indians diabetes pima numeric dna sequences promoters nominal sonar sonar numeric large soybean soybean nominal vehicle silhouettes vehicle numeric house votes votes nominal vowel recognition vowel numeric wine recognition wine numeric zoo database zoo numeric nominal abstracts numeric abstracts numeric news articles reuters numeric news articles reuters numeric news articles reuters numeric entire set features available 
table summarizes characteristics datasets 
medical datasets include measurement costs features datasets lack information 
experiments datasets uci repository focused identifying minimal subset features yield high accuracy neural network classifiers 
measurement costs available performance considering cost addition accuracy compared obtained considering accuracy 
document datasets 
abstracts chosen different sources ieee expert magazine journal artificial intelligence research neural computation 
news articles obtained reuters dataset 
document represented form vector numeric weights words terms vocabulary 
weights correspond term frequency inverse document frequency tfidf salton mcgill yang values corresponding words 
training sets abstracts generated classification corresponding documents classes interesting interesting different individuals resulting different data sets classifications news articles topics classes koller sahami resulting different datasets reuters reuters feature subset selection genetic algorithm reuters respectively 
datasets summarized table 
datasets measurement costs features experiments document datasets focused identifying minimal subset features yield high accuracy neural network classifiers 
experimental results different sets experiments run explore performance 
set experiments designed explore effect feature subset selection performance distal choice training test sets 
dataset randomly partitioned training test set data training remaining testing 
genetic algorithm select best feature subset basis choice training test sets 
results averaged independent runs genetic algorithm choice training test set 
process repeated times different choices training test set 
results experiments represent theta runs genetic algorithm shown table 
entries tables give means standard deviations form mean sigma standard deviation 
second set experiments explored somewhat different related question 
feature subset selection guided fitness function reasonable expect quality fitness estimates impact performance distal 
interesting explore performance fitness estimates obtained training test sets 
set experiments fitness estimates obtained averaging observed fitness values different partitions data training test sets 
reported results represent averages independent runs algorithm 
results shown table 
improvement generalization feature subset selection 
study effect feature subset selection generalization experiments run classification accuracy fitness function 
results table indicate networks constructed subset features compare quite favorably networks features randomly partitioned datasets 
particular feature subset selection resulted substantial improvement generalization datasets 
example accuracy yielded promoters zoo datasets 
number features selected significantly smaller total number features original data representation datasets 
results shown table indicate networks constructed ga selected subset features comparable networks features datasets fold cross validation 
clearly outperformed plain distal features parity problem sense successfully selected important features giving generalization 
remaining datasets improvement generalization ranged modest cases marginal 
best individual generated outperformed distal datasets 
number features selected significantly smaller total number features original data representation datasets 
table compares results results ga non ga approaches available literature liu setiono liu setiono kohavi kohavi koller sahami koller sahami :10.1.1.155.2293
indicates result reported corresponding 
results indicate gave higher generalization accuracy techniques comparable accuracy cases vehicle dataset occasionally selected features 
produced feature subsets larger number features approach koller sahami koller sahami reuters datasets :10.1.1.155.2293
explained feature subsets genetic algorithm datasets relatively large number features set number features select priori 
noted generally feasible completely fair thorough comparison different approaches complete knowledge parameters set experiments 
table comparison neural network pattern classifiers constructed distal entire set features best network constructed ga selected subsets features randomly partitioned datasets 
features number features accuracy generalization accuracy obtained neural networks 
reported accuracy distal obtained fold cross validation represents averages runs genetic algorithm partitions dataset runs partition 
see section 
details 
distal dataset features accuracy features accuracy sigma sigma sigma annealing sigma sigma sigma audiology sigma sigma sigma bridges sigma sigma sigma cancer sigma sigma sigma crx sigma sigma sigma flag sigma sigma sigma glass sigma sigma sigma heart sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma hepatitis sigma sigma sigma horse sigma sigma sigma ionosphere sigma sigma sigma pima sigma sigma sigma promoters sigma sigma sigma sonar sigma sigma sigma soybean sigma sigma sigma vehicle sigma sigma sigma votes sigma sigma sigma vowel sigma sigma sigma wine sigma sigma sigma zoo sigma sigma sigma sigma sigma sigma sigma sigma sigma reuters sigma sigma sigma reuters sigma sigma sigma reuters sigma sigma sigma minimizing cost maximizing accuracy feature subset selection 
selection generalization accuracy measurement cost features 
see fitness function equation 
bit parity problem cleveland heart disease hepatitis domain pima indians diabetes datasets experiment random costs bit parity problem 
results shown table randomly partitioned fold cross validation datasets respectively 
see table fitness function combined accuracy cost outperformed accuracy respect number features generalization accuracy cost 
surprising tries minimize cost maximizing accuracy reduces number features emphasizes accuracy 
table shows fitness function combined accuracy cost outperforms accuracy datasets 
generalization accuracy higher cost higher fitness function accuracy feature subset selection genetic algorithm table comparison neural network pattern classifiers constructed distal entire set features best network constructed fitness estimates fold cross validation 
best represents mean standard deviation accuracy best network produced fold crossvalidation independent runs genetic algorithm 
average represents mean standard deviation computed independent runs genetic algorithm accuracy best network produced 
see section details 
distal average best dataset features accuracy features accuracy features accuracy sigma sigma sigma sigma annealing sigma sigma sigma sigma audiology sigma sigma sigma sigma bridges sigma sigma sigma sigma cancer sigma sigma sigma sigma crx sigma sigma sigma sigma flag sigma sigma sigma sigma glass sigma sigma sigma sigma heart sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma hepatitis sigma sigma sigma sigma horse sigma sigma sigma sigma ionosphere sigma sigma sigma sigma pima sigma sigma sigma sigma promoters sigma sigma sigma sigma sonar sigma sigma sigma sigma soybean sigma sigma sigma sigma vehicle sigma sigma sigma sigma votes sigma sigma sigma sigma vowel sigma sigma sigma sigma wine sigma sigma sigma sigma zoo sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma reuters sigma sigma sigma sigma reuters sigma sigma sigma sigma reuters sigma sigma sigma sigma dataset 
explains fitness function equation works verifies rationale 
note runs resulted feature subsets necessarily minimum cost 
suggests possibility improving results principled choice fitness function combines accuracy cost 
summary discussion approach feature subset selection genetic algorithm neural network pattern classifiers proposed 
fast inter pattern distance constructive neural network algorithm distal employed evaluate fitness terms generalization accuracy candidate feature subsets genetic algorithm 
results indicate genetic algorithms offer attractive approach solving feature subset selection problem different cost performance constraints inductive learning pattern classifiers general neural network pattern classifiers particular 
table comparison various approaches feature subset selection 
column non ga shows best performance non ga approaches cited section liu setiono liu setiono kohavi kohavi koller sahami koller sahami second column adhoc shows performance reported column shows performance approach :10.1.1.155.2293
non ga adhoc dataset features accuracy features accuracy features accuracy annealing cancer crx glass heart hepatitis horse pima sonar vehicle votes reuters reuters reuters table comparison performance neural network pattern classifiers constructed features selected accuracy vs features selected accuracy cost randomly partitioned datasets 
accuracy accuracy cost dataset features accuracy cost features accuracy cost hepatitis pima table comparison performance neural network pattern classifiers constructed features selected accuracy vs features selected accuracy cost datasets arranged fold cross validation 
accuracy accuracy cost dataset features accuracy cost features accuracy cost hepatitis pima ga approach feature subset selection rely monotonicity assumptions traditional approaches feature selection limits applicability real world classification knowledge acquisition tasks 
offers natural approach feature subset selection account distribution available data 
due fact feature selection driven estimated fitness values multiple partitions dataset training test data provide robust measure performance feature subset 
generally case greedy stepwise algorithms select features single partition data training test sets 
consequently feature subset selection genetic algorithm feature subsets selected algorithms perform poorly random partitions data training test sets 
approach feature subset selection able naturally incorporate multiple criteria accuracy cost feature selection process 
finds applications cost sensitive design classifiers tasks medical diagnosis computer vision 
interesting application automated data mining knowledge discovery datasets abundance irrelevant redundant features 
cases identifying relevant subset adequately captures regularities data particularly useful particularly scientific knowledge discovery tasks 
techniques similar discussed successfully select feature subsets pattern classification tasks arise power system security assessment zhou sensor subsets design behavior control structures autonomous mobile robots balakrishnan honavar balakrishnan honavar balakrishnan honavar 
additional experiments scientific knowledge discovery tasks bioinformatics discovery protein structure function relationships carcinogenicity prediction gene sequence identification currently progress 
directions research include extension feature subset selection incorporating feature construction genetic programming koza extensive experimental feasible theoretical comparison performance proposed approach conventional methods feature subset selection principled design multi objective fitness functions feature subset selection domain knowledge mathematically founded tools multi attribute utility theory keeney raiffa 
acknowledgments research partially supported national science foundation iri john foundation honavar 
authors wish mehran sahami providing reuters document datasets 
authors grateful dr pazzani department information computer science university california irvine managing repository machine learning datasets making available 
earlier version appears ieee expert 
fl ieee 
almuallim dietterich 

learning boolean concepts presence irrelevant features 
artificial intelligence 
balakrishnan honavar 

properties genetic representations neural architectures 
proceedings july washington volume pages 
balakrishnan honavar 

analysis designed simulated evolution 
proceedings international conference neural networks washington balakrishnan honavar 

sensor evolution robotics 
koza goldberg fogel riolo editors proceedings genetic programming conference gp pages 
mit press cambridge ma 
balakrishnan honavar 

experiments evolutionary synthesis robotic 
proceedings world congress neural networks pages san diego ca 
nordin keller 

genetic programming 
morgan kaufmann palo alto ca 
brill brown martin 

fast genetic selection features neural network classifiers 
ieee transactions neural networks 
caruana freitag 

greedy attribute selection 
proceedings eleventh international conference machine learning pages new brunswick nj 
morgan kaufmann 
cormen leiserson rivest 

algorithms 
mit press cambridge ma 
cost salzberg 

weighted nearest neighbor algorithm learning symbolic features 
machine learning 
cover hart 

nearest neighbor pattern classification 
ieee transactions information theory 
dasarathy 

nearest neighbor nn norms nn pattern classification 
ieee computer society press los alamitos ca 
dash liu 

feature selection classification 
intelligent data analysis 
devijver 

pattern recognition statistical approach 
prentice hall 
diday 

progress distance similarity measures pattern recognition 
proceedings second international joint conference pattern recognition pages 


evaluation feature selection methods application computer security 
technical report cse department computer science university california davis ca 
duda hart 

pattern classification scene analysis 
wiley new york 
fogel 

evolutionary computation new philosophy machine intelligence 
ieee press piscataway nj 
sklansky 

feature selection automatic classification non gaussian data 
ieee transactions systems man cybernetics 
fukunaga 

statistical pattern recognition 
academic press new york 
gallant 

neural network learning expert systems 
mit press cambridge ma 
goldberg 

genetic algorithms search optimization machine learning 
addisonwesley new york 
guo 

nuclear power plant fault diagnostics thermal performance studies neural networks genetic algorithms 
phd thesis university tennessee knoxville tn 
guo uhrig 

genetic algorithms select inputs neural networks 
proceedings pages 


fundamentals artificial neural networks 
mit press boston ma 
holland 

adaptation natural artificial systems 
mit press cambridge ma 
honavar 

learning systems integrate multiple strategies representations 
honavar uhr editors artificial intelligence neural networks steps principled integration pages 
academic press new york 
honavar 

machine learning principles applications 
webster editor encyclopedia electrical electronics engineering 
wiley new york 
appear 
honavar 

structural learning 
webster editor encyclopedia electrical electronics engineering 
wiley new york 
appear 
honavar uhr 

generative learning structures generalized connectionist networks 
information sciences 
john kohavi pfleger 

irrelevant features subset selection problem 
proceedings eleventh international conference machine learning pages new brunswick nj 
morgan kaufmann 
keeney raiffa 

decisions multiple objectives preferences value tradeoffs 
wiley new york 
kira rendell 

practical approach feature selection 
proceedings ninth international conference machine learning pages 
morgan kaufmann 
kohavi 

feature subset selection search probabilistic estimates 
aaai fall symposium relevance 
kohavi 

useful feature subsets rough set reducts 
third international workshop rough sets soft computing 
koller sahami 

optimal feature selection 
machine learning proceedings thirteenth international conference 
morgan kaufmann 
koller sahami 

hierarchically classifying documents words 
proceedings international conference machine learning pages 
kononenko 

estimating attributes analysis extension relief 
proceedings european conference learning pages 


lateral connections feed forward neural networks 
proceedings international conference neural networks pages 
feature subset selection genetic algorithm koza 

genetic programming programming computers means natural selection 
mit press cambridge ma 
langley 

selection relevant features machine learning 
proceedings aaai fall symposium relevance pages new orleans la aaai press 
langley 

elements machine learning 
morgan kaufmann palo alto ca 
liu setiono 

feature selection classification probabilistic wrapper approach 
proceedings ninth international conference industrial engineering applications ai es 
liu setiono 

probabilistic approach feature selection filter solution 
proceedings thirteenth international conference machine learning 
morgan kaufmann 
michalewicz 

genetic algorithms data structures evolution programs 
springerverlag new york third edition 
mitchell 

genetic algorithms 
mit press cambridge ma 
mitchell 

machine learning 
mcgraw hill new york 


feature selection rough sets theory 
proceedings european conference machine learning pages 
springer 
motwani raghavan 

randomized algorithms 
acm computing surveys 


comparison techniques choosing subsets pattern recognition 
ieee transactions computers 
murphy aha 

repository machine learning databases 
department information computer science university california irvine ca 
narendra fukunaga 

branch bound algorithm feature subset selection 
ieee transactions computers 
parekh yang honavar 

constructive neural network learning algorithms multi category real valued pattern classification 
technical report isu cs tr department computer science iowa state university 
parekh yang honavar 

constructive neural network learning algorithm multi category pattern classification 
proceedings ieee inns international conference neural networks icnn pages 


rough sets theoretical aspects reasoning data 
kluwer academic 
punch goodman pei chia shun 

research feature selection classification genetic algorithms 
proceedings international conference genetic algorithms pages 
springer 
quinlan 

programs machine learning 
morgan kaufmann san mateo ca 


performing effective feature selection investigating deep structure data 
proceedings second international conference knowledge discovery data mining pages 
aaai press 
ripley 

pattern recognition neural networks 
cambridge university press new york 
rissanen 

modelling shortest data description 
automatica 
russell norvig 

artificial intelligence modern approach 
prentice hall englewood cliffs nj 
salton mcgill 

modern information retrieval 
mcgraw hill new york 
schlimmer 

efficiently inducing determinations complete systematic search algorithm uses pruning 
proceedings tenth international conference machine learning pages amherst ma 
morgan kaufmann 
dom niblack 

modelling approach feature selection 
proceedings tenth international conference pattern recognition pages 
siedlecki sklansky 

automatic feature selection 
international journal pattern recognition 
siedlecki sklansky 

note genetic algorithms large scale feature selection 
ieee transactions computers 
skalak 

prototype feature selection sampling random mutation hill climbing algorithms 
proceedings eleventh international conference machine learning pages new brunswick nj 
morgan kaufmann 
vafaie de jong 

robust feature selection algorithms 
proceedings ieee international conference tools artificial intelligence pages 
wettschereck aha mohri 

review empirical evaluation feature weighting methods class lazy learning algorithms 
technical report aic naval research laboratory navy center applied research artificial intelligence washington yang pai honavar miller 

mobile intelligent agents document classification retrieval machine learning approach 
th european meeting cybernetics systems research 
symposium agent theory agent implementation vienna austria 
yang parekh honavar 

constructive neural network learning algorithm multi category pattern classification 
proceedings world congress neural networks pages san diego 
yang parekh honavar 

distal inter pattern distance constructive learning algorithm 
proceedings international joint conference neural networks anchorage alaska 
appear 
zhou honavar 

power system security margin prediction radial basis function networks 
proceedings th annual north american power symposium wyoming 
feature subset selection genetic algorithm yang graduate student computer science iowa state university 
current research interests include intelligent agents data mining knowledge discovery machine learning neural networks pattern recognition evolutionary computing 
holds computer science university seoul korea computer science iowa state university currently working ph computer science iowa state university 
yang member aaai ieee 
honavar associate professor computer science neuroscience iowa state university 
current research interests include artificial intelligence intelligent agents machine learning data mining knowledge discovery neural evolutionary computing bioinformatics 
dr honavar holds electronics engg 
bangalore university india electrical computer engg 
drexel university ph degrees computer science university wisconsin madison 
dr honavar member aaai acm ieee 
