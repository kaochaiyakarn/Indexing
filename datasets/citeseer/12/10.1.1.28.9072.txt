ieee transactions pattern analysis machine intelligence vol 
pp 
pca versus lda mart kak robot vision lab school electrical computer engineering purdue university ecn purdue edu context appearance paradigm object recognition generally believed algorithms lda linear discriminant analysis superior pca principal components analysis communication show case 
case intuitively plausible arguments showing actual results face database 
training dataset small pca outperform lda pca sensitive different training datasets 
keywords face recognition pattern recognition principal components analysis linear discriminant analysis learning undersampled distributions small training datasets 
computer vision systems reported literature employ appearance paradigm object recognition 
primary advantage appearance methods necessary create representations models objects object model implicitly defined selection sample images object 
appearance methods usually represent image size theta pixels vector delta dimensional space 
practice delta dimensional spaces large allow robust fast object recognition 
common way attempt resolve problem dimensionality reduction techniques 
popular techniques purpose principal components analysis pca linear discriminant analysis lda known fisher discriminant analysis fda 
pca face recognition recognition human object recognition industrial robotics mobile robotics :10.1.1.12.7580
lda face recognition mobile robotics 
lda proposed generic object recognition results large database objects reported 
late tendency prefer lda pca intuition suggest deals directly discrimination classes deals data entirety principal components analysis paying particular attention underlying class structure 
tendency vision community subject examination 
pca lda pca lda different classes embedded different gaussian distributions 
sample class supplied learning procedure pca lda 
classification result pca procedure eigenvector desirable result lda 
represent decision thresholds obtained nearest neighbor classification 
show switch pca lda may warranted may lead faulty system design especially size learning database small 
claim carries intuitive plausibility established help fig 

shows learning instances marked circles crosses class underlying unknown distribution shown dotted curve 
data account pca compute vector largest variance associated 
shown vertical line labeled pca 
hand lda compute vector best discriminates classes 
vector shown diagonal line labeled lda 
decision thresholds yielded nearest neighbor approach cases marked seen manner decision thresholds intersect ellipses corresponding class distributions pca yield superior results 
examples depicted fig 
quite convincing regard claim lda superior pca bear burden establishing claim help actual data 
rest help face databases ar face database publicly available data set 
additional evidence support claim draw attention reader results september feret competition 
particular wish point lda results obtained university maryland compare respect standard pca approach described 
notable characteristic data experiments learning samples class system 
course expect large representative learning datasets lda outperform pca 
simply confirm intuitive show results ar database faces 
database sample size class learning larger case feret competition 
example database show lda outperforming pca images different facial shots corresponding different expressions illumination conditions occlusions subject 
cast system design employed previously cited contributions 
claim validity size learning database insufficiently large non uniformly distributed 
note feret deals large number classes number classes issue 
main concern problems caused insufficient data class available learning 
localization morphing morph image localize boundaries face shown overlaid horizontal vertical lines left image 
locate basic features nose shown overlaid line running nose image eyes shown white dots eyes right image 
shown different examples morphed faces 
localization morphing face images comparing pca lda regard identification faces independently localization scale related issues 
manually carried localization step followed morphing step face occupies fixed size array pixels 
formally consider set sample images number columns number rows image number ng 
manually localize left right top bottom limits face left right eyes nose shown fig 

localization faces morphed fit grid size 
figs 
show final results morphing different subjects 
shown morphing eye centers medial line nose arc lips join pixel coordinates images 
refer new images thetam dimensions morphed image 
images segmented means oval shaped mask centered middle morphed image rectangle 
pixels oval vectorized dimensional vector corresponds number pixels oval shaped segment reading pixel values oval segment raster scan manner 
vectors obtained manner sample images denoted fx xn pca space dimensional vector representation face principal component analysis pca find subspace basis vectors correspond maximum variance directions original space 
represent linear transformation maps original dimensional space dimensional feature subspace normally new feature vectors defined columns eigenvalues obtained solving decomposition qe xx covariance matrix eigenvalue associated eigenvector obtaining eigenvectors vectors normalized kx system invariant intensity illumination source ii average images subtracted normalized vectors ensure eigenvector highest eigenvalue represents dimension eigenspace variance vectors maximum correlation sense 
covariance matrix normally large easy computation eigenvectors 
fortunately ways get difficulty proposed 
pentland empirically shown superior face recognition results achieved eigenvectors eigenvectors represent changes illumination 
shown elimination eigenvectors general worsen results 
analyze elimination eigenvectors affects recognition performance 
lda space linear discriminant analysis lda searches vectors underlying space best discriminate classes best describe data 
formally number independent features relative data described lda creates linear combination yields largest mean differences desired classes 
mathematically speaking samples classes define measures called class scatter matrix sw gamma gamma ith sample class mean class number classes number samples class ii called class scatter matrix gamma gamma represents mean classes 
goal maximize class measure minimizing class measure 
way maximize ratio advantage ratio proven sw non singular matrix ratio maximized column vectors projection matrix eigenvectors gamma noted easy prove gamma nonzero generalized eigenvectors upper bound gamma ii require samples guarantee sw singular impossible realistic application 
solve propose intermediate space 
cases intermediate space chosen pca space 
original dimensional space projected intermediate dimensional space pca final dimensional space lda 
experimental results results section obtained ar face database 
database consists color images frontal images faces subjects 
different images subject 
subject images recorded different sessions separated weeks session consisting images 
illustration images subject shown fig 

images taken camera tightly controlled conditions illumination viewpoint 
image database consists theta array pixels pixel represented bits rgb color values 
experiments reported section different individuals males females randomly selected database 
stated earlier images morphed final theta pixel arrays segmented oval shaped mask converted gray level images adding color channels 
ar database face images publicly available ecn purdue edu html images subject ar face database 
images taken session images different session 
small training data sets discussed small non representative training data set guarantee lda outperform pca 
justified purely intuitive grounds help fig 

subsection study effect real data images ar face database 
simulate effects small training data set results images person training testing 
subsection non occluded images recorded sessions 
example subject shown fig 
images labeled 
unoccluded images subject obviously different ways total selecting training testing 
different ways separating data training testing parts results reported 
different training testing datasets created manner described applied pca ii pca eigenvectors iii lda 
testing carried nearest neighbor algorithm standard norm euclidean distance 
datasets indexed test results th dataset represented test fig 
shown results test test test 
horizontal coordinate represents parameter recall section dimensionality final subspace face identification takes place 
stated earlier lda need specify value parameter dimensionality intermediate space described section 
obviously value chosen strongly affect face recognition results 
order fair comparison pca lda value tried possible values low maximum possible value 
lda results shown fig 
value value yielded best recognition rate 
chose test test test display represents different type method pca pca lda table table summarizes results ways dividing data training testing subsets 
value dimensionality parameter 
top row shows number cases basic pca outperformed algorithms middle row number cases pca eigenvectors best row number cases lda best 
comparative performance algorithms tested 
performance curves test typical datasets pca outperformed lda 
performance curves test typical datasets pca proved superior lda values dimensionality inferior 
performance curves test typical datasets lda outperformed pca 
database yield different results surprising 
going back fig 
difficult visualize altered locations training samples shown get decision thresholds show lda outperforming pca lda pca yielding clear separation underlying class distributions 
fig 
focussed low dimensional spaces want comparison discriminant features lda case descriptive sense packing energy features pca case 
matter hold highdimensional spaces need achieving sufficiently high recognition rates demanded practical face recognition systems 
shown fig 
results similar fig 
number dimensions large 
test cases shown fig 
chosen reflected pca outperforming lda test lda outperforming pca test third conclusively outperforming test number dimensions increased 
case reader wondering lda curves go dictated considerations ffl dimensionality lda upper bounded gamma number classes rank gamma matrix 
classes gives upper bound dimensionality lda space 
ffl dimensionality underlying pca space lda space carved allowed exceed gamma total number samples available 
prevent sw singular 
samples classes dimensionality underlying pca space allowed exceed 
sense extract dimensional lda subspace dimensional pca space arbitrarily hard limited dimensionality lda space 
table summarizes results cases training testing datasets case 
table case high dimensionality 
value dimensionality parameter top row shows number cases basic pca outperformed algorithms middle row number cases pca eigenvectors best row number cases lda outperformed pca 
interesting note table limit dimensionality final subspace roughly pca including pca eigenvectors expected outperform lda method pca pca lda table table high dimensional spaces 
just frequently way 
established dataset test results shown fig 
expect pca outperform lda regardless value dimensionality parameter stretch imagination say pca outperforms lda important learn general appearance face training samples supplied best discriminate faces different subjects 
high dimensional spaces draw comparable lda greater chance outperforming pca data set 
note applied specific data set experiments reported 
may entirely different different data set 
observation wish suppression eigenvectors improves performance pca presence illumination variations lda transform usually better small training datasets 
reason pca eigenvectors wins infrequently 
observation regarding small training datasets relative behavior pca lda dimensionality parameter larger 
performance transforms gets better value increases 
different recognition rate pca saturates different datasets performance lda vary widely 
experiments discussion recognition rate obtained lda varied low high 
representative samples class previous subsection showed unequivocally number training samples class small possible pca outperform lda 
subsection reinforce intuition naturally suggest number learning samples large representative class lda outperform pca 
study subsection carried images subject ar database 
means subject shown fig 
images labeled 
mentioned earlier thirteen taken session training 
thirteen taken second session testing 
experiments described preceding subsection lda algorithm chose value dimensionality parameter gave best final classification results value dimensionality final subspace classification carried 
recall intermediate subspace needed implementation lda 
value tried values low maximum allowed value 
best final results usually obtained small values explained basis fact number samples learning technique needs proportional dimensionality dataset 
implies order obtain results lda total number face recognition circles referred problem recognizing duplicates 
duplicate image face taken different time weeks months 
duplicate images testing taken roughly illumination conditions similar occlusions original set 
facial expressions nearly 
shown performance curves different ways dividing data training set testing set 
performance curves high dimensional case 
results obtained algorithms individuals classes 
samples larger equivalently larger value fig 
shows results 
expected lda outperforms pca large representative training dataset 
appearance methods widely object recognition systems 
paradigm pca lda demonstrated useful applications face recognition 
think lda outperform pca deals directly class discrimination empirical evidence suggests 
discusses reasons seemingly anomalous behavior 
fig 
illustrates pca outperform lda number samples class small training data non uniformly sample underlying distribution 
practical domains especially domain face recognition knows advance underlying distributions different classes 
argue practice difficult ascertain available training data adequate job 
experiments report validate claim 
experiments show superiority pca lda show superiority lda pca 
pca outperforms lda number training samples class small atypical data sizes previously researchers 
hespanha kriegman eigenfaces vs fisherfaces recognition class specific linear projection ieee transactions pattern analysis machine intelligence pami 
discriminant analysis recognition human face images journal optics american 
fisher statistical utilization multiple measurements annals 
fukunaga statistical pattern recognition second edition academic press 
kirby sirovich application karhunen loeve procedure characterization human faces ieee transactions pattern analysis machine intelligence pami 
mart ar face database cvc technical report june 
mart recognition partially occluded imprecisely localized faces probabilistic approach proc 
computer vision pattern recognition vol 
pp 
hilton head june 
moon phillips analysis pca face recognition algorithms empirical evaluation techniques computer vision bowyer phillips eds ieee computer science 
moghaddam pentland probabilistic visual learning object representation ieee transactions pattern analysis machine intelligence pami 
murakami vijaya kumar efficient calculation primary images set images ieee transactions pattern analysis machine intelligence pami 
murase kimura miyake improvement auto correlation matrix pattern matching method application transactions 
murase nayar visual learning recognition objects appearance international journal computer vision 
nayar nene murase subspace methods robot vision ieee transactions robotics automation ra 
pentland starner turk experiments eigenfaces looking people workshop ijcai 
phillips moon rauss rizvi feret evaluation methodology face recognition algorithms proceedings international conference audio video biometric person montana switzerland 
sirovich kirby low dimensional procedure characterization human faces opt 
soc 
amer 

swets weng discriminant eigenfeatures image retrieval ieee transactions pattern analysis machine intelligence pami 
turk pentland eigenfaces recognition journal cognitive neuro science 
weng comprehensive visual learning early visual learning nayar poggio eds pp 
oxford university press 

