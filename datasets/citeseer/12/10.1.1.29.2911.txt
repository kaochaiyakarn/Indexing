blind separation mixture independent sources maximum likelihood approach pham member philippe laboratory modeling computation imag univ grenoble grenoble cedex france preprint submitted ieee trans 
signal processing preliminary version appeared rapport de recherche rr imag may acknowledgment authors jutten fruitful discussions nguyen thi provided speech data referees carefully read manuscript suggested improvements 
keywords separation sources 
maximum likelihood 
independence 
higher order moments 
linear filters 
classification numbers sp 
sp sp propose methods separating mixtures independent sources precise knowledge probability distribution 
obtained considering maximum likelihood solution corresponding distributions sources relaxing assumption afterward 
method specially adapted temporally independent non gaussian sources nonlinear separating functions 
second method specially adapted correlated sources distinct spectra linear separating filters 
theoretical analysis performance methods 
simple procedure choosing optimally separating functions linear space functions proposed 
second method simple implementation simultaneous diagonalization symmetric matrices provided 
numerical simulation results illustrating performance method agreement experiments theory 
deals problem blind separation mixture independent sources precise knowledge probability distribution 
problem important applications signal processing speech analysis radar sonar processing 
methods proposed literature higher order moment statistics 
approach consider maximum likelihood solution corresponding hypothetical model sources shall relax 
temporally independent white sources leads general statistics nonlinear functions data 
similar algorithm derived ad hoc manner appeared jutten authors separating functions number sources 
choosing functions priori authors performance study derived method choosing optimally linear space functions 
results appeared condensed form 
propose adaptive method order follow changes mixture weights 
maximum likelihood principle applied source density approximated gram charlier expansion cumulants fourth order 
leads solution essentially fourth order cumulant method uses general nonlinear statistics 
practice sources rarely white separation procedure works exploits independence different sources 
may expect method exploiting temporal dependency sources perform better 
introduce method likelihood account correlation structure sources assumed stationary leads set linear separating filters nonlinear separating functions 
note new procedure exploits second order lagged covariance structure data able separate mixture gaussian sources provided distinct spectra 
second order statistics requires little computations 
particular case sources procedure reduces simultaneous diagonalization covariance matrices iterative computation necessary 
simple separation procedure works general case performance optimal 
generally derived expression asymptotic covariance matrix estimators lowest asymptotic bound achievable 
observation record denoted random vector components xk corresponding observation channels 
underlying assumption linear combination independent sources unknown matrix fs stationary sequence random vectors components sk sequences fs mutually independent 
source separation problem reconstruct sources observations 
separation called blind independence sources probability model 
simplify problem shall sequel restrict case number sources equals number sensors 
note observation vector lies linear subspace dimension ir change basis gamma components vanish led back case practice situation complicated observations may corrupted additive noises case setup similar factor analysis considered psychometric literature see ex 

factor analysis factors sources noises assumed gaussian adopted method second order yields factors loading matrix orthogonal rotation 
feature distinguishes factor analysis problem identification separation problem 
high respect identify loading matrix noise variances factor scores sources reconstructed best estimate conditional expectations observed channels contain contamination noises 
maximum likelihood approach white sources number parameters factor model gamma gamma matrix loading account indeterminacy associated rotation noise variances 
identification elements sample covariance matrix observations gamma gamma gamma derivation separation procedure section shall assume sources white sense different times independent identically distributed 
assumption may unrealistic working assumption order write likelihood derive separation procedure 
shall see method justified 
write likelihood needs density functions sources unknown blind context 
reason inherent scaling ambiguity problem see shall assume moment densities known scale factor 
source density delta oe oe known oe unknown 
density oe oe related logarithm density data written assuming invertible lt tf ln gamma oe oe gamma ln det denotes time average operator fg delta delta delta denotes th column identity matrix order denotes transpose 
expression function unknown parameter log likelihood function maximum likelihood method consists maximizing 
may equate partial derivatives lt zero find convenient differential 
da gamma gammaa gamma daa gamma ln det aj tr gamma da tr denoting trace get gamma dlt gamma oe gamma daa gamma oe gamma tr gamma da gamma oe gamma oe gamma oe doe gamma ln prime denotes derivative 
denoting ij general element gamma da terms right hand side gamma oe gamma oe ij gamma ii maximum likelihood estimators oe oe dlt vanish infinitesimal increments da equivalently ij doe yields estimating equations gamma oe gamma oe gamma oe gamma note parameter set fa oe oe redundant post multiplying diagonal matrix dividing oe corresponding diagonal elements change likelihood 
estimated post multiplication diagonal matrix possibly permuted merely reflects scaling permutation ambiguities problem see 
gamma equations fact determine scaling scaling convention adopted uniquely equations merely serve estimate oe interest values depend scaling convention just drop equations 
hand densities sources unknown practice take function priori absorb factor oe 
proposed separation procedure consists solving respect system gamma gamma just equations gamma solve scaling factor column 
applications scaling convention adopted matrix unique 
note choosing priori procedure longer maximum likelihood 
procedure justified sources centered possess stationarity ergodicity property assumed 
assumptions asymptotically satisfied replaced true mixing matrix scaling permutation matrix left hand sides converge expectations oe oe scaling factors expectations vanish zero mean independent similar ad hoc procedure proposed jutten 
adaptive procedure easily changed batch processing amounts solving gamma gamma odd functions 
approach leads identity function different functions restricted odd source 
asymptotic properties estimator noted separation source procedure provide estimator mixing matrix set estimators related post multiplication permuted diagonal matrix 
permutation scaling convention adopted get unique estimator natural permutation convention permute columns denoting ij general element largest pk running permutations scaling convention simple having unit diagonal elements conventions may making sample covariance matrix gamma having unit diagonal elements 
note scaling procedure truly maximum likelihood chosen proportional logarithm true density th source 
permutation scaling ambiguities separation procedure needs produce close true gamma close permuted diagonal matrix 
permute scale convention may expect ffi gamma gamma small 
taylor expansion equations relation gamma gamma ffia gamma gamma gamma ffi ij ffi ij general element ffi gamma th source possibly scaled permuted result permutation scaling convention applied 
left hand side approximatively equals gamma ffi ik ffi jk converge expectation vanish ffi ij ffi ji expectation operator 
write ffi form gamma vector delta ffi ffi delta delta delta psi psi psi delta delta delta psi ij equation written delta psi block diagonal matrix blocks ij indexed pairs hand central limit theorem psi asymptotically gaussian mean zero covariance matrix psi psi temporally independent 
assumption simplify computation central limit theorem applied ergodic sequences satisfying mild conditions complicated analysis 
delta asymptotically gaussian covariance matrix gamma gh gammat provided invertible 
sequel notation gammat short hand gamma note block diagonal random variables zero mean case example symmetric distributions odd functions 
case random vectors ffi ij ffi ji different pairs asymptotically independent covariance matrix gamma ij ij gammat ij ij derived asymptotic distribution diagonal terms matrix ffi 
distribution depends unknown mixing matrix sources characteristics reason consider ffi gamma reason ffi ij directly related set natural performance indexes called contamination coefficients completely characterize separation procedure 
right hand side equation represents reconstructed th source written gamma ffi ii gamma ij ij ffi ij gamma ffi ii 
ij represents contamination th source reconstructed th source called contamination coefficient 
ffi ii permuted scaled convention ij asymptotic distribution ffi ij note source may scaled different variates powers 
preferable relative contamination coefficient ij oe oe oe denoting variance th source 
fact maximum likelihood estimator asymptotically efficient mild conditions obtain asymptotic covariance matrix estimator minimum chosen proportional gamma ln true density th source 
argument quite rigorous dropped scaling equations likelihood equations introduced scaling convention leading restriction diagonal elements ffi 
give brief direct proof result 
note integration parts differentiable function result useful 
checked psi psi psi psi psi psi 
non negativity matrix left hand side gamma hj gamma yielding gamma gh gammat gamma easily seen equality attained taken proportional announced result 
note matrix block diagonal 
derive explicitly asymptotic covariance matrix ffi ffi delta delta delta terms source characteristics 
exists decoupling different pairs channels mild assumption see need compute pair asymptotic covariance matrix gamma ij ij gammat ij ffi ij ffi ji interesting look asymptotic covariance matrix relative contamination coefficients ffi ij oe oe introduce dimensionless parameters ae oe oe denoting ij diagonal matrix diagonal elements oe oe ij ij ae ae ae ae ij ij ij ae ae ae ae oe oe oe oe asymptotic covariance matrix ffi ij oe oe ffi ji oe oe seen ae ae ae ae gamma ae ae ae ae ae ae ae ae gamma simple algebra yields matrix equals gamma ae ae ae ae gamma ae ae ae ae gamma ae ae ae ae gamma ae ae ae ae gamma ae ae diagonal elements matrix represent mean square relative th source reconstructed th source vice versa diagonal element covariance 
indexes depend dimensionless parameters ae ae true asymptotic covariance ffi ij ffi ji significant simplifications results arise ae happens chosen proportional orthogonal projection linear space functions containing identity function particular 
case ffe ffe real number ff 
equalities consequences definition seen reduce ae case ae ae ae ae ae gamma ae ae ae ae ae ae gamma ae ae gamma ae ae ae ae ae ae gamma ae ae ae ae ae ae gamma gammaae ae gamma ae ae asymptotic covariance matrix ffi ij oe oe ffi ji oe oe reduces gamma ae ae ae gammaae ae gammaae ae ae ae gamma ae gamma gamma equality shows case ae asymptotic covariance matrix estimator increases ae lower bound asymptotic covariance matrix estimator attained chosen proportional sides equality ae replaced ae different simplification obtained case signals distribution 
oe oe oe asymptotic covariance matrix ffi ij ffi ji reduces gamma ae gamma ae ae gamma ae gamma gamma ae ae computation asymptotic covariance matrix ffi ij ffi ji reduces ae gamma ae gammaae gammaae ae gamma ae gamma gamma matrix ae ae depend constitutes lower bound asymptotic covariance matrix estimator 
construction estimator algorithm finding solution similar fisher scoring technique maximizing likelihood newton raphson iteration see ex described 
expanding initial estimate computation section step estimate denoted gamma ffi gamma constructed solution linear system ffi ik ffi jk gamma ffi ij general element ffi 
close may approximate vanishes vanishes system equations may replaced ffi ij ffi ji system defines diagonal elements ffi diagonal ones fact arbitrary long small enter equations multiplication vanish 
recommend put zero apply scaling convention new estimate amounts rescaling rows gamma ffi 
prevents new estimator large small 
procedure may iterated convergence 
algorithm easily modified adaptive context estimate updated time new observation available 
gamma estimate time gamma 
application iteration leads new estimate time gamma gamma ffi gamma ffi matrix element ffi ij solution ffi ij ffi ji note computed gamma estimate gamma constructed order satisfy approximately likelihood equation data gamma 
gamma close 
making approximation right hand side reduces yielding algorithm gamma gamma omega ii omega ii gamma omega ij omega ij gamma ffi ij ffi solution omega ij ffi ij omega ii ffi ji forgetting factor introduced track possible evolution mixing matrix choice separating function optimal choice depends unknown distribution source easily estimated shall restrict choice linear space functions spanned basis oe oe oe coefficients goal minimize asymptotic covariance matrix estimator equivalently maximize ij gamma ij ij ij ij defined section 
result shows done quite easily projection 
proposition assume basis function oe identity function 
matrices ij gamma ij ij maximized choices oe chosen proportional orthogonal projection space spanned oe oe explicitly optimal choice corresponds solution linear system oe oe oe proof matrices ij ij written psi ij psi ij psi ij psi ij psi ij psi ij defined similarly place assumption oe may take oe identity function 
psi ij ij phi ij ij delta delta delta delta delta delta delta delta delta delta delta delta jn phi ij oe oe oe oe hand orthogonal projection psi ij psi ij linear space spanned components psi ij satisfies definition psi ij phi ij psi ij phi ij 
equality ij gets psi ij psi ij psi ij psi ij ij yielding psi ij psi ij psi ij psi ij ij ij ij psi ij psi ij non negativity matrix left hand side ij gamma ij ij psi ij psi ij equality achieved psi ij psi ij gets result proved psi ij form ij phi ij ij defined way ij jn place jn suffices prove matrix equality ij phi ij phi ij psi ij phi ij 
observe elements row matrix left hand side equality oe oe oe oe independence fact oe elements reduce oe oe oe oe yields desired matrix equality far row concerned 
proof similar row 
result provides simple way estimate optimal choice equivalently coefficients left hand sides replaced oe 
replacing expectation operator time average operator gets system estimating equations maximum likelihood approach temporally correlated sources derivation separation procedure known technique decorrelate stationary signal perform discrete fourier transform transforms dx gammai tk gamma known dx soon dx converge limit spectral density process complex conjugate 
random vectors asymptotically gaussian mild assumptions 
reason usual practice time series literature regard fdx independent gaussian random vectors 
note dx dx gammak concerned real signal 
done particular deconvolution problem having similarities problem considered 
higher number channels sources indexed pair single noise corrupted problem fact kind factor analysis problem mentioned section frequency domain 
note practice quite justified ignores higher second order structure process 
flaw comes fact mentioned convergence holds fixed finite subset frequency index uniformly 
practice fully justified gaussian signals 
stress meant derive system estimation equations full justification coming performance study 
take joint probability distribution dx dx dx det exp gamma gamma dx gamma dx gamma det exp gamma gamma dx ji denotes matrix spectral density process 
note dx dx real dx complex gaussian vectors 
expression joint density odd quite similar terms corresponding dx simply disappear 
diag gk spectral densities sources diag delta denote diagonal matrix indicated diagonal element logarithm joint density reduces lt gamma gamma je gamma dx ln gamma ln det aj constant logarithm joint density data linear map unit jacobian dx 
moment working assumption spectral densities sources known constant 
known functions unknown parameters 
maximum likelihood approach consists maximizing lt respect section shall consider differential function 
da gamma gammaa gamma daa gamma ln det aj tr gamma da get dlt gamma gamma daa gamma dx gamma dx gamma tr gamma da gamma je gamma dx gamma introducing general element ij matrix gamma da terms right hand side rewritten gamma gamma dx gamma dx ij gamma ii maximum likelihood estimators differential lt vanish identically infinitesimal increments da dk arguments 
yields estimating equations gamma gamma dx gamma dx gamma je gamma dx second set equations merely serves determine scale factors interest 
estimated mixing matrix determined scale factor set 
simpler revert time domain slightly modifying estimating equations 
approximating sums integrals written approximately gamma dx gamma dx gamma note gammai fourier series coefficients convolution linear filter oe frequency response gamma convention 
parseval equality equations written denoting convolution operator gamma oe gamma time average operator section 
note filter oe may infinite impulse response oe involve tg put zero 
case oe finite impulse response fir may choose restrict averaging smaller range oe range computed 
interpretation equation clear estimator reconstructed sources gamma transformed linear filter oe empirically uncorrelated sense time average operator reconstructed sources 
practice spectral densities sources unknown take oe choose priori 
procedure justified seen interpretation exploits fact filtered version source uncorrelated sources time average product processes converges expectation time interval increases infinity 
note gamma estimating equations just estimate matrix scaling 
course choice filters oe affect performance method 
section evaluate asymptotic performance show highest filters chosen frequency responses proportional inverses spectral densities sources 
derive equivalent forms 
oe gamma denote impulse response filter oe noting put zero tg written explicitly gamma gammat oe min gammak max gammak gamma gamma gamma rt min gammak max gammak gamma rt jkj denote sample autocovariance matrices observed process estimating equations gamma gammat oe gamma rt gammat system equations particularly interesting oe fir filters oe jkj say case computations done matrices rt rt 
restrict oe form oe oe oe causal oe mirror obtained reversing time order impulse response 
restriction justified fact optimal choice oe filter frequency response inversely proportional spectral density th source written form provided logarithm frequency response integrable frequency response nonnegative written squared modulus sided fourier series 
checked sequences gamma oe gamma oe 
equivalent form oe gamma oe gamma filter oe frequency response function proportional inverse spectral density th source oe fact whitening filter source sources 
case estimation procedure interpreted making reconstructed source whitened whitening filter empirically uncorrelated filtered versions sources filter 
practice spectral densities sources unknown filters guess estimate true whitening filters need really whiten sources 
asymptotic properties estimator shall approach section study asymptotic properties estimator 
write gamma ffi gamma ffi small matrix 
gamma gamma ffi ik gamma ffi ij denotes general element matrix ffi 
right hand side written approximately oe gamma oe ffi jk oe ffi ik assuming ergodicity oe converges oe vanishes equals gamma gamma denoting true spectral density signal denoting frequency response filter oe system equations rewritten approximately ffi ji ffi ij oe simplicity integration variable range dropped 
avoid new notations shall reuse section different similar quantities 
put delta psi gamma vector ffi ffi psi psi psi ij oe 
system equations delta gamma psi bloc diagonal matrix blocks ij indexed pairs hand mild conditions central limit theorem applied average stationary process psi yield psi converges distribution gaussian random vector zero mean covariance matrix lim psi psi compute need auto covariance functions fl fl oe oe processes oe cross covariance function fl oe processes 
mutual independence zero mean stationary processes matrix seen block diagonal blocks ij gamma fl fl fl fl gammak fl fl gammak fl fl indexed pairs spectral density process oe cross spectral density oe parseval equality ij block diagonal ffi ij ffi ji different pairs asymptotically independent covariance matrix gamma ij ij gamma ij show covariance matrix minimum filters oe chosen frequency response gamma inversely proportional true spectral densities sources 
observe matrices ij ij written defining ij ij ij ij ij non negativity matrix left hand side equality gamma ij ij gamma ij gamma ij equality attained proportional corresponding yields announced result 
minimum asymptotic covariance matrix estimator gamma block diagonal matrix blocks ij indexed pairs results derived implicit assumption matrix ij invertible 
order necessary sufficient hand hand proportional ij proportional rows proportional columns 
interestingly asymptotic covariance matrix ffi ij ffi ji depends linear space spanned gamma gamma functions chosen nearly proportional affecting asymptotic performance method numerical instability may arise 
see suppose set filters chosen frequency responses components ij gamma gamma ij non singular matrix response proportional 
simple computation shows new matrices ij ij associated set filters ij ij ij ij ij ij ij asymptotic covariance matrix ffi ij ffi ji remains 
hand essential true spectral densities similar procedure perform 
determinant matrix ij result shown equal gamma gamma diagonal elements matrix gamma ij gamma gamma gamma gamma large constant close integral gamma 
construction estimator general iterative method approach section derive iterative algorithm finding solution 
expanding equation initial estimate computation section step estimate denoted gamma ffi gamma constructed solution oe ffi ik oe ffi jk oe gamma ffi ij denotes general element ffi 
close may approximate oe oe vanishes ffi ij pairwise computed system equations oe ffi ij oe ffi ji oe yields new estimator iterate procedure convergence 
oe fir filters order computations done sample covariance matrices rt rt 
oe sm gamma gammap oe rt gammat em rt gammak 
filter oe form oe oe oe causal filter oe mirror image oe sm oe oe sm discuss modification procedure adaptive context 
gamma estimate aat time gamma 
new estimate time gamma gamma ffi gamma ffi matrix general element ffi ij solution equation analogous omega ij ffi ij omega ii ffi ji ij concerning definition omega ij ij possibilities 
firstly case filters oe fir may define omega ij gamma gamma gammap oe gamma gamma ij gamma gamma gammap oe gamma gamma adaptive estimate auto covariance matrix lag factor gamma defined recursion gamma gamma forgetting factor 
case oe convolution causal filter oe mirror image equivalent procedure consists updating directly matrices gammap oe follows gamma oe oe second possibility consists updating directly omega ij ij omega ij omega ij gamma oe ij oe oe gamma 
approach quite similar adaptive algorithm section 
direct computation interesting particular case estimator computed directly iteration 
happens filters oe symmetric belong linear space dimension 
precisely suppose oe ff ff symmetric filters coefficients ff ff 
estimating equations written ff gamma gamma ff gamma gamma ff gamma gamma ff gamma gamma grouped indices pairs gamma gamma gamma gamma symmetric pair equations takes form ff ff ff ff gamma gamma gamma gamma note row vectors ff ff ff ff proportional filters oe oe proportional frequency responses 
matrix equation equivalent gamma gamma gamma gamma linear filtering operator commutes linear instantaneous transformation gets equivalent gamma gammat introducing matrices equations mean matrices gamma gammat gamma gammat diagonal 
estimator defined condition inverse simultaneously matrices idea joint diagonalization 
problem joint diagonalization symmetric matrices known solution positive definite 
solution positive definite ul gamma ll factorization orthogonal matrix diagonalizing gamma pl gammat eigenvalues matrix zeros det gamma distinct solution unique permutation scaling 
assumption lu ul diagonal matrix distinct diagonal elements 
solution matrix satisfies vv vv diagonal 
row vectors orthogonal orthogonal proportional diagonal elements distinct possible exactly non zero component meaning permuted diagonal matrix 
method generally provides unique closed form estimator positive definite 
positive definiteness guaranteed chooses filter positive frequency response natural choice inverse spectral density positive 
written convolution filter mirror image computation section sample covariance matrix filtered process 
note fir filter linear combination sample covariance matrix 
idea consider linear combination get positive definite matrix 
simplest situation filters oe oe belong linear space dimension symmetric fir filters order 
takes ex filter impulse response sigma gamma sigma 
estimation procedure consists simply finding matrix inverse simultaneously rt rt rt gamma rt gamma rt gamma rt gamma sample covariance matrices processes gamma gamma gamma respectively 
frequency response span space trigonometric polynomials degree simple procedure optimal case spectral densities sources order autoregressive processes 
simple situation joint diagonalization applies sources 
clearly take oe oe led problem diagonalizing simultaneously theta matrices 
simple case solution written explicitly 
symmetric matrices order 
checked matrix jp gamma delta sign gamma pq gamma sign gamma pq gamma jp gamma delta sign denotes sign delta gamma pq gamma pq gamma simultaneously required condition delta delta singular 
proportional condition shown satisfied matrices positive determinant weaker condition positive definite 
proved solution unique pre multiplication permuted diagonal matrix 
fact choose diagonal elements equal greater square root absolute value product diagonal elements th source contributed th sensor 
computational point view interest choose fir filters order say 
matrices computed sample covariance matrices rt rt 
direct method adaptive context updating matrices diagonalizing 
fir filters done updating sample covariance matrices filters factored convolution causal filter mirror image updated directly fact sample covariance matrix filtered process 
numerical simulation results give numerical calculations concerning choice separating functions simulation examples 
simulations intended illustrations method means constitutes empirical performance study topic separate 
choice separating functions study effect choice separating functions consider case sources distribution taken family densities exp shape parameter normalization constant 
reason considering family analytical computation possible covers wide range shapes short tail large long tail small distributions including near uniform distribution large gaussian distribution exponential distribution 
simplicity shall take function 
look lower bound asymptotic variance achieved choice sign jsj gamma 
note jump origin result section proved interpreting twice dirac function twice density origin 
proof complex basically show holds interpretation 
plots bound times sample size versus shape parameter seen bound increases rapidly infinity approaches showing impossible separate gaussian sources known difficult separate nearly gaussian ones 
minimum asymptotic variance estimator ratios lower bound asymptotic variance actual asymptotic variance estimator called efficiency 
plots efficiencies corresponding choices prj prj chosen method projection section opt opt chosen optimally trial obtained method projection optimal 
seen efficiency quite large poor 
efficiency hand poor 
may expect achieve efficiency linear combination seen case coefficient combination chosen optimally 
trial error method projection section optimal identity function missing projection space 
seen method lead catastrophic result near 
choosing optimally linear combination impractical requires knowledge true density sources led consider linear combination functions identity function order apply method projection 
obtained efficiency better approaching wide range value asymptotic efficiencies estimator separating functions opt prj note efficiency curves smooth asymptotic variance diverges infinity 
line iterative algorithm simulation experiments test iterative non adaptive method construction estimator section sources 
illustrates typical result sources uniform distribution mixed separated algorithm oe oe block sample points 
normalize separating matrix gamma rows unit euclidean norm represent diagonal elements coordinates point graph 
dotted curves represent sets solutions equations 
intersections yield estimator gamma unfortunately unique 
ignoring permutation gamma lying quite close true value gamma graph spurious 
iteration may converge right st nd iterations spurious solution rd iteration depending initial value converges fast starts point circle corresponding matrix singular 
resolve ambiguity may compute index dependence sum squares correlations reconstructed sources retain solution dependent 
starting initial estimate usually st iteration nd iteration true value rd iteration examples iterative computation estimator attains right solution 
performance estimator study performance estimator repeated experiments performed 
experiments temporally independent signals identical density generated apply method section separate 
note algorithms transformation invariant data vectors matrix starting point algorithm matrix result multiplied shall take observations non mixed signals look effect staring point 
shall assume initial mixing matrix may disregard situations algorithm converges spurious estimate see 
produces estimate independent starting point performance independent true mixing matrix note direct method section need iteration performance really independent situations considered sources uniform distribution gamma bilateral exponential distribution respectively 
plots square root product sample size mean squares ffi ffi versus quantity approximately constant respect asymptotic analysis 
theoretical asymptotic values computed displayed dotted horizontal lines 
see agreement experimental theoretical results large uniform distribution attained fairly quick hundreds bilateral exponential distribution attained 
cases root mean squares contamination coefficients tend higher asymptotic values smaller uniform uniform exponential exponential simulations results separating white sources simulation experiments concerns separation correlated sources 
generated signals autoregressive ar models gamma gamma gamma gamma independent sequences independent random variables uniform distribution gamma 
applied methods sections separate 
shall take non mixed signals observations 
note method section designed correlated signals include get idea perform 
method section shall optimal filter non optimal linear filter 
table indicates methods descriptions methods description cubic separating function linear filter non optimal oe oe gamma gamma linear filter optimal oe gamma gamma oe gamma gamma speed estimators method computed direct method simultaneous diagonalization described section 
plots square root product mean squares ffi ffi obtained repeated experiments versus quantity approximately constant respect theoretical asymptotic values display dotted horizontal lines simple formula asymptotic mean square ffi ij method separating functions sources correlated 
observe agreement experimental results theory attained fairly moderate hundreds 
previous experiment mean squares contamination coefficients tend higher asymptotic values smaller filter non optimal filter non optimal filter optimal filter optimal cubic function cubic function simulations results separating correlated sources real data example final example performed experiments unrelated segments real speech signal length sec samples sampling frequency hz 
artificially mixed apply method section separate 
signals obviously non stationary shall adaptive implementation direct computation explained section 
separating filters impulse responses gamma gamma respectively forgetting factor taken 
yields effective window size points large corresponds sec 
show original speech signals reconstructed signals mixtures respectively 
plot mixtures direct method depend 
seen figures start time tens original speech signals reconstructed signals mixtures samples adaptive algorithm able separate signals nicely 
near record sources similar algorithm trouble separate 
expected variance estimator infinite spectra sources see section 
example separation acceptable noting time span speech signals similar spectra usually quite short algorithm performs globally 
proposed methods separating mixture independent sources 
nonlinear separating functions linear filters 
method specially adapted temporally independent non gaussian sources second specially adapted correlated sources distinct spectra 
theoretical analysis performance methods choice separating function discussed 
derived simple implementation second method simultaneous diagonalization symmetric matrices 
simulations illustrate performance methods 
local convergence procedure quite global convergence suffers possibility landing spurious estimate 
drawback similar jutten method 
currently working way avoid 
brillinger time series data analysis theory holt rinehart winston new york 
cardoso 
source separation higher order moments proc icassp glasgow scotland may pp 

cardoso 
iterative technique blind source separation fourth order cumulants proc 
eusipco brussels aug pp 

cardoso efficient technique blind separation complex sources proc 
ieee sp workshop higher order statistics lake tahoe pp 

comon comon independence components analysis new concept 
signal processing 
principe des ethodes de de sources fond ees sur les moments ordre sup erieur traitement du signal num sp non lin non pp 

ga eta 
source separation priori knowledge maximum likelihood solution proceedings eusipco barcelona spain pp 

golub loan matrix computations 
johns hopkins university press 
jutten blind separation sources part adaptative algorithm neuromimetic structure signal processing pp 
ruiz source identification solution cumulants proc 
th assp workshop spectral estimation modelling minneapolis usa august pp 

ph adaptive de sources ind par une de proc 
colloque juan les pins france sept pp 

lo eve 
probability theory 
van nostrand princeton 
moreau new self adaptive algorithm source separation constrast functions proc 
ieee sp workshop higher order statistics lake tahoe pp 

pham ph jutten separation mixture independent sources maximum likelihood approach proc 
eusipco brussels aug pp 

pham ph 
de sources corr el ees proc 
colloque juan les pins france sept pp 

rosenblatt stationary sequences random fields birkhauser boston 
multivariate observation wiley new york 
shumway der deconvolution multiple time series 
technometrics 
tong soon huang liu indeterminacy identifiability blind identification ieee trans 
circuits systems pp 
tong inouye liu waveform preserving blind estimation multiple independent sources ieee trans 
sp pp 

yellin weinstein criteria signal separation ieee trans 
sp pp 

