probabilistic kernel regression models tommi jaakkola david haussler department computer science university california santa cruz ca cse ucsc edu introduce class flexible conditional probability models techniques classification regression problems 
existing methods generalized linear models support vector machines subsumed class 
flexibility class techniques comes kernel functions support vector machines generality dual formulations standard regression models 
support vector machines linear maximum margin classifiers exploiting idea kernel function 
kernel function defines embedding examples high infinite dimensional feature vectors allows classification carried feature space explicitly representing 
support vector machines non probabilistic classifiers extended formalized probabilistic settings topic 
identify new formulations statistical methods gaussian processes 
defining class kernel regression techniques binary classification establish connection methods provide practical measure assessing generalization performance methods 
subsequently extend results sequential bayesian estimation 
provide theorem governing general kernel reformulation probabilistic regression models 
binary classification start considering gaussian process classifiers fully bayesian methods 
define set zero mean jointly gaussian random variables fz corresponding example classified 
assume covariance cov variables kernel function corresponding examples need kernel function strictly positive definite case 
assume binary sigma labels fs generated probabilities jz oe example oe gammaz gamma logistic function 
example vectors fx specify gaussian variables fz subsequently passed transfer functions yield probabilities labels 
similar similar labels assigned input vectors close sense kernel 
training set labels fs example vectors fx principle compute posterior distribution latent gaussian variables fz assigning labels unknown examples gaussian variable corresponding new example correlated fz constrained fixed training labels 
calculations involved procedure typically infeasible 
trying maintain full posterior distribution latent gaussian variables may settle map configuration assign label new example probability oe 
definition gives generic formulation procedure terms dual parameters 
dual parameters arise legendre transformations concave functions see concave functions case classification losses log 
consider transformations detail 
definition define kernel regression classifier classification technique properties example vector method predicts maximum probability label ac cording rule sign labeled training examples non negative coefficients kernel function positive semi definite 
coefficients weighting training examples classification rule obtained maximizing gamma subject potential function delta continuous concave strictly concave kernel positive semi definite 
assumptions positive semi definite kernel function strictly concave continuous potential functions delta introduced primarily ensure solution maximization problem unique 
practice solution achieved monotonically successively updating individual coefficient gamma holding fixed 
solutions dimensional equations relatively easy find kernel method class 
optimal solution characterized fixed point equations reminiscent mean field equations 
consider examples gain insight nature generality class 
support vector machines example seen realizations class simply setting see 
generalized linear models seen members class 
example consider logistic regression model probabilities labels jx oe prior distribution parameters zero mean gaussian 
input vector associate new variable defines conditional probability label note support vector machines bias term added explicitly classification rule treated separately optimization problem 
formulation bias term realized indirectly additive constant kernel magnitude constant specifies prior variance bias term 
put setting support vector machines assume flat prior consequently definitions agree far constant term kernel appropriately large 
jx oe 
gaussian prior assumption new variables fz jointly gaussian random variables covariance matrix cov ef sigmax sigma prior covariance 
logistic regression problem equivalent particular gaussian process classifier 
consequently map estimation parameters logistic regression models corresponds exactly map gaussian process formulation definition 
relation map estimation definition generally theorem 
note potential function logistic regression case binary entropy function gamma log gamma gamma log gamma see appendix kernel function covariance function cov eq 

kernel function discuss properties kernel function eq 

interpreting kernel function covariance function gaussian process classifier suggests treating similarity function 
sense examples similar associated labels priori positively correlated 
simple inner product necessarily measure similarity example possible example similar example 
typically kernel function simple inner product examples inner product feature vectors corresponding examples 
example eq 
feature vectors oe sigma valid kernel function reduced simple inner product possibly infinite dimensional feature vectors 
feature mapping non linear kernel define reasonable similarity measure original example space property doesn hold feature space 
going logistic regression models gaussian process classifiers prior covariance matrix sigma original parameters plays special role specifying inner product eq 

words prior covariance matrix directly changes metric example space 
metric inverse natural parameter space sigma gamma inverse relation follows general property riemannian metrics dual coordinate systems 
change kernel function assumptions concerning examples similarity metric properties change 
suggests modeling effort classifiers go finding appropriate kernel function 
example derive kernels generative probability models directly encode invariances kernel function 
measure generalization error definition provides large class techniques relatively restrictions choice kernel function 
compensate flexibility provide means assessing generalization performance order able limit complexity final classifier appropriate level 
emphasis practical measures 
support vector machines attain sparse solutions sense coefficients set zero result optimization 
computationally attractive property yields direct assessment generalization expected ratio number non zero coefficients number training examples bounds true generalization error 
applicability measure limited support vector machines probabilistic classifiers generally attain sparse solutions making sparsity measure vacuous 
lemma provides general cross validation measure applies kernel classifiers definition lemma training set fs examples labels kernel regression classifier definition leave crossvalidation error estimate classifier bounded step gammas coefficients optimized training examples 
step functions lemma count number times sign training label disagrees sign prediction examples 
include missing th terms predictions error estimate reduce training error cf 
prediction rule definition 
cross validation error bound costly evaluate training error obviously requires retraining 
accuracy bound note case support vector machines shown result provides slightly better estimate sparsity bound proof lemma appendix sparsity bound principle defined bayesian formulation map formulation kernel function remains fixed regardless nature number training examples 
contrast full bayesian approach kernel function modified observations 
precisely formulation prior distribution parameters specifies simple inner product examples bayesian setting roughly speaking inner product defined terms posterior distribution 
full bayesian approach unfortunately feasible cases possible employ approximate methods updating kernel function observations 
approaches proposed purpose including laplace approximation context multi class regression variational methods 
approach complementary sense provide recursive variational approach avoids need simultaneously optimizing large number variational parameters discussed 
bayesian logistic regression consider bayesian formulation logistic regression models 
start briefly reviewing variational approximation technique enables estimate posterior distribution parameters models 
subsequently extend approximate solution kernel functions 
bayesian estimation principle update parameter distribution sequentially example time jd jx jd gamma oe jd gamma set examples observed time constrain general formulation bit assuming prior distribution jd parameters multivariate gaussian possibly arbitrary covariance structure 
assumption sequential updating feasible terms able represent true posterior distribution opens way closed form approximate solution 
employ variational terms essential support vectors just non zero coefficients 
improve estimate difficult evaluate practice 
transformation logistic function oe oe exp phi gamma gamma psi oe adjustable parameter known variational parameter 
inserting approximation oe back sequential update equation eq 
obtain jd oe jd gamma transformed logistic function oe delta quadratic function argument exponent follows gaussian prior jd gamma result gaussian posterior approximation 
mean covariance gaussian related mean covariance prior sigma sigma gamma gamma sigma gamma sigma gamma sigma sigma gamma gamma gamma set examples observed far 
variational parameter defines extent covariance matrix updated defines sigma gamma tanh 
set variational parameter improve accuracy approximation 
suitable error measure approximation derived fact variational transformation introduces lower bound 
approximation fact yields lower bound likelihood conditional observation jx jx gamma oe jd gamma oe jd gamma integral computed closed form 
maximization bound yields fixed point equation sigma solved iteratively note sigma depend equations 
approximations possible laplace approximation 
kernel extension order able employ kernels context bayesian calculations reduce calculations input examples appropriate inner products 
inner products replaced arbitrary positive semi definite kernels 
define priori inner product sigma valid prior covariance positive definite 
simplicity assume mean gaussian prior parameters zero 
consequently remains show sequential updating scheme carried referring value form inner products sigma adopt compact representations posterior mean time dependent kernel sigma clear necessary consider predictive quantities express update equations mean covariance terms new quantities 
consider eq 
covariance update formula 
prior post multiplying update formula respectively definition obtain gamma gamma gamma gamma satisfy simple recurrence relation connects back priori kernel sigma derive recurrence relation similar way see appendix giving gamma gamma gamma gamma assumption prior mean zero values rooted kernels observed labels iterations coefficients gamma tanh need specified 
words need able optimize variational parameter terms gamma gamma order preserve recurrence relations 
starting fixed point equation eq 
get details appendix sigma gamma gamma gamma note appears right hand side expressions 
follows optimize process absorbing new observation need know gamma gamma values computed stored efficiently illustrated section 
efficient implementation due form dependencies recurrence relations carry computations sequential estimation procedure efficiently compactly 
show proceed inductively 
assume lower diagonal matrix vector form 
gamma theta gamma constructed observed examples 
absorb new training example evaluate predictive probability need able optimize variational parameter associated example 
consider fixed point equation 
required quantities corresponding diagonal component matrix component vector respectively 
start computing quantities filling row kernels 
consequently apply recurrence relation eq 
replace values 
note replace values reverse order due dependence structure recurrence relation 
manner fill ft ultimately get time 
computed directly starting recurrence relation eq 
give time 
generic kernel regression definition discussion previous sections generalized multi class continuous response setting 
theorem jx conditional probability model discrete continuous variable finite real vector inputs denotes parameters 
assume jx jz zm values log zm jointly concave continuously differentiable function zm prior distribution parameter vectors zero mean multivariate gaussian block diagonal covariance matrix sigma diag sigma sigma sigma 
training set fy conditional probability model corresponding maximum posteriori map set parameters form jx map zm sigma coefficients attain unique maximum value gamma sigma potential functions legendre transformations classification loss functions log jz zm min gamma log jz zm strong assumption continuous differentiability easily relaxed piece wise differentiability proof appendix legendre transformations theorem easy compute typical cases conditional piece wise continuously differentiable functions obtained limits differentiable functions 
probabilities jz zm softmax functions zy zy 
listed examples appendix inner products sigma appearing dual formulation replaced valid kernel function gaussian kernel note definition stronger assumptions legendre transformations positive definiteness kernel function unique solution terms new parameters purpose prediction possible non uniqueness immaterial resulting predictive distribution remains unique 
concavity objective function assures solution relatively easy find cases 
discussion classification regression problem necessary select appropriate representation examples model parameter estimation method 
focused deriving generic class probabilistic regression models parameter estimation technique arbitrary kernel functions 
allows greater flexibility specifying probabilistic regression models various complexity levels fear local minima 
obtain quick assessments generalization performance 
issue concerning choice kernel function equivalently representation examples addressed burges 

geometry invariance kernel methods 
appear advances kernel methods support vector learning 
mit press 
jaakkola jordan 
variational approach bayesian logistic regression problems extensions 
proceedings sixth international workshop artificial intelligence statistics 
jaakkola haussler 

exploiting generative models discriminative classifiers 
available www cse ucsc edu research ml publications html 
mackay 
gaussian processes 

available wol ra phy cam ac uk mackay 
gaussian kernel gammafi gammax sigma gammax gibbs mackay 

variational gaussian process classifiers 
draft manuscript available ftp wol ra phy cam ac uk mackay 
mccullagh nelder 

generalized linear models 
london chapman hall 
rockafellar 

convex analysis 
princeton univ press 
smola 
general cost functions support vector regression 
australian congress neural networks 
spiegelhalter lauritzen 
sequential updating conditional probabilities directed graphical structures 
networks 
vapnik 

nature statistical learning theory 
springer verlag 
wahba 

spline models observational data 
cbms nsf regional conference series applied mathematics 
wahba 

support vector machines reproducing kernel hilbert spaces randomized 
university wisconsin madison technical report tr rr 
williams barber 

bayesian classification gaussian processes 
manuscript preparation 
proof cross validation bound consider case th example removed training set case optimize remaining coefficients gammat maximizing jd gammat gammat gamma assumptions solution unique denote gammat consider adding th example back training set optimal setting coefficients gammat naturally change need optimized jointly assess effect adding th example perform joint optimization follows 
fix value resulted joint optimization call remaining coefficients gammat obtained reduced objective function terms solely omitted 
th example included remaining coefficients gammat obtained maximizing jd gammat jd gammat gammat gamma gammat maximizing coefficients 
clearly jd gammat jd gammat expanding side eq 
rearranging terms get jd gammat gammat gamma jd gammat gammat fact jd gammat gammat jd gammat gammat coefficients gammat maximize jd gammat delta 
divide term eq 
get desired result sign term indicates correctness cross validation prediction positive correct lower bound term appears lemma uses coefficients optimized training examples 
note th example classifier lemma holds trivially 
recurrence relation start simplifying posterior mean update sigma gamma gamma sigma gamma sigma gamma theta theta sigma gamma gamma gamma gamma gamma gamma sigma gamma gamma sigma gamma sigma gamma gamma gamma gamma sigma gamma sigma gamma gamma gamma gamma sigma gamma fact gamma sigma gamma see definition text 
terms write result gamma gamma gamma gamma fixed point equation objective transform fixed point equation form explicates dependence right hand side applying recurrence relation find gamma gamma gamma gamma gamma gamma gamma gamma independent depends gamma 
similarly expand gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma dependence 
combining results gives gamma gamma gamma proof theorem training set examples fy map parameter solution obtained maximizing penalized likelihood function log jz gamma sigma gamma term log probability observed labels second comes log block diagonal gaussian prior distribution 
omitted terms depend parameters overloaded previous notation sense refers vector fz solution map unique strictly concave owing log prior term 
assumptions log jz jointly concave continuously differentiable function convex duality see get exists function properties log jz min gamma min gamma log jz transformations known legendre transformations function known dual conjugate function 
note conjugate function log jz general different distinct additional 
introduce transformations objective function define gamma gamma sigma gamma dropped associated respect coefficients 
clearly min 
lemma establishes connection theorem lemma objective function theorem concave negative max result implies maximizing objective function theorem equivalent computing min notation 
proof recall implies fixed setting quadratic function parameters 
solve maximizing sigma substitute back giving sigma gamma negative objective function appearing theorem desired 
remains show convex gammaj concave 
note conjugate functions concave gammaf terms convex 
term eq 
corresponds sigma gamma linear function term convex 
additional assumptions may strictly convex solution terms may unique 
min associated remain unique 
definition maximizing means evaluating max min just shown min max corresponds maximizing objective function theorem remains show max min min max 
state lemma lemma eq 
max min min max proof unique maximum left hand side 
finite general fixed max min min max max suffices show exists max 
finiteness continuous differentiability assumption guarantees rz log jz exists shown minimizing coefficients legendre transformations 
consequently minimum attained min minimum vanishes equality gives sufficient guarantees choice max comparing eq 
completes proof 
concavity log conditional probabilities implies sublinear asymptotics 
maximization quadratic prior term eventually dominate 
examples provide examples compute legendre transformations 
consider logistic regression case log jz log oe treating log probability function find legendre transformation max gamma log oe perform maximization take derivative respect set zero gamma oe gammas implies gammas log gamma 
substituting back eq 
get gammas log gamma gamma log gamma delta binary entropy function 
addition change variables longer function objective function theorem expressed terms new reduces form definition 
calculations generalized multiclass setting probability model softmax log jz zm gamma log legendre transformation obtained max gamma log similarly class case find maximum setting respect zero gamma ffi note implies 
solution variables unique constant log ffi gamma constant substituting back transformation gives gamma ffi gamma log ffi gamma surprisingly entropy function 
change variables ffi gamma simplifies transformation longer depends new variables objective function theorem reduces gamma ffi gamma ffi gamma sigma rewrite predictive probability model theorem terms new get zm zy ffi gamma sigma 
