nonlinear prediction chaotic time series support vector machines appear proc 
ieee amelia island fl sep mukherjee edgar osuna federico girosi ai mit edu ai mit edu girosi ai mit edu center biological computational learning massachusetts institute technology cambridge ma method regression proposed vapnik 
technique called support vector machine svm founded mathematical point view provide new insight function approximation 
implemented svm tested data base chaotic time series compare performances di erent approximation techniques including polynomial rational approximation local polynomial techniques radial basis functions neural networks 
svm performs better approaches 
study particular time series variability performance respect free parameters svm 
analyze performance new regression technique called support vector machine 
technique seen new way train polynomial neural network radial basis functions regressors 
main di erence technique conventional regression techniques uses structural risk minimization empirical risk minimization induction principle 
equivalent minimizing upper bound generalization error minimizing training error technique expected perform better conventional techniques 
results show svm promising regression technique order assess reliability performances extensive experimentation need done 
applying svm chaotic time series data sets casdagli test compare performances di erent approximation techniques 
svm technique free parameters 
absence principled way parameters performed experimental study examine variability performance parameters vary reasonable limits 
organized follows 
section formulate problem time series prediction see equivalent regression problem 
section brie review svm approach regression problem 
section chaotic time series benchmark previous regression methods introduced 
section contains experimental results comparison techniques 
section focuses particular series mackey glass examines relation parameters svm generalization error 
time series prediction dynamical systems purpose dynamical system smooth map open set euclidean space 
writing ft map satisfy conditions 

ft fs fs initial condition dynamical system de nes trajectory ft set direct problem dynamical systems consists analyzing behavior properties trajectories di erent initial conditions 
interested problem similar inverse problem stated 
nite portion time series component represents avariable evolving unknown dynamical system 
assume trajectory lies manifold fractal dimension strange attractor 
goal able predict behavior time series 
remarkably done principle knowledge components vector 
fact takens embedding theorem ensures certain conditions smooth map value called embedding dimension smallest value true called minimum embedding dimension map known value time uniquely determined values past 
simplicity notation de ne dimensional vector xn eq 
written simply xn 
observations fx gn time series known knows values function problem learning dynamical system equivalent problem estimating unknown function set sparse data points regression techniques solve problems type 
concentrate support vector algorithm novel regression technique developed vapnik 
support vectors machines regression section sketch ideas support vectors machines svm regression detailed description 
regression problem data set xi yi obtained sampling noise unknown function asked determine function approximates knowledge svm considers approximating functions form dx ci functions gd called features estimated data 
form approximation considered hyperplane dimensional feature space dened functions 
dimensionality feature space necessarily nite examples nite 
unknown coe cients estimated minimizing functional nx yi xi kck constant robust error function de ned yi xi yi xi xi vapnik showed function minimizes functional eq 
depends nite number parameters form nx xi called kernel function describes inner product dimensional feature space dx interesting fact choices set including nite dimensional sets form analytically known simple features need computed practice algorithm relies computation scalar products feature space 
choices kernel available including gaussians tensor product splines trigonometric polynomials 
coe cients obtained maximizing quadratic form nx nx yi nx xi xj subject constraints pn 
due nature quadratic programming problem number coe cients di erent zero data points associated called support vectors 
parameters free parameters theory choice left user 
control vc dimension approximation scheme di erent ways 
clear theoretical understanding missing plan conduct experimental understand role 
benchmark time series tested svm regression technique set chaotic time series test compare approximation techniques 
mackey glass time series considered time series generated mackey glass delay di erential equation dx dt parameters embedding dimensions respectively 
denote time series mg mg 
order consistent initial condition equation sampling rate 
series generated numerical integration fourth order runge kutta method 
ikeda map ikeda map dimensional time series generated iterating map cos sin sin cos 
casdagli considered time series denote ikeda generated fourth iterate map complicated dynamic denoted ikeda 
lorenz time series considered time series associated variable lorenz di erential equation rx xz xy bz 
considered di erent sampling rates generating time series lorenz lorenz 
series generated numerical integration fourth order runge kutta method 
comparison techniques section report results svm time series compare results reported di erent approximation techniques polynomial rational local polynomial radial basis functions basis function neural networks 
cases time series fx generated rst points training remaining points testing 
cases set ikeda set 
data sets 
denoting fn predictor built data points quantity measure generalization error fn fn mx fn xn var var variance time series 
implemented svm minos solver quadratic programming problem eq 

details implementation 
series choose kernel parameters kernel gave smallest generalization error 
consistent strategy adopted 
results reported table 
column table contains results experiments rest table parameters kernels set remaining part section 
mackey glass time series kernel chosen form xi mx sin sin th component dimensional vector integer 
kernel generates approximating function additive trigonometric polynomial degree correspond features trigonometric monomials degree tried various values embedding dimension series mg mg accordance casdagli lorenz time series lorenz lorenz series polynomial kernels order 
embedding dimensions respectively 
value ikeda map spline order kernel value poly rational loc loc rbf net svm mg mg ikeda ikeda lor lor table estimated values log fn svm algorithm various regression algorithms reported 
degrees best rational polynomial regressors superscripts estimates 
loc loc refer local approximation polynomials degree respectively 
numbers parenthesis near svm estimates number support vectors obtained algorithm 
neural networks results missing missing 
sensitivity svm parameters embedding dimension section report observations generalization error number support vectors vary respect free parameters svm choice embedding dimension 
parameters analyze dimensionality feature space embedding dimension results mg series 
demonstrates little ect generalization error plot spans orders magnitude 
parameter little ect number support vector shown gure remains constant range results similar kernels low high nite dimensionality feature spaces 
generalization error versus mg series 
number support vectors versus series 
kernel additive trigonometric polynomial 
parameter strong ect number support vectors generalization error relevance related order see happens remember respectively expected risk empirical risk probability log log bound cost function de ne expected risk vc dimension approximation scheme 
known vc dimension satis es min radius smallest sphere contains data points feature space bound norm vector coe cients 
small vc dimension dependent second term bound generalization error constant small cause tting 
reason large term sensitive tting occurs small numerical results con rm 
example gures correspond feature spaces nite dimensions respectively tting 
kernels additive trigonometric polynomial spline order respectively 
corresponds feature space dimensions tting 
kernel additive trigonometric polynomial 
generalization error versus dimensional feature space 
inset magni es boxed region lower left section plot 
note tting occurs 
number support vectors versus feature space 
ect embedding dimension generalization error examined 
takens theorem generalization error decrease approaches minimum embedding dimension decrease generalization error 
regression algorithm sensitive tting generalization error increase minimal embedding dimension mg series 
numerical results demonstrate svm case dimensional kernel ts slightly high dimensional kernels see gure 
additive trigonometric polynomial gure 
generalization error versus nite dimensional feature space 
inset magni es boxed region lower left section plot 
note ove tting occurs number support vectors versus feature space 
discussion svm algorithm showed excellent performances data base chaotic time series outperforming techniques benchmark case 
generalization error sensitive choice stable respect wide range 
variability performances consistent theory vc bounds 
figures displayed casdagli 
nonlinear prediction chaotic time series 
physica 
ikeda 
multiple valued stationary state instability light ring cavity system 
opt 
commun 
lorenz 
deterministic non periodic ow 
atoms 
sci 
mackey glass 
oscillation chaos physiological systems 
science 
murtagh saunders 
large scale linearly constrained optimization 
mathematical programming 
osuna freund girosi 
support vector machines training applications 
memo mit lab 
generalization error versus dimensional feature space 
note tting number support vectors versus feature space 
generalization error versus embedding dimension 
solid line dimensional feature space dashed line dimensional feature space 
takens 
detecting strange attractors uid turbulence 
rand young editors dynamical systems turbulence 
springer verlag berlin 
vapnik 
nature statistical learning theory 
springer new york 
vapnik smola 
support vector method function approximation regression estimation signal processing 
advances neural information processings systems san mateo ca 
morgan kaufmann publishers 
press 
vapnik 
statistical learning theory 
wiley 
