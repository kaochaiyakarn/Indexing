combining class classifiers david tax robert duin pattern recognition group delft university technology institute 
problem class classification target objects distinguished outlier objects 
problem assumed information target class available known outlier class 
standard class classifiers class classifiers hardly fit data distribution perfectly 
best classifier discarding classifiers poorer performance waste valuable information 
improve performance results different classifiers may differ complexity training algorithm combined 
increase performance increase robustness classification 
class classifiers information classes combining class classifiers difficult 
investigate class classifiers combined best handwritten digit recognition problem 
goal data description class classification distinguish set target objects possible objects definition considered outlier objects 
mainly detect new objects resemble known set objects 
new object resemble data outlier novelty 
accepted data description higher confidence subsequent classification 
different methods developed data description 
cases probability density target set modeled 
requires large number samples overcome curse dimensionality 
techniques estimating probability density estimate exist 
possible distance model just estimate boundary class estimating probability density 
neural network restricted form closed decision surface various forms vector quantization possible method support vector classifier support vector data description proposed 
normal classification problems classifier hardly captures characteristics data 
combining classifiers considered 
commonly combined decision obtained just averaging estimated posterior probabilities 
simple algorithm gives results 
somewhat surprising especially considering fact averaging posterior probabilities solid bayesian foundation 
bayes theorem adopted combination different classifiers product combination rule automatically appears assumption independence outputs individual classifiers multiplied normalized called logarithmic opinion pool 
class classifiers provide posterior probabilities target objects information outlier data available 
uniform distribution feature space assumed posterior probability estimated target class probability 
class classifier estimate density output mapped probability combined classifiers 
investigate influence feature sets dependent type class classifiers best choice combination rule 
theory assume data objects xi represented feature spaces xk object target object labeled outlier object training class classifiers assume example outlier objects available 
feature space different class classifiers trained 
theoretical framework combining estimated posterior probabilities normal classifiers developed 
different types combination rules derivations obtained 
classifiers applied identical data representations 
xr classifiers estimate class posterior probability potentially suffering noise data 
suppress errors estimates overfitting individual classifiers classifier outputs may averaged 
results mean combination rule fj indexes target outlier class 
hand independent data representations xi available classifier outcomes multiplied gain maximally independent representations 
results product combination rule class classifiers fj xk xk class classifiers trained accept data target class reject outlier data 
distinguish types class classifiers 
type density estimators just estimate target class probability density 
normal density mixture gaussians parzen density estimation 
second type methods fit model data compute distance model 
simple models support vector data description means clustering center method autoencoder neural network 
descriptive model fitted data resemblance distance model 
svdd hypersphere put data 
applying kernel trick analogous support vector classifier model flexible follow characteristics data 
target density distance center hyper sphere 
means center method data clustered distance nearest prototype 
auto encoder network network trained represent input pattern output layer 
network contains bottleneck layer force learn nonlinear subspace data 
reconstruction error object output layer distance model 
posterior probabilities class classifiers accept reject decision class methods threshold set estimated probability distance 
principled way setting threshold supply fraction target set ft accepted 
defines threshold ft ft dx ft indicator function 
assumed methods threshold put ft target data accepted ft 
class classifiers combined posterior probabilities bayes rule compute outlier distribution unknown prior probabilities hard estimate equation directly 
problem solved outlier distribution assumed 
independent uniform distribution area feature space considering 
regardless fact class classifier estimates density reconstruction error distance types chance accepting rejecting target object acc rej available 
approximated just values ft ft binary outputs class methods replaced probabilities 
just binary output accept reject different class methods combined majority voting 
advanced combining rules required equations available distance resemblance transformed resemblance 
heuristic mapping applied 
possible transformation exp models gaussian distribution model squared distance 
scale parameter fitted distribution 
furthermore advantage probability bounded 
oc combining rules set posterior probability estimates set combining rules defined mean vote combines binary output labels pk threshold method heuristic method computing probability pk distance equation original threshold method mapped 
threshold rule majority vote class problem 
second combining rule mean weighted vote weighting ft ft introduced 
ft fraction target class accepted method ft ki pk ft pk smoothed version previous version gives identical results threshold applied 
third product weighted votes ft ki pk ft ki pk ft pk mean estimated probabilities pk product combination probabilities pk pk pk approximation outliers uniformly distributed pk combining rules compared real world class problem section 
error outlier objects accepted morph class mean vote mean weighted vote prod weighted vote mean rule prod rule target objects rejected fig 

roc curves combining rules 
individual classifiers shown stars 
evaluation class classifiers combination rules consider receiver operating characteristic curve roc curve 
gives target acceptance outlier rejection rates varying threshold values 
note estimating outlier rejection rate need example outlier objects 
example shown 
results individual classifiers trained identical feature set combination rules shown 
classifier trained target rejection rate method optimized just point roc curve ideally vertical line target rejection rate 
points indicated thick dot 
dimensional curves roc curves combining rules 
product combination rule performs best fraction target objects rejected outlier objects accepted methods 
comparisons classifiers dimensional error derived curve 
called area curve auc measures total error integrated principle threshold values 
mainly interested situations accept large fractions target set threshold values target rejection rate 
classifier optimized reject target data evaluation combination rules complete range roc curve considered 
difference mean product rule individual oc classifiers mean product fig 

left target probability density estimates combined 
right combination target probability density estimates combination normal classifiers appears robust average combination rule preferred 
extreme posterior probability estimates averaged 
class classification target class modeled low uniform distribution assumed outlier class 
classification problem asymmetric extreme target class estimates cancelled extreme outlier estimates 
class classifiers shown artificial dimensional problem data normally distributed round origin unit variance 
due atypical training samples classifiers somewhat remote 
resulting estimates product mean combination rules shown 
mean combination covers broad domain feature space product rule restricted range 
especially high dimensional spaces extra area cover large volume potentially large number outliers 
effect observable 
target rejection rates product combination rule accepts outlier objects mean combination combination rules 
indicates covered volume combining rules 
experiments apply combining rules class classifiers trained handwritten digits dataset 
dataset consists feature sets profile features table 
results individual classifiers class 
results multiplied averaged runs 
number brackets indicates standard deviation outcome 
profile fourier kl morph pixel zernike gauss mog parzen svdd kmeans fourier features karhunen lo features morphological features pixel features zernike features extracted scanned handwritten digits 
class combining problem class digit class handwritten digits described data descriptions distinguished classes 
training objects drawn target class negative examples 
testing objects class target outlier classes 
gives total target outlier objects 
feature sets mapped pca retain variance data 
pca features scaled zero mean unit variance 
class classifiers contain magic parameters 
normal distribution covariance matrix regularized inversion matrix possible taken small possible inversion possible 
number clusters mixture gaussians means center methods respectively 
number units bottleneck layer autoencoder network svdd trained reject target data 
width parameter parzen density optimized maximum likelihood optimization 
table auc errors individual methods shown different feature sets 
methods density estimators distance methods 
different classifiers give different performances cases parzen density estimator performs best 
difficult dataset morphological dataset normal distribution performs better average 
best individual classifier parzen density estimator easiest dataset classify pixel dataset 
apparently pixel training set representative sample true distribution number training objects sufficient proper density estimation parzen density estimation 
note cases variance large 
table auc errors shown target class different classifiers combined dataset 
top part table density methods combined normal density mixture gaussians parzen density estimation 
cases output methods require mapping probabilities 
results show product table 
results combination classifiers combination rules class 
numbers bold indicate improvement best individual classifier 
combining density methods profile fourier kl morph pixel zernike mv mp pp combining distance methods profile fourier kl morph pixel zernike mv mp pp combining methods profile fourier kl morph pixel zernike mv mp pp combination rule combining rule 
density methods estimate approximately probability mean combination give robust estimate 
fact density models vary combined effect mean combination rule tends increase estimated target class volume see section causes somewhat worse results product combination rule 
cases combination rules achieve improvement best individual performances class classifiers 
cases product combination rule comes close 
case mean combination rule improves product combination rule 
furthermore combination rules significantly worse indicating approximating probabilities value insufficient 
differences rules small 
averaging behavior approach best individual performance 
middle part table combining results combination distance methods shown 
mapping probabilities performed equation 
improvement best individual classifier observed product combination rule reliable improvements observed 
individual performances zernike dataset poor combination rules mean combination rule improve 
performance product combination rule somewhat surprising classifiers trained identical data mapping distance probability introduce extra noise 
large diversity methods errors uncorrelated extreme estimates suppressed product combination rule 
part table shows results combining density distance methods 
best performance beats best individual performance parzen density estimator 
observed product combination rule performs best 
cases adding distance methods improves combining rules deteriorates 
table 
roc errors obtained combining classifiers trained different feature sets 
gauss mog parzen svdd kmeans mv mp pp table results combining classifiers different feature sets shown 
clearly combining different feature sets effective combining different classifiers 
cases performance worse best individual classifier 
density methods mean combination rule methods kmeans autoencoder network mean product combination rule perform worse rules 
results different feature sets vary 
appears majority vote weighted versions robust 
investigated combining class classifiers 
best individual class classifiers problem appears parzen density estimator pixel dataset 
improving results parzen estimator appears hard training sample dataset appears representative sample true distribution 
expected combining classifiers trained different feature spaces useful 
different feature sets contain independent information results classification results 
situations product combination rule gives best results 
approximating probability just values harm combination rules useful complete density distance model 
mean combination rule suffers fact area covered target set tends overestimated outlier objects accepted necessary 
acknowledgments partly supported foundation applied sciences stw dutch organization scientific research nwo 

swain 
consensus theoretic classification methods 
ieee transactions systems man cybernetics july august 

bradley 
area roc curve evaluation machine learning algorithms 
pattern recognition 

carpenter grossberg rosen 
art adaptive resonance algorithm rapid category learning recognition 
neural networks 

duda hart 
pattern classification scene analysis 
john wiley sons new york 

duin 
uci dataset multiple features database 
available ftp ftp ics uci edu pub machine learning databases 

japkowicz 
concept learning absence counter examples autoassociation approach classification 
phd thesis new brunswick rutgers state university new jersey 

kittler duin matas 
combining classifiers 
ieee transactions pattern analysis machine intelligence 

kittler 
weighting factors multiple expert fusion 
clark editor proceedings th british machine vision conference pages 
university essex printing service 

duin 
criterion smoothing parameter parzen estimators probability density functions 
technical report delft university technology september 

koch 
class classifier networks target recognition applications 
proceedings world congress neural networks pages portland 
international neural network society inns 

tresp 
averaging regularized estimators 
neural computation 

tarassenko hayton brady 
novelty detection identification masses mammograms 
proc 
fourth international iee conference artificial neural networks volume pages 

tax duin 
data domain description support vectors 
verleysen editor proceedings european symposium artificial neural networks pages 
facto brussel april 

tax duin 
support vector domain description 
pattern recognition letters december 

duin 
support objects domain approximation 
icann sweden september 
