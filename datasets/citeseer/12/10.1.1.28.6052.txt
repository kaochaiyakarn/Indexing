structural maximum posteriori linear regression unsupervised speaker adaptation tor andr olivier chin hui lee wu chou multimedia communications research lab bell laboratories lucent technologies mountain ave murray hill nj usa www multimedia bell labs com introduce approach transformation model adaptation techniques 
previously published schemes mllr define set affine transformations applied clusters model parameters 
shown approach yield results adaptation data scarce inherent problem needs considered number transformations significant influence adaptation performance 
transformations result poorly estimated transformation parameters eventually leading model overfits adaptation data 
hand transformations restricted mapping obtained leading suboptimal adapted model 
address problem estimating transform parameters maximum posteriori sense set hierarchical priors arranged tree structure 
show approach yields significant improvement compared mllr doing unsupervised model adaptation wsj spoke test 

known performance automatic speech recognition systems sensitive mismatches training testing conditions typical mismatches channel speaker variations 
different approaches problem put forward including preprocessing speech signal adaptation model parameters robust decision strategies 
consider model adaptation technique 
general hidden markov model hmm specified parameters relevant adaptation data obtain new model parameters predefined mapping 
practical reasons commonplace split model parameters disjoint clus associate unique mapping ters cluster index parameter set specifying mapping 
case parametric mapping model adaptation problem estimate parameters done maximum likelihood ml sense approach maximum posteriori map formulation 
prior knowledge plausibility done leave de telecommunications norwegian university science technology norway 
different values expressed prior distribution known hyperparameters 
map estimation problem formulated maximum likelihood linear regression mllr best know implementations ideas expressed 
parametric mapping takes form affine transformation mean vectors gaussian mixtures hmm augmented mean vector 
mllr formulation parameters estimated ml formulation equation 
applying transformations mean vectors respective clusters parameters updated allowing update mixtures observed possibly sparse adaptation data 
naive implementation mllr serious shortcomings 
set predetermined clusters allows possibility little data available proper estimate 
problem alleviated choosing clusters dynamically arranging clusters hierarchical tree structure choosing cluster threshold amount data available 
improvement specify distribution prior transformations 
leads maximum posteriori linear regression formulation 
provided priors chosen estimation transformations robust 
extend framework hierarchical priors 
new development enables choose better prior distributions yielding robust behaved transformations small amounts adaptation data 
section explain concept hierarchical priors assumptions regarding parametric form introducing necessary approximations 
show arrange priors tree structure corresponds hierarchical tree hmm parameter clusters correspondence estimate transformation cluster top manner 
experimental results unsupervised speaker adaptation wsj spoke task 

structural maximum posteriori linear regression 
hierarchical priors hierarchical priors model adaptation seen model acoustic mismatch probability density function pdf 
similar approach extending algorithm framework 
basic assumption prior hyperprior hyperprior hyper hyperprior 
priors hierarchy parametric form estimated recursive manner relative position tree structure 
informally want 
consider subset hmm parameters transformation applied mean vectors prior data find map estimate 
consider subset pos distribution new prior distribution find map estimate transformation applied mean vectors steps recursive manner obtain set transformations estimated map sense relevant prior distribution 
exact form prior posterior distributions way constructing set parameter clusters discussed 
bayesian paradigm involved choice priors delicate 
assume transformation normal distribution mean ance immediate problem choice prior posterior distribution contained similar parametric family easily seen expression sets feasible state mixture sequences respectively 
exact form clearly infeasible number terms expression increases exponential rate level priors 
approximations approximate normal distribution mean equal map estimate covariance prior applying steps specified normality assumption approximation posterior normal distribution obtain sequence prior distributions subsection show arrange priors trees 

tree structure transforms allowing priors share hyperprior gives tree structure 
want combine hierar consider set mean vectors gaussian mixture distributions hmm parameters adapted 
hierarchical tree structure priors 
tree structure priors corresponding structure hmm parameters 
adapting gaussian mean vectors concerned clusters gaussians 
tree structure parameter clusters defined follows 
top node contain relevant parameters hmm 
split node separate child nodes containing subset procedure repeated child nodes eventually defining tree practice accomplished ways 
hand phonetic knowledge cluster states belonging acoustically similar models 
alternative define measure distance gaussians known clustering algorithms 
followed approach distortion measure kullback leibler divergence 
starting gaussians hmm top node distortion measure means algorithm divide node clusters 
doing recursively clusters gives tree increasingly fine resolution 
assume node tree yield prior distribution child nodes 
starting top node find map estimate transformation matrix described 
matrix propagated tree define prior level transformations 
process terminate reasons final node tree reached amount data available considered insufficient reliable estimate local transform process termi nated gaussian hmm adapted local transform transform successfully estimated tree node gaussian resides 
transform estimated closest transform higher level tree applied 
illustrated circles filled black illustrates successfully estimated transforms white circles nodes little data available 
section show estimate transform node 

estimating transforms map estimate transform node available closed form solution 
problem lends em algorithm yields object function minimized adaptation closest transformation available 
previously sets feasible state mixture sequences respectively 
prior propagated parent node 
differentiating respect element setting result equal zero gives set linear equations equivalent estimation problem 
full covariance matrix prior results system equations unknowns mean vector dimension clearly considerable numerical problem 
diagonal covariance matrix results sets equations unknowns computational complexity mllr 
derivation equations see 
final note solving linear equations 
go tree systems equations solved increasingly ill conditioned making standard approaches lu factorization behave poorly 
iterative methods conjugate gradient algorithm behave better care taken regard convergence 

experiments results 
database baseline system experiments performed spoke test set wsj task 
spoke data consists non native speakers american english 
speaker provided utterances model adaptation utterances testing 
model adaptation experiments carried speaker various amount adaptation data ranging utterances 
order get statistically representative results adaptation experiments repeated different selections set adaptation utterances utterances single set 
example experiments involving adaptation utterances done different sets adaptation utterances randomly selected utterances available adaptation data 
triphone hmm models built wsj si training set decision tree state tying algorithm 
total tied states average gaussian mixture components state obtained 
word pronunciation lexicon generated automatically general english text speech system 
language model experiments standard trigram language model provided nist wsj task 
standard mel frequency cepstral coefficient mfcc frontend create feature vector components consisting mfcc component plus log energy term word error rate batch unsupervised adaptation spoke baseline mllr number adaptation utterances word error rate batch unsupervised experiments mllr various amount adaptation utterances 
second derivatives 
cepstral mean normalization applied sentence 
tree gaussian densities adaptation designed number children node layer tree contains terminal single gaussian nodes 

experimental results series adaptation experiments performed 
series unsupervised batch adaptation experiments acoustic models adapted adaptation utterances extracted adaptation data adapted models recognize test data 
second series experiments unsupervised online adaptation called auto adaptation self adaptation recognizing test data acoustic models periodically updated previously recognized utterances estimate transformation parameters 
unsupervised batch adaptation experiments speaker adaptation data recognized lexicon obtained hypothesized transcriptions carry adaptation 
point lexicon derive hypothetical transcription adaptation data closed lexicon testing 
vocabulary adaptation utterances different vocabulary test set adaptation utterances contain vocabulary words 
especially difficult adaptation scenario hypothesized transcription adaptation data contains errors 
experimental design mllr number transformation matrices located nodes tree sets adaptation utterances 
difference adaptation techniques estimation criterion maximum likelihood mllr maximum posterior hierarchical priors 
represents experimental results mllr adaptation various amount adaptation data 
results terms av adapt 
method wer baseline mllr table word error rate unsupervised online adaptation experiments mllr 
erage word error rate speakers 
clearly appears outperforms mllr amount adaptation data observed previous experiments supervised adaptation :10.1.1.28.6052
point attempt optimize performance mllr doing careful selection number transformation matrices amount adaptation data 
threshold control number transformations selected modified 
experimental setting similar batch supervised experiments described :10.1.1.28.6052
obviously setting mllr clearly overfits adaptation data 
prior information estimates avoid overfitting reasonable generalization obtained 
argue reducing drastically number transformation matrices mllr avoid overfitting experimentally shown supervised adaptation :10.1.1.28.6052
illustrates sensitive mllr implementation details selection number transformations 
believe number transformation matrices carefully optimized best setting outperform best mllr scenario amount adaptation data 
illustrated finely tuned mllr compared showing systematic advantage bayesian estimation maximum likelihood estimation :10.1.1.28.6052
online unsupervised adaptation experiments models adapted periodically test utterances starting initial speaker independent models 
complex online approach recursive bayesian formulation choose keep sets acoustic models memory original speaker independent model adapted model 
adapted model recognition generate state segmentation adaptation 
original speaker independent model carry estimation transformation matrices sufficient statistics accumulated current test utterance 
table online unsupervised adaptation results mllr adaptation 
show improvement ml lr 
surprising word error rate close obtained batch unsupervised adaptation adaptation experiments expect worse utterance unsupervised adaptation 
important remember experiments batch adaptation utterances contain vocabulary words online adaptation uses closed lexicon explaining relatively bad performance batch unsupervised adaptation compared online adaptation 

extension approach model adaptation 
hierarchical priors arranged tree structure estimated top manner find robust estimates transformation matrices linear regression step 
compared original maximum likelihood approach mllr obtain robust estimates range available adaptation data minimum parameter tuning 
hierarchical prior information algorithm sensitive mllr number transformations introduces dependency estimation transformation matrices nodes having common parent provides way derive prior distribution tree lacking original formulation 
unsupervised adaptation experiments spoke test set wsj task show superiority proposed approach mllr 

node 
lee stochastic feature model compensation approaches robust speech recognition speech communication vol 
pp 


lee hidden markov model adaptation maximum posteriori linear regression workshop robust methods speech recognition adverse conditions tampere finland 

lee structural map speaker adaptation hierarchical priors proc 
ieee workshop speech recognition understanding 

lee structural bayes approach speaker adaptation ieee transactions speech audio processing 
appear 
dempster laird rubin maximum likelihood incomplete data em algorithm journal royal statistical society ser 
vol 
pp 


lee joint maximum posteriori adaptation transformation hmm parameters submitted ieee trans 
speech audio processing 
press flannery teukolsky vetterling numerical recipes art scientific computing 
cambridge university press 
chou decision tree state tying segmental clustering acoustic modeling proc 
ieee int 
conf 
acoustics speech signal processing seattle washington usa pp 

sproat olive text speech synthesis technical journal vol 
pp 

:10.1.1.28.6052

lee structural maximum posteriori linear regression fast hmm adaptation workshop automatic speech recognition challenges new millenium paris france isca asr sept 
:10.1.1.28.6052
lee maximum posteriori linear regression hidden markov model adaptation proceedings european conference speech communication technology vol 
budapest hungary pp 

wang zhao line bayesian speaker adaptation tree structured transformation robust priors proc 
ieee int 
conf 
acoustics speech signal processing istanbul turkey june 
