programming research group questions answers bsp skillicorn department computing information science queen university kingston canada skill ca jonathan hill mccoll computing laboratory university oxford oxford hill bill comlab ox ac uk revised th november prg tr delta oxford university computing laboratory wolfson building parks road oxford ox qd bulk synchronous parallelism bsp parallel programming model abstracts low level program structures favour supersteps 
superstep consists set independent local computations followed global communication phase barrier synchronisation 
structuring programs way enables costs accurately determined simple architectural parameters permeability communication network uniformly random traffic time synchronise 
permutation routing barrier synchronisations widely regarded inherently expensive case 
result structure imposed bsp comes free performance terms bringing considerable benefits application building perspective 
answers common questions asked bsp justifies claim major step forward parallel programming 
model needed 
large number different types parallel architectures developed 
hindsight see variety unnecessary unhelpful 
commercial development parallel applications software achieve acceptable performance software tailored specific architectural properties machine 
today number parallel computation models languages probably greatly exceeds number different architectures parallel programmers contend years ago 
inadequate hard achieve portability hard achieve performance 
message passing inadequate complexity correctly creating paired communication actions send receive large complex software 
systems prone deadlock result 
furthermore performance programs impossible predict interactions large numbers individual data transfers 
take view models shared memory easier program provide abstraction single shared address space class placement decisions avoided 
moderately parallel architectures capable providing abstraction certainly built believe modest parallelism provide satisfy performance demands foreseeable 
dubious claims 
shared memory reduce need placement creates need control simultaneous access location 
requires careful crafting programs pram style expensive lock management 
implementing shared memory abstractions requires larger larger fraction computer resources devoted communication maintenance coherence 
worse technology required provide abstraction commodity nature expensive 
bulk synchronous parallel bsp model provides software developers attractive escape route world architecture dependent parallel software 
emergence model coincided convergence commercial parallel machine designs standard architectural form compatible 
developments welcomed rapidly growing community software engineers produce scalable portable parallel applications 
parallel applications community welcomed approach surprising degree skepticism parts computer science research community 
people regard claims support bsp approach true 
sensible way evaluate architecture independent model parallel computation bsp consider terms properties usefulness basis design analysis algorithms applicability range general purpose architectures ability provide efficient scalable performance support design fully portable programs performance 
focus time simply replace zoo parallel architectures new zoo parallel models 
viewpoint nature role models gain support move straightforward world parallel algorithms complex world parallel software systems 
bulk synchronous parallelism 
bulk synchronous parallelism style parallel programming developed generalpurpose parallelism parallelism application areas wide range architectures 
goals ambitious parallel programming systems aimed particular kinds applications particular classes parallel architectures 
bsp fundamental properties ffl simple write 
bsp programs look sequential programs 
bare minimum extra information needs supplied describe parallelism 
ffl independent target architectures 
parallel programming systems bsp designed architecture independent programs run unchanged moved architecture 
bsp programs portable strong sense 
ffl performance program architecture predictable 
execution time bsp program computed text program simple parameters target architecture 
design possible effect decision performance determined time 
bsp achieves properties raising level abstraction programs written implementation decisions 
considering individual processes individual communication actions bsp considers computation communication level entire program executing computer 
determining bulk properties program bulk ability particular computer satisfy possible design new clarity 
way bsp able achieve abstraction locality performance optimisation 
simplifies aspects program implementation design adversely affect performance application domains 
application domains locality critical example low level image processing bsp may best choice 
local computations virtual processors barrier synchronisation global communications superstep bsp programming style look 
bsp programs vertical structure horizontal structure 
vertical structure arises progress computation time 
bsp sequential composition global supersteps conceptually occupy full width executing architecture 
superstep subdivided ordered phases consisting ffl computation locally process values stored memory processor ffl communication actions processes involving movement data processors ffl barrier synchronisation waits communication actions complete data moved available local memories destination processors 
horizontal structure arises concurrency consists fixed number virtual processes 
processes regarded having particular linear order may mapped processors way 
locality plays role placement processes processors 
superstep shown 
denote virtual parallelism program number processes uses 
target parallel computer fewer processors virtual parallelism extension brent theorem transform bsp program version 
communication 
parallel programming systems handle communication conceptually terms implementation level individual actions memory memory transfers sends receives active messages 
level difficult simultaneous communication actions parallel program interactions complex 
hard say time single communication action take complete 
considering communication actions en masse simplifies treatment possible bound time takes deliver set data 
bsp considering communication actions superstep unit 
time imagine messages fixed size 
superstep process designated set outgoing messages expecting receive set incoming messages 
maximum number incoming outgoing messages processor communication pattern called relation 
communication pattern relation 
random placement processes processors structure set messages certainly reflected structure target architecture communication topology 
destination processor addresses relation approximate sequence permutations processor identifiers 
ability communication network deliver data captured parameter measures permeability network continuous traffic addressed destinations 
random placement processes processors techniques adaptive routing help load generated relations approximate load generated sequences random permutations 
applied load communication network kind characteristics appropriate measure 
parameter defined takes time hg deliver relation 
subject small provisos discussed hg accurate measure communication performance large range architectures 
value normalised respect clock rate architecture units time executing sequences instructions 
sending message length clearly takes longer sending message size 
reasons clear bsp distinguish message length messages length cost case 
messages varying lengths may form number messages message lengths folded number units data transferred 
parameter related bisection bandwidth communication network equivalent 
depends factors ffl protocols interface communication network ffl buffer management processors communication network ffl routing strategy communication network ffl bsp runtime system 
bounded ratio bisection bandwidth suitable normalised may larger 
unusual network bisection bandwidth grew faster means monotonically increasing function value practice determined empirically parallel computer running suitable benchmarks 
bsp benchmarking protocol appendix note normalised single word delivery time single word delivery time continuous traffic conditions 
difference subtle crucial 
surely isn precise measure long communication takes 
don hotspots congestion inaccurate 
difficult problems determining performance conventional messaging systems precisely congestion upper bounds hard determine quite pessimistic 
bsp largely avoids difficulty 
apparently balanced communication pattern may generate hotspots region interconnection topology 
bsp prevents ways 
random allocation processes processors breaks patterns arising problem domain 
second bsp runtime system uses routing techniques avoid localised congestion 
include randomised routing particular kinds randomness introduced choice route communication action adaptive routing data diverted normal route controlled way avoid congestion 
congestion occurs architecture limited range deterministic routing techniques bsp runtime system choose limitation continuous message traffic reflected measured value notice definition relation distinguishes cost balanced communication pattern skewed 
communication pattern processor sends single message distinct processor counts relation 
communication pattern transfers number messages form broadcast processor counts relation 
unbalanced communication cause congestion charged higher cost 
cost model take account congestion phenomena arising limits processor capacity send receive data extra traffic occur communication links near busy processor 
experiments shown accurate measure cost moving large amounts data wide range existing parallel computers 
isn expensive give locality 
application domains exploiting locality key achieving performance 
naive analysis suggest reason 
performance limited problems large amounts data exploit large amounts virtual parallelism 
existing parallel computers modest numbers processors 
programs mapped parallel architectures virtual processes multiplexed physical processor programmer 
done locality lost communication network happens match structure problem domain closely 
problems apparently large amounts locality tend locality execute 
parallel computers considerable cost associated starting communication 
doesn mean cost model inaccurate small messages doesn account start costs 
cost model inaccurate special circumstances 
recall communications superstep regarded place superstep 
semantics possible implementations wait computation part superstep communication actions requested 
package data transferred larger message units 
cost starting data transfer paid destination superstep folded value total amount communication superstep small start effects may noticeable difference performance 
address quantitatively 
aren barrier synchronisations expensive 
costs accounted 
barriers expensive today architectures sparingly possible 
hand barriers nearly inherently expensive believed high performance computing folklore 
architecture developments may cheaper 
cost barrier synchronisation comes parts ffl cost caused variation completion times computation steps participate 
implementation suggest balance computation parts superstep thing 
ffl cost reaching globally consistent state processors 
depends course communication network special purpose hardware available synchronising way interrupts handled processors 
architecture cost barrier synchronisation captured parameter diameter communication network length longest path allows state moved processor clearly imposes lower bound affected factors practice accurate value parallel architecture obtained empirically 
notice barriers potentially costly number attractive features 
possibility deadlock livelock bsp program barriers circularities data dependencies impossible 
need tools detect deal 
barriers permit novel forms fault tolerance 
parameters allow cost programs determined 
cost single superstep sum terms maximum cost local computations processor cost global communication relation cost barrier synchronisation superstep 
cost cost superstep max processes max processes ranges processes time local computation process maxima assumed bsp costs expressed form hg cost entire bsp program just sum cost superstep 
call standard cost model 
sum meaningful allow comparisons different parallel computers parameters expressed terms basic instruction execution rate target architecture 
vary constant factor architectures asymptotic complexities programs constant factors critically important 
note assuming processors homogeneous hard avoid assumption expressing performance factors common unit 
existence cost model tractable accurate possible truly design bsp programs consciously justifiably choices different implementations specification 
example clear strategies write efficient bsp programs ffl balance computation superstep processes maximum computation times barrier synchronisation wait slowest process ffl balance communication processes maximum fan fan data ffl minimise number supersteps determines number times appears final cost 
cost model shows predict performance target architectures 
values superstep number supersteps determined inspection program code subject usual limits determining cost sequential programs 
values inserted cost formula estimate execution time program executed 
cost model ffl part design process bsp programs ffl predict performance programs ported new parallel computers ffl guide buying decisions parallel computers bsp program characteristics typical workloads known 
cost models bsp proposed incorporating finer detail 
example communication computation conceivably overlapped giving superstep cost form max hg optimisation usually idea today architectures 
argued cost relation limited time taken send messages receive messages communication term form variations alter costs small constant factors continue standard cost model interests simplicity clarity 
important omission standard cost model restriction amount memory required processor 
existing cost model encourages balance communication limited barrier synchronisation encourages memory 
extension cost model bound memory associated processor investigated 
cost model possible bsp design algorithms just programs 
goal build solutions optimal respect total computation total communication total number supersteps widest possible range values designing particular program matter choosing known algorithms optimal range machine sizes envisaged application 
example bsp algorithms matrix multiplication developed 
block parallelization standard algorithm asymptotic bsp complexity block mm cost requiring memory processor size optimal time memory requirement 
sophisticated algorithm due mccoll valiant bsp complexity block broadcast mm cost requiring memory processor size optimal time communication supersteps requires memory processor :10.1.1.52.5671
choice algorithms implementation may depend relationship size problem instances memory available processors target architecture 
bsp programming discipline programming language 
bsp model parallel computation 
concerned high level structure computations 
prescribe way local computations carried communication actions expressed 
existing bsp languages imperative intrinsic reason need 
bsp expressed wide variety programming languages systems 
example bsp programs written existing communication libraries pvm mpi cray 
required provide non blocking communication mechanisms way implement barrier synchronisation 
values depend hardware performance target architecture amount software overhead required achieve necessary behaviour systems designed bsp mind may deliver values common approach bsp programming spmd imperative programming fortran bsp functionality provided library calls 
bsp libraries years oxford bsp library green bsp library 
standard agreed library called bsplib 
bsplib contains operations delimiting supersteps variants communication direct memory transfer buffered message passing 
bsp languages developed 
include gpl opal 
gpl attempt develop mimd language permitting synchronisation subsets executing processes 
opal object bsp language 
easy program bsplib library 
bsplib library provides operations shown table 
operations ffl set bsp program ffl discover properties environment process executing ffl participate barrier synchronisation ffl communicate directly remote memory message queue ffl abort computation inside ffl communicate high performance unbuffered mode 
bsplib library freely available fortran www bsp worldwide 
org htm 
complete description library appendix higher level library provides specialised collective communication operations 
considered part core library easily realised terms core 
include operations broadcast scatter gather total exchange 
application domains bsp 
bsp number application areas primarily scientific computing 
done part contracts oxford parallel www 
comlab ox ac uk 
computational fluid dynamics applications bsp include implementation bsp version library solving multigrid viscous flows computation flows aircraft complex parts aircraft project rolls royce bsp version flow computational fluid dynamics code oil reservoir modelling presence discontinuities project schlumberger class operation meaning initialisation bsp init simulate dynamic processes bsp start spmd code bsp spmd code enquiry bsp pid find process id bsp number processes bsp time local time synchronisation bsp sync barrier synchronisation bsp region globally visible bsp remove global visibility bsp put push remote memory bsp get pull remote memory bsp set tag size choose tag size bsp send send remote queue bsp get tag match tag message bsp move fetch queue halt bsp abort process halts high performance bsp unbuffered versions bsp communication bsp primitives table core bsp operations computational applications bsp include modelling electromagnetic interactions complex bodies unstructured meshes project british aerospace parallelisation scala codes demonstrations problems design electric motors permanent magnets mri imaging parallel implementation time domain electromagnetic code absorbing boundary conditions parallelisation emma code calculating electromagnetic properties wires cables antennae 
involving parallelising merlin code project register shipping ford motor 
bsp applied plasma simulation rensselaer polytechnic institute new york 
bsp programs look 
bsp programs real problems large impractical include source 
include small example programs show bsplib interface 
illustrate different possibilities standard parallel prefix scan operation gamma stored process compute delta delta delta process sums version 
function calculates partial sums integers stored processors 
algorithm uses logarithmic technique performs dlog pe supersteps sums logarithmic technique th superstep processes range gamma combine local partial sums process gamma gamma shows steps involved summing values bsp pid processors 
int int int left right left sizeof int right right left sizeof int right left right left return right bsp put bsp pid right left sizeof int executed process bsp pid single integer right copied memory processor bsp pid address left left previously registered data structure 
procedure bsp allows processors declare variable left willing data put operation 
reason registration required processor copy data structure left necessarily stored address 
registration creates correspondence data structures name different processors 
cost algorithm dlog pe dlog pe supersteps including registration superstep local addition performed costs flop message size word enters exits process 
sums version 
alternative implementation prefix sums function achieved single superstep temporary data structure containing integers 
process puts data summed th element temporary array processes 
communications completed local sum performed accumulated data 
cost algorithm pg 
int int int result array calloc sizeof int array null unable allocate element array array sizeof int array sizeof int sizeof int result array result array free array array return result algorithm performs logarithmic number additions supersteps second algorithm performs linear number additions constant number supersteps 
operation performed iteration algorithm changed addition costly associative operator bsp cost analysis provides simple mechanism determining better implementation 
sums array 
routines defined sum values held blocks distributed processors 
algorithm proceeds phases 
running sum block integers computed locally processor 

element block contains sum element segment simple algorithms calculate running sums element block call 

processor gets value left neighbouring processor call 

adding locally summed elements produces desired effect running sums elements 
void int array int int sizeof int array array array sizeof int array void main int xs xs calloc sizeof int xs xs printf process printf xs printf stdout typical values common parallel computers 
values bsp cost model parameters shown table 
values parameters normalised instruction rate processor aid comparisons machines raw rates microseconds 
instruction rate depends heavily kind computations done average different measured values bsc measures cost inner product operations performed data structure size value chosen far greater cache size processor 
benchmark gives lower bound rate processor arithmetic operation induces cache dse measures cost dense matrix multiplication operations performed data structures size large percentage computation kept cache benchmark gives upper bound rate processor 
mentioned bsp algorithm design balanced patterns communication 
illustrate communication capacity balanced communications 
particularly easy relation cyclic shift data neighbouring processors 
benchmark provides upper bound rate communication 
parallel computers far greater difficulty achieving scalable communication patterns communication move lots data destinations 
extreme example consider relation generated total exchange processors 
scalable architecture provide dedicated wires expensive 
sparser interconnections 
example cray uses torus ibm sp uses hierarchy node fully connected crossbar switches 
value total exchange provides measure lower bound rate communication architecture 
surprisingly values derived directly relation pg cost relation total exchange quite different 
mean relation performance network example ring takes time proportional deliver relation relation usually means network effective capacity large link bandwidth suggest 
cost modelling algorithms advisable value produced total exchange benchmark 
represents memory speed processor account buffering communication may occur implementation bsplib efficiency communication network roughly estimated comparing cost processor 
gives ratio inter processor communication memory speed ibm sp nodes switch communication sgi power challenge nodes cray nodes 
machine mflops local total exch 
bsc dse flops flop word word flop word word words sgi cray ibm sp switch continued page continued previous page machine mflops local total exch 
bsc dse flops flop word word flop word word words multiprocessor sun hitachi sr convex exemplar digital alpha farm gc ibm sp ethernet table bsp machine parameters 
values communications bit words benchmarks performed optimisation level cray sgi ibm sp gc hitachi sr native implementations toolset toolset multiprocessor sun built generic system shared memory facilities digital alpha farm consists cluster alpha workstations connected fddi giga switch 
toolset implementation built top generic version mpi mpich 
appendix shows figures obtained 
meaning explained section 
bsplib implemented efficiently today architectures 
semantics bsplib operations reflects high level view bsp computation communication overlap 
oxford implementation bsplib keeps phases separate 
semantics calls put get permits executing concurrently local process computation performance advantages postponing turn larger exploiting potential overlap 
approach contradicts current practice communication libraries overlapping computation communication considered thing create best factor improvement 
course treating communication level single messages provides obvious opportunity improve performance postponing communication 
postponing communication local computation creates major performance enhancement opportunities 
combining messages processor pair means transmission startup costs paid superstep message require memory buffering 
freedom reorder transmissions different processors means patterns guaranteed avoid congestion set software requiring expensive hardware solutions operating data transfers 
important congestion inside network significant problem common processor network interface 
performance gains delaying communication large highperformance versions put get operations designed computation communication overlapped buffering postpone transmissions computation phase superstep 
general structure oxford implementation bsplib put get operations initiated superstep delayed superstep optimisations effect minimise absolute value variance applied entire relation 
regardless type parallel architecture ability reorder messages transmission crucial creating consistent bulk communication behaviour increasing value mechanisms ffl randomly ordering messages reduce likelihood troublesome patterns ffl latin square schedule transmissions guaranteed contention free way 
mechanisms preferred architecture dependent 
recall latin square theta square values appears times repetition row column 
square schedule routing relation row schedule processor contents row regarded destinations communication time step 
mechanisms major effect performance 
example consider total exchange algorithm shown processor data size exchanged processor 
communication processor contain data structure size np containing bsp cost algorithm png messages enter exit processor 
naive implementation may processor send message processor time step processor second 
causes messages contend process contend process 
cost communication linear cost predicted bsp cost formula png alternative ordering cause contention processors send data order mod processor identifier simple latin square 
expected linear cost achieved 
communication communication ae ae ae total exchange processors procs immediate transmission bsplib delaying reordering contention latin square contention latin square table effects node contention cray 
entries table seconds routing relation 
processors integers process 
table shows results implementation routes total exchanges 
columns show happens programmer writes puts order causes maximum contention latin square order avoids 
runtime system combining reordering transmits data soon put executed 
final columns show happens user programs bsplib delays messages reorders 
expected reordering significant difference library reordering induces improved performance regardless textual form program 
reordering implementation consistent model large sacrifice efficiency 
precise details handling communication building barriers differs depending specifics target architectures distributed memory machines remote memory access cray 
barrier synchronisation performed ensure process finished local computation 
processors passed barrier sided memory accesses route messages memories remote processors 
communication phase superstep completed performing barrier synchronisation 
distributed memory machines message passing ibm sp hitachi sr alpha farm gc 
architectures provide native non blocking send blocking receive message passing primitives relation routed communica tion network phases 
total exchange performed exchanging information number sizes destination addresses messages 
total exchange considered barrier synchronisation superstep 

gets translated puts data refer buffered source processor 

total exchange processor knows messages process expecting 
process knows communication phase superstep complete counting incoming messages 
communication performed interleaving outgoing incoming messages minimum buffering requirements placed underlying message passing system 
shared memory architectures sgi power challenge sun convex exemplar 
implementation shared memory architectures combines features implementations 
information number size messages sent processor pair constructed region shared memory call put get 
computation phase barrier synchronisation takes place ensure information frozen 
message information shared memory implicit total exchange considered occurred point 
actual exchange data performed message passing style 
messages copied buffers associated process shared memory 
buffers inspected remote process contents copied remote processor memories 
contention limiting order messages number message passing buffers associated process minimised 
message information region cleared barrier synchronisation takes place allow renewed access 
effect message size value 
seen way bsplib delays communication superstep combines messages largest possible units reduces importance message size 
cost model distinction cost process sending messages size single message size communications relation cost hg 
superstep little total communication occurs may deviate cost model effects startup costs message transmission 
miller refined standard cost model technique model effect message granularity communication cost :10.1.1.52.5671
refined model defined function message size asymptotic communication cost large messages reported table size message produces half optimal bandwidth machine message size words actual cost single word messages actual cost combined messages theoretical model eqn 

theoretical model eqn 

fitting experimental values flops word equation processor ibm sp switch communication 
messages communicated sided put communication process puts data processor memory 
top curve represents single word messages bottom curve uses message combining scheme 
value equation determined experimentally machine configuration fitting curve actual values 
shows actual values processor ibm sp 
messages combined superstep value effectively reduced words 
comparison purposes effect naively communicating messages separately shown data points labeled actual cost single word messages 
fitting curve data gives words 
parameter discover minimum message size standard cost model percentage detailed cost model 
standard model accuracy cost attributed model includes message granularity words valiant parameter measures minimum size relation achieve throughput 
percentage error communication cost ibm sp switch communication error standard bsp model communicating bit words 
expected size relation increases error standard bsp model decreases 
tools available help building tuning bsp programs 
intensional properties parallel program computes result hard understand 
bsp model goes way alleviated problem cost analysis guide program development 
unfortunately large scale problems cost analysis rarely time program development 
role current bsp tools aid programmers understanding intensional properties programs graphically providing profiling cost information 
tools analyse actual communication properties program analyse predicted performance code assuming parallel machine program run acts real bsp computer scalable machine constant routes relations time hg 
central problem parallel profiling system effective visualisation large amounts profiling data 
contrast conventional parallel profiling tools highlight patterns communication individual sender receiver pairs message passing system bsp approach significantly simplifies visualisation communications occur superstep visualised single monolithic unit 
oxford bsp toolset flags puts res 
oxford bsp toolset flags puts res seconds elapsed ibm sp fri jul milliseconds bytes milliseconds bytes main main main main main step filename line process process process process process process process process sums elements logarithmic technique processor ibm sp example results bsp profiling tool running ibm sp 
shows communication profile parallel prefix algorithm developed page 
top bottom graphs show axis volume data moved axis elapsed time 
pair vertically aligned bars graphs represents total communication superstep 
communication bar series bands 
height band represents amount data communicated particular process identified band shade 
sum bands height bar represents total amount communication superstep 
width represents oxford bsp toolset flags puts res 
oxford bsp toolset flags puts res seconds elapsed ibm sp fri jul running sums milliseconds bytes milliseconds bytes main main main main main step filename line process process process process process process process process sums elements total exchange processor ibm sp elapsed time spent communication barrier synchronisation 
label top left hand corner bar conjunction legend right graph identify superstep call bsp sync user code 
white space represents local computation time superstep 
start running sums identified points labelled 
white space graphs supersteps shows computation running sums executed locally process block size superstep hidden label scale shows synchronisation arises due registration function bsp 
successively smaller bars represent logarithmic number communication phases parallel prefix technique 
contrasting sizes communication bars schematic diagram graphically shows diminishing numbers processors involved communication parallel prefix algorithm proceeds 
contrasting method running sums algorithm shows number synchronisations algorithm reduced dlog pe time spent total exchange bsp approximately algorithm logarithmic technique 
due larger amount data transferred milliseconds spent summing values processes parallel prefix technique compared milliseconds total exchange 
figures show profiles algorithms running processor cray data set size ibm sp 
lower value barrier synchronisation latency ibm sp see table reducing number supersteps dlog supersteps marked effect efficiency 
version bsp logarithmic takes milliseconds compared milliseconds oxford bsp toolset flags resolve oxford bsp toolset flags resolve seconds elapsed cray fri jul running sums milliseconds bytes milliseconds bytes main main main main main step filename line proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
sums elements logarithmic technique processor cray bsp total exchange 
data show today parallel computers better reduce number supersteps expense requiring communication 
bsplib compare communication systems pvm mpi 
years pvm message passing library widely implemented widely 
respect goal source code portability parallel computing achieved pvm 
advantages bsp programming message passing framework pvm 
shared memory architectures modern distributed memory architectures powerful global communications messagepassing models pvm efficient bsp model communication synchronisation decoupled 
especially true modern distributed memory architectures hardware support direct remote memory access sided communications 
pvm message passing systems pairwise barrier synchronisation suffer having simple analytic cost model performance prediction simple means examining global state computation debugging 
mpi proposed new standard want write portable message passing programs fortran level point point communications send receive mpi similar pvm comparisons apply 
mpi standard general appears complex relative bsp model 
carefully chosen combination various non blocking communication oxford bsp toolset flags resolve oxford bsp toolset flags resolve seconds elapsed cray fri jul running sums milliseconds bytes milliseconds bytes main main main main main step filename line proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
sums elements total exchange processor cray primitives available mpi barrier synchronisation primitive produce mpi bsp programming model 
higher level collective communications mpi provides support various specialised communication patterns arise frequently message passing programs 
include broadcast scatter gather total exchange reduction scan 
standard communication patterns provided bsp higher level library 
attempt compare bsp performance mpi network workstations 
results show performance differences small order percent 
compared pvm mpi bsp approach offers simple programming discipline supersteps easier determine correctness programs cost model performance analysis prediction simpler compositional efficient implementations machines 
bsp related logp model 
logp differs bsp ways ffl uses form message passing pairwise synchronisation :10.1.1.160.9052
ffl adds extra parameter representing overhead involved sending message 
general purpose parameter bsp applies communication bsp parameter ignored unusual programs 
ffl defines local terms 
parameter bsp regarded capturing throughput architecture processor inserts message uniformly distributed address step 
takes account actual capacity network distinguish delays network caused inability enter network blocking back sending processor 
contrast logp regards network having finite capacity treats minimal permissible gap message sends single process 
amounts thing cases reciprocal available processor network bandwidth bsp takes global view meaning logp takes local view 
years experience developing software logp model shown analyse correctness efficiency logp programs necessary convenient barriers 
major improvements network hardware communications software greatly reduced overhead associated sending messages 
early multiprocessors overhead substantial single processor handled application communication 
manufacturers learned bad idea newer multiprocessors provide dedicated processor handle message traffic node direct remote memory access 
new scenario overhead application processor sending receiving message time move user address space system buffer 
small relatively machineindependent may disappear communication processors gain access user address space directly importance overhead parameter long term negligible 
logp barriers overhead bsp points suggest logp model improve bsp significant way 
natural ask flexible logp model enables designer produce efficient algorithm program particular problem expense complex style programming 
results show case 
shown bsp logp models efficiently simulate loss performance structured bsp programming style 
bsp related pram model 
bsp model regarded generalisation pram model permits frequency barrier synchronisation demands routing network controlled 
bsp architecture small value regarded pram hashing automatically achieve efficient memory management 
value determines degree parallel slackness required achieve optimal efficiency 
case corresponds idealised pram parallel slackness required 
bsp related data parallelism 
data parallelism important niche field scalable parallel computing 
number interesting programming languages elegant theories developed support data parallel style programming see 
high performance fortran example practical data parallel language 
data parallelism particularly appropriate problems locality crucial 
bsp approach principle offers flexible general style programming provided data parallelism 
current spmd language implemented bsplib large grain data parallel language locality considered programmers great deal control partitioning functionality 
case approaches incompatible fundamental way 
applications flexibility provided bsp approach may required limited data parallel style may offer attractive productive setting parallel software development frees programmer having provide explicit specification various processor scheduling communication memory management aspects parallel computation 
situation bsp cost model play important role terms providing analytic framework performance prediction data parallel program 
bsp handle synchronisation subset processes 
synchronising subset executing processes complex issue ability architecture synchronise necessarily bulk property sense processing power communication resources 
certain architecture provide special hardware mechanism barrier synchronisation processors 
example cray provides add broadcast tree purdue created generic fast cheap barrier synchronisation hardware wide range architectures 
sharing single synchronisation resource concurrent subsets may wish time difficult 
currently exploring issue 
architectures barrier synchronisation implemented software difficulty implementing barriers subsets processors 
remaining difficulty language design clear mimd subset synchronising language retain characteristics bsp 
bsp vector pipelined vliw architectures 
bsp presupposes sequential parts computation processes processor computed 
architectures processor uses specialised technique improve performance harder determine value particular program affect bsp operation cost modelling 
purpose normalising respect processor speed enable terms form hg compared computation times balance computation communication program obvious 
architectures issue multiple instructions cycle require sophisticated normalisation keep quantities comparable useful ways 
bsp doesn model input output memory hierarchy 
properties modelled part cost executing computation part superstep 
modelling latency deep storage hierarchies fits naturally bsp approach latency communication investigations extensions bsp cost model applicable databases underway 
bsp formal semantics 
formal semantics bsp developed 
shows may give algebraic laws developing bsp programs 
bsp semantics case study forthcoming book 
bsp influence design architectures generation parallel computers 
contribution bsp architecture design clarifies factors important performance problems locality 
suggests critical properties architecture ffl high permeability communication system ability move arbitrary patterns data quickly ffl ability reach consistent global state quickly barrier synchronisation 
subtly suggests predictability communication delivery wide range communication patterns important extremely high performance special communication patterns low performance 
words low variance significant low mean 
parameters capture direct way architecture achieves major performance properties 
details exactly topology routing technology congestion control scheme subsumed single consideration total throughput 
bsp model considered felt necessarily inefficient permutation routing 
came appreciated permutation routing necessarily expensive architectures developed 
bsp model considered inefficient requirement barrier synchronisation 
understood barriers need expensive architectures handle developed 
may total exchange primitive central bsp arguments inefficiency may 
new communication technologies atm communication patterns may total exchange turn reasonable standard building block parallel architectures 
find bsp 
development bsp coordinated bsp worldwide organisation researchers users 
information web site www bsp worldwide org 
standard bsplib agreed 
bsp worldwide organises workshops bsp 
general papers bsp :10.1.1.52.5671
groups bsp researchers ffl oxford www comlab ox ac uk groups bsp ffl harvard das www harvard edu cs research bsp html ffl utrecht www math ruu nl people html ffl carleton www scs carleton ca bsp html ffl central florida cs ucf edu faculty html individuals working bsp number universities 

appreciate helpful comments earlier drafts david burgess ga jifeng miller heiko schroder szymanski alexandre 
skillicorn supported part epsrc research gr unified framework parallel programming 
hill mccoll supported part epsrc research gr bsp programming environment bsplib library appendix provides slightly detail current major bsp system bsplib describe interfaces library fortran version available 
initialisation processes created bsplib program operations bsp bsp 
instance bsp bsp pair program different ways start bsplib program bsp bsp statements program entire bsplib computation spmd 
alternative mode single process starts execution determines number parallel processes required calculation 
spawns required number processes bsp 
execution spawned processes continue spmd manner bsp encountered processes 
point processes process zero terminated process zero left continue execution rest program sequentially 
problem providing mode parallel machines available today example distributed memory machines ibm sp cray meiko cs gc hitachi sr provide dynamic process creation 
simulate dynamic spawning operation bsp init takes argument procedure name 
procedure named bsp init contain bsp bsp statements 
interface library operations void void void int argc char argv void int void number processes requested user 
name procedure contains bsp bsp statements 
argc argv command line size arguments 
enquiry operations determine total number processes process identify interface operations int int function bsp called bsp returns number processors available 
called bsp returns actual number processes allocated program number processes requested bsp 
processes created bsp unique associated value range gamma 
function bsp pid returns associated value process executing function call 
synchronisation bsplib calculation consists sequence supersteps 
superstep start identified call library procedure bsp sync interface void ways communicating processes direct remote memory access bsp version message passing 
communication operations defined stack heap allocated data structures static data 
achieved allowing process certain registered areas remote memory 
registration procedure processes operation bsp announce address start local area available global remote 
possible execute bsp programs heterogeneous processor architectures 
registration takes effect barrier synchronisation 
void void region int nbytes void void region region starting address region registered unregistered 
name region logically related calls bsp bsp implementations may check true 
nbytes size region range checking 
processor maintains stack registration slots 
logically related calls bsp different processes ith call process related ith call associate variable name addresses mapped process available slot 
bsp invalidates slot top stack association variable name addresses different processors 
argument logically unnecessary may implementation check user action intent match 
intent registration simple refer variables processes requiring locations explicitly known 
registered name put get translated address corresponding remote variable name 
example process int sizeof int sizeof int process int sizeof int process process register slot 
process executes put destination region name mapped region address associated slot process 
variable process value placed result put 
overlapping regions may registered slot 
slots form stack processes unregister regions reverse order registered 
operation bsp put pushes locally held data registered remote memory area target process active participation target process 
operation bsp get reaches registered local memory process copy data values held data structure local memory 
gets executed puts superstep line semantics communications take effect locally superstep 
interfaces void bsp hp put int pid const void src void dst int offset int nbytes pid identifier process data stored 
src location byte transferred put operation 
calculation src performed process initiates put 
dst base address area data stored 
previously registered data area 
offset displacement bytes dst src copy 
calculation offset performed process initiates put 
nbytes number bytes transferred src dst 
assumed src dst addresses data structures nbytes size 
void bsp hp get int pid const void src int offset void dst int nbytes pid identifier process data obtained 
src base address area data obtained 
src previously registered data structure 
offset offset src 
calculation offset performed process initiates get 
dst location byte data obtained placed 
calculation dst performed process initiates get 
nbytes number bytes transferred src dst 
assumed src dst addresses data structures nbytes size 
semantics adopted bsplib bsp put communication buffered locally 
put executed data transferred copied user address space immediately 
executing process free alter contents locations return call put 
semantics clean safety maximized puts may unduly tax memory resources implementation preventing large transports data 
consequently bsplib provides high performance put operation bsp semantics unbuffered locally unbuffered remotely 
operation requires care correct data delivery guaranteed communication local remote computations modify source destination areas superstep 
main advantage operation economical memory 
particularly useful applications repeatedly transfer large data sets 
bsp get bsp operations reach local memory process copy previously registered remote data held data structure local memory process initiated 
bulk synchronous remote memory access convenient style programming bsp computations statically analysed straightforward way 
convenient computations volumes data communicated irregular computation performed superstep depends quantity form data received start 
appropriate style programming cases bulk synchronous message passing 
non blocking send operation delivers messages system buffer associated destination process 
message guaranteed destination buffer subsequent superstep accessed destination process superstep 
collection messages sent process implied ordering receiving 
messages may tagged programmer identify tag 
bsplib bulk synchronous message passing idea part messages fixed length part carrying tagging information help receiver interpret message variable length part containing main data payload 
call portion tag variable length portion payload 
programs part complicated structure 
length tag required fixed particular superstep may vary supersteps 
buffering mode operations buffered locally buffered remotely 
procedure set tag size called collectively processes 
superstep bsp set tag size called called sending messages 
void int tag bytes entry procedure specifies size fixed length portion message current superstep updated default tag size zero 
return procedure tag bytes changed reflect previous value tag size allow inside procedures 
tag size incoming messages prescribed outgoing tag size previous step 
bsp send operation send message consists tag payload specified destination process 
destination process able access message subsequent superstep 
interface void int pid const void tag const void payload int pid identifier process data sent 
tag token identify message 
size determined value specified bsp set size tag 
payload location byte payload communicated 
payload bytes size payload 
copies tag payload message user space system returning 
tag payload inputs may changed user immediately bsp send 
receive message operations bsp get tag bsp move 
operation bsp get tag returns tag message buffer 
operation bsp move copies payload message buffer payload removes message buffer 
interface void int status void tag status returns system buffer empty 
returns length payload message buffer 
length allocate appropriately sized data structure copying payload bsp move 
tag unchanged system buffer empty 
assigned tag message buffer 
void void payload int payload address message payload copied 
buffer advanced message 
reception nbytes specifies size reception area payload copied 
reception nbytes copied payload 
int void void bsp function returns system buffer empty 
returns length payload message buffer places pointer tag tag ptr buf places pointer payload payload ptr buf conceptually removes message advancing pointer representing head buffer 
note bsp move flushes corresponding message buffer bsp get tag 
allows program get tag message payload size bytes obtaining payload message 
require program uses fixed length tag incoming messages program call bsp move get successive message tags 
bsp get tag called repeatedly return tag call bsp move 
halt function bsp abort print error message followed halt entire bsplib program 
routine designed require barrier synchronisation processes 
single process halt entire bsplib program 
void char format format style format string printf 
arguments interpreted way variable number arguments printf 
function bsp time provides access high precision timer accuracy timer implementation specific 
function local operation process issued point bsp 
result timer time seconds bsp 
semantics bsp time bsp timers process 
bsplib impose synchronisation requirements timers process 
double benchmarking oxford bsp toolset flags resolve oxford bsp toolset flags resolve seconds elapsed cray tue jul seconds bsp seconds kbytes bsp fold bsp fold bsp fold bsp probe bsp probe bsp probe bsp probe bsp probe step filename line process process process process process process process process cyclic shift followed total exchange processor cray bsp parameter measures minimum time processors barrier synchronise 
benchmarked repeatedly sampling barrier synchronisation whilst measuring wall clock time synchronisations 
repeated barrier synchronisation produces pessimistic value models case computation part superstep completes processor moment 
produces contention resources synchronising 
values bsp parameter calculated 
value experienced routing local communication cyclic shift second global communication total exchange 
calculating value benchmark calculates value equation 
done routing fixed sized relation sampling iterations performed relation single message size messages size messages size words 
figures show communication profiles benchmark program running cray ibm sp 
contains graphs 
upper graph contains breakdown communication patterns arise superstep benchmark 
benchmark repeatedly routes relation albeit different mix message sizes time bars upper graph size 
lower graph shows actual value attained superstep superstep basis calculated execution time superstep 
oxford bsp toolset flags resolve oxford bsp toolset flags resolve seconds elapsed cray tue jul seconds bsp seconds mbytes bsp fold bsp fold bsp fold bsp probe bsp probe bsp probe bsp probe bsp probe step filename line proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
proc 
cyclic shift followed total exchange processor cray exponential curve shows value local communication phase cyclic shift benchmark 
notice curve match equation uses parameter account extra cost communicating small messages 
second curve shows value routing series total exchanges 
size relation mix message sizes benchmark local communication benchmark 
ensures benchmarks total theoretical cost take time run 
left hand side curve shows value communication device calculated benchmark program dotted line graph shows value table 
noted implementation bsplib cray optimisation combines small messages optimisation 
little need optimisation close fit bsp computer constant scalable predictable values borne larger number processors benchmark seen 
oxford bsp toolset flags puts res 
oxford bsp toolset flags puts res seconds elapsed ibm sp tue jul seconds bsp seconds kbytes bsp fold bsp fold bsp fold bsp probe bsp probe bsp probe bsp probe bsp probe step filename line process process process process process process process process cyclic shift followed total exchange processor ibm sp shows benchmark running processor ibm sp 
cray value unpredictable 
value times larger cray sp node computation rate twice absolute values closely matched machines 
upper graph seen amount data communicated gradually grows benchmark routes fixed size relation 
reason difference sp small messages combined 
combining information concerning size destination individual communications sent combined individual communications destination process unpack data correctly 
total size data sent may triple due extra unpacking information 
difference implementation issue reflected values reported table benchmark calculates value fixed size data communicated 
beguelin dongarra geist manchek sunderam 
enhancements pvm 
international journal supercomputing applications high performance computing 
adam beguelin jack dongarra geist robert manchek sunderam 
users guide pvm parallel virtual machine 
technical report cs university tennessee july 
bilardi spirakis 
bsp vs logp 
proceedings th annual symposium parallel algorithms architectures pages june 
boppana chalasani 
comparison adaptive wormhole routing algorithms 
acm sigarch computer architecture news 
brent 
parallel evaluation general arithmetic expressions 
journal acm april 
giles 
multigrid aircraft computations parallel library 
parallel computational fluid dynamics implementation results parallel computers 
proceedings parallel cfd pages pasadena ca usa june 
elsevier north holland 
culler karp patterson schauser santos von eicken :10.1.1.160.9052
logp realistic model parallel computation 
fourth acm sigplan symposium principles practice parallel programming san diego ca may 
dietz 
papers purdue adapter parallel execution rapid synchronization 
technical report tr ee purdue school electrical engineering march 
geist adam beguelin jack dongarra jiang robert manchek sunderam 
pvm users guide manual 
oak ridge national laboratory oak ridge tennessee may 
geist 
pvm network computing 
editor parallel computation lecture notes computer science pages 
springer 
lang rao suel 
efficiency portability programming bs model 
proceedings th annual symposium parallel algorithms architectures pages june 
mark kevin lang satish rao 
green bsp library 
technical report university central florida august 
hill lang mccoll rao stefanescu suel 
proposal bsp worldwide standard 
bsp worldwide www bsp worldwide org april 
gropp lusk skjellum 
mpi portable parallel programming 
mit press cambridge ma 
miller chen 
algebraic laws bsp programming 
proceedings europar 
springer verlag lecture notes computer science appear 
hill skillicorn 
lessons learned implementing bsp 
technical report tr oxford university computing laboratory november 
hill skillicorn 
practical barrier synchronisation 
technical report tr oxford university computing laboratory august 
jonathan hill paul david burgess 
theory practice tool bsp performance prediction 
europar number lecture notes computer science pages 
springer verlag august 
hoare 
unified theories programming 
prentice hall international appear 
:10.1.1.52.5671
performance parameters benchmarking supercomputers 
parallel computing 
simon knee 
program development performance prediction bsp machines opal 
technical report prg tr oxford university computing laboratory august 
koelbel schreiber steele jr 
high performance fortran handbook 
mit press cambridge ma 
mccoll :10.1.1.52.5671
scalable computing 
van leeuwen editor computer science today trends developments volume lecture notes computer science pages 
springer verlag 
mccoll miller 
gpl language manual 
technical report esprit project oxford university computing laboratory october 
mccoll 
general purpose parallel computing 
gibbons spirakis editors lectures parallel computation cambridge international series parallel computation pages 
cambridge university press cambridge 
mccoll 
special purpose parallel computing 
gibbons spirakis editors lectures parallel computation cambridge international series parallel computation pages 
cambridge university press cambridge 
message passing interface forum 
mpi message passing interface 
proc 
supercomputing pages 
ieee computer society 
richard miller 
library bulk synchronous parallel programming 
proceedings bcs parallel processing specialist group workshop general purpose parallel computing pages december 
richard miller 
approaches architecture independent parallel computation 
phil thesis oxford university computing laboratory wolfson building parks road oxford ox qd 
monk 
parallel finite element method electromagnetic scattering 
compel supp 
norton szymanski 
plasma simulation networks workstations bulk synchronous parallel model 
proceedings international conference parallel distributed processing techniques applications athens ga november 
quinn hatcher 
utility communication computation overlap data parallel programs 
parallel distributed computing 
reed 
portability predictability performance parallel computing bsp practice 
concurrency practice experience appear 
skillicorn 
foundations parallel programming 
cambridge series parallel computation 
cambridge university press 
leslie valiant 
bridging model parallel computation 
communications acm august 
valiant 
general purpose parallel architectures 
van leeuwen editor handbook theoretical computer science vol 
elsevier science publishers mit press 

