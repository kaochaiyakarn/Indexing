bayesian interpolation david mackay computation neural systems california institute technology pasadena ca mackay hope caltech edu may submitted neural computation review bayesian analysis laplace bayesian method model comparison developed depth 
bayesian approach regularisation model comparison demonstrated studying inference problem interpolating noisy data 
concepts methods described quite general applied problems 
regularising constants set examining posterior probability distribution 
alternative regularisers priors alternative basis sets objectively compared evaluating evidence 
occam razor automatically embodied framework 
way bayes infers values regularising constants noise levels elegant interpretation terms effective number parameters determined data set 
framework due gull skilling 
data modelling occam razor science central task develop compare models account data gathered 
particular true problems learning pattern classification interpolation clustering 
levels inference involved task data modelling 
level inference assume models invented true fit model data 
typically model includes free parameters fitting model data involves inferring values parameters probably take data 
results inference summarised probable parameter values error bars parameters 
repeated model 
second level inference task model comparison 
wish compare models light data assign sort preference ranking alternatives 
example consider task interpolating noisy data set 
data set interpolated splines model radial basis functions polynomials feedforward neural networks 
level inference take model individually find best fit interpolant model 
second level inference want rank alternative models state particular data set example splines probably best interpolation model interpolant modelled polynomial probably cubic 
bayesian methods able consistently quantitatively solve inference tasks 
popular myth states bayesian methods differ orthodox statistical methods inclusion subjective priors arbitrary difficult assign usually don difference 
true level inference bayesian results differ little outcome orthodox attack 
widely appreciated bayes performs second level inference 
bayesian methods totally different orthodox methods 
model comparison task virtually ignored statistics texts general orthodox method exists solving problem 
model comparison difficult task possible simply choose model fits data best complex models fit data better maximum likelihood model choice lead inevitably implausible parameterised models 
occam razor principle unnecessarily complex models preferred simpler ones 
bayesian methods automatically quantitatively embody occam razor ad hoc penalty terms 
complex hypotheses automatically self penalising bayes rule 
gives basic intuition expected 
bayesian methods laid depth jeffreys 
general review bayesian philosophy see excellent papers loredo jaynes 
jeffreys emphasis bayesian probability theory formally utilize prior information perform inference way explicit prior knowledge ignorance orthodox methods omit 
jeffreys laid foundation bayesian model comparison involve emphasis prior information 
aspect bayesian gather data create alternative models fit model data assign preferences alternative models gather data create new models choose actions oe bayesian inference fits science particular pattern classification learning interpolation illustrates abstraction central part scientific process processes involving collecting modelling data 
double framed boxes denote steps involve inference 
steps bayes 
bayes tell invent hypotheses example 
box fitting model data task inferring model parameters model data 
bayes may find probable parameter values error bars parameters 
result applying bayes problem little different result orthodox statistics 
second inference task model comparison light data bayes class 
second inference problem requires quantitative occam razor penalise complex models 
bayes assign objective preferences alternative hypotheses way automatically embodies occam razor 
oe data set evidence djh djh bayes embodies occam razor gives basic intuition complex hypotheses penalised 
horizontal axis represents space possible data sets bayes rule rewards hypotheses proportion predicted data occurred 
predictions quantified normalised probability distribution probability data model called evidence djh 
simple hypothesis limited range predictions shown djh powerful hypothesis example free parameters able predict greater variety data sets 
means predict data sets region strongly assume equal prior probabilities assigned hypotheses 
data set falls region powerful model probable hypothesis 
analysis developed applied real world problems 
review bayesian model comparison regularisation noise estimation studying problem interpolating noisy data 
bayesian framework describe tasks due gull skilling bayesian methods achieve state art image reconstruction 
approach regularisation developed part szeliski 
bayesian model comparison discussed bretthorst bayesian methods push back limits nmr signal detection 
quantities data collected science engineering continue increase computational power techniques available model data multiply believe bayesian methods prove important tool refining modelling abilities 
hope review help introduce techniques neural modelling community 
companion demonstrate techniques applied backpropagation neural networks :10.1.1.27.5965
evidence occam factor write bayes rule levels inference described see explicitly bayesian model comparison works 

model fitting 
assuming model true infer model parameters data bayes rule posterior probability parameters wjd djw wjh djh words posterior likelihood prior evidence normalising constant djh commonly ignored irrelevant level inference choice important second level inference name evidence common gradient methods find maximum posterior defines probable value parameters wmp error bars best fit parameters obtained curvature posterior 
writing hessian rr log wjd taylor expanding log posterior wmp wjd wmp jd exp see posterior may locally approximated gaussian covariance matrix error bars interpolation models discussed single maximum posterior distribution gaussian approximation exact course case general problem 
multiple maxima complicate analysis bayesian methods successfully applied :10.1.1.27.5965

model comparison 
second level inference wish infer model plausible data 
posterior probability model jd djh notice data dependent term djh evidence appeared normalising constant 
assuming reason assign strongly differing priors alternative hypotheses hypotheses ranked evaluating evidence 
equation normalised scientific process may develop new models data arrived failure models occurs example 
start completely defined space hypotheses 
inference open ended continually seek probable models account data gather 
new models compared previous models evaluating evidence 
evidence bayesian transportable quantity comparing alternative hypotheses 
key concept assign preference alternative models bayesian evaluates evidence djh 
course evidence story reason assign unequal priors alternative hypotheses 
evidence model comparison equivalent maximum likelihood parameter estimation 
classic example sure thing hypothesis fl jaynes hypothesis data set precise data set occured evidence sure thing hypothesis huge 
sure thing belongs immense class similar hypotheses assigned correspondingly tiny prior probabilities posterior probability sure thing negligible alongside sensible model 
clearly models developed need think precisely priors appropriate 
models sure thing rarely seriously proposed real life 
modern bayesian approach priors pointed emphasis modern bayesian approach inclusion priors inference widely held 
significant subjective prior entire 
interested see problems subjective priors arise see 
emphasis degrees preference alternative hypotheses represented probabilities relative preferences hypotheses assigned evaluating probabilities 
historically bayesian analysis accompanied methods right prior problem 
modern bayesian take fundamentalist attitude assigning right priors different priors tried particular prior corresponds hypothesis way world compare alternative hypotheses light data evaluating evidence 
way alternative regularisers compared example 
try hypothesis obtain awful predictions learnt 
failure bayesian prediction opportunity learn able come back data set new hypotheses new priors example 
evaluating evidence explicitly study evidence gain insight bayesian occam razor works 
evidence normalising constant equation jh djw wjh dw problems including interpolation common posterior wjd djw wjh strong peak probable parameters wmp 
evidence approximated height peak integrand djw wjh times width jh evidence best fit likelihood occam factor wmp wjh wjd occam factor shows quantities determine occam factor hypothesis having single parameter prior distribution dotted line parameter width posterior distribution solid line single peak wmp characteristic width 
occam factor evidence best fit likelihood model achieve multiplying occam factor term magnitude penalises having parameter interpretation occam factor posterior uncertainty imagine simplicity prior wjh uniform large interval representing range values thought possible data arrived 
occam factor ratio posterior accessible volume parameter space prior accessible volume factor hypothesis space collapses data arrives 
log occam factor interpreted amount information gain model data arrives 
summary complex model parameters free vary large range penalised larger occam factor simpler model 
model achieves greatest evidence determined trade minimising natural complexity measure minimising data misfit 
occam factor parameters dimensional posterior approximated gaussian occam factor determinant gaussian covariance matrix jh det evidence best fit likelihood occam factor rr log wjd hessian evaluated calculated error bars wmp comments ffl bayesian model selection simple extension maximum likelihood model selection evidence obtained multiplying best fit likelihood occam factor 
evaluate occam factor need hessian bayesian method model comparison evaluating evidence computationally demanding task finding model best fit parameters error bars 
ffl common degeneracies models parameters equivalent parameters relabeled affecting likelihood 
cases right hand side equation multiplied degeneracy wmp give correct estimate evidence 
ffl minimum description length mdl methods closely related bayesian framework 
log evidence log djh number bits ideal shortest message encodes data model akaike criteria approximation mdl 
implementation mdl necessitates approximations evaluating length ideal shortest message 
see advantage mdl recommend evidence approximated directly 
noisy interpolation problem bayesian interpolation noise free data studied skilling 
study case dependent variables assumed noisy 
am examining case independent variables noisy 
different difficult problem studied case straight line fitting gull 
assume data set interpolated set pairs mg label running pairs 
simplicity treat scalars method generalises multidimensional case 
define interpolation model set fixed basis functions foe chosen interpolated function assumed form oe parameters inferred data 
data set modelled deviating mapping additive noise process xm case adaptive basis functions known feedforward neural networks examined companion 
modelled zero mean gaussian noise standard deviation oe probability data parameters jw fi exp fied djw zd fi fi oe ed zd fi jw fi called likelihood 
known finding maximum likelihood parameters wml may ill posed problem 
minimises ed underdetermined depends sensitively details noise data maximum likelihood interpolant cases oscillates wildly fit noise 
complete interpolation model need prior expresses sort smoothness expect interpolant 
may prior form ff exp ffe ff example functional dx regulariser cubic spline interpolation 
parameter ff measure smooth expected 
prior written prior parameters ff exp ffe zw ff zw exp ew commonly referred regularising function 
interpolation model complete consisting choice basis functions noise model parameter fi prior regulariser regularising constant ff 
level inference ff fi known posterior probability parameters wjd ff fi djw fi djff fi writing ffe fie posterior wjd ff fi exp zm ff fi strictly probability written fi interpolation models predict distribution input variables liberty notation taken companion 
strictly particular prior may improper form constrained prior 
regulariser omitted conditioning variables likelihood data distribution depend prior known 
similarly prior depend fi 
name stands misfit demonstrated natural measure misfit fie zm ff fi exp see minimising combined objective function corresponds finding probable interpolant wmp error bars best fit interpolant obtained hessian rrm evaluated wmp known bayesian view regularisation 
bayes lot just provide interpretation regularisation 
described far just levels inference 
second level model comparison described sections splits second third level problem interpolation model continuum sub models different values ff fi 
second level bayes allows objectively assign values ff fi commonly unknown priori 
third bayes enables quantitatively rank alternative basis sets regularisers priors principle alternative noise models 
furthermore quantitatively compare interpolation model interpolation learning models neural networks similar bayesian approach applied 
second third level inference succesfully executed occam razor 
bayesian theory second third levels inference worked goal review framework 
section describe bayesian method choosing ff fi section describe bayesian model comparison interpolation problem 
inference problems solved evaluating appropriate evidence 
selection parameters ff fi typically ff known priori fi unknown 
ff varied properties best fit probable interpolant vary 
assume prior splines prior defined earlier imagine interpolate large value ff constrain interpolant smooth flat fit data 
ff decreased interpolant starts fit data better 
ff smaller interpolant oscillates wildly overfit noise data 
choice best value ff occam razor problem large values ff correspond simple hypotheses constrained precise predictions saying interpolant expected extreme curvature tiny value ff corresponds powerful flexible hypothesis says interpolant prior belief smoothness weak 
task find value ff small data fitted small overfitted 
severely ill posed problems deconvolution precise value regularising parameter increasingly important 
orthodox statistics ways assigning values parameters example misfit criteria cross validation 
gull demonstrated popular misfit criteria incorrect bayes sets param interpolant data interpolant error bars data interpolant data best interpolant depends ff figures introduce data set interpolated variety models 
notice density data points uniform axis 
figures data set interpolated radial basis function model basis equally spaced cauchy functions radius 
regulariser ew coefficients basis functions 
shows probable interpolant different value ff note extreme values data overfitted respectively 
ff probable value ff 
probable interpolant displayed oe error bars represent uncertain interpolant point assumption interpolation model value ff correct 
notice error bars increase magnitude data sparse 
error bars include datapoint close radial basis function model expect sharp discontinuities point interpreted improbable outlier 
eters 
cross validation may unreliable technique large quantities data available 
see section discussion cross validation :10.1.1.27.5965
explain bayesian method setting ff fi reviewing statistics misfit 
misfit effect parameter measurements gaussian variables mean standard deviation oe statistic oe measure misfit 
known priori expectation fitted data setting degree freedom expectation 
second case measured parameter measured data 
parameter determined data way unavoidable parameter fits noise 
expectation reduced 
basis distinction oe oe buttons calculator 
common distinction ignored cases interpolation number free parameters similar number data points essential find analogous distinction 
demonstrated bayesian choice ff fi simply expressed terms effective number measured parameters fl derived 
misfit criteria principles set parameters ff fi requiring particular value 
discrepancy principle requires principle requires number free parameters 
find intuitive misfit criterion arises optimal value fi hand bayesian choice ff unrelated misfit 
bayesian choice ff fi infer data value ff fi bayesians evaluate posterior probability distribution ff djff fi ff fi dja data dependent term djff fi appeared earlier normalising constant equation called evidence ff fi 
similarly normalising constant called evidence turn compare alternative models light data 
ff fi flat prior evidence function assign preference alternative values ff note satisfactory simply maximise likelihood ff fi likelihood skew peak maximum likelihood value parameters place posterior probability 
get feeling familiar problem examine posterior probability parameters gaussian oe samples maximum likelihood value oe oe probable value oe oe fi 
djff fi zm ff fi zw ff fi defined earlier 
occam razor implicit formula ff small large freedom range possible values automatically penalised consequent large value zw models fit data achieve large value zm model finely tuned fit data penalised smaller value zm optimum value ff achieves compromise fitting data powerful model 
assign preference ff fi computational task evaluate integrals zm zw zd come back task moment 
sounds determining prior data arrived 
aside omitted reading 
heard preceding explanation bayesian regularisation prior chosen ensemble possible priors data arrived 
precise described probable prior probable value ff selected prior infer interpolant 
bayes infer interpolant combined ensemble priors define prior integrate ensemble inference 
happens follow proper approach 
preceding method probable prior emerge approximation 
examine true posterior wjd obtained integrating ff fi wjd wjd ff fi ff dff dfi words posterior probability written linear combination posteriors values ff fi 
posterior density weighted probability ff fi data appeared 
means ff single peak ff fi true posterior wjd dominated density wjd ff fi 
long properties posterior wjd ff fi change rapidly ff fi near ff fi peak ff strong justified approximation wjd wjd ff fi evaluating evidence return train thought equation 
evaluate evidence ff fi want find integrals zm zw zd typically difficult integral evaluate zm zm ff fi exp ff fi regulariser quadratic functional favourites ed ew quadratic functions evaluate zm exactly 
letting ffc fib wmp wmp wmp wmp means zm gaussian integral zm mmp det cases regulariser quadratic example entropy gaussian approximation 
write log evidence ff fi log djff fi ffe mp fie mp log det log zw ff log zd fi log term fie mp represents misfit interpolant data 
terms ffe mp log det log zw ff constitute occam factor penalising powerful values ff ratio det zw ff ratio posterior accessible volume parameter space prior accessible volume 
illustrates behaviour various terms function ff radial basis function model illustrated 
just proceed evaluate evidence numerically function ff fi deep fruitful understanding problem possible 
properties evidence maximum maximum ff fi djff fi zm ff fi zw ff zd fi remarkable properties give deeper insight bayesian approach 
results section useful numerically intuitively 
gull transform basis hessian ew identity transformation simple case quadratic ew rotate eigenvector basis stretch axes quadratic form ew homogeneous 
natural basis prior 
continue refer parameter vector basis rrm differentiate log evidence respect ff fi find condition satisfied maximum 
log evidence log djff fi ffe mp fie mp log det log fi log ff log differentiating respect ff need evaluate dff log det ffi fib dff log det trace da dff alpha log evidence data log occam factor alpha log evidence test error test error gamma choosing ff evidence function ff radial basis function model graph shows log evidence function ff shows functions log evidence data misfit fied weight penalty term log occam factor det zw ff 
criteria optimising ff graph shows log evidence function ff functions intersection locates evidence maximum number parameter measurements fl shown test error rescaled test sets finding test error minimum alternative criterion setting ff 
test sets twice large size interpolated data set 
note point fl clear unambiguous said minima test energies 
evidence gives ff confidence interval 
test error minima widely distributed finite sample noise 
wml wmp bad parameter measurements components parameter space directions parallel eigenvectors data matrix circle represents characteristic prior distribution ellipse represents characteristic contour likelihood centred maximum likelihood solution wml wmp represents probable parameter vector 
direction small compared ff data strong preference value poorly measured parameter term ff close zero 
direction large determined data term ff close 
result exact ew ed quadratic 
result approximation omitting terms ff 
differentiating setting derivative zero obtain condition probable value ff ffe mp quantity left dimensionless measure amount structure introduced parameters data fitted parameters differ null value 
interpreted parameters equal oe ff oe quantity right called number parameter measurements fl value written terms eigenvalues fib eigenvalues ff fl ff ff ff ff eigenvalue measures strongly parameter determined data 
ff measures strongly parameters determined prior 
term ff number measures strength data relative prior 
direction parameter space small compared ff contribute number parameter measurements 
fl measure effective number parameters determined data 
ff fi fl increases concept important locating optimum value ff fl parameter measurements expected contribute reduction data misfit occurs model fitted noisy data 
process fitting data unavoidable fitting model noise occur components noise indistinguishable real data 
typically unit noise fitted determined parameter 
poorly determined parameters determined regulariser reduce way 
examine concept enters bayesian choice fi 
recall expectation misfit true interpolant data know true interpolant misfit measure access data fie discrepancy principle orthodox statistics states model parameters adjusted orthodox approaches suggest estimate noise level set number free parameters 
find opinion bayes rule matter 
differentiate log evidence respect fi obtain fie fl probable noise estimate fi satisfy fl 
bayesian estimate noise level naturally takes account fact parameters determined data inevitably suppress noise data poorly determined parameters 
note value enters determination fi misfit criteria role bayesian choice ff 
summary optimum value ff fi fl fl 
notice implies total misfit ffe fie satisfies simple equation bayesian choice ff illustrated 
illustrates functions involved bayesian choice ff compares test error approach 
demonstration bayesian choice fi omitted straightforward fi fixed true value demonstrations 
results generalise case separate regularisers independent regularising constants ff ff 
case regulariser number measured parameters fl associated 
multiple regularisers companion neural networks 
finding evidence maximum head approach involve evaluating det searching ff fi results enable speed search example re estimation formulae ff fl replace evaluation det evaluation large dimensional problems task demanding skilling developed methods estimating statistically 
bayesian model comparison rank alternative basis sets regularisers priors light data examine posterior probabilities dja data dependent term evidence appeared earlier normalising constant evaluated integrating evidence ff fi dja dja ff fi ff fi dff dfi assuming reason assign strongly differing priors alternative ranked just examining evidence 
evidence compared evidence equivalent bayesian analysis learning interpolation models allow data assign preference alternative models 
notice pointed earlier modern bayesian framework includes emphasis defining right prior ought interpolate 
invent priors regularisers want allow data tell prior probable 
evaluating evidence ff fi vary single evidence maximum obtained ff fi quadratic ed ew 
evidence maximum usually approximated separable gaussian differentiating twice obtain gaussian error bars log ff log fi log ff fl log fi fl putting error bars obtain evidence 
dja djff fi ff fi log ff log fi prior ff fi assigned 
time met infamous subjective priors supposed plague bayesian methods 
answers question 
method assigning preference alternatives implicitly assign priors 
bayesians adopt healthy attitude sweeping carpet 
thought reasonable values usually assigned subjective priors degree reasonable subjectivity assignments quantified 
example reasonable prior unknown standard deviation states oe unknown range orders magnitude 
prior contributes subjectivity factor value evidence 
degree subjectivity negligible compared evidence differences 
noisy interpolation example models considered include free parameters ff fi 
need assign value ff fi assume flat prior cancels compare alternative interpolation models 
condition approximation spectrum eigenvalues fib number eigenvalues fold ff 
analytic methods performing integrals fi 
log evidence number basis functions hermite polynomials hermite functions log evidence number basis functions cauchy functions gaussians log evidence number coefficients test error number coefficients evidence data set log evidence hermite polynomials functions 
notice evidence maximum 
gentle slope right due occam factors penalise increasing complexity model 
log evidence radial basis function models 
notice occam penalty additional coefficients models increased density radial basis functions model powerful 
oscillations evidence due details basis functions relative data points 
log evidence splines 
evidence shown alternative splines regularisers see text 
representation spline model obtained limit infinite number coefficients 
example yields cubic splines model 
test error splines 
number data points test set number data points training set 
axis shows ed value ed true interpolant expectation 
demonstration demonstrations dimensional data sets imitation 
data set discontinuities derivative second smoother data set 
demonstrations fi left free parameter fixed known true value 
bayesian method setting ff assuming single model correct demonstrated quantified error bars placed probable interpolant 
method evaluating error bars posterior covariance matrix parameters get variance linear function parameters oe error bars single point var oe oe 
access full covariance information entire interpolant just pointwise error bars 
possible visualise joint error bars interpolant making typical samples posterior distribution performing random walk posterior bubble parameter space 
simple program random walk interpolation problems examined lack dynamic leave demonstration imagination 
section objective comparison alternative models demonstrated illustrated models differing number free parameters example polynomials different degrees comparisons models disparate splines radial basis functions feedforward neural networks 
individual model value ff optimised evidence evaluated integrating ff gaussian approximation 
logarithms base hermite polynomials hermite functions occam razor number basis functions shows evidence hermite polynomials different degrees data set shows evidence hermite functions mean hermite polynomials multiplied regulariser form ew cases leading polynomial coefficient hermite polynomials evidence flat regulariser smaller 
notice evidence maximum obtained certain number terms evidence starts decrease 
bayesian occam razor 
additional terms model powerful able predictions 
power automatically penalised 
notice characteristic shape occam hill 
left hill steep simple models fail fit data penalty data scales number data measurements 
side hill steep occam factors scale log number parameters 
note table values maximum evidence achieved models move alternative models 
fixed radial basis functions basis functions oe equally spaced range interest 
examine choices gaussian cauchy function quantitatively compare alternative models spatial correlation data set evaluating evidence 
regulariser ew note model includes new free parameter demonstrations parameter set probable value value maximises evidence 
penalise free parameter occam factor included rp posterior uncertainty prior usually subjective small degree 
radial basis function model identical intrinsic correlation model gull skilling 
shows evidence function number basis functions note models occam penalty large numbers parameters 
reason extra parameters model powerful fixed ff 
increased density basis functions enable model significant new predictions kernel band limits possible interpolants 
splines occam razor choice regulariser implement splines model follows basis functions fourier set cos hx sin hx 
regulariser ew cos sin limit cubic splines regulariser dx regulariser dx shows evidence data set function number terms 
notice terms occam razor cases discussed occur increases model powerful occam penalty 
increasing gives rise penalty 
case fence 
increases regulariser opposed strong curvature 
reach model improbable data fact sharp discontinuities 
evidence choose order splines regulariser 
data set turns probable value multiples results smoother data set shows data set comes smoother interpolant data set table summarises evidence alternative models 
note differences data set splines family probable value shifted upwards models penalise curvature strongly intuitively expect radial basis function models gaussian correlation model slight edge hermite functions poor model data set place reason clear shortly 
data set interpolant splines interpolant error bars data data set interpolated splines 
bayes systematically reject truth ask frequentist question hypotheses offer bayes true model data generated possible bayes systematically ensemble possible data sets prefer false hypothesis 
clearly worst case analysis bayesian posterior may favour false hypothesis 
furthermore skilling demonstrated data sets free form maximum entropy hypothesis greater evidence truth possible happen typical case skilling claim 
show answer effect skilling demonstrated systematic 
precise expectation possible data sets log evidence true hypothesis greater expectation log evidence fixed hypothesis 
cause skilling result 
presumably particular parameter values true model generated data typical prior evaluating evidence true model 
proof 
suppose truth single data set arrives compare evidences different fixed hypothesis 
hypotheses may free parameters irrelevant argument 
intuitively expect evidence djh usually greatest 
examine difference log evidence expectation difference true log djh djh ae djh log djh djh note integral implicitly integrates parameters prior distribution known normalised log minimum value achieved setting distinct hypothesis expected systematically defeat true hypothesis just reason wise bet differently true odds 
ffl important implications 
gives frequentist confidence ability bayesian methods average identify true hypothesis 
secondly provides severe test numerical implementation bayesian inference framework imagine written program evaluates evidence hypotheses generate mock data sources simulating evaluate evidences systematic bias averaged mock data sets estimated evidence favour false hypothesis sure numerical implementation evaluating evidence correctly 
issue illustrated data set truth data set generated quadratic hermite function argument evidence ought probably favour hypothesis interpolant coefficient hermite function hypotheses 
evaluate evidence variety hypotheses confirm greater evidence true hypothesis 
table shows evidence true hermite function model models 
notice truth considerably probable alternatives 
having demonstrated bayes systematically fail hypotheses true examine way framework fail hypotheses offered bayes 
comparison generalisation error popular intuitive criterion choosing alternative interpolants compare errors test set derive interpolant 
cross validation refined version idea 
method relate evaluation evidence described 
displayed evidence family spline interpolants 
shows corresponding test error measured test set size twice big training data set determine interpolant 
similar comparison 
note trends shown evidence matched trends test error flip graph upside 
particular problem ranks alternative spline models evidence similar ranks test error 
evidence maximum surrounded test error minima 
suggests evidence reliable predictor generalisation ability 
necessarily case 
reasons evidence test error correlated 
test error noisy quantity 
necessary devote large quantities data test set obtain reasonable signal noise ratio 
twice data test set difference table evidence data sets data set data set model best parameter values log evidence best parameter values log evidence hermite polynomials hermite functions gaussian radial basis functions cauchy radial basis functions splines splines splines splines splines neural networks neurons neurons ff test error minima exceeds size bayesian confidence interval ff 
second model greatest evidence expected best model time bayesian inferences uncertain 
point bayes quantifies precisely uncertainties relative values evidence alternative models express plausibility models data underlying assumptions 
third evidence generalisation error 
example imagine models probable interpolants happen identical 
case generalisation error solutions 
evidence general typically model priori complex suffer larger occam factor smaller evidence 
fourth test error measure performance single probable interpolant evidence measure plausibility entire posterior ensemble best fit interpolant 
stronger correlation evidence test statistic obtained test statistic average test error posterior ensemble solutions 
ensemble test error easy compute 
fifth interesting reason evidence correlated generalisation error flaw underlying assumptions hypotheses compared poor hypotheses 
poor regulariser example ill matched statistics world bayesian choice ff best terms generalisation error 
failure occurs companion neural networks 
attitude failure bayesian prediction 
failure evidence mean discard evidence generalisation error criterion choosing ff 
failure opportunity learn healthy scientist searches failures yield insights defects current model 
detection failure evaluating generalisation error example motivates search new hypotheses fail way example alternative regularisers tried hypothesis data probable 
uses generalisation error criterion model comparison denied mechanism learning 
development image deconvolution held decades bayesian choice ff bayesian choice ff results obtained making clear poor regulariser motivated immediate search alternative priors new priors discovered search heart state art image deconvolution 
admitting neural networks canon bayesian interpolation models second discuss apply bayesian framework task evaluating evidence feedforward neural networks 
preliminary results methods included table 
assuming approximations valid interesting evidence neural nets spiky smooth data sets 
furthermore neural nets spite arbitrariness yield relatively compact model fewer parameters needed specify splines radial basis function solutions 
developed methods bayesian model comparison regularisation 
models ranked evaluating evidence solely data dependent measure intuitively consistently combines model ability fit data complexity 
regularising constants set maximising evidence 
regularisation problems theory number determined parameters possible perform optimisation line 
interpolation examples discussed evidence set number basis functions polynomial model set characteristic size radial basis function model choose order regulariser spline model rank different models light data 
needed formalise relationship framework pragmatic model comparison technique cross validation 
techniques parallel possible detect flaws underlying assumptions implicit data models 
failures direct search superior models providing powerful tool human learning 
thousands data modelling tasks waiting evidence evaluated 
exciting see learn done 
berger 
statistical decision theory bayesian analysis springer 
bretthorst 
bayesian analysis 
parameter estimation quadrature nmr models 
ii 
signal detection model selection 
iii 
applications nmr magnetic resonance 
davies 
optimization regularization ill posed problems austral 
mat 
soc 
ser 

jr editor 
maximum entropy bayesian methods kluwer 
gull 
bayesian inductive inference maximum entropy maximum entropy bayesian methods science engineering vol 
foundations erickson smith eds kluwer 
gull 
developments maximum entropy data analysis 
gull 
bayesian data analysis straight line fitting 
gull skilling 
quantified maximum entropy 
user manual north sg nr england 
haussler kearns schapire 
bounds sample complexity bayesian learning information theory vc dimension preprint 
jaynes 
bayesian methods general background maximum entropy bayesian methods applied statistics ed 
justice jeffreys 
theory probability oxford univ press 
loredo 
laplace supernova sn bayesian inference astrophysics maximum entropy bayesian methods ed 
kluwer 
mackay :10.1.1.27.5965
practical bayesian framework backprop networks submitted neural computation 
neal 
bayesian mixture modeling monte carlo simulation preprint 
poggio torre koch 
computational vision regularization theory nature 
schwarz 
estimating dimension model ann 
stat 


bayesian interpolation 
skilling robinson gull 
probabilistic displays 
skilling editor 
maximum entropy bayesian methods cambridge kluwer 
skilling 
parameter estimation quantified maxent 
skilling 
eigenvalues mega dimensional matrices 
szeliski 
bayesian modeling uncertainty low level vision kluwer 
titterington 
common structure smoothing techniques statistics int 
statist 
rev 
wallace boulton 
information measure classification comput 

wallace freeman 
estimation inference compact coding statist 
soc 

mike lewicki nick weir david robinson helpful conversations andreas herz comments manuscript 
am grateful dr goodman dr smyth funding trip maxent 
supported caltech fellowship studentship serc uk 

