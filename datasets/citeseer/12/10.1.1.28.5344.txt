gary william flake steve lawrence 
efficient svm regression training smo accepted publication machine learning 
efficient svm regression training smo gary william flake flake research nj nec com steve lawrence lawrence research nj nec com nec research institute independence way princeton nj sequential minimal optimization algorithm smo shown effective method training support vector machines svms classification tasks defined sparse data sets 
smo differs svm algorithms require quadratic programming solver 
generalize smo handle regression problems 
problem smo rate convergence slows dramatically data non sparse support vectors solution case regression kernel function evaluations tend dominate runtime case 
caching kernel function outputs easily degrade smo performance smo tends access kernel function outputs unstructured manner 
address problem modifications enable caching effectively smo 
regression problems modifications improve converge time order magnitude 
keywords support vector machines sequential minimal optimization regression caching quadratic programming optimization support vector machine svm type model optimized prediction error model complexity simultaneously minimized 
despite having qualities research area svms hindered fact quadratic programming qp solvers provided known training algorithm years 
theorem proved introduced new family svm training procedures 
nutshell osuna theorem showed global svm training problem broken sequence smaller subproblems optimizing subproblem minimizes original qp problem 
sequential minimal optimization algorithm smo introduced extreme example osuna theorem practice 
smo uses subproblem size subproblem analytical solution 
time svms optimized qp solver 
smo shown effective sparse data sets especially fast linear svms algorithm extremely slow non sparse data sets problems support vectors 
regression problems especially prone issues inputs usually non sparse real numbers opposed binary inputs solutions support vectors 
constraints reports smo successfully regression problems 
derive generalization smo handle regression problems address runtime issues smo modifying heuristics underlying algorithm kernel outputs effectively cached 
conservative results indicate high dimensional non sparse data especially regression problems convergence rate smo improved order magnitude 
divided additional sections 
section contains basic overview svms provides minimal framework sections build 
section generalize smo handle regression problems 
simplest implementation smo regression optimize svms regression problems poor convergence rates 
section introduce modifications smo allow kernel function outputs efficiently cached 
section contains numerical results show modifications produce order magnitude improvement convergence speed 
section summarizes addresses research area 
svms consider set data points 
xn input target output 
svm model calculated weighted sum kernel function outputs 
kernel function inner product gaussian basis function polynomial function obeys mercer condition 
output svm linear function inputs linear function kernel outputs 
generality svms take forms identical nonlinear regression radial basis function networks multilayer perceptrons 
difference svms methods lies objective functions optimized respect optimization procedures uses minimize objective functions 
linear noise free case classification output svm written 
optimization task defined minimize jjwjj subject 
intuitively objective function expresses notion find simplest model explains data 
basic svm framework generalized include slack variables classifications nonlinear kernel functions regression extensions problem domains 
scope describe derivation extensions basic svm framework 
refer reader excellent tutorials introductions svms classification regression respectively :10.1.1.127.1519
delve derivation specific objective functions far necessary set framework 
general easily construct objective functions similar equation include slack variables misclassification nonlinear kernels 
objective functions modified special case performing regression 
objective functions component minimizes linear constraints obeyed 
optimize objective function converts primal lagrangian form contains minimization terms minus linear constraints multiplied lagrangian multipliers 
primal lagrangian converted dual lagrangian free parameters lagrangian multipliers 
dual form objective function quadratic lagrangian multipliers obvious way optimize model express quadratic programming problem linear constraints 
contribution uses variant platt sequential minimal optimization method generalized regression modified efficiencies 
smo solves underlying qp problem breaking sequence smaller optimization subproblems unknowns 
unknowns parameters analytical solution avoiding qp solver 
smo qp solver dual lagrangian objective functions 
define output function nonlinear svms classification regression primal lagrangian objective functions optimized respect 
case classification output svm defined underlying kernel function 
primal objective function minimized subject box constraint linear constraint userdefined constant represents balance model complexity approximation error 
regression svms minimize functionals form jy jjwjj 
insensitive error function defined jxj jxj jxj output svm takes form intuitively positive negative lagrange multipliers single weight obey primal form equation written minimize objective function respect subject constraints parameter user defined constant represents balance model complexity approximation error 
sections extensive primal equations svm output functions equations 
smo regression mentioned earlier smo new algorithm training svms 
smo repeatedly finds lagrange multipliers optimized respect analytically computes optimal step lagrange multipliers 
lagrange multipliers optimized original qp problem solved 
smo consists parts set heuristics efficiently choosing pairs lagrange multiplier analytical solution qp problem size 
scope give complete description smo heuristics 
reader consult platt papers information 
smo originally designed svms applicable classification problems analytical solution size qp problem generalized order smo regression problems 
bulk section devoted deriving solution 
step size derivation transforming equations substituting new unknowns obey box constraint shorthand ij assume ij ji model output objective function written ij linear constraint 
goal express analytically minimum equation function parameters 
parameters indices unknowns 
rewrite equation aa bb ab term strictly constant respect defined ij ai bi 
note superscript explicitly indicate values computed old parameter values 
means portions expression function new parameters simplifies derivation 
assume constraint true prior change order constraint true step parameter space sum held fixed 
mind rewrite equation function single lagrange multiplier substituting js aa bb ab solve equation need compute partial derivative respect equation strictly differentiable absolute value function 
take dx sgn resulting derivative algebraically consistent sgn sgn aa bb ab setting equation zero yields bb aa ab sgn sgn aa ab sgn sgn aa ab aa ab aa ab ab bb sgn sgn bb aa ab equation write recursive update rule terms old value sgn sgn bb aa ab 
equation recursive sgn 
functions single solution quickly shown subsection 
finding solutions illustrates partial derivative equation primal lagrangian function respect behaves 
kernel function svm obeys mercer condition common ones guaranteed bb aa ab true 
strictly positive equation increasing 
zero piecewise linear discrete jumps illustrated 
putting facts means consider possible solutions equation 
possible solutions correspond equation sgn sgn sgn sgn derivative function kernel function obeys mercer condition derivative equation strictly increasing 
set 
candidates correspond setting transitions need consider linear boxed constraints relate 
particular need lower upper bounds insure range 
max min lower upper bounds respectively guarantees parameters obey boxed constraints 
kkt conditions step described section minimize global objective function parameters violates kkt condition 
kkt conditions regression jy jy jy kkt conditions yield test convergence 
parameter violates kkt condition global minimum reached machine precision 
updating threshold update svm threshold calculate candidate updates 
update new parameters forces svm second forces update parameters hits constraint candidate updates threshold identical 
average candidate updates 
new old aa new old ab old new old ab new old bb old update rules nearly identical platt original derivation 
complete update rule smo regression problems steps performed 
pick parameters parameter violates kkt condition defined equation 
compute raw try equation sgn sgn equal 
new value zero equation accept new value 
step failed try equal accept value property positive negative perturbations yield positive negative value equation 
raw set new raw set new set new raw set new old old new set new specified equations 
outer loop smo non numerical parts heuristics remain 
discussed section improvements smo improve rate convergence regression problems order magnitude 
simple example shows plot scalar function approximated nonlinear svm 
svm gaussian kernel trained proposed algorithm set 
true function shown solid line svm approximation shown dashed line resulting support vectors input points corresponding non zero lagrange multipliers shown points 
seen approximation reasonable choice 
true function approximation support vectors simple svm regression experiment true approximated function shown calculated support vectors 
progress 
iteration previous iteration progress working set data points 

working set consist data points non bounded lagrange multipliers 

data points working set try optimize corresponding lagrange multiplier 
find second lagrange multiplier 
try best looping non bounded multipliers platt heuristic 
try working set 
try find entire set lagrange multipliers 

progress working set data points done 
table basic pseudo code smo building better smo described section smo repeatedly finds lagrange multipliers optimized respect analytically computes optimal step lagrange multipliers 
section concerned analytical portion algorithm 
section concentrate remainder smo consists heuristics pick pairs lagrange multipliers optimize 
scope give complete description smo table gives basic pseudo code algorithm 
information consult platt papers 
referring table notice lagrange multiplier chosen line counterpart chosen line 
smo attempts concentrate effort needed maintaining working set non bounded lagrange multipliers 
idea lagrange multipliers bounds classification regression irrelevant optimization problem tend keep bounded values 
best optimization step take time proportional number lagrange multipliers working set worst take time proportional entire data set 
runtime slower analysis implies candidate second lagrange multiplier requires kernel functions evaluated 
input dimensionality large kernel evaluations may significant factor time complexity 
told express runtime single smo step 



probability second lagrange multiplier working set size working set input dimensionality 
goal section reduce runtime complexity single smo step 


additionally method reducing total number required smo steps introduced reduce cost outer loop smo 
subsections improvements smo described 
fundamental change cache kernel function outputs 
naive caching policy slows smo original algorithm tends randomly access kernel outputs high frequency 
changes designed improve probability cached kernel output exploit fact kernel outputs precomputed 
caching kernel outputs cache typically understood small portion memory faster normal memory 
cache refer table precomputed kernel outputs 
idea frequently accessed kernel outputs stored reused avoid cost recomputation 
cache data structure contains inverse index entries refers index main data set ith cached item 
maintain dimensional array store cached values 
precomputed value ab stored cache space allocated value flag set indicate kernel output needs computed saved 
cache operations applied query returns values indicate ab cache allocated cache cache 
insert compute ab force cache 
indices replaced access return ab fastest method available 
mark indices elements 
policy updating cache expected exceptions ii maintained separate space accessed frequently 
smo working set lagrange multipliers determined step table accesses cache done inserts 
working set proper subset requested indices part working set access done insert 
defined subsection caching kernel outputs smo usually degrades runtime frequency cache misses extra overhead incurred 
problem worse cache size number support vectors solution 
eliminating thrashing shown lines table smo uses hierarchy selection methods order find second multiplier optimize 
tries find heuristic 
fails settles working set 
fails smo starts searching entire training set 
line causes problems smo reasons 
entails extreme amount results multipliers changing 
second caching line interfere update policies cache 
avoid problems heuristic entails modification smo line executed working set entire data set 
execute case sure convergence achieved 
platt proposed modification similar goal mind 
example source code accessed url heuristic corresponds command line option lazy short lazy loops 
optimal steps modification smo takes advantage fact cached kernel outputs accessed constant time 
line table searches entire working set finds multiplier approximately yields largest step size 
kernel outputs multipliers cached computing change objective function results optimizing multipliers takes constant time calculate 
exploiting cached kernel outputs greedily take step yields improvement 
multiplier selected line table 
ab cached calculate new values multipliers analytically constant time 
old values multipliers superscripts shorthand new old values svm output 
change classification objective function equation results accepting new multipliers 
aa aa bb bb ab equation derived substituting equation rewriting equation terms trivially dependent independent difference choices multipliers calculated summations independent terms cancel 
change regression objective function equation similarly calculated 
aa aa bb bb ab modify smo replacing line table code looks best second multiplier equation ab cached 
example source code heuristic corresponds command line option best short best step 
demand incremental svm outputs modification smo method calculate svm outputs rapidly 
loss generality assume svm classification output svm determined equation substituted 
different ways calculate svm outputs single lagrange multiplier changed equation extremely slow 
change equation summation nonzero lagrange multipliers 
incrementally update new value ij clearly method fastest 
smo original form uses third method update outputs multipliers non bounded needed second method output needed incrementally updated 
improve method updating outputs needed computing second third method efficient 
need queues note section refer lagrange multipliers maintain consistency earlier sections notation conflicts equations 
maximum sizes equal number lagrange multipliers third array store time stamp particular output updated 
lagrange multiplier changes value store change multiplier change queues overwriting oldest value 
particular output required number time steps elapsed output updated number nonzero lagrange multipliers calculate output known value changed values queues 
fewer nonzero lagrange multipliers efficient update output second method 
outputs updated demand svm outputs accessed nonuniform manner update method exploit statistical irregularities 
example source code heuristic corresponds command line option clever short clever outputs 
smo decomposition smo caching proposed heuristics yields significant runtime improvement long cache size large number support vectors solution 
cache size small fit kernel outputs support vector pair accesses cache fail runtime increased 
particular problem addressed combining osuna decomposition algorithm smo 
basic idea iteratively build subproblem solve subproblem iterate new subproblem entire optimization problem solved 
qp solver solve subproblem smo chose large cache 
benefits combination fold 
evidence indicates decomposition faster qp solver 
combination smo decomposition functionally identical standard decomposition smo qp solver expect benefit 
second subproblem size cache guarantees kernel outputs required available smo iteration subproblem 
note implementation decomposition naive way constructs subproblems essentially works randomly selected data points violate kkt condition 
example source code heuristic corresponds command line option short subset size 
experimental results simple experiment model mackey glass delay differential equation dx dt ax bx regression svm uses gaussian kernel functions 


problem consists data points inputs delay value forecasts time steps 
equal gaussian kernels variance solution support vectors 
problem simple 
heuristics subset cache time heuristics subset cache time table experimental results results averaged trials 
entries heuristics value indicate lazy loops section best step section clever outputs section 
entries subset indicate subset sized decomposition meaning decomposition 
times cpu seconds mhz pentium ii machine running linux 
table shows average runtime problem different experimental setups 
seen runtime difference plain smo smo proposed changes order magnitude size 
shown smo generalized handle regression problems 
runtime smo greatly improved large dense high dimensional data sets adding caching heuristics 
heuristics designed improve probability cached kernel outputs exploit fact cached kernel outputs ways infeasible non cached kernel outputs 
changes particularly beneficial smo regression problems problems typically support vectors solution 
numerical result section simple experiment significant problem simplicity 
specifically problem data points higher dimensionality support vectors solution differences plain smo modified smo pronounced 
preliminary results indicate changes greatly improve performance smo classification tasks involve large high dimensional non sparse data sets 
concentrate incremental methods gradually increase numerical accuracy 
recursive decompositions may yield improvements 
tommy poggio john platt edgar osuna constantine helpful discussions 
special tommy poggio center biological computational learning mit hosting author research eric baum nec research institute funding portion time write results 
burges 
tutorial support vector machines pattern recognition 
data mining knowledge discovery 
osuna freund girosi 
improved training algorithm support vector machines 
proc 
ieee 
platt 
fast training support vector machines sequential minimal optimization 
scholkopf burges smola editors advances kernel methods support vector learning 
mit press 
platt 
private communication 
platt 
sparseness analytic qp speed training support vector machines 
kearns solla cohn editors advances neural information processing systems 
mit press 
smola scholkopf :10.1.1.127.1519
tutorial support vector regression 
technical report nc tr neurocolt 
vapnik 
nature statistical learning theory 
springer verlag new york 

