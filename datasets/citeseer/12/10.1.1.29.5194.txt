data mining knowledge discovery fl kluwer academic publishers boston 
manufactured netherlands 
comparing classifiers pitfalls avoid recommended approach steven salzberg salzberg cs jhu edu department computer science johns hopkins university baltimore md usa editor usama fayyad 
important component data mining projects finding classification algorithm process requires careful thought experimental design 
done carefully comparative studies classification types algorithms easily result statistically invalid 
especially true data mining techniques analyze large databases inevitably contain statistically data 
describes phenomena ignored invalidate experimental comparison 
phenomena follow apply classification computational experiments aspect data mining 
discusses comparative analysis important evaluating types algorithms provides suggestions avoid pitfalls suffered experimental studies 
keywords classification comparative studies statistical methods 
data mining researchers classifiers identify important classes objects data repository 
classification particularly useful database contains examples basis decision making assessing credit risks medical diagnosis scientific data analysis 
researchers range different types classification algorithms disposal including nearest neighbor methods decision tree induction error back propagation reinforcement learning rule learning 
years variations algorithms developed studies produced comparing effectiveness different data sets real artificial 
classification research past means researchers today confront problem algorithms choose algorithm new problem 
addresses methodology answer question discusses addressed classification community 
discusses pitfalls confront trying answer question demonstrates misleading results easily follow lack attention methodology 
examples machine learning community illustrate careful fast computational methods mine large database 
examples show repeatedly searches large database powerful algorithms easy find phenomenon pattern looks impressive discover 
natural experimental researchers want real data validate systems 
culture evolved machine learning community insists convincing evaluation new ideas takes form experimental testing 
steven salzberg healthy development represents important step maturation field 
indication maturation creation maintenance uc irvine repository machine learning databases contains datasets appeared published 
repository easy machine learning researchers compare new algorithms previous 
data mining field newer area research evolving methodology compare effectiveness different algorithms large databases 
large public databases increasingly popular areas science technology bringing great opportunities technical dangers 
see careful design experimental study publicly available databases 
development maintenance data repositories general positive research classification algorithms relied heavily uci repository shared datasets consequently produced comparative studies results best confusing 
precise commonplace take classifiers compare random selection datasets uci repository 
differences classification accuracy reach statistical significance provided supporting evidence important differences algorithms 
argued comparisons statistically invalid 
message data mining community exceedingly careful powerful algorithms extract information large databases traditional statistical methods designed process 
give examples modify traditional statistics computational evaluations 

comparing algorithms empirical validation clearly essential process designing implementing new algorithms criticisms intended discourage empirical 
classification research component data mining subfield machine learning need specific focused studies compare algorithms carefully 
evidence date evaluations done nearly example prechelt surveyed nearly experimental papers neural network learning algorithms serious experimental deficiencies 
survey strikingly high percentage new algorithms evaluated real problem compared alternative real data 
survey experimental neural network papers studies leading journals separate data set parameter tuning leaves open possibility reported results overly optimistic 
classification research comes variety forms lays new algorithms demonstrates feasibility describes creative new algorithms may require rigorous experimental validation 
important designed primarily comparative undertake criticize intended introduce creative new ideas demonstrate feasibility important domain 
serves suppress creative new algorithm perform encourages people focus narrow studies incremental changes previous 
comparing classifiers hand new method outperforms established important tasks result worth reporting yield important insights algorithms 
important comparative done statistically acceptable framework 
intended demonstrate feasibility contrast purely comparative need statistical comparison measures convincing 

data repositories conducting comparative studies classification researchers data miners careful rely heavily stored repositories data uci repository source problems difficult produce major new results widely shared data 
example fisher iris data years hundreds thousands studies 
nettalk dataset english pronunciation data introduced sejnowski rosenberg numerous experiments protein secondary structure data introduced qian sejnowski cite just examples 
holte collected results different datasets different published accuracy figures 
new experiments uci datasets run risk finding significant results statistical accidents explained section 
note repository serves useful functions things allows new algorithmic idea test plausibility known problems 
mistake conclude differences show new method significantly better datasets 
hard impossible argument convincing statistically correct way 

comparative studies proper methodology comparative study involves classification algorithms data extraction techniques usually propose entirely new method proposes changes algorithms uses comparisons show changes improve performance 
studies may appear superficially quite easy fact requires considerable skill successful improving known algorithms designing experiments 
focus design experiments subject little concern machine learning community exceptions 
included comparative study category papers introduce new algorithm improve old consider known algorithms conduct experiments known datasets 
may include variations known algorithms 
goal papers ostensibly highlight strengths weaknesses algorithms compared 
goal worthwhile approach taken papers valid reasons explained 
steven salzberg 
statistical validity tutorial statistics offers tests designed measure significance difference treatments 
tests adapted comparisons classifiers adaptation done carefully statistics designed computational experiments mind 
example machine learning study fourteen different variations classification algorithms compared eleven datasets 
unusual studies report comparisons similar numbers algorithms datasets 
variations study compared default classifier differences reported significant paired test produced value 
particular significance level nearly stringent experiments chances significant expected number significant results level 
obviously wants 
order get results truly significant level need set stringent requirement 
statisticians aware problem long time known multiplicity effect 
papers focused attention nicely classification researchers address effect 
particular ff probability differences exist algorithms mistake find significant difference 
ff percent time experimenters error 
tests experiment nominal significance level ff chance making right experiment gamma ff conduct independent experiments chance getting right gamma ff 
note true experiments independent tighter bounds computed 
set different algorithms compared test data tests clearly independent 
fact comparison draws training test sets sample independent 
suppose fact real differences exist algorithms tested chance get right words chance mistake ff gamma gamma ff suppose example set nominal significance level ff experiment 
odds making mistake experiments ff gamma gamma 
clearly chance drawing incorrect want 
assuming true differences exist conditional probability 
precisely chance results incorrectly reach significance level 
order obtain results significant level tests need set gamma gamma ff gives ff 
criterion times stringent original ff criterion 
argument rough assumes experiments independent 
assumption correct adjustment significance described known statistics community comparing classifiers bonferroni adjustment 
experiments identical training test sets tests clearly independent 
wrong value experiments find significance exists 
researchers proceed simple test compare multiple algorithms multiple datasets uci repository see wettschereck dietterich 
easy conduct test simply wrong test experimental design 
test assumes test sets treatment algorithm independent 
algorithms compared data set obviously test sets independent share examples assuming training test sets created random partitioning standard practice 
problem widespread comparative machine learning studies 
authors study cited written paired test high probability type error 
worth noting statisticians difficulty agreeing correct framework hypothesis testing complex experimental designs 
example framework alpha levels values questioned hypotheses consideration 

alternative statistical tests obvious problem experimental design cited considers accuracy test set 
common test set compare algorithms comparison consider numbers number examples algorithm got right got wrong number got right got wrong number algorithms got right number got wrong 
just algorithms compare simple improved test way compare compare percentage times versus throw ties 
simple binomial test comparison bonferroni adjustment multiple tests 
alternative random distinct samples data test algorithm analysis variance anova compare results 
simple example shows binomial test compare algorithms 
measure algorithm answer series test examples 
number examples algorithms produce different output 
assume series tests independent observing set bernoulli trials 
assumption valid test data randomly drawn sample population 
successes number times failures number times algorithms perform equally expected value 
suppose looks better calculate observed success probability wins times observed experiment 
typically reported value double value sided test 
easily computed binomial distribution gives probability successes trials 
gamma 
gammas steven salzberg expect differences algorithms 
suppose examples algorithms differed cases algorithm correct wrong 
compute probability result 
gamma 
highly algorithms accuracy reject null hypothesis high confidence 
note computation uses binomial distribution exact 
nearly identical form test known mcnemar test uses distribution 
statistic mcnemar test js gamma gamma simpler compute 
sided test double probability reject null hypothesis 
observed probability rise sided test just 
case say reject null hypothesis words algorithms may fact equally accurate population 
just example meant cover comparative studies 
method applies classifiers data mining methods attempt extract patterns database 
binomial test relatively weak test handle quantitative differences algorithms handle algorithms consider frequency agreement algorithms 
number agreements 
argued belief algorithms doing thing increase regardless pattern disagreement 
pointed finding proper statistical procedure compare classification algorithms quite difficult requires introductory level knowledge statistics 
general experimental design cochran cox descriptions anova experimental design introductory texts 
jensen discusses framework experimental comparison classifiers addresses significance testing cohen jensen discuss specific ways remove optimistic statistical bias experiments 
important innovation discuss randomization testing derives distribution follows 
trial data set copied class labels replaced random class labels 
algorithm find accurate classifier methodology original data 
estimate accuracy greater random copied data reflects bias methodology distribution adjust estimates real data 

community experiments cautionary notes fact problem machine learning community worse stated people sharing small repository datasets repeatedly datasets experiments 
substantial danger published results strict significance criteria appropriate significance tests mere accidents chance 
problem follows 
suppose different people comparing classifiers studying effects algorithms trying determine better 
suppose fact mean accuracy large population datasets algorithms vary randomly performance specific datasets 
people studying effect algorithms expect get results statistically significant level get significance level 
picture bit complicated assumes experiments independent 
clearly case results due chance people working separately ones get significant results publish simply move experiments 
similar observations reviewing publication process skew results 
data mining community broader classification community benchmark databases emerge different researchers test mining techniques 
experience machine learning community serve cautionary tale 
communities testing new drugs experimenters try deal phenomenon community experiments duplicating results 
proper duplication requires drawing new random sample population repeating study 
happens benchmark databases normally static 
wants duplicate results re run algorithms parameters data course results 
different partitioning data training test sets help duplication new data available 

repeated tuning substantial problem reporting significance results computational experiments left unstated researchers tune algorithms repeatedly order perform optimally datasets 
system developed developer spends great deal time determining parameters optimal values 
example back propagation algorithm learning rate momentum term greatly affect learning architecture neural net degrees freedom enormous effect learning 
equally important problems representation data may vary study basic dataset 
example numeric values converted discrete set intervals especially decision tree algorithms 
tuning takes place adjustment really considered separate experiment 
example attempts different combinations parameters significance levels values order obtain levels comparable single experiment level 
assumes unrealistically experiments independent 
experimenters keep careful count adjustments consider 
kibler langley aha suggest alternative parameter settings studied independent variables effects measured artificial data 
greater problem occurs uses algorithm algorithm may tuned steven salzberg public databases 
know tuning took place attempt claim significant results known data simply valid 
fortunately perform virtually unlimited tuning corrupting validity results 
solution cross validation entirely training set 
recommended procedure reserve portion training set tuning set repeatedly test algorithm adjust parameters tuning set 
parameters appear optimal settings accuracy measured test data 
doing comparative evaluations done modify prepare algorithms done advance seeing test data 
point obvious experimental researchers fact papers appearing methodology followed 
survey experimental papers leading neural network journals separate data set parameter tuning remaining papers explain adjusted parameters adjustments test set 
point worth stating explicitly tweaking modifying code decisions experimental design parameters may affect performance fixed experimenter test data 
valid data larger population problems 

generalizing results methodological approach comparative studies pick datasets uci repository public source data perform series experiments measuring classification accuracy learning rates criteria 
choice datasets random experiments general statements classification algorithms necessarily valid 
fact draws samples population conduct experiment inferences applied original population case means uci repository 
valid general statements datasets 
way valid uci repository known represent larger population classification problems 
fact argued holte uci repository limited sample problems quite easy classifier 
may represent concept class example linearly separable suggested strong performance perceptron algorithm known comparative study 
evidence strong results uci datasets apply classification problems repository unbiased sample classification problems 
means say uci repository exist 
repository serves important functions 
having public repository keeps community honest sense published results checked 
second function new idea repository way checking plausibility 
works worth investigation 
course may idea creator think means demonstrate feasibility 
dangers repeated re data spur research community comparing classifiers continually enlarge repository time representative classification problems general 
addition artificial data considered way test precisely strengths weaknesses new algorithm 
data mining community likewise come rely heavily standard set databases 
second related problem making broad statements results common repository data 
problem write algorithm works datasets specifically algorithm designed datasets mind 
researchers familiar datasets biased developing new algorithms consciously attempt tailor algorithms data 
words people inclined consider problems missing data outliers know problems represented public datasets 
problems may important different datasets 
unavoidable fact familiar data may biased 

recommended approach probably impossible describe methodology serve recipe comparisons computational methods approach allow designer new algorithm establish new algorithm comparative merits 

choose algorithms include comparison 
sure include algorithm similar new algorithm 

choose benchmark data set illustrates strengths new algorithm 
example algorithm supposed handle large attribute spaces choose data set large number attributes 

divide data set subsets cross validation 
typical experiment uses values may chosen depending data set size 
small data set may better choose larger leaves examples training set 

run cross validation follows 
subsets data set create training set gamma 
divide training set smaller subsets 
training tuning 
purpose allow experimenter adjust parameters algorithm 
methodology forces experimenter explicit parameters 
parameters optimized re run training larger set 
measure accuracy 
accuracy averaged partitions 
values give estimate variance algorithms 

compare algorithms binomial test described section mcnemar variant test 
may tempting easy powerful computers run cross validations data set report cross validation single trial 
steven salzberg produce valid statistics trials design highly interdependent 
procedure applies single data set 
experimenter wishes compare multiple data sets significance levels increased bonferroni adjustment 
conservative adjustment tend significance cases enforces high standard reporting algorithm better 

researchers observed single classification algorithm best problems 
observation data mining single technique best databases 
theoretical shown certain assumptions classifier better 
experimental science concerned data occurs real world clear theoretical limitations relevant 
comparative studies typically include new algorithm known methods studies careful methods claims 
point discourage empirical comparisons clearly comparisons benchmarks important central component experimental computer science powerful done correctly 
goal help experimental researchers steer clear problems designing comparative study 
acknowledgments simon kasif eric brill helpful comments discussions 
alan salzberg help statistics comments 
supported part national science foundation iri iri national institutes health hg 
views expressed author necessarily represent views acknowledged national science foundation national institutes health 
notes 
study published similar study data sets algorithms 

value simply probability result occurred chance 
value indicates probability observed results merely random variation kind indicate true difference treatments 
description perform paired test see introductory statistics text 

cross validation refers widely experimental testing procedure 
idea break data set disjoint subsets approximately equal size 
performs experiments experiment th subset removed system trained remaining data trained system tested held subset 
fold cross validation example test set exactly 
procedure advantage test sets independent 
training sets clearly independent overlap substantially 
limiting case set gamma size entire data set 
form cross validation called leave 
comparing classifiers 
aha 
generalizing case studies case study 
proc 
ninth intl 
workshop machine learning pages san mateo ca 
morgan kaufmann 

cochran cox 
experimental designs 
wiley nd edition 

cohen jensen 
overfitting explained 
prelim 
papers sixth intl 
workshop artificial intelligence statistics pages january 


data mining industry 
review economics statistics 

dietterich 
statistical tests comparing supervised learning algorithms 
technical report oregon state university corvallis 

everitt 
analysis contingency tables 
chapman hall london 

fayyad irani 
multi interval discretization continuous valued attributes classification learning 
proc 
th intl 
joint conf 
artificial intelligence pages chambery france 
morgan kaufmann 


method learns data 
prelim 
papers fifth intl 
workshop artificial intelligence statistics pages fort lauderdale florida 


statistical evaluation neural network experiments minimum requirements current practice 
trappl editor cybernetics systems proc 
th european meeting cybernetics systems res pages 
austrian society cybernetic studies 

gascuel 
statistical significance inductive learning 
proc 
european conf 
artificial intelligence ecai pages new york 
wiley 


statistical thinking behavioral scientists 
duxbury press boston ma 

holte 
simple classification rules perform commonly datasets 
machine learning 

jensen 
knowledge discovery induction randomization testing 
piatetsky shapiro editor proc 
knowledge discovery databases workshop pages menlo park ca 
aaai press 

jensen 
labeling space tool thinking significance testing knowledge discovery 
office technology assessment congress 

kibler langley 
machine learning experimental science 
proc 
euro 
working session learning pages 

kohavi sommerfield 
oblivious decision trees graphs top pruning 
proc 
th intl 
joint conf 
artificial intelligence pages montreal 
morgan kaufmann 

murphy 
uci repository machine learning databases machine readable data repository 
maintained department information computer science university california irvine 
anonymous ftp ics uci edu directory pub machine learning databases 

prechelt 
quantitative study experimental evaluations neural network algorithms current research practice 
neural networks 

qian sejnowski 
predicting secondary structure globular proteins neural network models 
journal molecular biology 

raftery 
bayesian model selection social research discussion andrew gelman donald rubin robert hauser 
peter marsden editor sociological methodology pages 
oxford uk 

sejnowski rosenberg 
parallel networks learn pronounce english text 
complex systems 

shavlik mooney towell 
symbolic neural learning algorithms experimental comparison 
machine learning 

wettschereck dietterich 
experimental comparison nearest neighbor algorithms 
machine learning 

wolpert 
connection sample testing generalization error 
complex systems 
