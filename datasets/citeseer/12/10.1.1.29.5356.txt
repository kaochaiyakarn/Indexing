interaction intelligent behavior maja matari submitted department electrical engineering computer science partial fulfillment requirements degree doctor philosophy massachusetts institute technology may fl massachusetts institute technology 
rights reserved 
author department electrical engineering computer science may certified rodney brooks professor computer science engineering thesis supervisor accepted frederic chairman committee graduate students interaction intelligent behavior maja matari submitted department electrical engineering computer science may partial fulfillment requirements degree doctor philosophy thesis addresses situated embodied agents interacting complex domains 
focuses problems synthesis analysis intelligent group behavior learning complex group environments 
basic behaviors control laws cluster constraints achieve particular goals appropriate compositional properties proposed effective primitives control learning 
thesis describes process selecting basic behaviors formally specifying algorithmically implementing empirically evaluating 
proposed ideas validated group mobile robots basic behavior set consisting safe wandering aggregation dispersion homing 
set basic behaviors acts substrate achieving complex high level goals tasks 
behavior combination operators introduced verified combining subsets basic behavior set implement collective flocking foraging docking 
methodology introduced automatically constructing higher level behaviors learning select basic behavior set 
novel formulation reinforcement learning proposed behavior selection learnable noisy uncertain multi agent environments stochastic dynamics 
consists conditions behaviors robust control minimized state spaces reinforcement shaping methodology enables principled embedding domain knowledge types shaping functions heterogeneous reward functions progress estimators 
methodology validated collection robots learning forage 
generality approach compatible existing reinforcement learning algorithms allowing accelerate learning variety domains applications 
methodologies results aimed extending understanding synthesis analysis learning group behavior 
thesis supervisor rodney brooks title professor computer science engineering acknowledgments nobel prize oscar acceptance need short 
people contributing great years mit artificial intelligence lab 
rod brooks great advisor excellent motivator rebel role model importantly conservative 
am grateful advice patience continuing support 
am grateful gerry sussman lynn stein patrick winston thesis related job related life relate advice year mit 
unlimited go mike jose great friends participated classes heated discussions latex hacking hours thesis preparation 
brian narasimhan kept honest years asking hard questions accepting mediocre answers 
special mike proof reading numerous papers encouraging learn russian introducing superb friend vancouver 
special jose patient friend dance partner source advice way back days toto masters thesis formalization basic behaviors document 
infinite jose willingness read thesis provide numerous invaluable comments tech report 
numerous anita flynn inspiring force fantastic running research great talk aspects life 
mike erdmann supportive years mit initiating culture talking walking running sailing things done 
endless gratitude nancy pollard patient fun supportive great years 
sharing including running squash research dilemmas plant watering 
matt marjanovi crazy just right way managing job interview thesis season making put back name course pig 
ian horswill great companion flights far away conferences patient source advice introducing weird world 
sibling paul viola bro early years lab giving outstanding advice theses committees research directions people talk saying exactly thinking 
iii cindy ferrell having endless supply great spirit excellent taste especially tech report covers enthusiasm research sports exotic vacations 
pattie maes friend part time advisor matters imaginable having taste matters imaginable superb role model 
lynne parker understanding robots quirks better having endless patience great person share research views hardware 
hard working matt marjanovi stanley wang owen making herd behave 
phillip rock climbing lessons bad movie advice home movies endless enthusiasm 
david beymer gideon stein sandy wells matt williamson great running talking partners 
added matt squash games scottish accent 
carl de writing memorable girl scout benefit announcements expert patient teacher outdoor activity known mankind 
adonis greek great taste movies excellent 
bruce blumberg david brock barb moore bryant amy great discussions holly yanco joanna bryson enthusiasm organizational skills mike bizarre humor years broken talking clock great people cog group trevor darrell having interesting new say variety topics keeping touch lisa dinner parties charles isbell great enthusiasm unique quotes tom knight lab weird hours knowing willing discuss topics green building ceiling swinging robot tina kapur lee adopting fridge office marina meila great travel mate wonderful talk machine learning henry minsky cow supplier annika administrative help years great talent cello robert patience helping creature library video equipment ruth great friend talk loose touch months time laurel simmons keeping lab collapsing patrick sobalvarro great literary skills movie advice jeanne organizing dance group everybody unintentionally left 
deepest bora served boston family ready listen stories 
great milan iv smart uncle talk talk playing devil advocate keep toes 
enduring rich roberts decade love support 
minute fun memorable worthwhile 
infinite incomparable gratitude mira matari wonderful mother 
taught lessons example worked harder life anybody known energy inspired accomplished 
thesis dedicated 
contents overview thesis synthesis analysis group behavior learning complex group environments thesis outline motivation issues agent control biological sociological motivation pragmatic motivation key issues terms definitions behaviors goals interaction domain description recognition kin mental models theory mind communication cooperation issues agent control individual agent control multi agent control analysis behavior emergent behavior limits analysis interference conflict individual vs group benefit estimating interference summary related robotics behavior control control multiple physical robots vi simulations multiple agents artificial life distributed artificial intelligence behavior analysis summary basic behavior approach selecting evaluating basic behaviors criteria selection basic behaviors movement plane basic behavior experiments experimental environments agent interaction modeler mobile robot herd hardware limitations experimental procedure basic behavior specifications basic behavior algorithms safe wandering dispersion aggregation homing basic behavior evaluation empirical evaluation basic behaviors evaluation heterogeneous groups evaluating distributed centralized algorithms summary combining basic behaviors types behavior combination direct combinations basic behaviors temporal combinations basic behaviors implementations compound behaviors flocking foraging docking parking combining behaviors different agents summary vii learning situated systems motivation relevant learning models learning declarative knowledge learning control learning new behaviors learning select behaviors reinforcement learning markov decision process models state state transitions algorithms learning trials reinforcement multiple goals related summary learning approach reformulating problem behaviors conditions reinforcement accelerated learning heterogeneous reward functions progress estimators summary learning experiments robots learning task learning algorithm control algorithm experimental results evaluation evaluation evaluation scaling discussion extensions social rules transition models viii heterogeneous learning structuring learning signal symbol learning summary summary learning glossary ix list figures ai perspective examples group behaviors example foraging simulator environment herd dispersion homing behavior combination architecture flocking example foraging learning robots comparative performance different learning strategies interaction modeler herd robot robot sensors example robot data plot robots dispersion modeler data dispersion robot data homing robots homing robots ii homing modeler data homing robots data duration data repeatability data robustness data robustness ii mean time robots mean time robots homogeneous vs hierarchical aggregation homogeneous vs hierarchical dispersion packed initial conditions ideal knowledge vs homogeneous hierarchical dispersion direct temporal behavior combination architecture behavior combination direct behavior combination flocking direct behavior combination graph direct behavior combination repetitions temporal behavior combination foraging heterogeneous behavior combination graph flocking data robustness flocking data flocking data flocking data foraging data foraging data foraging data docking robots robots robots learning robots learning robot robot sensors experimental environment learning schematic experimental environment learning photo initial conditions learning learning learning resting behavior comparative performance different learning strategies family photo xi list tables centralized distributed approaches levels description summary basic behavior approach basic behavior spatial domain controller foraging summary situated learning approach foraging policy initial policy comparative learning performance learned policy xii list algorithms avoid agents avoid safe wander follow centroid disperse neighbor disperse aggregate home flock dock xiii chapter overview thesis main goals artificial intelligence ai gain insight natural intelligence synthetic approach generating analyzing artificial intelligent behavior 
order glean understanding phenomenon complex natural intelligence need study complex behavior complex environments 
traditionally ai concerned complex agents relatively simple environments simple sense precisely modeled involved little noise uncertainty 
contrast traditional systems reactive behavior systems placed agents low levels cognitive complexity complex noisy uncertain environments 
thesis describes attempts simultaneously scale dimensions 
environmental complexity scaled introducing agents cognitive complexity scaled introducing learning capabilities agents 
thesis addresses problems 
synthesis analysis intelligent group behavior 
learning complex group environments ideas notion basic behaviors means combining constraints agent mechanical sensory characteristics constraints environment types interactions sensory information agent obtain order construct appropriate abstraction structuring primitives control 
methodology uses basic behaviors generate various robust group behaviors including homing flocking 
introduce formulation reinforcement learning behaviors unit representation allows group agents learn complex tasks foraging cognitive complexity environment complexity group behavior group learning reactive systems behavior systems traditional ai traditional ai addressed complex agents simple environments reactive behavior approaches dealt simple agents noisy uncertain worlds 
attempts scale dimensions simultaneously addressing synthesis learning complex group behavior 
frame time frame time homing frame time flocking shows examples real robot data different group behaviors homing flocking 
robots physically inches long scaled plotted black rectangles white arrows indicating heading 
dark robots row rectangles bottom shows robots experiment 
boxes lower right indicate frame numbers elapsed time seconds runs 
frame time ah example foraging behavior robots shown minutes running 
pucks delivered home region marked grey box 
robots near home way drop 
robots wandering search additional pucks 

validate proposed approaches experiments homogeneous groups mobile robots 
chapter gives brief summary novel approaches experimental data implications thesis 
organization thesis outlined chapter 
synthesis analysis group behavior thesis belief intelligent collective behavior decentralized system results local interactions simple rules 
basic behaviors proposed methodology structuring rules principled process synthesis evaluation 
behavior control law clusters set constraints order achieve maintain goal 
example safe wandering behavior maintains goal avoiding collisions agent moving 
postulate domain set behaviors basic required generating behaviors minimal set agent needs reach goal repertoire 
process choosing set basic behaviors domain dually constrained 
bottom process constrained dynamics agent environment 
top process constrained agent goals specified task 
combination types constraints helps prune agent behavior space 
example group interactions situated embodied agents illustrate process selecting basic behavior set 
agents mobile robots embodied endowed specific mechanical sensory effector constraints 
define high level goals system consisting collectively moving objects pucks environment efficient fashion 
efficiency defined terms minimizing energy minimizing amount time required complete task number moves required agents 
effective set basic behaviors spatial domain enable agents employ variety flexible strategies puck manipulation collection distribution 
effectiveness strategies depends maximizing synergy agents achieving necessary goals minimizing inter agent interference 
propose set basic behaviors ffl safe wandering minimizes collisions agents environment ffl minimizes interference structuring movement agents ffl aggregation gathers agents ffl dispersion dissipates agents ffl homing enables agent proceed particular location definition behavior set minimal basic members reducible 
additionally show sufficient achieving set pre specified goals 
described basic behaviors defined respect group 
utility behaviors grasping dropping part agent repertoire 
basic behavior set evaluated giving formal specification behaviors comparing collection specifications formal specification set global tasks required group 
basic behavior set established implemented variety algorithms 
step verification basic behavior algorithms comparison formal behavior specification formal correctness algorithm 
argue difficult prove properties exact behavior individual agents group possible evaluate predict behavior ensemble 
notion ensemble behavior simulator environment called interaction monitor validate methodologies synthesizing analyzing group behavior described thesis 
agents shown black circles white markers indicating heading 
large rectangle represents agents workspace 
propose group behavior algorithms utilize centroid operator averages inputs multiple agents 
operator statistical properties allow analyzing making predictions behavior group 
thesis provides detailed specifications algorithms basic behaviors 
analytical proofs provides empirical evaluations performance algorithms criteria ffl repeatability consistent behavior different trials 
ffl stability behavior oscillate conditions 
ffl robustness robust behavior presence sensor effector error noise 
ffl scalability behavior effected increased decreased group sizes 
criteria applied data obtained performing trials basic behavior 
experiments performed different multi agent environments order minimize domain biases 
environment multi agent simulator interaction monitor featuring agents local sensing distributed local control 
second environment collection physical mobile robots equipped local sensors local control 
robots equipped suite infra red sensors collision avoidance puck detection stacking micro switches bump sensors contact detection 
addition mobile robots validate group behavior methodologies described thesis 
robots demonstrated group safe wandering aggregation dispersion flocking foraging 
local sensors robots equipped radios sonars triangulating position relative stationary beacons broadcasting position limited radius 
radios detect robots gather data local centroid computations 
basic behaviors consisting rule small set simple rules generated robust group behaviors met prespecified evaluation criteria 
small subset data shown real time viewer software package displaying replaying robots runs plotting positions time displaying frame elapsed time experiment 
figures show dispersion homing 
data algorithms specifications detailed evaluation chapter 
basic behaviors intended building blocks achieving higher level goals 
behaviors embedded architecture allows types combination direct summation temporal switching see 
types combination operators tested empirically 
simple robust flocking behavior generated summing outputs safe wandering aggregation homing 
complex foraging behavior involves finding collecting pucks implemented switching safe wandering dispersion written matthew marjanovi 
frame time frame time frame time continuous behavior robots 
entire time history robots positions plotted 
frame time frame time frame time dispersion behaviors robots 
frame time frame time frame time homing behaviors robots 
robots reach home quickly fifth joints second 
basic behaviors composite behaviors sensory inputs effector outputs control architecture generating group behaviors consists direct temporal combinations sums switches subsets fixed basic behavior set 
direct combinations marked temporal combinations frame time frame time frame time flocking behavior robots 
robots started nearly linear dispersed state 
quickly establish flock maintain positions individual robots flock fluctuate time 
frame time frame time frame time tf example foraging behavior robots 
pucks delivered home region marked grey box 
robots dropping pucks wandering search additional pucks pick deliver home 
homing 
addition empirical testing behaviors combinations proposed methodology generating decentralized group behavior compared centralized total knowledge approach 
experimental results showed simple fully distributed strategies applied dispersion aggregation tasks converged constant factor slower centralized approach 
learning complex group environments part thesis introduces basic behaviors methodology structuring simple rules flexible effective repertoires group behavior 
presents combination operators allow constructing achieving higher level goals 
second part thesis starting chapter describes methodology automatically combining basic behaviors higher level ones unsupervised reinforcement learning agents interactions environment 
reinforcement learning rl approaches agent learns external scalar reward punishment 
rl successfully applied variety domains largely modeled markovian agent environment interaction described markov decision process mdp 
mdp assumption directly apply noisy uncertain multi agent environments addressed 
external internal feedback natural sources information learning situated agents methods applying rl complex domains needed 
traditional formulation rl problems terms states actions reinforcement required reformulation order applied domain 
notion state monolithic descriptor agent environment scale multi agent domain continuous discrete aspects describing agent velocity ir sensors radio data existence agents environment 
furthermore commonly notion actions inappropriate atomic actions low level effect unpredictable noisy useful learning algorithm 
delayed reinforcement reward discounting insufficient learning domain 
learning possible propose reformulation level system description states actions conditions behaviors 
behaviors control laws achieve goals hide low level control details 
notion basic behaviors small basis set defined substrate learning 
actions replaced behaviors states replaced conditions necessary sufficient subsets state required triggering behavior set 
mobile robots validate group behavior learning methodologies described thesis 
robots demonstrated learning forage group safe wandering resting behaviors 
conditions fewer states greatly diminishing agent learning space speeding rl algorithm 
addition behaviors conditions propose ways shaping reinforcement function order aid learner nondeterministic noisy dynamic environment 
introduced heterogeneous reward functions partition task subgoals providing immediate reinforcement 
single behavior single goal introduced progress estimators functions associated particular conditions provided metric learner performance 
progress estimators internal critics decrease learner sensitivity noise minimize thrashing minimize effect fortuitous rewards correlating domain knowledge progress appropriate behaviors agent taken past 
details reformulation chapter 
proposed formulation validated task learning associate conditions behaviors group foraging collection robots 
behaviors included foraging subset basic behaviors safe wandering dispersion homing augmented grasping dropping resting new behavior triggered internal day time night time clock 
clustering condition set reduced power set predicates puck home night time near intruder 
smaller group robots reliable hardware learning experiments 
terms sensors effectors robots functionally identical set implemented basic behaviors combinations directly portable 
learning algorithms implemented tested foraging task 
performance reinforcement strategies learning forage 
axis shows reinforcement strategies 
axis maps percent correct policy agents learned averaged trials 
standard learning simply summed reinforcement received time 
learning received reward robot dropped puck home region 
second algorithm reinforcement received heterogeneous reward functions reaching subgoals including grasping dropping pucks reaching home 
third algorithm reinforcement heterogeneous reward functions progress estimators monitoring progress getting away intruder monitoring progress home 
progress estimators sufficient making learning task possible consistent complete learning performance 
absence disabled robots learning complete policy 
performance algorithms averaged trials 
analysis learning performance showed parts learned algorithms relied progress estimators successfully learned third case 
detailed analysis results chapter 
thesis outline preceding sections briefly summarized contributions thesis 
section outlines structure thesis summarizes chapters 
chapters deal synthesizing analyzing group behavior 
chapters address learning multi agent domains 
readers interested mov ing directly details basic behavior approach skip chapter 
interested going directly learning part thesis skip chapter 
newly introduced ambiguous frequently terms defined appendix summaries chapter contents 
chapter describes biological sociological pragmatic motivation 
describes key issues individual multi agent control introduces defines main concepts thesis 
chapter presents overview related robotics simulation artificial life distributed ai analysis behavior 
chapter introduces basic behavior approach describes methodology selecting basic behaviors illustrates process defining basic behaviors collection mobile agents interacting plane 
chapter describes experimental environments basic behavior specifications algorithms empirical data criteria evaluating performance behaviors efficacy relative centralized alternatives 
chapter describes methodologies combining basic behaviors complex higher level behaviors 
methodologies demonstrated combining basic behaviors described chapter generate different kinds higher level behaviors evaluate performance 
chapter discusses methods minimizing interference behaviors agent 
chapter motivates learning situated agents reviews existing learning type information acquired agent 
defines group learning problem discussed thesis instance reinforcement learning rl overviews existing rl models algorithms applied situated agent domain 
chapter describes formulation rl enables facilitates learning complex situated multi agent domain 
introduces behaviors conditions place actions states describes method shaping learning process heterogeneous reward functions progress estimators 
chapter presents experimental robot environment learning task validate methodologies proposed chapter 
describes experimental design learning algorithms implemented compared discusses results 
chapter addresses extensions including problem learning social rules multiple concurrent tasks 
chapter summarizes thesis 
chapter motivation issues agent control study multiple agents 
motivation comes quite different complementary directions desire understand analyze natural systems need design synthesize artificial ones 
biological sociological motivation intelligence social phenomenon 
intelligent animals live obey rules reap benefits society kin 
societies vary size complexity key common property provide maintain shared culture gould 
culture result cause intelligent behavior 
intelligent creatures create refine social rules order society 
rules constitute culture communicated shared society important effects individual members gould mcfarland 
culture allows genetic parsimony 
social interaction transfer information generations social learning mcfarland 
genetic investment necessary fewer abilities need innate 
interestingly culture adapts growing complexity social rules increased demands individual intelligence specifically ability absorb adapt culture 
humans extreme example cultural complexity requiring longest learning training developmental period animals gould 
culture allows faster adaptation 
alternative evolution culture lows testing adapting social behaviors shorter time scale 
social interactions created destroyed single generation 
example elephants shown learn avoid humans harm inflicted generations distant cultural memory past abuse gould 
culture allows lamarckian evolution 
enables direct transfer learned information generations 
single individual discovery adopted entire population passed 
example individual japanese macaque monkey discovered washing sweet potatoes 
practice transmitted culturally society generations gould 
culture genetic deficiencies 
social interactions compensate individual limitations terms physical cognitive capabilities 
example group organizations herds packs allow animals attack larger prey share information increase chance mating survival mcfarland 
order understood individual intelligence observed analyzed social cultural context 
contrast traditional ai addresses intelligence individual phenomenon belief intelligent behavior inextricably tied cultural context understood isolation 
emphasis similar principles ethology study animal behavior 
branch biology studies animals controlled laboratory settings ethology observes animals natural habitats 
research attempts study intelligent behavior natural habitat situated society 
complexity culture results interactions individuals 
research focus exploring simple social interactions result purposive group behaviors goal 
understanding social group behavior nature 
developing methodology principled design group behavior artificial systems 
study social agents culture basis structure intelligent behavior exploratory 
part thesis addresses domain phenomenological hopefully scientific attempt understand natural phenomena explain principled terms 
pragmatic motivation nature offers challenges analysis engineering demands synthesis 
particular strives efficient automated reliable repeatable methods synthesizing useful systems 
discoveries systems multiple interacting agents applied parallelizable problems 
idea applying multiple computational physical agents variety distributed domains terrain exploration mapping fire fighting harvesting micro surgery years 
spite potentially numerous applications distributed multi agent approach exception rule domains 
parallel decentralized non hierarchical computation requires paradigm shift resnick 
regardless domain application approach raises number difficult issues 
particular motivate research addressed thesis ffl common properties principles organization shared different domains application multi agent systems 
ffl interactions individuals affect behavior group 
ffl group get job done 
ffl individual need know group task environment agents 
ffl individual need communicate order get job done 
ffl simplest agents rules generate complex useful group behaviors 
research aimed finding common properties various domains multi agent interaction 
identifying properties allows classifying group behaviors common categories simplifies process design analysis 
section defines key terms 
key issues terms definitions behaviors goals notion behavior main building block 
years behaviors popularized ai control learning communities 
approaches labeled behavior ai behavior control mainstream behavior cleanly defined circumscribed 
define behavior control law reaching maintaining particular goal 
example robot domain control law takes inputs agent sensors uses generate actions keep agent moving fixed region moving object 
behavior sensory input vector internal state 
exclude state behavior definition reserve tasks needed 
definition behavior specifies behavior type operator guarantees particular goal 
order serve general building blocks basic behaviors capable dealing attaining maintaining goals 
attainment goals terminal states having reached goal agent finished 
goals include reaching home region picking object 
maintenance goals persist time representable terminal states dynamic equilibria maintained 
examples include avoiding obstacles minimizing interference 
maintenance goals usually expressed sequences achievement goals may require fine granularity description 
situated agents multiple concurrent goals including attainment goal maintenance goals 
thesis attempt show behaviors natural convenient efficient abstraction control planning learning situated agents 
behaviors hide low level details control deal precise control parameters 
allow specifying robot tasks goals terms higher level primitives cut state space intuitive user 
basis learning noisy uncertain situated domains 
ensemble collective group behavior observer defined temporal pattern interactions multiple agents 
possible behaviors domain small subset relevant desirable achieving agents goals 
interaction interaction foundational concept 
typically interaction viewed influence affects agent behavior 
definition agent interacts sense affected external observable internal state impact actions 
largely concerned interaction takes place agents 
propose stricter definition interaction mutual influence behavior 
consequently objects world interact agents may affect behavior 
presence object affects agent agent affect object objects definition behave agents 
domain description having defined key concepts thesis behavior interaction turn specification domain addressed 
order focus constrain study group behavior focuses interactions situated embodied agents 
key constraints imposed experimental domain order structure exploration providing sufficient variety behaviors study 
key constraining properties ffl agents homogeneous 
ffl agents explicit models 
ffl agents directed communication explicit cooperation 
reasons implications constraints described discussed sections 
implications homogeneity focuses groups agents homogeneous situated world goal structure case translating behavior set homogeneous agents referred similar furthermore agents embodied similar dynamics 
dynamics simulated agents identical physical robots vary significantly affect group behavior 
section hardware limitations explains detail 
terms homogeneous similar interchangeably 
distinct identical property ascribed simd style agents 
homogeneity important implications 
predictability fact agents similar behavior predictable require internal explicit models 
predictability explicitly allowing agents infer agents actions information individual decisions implicitly simplify control individual 
focuses approach 
example identical control laws take advantage inherent symmetries spatial domains 
homogeneity minimizes goal related conflicts resulting strategies cheating 
furthermore homogeneity allows leaving information world implicit 
agents explicit expectations agents behavior decision process implicitly takes information account 
similarity agents need identities require abilities identification 
presents significant cognitive savings 
homogeneity similarity greatly reduce individual cognitive requirements simplifying synthesis understanding group behavior 
homogeneity result increased global robustness redundancy 
failure subset agents seriously affect system agents similar interchangeable particular agent group agents critical accomplishment task 
preserve robustness specific roles leaders followers designated priori 
temporally spatially local replaceable leaders may emerge various situations 
recognition kin advantage homogeneity depends critical property agents able recognize similar agents 
postulate ability distinguish agents interacting environment necessary condition intelligent interaction group behavior 
ability innate ubiquitous nature enables creatures distinguish kind specifically recognize kin mcfarland mcfarland 
important note species kin recognition need explicit agent need know aware agent recognized kin long response kin specific 
example mold bases behavior concentration produced kin 
said actively recognizes kin act species specific ways result complex group behavior construction multi cellular organisms 
similarly ants presumed know pheromones sense produced 
appropriate responses pheromones result formation trails complex structures franks 
biologically inspired ability recognize kin pragmatic allows simplest rules produce purposive collective behavior 
mental models theory mind dominant school thought cognitive psychology ai premise social interaction requires theory mind woodruff 
order engage social discourse agents need mental models attribute mental states understand intentions maintain beliefs dennett cheney 
entire field theory mind rests necessity inferring internal workings mind agent interacting read miller 
maintaining theory mind complex task requires high computational cognitive overhead gasser huhns rosenschein genesereth axelrod 
controversy surrounds necessity developmental psychology ethology indicates theory mind necessary large repertoire complex social interactions tomasello kruger cheney mcfarland gould rosenthal zimmerman 
research developmental psychology shown young children engage various forms social interaction attaining sense self awareness necessary component constructing theory mind 
prior stage occurring age children incapable separating internal external perception world piaget walters 
achieving self awareness determined typical dot mirror test age children require number years reaching adult ability form theories mind 
research aimed testing primates theories mind 
demonstrated certain species monkeys involved complex social cooperative interactions apparently form theories mind cheney cheney 
contrast appear complex abilities able infer goals cheney mcfarland 
internal models represented explicit internal representations remains open study gomez 
alternative theory mind exploring existence limits theory mind biology difficult 
type amount knowledge representation animal brings bear social interactions impossible circumscribe 
ideal scenario experimenter able control type amount knowledge test resulting behavior order determine necessary 
computational robot experiments allow just 
agents experimented simpler nature exactly simplicity allows focus specific question internal social models 
order study necessity theory mind started bottom exploring agents 
studies group behaviors resulting simplest interactions simplest agents 
agents explicit models expectations intentions 
goal approach demonstrate types complex interactions achieved simple basic abilities 
results demonstrate particularly homogeneous groups significant amount information individual goals reflected observable external state behavior obtained direct communication cheney 
consequently theory mind necessary broad spectrum behaviors direct communication 
related issues communication discussed 
communication cooperation communication cooperation popular topics applied multi agent example see yanco stein dudek jenkin milios wilkes 
communication common means interaction intelligent agents 
observable behavior consequences interpreted form communication purposes clarity propose clarifying definitions 
direct communication purely communicative act sole purpose transmitting information speech act transmission radio message 
specifically directed communication direct communication aimed particular receiver 
directed communication cases receivers identified 
contrast indirect communication observed behavior communication agents effects environment 
type communication referred stigmergic biological literature refers communication modifications environment direct message passing 
direct indirect communication practiced species nature 
example bees signals dance sole purpose transmitting information recruiting 
contrast cues direction flight transmit hive information product behaviors 
cooperation form interaction usually communication 
certain types cooperative behavior depend directed communication 
specifically cooperative behaviors require negotiation agents depend directed communication order assign particular tasks participants 
analogously communication explicit cooperation defined set interactions involve exchanging information performing actions order benefit agent 
contrast implicit cooperation consists actions part agent goal achieving behavior repertoire effects world help agents achieve goals 
having defined precise terminology communication resulting cooperation constraints imposed experimental domain described 
order study role communication controlled fashion explore communication needed group behaviors described minimalist approach chosen 
directed communication agents experiments 
indirect communication sensing external state neighboring agents sensing density effects actions 
direct communication undirected limited local broadcast agents transmit messages received 
agents ability choose receivers messages engage directed communication 
undirected communication constraint affects kinds communication implemented emerge multi agent system 
focuses implicit cooperation explicit task sharing 
example addressing task moving large object agents deals distributed solutions problems moving numerous small objects task solved single agent benefit designed multi agent solutions 
alternative perspective see parker 
issues agent control section describes specifies problem controlling multi agent system approaches individual agent control discussing extensions multiple agents 
multi agent research covers vast array natural artificial systems ranging brain operating systems bird flocks collection robots 
purposes agent process capable perception computation action world multi agent system consists agents 
problem multi agent control viewed individual agent level collective level 
levels interdependent design strongly influenced 
multi agent control grown individual agent control history reflected control strategies collective level 
section describes main approaches individual agent control extensions applicability multi agent domains 
individual agent control extreme agent control spectrum lie traditional top planner deliberative strategies centralized world model verifying sensory information generating actions world chatila chatila laumond moravec cho laird rosenbloom 
information world model planner produce appropriate sequence actions task hand 
approaches allow explicitly formulating task goals system estimating quality agent performance 
sensing action changes environment require frequent replanning cost may prohibitive complex systems 
planner approaches criticized scaling poorly complexity problem consequently allowing reaction real time brooks brooks 
world may may physical 
various attempts achieving real time performance proposed 
prominent purely reactive bottom approaches implement agent control strategy collection preprogrammed condition action pairs minimal state brooks connell agre chapman connell 
systems maintain internal models perform search simply look command appropriate action set sensor readings 
rely direct coupling sensing action fast feedback environment 
purely reactive strategies proven effective variety problems defined design time inflexible run time due inability store information dynamically matari 
division reactive deliberative strategies drawn type amount computation performed run time 
reactive constant time run time strategies derived planner computing possible plans line advance 
example situated automata achieve real time performance compiling system goals ways achievement language compiles circuits constant time computation properties rosenschein kaelbling 
general entire control system agent precompiled decision graph collection reactive rules universal plans schoppers 
theoretically appealing strategies scale poorly complexity environment agent control system 
hybrid architectures attempt compromise purely reactive deliberative approaches usually employing reactive system low level control planner higher level decision making 
hybrid systems span large diverse body research 
includes reactive planning reactive execution reactive action packages raps higher level primitives planning hide take care details execution firby prs procedural reasoning system architecture flexible control rule invocation georgeff lansky schemas arkin payton connell 
systems tend separate control system communicating independent parts 
cases low level reactive process takes care immediate safety robot higher level uses planner select action sequences 
behavior approaches extension reactive systems fall purely reactive planner extremes brooks maes 
confused literature behavior strategies strictly powerful purely reactive approaches fundamental limitations internal state 
behavior systems embody properties reactive systems usually contain reactive components computation limited look 
centralized reasoning engine representation systems may different forms distributed internal representations perform distributed computations order decide effector action take matari 
comparative classification methodologies domains applicability undertaken 
multi agent control having overviewed single agent control section discusses described approaches scale multi agent problems 
extending planning paradigm single agent multi agent domains requires expanding global state space include state agents 
global state space exponential number agents 
specifically size global state space jgj size state space agent assumed equal agents worst maximum agents number agents 
exponential growth state space problem global line planning intractable smallest group sizes control synchronized simd form global planning requires communication agents controller bandwidth grow number agents 
additionally uncertainty perceiving state grows increased complexity environment 
consequently global planner approaches control appear suited problems involving multiple agents acting real time uncertain sensory information 
control spectrum extending reactive behavior approaches multi agent domain results completely distributed systems centralized controller 
systems identical local global levels global level systems collection reactive agents executing task related rules relying local sensing communication 
control distributed systems local scales number agents require global communication robust sensor effector errors 
global consequences local interactions agents difficult predict 
table summarizes properties approaches multi agent control planning paradigm includes includes traditional hybrid systems 
terms multi agent extensions hybrid systems fit planner category collective behavior generally result plan produced global controller 
agents perform behavior time 
centralized approaches distributed approaches optimize global parameters optimize locally scale poorly scale require global sensing local sensing require global communication may require communication computational bottleneck computational bottleneck impose hierarchical control flat control usually redundant usually redundant table comparative summary typical centralized distributed approaches 
centralized approaches advantage potential theoretical analysis 
contrast parallel distributed systems typically lend traditional analytical procedure 
analysis behavior thesis focuses fully distributed multi agent systems behavior agent determined control system centralized controller 
systems definition complex composed large number elements inter element interactions simple 
multi agent systems consisting situated agents uncertain sensors effectors display types complexity 
section addresses properties affect behavior analysis 
exact behavior agent situated nondeterministic world subject real error noise simplest algorithms impossible predict exactly 
induction exact behavior part multi agent system nature unpredictable 
simon system analyzable designed decomposable non interacting modules 
minimizing inter module interactions considered engineering principled ai traditional artificial intelligence relies style top modularity 
contrast nature abounds complex systems global behavior results precisely type interactions current research methodologies try avoid 
effects scales semantic minsky social deneubourg goss franks franks 
situated behavior interaction feedback environment agents 
negative positive feedback relevant 
negative feedback regulatory effect damping system response external influences positive feedback amplifying effect increasing system response 
multi agent spatial domain example negative feedback controls local structure agents positive feedback agents structure 
behaviors positive feedback usually require critical mass initiate accelerate increased group size 
behaviors variations recruitment agents engaged activity agents join 
behaviors usually unstable sensitive particular conditions resources required maintain recruitment effect 
numerous natural group behaviors positive feedback lynch public polls popularity ratings traffic jams ant trails worker recruitment ants bees instances positive feedback deneubourg deneubourg aron goss pasteels 
group interacting agents dynamical system 
global behavior complex systems determined local interactions individuals 
interactions merit careful study order understand global behavior 
natural systems interactions result evolution complex stable behaviors difficult analyze traditional top approaches 
postulate order reach level complexity synthetically behaviors generated similar interaction driven incrementally refined process 
precise analysis prediction behavior single situated agent specifically mobile robot physical world unsolved problem robotics ai 
previous shown synthesis analysis correct plans presence uncertainty intractable highly constrained domains lozano erez mason taylor canny erdmann simplest systems smithers 
physical environments pose great challenge usually contain structure determinism predictability usually required formal analysis brooks brooks 
predicting behavior multi agent system complex single agent case 
difficulty analyzing comes properties intrinsic complex systems 
actions agent depend states actions agents 
behavior system determined interactions agents individual behavior 
general mathematical tools available predicting behavior system numerous relatively complex interacting components collection situated agents 
contrast physical particle systems consist large numbers simple elements multi agent systems nature ai defined comparatively small groups complex agents 
statistical methods analyzing particle systems directly apply require minimal interactions components wiggins 
attempting analyze arbitrary complex behaviors focuses providing set behavior primitives synthesizing analyzing particular type complex multi agent systems 
primitives provide programming language designing analyzable control programs resulting group behaviors 
emergent behavior emergent behavior popular topic research field complex systems see forrest langton langton steels overviews 
behavior characterized property manifested global states time extended patterns explicitly programmed result local interactions system components 
emergent phenomena definition observed global level depend existence observer 
emergent behavior observed sufficiently complex system system contains local interactions temporal spatial consequences 
pervasiveness emergent phenomena objects interest objects analytical study long time 
property observer dependence emergent phenomena difficult study 
kolen pollack describe general complexity physical system intrinsic property dependent observer traditional measures complexity insufficient physical systems 
subjective evaluation discussed bonabeau 
emergent phenomena appealing researchers appear provide 
types systems referred self organizing apparent ability create order 
reality dynamics self organizing systems carefully crafted usually evolution produce results 
theoretical analysis multi agent systems type research difficult argued exact prediction behavior systems currently reach 
consequently situated group behavior benefit synthesis experimentation 
emergent behaviors result systems complex defy approach level description complex dynamics microscopic continuous 
macroscopic quasi continuous state spaces macroscopic discrete table desirable level system description control analysis lies commonly employed ends spectrum 
current tools predictive analysis require simulation prediction 
order structure simplify process experimental behavior design provide set basic group behaviors methods synthesizing local rules 
basic behaviors combinations emergent result local interactions predictable understood 
limits analysis difficulty analyzing complex multi agent systems lies level system description 
descriptions control usually low level detailed continuous 
contrast planning analysis usually done high level discrete model 
desirable manageable level may lie depicted table 
general concerned predicting global behavior system precise behavior components 
high level precision requiring detailed level description interactions chaotic unpredictable kolen pollack 
goal analysis gain predictive power modeling system right level 
case artificial complex systems possible determine level generating testing system 
case fully deterministic agent world possible usually realistic enumerate trajectories agent take action behavior space 
equivalent elaborating agent phase space 
early ai methods proving correctness consisted showing set possible initial conditions usually expressed discrete states agent series actions reach desired terminal state designed goal 
search methods plan action generation particularly amenable type analysis fikes nilsson 
scaling problem approach behavior analysis fails realistic worlds agent environment deterministic 
state transitions nondeterministic worlds modeled probabilistically doyle sacks obtaining appropriate values probabilities general difficult requires complete accurate model world 
small inaccuracies values accrue result artifactual dynamics global level 
consequently probabilistic models fail capture stochastic dynamics kinds complex behavior concerned 
crux problem determining appropriate level system description 
quantitative analysis extremely difficult simplest deterministic systems 
may appear problem researchers satisfied knowing system global qualitative behavior 
global behavior generally defined quantitative terms qualitative descriptions derived microscopic scale particle interactions abraham shaw macroscopic scale building maps chatila laumond environment 
path qualitative description system indirect requiring abstracting away details clustering analytical quantitative information 
qualitative description collection non analytic symbols words numbers complicated associated semantics 
semantics defined stated terms symbols eventually grounded numerical terms 
difficulty problem analytical approaches date limited constrained special case scenarios 
surprising general method analyzing complex systems interacting components powerful provide useful predictions 
prediction group behavior difficult individual perspective approaches focus describing analyzing ensemble properties appear better suited domains addressed 
section describes approach assessing qualitatively predicting global behavior measuring interference local property collective consequences 
interference conflict interference influence opposes blocks agents goal driven behavior 
societies consisting agents identical goals interference manifests competition shared resources 
diverse societies agents goals differ complex conflicts arise including goal deadlocks oscillations 
term sense sussman mcdermott chapman 
functionally distinct types interference relevant interference caused multiplicity called resource competition interference caused goal related conflict called goal competition 
resource competition includes interference resulting multiple agents competing common resources space information objects 
size group grows type interference increases causing decline global performance presenting impetus social rules 
resource competition manifests homogeneous heterogeneous groups coexisting agents 
contrast goal competition arises agents different goals 
agents may identical high level goals example family individuals pursue different potentially interfering subgoals particular instance functionally heterogeneous 
heterogeneity arise simd style groups functionally identical agents executing exactly program point time 
goal competition studied primarily distributed ai community gasser huhns 
usually involves predicting agents goals intentions requiring agents maintain models huber durfee miceli cesta 
prediction abilities require computational resources scale increased group sizes contrast discussed goal competition need agents model minimized agent homogeneity focus largely issues direct resource competition 
individual vs group benefit social rules attempt eliminate minimize resource goal competition 
particular purpose direct behavior away individual greediness global efficiency certain groups tasks agents give individual optimality favor collective efficiency 
cases greedy individualistic strategies perform poorly group situations resource competition grows size group 
agents described fall category 
social rules designed optimizing global resources interest individuals obey 
connection individual collective benefit rarely direct societies harbor social rules favor individual benefit 
game theory offers elaborate studies effects individual optimality axelrod domains problem maintaining internal models called theories mind discussed detail section 
cultural contexts global efficiency elevated common 
treated game theory cleanly constrained treated 
particular game theory deals rational agents capable evaluating utility actions strategies 
contrast concerned situated agent domains agents assumed rational due incomplete nonexistent world models models agents inconsistent reinforcement noise uncertainty 
furthermore goal devise optimal strategies specific group behavior provide methodologies finding efficient approaches variety related problems 
optimality criteria agents situated physical worlds maintaining long term achievement maintenance goals difficult characterize difficult achieve 
game theory interference part competing agent predictable strategy embodied multi agent domain interference largely result direct resource competition moderated relatively simple social rules 
example complex traffic jams alleviated appropriate yielding 
estimating interference understanding interference integral part synthesizing analyzing group behavior 
synthesis task distributed multiple agents way minimizes interference benefits concurrent execution lost 
analysis interference taken account order characterize realistic behavior distributed system motivate existence social rules protocols 
attempting precisely predict inter agent interference equivalent trying predict system exact behavior 
argued analysis general level prediction impossible reach 
section proposes qualitative alternative applied obtain useful estimates 
agent density key parameter estimating interference measures likelihood interaction 
higher density higher probability agents encounter interact 
evaluating outcome interaction able predict estimated frequency useful part describing dynamics group 
example probability interaction density determines collectively conscious agent greedy behavior get away 
density estimation straight forward 
define group density ratio sum agents footprints size available interaction space 
agent footprint sphere influence 
spatial domain agent footprint geometry motion constraints sensor range configuration 
size interaction space area physical space agents inhabit 
idea applies domains 
domains interaction space time agent footprint duration information exchange 
instance telecommunications domain density estimated duration calls unit time 
highway traffic example relevant space interactions time 
agent density represented ratio sum agents footprints total surface area road 
density metric allows computing interaction space necessary group perform task specific amount interaction space sufficient 
spatial domain example number size agents compute mean free path agent estimate collisions expected agents executing random walks 
similarly telecommunications domain average uninterrupted call duration relative average number calls unit time computed gives estimate phone interaction space available parameters 
highway domain computation yields average length free speeding approximate measure density estimate interaction space average required system specifics task considered 
bringing constraints task computation expected interference duration task estimated 
tasks interference vary depending fluctuations density lifetime task 
temporal density distribution demonstrates parts task require social rules 
exact computation relevant density dependent particular domain task rough approximation provides useful metrics estimating dynamics group evolution behavior system 
summary things chapter described constraints imposed agents order structure focus study group behavior 
thesis focused homogeneous agents explicit world models undirected communication implicit cooperation 
constraints chosen order approach group behavior problem bottom model include stationary police cars 
tally 
concerned testing limits minimal internal modeling communication order find simple abilities sufficient complex representation communication abilities necessary 
chapter related robotics behavior control thesis focuses problems involved synthesizing analyzing intelligent group behavior 
particular described applies agents embodied situated physically constrained worlds inhabited agents kind dealing multiple goals ranging basic survival accomplishing tasks 
experimental environments validated mobile robots multi agent simulations 
consequently related number lines research outside ai including mobile robotics intelligent control simulations multi agent systems distributed artificial intelligence artificial life machine learning ethology cognitive science 
section presents overview related fields exception machine learning covered second part thesis 
control multiple physical robots decade witnessed shift emphasis robotics general mobile robotics particular physical implementations 
robotics far focused control single agent 
majority projects dealt control multiple physical robots 
fukuda buss subsequent describe approach coordinating multiple homogeneous heterogeneous mobile robotic units demonstrate docking task 
choi latombe yim noreils noreils remain faithful state framework apply traditional planner control architecture box moving task implemented robots master slave configuration 
kube kube zhang describe series simulations robots performing collection simple behaviors incrementally transferred physical robots 
barman mackworth pai sahota wilkinson zhang report preliminary testbed studying control multiple robots soccer playing task 
parker parker describes behavior task sharing architecture controlling groups heterogeneous robots demonstrates set physical robots performing toxic waste cleanup box pushing 
donald jennings rus report theoretical grounding implementing cooperative manipulation task pair mobile robots 
closest philosophy choice task beckers holland deneubourg 
describes variant foraging task group lego robots controlled reactive distributed style 
beckers 
demonstrate group robots clustering initially randomly distributed pucks single cluster purely stigmergic communication 
terms cooperation communication fallen ends spectrum uses extensive explicit communication cooperation 
systems cooperative design robots aware existence sense recognize directly communication 
type research explores explicit cooperation usually directed communication represented 
noreils parker 
category includes implicit cooperation robots usually recognize merely coexist indirectly cooperate having identical compatible goals 
includes dallas kube 
described thesis falls nearer spectrum focused agents discriminate rest world ability basis social behavior 
simulations multiple agents exception described problem multi agent control treated simulation major categories simulations situated systems simulations agents 
simulations situated systems involve degree faithfulness physical world extent employing simple models sensors effectors physical laws 
number simulations behavior style controlled systems implemented 
instance steels describes simulation simple robots principles self organization perform gathering task 
brooks maes matari moore report set simulations similar task domain fully decentralized collection non communicating robots 
arkin describes schema approach designing simple navigation behaviors programming multiple agents working simulated environment extensions physical agents arkin balch apply approach multi agent retrieval task 
brock montana describe simulations large numbers tank robots performing avoidance formation 
kube zhang wang propose behavior arbitration scheme tested physical robots 
simulations tend simplify sensing actuation 
physically simulations realistic physics models agent allow generating testing realistic behavior 
example hodgins brogan describe experiments fully physically simulations groups hopping robots 
contrast simulations multiple robots swarm intelligence refers simulations agents dealing theoretical problems communication protocols design social rules strategies avoiding conflict deadlock societies large numbers simple agents 
representative includes fukuda arai dario dudek 
huang beni sandini beni dario sandini 
related dai see contrast dai deals agents comparatively low cognitive complexity 
artificial life field artificial life alife focuses bottom modeling various complex systems 
alife relevant thesis features simulations colonies ant agents described drogoul colorni dorigo maniezzo ferber travers 
deneubourg 
deneubourg goss deneubourg goss pasteels experimented real simulated ant colonies examined role simple control rules limited communication producing trail formation task sharing 
deneubourg beckers define key terms swarm intelligence discuss issues relating local global behavior distributed system 
packard hogeweg related report variety simulations simple organisms producing complex behaviors emerging simple interactions 
reports experiment amount knowledge agents increased decreased local encounters 
werner dyer describe systems evolve simple communication strategies 
theoretical keshet describes model trail formation fits biological data 
artificial life related thesis concerned exploiting dynamics local interactions agents world order create complex global behaviors 
alife usually concern agents situated physically realistic worlds 
additionally usually deals larger populations sizes 
commonly employs genetic techniques evolving agents comparatively simple control systems 
distributed artificial intelligence distributed artificial intelligence dai field deals multi agent interactions see gasser huhns overview 
dai focuses negotiation coordination multi agent environments agents vary knowledge systems sorting algorithms approaches vary heuristic search decision theory 
general dai deals cognitively complex agents compared considered research areas described far 
types environments deals relatively simple low complexity feature noise uncertainty accurately characterized 
dai divided subfields distributed problem solving dps multi agent systems mas rosenschein 
dps deals centrally designed systems solving global problems built cooperation strategies 
contrast mas deals heterogeneous necessarily centrally designed agents faced goal utility maximizing coexistence 
decker lesser example dps 
addresses task fast coordination reorganization agents distributed sensor network goal increasing system performance decreasing performance variance 
hogg williams example showing parallel search performs better distributed cooperative agents independent agents 
examples mas include ephrati describes master slave scenario agents essentially goals 
miceli cesta describe approach estimate usefulness social interactions individual agent level order agents select agents inter act 
decision estimate possible payoff terms help agents attitudes skills 
unfortunately estimation dependence relations scales poorly size group case dai best suited small number highly deliberative non situated knowledge agents 
similar lines kraus describes negotiations contracts selfish agents 
durfee lee gmytrasiewicz discuss game theoretic ai approaches deals rational agents 
describes advantages introducing meta level information 
certain aspects dai purely theoretical deal difficulty multi agent planning control environments 
example shoham tennenholtz discuss complexity automatically deriving social laws agent groups 
show problem np complete number restrictions polynomial 
dai draws heavily mathematical results field parallel distributed systems 
particular huberman describes effects information exchange performance collection agents applied class search problems 
addresses ubiquity log normal distributions performance different domains hypothesizes universal law distribution large systems interdependent agents resources allocated perceived progress 
clearwater huberman hogg related cooperative strategies solving constraint satisfaction problems 
dai alife merge experimental mathematics field studies computational ecosystems simulations populations agents defined interactions 
research focused global effects changes system time 
process global changes usually referred evolution kephart hogg huberman 
systems studied similarities global effects biological ecosystems complex details biological systems reasonably addressed 
evolution experiments find improved search optimization techniques 
example hillis demonstrates evolution overcome local maxima evolving optimal sorting algorithms 
behavior analysis previous section described related synthesis control group behavior 
section reviews related analysis group behavior 
described earlier distributed artificial intelligence dai deals multi agent negotiations coordination variety environments 
decker lesser example dai approach modeling distributed system 
depends ability specify agents beliefs intentions quality duration 
types models scale group size 
order apply need away low level properties system exact noise errors shown critically effect high level behavior wiggins 
similarly describes probabilistic method agent coordination markov processes 
method relies specifying agents inference mechanisms chains having agents compatible specifiable goals preferences 
type approach applies domains problem resource allocation clearly specified 
ability predict agents behavior order assess resource allocation problem extremely difficulty physical system noise uncertainty 
number mathematical game theoretic paradigms apply 
classical robotics field motion planning dealt problem planning multiple objects 
example erdmann lozano erez describe theoretical results motion planning problem multiple polygonal moving objects 
solution searches dimensional representation space time slices find safe path 
results depend having object move time constraint easily enforced situated systems 
furthermore proposed strategy computationally intensive applied real time control 
donald 
discuss motion planning algorithms coordinated manipulation different numbers agents different amounts priori knowledge object moved 
theoretical aspect focuses computing information requirements performing particular robot tasks 
directly applicable manipulation tasks box pushing addressed robots cooperating force 
contrast focus algorithms explicit cooperation tasks object manipulation distributed solutions problems necessitate cooperation benefit 
strategies proving distributed algorithm correctness tangentially related analyzing multi agent behavior 
lynch tuttle example describe methods distributed systems hierarchical components 
closely related lynch uses simulation method reasoning real time systems modeled general automata 
targeted proving properties message passing protocols constrained uncertain communication distributed physical agents 
stochastic analysis qualitative dynamics doyle sacks appealing qualitative nature 
proofs depend ability represent system series transitions graph system dynamics markov chain graph 
difficulty lies establishing model multi agent system 
general difficult obtain values transition probabilities capture complex dynamics systems 
simpler models constructed fail contain detail conserve dynamics 
related analysis group behavior conducted branches biology 
example beli deneubourg lax model constructions partial differential equations describing bee density distribution hive wax distribution behavior 
structured group behavior exploration foraging addressed 
instance benhamou describe probabilistic model foraging 
closest domains addressed thesis done deneubourg 
deneubourg 
deneubourg authors propose strategies describing analyzing various collective behaviors ants 
closest nature kind analysis propose viable describing group behavior situated embodied agents 
cases analysis performed level collective individual 
similarly sole goodwin framework describing ant behavior individually chaotic collectively stable periodic 
spatial distributions activity display similar symmetries 
brown describe model simple political voting system displays large array group behaviors simple local feedback recruitment persuasion mechanisms 
system stable states homogeneous distribution collection invariant blocks 
intuitively analogy equal power distribution imbalance results transient instability 
shows analogous pattern honey comb population nectar foraging brood sorting post travis demonstrate aggregation type behaviors shown fit pattern 
form common feedback behavior involves synchronization rhythmic patters activity 
example meier koll describe synchronization circadian rhythms human animal subjects models collection coupled oscillators 
analogous effects commonly observed hormonal cycles vander sherman luciano 
systems synchronized state stable behavior evenly dispersed equal power state states transient 
reports similar synchronization behavior insect rhythmic signaling proposes similar model behavior 
summary thesis shares motivations goals number related fields including ai robotics dai alife ethology 
chapter reviewed related lines research fields preparation chapter describes detail proposed approach 
chapter basic behavior approach hardest problems ai finding right level system description effective control learning modeling analysis 
thesis proposes particular description level instantiated called basic behaviors building blocks synthesizing analyzing complex group behavior multi agent systems 
biology provides evidence support basic behavior units variety levels 
particularly clean compelling case motor control 
controlling multi joint manipulator frog leg human arm complex task especially performed low level 
order cut complexity nature imposes abstraction 
mussa ivaldi giszter show relatively small set basis vector fields frog spine generates frog entire motor behavior repertoire applying appropriate combinations basis vectors 
bizzi mussa ivaldi giszter bizzi mussa ivaldi discuss control human arm similar approach 
described motor basic behaviors result types constraints dynamics manipulator dynamics motor tasks 
case motor control behaviors designed specific optimizations minimizing effort minimizing jerk executing straight line trajectories bell shaped velocity profiles atkeson 
idea motor control define behaviors control laws encapsulate sets constraints achieve particular goals 
basic behaviors defined minimal set behaviors appropriate compositional properties takes advantage dynamics system effectively accomplish repertoire tasks 
basic behaviors intended tool describing specifying predicting group behavior 
properly selecting behaviors generate repeatable predictable group behavior 
furthermore apply simple compositional problem synthesis analysis intelligent group behavior order understand phenomenon science apply engineering 
assertion complex group behavior results local interactions simple rules 
approach propose basic behaviors structuring simple rules 
validation implement robot group behaviors basic behavior set combinations 
table summary group behavior problem addressed thesis structure proposed solution 
operators generate large repertoire higher level group behaviors basic set 
idea basic behaviors general particular sets behaviors domain specific 
order demonstrate methodology basic behaviors group interaction spatial domain derived combined analyzed theoretically tested empirically 
table summarizes research goals approach experimental methodology 
selecting evaluating basic behaviors chapter describes basic behaviors selected specified implemented evaluated 
idea basic behaviors general intended primitives structuring synthesizing analyzing system behavior building blocks control planning learning 
basic behaviors related dynamic attractors equilibrium states various terms describe stable repeatable primitive behaviors system 
concerned finding ways identifying behaviors specific system structure rest system behavioral repertoire 
power basic behaviors lies individual reliability compositional properties 
focuses basic behaviors generating intelligent group interactions multi agent systems 
belief global behavior systems results local interactions furthermore interactions largely governed simple rules 
basic behaviors mechanism structuring space possible local rules small basis set 
chapter illustrate process selecting basic behaviors concrete examples behaviors group agents interacting physical space 
process identifying basic behaviors formally specifying implementing testing properties theoretically empirically combining carried 
criteria selecting basic behaviors domain spatially interacting agents described 
criteria selection propose domain small set basis basic behaviors selected complex relevant desirable group behaviors generated 
basic behavior sets meet criteria necessity behavior basic behavior set necessary achieves goal required agent accomplishment task goal achieved basic behaviors combinations 
basic behavior implemented terms behaviors reduced 
sufficiency basic behavior set sufficient accomplishing set tasks domain behaviors necessary 
basic behavior set combination operators generate desirable higher level group behaviors 
behaviors designed hand opposed observed existing system addition criteria properties 
simplicity behavior implemented simply possible 
locality framework behavior generated local rules utilizing locally available sensory information 
correctness model tested behavior provably attain cases maintain goal intended set conditions designed 
stability behavior sensitive perturbations external conditions designed 
repeatability behavior perform specification trial reasonable conditions error margins 
robustness performance behavior degrade significantly presence specified bounds sensory effector error noise 
scalability behavior scale increased decreased group size 
difficult imagine fixed metric selecting optimal set behaviors choice basic behavior set depends task applied 
attempt devise optimality criteria formal sense 
furthermore provide theoretical proofs correctness algorithms behaviors 
proofs may computable simple model agents environment prohibitively difficult increasingly realistic models include sensors effectors dynamics 
alternative simplified modeled environments behaviors tested fully complex worlds error noise 
order evaluation complete various initial conditions group sizes tested large amount data obtained analysis 
behavior evaluation described detail section 
section illustrates process selecting basic behaviors domain planar mobile agents 
basic behaviors movement plane experimental thesis focused interactions mobile agents dimensional space 
domain desired complexity properties number possible collective behaviors unbounded 
fortunately unbounded space possible spatial temporal patterns classified classes effectively viewed lower level resolution 
classification task domain specific criteria allow selecting comparatively relevant behavior classes focus 
proposed basic behaviors impose classes define observable group behaviors specifying particular rules implementing 
group behaviors spatial domain viewed spatio temporal patterns agents activity 
certain purely spatial fixed organizations agents relevant certain spatio temporal patterns 
purely spatial fixed organizations agents correspond goals attainment spatio temporal patterns correspond goals maintenance 
safe wandering ability group agents move avoiding collisions obstacles 
homogeneous nature agents inter agent collision avoidance 
distinct strategies devised avoiding collisions agents kind avoiding collisions 
ability agents move staying 
dispersion ability group agents spread area order establish maintain predetermined minimum separation 
aggregation ability group agents gather order establish maintain predetermined maximum separation 
homing ability reach goal region location 
table basic behavior set spatial domain intended cover variety spatial interactions tasks group mobile agents 
process selecting basic behaviors designer attempts decide behavior set suffice large repertoire goals 
dynamical properties system provide bottom constraints goals provide top structure 
influences guide behavior selection process 
energy minimization universal goal powered physical systems 
planar motion domain goal translates minimization non goal driven motion 
motion generated poor behavior design interference agents 
minimizing interference means maximizing goal driven behavior minimizing unnecessary motion 
minimizing interference translates directly achievement goal immediate avoidance maintenance goal moving collisions 
avoidance groups achieved dispersion behavior reduces interference locally 
serve minimize interference classes tasks require space coverage involving searching exploration 
contrast various goals minimize interaction decreasing physical proximity goals involve exchange resources physical proximity 
consequently aggregation useful primitive 
moving group requires form coordinated motion order minimize interference 
flocking examples structured group motion 
table shows list behaviors constitutes basic set flexible repertoire spatial group interactions 
biology offers numerous justifications behaviors 
avoidance wandering survival instincts ubiquitous obviates discussion 
innate seen numerous species mcfarland 
dispersion commonplace 
show elegant evidence aggregating dynamically rearranging positions field maintain fixed distance 
demonstrates similar gull behavior ledge 
people maintain similar arrangements enclosed spaces gleitman 
similarly floreano demonstrates simulated evolved ants dispersion consistently 
aggregation protective resource pooling sharing behavior species ranging mold social animals mcfarland 
combination dispersion aggregation effective tool regulating density 
density regulation ubiquitous generically useful behavior 
instance army ants regulate temperature aggregating local temperature gradient franks 
temperature regulation just side effects density regulation 
homing basis navigation manifested mobile species biological data pigeons bees rats ants salmon see gould muller wehner waterman foster castro mcnaughton matari 
described behavior set numerous useful group behaviors exist 
example biology suggest surrounding herding frequent patterns group movement related higher level achievement goal capture migration mcfarland 
behaviors generated combining basic primitives described demonstrated chapter 
basic behavior experiments remainder chapter describes experimental environments presents algorithms implementing proposed basic behaviors evaluates performance battery tests collection criteria 
experimental environments behavior observation primary methods validating theories synthetic ai projects described thesis 
order conclusive results necessary try separate effects caused particular experimental environment intrinsic theory tested 
order get heart group behavior issues specific dynamics test environment different environments results compared 
environments interaction modeler collection physical robots 
motivation physical modeled environment attempt isolate observable inconsistencies performance behaviors different environments 
general difficult determine features real world retained simulation abstracted away 
testing systems physical world effects arise artifacts simulation identified brooks 
motivation data physical robots 
token current state art physical robot environments imposes constraints biases types experiments conducted 
consequently results physical environment validated alternative setup 
different robot types order eliminate system specific biases 
concerned basic principles interaction group behavior specific domain especially concerned effects common modeled physical worlds 
agent interaction modeler interaction modeler im simulator allows modeling simplified version physics world agent sensors dynamics 
modeler control software agents written lisp 
purposes realism modeler divided distinct components simulator physics modeler agent specification 
simulator executes agent specifications moves agents control algorithms sensory readings 
simulator implements physics sensors physics world 
implemented physics modeler checks positions motions computed simulator simplified physical laws applies corrections 
im loops simulator physics modeler 
main purpose interaction modeler observe compare phenomena obtained physical robots 
modeler useful preliminary testing group behaviors implemented physical robots 
difficult directly transfer control strategies simulations interaction modeler environment 
agents shown black circles white markers indicating heading 
large rectangle indicates boundaries workspace 
agents equipped local sensors simplified dynamics 
herd robots long wheeled base equipped pronged picking carrying stacking pucks radio transmitter receiver inter robot communication data collection 
physical world modeler useful eliminating infeasible control strategies early stage testing vastly larger numbers agents performing experiments varying parameter values 
mobile robot herd group behavior experiments implemented tested collection physically identical mobile robots dubbed herd 
robot long wheeled vehicle equipped piezo electric bump sensor side rear chassis 
robot pronged picking carrying stacking pucks 
contains contact switches tip fork infra red sensors pointing forward detecting objects aligning pucks break beam sensors detecting puck jaw throat pointing sensors aligning fork stack pucks stacking 
pucks special purpose light metal foam filled disks inches diameter inches height 
sized fit fork held fork magnet 
robots equipped radio transceivers broadcasting byte data robot second 
system uses radio base stations triangulate ir bump contact radio ir ir bump bump bump contact irs herd robots equipped contact sensors ends fork piezo electric bump sensors side rear chassis infra red sensors fork 
forward pointing irs located ends forks break beam irs jaw throat fork pointing ir stacking pucks middle fork arms 
robots positions 
radio system data gathering simulating additional sensors 
particular radios distinguish robots objects environment ability implemented board ir sensors mechanical communication sensory capabilities robots allow exploration environment robot detection finding picking carrying pucks 
basic abilities construct various experiments robots run autonomously processing power board 
processing performed collection motorola hc microprocessors 
processors dedicated handling radio communication operating system brain robot executing loaded control system experiments 
control systems programmed behavior language parallel programming language subsumption architecture brooks 
irs frequency mechanically positioned obstacle detection communication 
hardware limitations properties physical hardware impose restrictions control strategies applied types tasks experiments implemented 
robot hardware various sensory mechanical computational limitations 
section describes relevant properties hardware effect 
robots mechanical steering system inaccurate rotational degrees 
furthermore position triangulation system works sufficiently robots predetermined range base stations 
exchange information robots nominally ought take place hz suffers extensive loss data 
consequently half transmitted data lost incorrect 
combined effect steering positioning uncertainty demanded robots move slowly order minimize error 
limiting factor robot speed imposed sensing actuation controller 
infra red sensors relatively long range inches vary sensitivity 
consequently different robots different sensing ranges tuned due hardware restrictions sensitivity sides fork single robot varies 
consequently amount time effort required detecting picking avoiding objects varied robots time 
control system dependent uniformity group 
uncertainty variability frustrating beneficial experimental validity 
instance hardware variability robots reflected group behavior 
programmed identical software robots behave differently due varied sensory actuator properties 
small differences individuals amplified robots interact extended time 
nature individual variability creates demand robust adaptive behavior 
variance mechanics resulting behavior provides stringent test experimental behaviors 
experimental procedure robot modeler programs archived basic behaviors tested domains 
robot implementations basic composite behaviors tested trials case modeler behaviors tested trials identical random initial conditions 
different strategies group behaviors tested compared domains 
modeler data gathered keeping record relevant state position orientation gripper state time 
data gathered robot experiments radio system 
system allowed recording robots position bytes state time 
robot experiment robots ids initial positions recorded 
experiments conducted random initial conditions random robot positions identical initial positions order measure repeatability behaviors 
robot data recorded video tape validation cross referencing 
chapter interaction modeler data shown form discrete snapshots global state system relevant times including initial state converged state 
robot data plotted real time viewer special purpose software package designed recording analyzing robot data uses transmitted radio data plot real time positions robots time history movements trail positions previously manipulated pucks position home 
allows replaying data recreating robot runs 
robots shown black rectangles aligned direction heading id numbers back white arrows indicating front 
experiments robot state indicated symbol bounding box 
shown data plots size rectangles representing robots scaled maintain correct ratio robot environment surface area order demonstrate relative proximity active robots 
bottom plot shows robots run 
corner display shows elapsed time seconds snapshot experiment 
shows typical data plot 
basic behavior specifications section gives formal specifications behavior terms goal achieves maintains 
basic behaviors space specified terms positions distances case foraging data obtained set robots described section 
implemented maintained matthew marjanovi 
frame time example robot data plot robots shown scaled black rectangles aligned direction heading id numbers back white arrows indicating front 
bottom plot shows robots run corner display shows elapsed time seconds 
distance thresholds ffi avoid ffi disperse ffi aggregate set robots fr home home home home home gamma home gamma gamma gamma notation specifications basic behavior goals 
safe wandering goal safe wandering keep moving maintaining minimum distance ffi avoid agents dp dt ffi avoid goal achieve maintain minimum angle position leader relative follower leader follower dp dt delta gamma dp dt kk gamma cos cos dp dt delta gamma dp dt kk gamma dispersion goal dispersion achieve maintain minimum distance ffi disperse agents ffi disperse ffi disperse ffi avoid aggregation goal aggregation achieve maintain maximum distance ffi aggregate agents ffi aggregate homing goal homing decrease distance agent goal location called home dp dt delta gamma home basic behavior algorithms section presents algorithms implement proposed basic behaviors interaction modeler robots 
algorithms formal notation algorithmic pseudo code 
algorithms formally expressed velocity commands form command operators computing algorithms 
neighborhood operator robot distance threshold ffi returns robots neighborhood ffi fj ffig centroid operator robot distance threshold ffi returns local centroid ffi ffi jn ffi global centroid operator jnj safe wandering strategies moving avoiding collisions studied topic mobile robotics 
thesis concerned finding avoidance strategies perform group situations scale increased group avoid agents agent nearest agent left turn right turn left 
algorithm avoid obstacle obstacle right turn left 
obstacle left turn right 
consecutive identical turns backup turn 
obstacle sides wait 
obstacle persists sides turn randomly back 
algorithm sizes 
finding guaranteed general purpose collision avoidance strategy agent situated dynamic world difficult 
multi agent world problem intractable 
inspired biological evidence indicates insects animals precise avoidance routines wehner general avoidance behavior command cos sin orientation incremental turning angle away obstacle 
simple avoid agents rule devised shown algorithm 
avoid agents behavior takes advantage group homogeneity 
agents execute strategy behavior rely take advantage resulting spatial symmetry 
agent fails recognize agent sensors case radios subsequently detect collision avoidance sensors case irs treat generic obstacle safe wander agent nearest agent left turn right turn left 
obstacle obstacle right turn left 
obstacle left turn right 
consecutive identical turns backup turn 
obstacle sides wait 
obstacle persists sides turn randomly back 
move forward turn randomly 
algorithm avoid behavior shown algorithm 
provably correct avoidance strategy arbitrary configurations multiple agents difficult devise 
order increase robustness minimize oscillations strategies take advantage unavoidable noise errors sensing actuation result naturally stochastic behavior 
stochastic component guarantees avoiding agent get stuck infinite cycles oscillations 
addition implicit stochastic nature robots behavior avoid utilizes explicit probabilistic strategy employing randomized move 
variations avoidance algorithm experimented compared amount time agent spent avoiding relative amount time spent moving freely 
ratio indirect measure quality avoiding strategy time agents spend avoiding worse strategy avoiding time dependent agent density controlled variable experiments 
ratio evaluate avoidance indirect metric direct measure stuck useful robots appropriate sensors determining state 
significant performance differences similar strategies tested 
strategy safe wandering combination avoidance strategies default rule moving changes heading shown algorithm 
follow agent agent right turn right 
agent left turn left 
algorithm implemented respect follower agent 
achieved simple rule steers follower position leader command leader gamma follower leader gamma follower implemented complement avoid behavior shown algorithm 
illustrates robots 
additional data analyzed section 
approach models behavior biology sensory organs stimulated difference stimuli determines motion insect mcfarland 
ant differential pheromone intensity perceived left right antennae deneubourg agents described binary state directional ir sensors 
conditions sufficient density safe wandering produce complex global behaviors 
instance behavior ants exhibits emergence unidirectional lanes regions ants move direction 
lane forming effect demonstrated robots executing avoiding behaviors 
complex sensors order determine direction follow 
irs agents distinguish agents heading away unable select follow 
frame time frame time frame time frame time example robots 
continuous time trails shown 
spite deviations individual paths queue conserved 
centroid disperse agents move away 
algorithm neighbor disperse find nearest neighbors compute angle compute negative bisector align direction go forward 
algorithm dispersion robust dispersion behavior designed extension existing safe wandering 
avoidance safe wandering reacts presence single agent dispersion uses local distribution nearby agents locations agents range robot sensors order decide direction move 
algorithm shown algorithm computes local centroid determine local density distribution nearby agents moves away highest density command gammav ffi disperse gamma ffi disperse gamma conditions high density system take long time achieve dispersed state local interactions propagate far motion individual disturb state 
dispersion best viewed ongoing process maintains desired distance agents performing tasks 
number dispersion algorithms tested modeled environment 
robot implementation approaches detecting position nearest agents 
modeler allowed precise information exact distance direction nearest neighbors 
dispersion algorithm shown algorithm successful terms efficiency reliability 
shows initial state final state dispersion experiment example dispersion 
agents initially packed half workspace 
dispersion set times agent diameter 
approximately time steps equilibrium reached agents moving 
frame time frame time frame time dispersion robots initiated close 
robots static dispersed equilibrium state seconds 
centroid dispersion rule tested interaction modeler 
initially crowded part available free space agents apply simple dispersion rule order establish disperse maximum available inter agent distance 
shows dispersion algorithm applied robots 
dispersion evaluated time convergence 
algorithms local centroid nearest agents compared potential field summation approach scalar distance nearby agent proportional magnitude repulsive vector associated 
vectors nearby agents summed agent moved direction resultant 
performance algorithms compared different initial conditions random densely packed 
tested order normalize different density distributions lifespan task 
expected random initial position results faster convergence times packed initial condition algorithms 
statistically significant difference algorithms 
aggregation aggregation inverse dispersion command ffi aggregate gamma ffi aggregate gamma aggregate nearest agent outside turn local go 

algorithm home home 
turn home go 
algorithm implemented centroid operator shown algorithm 
aggregation evaluated criteria evaluating dispersion experiments 
analogous algorithms implemented local centroid nearest neighbors potential fields 
varying initial conditions aggregation algorithms evaluated different terminating conditions 
difficult terminating condition required agents form single aggregate easier conditions required form groups agents fixed distance neighbors 
expected terminating condition required time achieved 
aside effect statistically significant difference algorithms 
homing simplest homing strategy greedy local command home gamma home gamma implemented simple pursuit shown algorithm 
illustrates homing behavior robots strategy 
data illustrate actual trajectories far optimal due mechanical sensory limitations particular due error sensed position 
frame time frame time frame time frame time homing behavior robots 
started arbitrary initial configuration robots reached home region seconds fifth joined seconds 
trails reflect errors position sensing interference robots approach home region 
frame time frame time frame time frame time example homing behavior robots started arbitrary initial positions 
trail histories demonstrate drastic errors positioning indicated large jumps consecutive robot location 
particular triangular path shown robot due repetitive position errors 
spite errors robots successfully reached home 
homing behavior large group simulated agents 
increased interference competition space obvious goal region 
algorithm tested interaction modeler produces direct homing trajectories 
shows robot run homing robots 
run entire time history robots positions shown positioning errors easily seen 
robots reach home 
illustrates homing simulation 
individual homing effective long density agents low 
agents homing confined space interfere 
case non holonomic robots interference enduring effects group 
shows growing interference robots approach goal region 
entire time trails shown demonstrate group interference slows individual performance 
simulation robot experiments described show interference increases agents non zero turning radii unequal velocities subject sensor control errors 
conditions common situated agents suggesting need form group structured navigation flocking introduced upcoming section 
basic behavior evaluation empirical evaluation basic behaviors evaluation difficult components research somewhat new field ai experimental robotics 
nature design fields building artificial computational physical systems 
results synthetic endeavors fall cleanly defined set evaluation criteria designed natural sciences 
analyzing designed intrinsically different analyzing externally imposed 
young diverse field ai lacks standardized evaluation criteria 
consequently left researcher establish criteria specific project generally acceptable 
ideas proposed thesis evaluated ways 
addresses merit general approach applicability various domains 
evaluation performed summary thesis entire 
second type evaluation addresses specific instantiation ideas spatial domain 
chapter presents evaluation criteria applied implementations performance spatial basic behaviors composites 
ai robotics research general exploratory prone homing behavior robots 
home located region 
trails marked different patterns order demonstrate increase interference proximity resulting circuitous paths 
logical evaluation 
prevent evaluation criteria experimental part established prior testing applied performance behaviors combinations 
earlier section basic behavior selection elaborated criteria choosing basic behavior set hinted evaluation procedures 
section gives detailed illustration empirical basic behavior evaluation example 
pre specified definition robot said maintained minimal angle leader 
repeatability robustness evaluated manifested average uninterrupted duration average time failure 
duration completely dependent reliably front pointing sensors detect leader 
illustrates continuous behavior robots minute period 
robot front queue moving forward wheels slightly turned tracing circular path 
robots follow local leader algorithm 
path robot smooth followers oscillate order keep robot ahead ir range 
robots separated minutes stayed duration shown second run 
illustrates robustness robot lead moves randomly follower keeps duration run 
range ir sensors directed short requiring agents stay close queue 
consequently errors steering cause follower lose sight leader failed turn sufficiently order maintain leader sight 
continued move direction higher level task follower catch leader 
separate 
narrow ir range explains long queues trains agents physically difficult maintain 
queues stable insensitive dynamic obstacles sensory mechanical irregularities form sensor noise errors steering perturbations velocity 
illustrates robots presence sensory effector error 
middle robot stalls due error robot stops turns follows leader senses robot range 
middle robot activates senses second robot range follows queue maintained 
demonstrates presence static constraints environment walls corners 
robots able avoid walls maintain queue 
evaluated scalability order test performance agents added removed 
data demonstrate behavior frame time frame time frame time frame time continuous behavior robots minutes 
initial conditions wheels front robot turned sideways resulting circular trajectory 
robots reliably maintain stable queue spite individual local variations control 
frame time frame time frame time frame time continuous performance robots minutes 
third robot range join 
robot front moves randomly follower stays close 
performance robots presence obstacles sensory steering errors cause middle robot stall 
third robot passes maintains robot range 
middle robot senses second robot range follows queue maintained 
frame time frame time frame time frame time performance behavior robots presence external obstacles constraints 
robots maintain queue avoiding wall going corner 
duration agents mean duration behavior robots 
axis plots individual trials axis plots duration uninterrupted 
mean duration indicated dashed line 
duration agents mean duration behavior robots 
axis plots individual trials axis plots duration uninterrupted seconds 
mean duration indicated dashed line 
results agent stalls removed middle queue 
set data deals performance new agents added queue situation expected happen commonly global level recruiting behavior 
demonstrates average time robots multiple runs 
plots data robots 
mean time agents nearly identical 
exactly expected completely local behavior agents 
failure pair failure pairs mutually independent agents dynamically added removed ends queue affecting rest 
section illustrated criteria evaluate proposed basic behaviors 
evaluation process illustrated example 
described criteria systematically applied basic behaviors 
evaluation heterogeneous groups obvious alternative fully distributed system identical agents hierarchical distributed system 
order evaluate performance homogeneous basic behaviors compared particular hierarchical implementations 
section describes performance hierarchical group agents basic behaviors aggregation dispersion 
behaviors chosen stated terms achievement goals sufficient space reach static state 
algorithms evaluated time number steps required reach defined state 
version hierarchical agents implemented classifying agents total order randomly assigned unique id number simulating established order group chase chase chase chase rohwer 
homogeneous algorithms agents moved simultaneously identical local rules hierarchical case id number determined agents allowed move waited 
cases simple precedence order spatially local hierarchy established small radius agent highest id got move 
multiple types dispersion aggregation algorithms tested hierarchical agents 
interaction monitor experiments conducted group size agents algorithms 
additionally algorithms tested different degrees task difficulty 
aggregation tested number agents steps convergence performance different aggregation algorithms time required reach static aggregated state 
termination conditions tested single group data points shown boxes stable groups data points shown dots 
performance hierarchical algorithms interpolated solid lines homogeneous ones interpolated dotted lines 
terminating conditions single aggregate containing agents small number stable aggregates 
terminating condition difficult 
similarly dispersion tested initial conditions random distribution initial positions packed distribution agents start half available space 
condition difficult 
case aggregation hierarchical strategies performed somewhat better homogeneous approaches 
plots average number moves agent takes aggregation task different group sizes different terminating conditions single aggregate stable groups 
hierarchical homogeneous algorithms behaved expected performing better simpler terminating conditions 
performance declined consistently growing group size 
aggregation case dispersion homogeneous strategies outperformed hierarchical ones 
plots average number moves agent dispersion task different group sizes different initial conditions random distribution packed initial state 
hierarchical homogeneous algorithms improved easier initial conditions 
performance difference homogeneous hierarchical algorithms repeatable consistent small magnitude barely surpassed standard deviation individual trials algorithms group sizes 
standard deviation particularly significant case small group sizes 
statistically significant difference number agents steps convergence performance different dispersion algorithms time required reach static dispersed state 
initial states tested random distribution data points shown stars packed distribution data points shown crosses 
performance hierarchical algorithms interpolated solid lines homogeneous ones interpolated dotted lines 
global performance hierarchical flat algorithms aggregation dispersion 
furthermore slight differences detected strategies negligible physical agents due sensor uncertainty effector errors 
believe similarity performance homogeneous simple heterogeneous algorithms caused ffl functionally homogeneous agents spite linear priority ordering agents fundamentally homogeneous functionally indistinguishable 
hierarchical relationships agents spatially temporally independent agents keep history past encounters 
ffl simplicity behavior behavior observed spatial domain consequences actions identical agents time extended consequences 
ffl large group sizes sufficiently large groups functionally identical agents temporary effects averaged fluctuations noise 
property crucial producing reliable global behavior presence local perturbations observable shown data general trends global performance consistent standard deviation trials quite large 
initial conditions comparing dispersion algorithms 
maximally packed states different group sizes tested 
experiments comparing simple hierarchical homogeneous algorithms demonstrate described domain simple hierarchical strategies affect global performance impact global behavior negligible 
complex hierarchical strategies devised order assure influence global behavior require increased perceptual cognitive overhead keeping history past encounters models previously encountered agents 
data permit hypothesize simple spatial domains simple homogeneous solutions quite complex strategies requiring individual agents perform recognition classification representation may required significantly improve group performance 
evaluating distributed centralized algorithms thesis compared centralized distributed approaches argued centralized approaches scale types systems thesis dealt 
purposes comparison set special case scenarios constructed optimal centralized solutions computed dispersion task 
computing optimal dispersion strategy arbitrary configuration agents difficult large group sizes intractable strategy computed special classes initial positions 
total knowledge dispersion homogeneous dispersion hierarchical dispersion performance optimal global total knowledge algorithm dispersion data points shown diamonds compared hierarchical homogeneous dispersion strategies data points shown boxes crosses respectively 
packed configurations agents designed group sizes shown 
configurations chosen reasons challenging initial conditions dispersion optimal dispersion solutions computed advantage symmetry configurations 
optimal solutions employed general strategy moving outer agents space cleared layer move 
average number moves agent obtaining dispersed state computed group sizes 
total knowledge algorithm tested existing hierarchical homogeneous algorithms interaction modeler 
data distributed algorithms averaged trials group size 
plots performance algorithms 
surprisingly total knowledge algorithm performs best 
important note performance declines slower distributed algorithm offset constant factor 
performance total knowledge algorithm practically attainable real time distributed alternative minimum computational sensing overhead presents useful alternative 
summary chapter introduced methodology selecting basic behaviors demonstrated spatial domain 
basic behavior set consisting safe wandering dispersion aggregation homing proposed implemented different experimental environments tested simulation physical robots 
experimental data evaluated collection criteria specified priori 
performance basic behaviors tested compared hierarchical total knowledge approaches 
chapter introduces ways described basic behaviors combined order achieve variety higher level goals tasks 
chapter combining basic behaviors types behavior combination basic behaviors designed substrate variety complex compound group behaviors domain 
generating compound behaviors requires applying kind combination operator properties understood produces desired output composite behavior 
considered challenges behavior control arbitration problem coordinating activity multiple input behaviors order produce desired output behavior 
depending complexity system arbitration usually performed multiple points 
level arbitration achieved designing mutually exclusive behavior conditions matari 
creating unique mapping conditions behaviors guarantees mutually exclusive set condition action couplings 
contrast mapping condition result possible behavior possibility behaviors may conflict 
mutually exclusive behavior conditions sufficiently powerful arbitrating system performs behavior time 
complex systems multiple behaviors contribute output parker payton kimble rosenblatt ferrell 
consequently practical systems mutually exclusive behavior conditions coherent layer submodule system dealing particular coherent set tasks 
modules layers level arbitration necessary implements type sum inputs switch 
general form behavior system involves combination operators levels 
flocking surrounding herding surrounding flocking aggregation dispersion aggregation foraging flocking homing dispersion basic behaviors combined generate variety complex behaviors 
basic behaviors composite behaviors sensory inputs effector outputs control architecture generating group behaviors consists direct temporal combinations subsets fixed basic behavior set 
direct combinations marked temporal combinations input behaviors 
composite behavior general form direct behavior combinations 
outputs behaviors summed 
architecture proposed combining basic behaviors described general form 
order take advantage expressive combinatorial power basic behaviors architecture uses combination operators behaviors combined directly executing multiple behaviors temporally sequencing behaviors time 
direct combinations allow multiple concurrently active behaviors contribute outputs 
temporal combinations assure coherent sequence outputs 
types combination operators applied fixed set basic behaviors generate unbounded repertoire collective behaviors temporal combinations extend arbitrarily time 
sections describe operators demonstrate implemented compound behaviors 
direct combinations basic behaviors direct combination behaviors function outputs subset basic behaviors illustrated 
spatial domain outputs basic behaviors form direction velocity vectors appropriately weighted sums vectors directly produce coherent higher level behaviors 
method illustrated direct combination implement flocking 
flocking defined collective motion satisfies constraints agents sensing range stay flocking range neighbors move 
aggregation flocking requires agents stay move goal generically referred home 
formally ffi flock dpc dt delta gamma home flocking implemented combining outputs safe wandering aggregation dispersion homing specified constraints satisfied composite behaviors sensory inputs effector outputs flocking basic behaviors safe wandering aggregation homing dispersion implementation flocking combination safe wandering dispersion aggregation homing 
safe wandering aggregation dispersion produce robust flocking homing gives flock goal location direction move 
flocking aggregation dispersion surrounding herding safe wandering homing example direct basic behavior combination higher level task 
shown 
intuitively aggregation keeps robots getting far dispersion keeps getting close safe wandering prevents agent individually flock colliding non agent obstacles homing moves flock goal 
flocking reduced combination just safe wandering aggregation homing range values ffi flock ffi flock ffi aggregate safe wandering effect 
set basic behaviors allows direct composites surrounding combination aggregation herding combination surrounding flocking shown 
high level goal structure direct behavior combination herding flocking aggregation dispersion surrounding surrounding safe wandering homing direct behavior combinations continuous summing functions 
consequently basic behaviors reused recombined repeatedly common higher level goal 
example types surrounding depending sensory conditions 
input behaviors 
composite behavior general form temporal behavior combinations switches mutually exclusive behaviors 
behavior active time resulting behavior sequence triggered different sensory conditions 
directed acyclic graph dag behaviors nodes inheritance relations arcs 
semantics arcs identical semantics combination operators 
basic behaviors originator nodes graph 
final high level behavior node nodes combinations originator intermediate nodes graph 
illustrates example graph aggregation shared intermediate nodes flocking surrounding 
behavior combinations continuous function weighted sums input parameters nodes multiple combinations 
example illustrates basic behaviors aggregation construct different types surrounding behaviors combining herding 
temporal combinations basic behaviors basic behaviors direct combinations achieve maintain single goals 
foraging sensory conditions homing dispersion safe wandering implementation foraging temporal combination safe wandering dispersion homing 
triggered different sensory conditions behaviors collectively result foraging 
example dispersion achieves goal establishing minimum distance agents maintains goal preserving queue moving agents distance direction neighbors 
order achieve higher level tasks defined multiple sequential goals basic behaviors properly temporally combined 
combinations temporal sequences basic behaviors triggered appropriate conditions environment shown 
combining interactions temporally relies agents ability perceive state triggers behavior change 
ability simple finite state machine controllers designed generate variety multi goal behaviors 
method illustrated implementation foraging group task gathering objects food environment 
foraging high level achievement goal group collect objects environment deliver home 
complex behavior prototype variety tasks including harvesting garbage collection clearing toxic spills mine fields 
foraging task addition having basic behavior repertoire individual agents able search pucks pick drop 
furthermore foraging uses restricted notion kinship defined agents puck state robots pucks kin carrying pucks 
robots directly sense external state puck state broadcast robots limited range radios 
foraging initiated dispersion safe wandering 
finding object triggers homing 
encountering agent different immediate goal floreano shows evolved systems ants favor dispersion step foraging 
condition behavior home 
puck 
crowded 
kin 
sense puck 
safe wandering dispersion dispersion homing dispersion dispersion safe wandering dispersion dispersion drop puck drop puck drop puck drop puck pickup puck pickup puck pickup puck pickup puck homing dispersion dispersion safe wandering dispersion dispersion drop puck drop puck drop puck drop puck table controller foraging 
brevity conditions avoidance left 
sensed agent executes avoidance rules safe wandering 
surrounding herding foraging homing aggregation dispersion flocking safe wandering example applying direct temporal combinations basic behaviors generate various higher level behaviors 
case safe wandering generate flocking foraging 
similarly aggregation foraging surrounding 
manifested external state carrying puck induces safe wandering away object 
conversely encountering kin triggers flocking 
reaching home depositing object triggers dispersion multiple robots home safe wandering robot 
shows controller task 
foraging demonstrates basic behaviors temporally combined higher level compound behavior 
combination simple conflicts interacting agents potentially executing different behavior resolved uniformly due agent homogeneity 
agents share goal structure respond consistently environmental conditions 
example group agents home encounters agents difference agents external state induce agents kind avoiding agents type dividing specializing group 
foraging just example variety spatial object manipulation tasks implemented described architecture basic behaviors 
tasks include sorting objects building structures surveying mapping unknown territory 
illustrates basic behaviors case dispersion safe wandering direct combination flocking temporal combination foraging 
section demonstrates robot implementations compound behaviors 
flock sum outputs safe wander disperse aggregate home 
algorithm implementations compound behaviors flocking described earlier flocking form structured group movement serves minimize interference protect individuals enable efficient information exchange 
flocking implemented simple algorithm shown algorithm 
choice weights different behavior outputs determined dynamics mechanics agents ranges sensors agents turning radii velocity 
robot implementation flocking consisted combination safe wandering aggregation appropriate combination ffi avoid ffi aggregate thresholds 
flocking coordinated motion behavior best evaluated testing duration repeatability robustness 
expected performance flocking dependent size flock 
small flocks consisting fewer agents stable larger flocks remained stable agents failed due mechanical problems 
demonstrates just case agents position sensors failed quickly diverged flock 
utility flocking easily seen interference minimizing properties 
instance efficient individualistic homing number homing agents increases 
flocking involves compromise individual group goals may individual agent path locally suboptimal collective behavior efficient agents get destination faster average greedy homing strategies typical flocking behavior shown figures 
flocking tested challenging environments 
example barrier roughly size robots front flock flock moving 
expected definition stability chapter traffic laws human forms flocking 
impose structure collective motion minimize average interference 
frame time frame time frame time frame time flocking behavior robots 
robots separates affecting behavior 
due failure position sensors robot falls group rejoin 
rest robots reorganize maintain global structure 
frame time frame time frame time frame time flocking behavior robots trial 
robots maintain coherent flock spite large position errors sensed individuals 
errors manifested variability spacing robots flock moves 
frame time frame time frame time frame time flocking behavior robots 
robots initiated line quickly move flock 
fixed leaders robots front flock occasionally exchange places due velocity control variations maintaining flock formation 
frame time frame time frame time frame time run flocking robots 
robots started difficult initial configuration facing 
initial reordering establish flock maintain move workspace 
shown frame position sensors robot path appears discontinuous actual trajectory keeps flock 
flock split groups obstacle side 
empirical data experiments available video tape 
idea flocking generated simple rules popular researchers 
example goss deneubourg beckers show similar approach demonstrating simple rules result gull flock formation simulation 
directly reynolds presents elegant graphical simulation bird flocking 
robot implementation required rules due complex dynamics 
foraging foraging consists finding pucks environment picking delivering home region 
efficient implementation foraging serves validate proposed behavior combination strategy 
foraging tested different types robots environments performance repeatable robust 
shown implementation foraging attempt directly optimize amount time required collect pucks criterion indirectly minimized diminishing interference agents 
foraging tested validate basic behavior sequencing appropriate robust higher level task collecting pucks accomplished effectively 
figures demonstrate typical foraging performance showing snapshots different stages foraging process 
foraging runs terminated minutes time thirds pucks collected 
duration runs largely due inefficient search strategy robots remember pucks 
improved strategy robots remembered location pucks returned repeatedly pucks transported part group learning algorithm described chapter 
advantage exact puck location partially justified course experimental run pucks outside home region pushed gradually dispersed expanding area 
turn affected global behavior system dispersed pucks robots stumble random search 
puck dispersion side effect result dynamics interaction robots environment 
influence dynamics affected global behavior performance system 
relatively simple effect predicted standard analytical models frame time frame time frame time frame time foraging behavior robots 
robots initiated home region 
pucks initially clustered bottom center workspace 
safe wander search pucks pick take home 
encounter robot puck carrying follow shown third frame data 
time pucks accumulate home region 
frame time frame time frame time frame time ah foraging behavior robots 
experiment robots effectively manage transport larger number pucks home group robots shown 
boxes robots indicate executing avoidance safe wandering see robots frame data avoiding walls workspace 
frame time frame time frame time frame time example foraging behavior robots 
robots gather area pucks top workspace picking gathering home region 
interference resolved safe wandering 
fall granularity model precision level 
system foraging accomplished single agent task require cooperation 
goal collective solution accelerate convergence growing size group 
arkin 
describe simulation results similar task varying numbers agents inter agent communication 
complementary results find performance improves simple communication 
report improvement performance growing group size fixed point particular retrieval gathering task 
result agreement results shown illustrate interference effects larger higher density groups confined workspaces 
number pucks collected collective solutions proposed outperformed single agent group size grew importance behaviors minimized interference 
relationship elaborated chapter describes approach group learning 
docking parking section gives example combining behaviors temporal switching environmental constraints 
achieving arbitrary agent behaviors difficult minimal match dynamics agent environment human specified task 
describe docking group behavior programmed top difficult achieve simply generated advantage system dynamics interaction simple basic behaviors 
docking behavior parks robots kind boundary 
general getting collection robots park line difficult 
guaranteed solution geometric planning intractable uncertain dynamic environments multiple agents 
contrast tightly controlled top approach demonstrate bottom alternative 
docking algorithm takes advantage environmental constraints existence agents boundary walls see 
individual robot goal keep moving safely avoid collisions drops 
collective goal achieve state robots parked edge step detect downward pointing ir sensors 
algorithm consists behaviors ffl safe wandering keeps robot moving forward avoiding collisions ffl avoiding drops stops robot falling edge 
docking behavior progress constraints environment rules avoid go forward don fall edge 
behaviors combined parallel avoiding behaviors precedence wandering shown algorithm 
dock ground sensed 
robot near avoid 
clear go forward 
algorithm behaviors executed confined space vertical boundary produce tight docking behavior 
position control specific docking positions determined priori 
algorithm insensitive initial conditions number robots avoidance strategies 
tested algorithm trials groups robots 
cases quickly resulted robots lined edge shown figures 
explored simplest case docking environment con result docking behavior robots 
view docking robots 
straints eliminated robots position control 
similar simple behaviors combinations behavior systems 
instance matari uses rules achieve boundary sonar mobile robot 
steels implements docking charger rules approaches light source charger avoids obstacles 
combining behaviors different agents chapter discussed ways combining behaviors higher level composites control system single agent 
described direct temporal combination operators rely agents ability respond external conditions consistently 
long agents follow consistent social rules compatible social repertoires conflict agents minimized 
homogeneity simplifies task combining behaviors concern conflict behaviors reduced consistent social rules followed agents 
agents homogeneous terms high level goals immediate goals may differ point time locally heterogeneous 
arbitration encounter agents fact analogous behavior selection problem level control single agent 
consequently similar strategies apply multi agent case behaviors agents combined form agents take precedence rest 
previously argued matari unambiguous precedence hierarchy competing behaviors agents simplest way guarantee globally consistent result 
ensuring minimal higher order effects interference locally heterogeneous society accomplished strict hierarchy control 
type social organization appears quite stable ubiquitous animal human societies 
employs elaborate dominance structures requiring maintenance identities distinguishing characteristics histories previous encounters mcfarland gould demanding higher cognitive overhead agents experimented 
discussed section overhead may necessary certain types complex time extended interactions 
summary chapter addressed methods minimizing interference behaviors single agent behaviors interacting agents 
general architecture introduced combining finite set basic behaviors unbounded repertoire higher level behaviors direct temporal combinations 
types combination operators architecture demonstrated compound spatial behaviors flocking foraging docking implemented tested collection mobile robots 
chapter introduces methodology automating behavior combination process learning 
chapter learning situated systems far dealt problem synthesizing intelligent group behavior hand 
extend ideas include learning ability allows agent acquire new adapt old behaviors individual group benefit 
motivation learn 
learning purposes universal domains 
useful 
adapting external internal changes 
simplifying built knowledge ability cope changes environment termed adaptability 
allows agents deal noise internal external sensors inconsistencies behavior environment agents 
adaptability comes cognitive cost creatures adapted specific niche 
consequently creatures natural fail tasks certain conditions 
purpose learning set conditions smaller 
adaptability necessitate learning 
species genetically equipped elaborate knowledge abilities specific ability record utilize celestial maps waterman general plasticity learning motor control mcfarland language pinker 
genetic code finite 
fact primate human cortical neural topology complicated fully specify available genome established problem learning complex situated domains 
assertion traditional reinforcement learning reformulated 
approach replace states actions reinforcement conditions behaviors heterogeneous reward functions progress estimators 
validation implement learning group mobile robots learning forage 
table summary situated learning problem addressed structure proposed solution 
spontaneous synaptic firing decade life vander 
addition compensating genetic parsimony learning useful optimizing agent existing abilities necessary coping complex changeable worlds 
argued societies exist largely conservation propagation behavior strategies complex passed genetically 
answer built versus learned tradeoff varies species environments 
described addresses fundamental tradeoff domain situated multi agent systems 
rest thesis address problem collection situated agents learn group environment 
problem addressed nondeterministic noisy error prone domain stochastic dynamics agent priori model world 
propose formulation reinforcement learning uses level description state space manageable making learning possible 
furthermore methods shaping reinforcement take advantage information readily available agent learning efficient 
ideas validated demonstrating effective learning algorithm group robots learning forage 
table summarizes problem approach 
relevant learning models things agent learn ways learn 
learned existing approaches classified categories ffl learning declarative knowledge ffl learning control ffl learning new behaviors ffl learning select behaviors actions learning declarative knowledge learning declarative knowledge founding areas ai directly related thesis 
type declarative knowledge situated agents deal date maps environment 
robotics literature deals problem constructing updating maps variety situated domains see matari review literature 
maps world models closely tied action world primary type declarative knowledge far situated agents contrast thesis focuses procedural knowledge directly tied acting interacting world 
remaining learning categories directly tied action learning control learning control growing field adaptive control branch control theory 
problems adaptive control deal learning forward inverse model system plant 
forward models provide predictions output expected performing action state 
analogously inverse models provide action current state desired output jordan rumelhart 
learning control applied variety domains number different learning methodologies 
connectionist algorithms popular see miller sutton werbos representative collection note maps explicit declarative 
see matari examples 
author bias declarative learning divided interesting categories area pursued 
approaches studied atkeson atkeson schaal atkeson 
adaptive control problems typically deal learning complex dynamical systems non linearly coupled degrees freedom usually involved moving multi jointed manipulators objects physical bodies 
learning new behaviors learning new behaviors deals problem acquiring strategies achieving particular goals 
notion behavior defined behavior learning problem 
defined behavior control law particular goal wall collision avoidance 
definition general meant refer level description basic control specifying level varies domain 
furthermore concept behavior contains informal notions generality adaptivity difficult state precisely domain specific grounding 
consequently learning control problems appear instances behavior learning learning balance pole barto sutton anderson play moore juggle schaal atkeson 
furthermore action selection deciding action state viewed learning higher level behavior abstraction state action space 
example maze learning system said learn specific maze solving behavior 
genetic learning addressed learning behaviors simulated worlds koza 
learning behaviors requires finding appropriate parameter settings control cast optimization problem genetic algorithms particularly suited goldberg 
genetic algorithms operate encoding learning problem encoding requires model agent environment order generate useful behaviors 
problem modeling situated worlds notoriously difficult genetic algorithms produced behaviors successfully transferred physical systems steels cliff husbands harvey gallagher beer 
learning approaches said learn new behaviors precise definition problem 
posed behavior learning problem brooks matari requires agent acquire new behavior perceptual effector systems assign semantic label behavior order recognize coherent independent unit 
behavior learning appears require bridging elusive signal symbol gap limited notion symbol 
definition existing performs behavior learning 
learning control learning action selection strictly instances behavior learning cases definition single behavior learned abstraction performed 
similarly genetic algorithms address stated behavior learning problem domain semantics provided designer 
signal symbol problem hallmark challenges ai 
bridges gap communities received attention 
challenge problem setting avoid biasing learner inappropriately able evaluate performance 
behaviors concepts symbolic representations automatically generated situated agent map neatly agent human observer semantic space 
situated domain particularly suited type allows grounding agents learning physical behavior observable evaluated externally mechanism representation 
learning select behaviors learning new behaviors learning learning select behaviors learning 
behavior selection extensively studied far largely due lack formalization behavior building block control 
done topic reinforcement learning techniques maes brooks maes 
learning behavior selection definition reinforcement learning problem correlating behaviors agent performs feedback receives result 
reinforcement learning reinforcement learning rl class learning methodologies agent learns external feedback received environment 
feedback interpreted positive negative scalar reinforcement 
goal learning system maximize positive reinforcement reward minimize negative reinforcement punishment time 
traditionally learner explicit built knowledge task 
learner receives direct instruction answers environment learning considered unsupervised barto 
learner produces mapping states actions called policy 
reinforcement learning originated ivan pavlov classical conditioning experiments gleitman 
embraced stimulus response learning predominant methodology studying animal behavior psychology biology 
ethology study animals natural habitats developed response tightly controlled laboratory experimental conditions commonly 
mean time rl adopted adapted computational community applied various machine learning problems 
maze learning formulated reinforcement learning problem reward punishment known application rl minsky 
soon problem learning scoring functions playing checkers successfully addressed rl algorithm samuel 
subsequently rl applied variety domains problems notably bucket brigade algorithm classifier systems holland class learning methods temporal differencing sutton 
reinforcement learning implemented variety algorithms ranging table lookup neural networks broad spectrum applications including tuning parameters playing backgammon 
concerned reinforcement learning situated embodied agents 
particular focused issues arise traditional models rl algorithms applied models complex multi agent domain working 
address issues describing commonly exclusively rl model 
markov decision process models computational models reinforcement learning assumption agent environment interaction modeled markov decision process mdp defined 
agent environment modeled synchronized finite state automata 

agent environment interact discrete time intervals 

agent sense state environment actions 

agent acts environment transition new state 

agent receives reward performing action 
interesting learning domains modeled mdps situated agents learning nondeterministic uncertain environments fit model 
section describes reasons addressing model assumptions turn 
state rl models assumption agent environment clearly defined state agent sense 
situated domains world readily appropriate states world state readily consistently accessible agent 
world continuous partially observable 
continuity state situated agent consists collection properties discrete inputs binary sensors continuous velocities wheels 
simplest agents monolithic descriptor state properties prohibitively large 
scales poorly increased sensory capabilities agent complexity general results combinatorial explosion standard rl 
models date bypassed continuous state presuming higher level sensory operators see chair front operators shown unrealistic largely systems physical sensors agre chapman brooks matari 
general problem partitioning continuous state discrete states hard reasonable partitioning world may mapping space sensor readings partitioning 
observability continuous complex sensors limited abilities 
providing descriptions world return simple properties presence distance objects fixed sensing region 
consequently distinguish potentially relevant world states 
collapse multiple states results partial observability perceptual aliasing mapping world internal states 
inability distinguish different states difficult impossible learning algorithm assign appropriate utility actions associated states whitehead ballard 
partially observable markov decision processes pomdps developed operation research community dealing problem 
partial observability added markov model introducing discrete probability distribution set possible observations state 
pomdps studied successfully applied theoretical learners cassandra kaelbling littman empirically largely due fact observability models situated systems generally available 
generalization learner caught paradox disambiguate relevant inputs discard irrelevant inputs order minimize search space 
may structured learner space traditional rl exponential size input curse dimensionality bellman 
form input generalization collapsing states functional equivalence classes necessary problems 
human programmers perform generalization implicitly clever orderings rules careful arbitration default conditions crafting control strategies 
minimize ambiguity maximize parsimony advantage domain knowledge 
rl absence domain knowledge state generalization addressed statistical clustering methods recursive partitioning state space individual bit relevance chapman kaelbling mahadevan connell moore moore 
confronted classifier systems binary strings state descriptors holland 
state contain wild cards allow clustering states flexible grouping potential full generality full specificity 
generalization results called default hierarchies relevance individual bits changed specific values 
process analogous statistical rl methods matari 
input generalization problem addressed connectionist rl literature 
multi layer networks trained variety learning problems hidden layers constructed generalized intermediate representation inputs hinton 
rl generalization techniques non semantic table methods classifier system approaches somewhat readable results direct consequence explicit hand coded criteria 
connectionist approaches contrast utilize potentially complex network dynamics produce effective largely generalizations 
described generalization techniques effective require large numbers trials obtain sufficient statistical information clustering states 
incremental improvement overwhelmingly slow exponential learning algorithms 
explore different alternative takes principled advantage domain knowledge purely statistical generalization 
paradoxically unwieldy fully exponential state action search space standard rl models gives main positive properties asymptotic completeness 
hand coded reactive policies take advantage cleverness designer rarely provably complete 
irrelevant input states easily eliminated potentially useful ones overlooked 
hand complete state spaces guarantee sufficient time sufficiently rich reinforcement agent produce provably complete policy 
unfortunately quality little time bounded situated domains 
state transitions simple mdp models employ discrete synchronized state transitions 
contrast situated domains world state agent state change asynchronously response various events 
dynamic domains subset events directly caused agent actions agent control 
general events take different amounts time execute delayed effects different consequences identical conditions 
short situated domains difficult model properly 
deterministic models capture dynamics situated domains nondeterministic alternatives considered lin 
unfortunately unrealistic models sensor effector uncertainty overly simplified error properties 
typically adding gaussian noise sensed state commanded action 
uncertainty situated domains follow gaussian distributions results structured dynamics interaction system environment 
dynamics play important role behavior system generally description level low accurately modeled simulated 
example consider properties realistic proximity distance sensors 
accuracy ultrasound sensors largely dependent incident angle sonar beam surface surface materials difficult tedious model accurately 
infra red vision sensors similarly detailed entirely different properties accurately represented simple models 
simple noise models tempting produce artificial dynamics potentially complex model true complexity number state bits 
realistic physical systems 
consequently elegant results simple simulations successfully repeated complex agents environments 
challenges realistic modeling generally difficult obtain transition probabilities nondeterministic models situated domains 
models domains readily available obtained empirically system process analogous learning world model 
difficult estimate obtaining world model domain requires time learning policy set goals 
consequently insightful learning world models intelligent exploration sutton kaelbling applicable complex situated domains 
argued accurate models situated domains difficult obtain learn 
focus learning policies systems explicit world models 
section describes general form rl algorithms policy learning 
algorithms reinforcement learning algorithms general form kaelbling 
initialize learner internal state 
forever observe current world state choose action evaluation function execute action immediate reward executing world state update internal state update function internal state encodes information learning algorithm saves world usually form table maintaining state action data 
update function adjusts current state received reinforcement maps current internal state input action reinforcement new internal state 
evaluation function maps internal state input action information stored internal state 
different rl algorithms vary definitions predominant methodology rl class temporal differencing td techniques sutton 
td methods deal assigning credit blame past actions attempting predict long term consequences action state 
sutton original formalization temporal differencing td deals predictions markovian environments covers large class learning approaches 
example bucket brigade delayed reinforcement learning method classifier systems instance td matari 
learning watkins commonly known td algorithm defined explained appendix background subsequent comparison 
learning trials performance properties various forms td applied markovian environments extensively studied watkins dayan barto bradtke singh jaakkola jordan 
provable convergence td related learning strategies dynamic programming asymptotic requires infinite trials watkins 
generating complete policy incorrect requires time exponential size state space optimality policy converges limit number trials approaches infinity 
practice translates hundreds thousands trials bit states 
ideal markovian worlds number trials required learning prohibitive smallest state spaces 
situated learning problem difficult 
assuming appropriately minimized state space learner may fail converge due insufficient reinforcement 
reinforcement temporal credit assignment assigning delayed reward punishment considered difficult important problems reinforcement learning temporal credit assigned propagating reward back appropriate previous state action pairs 
temporal differencing methods predicting expected value rewards state action pair assigning credit locally difference successive predictions sutton 
reward functions determine credit assigned 
design functions discussed difficult aspect setting statement problem due samuel checkers learning program learned reward moves eventually lead triple jump 
reinforcement learning algorithm 
delayed reward trials learning algorithm requires longer takes converge 
algorithms immediate reinforcement naturally learn fastest 
reinforcement learning date types reward immediate delayed 
postulate situated domains tend fall popular extremes providing immediate rewards plenty intermittent ones delayed ones 
delayed reinforcement particularly impulse reinforcement delivered single goal eliminates possibility biasing learning usually prohibitively difficult 
situated learning problems resemble mazes reward 
estimates progress available way 
estimate intermittent internally biased inconsistent occasionally incorrect appropriately significantly speed learning 
approach chapter takes advantage intermediate estimates shape reinforcement accelerate learning 
multiple goals argued impulse reinforcement related single goal learning prohibitively slow 
furthermore single goal agents rare situated domains 
situated agents best viewed having multiple goals maintained concurrently achieved sequentially 
example previously described foraging task agent maintains continuous low level goal collision avoidance keeps minimal distance agent order minimize interference may attempt remain flock may heading home puck 
rl models require learning problem phrased search single goal optimal policy specified global reward function 
surprisingly world goal changes new policy learned new reward function 
existing policy conflict new learning need forgotten 
order enable learning multi goal policy goals formulated subgoals higher level single optimal policy 
sequential consistent 
enforce specific goal sequence state space explicitly encode goals reached point time requiring added bits input state vector singh 
natural extension rl framework method requires state space grow added goal address concurrent goals 
sequences goals fail capture dynamics complex situated worlds agents may high level goals achievement number maintenance goals interaction important effects agents behavior rate learning 
general solution multiple goals traditional framework separate state spaces reinforcement functions goals merge whitehead karlsson tenenberg 
merging policies assumes necessary information utility evaluation available agent 
previously discussed relation game theoretic approaches see section assumption may hold situated domains 
related computational rl active particularly lively decade 
majority contributions theoretical nature 
thorough reviews reinforcement learning applied behaved learning problems see watkins sutton 
improved learning algorithms situated agents largely applied simulated domains see kaelbling whitehead 
section focus empirical learning situated agents 
whitehead ballard whitehead addressed perceptual aliasing problem situated rl 
proposed approach adaptive active perception action divided control problem stages state identification stage control stage applied appropriate learning methods 
approach demonstrated simulated block stacking task tested embodied domain 
kaelbling simple mobile robot validate rl algorithms immediate delayed reinforcement applied learning obstacle avoidance 
maes brooks applied statistical reinforcement learning technique immediate reward punishment order learn behavior selection walking legged robot 
approach appropriate appropriately reduced size learning space available immediate accurate reinforcement derived contact sensor robot wheel estimating walking progress 
delayed reinforcement mahadevan connell box pushing task implemented mobile robot subgoals introduced provide immediate reward 
mahadevan connell experimented learning monolithic partitioned goal functions learning box pushing subgoals necessary 
chapman kaelbling mahadevan connell demonstrated complementary approaches generalization 
chapman kaelbling started single general state iteratively split statistics accumulated time 
splitting relevance state bit relevant state space split bit 
contrast mahadevan connell started fully differentiated specific set states consolidated similarity statistics accumulated time 
aside traditional unsupervised reinforcement learning methods described techniques explored 
pomerleau supervised connectionist learning approach train steering control autonomous vehicle generalizing visual snapshots road ahead 
thrun mitchell demonstrated connectionist approach learning visual features camera mounted mobile robot 
features assigned designer selected network intermediate representations 
surprisingly result semantically meaningful human observer suited robot navigation task 
best author knowledge attempt applying reinforcement learning collection physical robots learning complex task consisting multiple goals 
parker implemented non rl memory style parameter learning adjusting activation thresholds perform task allocation multi robot system 
tan applied traditional rl simulated multi agent domain 
due simplicity simulated environment relied mdp model applicable domain 
furthermore tan simulation uses communication agents relies assumption agents correctly exchange learned information 
hold true physical systems noise uncertainty properties extend communication channels 
summary chapter overviewed key properties reinforcement learning strategies markov decision process models implications learning situated domains 
learning algorithms dynamic programming traditionally applied markovian domains discussed 
related robot learning reinforcement learning reviewed 
main problems arise standard mdp formulation applied multi agent domain state space prohibitively large delayed rein insufficient learning foraging task 
chapter introduces method reformulating learning problem order learning possible efficient complex domain 
chapter learning approach chapter describes formulation proposed reinforcement learning problem order learning possible efficient complex situated domain hand situated domains general 
order deal complexity uncertainty situated domains learning algorithm appropriate level description 
learner low level description result state space large learning prohibitively slow 
contrast learner level description discover novel potentially useful strategies outside structured space allowed coarse representation 
appropriate representation shapes state space expressive tractable learning space 
effective learning algorithm searches learning space efficiently 
complexities situated agents environments reinforcement learning algorithms approach situated learning properties 
model situated learning 
minimize learner state space 
maximize learning trial chapter address desired properties turn 
approach described minimizing state space order learning problem tractable 
second approach shaping reinforcement proposed learning efficient 
cases traditional primitives reinforcement learning states actions reinforcement reformulated gamma 
subtly different pragmatically effective counterparts follows 
states actions gamma 
conditions behaviors 
reinforcement gamma 
multi modal feedback reformulating problem traditional state action models rl approaches tend level description inappropriate complex situated domains 
representations away important control details search excessively large state spaces representing agent entire world 
large state space sign difficult problem poorly formulated 
propose reformulation uses appropriate representation problem learning noisy inconsistent worlds reinforcement learning situated domains formulated learning conditions necessary sufficient activating behaviors repertoire agent behavior time maximizes received reward 
formulation accomplishes desired goal diminishing learning space conditions behaviors states actions effect elevating level description learning problem 
behaviors part thesis argued behaviors intuitive effective level description control described methodology selecting combining basic behaviors domain set goals 
behaviors defined goal driven control laws hide details control 
reasons behaviors useful abstraction control appropriate efficient basis learning 
behaviors general actions tied specific detailed states triggered set general conditions 
instance wall behavior applies environment wall agent sense dependent agent exact state including information position carrying puck front 
said rl literature uses behaviors labeling 
example action called left transports agent square grid turns degrees requires complex control sequence 
control law guarantees output agent position orientation identical effect definition behavior 
behavior may realistic continuous noisy domains 
general atomic actions simulated grid worlds translate arbitrarily complex behaviors embodied systems 
consequently situated embodied agents different set behavior primitives specifically designed particular dynamics agent interaction world 
behaviors elevate control higher realizable level 
complexity reinforcement learning lies size learning space traditionally exponential state space agent 
order significantly accelerate learning minimize space 
propose abstracting learning space higher level structured granularity conditions necessary executing behaviors 
behaviors abstracts away details low level controller realizable units control guaranteeing results postconditions behavior 
similarly conditions away low level details agent state space define learning space higher level state clustering 
conditions conditions predicates sensor readings map proper subset state space 
condition defined part state necessary sufficient activating particular behavior 
instance necessary sufficient conditions picking puck puck fingers robot 
space conditions usually smaller complete state space agent resulting smaller space learning algorithm 
furthermore fewer state elements need sensed system suffer error uncertainty 
events relevant agent change truth value predicates current condition 
events trigger terminate behaviors 
reformulating states actions conditions behaviors effectively reduces state space manageable size making learning possible complex domain 
step learning efficient appropriate reinforcement 
reinforcement accelerated learning amount quality reinforcement determines quickly agent learn 
nondeterministic uncertain worlds learning bounded time requires shaping reinforcement order take advantage information available agent 
general reinforcement learning accelerated ways building information providing reinforcement 
reward function implicitly encodes domain knowledge biases agent learn 
simplifying minimizing reinforcement practiced early rl algorithms sutton diminish bias greatly situated domains completely learner 
domain knowledge embedded reward rich complex reinforcement function 
approach effective process embedding semantics world reward function usually ad hoc 
ideal case reinforcement immediate meaningful 
immediate error signals provide sign magnitude error result fastest learning 
supervised learning provide agent correct answer trial 
learning control jordan rumelhart atkeson schaal atkeson error signals critical learning problem usually finding complex mapping collection input parameters desired output 
immediate reinforcement rl typically weak version error signal reduced sign error magnitude direction 
propose intermediate solution shaping version error signal principled embedding domain knowledge 
heterogeneous reward functions monolithic reward functions single high level goal applied situated domains require large amount intermediate reinforcement order aid agent learning 
intuitively subgoals frequently reinforcement applied faster learner converge 
argued situated agents maintain multiple concurrent goals goals achieved maintained behaviors basic unit control learning 
task situated domain represented collection concurrent goal achieving behaviors 
reaching goals generates event provides primary reinforcement learner 
change conditions 
general form event driven reinforcement functions event occurs event driven reinforcement event function conditions time received reinforcement may positive negative 
necessary information task appropriate sensors available goals broken subgoals associated secondary reinforcement 
general specification high level behavior provides collection subgoals need achieved maintained 
achievement subgoal detected directly translated reinforcement function 
general heterogeneous reward function form event occurs event occurs en event en occurs complete reward function sum inputs individual event driven functions 
multiple events occur simultaneously appropriate reinforcement received multiple sources 
driven reinforcement functions illustrated example ffl robot receives reward avoids obstacle reward reaches home 
ffl corresponding reward function appears follows obstacle avoided home reached ffl robot happens avoiding obstacle reaches home time receives reinforcement sources concurrently example illustrates heterogeneous reward functions provides part structure learning task speeds learning 
event driven reward functions associate reinforcement achievement goals subgoals application associated behaviors 
deliver reward punishment response events behaviors 
section describes shaping mechanism providing reinforcement execution behavior 
progress estimators goals immediately available measures progress tasks need defined long sequences behaviors feedback 
progress estimators domain knowledge measure progress behavior necessary trigger principled behavior termination 
feedback learning signal received goals 
consider example ffl robot task learn take pucks home 
ffl having puck robot wait accidentally finds home receives reward 
ffl alternatively related subgoal getting away food puck pile feedback 
ffl scheme longer robot puck stays near food negative reinforcement receives 
ffl strategy encourage behaviors take robot away food homing 
immediate reinforcement available domains intermittent reinforcement provided estimating agent progress relative current goal weighting reward accordingly 
measures progress relative particular goal estimated standard sensors furthermore feedback available different sensory modalities 
general forms progress estimator functions 
progress progress ae progress regress ae set conditions set conditions associated progress estimator conditions progress estimator active 
different dynamics 
valued function monitors presence absence progress 
valued function monitors presence absence progress negative progress regress 
progress estimators diminish brittleness learning algorithm ways ffl decrease sensitivity noise ffl encourage exploration behavior space ffl decrease fortuitous rewards described turn 
decreasing sensitivity noise progress estimators provide implicit domain knowledge learner 
strengthen appropriate condition behavior correlations serve filters spurious noise 
noise induced events consistently supported progress estimator credit impact learner 
consider example ffl agent executing behavior condition receives positive reinforcement progress estimator ffl receives negative reinforcement result event induced sensor error 
ffl impact negative reinforcement diminished continuous reinforcement received execution domain knowledge progress estimators provides continuous source reinforcement counter intermittent potentially incorrect credit 
encouraging exploration exploration versus exploitation critical tradeoffs machine learning 
agent exploration discover new potentially efficient condition behavior combinations optimize performance best known pairings 
ineffective exploration results thrashing repeatedly attempting inappropriate behaviors 
situated environments event driven behavior may persist potentially long period time 
agent impetus terminating behavior attempting alternatives behavior may eventually produce reward 
learning algorithm principled strategy terminating behaviors order explore condition behavior space effectively 
progress estimators provide method behavior fails progress relative current goal terminated tried 
domain knowledge judge progress progress estimators induce exploration terminating behaviors common sense arbitrary internal clock ad hoc heuristic 
decreasing fortuitous rewards fortuitous reward received inappropriate behavior happened achieve desired goal particular situation effect general 
consider scenario ffl agent puck attempting various behaviors 
ffl executing avoidance safe gamma wandering enters home region 
ffl progress estimator receive reward reaching home positively associate avoiding behavior goal getting home 
require repeated trials order discover implicitly correlation direction moving safe gamma wandering 
ffl suppose progress estimator added learning algorithm 
generates reward agent decreases distance home 
fails time interval behavior terminated 
ffl receive fortuitous rewards impact smaller compared consistent progress estimator 
continuous reward approaching home discounting effect fortuitous rewards agent receives 
bias agent behaviors decrease distance home 
general way eliminate fortuitous rewards know relevance context priori 
progress estimators achieve effect incrementally behaviors measurable duration allows progress estimators contribute reinforcement 
summary chapter introduced formulation reinforcement learning conditions behaviors shaped reinforcement order learning possible learning efficient complex situated domains 
described formulation direct extension behavior control matari brooks brooks 
heterogeneous reward functions related subgoals mahadevan connell subtasks whitehead 
previous focused learning action sequences higher level description 
proposed subgoals directly tied behaviors basis control learning 
similarly progress estimators mapped behaviors expedite learning associated goals single complete external critic monolithic reinforcement function whitehead 
elevating description control learning level system perceptual conditions behaviors perceptual states atomic actions greatly diminishes agent learning space learning tractable 
heterogeneous reward functions progress estimators builds domain knowledge contextual information making learning efficient 
proposed reformulation forms better foundation situated learning impose constraints kind learning algorithm applied 
completely general compatible reinforcement learning approaches 
chapter demonstrates formulation applied task learning foraging situated multi robot domain 
chapter learning experiments chapter describes learning experiments conducted test approach setting learning space enable learning shaping reinforcement accelerate learning situated domains 
robots learning experiments performed group fully autonomous mobile robots board power sensing 
robot consists differentially steerable wheeled base gripper grasping lifting objects 
robots sensory capabilities include piezo electric bump sensors detecting contact collisions monitoring grasping force gripper set infra red ir sensors obstacle avoidance grasping 
robots equipped radio transceivers determining absolute position inter robot communication 
position information obtained triangulating distance computed synchronized ultrasound pulses fixed beacons 
inter robot communication consists broadcasting byte messages rate hz 
experiments described radios determine presence nearby robots 
set robot experiments robots programmed behavior language brooks 
learning task learning task consists finding mapping conditions behaviors effective policy group foraging 
individually robot learns select robots learning experiments 
robots demonstrated learning forage selecting basic behavior repertoire appropriate sensory conditions 
learning robots consists differentially steerable wheeled base gripper grasping lifting objects 
robot sensory capabilities include piezo electric bump gripper sensors infra red sensors collision avoidance radio transmitter absolute positioning 
ir ir bump bump bump bump bump bump bump pressure radio robot sensory capabilities include piezo electric bump gripper sensors detect collisions grasp pucks infra red sensors collision avoidance radio transmitter absolute positioning message passing 
best behavior condition order find take home pucks 
foraging chosen complex biologically inspired task previous group behavior described earlier sections matari matari provided basic behavior repertoire learn behavior selection 
described section foraging achieved small basic behavior set 
set robots priori consisted fixed behavior repertoire ffl safe wandering ffl dispersion ffl resting ffl homing resting introduced expand agents behavior space introduce internal clock trigger internally generated events 
internal clock imposed cyclic circadian schedule consisting periods day time shorter periods night time 
resting part regular recharging cycle chance robots aggregate exchange information utility behaviors grasping dropping objects included robots capabilities conditions learned included basis set learning space 
options shown generation robots 
behavior repertoire robots task learning appropriate conditions triggering behaviors 
considering space conditions necessary sufficient triggering behavior set state space reduced power set clustered condition predicates ffl puck 
ffl home 
ffl near intruder 
ffl night time 
conditions grasping dropping built 
soon robot detects puck fingers grasps 
similarly soon robot reaches home region drops puck carrying 
robot near obstacle avoids 
reflexive behaviors deemed instinctive learning high cost 
learning avoid potentially prohibitive damaging cost robot natural learning task appears innate nature easily programmed systems 
puck manipulation requires fast accurate response gripper motors basic behaviors best suited parameter learning 
remaining behaviors dispersion safe wandering homing resting formed appropriate basis learning general executable variety situations finding appropriate subset situations conditions activation interesting learning problem useful application control 
described foraging task may appear quite simple learning space appropriately minimized include clustered conditions basic behaviors 
theory agent able quickly explore learn optimal policy 
practice quick uniform exploration possible 
relatively small learning space presents challenge agent situated nondeterministic noisy uncertain world 
soon demonstrate reformulated version problem poses challenge traditional rl methodologies delayed reward justifies proposed shaped reinforcement strategy 
improved reinforcement necessary partially domain learner provided model world 
discussed earlier model difficult obtain 
agent faced implicitly deducing structure dynamic environment includes agents behavior occasionally facilitates largely interferes individual learning process see figures scaled top view experimental area learning experiments conducted 
workspace small result frequent interaction interference robots 
home region shaded 

shown scenario poses difficult challenge reinforcement learning paradigm 
section describes solution 
learning algorithm learning algorithm produces maintains total order appropriateness behaviors associated condition expressed matrix 
value condition behavior pair sum reinforcement received point values matrix fluctuate time received reinforcement 
updated asynchronously received learning signal 
events produce immediate positive reinforcement ffl grasped puck ffl gd dropped puck home ffl gw home waking refers event internal clock indicating night time day time 
camera view experimental environment learning 
boundary home region indicated row pucks purposes photo 
pile pucks marked 
events result immediate negative reinforcement ffl bd dropped puck away home ffl bw away home events combined heterogeneous reinforcement function re ep occurs gd egd occurs bd occurs gw occurs bw occurs gd gw bd bw progress estimating functions associated minimizing interference triggered agent close agent 
behavior executed effect increasing physical distance agent agent receives positive reinforcement 
conversely lack progress away agent punished fixed time period progress current behavior terminated 
formally intruder avoidance progress function distance intruder increased near intruder progress estimator associated homing initiated puck grasped 
distance home decreased active agent receives positive reinforcement status quo delivers reinforcement movement away home punished 
formally homing progress function rh nearer home farther home puck simplest learning algorithm uses reinforcement functions implemented tested 
algorithm simply sums reinforcement time influence different types feedback weighted values feedback constants 
equivalent alternative weighting contributions sum follows ure vr binary valued real valued re rh functions tested 
results showed different weights reinforcement functions result faster stable learning 
surprising subgoals foraging task independent learning speed uncorrelated 
control algorithm complete control algorithm learning foraging 
behavior selection induced events change condition predicates 
events triggered 
externally robot gets way 
external events include gd bd 
internally internal clock indicates night time 
internal events include gw bw 
progress estimators interference estimator detects lack progress terminates current behavior 
estimator events triggered intruder gamma threshold rh homing gamma threshold 
event detected control sequence executed 
appropriate reinforcement delivered current condition behavior pair 
current behavior terminated 
behavior selected rule choose untried behavior available choose best behavior 
choosing untried behaviors encourages exploration 
policy total ordering condition behavior pairs agent explore entire behavior space said converged 
small size behavior set strategy accelerating effect establishing initial ordering behaviors condition 
best behavior condition defined highest associated value 
number behaviors small selection easy compute 
positive negative reinforcement progress estimator induced exploration strategy learning algorithm tend fall local maxima 
consequently need add randomness selection mechanism 
typical initial conditions learning trials 
robots initiated home region random positions workspace 
typical environment state course learning experiment 
learning independently robots acquired different parts policy 
interactions objects world accumulate learning trials order complete learning 
typical environment state learning 
pucks collected brought home region 
robots learned go get pucks competing remaining moved 
learning continuous incremental lifetime agent ensuring agent remains responsive changes environment pucks left particular location internal changes function dying battery slows motion 
described learning task optimal policy derived hand empirical data foraging experiments described section addition new resting behavior 
policy shown 
performance desired policy tested independently compared alternative solutions order establish superiority relative imposed evaluation criteria 
snapshots learning experiment shown illustrate progression typical experiment 
shows typical initial conditions demonstrates stage course learning shows environment experiment pucks collected 
illustrates resting behavior 
learning process consists adjusting values table total entries conditions behaviors 
table shows table policy agents initialized 
utility behaviors conditions equal initialized average minimum maximum value 
example resting recharging behavior robots triggered internal clocks 
case robots learned go home rest photo illustrates late stage learning demonstrated small number remaining pucks 
condition behavior near intruder 
puck 
home 
night time 
safe wandering homing safe wandering resting homing homing safe wandering resting safe wandering safe wandering dispersion resting homing homing safe wandering resting table optimal foraging policy 
top ranked behavior shown condition 
full table total numerical ordering behaviors condition total entries 
condition behavior safe wandering homing dispersion resting table policy agents initiated 
utility behaviors conditions equal initialized average minimum maximum 
experimental results evaluation effectiveness proposed reinforcement functions evaluated testing different types reinforcement 
approaches compared 
monolithic single goal puck delivery home region reward function re gd learning algorithm 
heterogeneous reward function multiple goals re reinforcement summation algorithm 
heterogeneous reward function multiple goals re progress estimator functions rh reinforcement summation algorithm 
data strategies collected averaged 
experiments run different robots significant robot specific differences 
data runs persistent sensor failures occurred discarded 
performance reinforcement strategies learning forage 
axis shows reinforcement strategies 
axis maps percent correct policy agents learned minutes averaged trials 
error bars show best worst performance histograms average value 
data values collected twice minute learning experiment completion experiment showing final values 
experiments lasted minutes 
minute threshold empirically derived majority learning trials reached steady state minutes small number rare conditions discussed 
evaluation evaluating performance situated systems notoriously difficult reasons standard metrics evaluating learning mechanisms absolute time convergence directly apply 
amount time required robot discover correct policy depends frequency external events trigger different states learning space 
additionally noise error certain parts policy fluctuate waiting specific point absolute convergence feasible 
convergence defined relative ordering condition behavior pairs 
performance approaches compared 
axis shows reinforcement strategies 
axis maps percent correct policy agents learned minutes averaged trials ratio correct condition behavior pairings optimal policy 
error bars show best worst performance histograms averaged value 
learning performance described learning tested reduced learning space enumerated conditions behaviors 
terms reinforcement learning simplified version second algorithm impulse function delivering positive reinforcement single goal dropping puck home region 
nondeterminism world uncertainty sensing state transitions single goal provides insufficient feedback learning aspects foraging particular rely accurate delayed credit assignment 
performance learning vulnerable interference robots declined rapidly approaches tested increased group sizes 
performs poorly partial policy discovers consistent trials condition behavior pairs receive immediate reliable reinforcement 
performance indicates difficulty learning task extent demonstrating immediately reinforced parts parts capable learning 
important note unable take advantage reward discounting particularly useful ordering sequence behaviors agent executes time domain agent behavior dependent behavior interact time 
interactions individually modeled learned order avoid prohibitively large learning space high sensing overhead 
consequently agent deduce structure sequential behaviors discounting representational level structure 
needs acquire fully reactive policy benefit temporal discounting 
multiple goal performance second learning strategy utilizing reinforcement multiple goals outperforms detects achievement subgoals way top level goal depositing pucks home 
suffers credit assignment problem cases delayed reinforcement nondeterministic environment agents guarantee consistency rewards time 
furthermore strategy prevent thrashing certain behaviors active unnecessarily long 
example safe wandering grasping pursued persistently expense behaviors delayed reinforcement homing 
performance heterogeneous reinforcement gives evaluation difficulty proposed learning task 
correct policy learned average demonstrates additional structure necessary aid learner acquiring rest 
structure provided progress estimators 
progress estimator performance complete heterogeneous reinforcement progress estimator approach maximizes potentially available information condition behavior 
predicted thrashing eliminated case learning conditions dispersion homing progress estimator functions encourage exploration 
furthermore fortuitous rewards impact alternative algorithms 
implicit domain knowledge effectively spread reinforcement order guide learning process continually maximizing utility learning trials consequently speeding learning 
design foraging task basic behaviors guarantees subgoals independent 
consequently associated reinforcement functions directly affect simple ones mutually consistent contribute common high level goal 
theory reinforcement faster learning practice noise error different reinforcement sources opposite effect 
experiments demonstrated significant amount noise inconsistency different reinforcers progress estimators adversely affect learner 
example robot estimate position proximity frequently inaccurate due radio transmission delays 
errors resulted faulty homing interference progress estimates 
condition behavior pairs involved carrying puck converged quickly 
furthermore values tend oscillate 
fast rate convergence associations behaviors involved dispersion homing result directly effects progress estimators 
removed second tested algorithm performance declines accordingly 
conversely set conditions associated finding pucks uniformly took longer learn direct progress measure accelerate learning 
furthermore learned values initially tended oscillate differences behavior alternatives great due lack intermediate rewards 
empirical results show noise error induced inconsistencies progress estimators significantly diminish benefit reinforcement effect re gd converges policy re converges policy re rh converges policy table qualitative summary performance types reinforcement foraging task 
domain 
evaluation table shows coarse performance ordering approaches 
intuitive ordering particularly informative 
better way analyze approaches evaluate part policy separately measuring robot learning 
table illustrates final state learner heterogeneous reward functions progress estimators 
table provides additional information analysis 
capture dynamics learning process condition behavior pair evaluated criteria 
number trials required 
correctness 
stability 
number trials measured relative stable solution solution optimal 
second criterion sought incorrect terms optimality stable solutions 
third criterion focused unstable policies looking behavior orderings tended fluctuate 
criteria condition behavior pairs proved difficult learn 
prominent source difficulty delay reinforcement predictable results clearly demonstrated performance differences strategies 
learning conditions safe wandering difficult available progress estimator robot executing correct behavior long reaching pucks receiving reward 
mean time repeatedly interrupted activities avoiding obstacles intruders homing resting onset night time 
condition behavior safe wandering homing dispersion resting table example policy learned robots heterogeneous reward functions progress estimators 
source difficulty occurrence combinations conditions 
particular condition consisting onset night time robot carrying puck avoiding robot rarely occurred 
consequently correct mapping difficult learn robots get chance explore behavior alternatives 
accounts incomplete policy case successful reinforcement strategy 
combination positive negative reinforcement pushes learner local maxima allows oscillations instabilities ordering values table 
conditions alternatives resulted equally effective solutions 
situations robot carrying puck encounters intruder motion away intruder beneficial rewarded progress estimator consequently homing safe wandering effective dispersion 
contrast robot carrying puck dispersion homing effective rewarded contributions rh progress estimators 
described earlier combination estimators speeds exploration minimizes fortuitous rewards 
specific progress measure minimizes travel time goal eliminate effect 
optimization difficult systems largely local sensing control dealing interference agents 
challenges policy robots appropriate properties domain 
scaling evaluated reinforcement alternatives groups robots interference detriment 
general robots learning time longer took converge 
particularly pronounced condition behavior pairs directly associated progress estimators involved conditions involve carrying puck 
behavior capable benefits interference dispersion learned faster accurately crowded situations 
considered adding social behavior called yielding order minimize interference having robot move time crowded situations 
previous results described section showed hierarchical behavior little effect individual basic behaviors aggregation dispersion 
believe yielding effective case foraging 
fixed home puck locations task structured take advantage rules produce structured motion 
discussion extensions social rules noted decline performance algorithms observed increased group size associated increased interference agents 
surprising undesirable effect 
ideal scenario presence agents speed slow individual learning 
synergy possible societies individuals benefit experience interact mutually beneficial social rules 
addressed problem learning social rules 
challenging learning problem social rules necessarily immediate delayed payoff individual may benefit individual average having global effect 
consequently social rules involve altruistic behavior simplest levels yielding traffic 
behavior difficult learn reinforcement learning strategies 
currently working algorithm utilizes observation neighboring agents behavior received reinforcement order acquire practice social behaviors matari 
transition models learning problem involving collection concurrently learning agents noisy uncertain environment purposefully chosen complexity 
fact state transition model available aid learner major challenges 
argued earlier models generally available partial models constructed empirically prior learning process 
implemented reinforcement functions take advantage immediate information world generate reinforcement 
accelerating effect learning domain regardless transition model available 
interesting extension apply described reinforcement approach problems involve incomplete approximate state transition models order study effects combining immediate reinforcement discounted rewards commonly applied rl problems 
heterogeneous learning key advantages heterogeneous reinforcement possibility learning multiple types behaviors parallel 
concurrent multi modal learning biologically pragmatically inspired ongoing challenge learning community franklin selfridge brooks matari 
foraging task basic behaviors designed hand behavior selection learned 
basic behaviors learned optimized parallel learning behavior selection 
example agents parameter learning scheme optimize grasping behaviors puck carrying state 
order avoid extending learner state space reverting traditional problems monolithic learners multi modal learning implemented multiple correlation mechanisms monolithic matrix 
described reinforcement techniques applied learning level 
explicit merging learned policies needed learning modules independent 
structuring learning difficulties facing learning community lack structure existing learning methodologies delineates applicability 
consequently choice methodology passing trends dogma objective applicability performance criteria 
goals learning described thesis introduce structure popular methodology broadly characterized reinforcement learning 
applying reinforcement learning novel complex domain experimented date able establish limitations domain propose reformulation representation reinforcement learning domain possible efficient 
appropriately setting learning task effective results achieved single learning methodology 
interesting direction pursue deal learning problems complex require learning strategy means relating different techniques 
signal symbol learning signal symbol learning encapsulates entire learning process grounding agent experiences world resulting comparatively high level representations 
date systems learned low level signals sensory information bypassed symbolic representations built designer 
spectrum symbolic high level learning traditionally concerned grounding physical world 
situated systems connection direct sensory experiences high level cognitive activities symbol grounding important problem addressed harnad 
situated agents date dealt considered highly cognitive tasks 
learning lower level capacities complex motor behaviors requires intermediate increasingly representations 
process relabeling information forms subsystems achieving different goals step direction bridging signal symbol gap 
learning level simple mapping conditions behaviors 
process constructing reusable behavior combinations requires way labeling combinations 
learning strategy described able built mapping labeled behaviors 
general solution problem desirable hope address 
summary goal described learning bring light important properties situated domains impact reinforcement learning strategies 
described mdp models agent world interactions effective noisy multi agent domain traditional notions state action inappropriately low level system description control learning delayed reinforcement sufficient learning domain domains similar level complexity 
introduced higher level description learning system conditions behaviors greatly diminishes learner state space results robust control 
introduced methodology shaping reinforcement order take advantage information available agent 
domain shaping necessary complexity environment agent agent agent interactions 
approach consists methods partitions learning task natural subgoals behaviors reinforces separately employs progress estimators generate immediate feedback agent 
proposed formulation evaluated group physical robots learning forage shown effective superior alternatives 
approach general compatible existing reinforcement learning algorithms serve learning efficient variety situated domains variety methodologies 
chapter summary aim thesis gain insight intelligent behavior increasing level complexity systems designed studied 
contrast ai systems focused complex cognition situated simple worlds vice versa described addressed situated embodied agents coexisting interacting complex domain 
hope methodologies results extended understanding synthesis analysis learning group behavior 
selection appropriate representation level control planning learning motivating forces 
proposed methodology constraints order derive behaviors control laws guarantee achievement maintenance goals 
furthermore described methodology selecting basic behaviors basis set behaviors substrate control learning agent environment 
demonstrated ideas problem synthesizing coherent group behavior domain planar spatial interactions 
devised basic behavior set showed meets defining criteria including mutual reducibility simple combination 
showed basic behaviors conditions substrate learning 
furthermore described methodology shaping reinforcement heterogeneous reinforcement functions progress estimators order learning possible efficient dynamic multi agent domains 
main idea approach combining constraints agent mechanical sensory characteristics constraints environment types interactions sensory information agent obtain order construct constraint primitives control 
family photo physical experimental agents demonstrate verify group behavior learning described thesis 
sensory called primitives conditions action referred behaviors 
cases clustering constraints provide abstraction level control learning efficient 
dealt complex multi agent domain complex learning problem order fully confront issues selecting right abstraction representation level situated agents 
complexity chosen environment combined requirement acting real time enforced necessity representation level low computationally intractable high remove potential novel behavior strategies designed learned agents 
intended foundation continuing effort studying increasingly complex behavior complex intelligence 
basic behaviors general approach control planning learning 
brings light theoretically empirically challenging problems offers effective solutions situated learning 
analytically tighten experimentally broaden understanding issues 
demonstrated results group behavior learning meant stepping stones studying increasingly complex social agents capable complex learning ultimately leading better understanding biological intelligence 
appendix learning watkins introduced family methods called learning solving markov decision problems incomplete information delayed reinforcement 
simplest version called step learning commonly described 
learning temporal differencing strategy attempts maximize time step 
expected discounted reward action input state values state action pairs stored table updated time step 
utility state maximum value actions taken state 
value doing action state defined sum immediate reward utility state state transition function discounted parameter fl 
formally max fle fl values updated rule fi fle gamma fi rl algorithm learning form 
initialize select 
forever observe current world state choose action maximizes 
execute action immediate reward executing state update rule 
new state 
fi fl tunable learning parameters 
fi determines learning rate 
fi disregards history accumulated current value resets sum received expected reward time step usually resulting oscillations 
fl discount factor reward 
ideally fl close possible relevance reward maximized 
deterministic worlds fl set general case algorithms fl compared limit expected reinforcement go infinity 
choice initial values affect speed convergence farther optimal policy longer takes converge 
initialized problem set positive optimal policy algorithm tend converge positive value exploring alternatives random actions added guarantee entire action space explored kaelbling 
alternatively optimal policy roughly estimated values initialized higher decreased time 
sensitive coupling initial values reinforcement function 
reinforcement function strictly positive table initialized values exceeding optimal policy system take longer converge reinforcement function contains negative signals 
appendix glossary adaptability ability cope internal external changes 
agent entity computational process senses world acts 
arbitration problem coordinating activity multiple input behaviors order produce desired output behavior 
basic behaviors building blocks control planning learning 
basic behavior set basis set behaviors directly combination sufficient reaching goals system 
elements set mutually reducible 
behavior control law achieves maintains goal 
behavior conditions proper subsets state space necessary sufficient activating behavior 
collective behavior observer subjective definition spatial temporal pattern interactions multiple agents 
condition predicate sensor readings maps proper subset state space 
cooperation form interaction usually communication 
ensemble behavior observable global behavior group collection agents event change agent perceptual condition vector 
external state externally observable state agent 
fortuitous reward reward received inappropriate behavior happened achieve desired goal 
group density ratio sum agents footprints size available interaction space 
direct communication action sole purpose transmitting information 
directed communication communication aimed particular receiver set receivers 
direct behavior combination temporal overlap behaviors 
behavior active time 
implemented summation operator 
embodiment state embodied having body physical constraints properties 
explicit cooperation set interactions involve exchanging information performing actions order benefit agent 
footprint sphere agent influence 
implicit cooperation form interactions consisting actions part agent goal achieving behavior may effects world help agents achieve goals 
impulse reinforcement reinforcement delivered agent reaches single goal state 
group collection size 
homogeneity property situated world embodied similar dynamics executing identical control programs 
heterogeneity property different agent terms environment embodiment control 
interaction mutual influence behavior 
interference influence partially completely blocks agents goal driven behavior 
multi agent control generating desired behavior multi agent system 
niche habitat class environments agent adapted 
non directed communication communication limited particular receiver set receivers includes indirect direct communication 
multi agent system system consisting agents 
policy mapping inputs states conditions actions behaviors 
situatedness property situated existing context environment involves interaction dynamics 
stigmergic communication communication modifications environment direct message passing 
temporal behavior combination temporal sequence behaviors 
behavior active time 
implemented switching operator 
thrashing repeated execution inappropriate behaviors 
bibliography abraham shaw 
dynamics geometry behavior addison wesley california 

learning observation second year life developmental psychology 
agre chapman 
pengi implementation theory activity proceedings aaai seattle wa pp 

agre chapman 
plans maes ed designing autonomous agents theory practice biology engineering back mit press pp 


adaptive resource allocation multiple mobile robot system communication technical report tr north dakota state univeristy 

initial results inter robot communication multiple mobile robotic system proceedings ijcai workshop dynamically interacting robots chambery france pp 

arkin 
unification navigational planning reactive control aaai spring symposium robot navigation pp 

arkin 
cooperation communication multiagent schema robot navigation journal robotic systems 
arkin balch 
communication behavioral state multi agent retrieval tasks ieee international conference robotics automation pp 


self awareness awareness mirror self recognition synchronic imitation unfamiliar peers developmental psychology 
packard 
emergent artificial ecology varela bourgine eds practice autonomous systems proceedings european conference artificial life mit press pp 

atkeson 
learning arm kinematics dynamics annual review neuroscience 
atkeson 
memory approaches approximating continuous functions proceedings sixth yale workshop adaptive learning systems 
atkeson 
modelbased robot learning technical report aim mit 
axelrod 
evolution cooperation basic books new york 

analysis modeling processes ed psychological modeling conflicting theories atherton pp 


social learning theory prentice hall englewood cliffs walters 
social learning personality development holt rinehart winston new york 
barman mackworth pai sahota wilkinson zhang 
dynamite testbed multiple mobile robots proceedings ijcai workshop dynamically interacting robots chambery france pp 

barto 
learning tasks control perspective technical report coins tr university massachusetts 
barto bradtke singh 
learning act real time dynamic programming ai journal 
barto sutton anderson 
neuronlike elements solve difficult learning control problems ieee transactions systems man cybernetics 
beckers holland deneubourg 
local actions global tasks stigmergy collective robotics brooks maes eds artificial life iv proceedings fourth international workshop synthesis simulation living systems mit press 
beli deneubourg lax 
mathematical model construction mathematical biology 
bellman 
dynamic programming princeton university press princeton new jersey 
benhamou 
modeling simulation animal movements meyer wilson eds animals animats international conference simulation adaptive behavior mit press 
beni 
maximum entropy principle sensing swarm intelligence varela bourgine eds practice autonomous systems proceedings european conference artificial life mit press pp 

bizzi mussa ivaldi 
muscle properties control arm movement osherson kosslyn hollerbach eds visual cognition action vol 
mit press pp 

bizzi mussa ivaldi giszter 
computations underlying execution movement biological perspective science 
bonabeau 
dangers synthetic reductionism practice autonomous systems proceedings european conference artificial life pp 

brock montana 
coordination control multiple autonomous vehicles ieee international conference robotics automation pp 

brooks 
robust layered control system mobile robot ieee journal robotics automation ra 
brooks 
behavior language user guide technical report aim mit artificial intelligence lab 
brooks 
elephants don play chess maes ed designing autonomous agents mit press pp 

brooks 
artificial life real robots practice autonomous systems proceedings european conference artificial life mit press 
brooks 
intelligence reason proceedings ijcai 
brooks 
intelligence representation artificial intelligence 
brooks connell 
asynchronous distributed control system mobile robot spie cambridge massachusetts 
brooks matari 
real robots real learning problems robot learning kluwer academic press pp 

brooks maes matari moore 
lunar base construction robots ieee international workshop intelligent robots systems iros tokyo pp 

brown 
political life lattice practice autonomous systems proceedings european conference artificial life pp 

deneubourg 
model orientation journal theoretical biology 
choi latombe yim 
indoor automation mobile robots iros japan pp 


collective intelligence insect colonies means selforganization practice autonomous systems proceedings european conference artificial life pp 

canny 
complexity robot motion planning mit press cambridge massachusetts 
cassandra kaelbling littman 
acting optimally partially observable stochastic domains proceedings aaai seattle washington 
chapman 
planning conjunctive goals intelligence 
chapman kaelbling 
input generalization delayed reinforcement learning algorithm performance comparisons proceedings ijcai sydney australia 
chase 
dynamics hierarchy formation sequential development dominance relationships behaviour 
chase 
generating societies collective social patterns humans animals practice autonomous systems proceedings european conference artificial life pp 

chase rohwer 
methods quantifying development dominance hierarchies large groups application harris animal behavior 
chase 
aggressive interactions inter contest interval long winners keep winning animal behavior press 
chatila laumond 
position referencing consistent world modeling mobile robots ieee international conference robotics automation 
cheney 
monkeys see world university chicago press chicago 
cheney 
reading minds reading behaviour whiten ed natural theories mind basil blackwell 
clearwater huberman hogg 
cooperative solution constraint satisfaction problems science 
cliff husbands harvey 
evolving visually guided robots animals animats international conference simulation adaptive behavior mit press pp 

colorni dorigo maniezzo 
distributed optimization ant colonies varela bourgine eds practice autonomous systems proceedings european conference artificial life mit press pp 

connell 
minimalist mobile robotics colony architecture artificial creature academic press 
connell 
sss hybrid architecture applied robot navigation ieee international conference robotics automation nice france pp 

drogoul 
simulating process ant colonies manta practice autonomous systems proceedings european conference artificial life pp 

dallas 
operative search behavior group lego robots master thesis university edinburgh 
dario 
approach disassembly problem robotics ieee international conference intelligent robots systems yokohama japan pp 

dario sandini 
instinctive behaviors personalities societies cellular robots ieee international conference robotics automation pp 


emergent phenomena complexity brooks maes eds artificial life iv proceedings fourth international workshop synthesis simulation living systems mit press 
post travis 
positive feedback natural systems 
decker lesser 
shot dynamics coordination algorithm distributed sensor networks proceedings aaai washington dc pp 

decker lesser 
quantitative modeling complex computational task environments proceedings aaai washington dc pp 

deneubourg 
goss 
collective patterns decision making ethology ecology evolution pp 

deneubourg aron goss pasteels 
random behaviour amplification processes number participants contribute foraging properties ants physica pp 

deneubourg goss franks franks 
dynamics collective sorting animals animats international conference simulation adaptive behavior mit press pp 

deneubourg goss pasteels 
self organization mechanisms ant societies ii learning foraging division labor individual collective behavior social insects 
deneubourg beckers 
swarm architectures varela bourgine eds practice autonomous systems proceedings european conference artificial life mit press pp 

dennett 
intentional stance mit press cambridge massachusetts 

birds self organized social behaviours regulate dispersal wide areas evidences gull practice autonomous systems proceedings european conference artificial life pp 

donald jennings rus 
experimental information invariants cooperating autonomous mobile robots proceedings international symposium robotics research hidden valley pa doyle sacks 
stochastic analysis qualitative dynamics technical report lcs tm mit laboratory computer science 
ferber 
behavioral simulation model study emergent social structures varela bourgine eds practice autonomous systems proceedings european conference artificial life mit press pp 

dudek jenkin milios wilkes 
taxonomy swarm robotics ieee international conference intelligent robots systems yokohama japan pp 

durfee lee gmytrasiewicz 
reciprocal rationality mixed strategy equilibria proceedings aaai washington dc pp 

ephrati 
constrained intelligent action planning influence master agent proceedings aaai san jose california pp 

erdmann 
probabilistic strategies robot tasks phd thesis mit 
erdmann lozano erez 
multiple moving objects algorithmica 
ferrell 
robust agent control autonomous robot sensors actuators technical report ai tr mit artificial intelligence laboratory 
fikes nilsson 
strips new approach application theorem proving problem solving artificial intelligence 
firby 
investigation reactive planning complex domains proceedings sixth national conference artificial intelligence seattle pp 

floreano 
patterns interactions shared environments practice autonomous systems proceedings european conference artificial life pp 

forrest 
emergent computation self organizing collective cooperative phenomena natural artificial computing networks north holland amsterdam 
foster castro mcnaughton 
spatial selectivity rat hippocampal neurons dependence preparedness movement science pp 

franklin selfridge 
new directions adaptive control theory robotics miller sutton werbos eds neural networks control mit press pp 

franks 
army ants collective intelligence american scientist 
fukuda buss 
structure decision self organizing robots cell structures ieee international conference robotics automation scottsdale arizona pp 

fukuda arai 
efficient communication method cellular robotics system ieee international conference intelligent robots systems yokohama japan pp 

gallagher beer 
qualitative dynamics analysis evolved locomotion controller animals animats international conference simulation adaptive behavior mit press pp 

gasser huhns 
distributed artificial intelligence pitman london 
georgeff lansky 
reactive reasoning planning proceedings sixth national conference artificial intelligence seattle pp 

chatila 
integrated navigation motion control system autonomous multisensory mobile robots brady paul eds international symposium robotics research mit press cambridge massachusetts 
gleitman 
psychology norton new york 
goldberg 
genetic algorithms search optimization machine learning addison wesley reading ma 
gomez 
visual behavior window reading mind primates whiten ed natural theories mind basil blackwell 
goss deneubourg beckers 
recipes collective movement practice autonomous systems proceedings european conference artificial life pp 

gould 
ethology mechanisms evolution behavior norton new york 
gould 
flower shape landmark locale memory menzel mercer eds neurobiology behavior springer verlag 

quantum chaos scientific american 
harnad 
symbol grounding problem physica 
hillis 
evolving parasites improve simulated evolution optimization procedure physica 
hinton 
connectionist learning procedures kodratoff michalski eds machine learning artificial intelligence approach vol 
morgan kaufmann pp 

hodgins brogan 
robot herds group behaviors systems significant dynamics brooks maes eds artificial life iv proceedings fourth international workshop synthesis simulation living systems mit press 
hogeweg 
processes mirror modelling methodology journal theoretical biology 
hogg williams 
solving really hard problems cooperative search proceedings aaai washington dc pp 

holland 
properties bucket brigade algorithm proceedings international conference genetic algorithms applications pittsburgh pa pp 

holland 
escaping brittleness possibilities general purpose learning algorithms applied parallel rule systems michalski carbonell mitchell eds machine learning artificial intelligence approach vol 
morgan kaufmann los altos ca 
huang beni 
stationary waves dimensional cyclic swarms ieee international conference intelligent robots systems yokohama japan pp 

huber durfee 
observational uncertainty plan recognition interacting robots proceedings ijcai workshop dynamically interacting robots chambery france pp 

huberman 
performance cooperative processes physica 
jaakkola jordan 
convergence stochastic iterative dynamic programming algorithms submitted neural computation 
jordan rumelhart 
forward models supervised learning distal teacher cognitive science 
kaelbling 
learning embedded systems phd thesis stanford university 
kephart hogg huberman 
collective behavior predictive agents physica 
keshet 
trail adaptable mechanism population behaviour practice autonomous systems proceedings european conference artificial life pp 


development social amoeba american scientist 
kolen pollack 
apparent computational complexity physical systems proceedings fifteenth annual conference cognitive science society boulder colorado pp 


fast cut protocol agent coordination proceedings aaai washington dc pp 


control discrete event systems technical report ms cis grasp lab university pennsylvania 
koza 
evolution evolution computer programs control independently acting agents proceedings simulation adaptive behavior sab mit press paris france pp 

kraus 
agents contracting tasks non collaborative environments proceedings aaai washington dc pp 

kube 
collective robotic intelligence control theory robot populations master thesis university alberta 
kube zhang 
collective robotic intelligence animals animats international conference simulation adaptive behavior pp 

kube zhang wang 
controlling collective tasks aln ieee international conference intelligent robots systems yokohama japan pp 


fuzzy control group leader behaviors ieee international conference intelligent robots systems yokohama japan pp 

laird rosenbloom 
integrating execution planning learning soar external environments proceedings aaai pp 

langton 
artificial life addison wesley 
langton 
computation edge chaos phase transitions emergent computation physica 
lin 
self improvement reinforcement learning planning teaching proceedings eighth international conference machine learning morgan kaufmann evanston illinois pp 

lozano erez mason taylor 
automatic synthesis fine motion strategies robots international journal robotics research 
lynch 
simulation techniques proving properties real time systems technical report mit lcs tm mit 
lynch tuttle 
hierarchical correctness proofs distributed algorithms technical report mit lcs tr mit 

evolution communication population simple machines technical report computer science department technical report cs university tennessee 
maes 
dynamics action selection ijcai detroit mi pp 

maes 
learning behavior networks experience varela bourgine eds practice autonomous systems proceedings european conference artificial life mit press pp 

maes brooks 
learning coordinate behaviors proceedings aaai boston ma pp 

mahadevan connell 
automatic programming behavior robots reinforcement learning proceedings aaai pittsburgh pa pp 

mahadevan connell 
scaling reinforcement learning robotics exploiting subsumption architecture international workshop machine learning morgan kaufmann pp 

matari 
distributed model mobile robot environment learning navigation technical report ai tr mit artificial intelligence laboratory 
matari 
navigating rat brain inspired model robot spatial representation meyer wilson eds animals animats international conference simulation adaptive behavior mit press pp 

matari 
comparative analysis reinforcement learning methods technical report aim mit artificial intelligence lab 
matari 
behavior systems key properties implications ieee international conference robotics automation workshop architectures intelligent control systems nice france pp 

matari 
designing emergent behaviors local interactions collective intelligence animals animats international conference simulation adaptive behavior 
matari 
integration representation goal driven robots ieee transactions robotics automation 
matari 
kin recognition similarity group behavior proceedings fifteenth annual conference cognitive science society boulder colorado pp 

matari 
learning behave socially third international conference simulation adaptive behavior 
mcfarland 
animal behavior benjamin cummings 
mcfarland 
oxford companion animal behavior oxford university press 
meier koll 
time structure analysis village community indians mathematically simulated system oscillators practice autonomous systems proceedings european conference artificial life pp 

miceli cesta 
strategic social planning looking willingness multi agent domains proceedings fifteenth annual conference cognitive science society boulder colorado pp 

miller sutton werbos 
neural networks control mit press 
minsky 
theory neural analog reinforcement systems application brain model problem phd thesis princeton 
minsky 
society mind simon schuster new york 
sole goodwin 
ants metaphor hierarchical levels practice autonomous systems proceedings european conference artificial life pp 

moore 
variable resolution dynamic programming efficiently learning action maps multivariate real valued state spaces international workshop machine learning morgan kaufmann 
moore 
fast robust adaptive control learning forward models advances neural information processing pp 

moore 
parti game algorithm variable resolution reinforcement learning multidimensional state spaces advances neural information processing pp 

moravec cho 
bayesian method certainty grids aaai spring symposium robot navigation pp 

muller wehner 
path integration desert ants proceedings natural academy sciences 
mussa ivaldi giszter 
vector field approximation computational paradigm motor control learning biological cybernetics 
noreils 
architecture cooperative autonomous mobile robots ieee international conference robotics automation pp 

noreils 
robot architecture integrating cooperation mobile robots application indoor environment international journal robotics research 
parker 
experiment mobile robotic cooperation proceedings robotics challenging environment albuquerque new mexico 
parker 
learning cooperative robot teams proceedings ijcai workshop dynamically interacting robots chambery france pp 

parker 
heterogeneous multi robot cooperation phd thesis mit 
payton 
internalized plans representation action resources maes ed designing autonomous agents theory practice biology engineering back mit press 
payton kimble rosenblatt 
works robust approach fault tolerant autonomous control journal applied intelligence 
piaget 
play dreams imitation children norton new york 
pinker 
language instinct william morrow new york 
pomerleau 
neural network perception mobile robotic guidance phd thesis carnegie mellon university school computer science 
woodruff 
chimpanzee theory mind behavior brain science 
read miller 
explanatory coherence construction mental models proceedings fifteenth annual conference cognitive science society boulder colorado pp 

resnick 
centralized mindset exploration phd thesis mit 
reynolds 
flocks herds schools distributed behavioral model computer graphics 
rosenschein genesereth 
deals rational agents ijcai pp 

rosenschein 
agents negotiation mechanisms multiagent systems ijcai pp 

rosenschein kaelbling 
synthesis machines provable epistemic properties halpern ed theoretical aspects reasoning knowledge morgan kaufmann los altos ca pp 

rosenthal zimmerman 
social learning cognition academic press new york 
samuel 
studies machine learning game checkers ibm journal research development 
sandini 
gradient driven self organizing systems ieee international conference intelligent robots systems yokohama japan pp 

schaal atkeson 
robot juggling implementation memorybased learning control systems magazine 

knowledge tracking algorithm generating collective behavior individual populations practice autonomous systems proceedings european conference artificial life pp 

schoppers 
universal plans reactive robots unpredictable domains ijcai menlo park pp 


honey bee colony superorganism american scientist 
shoham tennenholtz 
synthesis useful social laws artificial agent societies proceedings aaai san jose california pp 

simon 
sciences artificial mit press 
singh 
transfer learning compositions sequential tasks proceedings eighth international conference machine learning morgan kaufmann evanston illinois pp 


synchronous alternating phase locked tropical science 
smithers 
better robots harder third international conference simulation adaptive behavior 
steels 
cooperation distributed agents selforganization workshop multi agent cooperation north holland cambridge uk 
steels 
artificial life roots artificial intelligence appear artificial life journal 
steels 
emergent functionality robot behavior line evolution brooks maes eds artificial life iv proceedings fourth international workshop synthesis simulation living systems mit press 
sussman mcdermott 
planner conniver genetic approach proceedings fall joint computer conference pp 

sutton 
learning predict method temporal differences journal machine learning 
sutton 
integrated architectures learning planning reacting approximating dynamic programming proceedings seventh international conference machine learning austin texas 
tan 
multi agent reinforcement learning independent vs cooperative agents proceedings tenth international conference machine learning amherst ma pp 

thrun mitchell 
integrating neural network learning explanation learning proceedings ijcai chambery france 
tomasello kruger 
cultural learning appear journal brain behavior sciences 
travers 
animal construction kits langton ed artificial life addison wesley 
vander sherman luciano 
human physiology mcgrawhill book new york 
waterman 
animal navigation scientific american library new york 
watkins 
learning delayed rewards phd thesis king college cambridge 
watkins dayan 
learning machine learning 
wehner 
matched filters neural models external world journal computational physiology 

complex system dynamics lecture notes vol 
ii santa fe institute studies sciences complexity addison wesley new york 
werner dyer 
evolution communication artificial organisms technical report ucla ai university california los angeles 
whitehead 
reinforcement learning adaptive control perception action phd thesis university rochester 
whitehead ballard 
active perception reinforcement learning proceedings seventh international conference machine learning austin texas 
whitehead karlsson tenenberg 
learning multiple goal behavior task decomposition dynamic policy merging connell mahadevan eds robot learning kluwer academic publishers pp 

wiggins 
applied nonlinear dynamical systems chaos springer verlag new york 
yanco stein 
adaptive communication protocol cooperating mobile robots animals animats international conference simulation adaptive behavior mit press pp 


