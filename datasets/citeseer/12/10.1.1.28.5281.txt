generalized clustering supervised learning data assignment pat langley isle court palo alto ca langley isle org wagstaff dept computer science cornell university ithaca ny cs cornell edu yoo computer science dept middle state university tn edu clustering algorithms increasingly important handling analyzing data 
considerable done devising effective increasingly specific clustering algorithms 
contrast developed generalized framework accommodates diverse clustering algorithms systematic way 
framework views clustering general process iterative optimization includes modules supervised learning instance assignment 
framework suggested novel clustering methods 
investigate experimentally efficacy algorithms test hypotheses relation unsupervised techniques supervised methods embedded 
categories subject descriptors computing methodologies artificial intelligence learning general terms algorithms design experimentation keywords clustering supervised learning iterative optimization 
motivation research machine learning focuses induction supervised training data situations class labels available require unsupervised methods 
widespread approach unsupervised induction involves clustering training cases groups reflect distinct regions decision space 
exists large literature clustering methods everitt long history development increasing interest application permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
kdd san francisco ca usa copyright acm xxxxx xx xx xx 
little understanding relation supervised unsupervised approaches induction 
remedy oversight examining situations supervised induction method occurs subroutine clustering algorithm 
suggests important ideas 
able generate new clustering methods existing techniques replacing initial supervised technique different supervised technique 
second expect resulting clustering methods behave form desirable clusters domains supervised component behaves provided labeled training data available 
pages follow explore ideas context iterative optimization common scheme clustering includes means expectation maximization special cases 
reviewing framework section describe approach embedding supervised algorithm learned classifier iterative optimizer section examine supervised methods taken step 
section report experimental studies designed test hypotheses relations behavior resulting clustering methods supervised components 
closing review related generative frameworks machine learning consider directions research 
update class models assign instances classes clustered data data instances labeled data iterative optimization procedure 

generalized clustering clustering systems rely notion iterative optimization 
depicts system iterates steps class model creation data reassignment reaching predetermined iteration limit changes occur 
variations general framework basic idea best illustrated known example methods 
means em iterative optimizers clustering algorithms popular simplicity flexibility means expectation maximization em 
methods studied experimentally problems widely applied settings 
review algorithms briefly note key similarities show differences suggest general clustering framework 
means algorithm represents class centroid computes mean attribute data points belonging class 
geometric terms corresponds finding center mass instances associated class 
data reassignment involves assigning instance class closest centroid 
contrast em models class probability distribution extracts training data class model creation step 
data continuous class generally modeled dimensional gaussian distribution consists mean variance attribute 
discrete case ck ja jl extracted possible combination class ck attribute attribute value jl cases finding parameters contribution instance weighted ck 
data reassignment done recalculating ck instance class ck new class models 
general framework clustering algorithms iterative optimization employ different methods developing class models 
view invoking different supervised learning technique distinguish classes 
algorithms differ assign instances classes means assigns instance single class em uses partial assignment instance distributed classes 
refer absolute method strict paradigm partial method weighted paradigm 
observations lead general framework clustering involves selecting supervised learning algorithm selecting assignment paradigms 
context means em framework immediately suggests variants 
weighted paradigm means classifier obtain weighted means algorithm 
similarly combining em probabilistic classifier strict paradigm produces variant instance assigned entirely probable class 
variant explored name em partial assignment method commonly 
classifiers utilized means em easily modified operate assignment paradigm supervised algorithms require sophisticated adaptations see shortly 

supervised learning methods argued possible embed supervised learning method generalized clustering framework 
evaluation focused simple induction algorithms limited representational power clustering process aims generate disjoint decision regions powerful supervised designed produce 
describe algorithms detail including adaptations weighted paradigm 
adaptations involve altering model production take account weights instances revising instance reassignment generate class weights instance produce generation class models 
prototype modeler supervised algorithm plays role means creates prototype centroid class extracting mean attribute training cases class 
prototype modeler classifies instance selecting class centroid closest dimensional space 
distance metric sensitive variations scale version normalizes data values zero creating prototypes 
weighted paradigm mean attribute weighted average training cases 
relative proximity instance centroid determines associated weight centroid class 
formally express jc gamma distance prototype ck jcj distance prototype cm jcj number classes 
new centroid composed weighted mean attribute mean attribute cluster ck calculated jxj jc delta ij jxj jc value jth attribute instance xm naive bayesian modeler selected naive bayes second induction algorithm 
described context em class modeled probability distribution described ck ck ja jl class ck attribute attribute value jl nominal attributes naive bayes represents ck ja jl discrete conditional probability distribution estimates counts training data estimates class probability ck similar manner 
continuous attributes typically uses conditional gaussian distribution estimates computing mean variance attribute training data class 
calculate probability new instance belongs class ck naive bayes employs expression ck jx ck jl jc assumes distribution values attribute independent class 
operating strict classifier naive bayes algorithm returns class highest probability instance 
weighted case conditional distributions calculated weighted sum strict sum expression jc ck jx jcj cm jx determines weight data reassignment process 
perceptron list modeler simple induction method perceptron combines evidence attributes classification uses expression output ae jaj wm delta xim threshold assign test case positive negative class 
weight wm specifies relative importance attribute taken weights determine hyperplane attempts separate classes 
learning algorithm invokes error driven scheme adjust weights associated attribute 
perceptron differentiate classes employed ordered list perceptrons operates decision list 
algorithm learns discriminate majority class generating perceptron 
instances majority class removed system trains distinguish new majority class rest producing perceptron 
process continues class remains treated default 
perceptron traditionally assumes assignment natural interpret scaled difference sum threshold likelihood 
weighted variant multiplies update attribute weight weight instance instance smaller weight smaller effect learning 
prevent small weights causing endless oscillations triggers updating cycle data incorrectly classified instance weight greater instances actual update 
reassignment weighted method calculates difference instance value threshold scaled sigmoid jc delta threshold produces bounds weight size 
instance evaluated perfectly threshold function return 
factor exponent distributes resulting weights larger range algorithm give close weight instances 
sigmoid tight useful generally small range values 
decision stump modeler final supervised learning algorithm selected decision stump induction holte differs selecting single attribute classify instances 
uses information theoretic measure info gamma jcj freq ck jsj delta log freq ck jsj freq ck frequency class ck training set jcj classes 
attribute continuous algorithm orders observed values considers splitting successive pair selecting split highest score 
method applies process recursively purposes study learning rate iterations training data classification tasks 
values subset continuing divisions gain information measured gain info gamma jp jt delta info tm training set tm subset jp number branches 
attribute nominal algorithm creates separate branch attribute value 
branch stump associated class constitutes majority class training cases sorted branch 
accommodate weighted assignment adjust equations sum weights instances strict frequencies keep simple statistical information branch 
reassignment weight instance class ck calculated jc jbj xm jc jcj jbj xm jbj number instances associated branch instance sorted 

experimental studies intuitions clustering framework suggested corresponding formal hypotheses 
expected algorithm exhibit preference data assignment paradigms demonstrating better performance paradigm different data sets 
second anticipated data sets high low predictive accuracy supervised method associated high low accuracy corresponding clustering algorithm 
section describe designs experiments test hypotheses results obtained 
experiments natural data test hypotheses ran generalized clustering system algorithm paradigm combination battery natural data sets 
evaluated supervised algorithm independently training measuring predictive accuracy separate test set 
independent variables assignment paradigm clustering tests supervised learning algorithm data set number instances training 
dependent variables classification accuracies unseen data 
standard accuracy metric evaluate supervised classifiers clustering algorithms acc ffi jt test set ffi classified correctly 
evaluating accuracy trained classifier labeled data set test set removed 
clustering algorithms create classes added step completed cluster assigned actual naturally expected single algorithm combination outperform data sets consistent general findings machine learning hardly deserves status hypothesis 
table supervised accuracies data sets 
prototype bayes perceptron stump promoters iris hayes roth glass class majority population 
example cluster consists instances class class instances cluster declared members class accuracy cluster 
approach loses detail evaluate clustering algorithm correct clusters 
selected data sets uci repository promoters iris hayes roth glass involved different numbers classes different numbers attributes different attribute types nominal continuous mixed 
factor selection led high accuracy supervised methods typically lower accuracy shown table 
differentiation supervised training data prerequisite testing predicted correlation accuracies supervised learning clustering 
remember supervised methods restricted representational power generally limited decision region class 
result fact method obtains high accuracy domains suggests classes maps single cluster 
lets assume number classes data set number clusters increasing chances meaningful results 
data set collected learning curve tenfold cross validation recording results increment data points 
typically clustering accuracy ceased improve early curve supervised accuracy continued increase 
results report involve accuracy measured point curve 
table unsupervised accuracies alternative data assignment paradigms strict weighted 
prototype bayes perceptron stump promoters iris hayes roth glass recall hypothesis predicted supervised method construct accurate clusters combined preferred data assignment paradigm 
results table shows classification accuracies method paradigm combination domains hypothesis 
general supervised algorithm better assignment scheme depending domain 
naive bayes prototype learner showed large supervised accuracy unsupervised accuracy decision stump perceptron list naive bayes prototype supervised unsupervised accuracies strict data assignment algorithms natural data sets 
shifts sort swings decision stump learner drastic 
perceptron method showed support prediction favoring weighted assignment data sets tied result fourth 
addressing hypothesis proceeded test second claim relatively higher lower accuracies supervised mode associated relatively higher lower accuracies unsupervised data correlated positively 
original plan measure unsupervised accuracy learning algorithm combined preferred data assignment paradigm 
having rejected notion preference resorted measuring correlation supervised accuracy achieved clustering strict assignment followed separate measure accuracy supervised learning weighted assignment 
computed correlation supervised accuracies algorithm domain combinations table analogous strict accuracies table 
resulting correlation coefficient significant level explained percent variance 
shows supervised accuracy reasonable predictor unsupervised accuracy generally supporting hypothesis 
calculated correlation supervised accuracies weighted accuracies table 
correlation significant level explained percent variance 
experiments synthetic data encouraging results natural data sets shows framework relevance real world clustering problems give limited understanding reasons underlying phenomena 
reason decided carry study employed synthetic data designed reveal detailed causes effects 
standard explanation induction methods outperforming relies notion inductive bias reflects fact formalisms represent certain decision regions easily 
supervised learning methods quite different inductive biases designed separate learning tasks intended easily learned methods 
learning task incorporated continuous variables classes single contiguous decision region class 
domain designed decision stumps mind involved splits relevant attribute prototype friendly domain involved distinct prototypes forth 
naive bayesian classifier difficult foil supervised method domain relatively poorly 
domain devised generator produced random instances uniform domain gaussian distribution class creating number instances class 
geometric metaphor clarifies reason method outperform supervised unsupervised mode suggests reason correlation behavior tasks imperfect 
conventional wisdom states clustering easy clusters separated difficult 
data generator included parameter vary systematically vary separation boundaries class 
predictive variables domain ranged varied separation distance 
expected synthetic domains reproduce positive correlation observed natural data predicted cluster separation influence effect 
particular thought correlation lower gap small iterative optimization difficulty assigning instances right unlabeled classes supervised learning difficulty 
correlation increase monotonically cluster distance process finding separated clusters dominated inductive bias supervised learning modules 
experimental runs synthetic data support predictions 
despite attempts design data sets distinguish supervised learning methods correlations supervised unsupervised accuracies cluster separation considerably lower strict weighted studies natural domains marginally significant level 
experiments showed evidence correlation increases cluster separation giving strict weighted giving strict weighted 
plots accuracies strict unsupervised learning supervised accuracy cluster separation suggests reason negative result 
apparently correlations reduced ceiling effect supervised accuracies generally higher results natural domains show little variation unsupervised accuracies range widely 
supervised methods typically learn accurate classifiers synthetic domains best design 
analogous plots higher values separation parameter show stronger versions effect indicating supervised supervised accuracy unsupervised accuracy decision stump perceptron list naive bayes prototype supervised unsupervised accuracies strict data assignment algorithms synthetic data sets 
induction benefits cluster separation unsupervised clustering explains correlation increase predicted 
expectations intuition inductive bias cluster separation dominant factors determining behavior iterative optimizer 
negative results high correlations natural domains infer factors vary experiment play equal important role 
candidates include number relevant attributes number irrelevant attributes amount attribute noise number classes known affect predictive accuracy learned classifiers 
domain characteristics varied systematically studies rely synthetic data explore relation clustering supervised learning 

related noted earlier exists large literature clustering everitt reviewed length 
relies iterative optimization group training cases exist variants means expectation maximization algorithms familiar readers 
instance michalski stepp cluster uses logical rule induction characterize clusters assign cases 
zhang described harmonic means method operates means invokes different distance metric usually speeds convergence 
despite diversity researchers proposed theoretical frameworks characterizing space iterative optimization methods software frameworks support rapid construction evaluation 
broader arena efforts link methods supervised unsupervised learning 
example langley sage adapted method inducing univariate decision trees operate unsupervised data generate taxonomy langley described similar sophisticated approach 
relationship supervised unsupervised algorithms rule learning transparent martin reported approach adapts supervised techniques construct association rules unlabeled data 
research focused specific algorithms general generative frameworks 
areas machine learning seen frameworks sort 
langley neches developed prism flexible language production system architectures supported combinations performance learning algorithms versions prodigy included variety mechanisms learning search control knowledge 
classification tasks kohavi mlc supported broad set supervised induction algorithms invoke considerable flexibility 
generative abilities framework apparent feature selection support novel combinations existing algorithms 
mlc effort comes closest spirit goals attempt provide flexible software infrastructure machine learning 

concluding remarks framework iterative optimization approaches clustering lets embed supervised learning algorithm model construction component 
approach produces familiar clustering techniques means em generates novel methods appeared literature 
framework evaluate hypotheses relation resulting clustering methods supervised modules tested natural synthetic data 
hypothesis supervised method preferred data assignment scheme produced accurate clusters borne experiments 
clustering practitioners continue combine prototype learning strict assignment giving means naive bayes weighted giving em evidence combinations superior alternatives 
experiments support second hypothesis revealing strong correlations accuracy supervised algorithms natural data sets accuracy iterative optimizers embedded 
augmented results experiments synthetic data gave control decision regions separation clusters 
studies produced positive correlations supervised unsupervised accuracy failed reveal effect cluster separation 
clearly remains considerable room additional research 
framework supports variety new clustering algorithms right important testing hypotheses relations supervised unsupervised learning 
carry experiments synthetic data vary systematically factors affect predictive accuracy irrelevant features attribute noise 
explore role cluster separation reason apparent influence studies 
specific results intriguing attach importance framework supports new direction studies clustering mechanisms 
encourage researchers view existing techniques examples generative framework utilize framework explore space clustering methods reveal underlying relations supervised unsupervised approaches induction 
ultimately strategy produce deeper understanding clustering process role broader science machine learning 

dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
duda hart 
pattern classification scene analysis 
john wiley sons 
everitt 
cluster analysis 
press new york nd edition 
holte 
simple classification rules perform commonly data sets 
machine learning 
kohavi john long manley pfleger 
mlc machine learning library 
tools artificial intelligence pages ieee computer society press 
langley 
elements machine learning 
morgan kaufmann san francisco ca 
langley neches 
prism user manual 
technical report carnegie mellon university department computer science pittsburgh pa 
langley sage 
conceptual clustering discrimination learning 
proceedings fifth biennial conference canadian society computational studies intelligence pages london ontario canada 
martin 
focusing attention observational learning importance context 
proceedings eleventh international joint conference artificial intelligence detroit mi 
morgan kaufmann 
michalski stepp 
learning observation conceptual clustering 
michalski carbonell mitchell editors machine learning artificial intelligence approach 
morgan kaufmann san francisco ca 
quinlan 
programs machine learning 
morgan kaufmann san mateo ca 
rosenblatt 
principles neurodynamics perceptrons theory brain mechanisms 
spartan washington dc 
smith medin 
categories concepts 
harvard university press cambridge ma 
veloso carbonell 
derivational analogy prodigy automating case acquisition storage utilization 
machine learning 
zhang hsu dayal 
harmonic means spatial clustering algorithm boosting 
proceedings pkdd workshop temporal spatial spatio temporal data mining 
