design implementation self compiler optimizing compiler object oriented programming languages dissertation submitted department computer science committee graduate studies stanford university partial fulfillment requirements degree doctor philosophy craig chambers march ii copyright craig chambers rights reserved iii certify read dissertation opinion fully adequate scope quality dissertation degree doctor philosophy 
david ungar principal advisor certify read dissertation opinion fully adequate scope quality dissertation degree doctor philosophy 
john hennessy certify read dissertation opinion fully adequate scope quality dissertation degree doctor philosophy 
mark linton approved university committee graduate studies dean graduate studies iv object oriented programming languages promise improve programmer productivity supporting data types inheritance message passing directly language 
unfortunately traditional implementations object oriented language features particularly message passing slower traditional implementations non object oriented counterparts fastest existing implementation smalltalk runs tenth speed optimizing implementation 
dearth suitable implementation technology forced object oriented languages designed hybrids traditional non object oriented languages complicating languages making programs harder extend reuse 
dissertation describes collection implementation techniques improve run time performance object oriented languages hopes reducing need hybrid languages encouraging wider spread purely object oriented languages 
purpose new techniques identify messages receiver single representation eliminate overhead message passing replacing message normal direct procedure call direct procedure calls amenable traditional inline expansion 
techniques include type analysis component analyzes procedures compiled extracts representation level type information receivers messages 
enable messages optimized away techniques include number transformations increase number messages single receiver type 
customization transforms single source method compiled versions version specific particular inheriting receiver type customization allows messages self inlined away replaced direct procedure calls 
avoid generating compiled code compiler invoked run time generating customized versions method receiver type pairs particular program 
splitting transforms single path source method multiple separate fragments compiled code fragment specific particular combination run time types 
messages expressions discriminated types optimized away split versions 
techniques designed coexist requirements language programming environment generic arithmetic user defined control structures robust error checking language primitives source level debugging automatic recompilation date methods programming change 
techniques implemented part compiler self language purely object oriented language designed refinement smalltalk 
pre existing implementation technology self programs self run orders magnitude slower counterparts written traditional non object oriented language 
applying techniques described dissertation performance self system times better fastest smalltalk system better optimizing scheme implementation close half optimizing implementation 
techniques applied object oriented languages boost performance enable object oriented programming style 
applicable non object oriented languages incorporating generic arithmetic generic operations including lisp icon apl 
applicable languages include multiple representations states single program structure logic variables prolog futures multilisp 
acknowledgments appreciation go advisor david ungar providing great opportunity participate self project 
david treated colleague promoted best prepare independent research 
taught research writing speaking advising 
enjoyable educational graduate experience 
strive students serving thesis reading committee providing important feedback mark linton provided advice direction year stanford 
weekly research group meetings years stanford interesting educational 
appreciate mark strong support 
john hennessy served reading committee perceptive comments dissertation significantly improved presentation process taught great deal research 
members self team stimulating discussions fun 
lee sharing office birth self system bay wei chang urs hlzle worked played self project matured 
patient people put occasional coffee induced continued listen discuss caffeine wore 
remember times shared 
randy smith ole agesen john maloney lars bak kept self group fun stimulating collection people 
stanford provided moral support social helped 
ross finlayson steve brat goldberg paul calder john vlissides particularly friends 
brother common law martin rinard time stanford interesting say 
appreciate support patience new colleagues university washington finished dissertation concurrently responsibilities 
helping transition student professor smooth enabled finish relatively quickly 
owe early training undergraduate mit barbara liskov research group 
mark day bob scheifler paul johnson bill weihl undergraduate thesis advisor taught research real system building experience enabled get started research stanford right away 
family continued support encouragement 
bill mclaughlin real brothers law help putting final version thesis care administrative details 
wife sylvia owe gratitude give deepest love 
research generously supported national science foundation sun microsystems ibm apple computer cray laboratories tandem computers ncr texas instruments digital equipment 
vi vii table contents chapter 
self language research outline chapter language features implementation problems 
data types benefits programmers implementation effects object oriented programming benefits programmers message passing inheritance implementation effects traditional compromises user defined control structures benefits programmers implementation effects traditional compromises safe primitives benefits programmers implementation effects traditional compromises generic arithmetic benefits programmers implementation effects traditional compromises summary chapter previous 
smalltalk systems smalltalk language deutsch schiffman smalltalk system typed smalltalk ts optimizing compiler atkinson hurricane compiler summary statically typed object oriented languages fast dispatch mechanisms trellis owl emerald eiffel summary viii scheme systems scheme language language orbit compiler shivers control flow analysis type recovery summary traditional compiler techniques data flow analysis interpretation partial evaluation common subexpression elimination static single assignment form wegman node distinction procedure inlining register allocation summary chapter self language 
basic object model object syntax message evaluation message syntax blocks implicit self sends primitives example cartesian polar points chapter overview compiler 
goals approach representation level type information key interface level type declarations help transforming polymorphic monomorphic code organization compiler chapter system architecture 
object storage system maps segregation garbage collection parser run time system stacks blocks summary ix chapter inlining 
message inlining assignable versus constant slots heuristics method inlining length checks recursion checks speeding compile time message lookup inlining primitives summary chapter customization 
customization customization dynamic compilation impact dynamic compilation compiled code cache lru cache flushing support customization static compilation customization partial evaluation line caching line caching customization line caching dynamic inheritance line caching performs summary chapter type analysis 
internal representation programs type information control flow graph names values types map types constant types integer subrange types unknown type union types exclude types type analysis assignment nodes merge nodes branch nodes message send nodes primitive operation nodes type casing type prediction block analysis deferred computations analysis exposed blocks common subexpression elimination eliminating redundant arithmetic calculations eliminating redundant memory eliminating unnecessary object creations initializations summary chapter splitting 
motivating example reluctant splitting splittable versus unsplittable union types splitting multiple union types simultaneously paths splitting merge nodes splitting branch nodes implementation paths heuristics reluctant splitting costs weights cost weight thresholds related eager splitting divided splitting lazy compilation uncommon branches summary chapter type analysis splitting loops 
pessimistic type analysis traditional iterative data flow analysis iterative type analysis compatibility initial loop head type information iterating analysis iterative type analysis splitting summary chapter back compiler 
global register allocation assigning variables names live variable analysis register selection inserting register moves common subexpression elimination constants xi eliminating unneeded computations filling delay slots code generation portability issues summary chapter programming environment support 
support source level debugging compiler generated debugging information virtual physical program counter translation current debugging primitives interactions debugging optimization support programming changes ways supporting programming changes dependency links invalidation summary chapter performance evaluation 
methodology benchmarks hardware charting technique performance versus languages relative effectiveness techniques remaining sources overhead summary chapter 
results applicability techniques appendix object formats 
tag formats object layout appendix detailed performance evaluation 
detailed description benchmarks measurement procedures relative effectiveness techniques inlining caching compile time message lookups customization xii line caching type analysis value type analysis integer subrange analysis type prediction block analysis deferring block creations exposed block analysis common subexpression elimination register allocation eliminating unneeded computations filling delay slots summary effectiveness techniques splitting strategies reluctant splitting strategies lazy compilation uncommon branches vector type prediction eager splitting strategies divided splitting summary splitting strategies remaining sources overhead type tests overflow checking array bounds checking block zapping primitive failure checking debugger visible names interrupt checking stack overflow checking lru compiled method reclamation support summary remaining sources overhead additional space costs appendix raw benchmark data 
recur sumto tak sieve perm towers queens quick bubble tree oo perm xiii oo towers oo queens oo oo quick oo bubble oo tree puzzle richards parser primmaker pathcache bibliography 
xiv chapter programming language designers searching programming languages features ease programming process improve programmer productivity 
promising current approach object oriented programming 
object oriented programming languages provide programmers powerful techniques writing extending reusing programs quickly easily 
object oriented languages typically involve sort inheritance implementation allowing programmers implement new data types terms implementations existing data types sort message passing invoke operations objects unknown implementation 
object oriented languages designed implemented including smalltalk gr str es trellis owl eiffel mey mey mey modula nel har clos bdg ra sla ram 
unfortunately traditional implementations object oriented language features particularly message passing slower traditional implementations non object oriented counterparts gap run time performance limited widespread object oriented language features hindered acceptance purely object oriented languages 
language designers developed important language environment features improve programmer productivity 
class closures allow programmers define control structures iterators collection style data types exception handling routines 
generic arithmetic supports general numeric computation variety numeric representations explicit programmer intervention 
safe robust implementation performs necessary error checking ensure programs behave implementation dependent way get mystery core dump contain errors array access bounds stack overflow 
complete source level debugging helps programmers get programs working quickly significantly improving programmer productivity 
unfortunately languages implementations support desirable features historically high cost run time performance 
self language maximize potential benefits object oriented programming david ungar randy smith designed self programming language hcc cuch refinement simplification smalltalk language 
self incorporates purely object oriented programming model closures user defined control structures generic arithmetic support safe robust language implementation support complete source level debugging 
self described detail chapter 
ungar smith provide simple flexible language environment maximized expressive power productivity programmer 
self powerful features initially appeared implementation prohibitively inefficient fastest implementation smalltalk language include features self runs set small benchmark programs tenth speed optimized programs 
research goal described dissertation design build efficient implementation self stock hardware sacrifice advantages language environment 
achieving goal required develop new implementation strategies message passing closures user defined control structures generic arithmetic robust primitives source level debugging 
results surprisingly set benchmarks measure performance smalltalk programs indicate self programs run third half speed optimized programs roughly times faster smalltalk implementation 
new techniques practical self compilation speed roughly optimizing compiler self compiled code space usage usually factor optimized new implementation strategies overcoming obstacles performance self 
fortunately new techniques included implementations object oriented languages improve smalltalk trademark parcplace systems run time performance 
non object oriented languages incorporating user defined control structures generic arithmetic benefit new implementation techniques 
importantly hope techniques developed pave way purely object oriented languages designed implemented including compromises restrictions solely sake efficiency 
outline chapter dissertation describes benefits object oriented programming user defined control structures generic arithmetic support robust implementation associated costs run time performance 
outlines various compromises restrictions languages included achieve better run time efficiency 
chapter reviews related detail 
chapter describes self language 
chapters contain meat dissertation 
chapter presents goals outlines organization compiler 
chapter describes framework compiler functions including memory system architecture run time system 
early design implementation memory system described lee thesis lee 
chapters bulk new techniques developed improve run time performance 
techniques include customization type analysis type prediction splitting 
earlier designs implementations techniques described papers cu cul cu cu 
chapter describes compiler support self programming environment particular techniques mask effects optimizations inlining splitting self programmer debugging techniques described papers cul hcu 
section contains detailed outline part dissertation 
performance self implementation analyzed chapter 
analysis measures various configurations implementation identifies individual contributions performance particular techniques 
compile time space costs system individual techniques analyzed 
chapter concludes dissertation outlines areas 
chapter language features implementation problems chapter describes desirable language features included self data types message passing inheritance user defined control structures error checking primitives generic arithmetic 
feature describe advantages programmers adverse implementation consequences typical compromises languages sake efficient implementation 
readers familiar language features implementation challenges may choose skim chapter 
data types benefits programmers ability describe manipulate data structures central expressive power language 
traditional programming languages kr pascal jw include record array data type declarations lisp wh ste prolog ss functional programming languages mth wik pey include cons cells 
type declarations build concrete data types 
manipulating concrete data structures simply matter extracting fields records cons cells indexing arrays 
data types lz lsas lab lg provide expressive mechanism describing manipulating data structures 
data type abstracts away concrete data type providing set operations interface clients manipulate objects type 
data type implemented terms lower level data type representation implementation hidden clients data type abstraction boundary 
example canonical data type stack data type supporting create push pop top isempty operations represented array stack elements integer top stack index 
enforced abstraction boundary provides advantages implementors clients data types traditional concrete data types 
implementors free change representation data type long interface remains clients data type remain unaffected 
example stack data type reimplemented linked list place array integer clients unaffected 
data types encapsulate design decisions may change especially representation critical data structures 
clients data types provide natural interface manipulating data structures language primitives concrete data structures 
operations data types directly reflect conceptual operations data type programmer mind translated series extraction indexing operations 
stack example clients may natural push pop operations place array indexes integer increments 
operations improve reliability system adding stack data type create push pop isempty top abstraction boundary external representation operations clients element stack implemented single place debugged repeated client call 
data types provide principle organizing programs 
data types task programming application tends revolve identifying designing implementing data types 
applications orientation better traditional orientation top refinement procedures functions wir 
addition libraries common data types developed may reused applications reducing development maintenance costs 
implementation effects widespread data types greatly increases frequency procedure calls traditional programming styles concrete data types 
manipulation concrete data type record field extraction array indexing built language construct easily implemented simple compilers machine instructions 
data types manipulation conceptually procedure call invokes programmer implementation operation 
system heavy data types operations implemented programmer just call lower level operations representation data type magnifying overhead data types 
eliminate run time cost abstraction implementations expand body called procedure place procedure call technique known procedure integration inlining 
operation data type invoked compiler expand implementation operation data type line eliminating procedure call 
aggressive inlining overhead data types virtually eliminated removing performance barrier discourage important program structuring tool 
inlining depends fact program single implementation particular data type condition exist object oriented programming message passing described section 
implementation data type changes program may need recompiled inline new operation implementations 
non inlining implementations data types amount relinking changing implementation data type usually necessary 
inlining implementation operation place call compiler ways violated abstraction boundary data type 
fortunately compiler need follow restrictions human programmers violation quite reasonable 
abstraction boundaries great people help organize programs serve little purpose implementation 
object oriented programming benefits programmers object oriented programming languages improve data types provide objects classes weg 
object oriented languages typically include features languages data types message passing inheritance 
message passing data types clients insulated implementation details data types allowing implementor data type replace implementation data type new rewriting client code 
unfortunately implementation data type exist system single time changing implementation data type compile time operation requires recompiling relinking application new implementation 
object oriented programming languages rectify problem allowing multiple implementations data type coexist application run time 
client code depend implementation data type accessed fact different implementations data type manipulated different times client code 
example array stack implementations stacks manipulated clients interchangeably 
flexibility sense operation invoked call determined dynamically actual implementation call 
instance invoking push operation stack object oriented terminology sending push message procedure gets run method implements message depends implementation stacks operated 
stack passed argument push operation receiving push message linked list stack linked list stack specific push method invoked stack array stack array stack specific push method run 
different calls different stack implementations determination push implementation invoke determined dynamically run time 
message passing arguably key expressive power object oriented programming 
statically typed language variables associated types typically explicit programmer declaration automatic compiler inference type variable describes operations may performed data values objects stored variable 
traditional languages including data types data values stored variable type variable 
object oriented languages clients manipulate objects different implementations interchangeably message passing restriction types objects stored variable relaxed object stored variable long object supports operations expected variable declared type 
object stored variable provide operations expected variable subtype variable declared type extra operations ignored client code 
example client code operates stacks say sending push pop messages continue operate correctly object supports push pop messages double ended queue supports stack operations additionally push bottom pop bottom operations add remove elements opposite stack 
general various types objects system form lattice general types types fewer required operations higher lattice specific types lower lattice 
object oriented languages type lattice restricted implementation inheritance graph implementation inheritance described section 
looser connection object oriented languages statically declared type variable actual run time type contents variable enabled message passing dynamically select appropriate implementation call dramatically increases potential reusability applicability client code 
clients abstracted implementation representation issues specifying operations required objects explicitly static type declarations implicitly operations invoked implementation objects precise interface data type objects 
level abstraction limits dependencies clients implementations just strictly required correctly stack object create push pop isempty top double ended queue object create push pop isempty top push bottom pop bottom bottom interchangeable clients sending push pop performing client task allows client code implementations written imagined time client code written 
inheritance frequently data types may similar implementations 
commonality may separated factored third data type shared inherited original data types 
example linked list implementation double ended queues may similar linked list implementation stacks consequently programmer factor similar parts third module inherited linked list stacks linked list double ended queues 
fact double ended queue inherit directly stack implementation third shared implementation necessary 
situation stack implementation play role reusable implementation implementation maintenance double ended queue easier 
factoring enables programs modified easily copy code changed changes factored code automatically propagated inheriting data types 
factoring facilitates extensions shared objects provide natural places new operations implemented automatically inherited similar data types 
example programmer add size operation stacks double ended queues automatically receive capability inheritance 
hierarchies related data types characteristic feature object oriented systems 
object oriented programming extends data type programming organizing principle programs supporting hierarchies implementations statically typed object oriented languages support hierarchies lattices interfaces types described previous section 
hierarchies offer focus initial design problem stack object create push pop isempty top double ended queue object create inherited push inherited pop inherited isempty inherited top inherited push bottom pop bottom bottom inherits catalog pre designed pre implemented components new applications build framework new components integrated available programmers 
implementation effects client manipulates object sending messages listed object interface 
object oriented languages allow implementations exist interface method invoked particular message send depends implementation receiver message 
implementation determined statically fact frequently may vary invocation 
system able determine correct binding message send site invoked method dynamically potentially call 
dynamic binding key expressive power object oriented programming chief obstacle performance object oriented systems 
dynamic binding incurs extra run time cost needed locate correct method implementation message receiver object 
lookup involves extra memory implementations eiffel hash table probe smalltalk trellis owl top normal procedure call overhead 
disastrous problem dynamic binding prevents inlining optimization reduce overhead data types 
inlining requires knowing single possible implementation operation 
requirement directly conflicts object oriented programming purposefully links client operation calls particular implementations invoke 
consequently general dynamically bound message sends inlined reduce call overhead 
inheritance slow execution requiring run time message dispatcher perform potentially lengthy search inheritance graph locate method matching message name 
consequently implementations message passing inheritance form cache speed search 
inheritance slow programs subtle way encouraging programmers write factored programs higher call density traditional programming styles 
overhead takes form messages sent self existed program factored inheritance 
traditional compromises pure object oriented language uses message passing computation include oriented features statically bound procedure calls built operators offers maximum benefit message passing inheritance 
unfortunately message passing slows procedure calls extra run time dispatching prevents crucial inlining optimizations needed reduce overhead abstraction boundaries 
supporting pure object oriented language inefficient existing practical implementations object oriented languages support completely pure object oriented model making various compromises name efficiency 
language designers compromise including non object oriented features extending existing oriented language object oriented features clos 
languages include builtin control structures data types base non object oriented languages base language features suffer performance problems associated object oriented features 
example includes built control structures available built data types integers arrays structures may manipulated traditional operators extra overhead 
mixed languages serious drawback code written non object oriented features benefit advantages object oriented features 
example programs written manipulate standard fixed precision bit integers arbitrary precision integers data types implement operations 
additionally code collections objects create collection fixed precision integers integers objects 
programmers hybrid language choose written reusable program run time performance 
languages supposedly pure object oriented languages data structures objects operations dynamically bound messages frequently cheat common language features effort improve performance 
example smalltalk widely regarded purest object oriented languages hard wires implementation definitions common operations applied integers preventing programmers changing implementation 
operations iftrue treated specially implementation dynamically bound operations single implementation messages built compiler changed overridden programmer 
simply built operations control structures smalltalk albeit written normal message sending syntax 
object oriented languages limit power instance variables parts representation objects fields records 
languages instance variables accessed directly methods object implementation sending messages self access instance variables 
accesses may implemented just load store instruction significantly faster normal dynamically bound operations 
unfortunately practice reduces potential reusability abstractions preventing instance variables overridden inheriting abstractions manner dynamically bound methods 
example polygon data type define vertices instance variable containing list vertices making polygon 
programmer wish define rectangle data type inheriting polygon data type new representation integers defining top bottom left right sides rectangle 
programmer rectangles compatible polygons overriding vertices instance variable vertices method computed list vertices integer instance variables 
unfortunately object oriented languages overriding instance variable possible rectangle inherit directly polygon pictured 
trellis owl self notable exceptions unfortunate practice 
object oriented languages static typing usually restrict type lattice inheritance graph object inherits child object subtype parent type subtype inherit 
restriction may justified language simplification 
languages go restricting inheritance graph form tree object may inherit object 
restriction single inheritance simplifies implementation allowing relatively efficient implementations dynamic dispatching indirect procedure calls implementation virtual function calls versions supporting single inheritance 
unfortunately subtyping tied inheritance implementation single inheritance limiting programmers 
general types comparable printable easily defined supertypes appropriate objects particular object subtype type 
supporting multiple supertypes object imposes significant extra run time overhead message sends existing implementation technology described chapter 
polygon object polygon operations rectangle object inherits vertices top bottom left right polygon rectangle operations user defined control structures benefits programmers programs smaller powerful language allows arbitrary chunks code passed arguments operations 
chunks code called closures blocks ss ste enable programmers implement iterators exception handlers sorts control structures 
example stack data type provide operation called iterate take closure argument 
operation iterate elements stack invoking closure element turn 
arrangement similar traditional loop defined entirely programmer data types closures 
body loop closure lexically scoped meaning access local variables scope defined caller iterate operation 
stack data type provide operation named take closure argument pop stack empty invoke closure handle empty stack error 
situation closure act exception handler 
closures typically provide way prematurely exit computations class continuations scheme rc hdb non local returns smalltalk self 
returning non locally closure returns caller operation lexically enclosing operation caller 
non local returns effect similar return statements closures support traditional control structures including loops loops case statements 
object oriented language basic control structures statements may completely implemented closures messages implementation message true object different false object instance 
closures enable pure object oriented languages defined built control structures message passing non local returns sort primitive loop tail recursion operation simplifying language moving definition control structures domain programmer 
implementation effects unfortunately straightforward implementations control structures closures introduces run time overhead traditional built control structures 
allocation deallocation closure objects bog user defined control structures compared built control structures usually implemented compare branch sequences 
allocation deallocation cost especially significant extremely simple control structures statements 
looping statements allocation deallocation cost amortized iterations body loop extra procedure calling cost invoking methods comprising user defined control structure invoking closure object time loop incurs significant amount overhead instructions execution comparable built control structure 
self system traditional loop runs methods execution control structure invoked iteration loop 
pure object oriented languages procedure calls really dynamically bound messages cost greater especially inlining user defined code implementing control structure harder 
traditional compromises difficulty efficient implementation languages support closures user defined control structures 
include built control structures require programmers build iterator data structures 
trellis owl provide built iterators exceptions supporting common uses closures 
kinds user defined control structures go simple iteration exception handling control structures implemented directly trellis owl 
scheme provides class closures continuations provides number built control structures built control structures implemented efficiently invoked concisely general user defined control structures closures 
scheme programs rely heavily built control structures get performance 
smalltalk nominally relies entirely user defined control structures blocks smalltalk term closures 
unfortunately mentioned section smalltalk restricts common control structures iftrue compiler provide efficient implementations create block objects run time 
primary disadvantage restrictions point view smalltalk programmer large performance differential restricted control structures optimized compiler control structures defined user programmer fast control structures appropriate control structures 
implementation compromise discourages abstraction 
safe primitives benefits programmers leaves call graph program primitive operations built system object creation arithmetic array accessing input output 
frequently primitive operation defined particular types arguments 
example arithmetic primitives defined numeric arguments array access operations defined arrays integer indices bounds corresponding array 
procedure calls considered primitive operations legal long stack space new activation records 
environments especially compiled optimized environments programmer responsible ensuring primitive operations invoked legal arguments 
program contains error leads primitive invoked illegally system corrupted probably crash execution 
example implementations check array accesses bounds bounds store array corrupt internal representation object 
subsequent behavior system unpredictable 
programs developed unsafe systems extremely difficult debug programming errors lead seemingly random behavior far away time space cause errors 
hand safe robust system verifies invocation primitive operation arguments legal primitive performed completion error 
primitive call illegal robust system halts gracefully example entering debugger invokes user definable routine closure enabling programmer handle error 
robust programming systems program development easier catching programming errors quickly giving programmer better chance identifying cause illegal invocation 
robust system internally corrupted result programming error penalty errors greatly reduced speeding debugging process 
implementation effects implementing safe primitives requires type checking range checking array accesses bounds arguments primitives 
statically typed non object oriented languages type checking primitive arguments done compile time 
object oriented languages dynamically typed languages type checking general performed statically incurring extra run time overhead 
runtime range checking general optimized away statically typed non object oriented languages 
traditional compromises languages provide completely robust primitives 
languages check types arguments primitives compile time statically typed languages run time dynamically typed languages check array bounds option 
systems handle procedure call stack overflow gracefully 
generic arithmetic benefits programmers languages incorporate multiple numeric representations integers floating point numbers various ranges precisions 
representations offer different trade offs accuracy efficiency 
languages allow numeric representations freely mixed programs support automatic conversion numeric representation 
example language supporting kind generic arithmetic include arithmetic primitives handle overflows underflows returning results representations greater range precision original arguments primitives providing means programmers implement conversion routines 
languages generic arithmetic relieve programmer burden dealing numeric representation issues 
code written numeric representation mind automatically reusable numeric representations explicit programmer interactions 
implementation effects generic arithmetic imposes significant run time overhead 
system perform extra run time dispatching select implementation numeric operation appropriate representation arguments 
dispatching overhead similar imposed message passing fact generic arithmetic viewed object oriented subpart language albeit non object oriented language may 
generic arithmetic requires extra run time checking overflows underflows 
overflows underflows impose serious indirect cost overlooked calculating cost generic arithmetic 
representation result arithmetic operation may different representation operation arguments compiler general statically determine representation result numeric operation compiler determined representation arguments 
example compiler knows type arguments operation represented standard machine integers result may represented arbitrary precision integer overflow occurs 
overflow checking limits effectiveness traditional flow analysis track representations numeric quantities 
traditional compromises costs languages support generic arithmetic 
provide alternative representation specific arithmetic operations avoid run time overhead associated generic arithmetic sacrifice safety expressiveness generic arithmetic 
summary object oriented languages provide number important enhancements traditional procedural programming languages data types message passing inheritance 
user defined control structures enhance data type model coupled object oriented features eliminates need built control structures 
safe primitives effective development environment 
support generic arithmetic increases programmer power program reliability 
unfortunately desirable language features don come cheap 
impose significant implementation costs particularly run time execution speed 
data types user defined control structures conspire dramatically increase frequency procedure calls dynamic binding increases cost procedure calls prevents direct application traditional optimizations procedure inlining 
generic arithmetic safe primitives increase expense basic operations leaves call graph 
standard approach solving problems existing language implementations cheat 
data types compromised distinguishing variables functions interfaces 
common control structures operations data types built language definition forcing programmers choose reusable malleable programs execution speed 
generic arithmetic support non existent expensive error checking primitives name execution speed 
self includes features described chapter important desirable language features 
self language described detail chapter 
unwilling cheat get performance 
dilemma driving force led research described dissertation 
chapter previous chapter describe previous software solutions optimizing execution speed object oriented languages 
review techniques originally developed traditional non object oriented languages relate techniques optimizing self 
smalltalk systems commercial languages smalltalk closest self 
consequently efforts improve performance smalltalk programs probably relevant achieving performance self 
smalltalk language smalltalk incorporates language features identified previous chapter contributing expressive power implementation inefficiency data types message passing inheritance dynamic typing user defined control structures error checking primitives generic arithmetic gr 
designers smalltalk included compromises definition language implementation smalltalk easier implement efficiently 
smalltalk object oriented languages treats variables differently methods 
variables accessed directly special mechanism avoids costly message send 
unfortunately run time benefit comes significant cost subclass longer override superclass instance variable method vice versa 
restriction prevents certain kinds code reuse class inheriting code superclass changing part representation 
canonical example described section programmer wants define rectangle class subclass general polygon class provide specialized representation rectangles differs polygons 
visible compromise sacrifices purity object oriented model introducing built control structures operations 
definitions common methods hard wired compiler including integer arithmetic methods boolean methods iftrue block iteration methods 
restricting semantics flexibility messages smalltalk compiler implement efficiently low level sequences special byte codes message sending overhead 
compromises significantly improve run time performance sacrifice purity flexibility language model 
programmers longer able change definitions hard wired methods extend instrument 
compiler assumes single system wide definition example new definitions added programmers ignored 
programmers define identity methods new object classes 
similarly messages iftrue iffalse block literals arguments compiler assumes receiver true false object allowed receiver programmer provides implementation iftrue iffalse object 
clearly restrictions compromise simple elegance extensibility language programmers inflicted completely unexpected behavior violate assumptions 
worse large difference performance handful control structures hard wired implementation remaining control structures written programmer programmers tempted faster built control structures inappropriate 
temptation lead programs malleable reusable 
better solution develop techniques improve performance user defined control structures uniformly including written programmer encourage programmer maintain high level abstraction 
deutsch schiffman smalltalk system definition smalltalk specifies source code methods translated byte codes machine instructions stack machine 
originally smalltalk ran xerox implementing instruction set microcode deu 
subsequent software implementations smalltalk stock hardware supplied virtual machine interpreted byte codes software 
needless say interpretation quite slow kra 
additionally smalltalk activation records defined implemented class objects allocated heap garbage collected longer referenced 
design slowed implementation method call return cost allocate eventually deallocate activation record objects 
peter deutsch allan schiffman developed techniques implementing smalltalk programs better interpreters specialized hardware support ds 
overcame interpretation overhead introducing extra invisible translation step virtual machine byte codes native machine code system directly executing native machine code interpreting original byte codes 
translation primarily eliminates overhead interpreter decoding byte code dispatching appropriate handler 
deutsch schiffman system dynamic compilation called dynamic translation deutsch schiffman byte codes translated machine code demand run time compile time 
dynamic compilation number advantages traditional static batch compilation 
compiler compile code gets reducing total time compile run program 
programming turn minimized program execution immediately programming change waiting compiler recompile changed code recompilations deferred needed run time 
dynamic compilation compiled code treated cache compact byte coded representation programs 
compiled method code thrown away flushed cache save compiled code space 
method needed simply recompiled byte coded representation 
deutsch schiffman compiler fast recompiling method byte code form faster paging compiled code disk caching technique especially useful machines small amounts main memory 
deutsch schiffman system optimizes method call return initially stack allocating activation records 
program begins manipulating stack allocated activation records class objects running debugger external activation record exist activation record return nested block outlives lexically enclosing activation record system promotes activation records stack heap preserving illusion activation records heap allocated 
activation records get promoted heap performance method call return approaches performance procedure call return traditional language implementation load garbage collector greatly reduced 
reduce cost dynamic binding deutsch schiffman introduce technique called line caching 
observe message send call sites class receiver message remains call 
symptomatic monomorphic code code polymorphism afforded dynamic typing dynamic binding actively 
take advantage situation call instruction originally invoked run time message lookup system overwritten call instruction invokes result lookup 
effectively replaces dynamically bound call statically bound call eliminating overhead run time message lookup system 
course static binding necessarily correct 
class receiver message change case result message lookup different 
handle situation deutsch schiffman system prepends prologue method compiled code checks see class receiver correct method 
class correct normal run time message lookup system invoked backing mistakenly called method 
call instruction acts cache entry big called methods 
hit rate high cost checking cache hit low system performance improved 
method may inherited receiver class receiver class correct cached method may expensive computation perform 
deutsch schiffman solve problem storing receiver class data word call instruction instruction 
method prologue simply compares current receiver class stored word call site reachable return address call 
static binding correct method executed 
different prologue calls run time lookup routine locate method new receiver class 
call system lookup handler name area area method rectangles call system lookup handler name area area method rectangles area method circles rcvr correct 
rcvr correct 
approach lower hit rate possible different class invoke method allows relatively fast check cache hit 
depending memory system organization technique requires memory extra instructions verify line cache hit 
techniques faster garbage collection strategy called deferred counting db significant improvements performance smalltalk systems stock hardware 
run time performance deutsch schiffman implementation close twice speed interpreted version smalltalk 
techniques plus compromises language definition commonly operations control structures performance smalltalk programs markedly slower implementations traditional statically typed non object oriented languages 
described chapter current performance implementation smalltalk set small benchmarks roughly times slower optimized measured sun workstation 
order magnitude performance difference unacceptable programmers certainly major reasons smalltalk widely 
deutsch schiffman techniques completely transparent user 
dynamic translation byte codes native code stack allocation activation records performance optimizations hidden user smalltalk programmers think simply getting better interpreter 
user intervention required invoke compiler optimizations user programming model remains level interpreting debugging source code 
speed translation byte codes machine code fast hard notice pauses compilation 
systems attempt achieve level unobtrusiveness 
typed smalltalk ts optimizing compiler researchers noted chief obstacle improving performance smalltalk programs lack representation level type information base optimizations procedure inlining joh 
effort improve speed smalltalk programs ralph johnson group university illinois urbanachampaign designed extension smalltalk called typed smalltalk joh mcc hei gra gj 
added explicit type declarations smalltalk built optimizing compiler called ts uses type declarations improve run time performance 
type typed smalltalk possibly singleton set classes signature 
variable declared set classes type guaranteed contain instances classes included set 
allow variable contain instance subclass listed classes subclass appear explicitly list 
signature type listing set messages may legally sent variables type 
object understands required messages stored variable declared signature type independent implementation 
signature type converted set classes type replacing signature classes compatible signature translation depends particular definition class hierarchy may change class hierarchy altered 
kinds types perform static type checking typed smalltalk programs ts optimizing compiler exploits set classes types ways improve run time performance 
variable declared contain instances single class set classes singleton set compiler statically bind message sent contents variable corresponding target methods 
similarly primitive operation expects arguments particular class check may performed compile time run time type argument singleton set 
methods user marked may inlined invoked statically bound call site 
set classes singleton set small say members set ts compiler performs case analysis 
compiler generates type testing code case class receiver run time branching sections code section member set classes type declaration 
exact class receiver known statically arm case allowing message statically bound inlined user marked target method 
case analysis usually takes compiled code space original message send potentially inlined versions message send compiled run time performance message improved especially message inlined case arms 
control flow rejoins arm case 
class types typed smalltalk parameterized classes array integer 
case analysis expensive compiled code space possibly execution time number possible classes large 
ts compiler falls back deutsch schiffman technique line caching possible classes 
line caching messages sent objects signature types types tend match different classes 
reduce burden programmer specifying static type declarations typed smalltalk system includes type inferencer invoked automatically infer appropriate type declarations routine 
programmer provide type declarations instance variables class variables justin graver states dissertation type declarations strictly necessary inferencer compute appropriate type declarations method arguments results locals 
unfortunately inferencing process complicated dynamic nature control flow languages dynamic binding messages methods existing smalltalk programming practice virtually requires flow sensitive type checking 
handle problems typed smalltalk type checker uses interpretation top level expressions combine inferred method signatures control flow required evaluate top level expression 
quite powerful style type checking slow worst case interpretation takes time exponential size smalltalk system process may need repeated new top level expression 
system infers signature types set types new type declarations useful ts optimizing compiler 
manual type declarations rule typed smalltalk comes improving run time performance 
ts compiler front translates typed smalltalk code relatively machine independent language rtl back optimizes rtl instructions converts native machine code 
primitive operations may written programmer register transfer language supporting user extensible system allowing inlining calls primitive operations mechanisms inlining statically bound message sends 
standard optimizations performed back compiler including common subexpression elimination copy propagation dead assignment elimination dead code elimination various peephole optimizations 
interesting extension constant folding included ts back called constant conditional elimination optimization related splitting technique described chapter 
conventional constant folding conditionals conditional expression evaluated true false compile time compiler eliminate test true false branch 
ts extension handles cases conditional expression compile time constant basic block containing test conditional expression compile time evaluable block predecessors 
situation algorithm called rerouting predecessors copies basic block containing test predecessor evaluate conditional expression compile time redirecting predecessor flow copy 
test eliminated copied basic block original basic block fewer predecessors 
opportunities optimization occur frequently smalltalk code self code 
common sequence smalltalk self form 
smalltalk self version standard conditional expression 
straightforward smalltalk compilers generate code displayed control flow graph sequence code loads true object false object common case arguments integers immediately tests true object false object 
test eliminated directly result message single constant 
rerouting predecessors copying tests possible outcomes tests turned constant conditionals predecessors eliminated subsequently loads true false may eliminated dead assignments leaving code comparable quality optimizing compilers initial type tests executed 
int 
int 
true false true 
false 
iftrue 
nop int 
int 
true false true 
false 
iftrue 
nop published performance results early version ts optimizing compiler indicated adding explicit type declarations set classes variety implementing optimizations described led performance improvement times tektronix smalltalk interpreter small examples tektronix workstation 
rough calculations speed deutsch schiffman smalltalk implementation similar machine indicate ts optimizing compiler runs typed smalltalk programs twice fast deutsch schiffman system runs comparable untyped smalltalk programs 
unfortunately times slower optimized implementation typed smalltalk complete 
serious limitation support generic arithmetic disabled overflow checking performed limited precision integers instances smalltalk type checker assumes result limited precision integer arithmetic operation limited precision integer 
additionally line caching currently implemented message sends type receiver may couple classes executed system type checked 
small amount smalltalk system converted typed smalltalk adding type declarations small benchmarks may executed full test type system implementation techniques performed 
disadvantage typed smalltalk approach users add type declarations programs fine 
see real performance improvements type declarations set classes variety users tempted set small possible achieve best speed ups limiting reusability code 
addition users need annotate small commonly methods ts compiler inline calls 
disadvantage typed smalltalk system compiles slowly 
disrupts user illusion sitting smalltalk interpreter 
johnson notes implementation fully tuned compile time performance annotated type declarations optimized 
johnson speculates improvements ts compiler run fast deutsch schiffman compiler skeptical optimizing compiler written typed smalltalk run fast simple compiler written deutsch schiffman compiler 
atkinson hurricane compiler robert atkinson pursued approach similar typed smalltalk project attempting speed smalltalk programs atk 
devised type system similar typed smalltalk set classes types allowed smalltalk programmers annotate programs type declarations 
designed partially implemented optimizing compiler called hurricane uses types exactly way ts optimizing compiler compiler statically bind inline messages sent receivers singleton types statically bind inline messages sent receivers small set types casing type receiver techniques designed implemented 
typed smalltalk approach type declarations hurricane hints guarantees static type checking performed verify type declarations correct 
requires optimizations type declarations prefixed run time test verify type declaration provided programmer correct 
case declaration incorrect hurricane compiler generates code restart unoptimized untyped version method 
atkinson notes restarting severely limits kinds methods optimized presumably just side effects code verifies types variables 
optimizations inlining statically bound messages implemented fact optimizations deutsch schiffman compiler including inlining calls common primitives integer arithmetic omitted 
limitations hurricane implementation atkinson reports factor speed deutsch schiffman system small examples running sun workstation 
speed appears similar obtained ts optimizing compiler precise comparisons difficult groups different machines compare different baseline systems deutsch schiffman dynamic compilation system versus tektronix interpreter 
information available speed compiler 
interesting note atkinson implemented hurricane compiler single summer 
summary deutsch schiffman smalltalk system dynamic compilation line caching techniques represented state art began implementing dynamically typed purely object oriented languages 
unfortunately serious compromises purity language programming environment sake better performance deutsch schiffman smalltalk system runs small benchmark programs just tenth speed traditional language compiled optimizing compiler 
attempts boost performance smalltalk stock hardware rely explicit user supplied type declarations provide information compiler enables statically bind inline away expensive messages 
approach typically doubles speed small benchmark programs leaves sizable performance gap type annotated smalltalk traditional language sacrifices ease programming flexibility reusability original untyped code 
statically typed object oriented languages statically typed object oriented languages implemented section describe techniques languages achieve relatively performance 
statically typed class object oriented language str es 
original version included single inheritance versions extended support multiple inheritance 
contains embedded sublanguage incorporates built control structures data types 
user defined control structures generic arithmetic supported directly 
operations built data types checked compile time type correctness checks array access bound checks built operations robust 
current versions support parameterized data structures exceptions versions language probably 
statically bound procedure calls available performing operations objects messages dynamically bound target method annotated virtual keyword 
language properties enable programmers reduce performance penalty objectoriented programming features incur additional cost dynamic binding 
course benefits object oriented programming lost non object oriented alternatives selected 
equates types classes 
subclass may specify considered subtype superclass es inheriting superclass publicly privately compiler verifies subclass legal subtype superclass es 
single inheritance static type system severely hampers reusability code instances classes unrelated inheritance hierarchy manipulated common code classes provide correct implementations operations required code 
deficiency rectified addition multiple inheritance previously unrelated classes may extended additional common parent defining virtual functions operations required common code relating classes 
implementations incorporate special technique speed dynamically bound message passing enabled presence static type checking 
versions supporting single inheritance object contains extra data field points class specific array function addresses vtbl array 
method declared virtual index increasing order base class subclass overriding implementations index method override 
vtbl arrays class initialized contain addresses appropriate method method address corresponding index vtbl array 
implement dynamically bound message send compiler generates sequence instructions loads address receiver vtbl array loads function address vtbl array index associated message sent jumps function address call method 
calls overhead memory indirections call 
course additional overhead arises fact dynamically bound calls inlined statically bound call 
vtbl technique may extended versions support multiple inheritance 
trick embed object complete representation including vtbl pointer superclass pass address embedded object assigning object variable expects object superclass casting expression subclass type superclass type 
users object know object manipulate real object manipulating embedded object 
struct int virtual int struct int virtual int virtual int struct int virtual int virtual int virtual int vtbl vtbl vtbl virtual function implementation single inheritance version assignment goes implicitly invoking virtual function defined object superclasses assigning receiver message argument virtual function vtbl array implementation message dispatch need extended 
vtbl array extended array address embedded object offset pairs 
function address works single inheritance 
offset added address receiver object change pointer embedded subobject necessary 
overhead invoking method memory indirections addition ignoring overhead able statically bind inline away message send entirely 
roughly overhead inline caching technique fast smalltalk implementations inline cache gets hit vtbl array technique slow message sends receiver class changes send extra cache cost 
multiple inheritance case vtbl array technique requires additional add operations assignments subclass superclass 
supporting virtual base classes adds complication 
multiple inheritance form directed acyclic graph particular base class inherited derived class derivation path 
base class declared virtual copy virtual base class instance variables exist final object non virtual base classes independent copies instance variables distinct derivation path 
embedding approach directly virtual base classes general derivation paths virtual base class embedded implementations include pointer virtual base class representation subclass virtual base class shown diagram page 
struct int virtual int virtual int struct int virtual int virtual int struct int virtual int virtual int virtual int vtbl vtbl vtbl virtual function implementation multiple inheritance version vtbl assignments subclass virtual base class casts subclass base class simply add constant variable address perform memory indirection load virtual base class address object increasing overhead sorts assignments add instruction memory indirection 
virtual base classes written obscure specific feature 
fact class objectoriented languages define multiple inheritance instance variables superclasses virtual base classes 
consequently extra overhead associated virtual base classes incurred objectoriented languages multiple inheritance 
space overhead vtbl virtual function implementation includes space taken vtbl function arrays extra words object point vtbl arrays virtual base classes 
vtbl function arrays may shared instances class instances subclasses 
single inheritance scheme single vtbl array class length equal number virtual functions defined inherited class subclasses may longer vtbl arrays superclasses 
multiple inheritance particular class define vtbl arrays superclass isn superclass class list superclasses vtbl arrays twice big single inheritance version include offsets addition function addresses 
space overhead instance includes pointer class vtbl arrays single inheritance case possibly multiple inheritance case pointer virtual base class subclass virtual base classes object 
space overhead significantly single extra word object required line caching technique fast smalltalk implementations 
vtbl implementation message passing may pose problems languages need support garbage collection support garbage collection 
multiple inheritance layout scheme create pointers middle object point embedded superclass virtual base class easy way locating outermost non embedded object 
problematic fast garbage collection algorithms 
slowing garbage collector offset extra performance advantage implementation approach simpler scheme line caching produces pointers middle object 
fast dispatch mechanisms john rose describes framework analyzing vtbl array style message send implementations ros 
framework describe variations array lookup implementation rose carefully analyzes relative performance 
variations practical object oriented languages techniques implemented single inheritance probably fastest available short statically binding optionally inlining message send 
researchers proposed techniques single vtbl array languages multiple inheritance implementation multiple inheritance uses multiple vtbl arrays object pointer adjusting assignments method calls 
proposed techniques potentially waste space having entries vtbl arrays unused classes need single vtbl array pointer object single vtbl array class 
central idea techniques determine system wide mapping message names vtbl array indices message names defined object map index 
dixon graph coloring technique determine message names distinct indices report small amount space wasted empty vtbl array entries 
pugh propose novel extension allows negative indices pw 
extra degree freedom assigning indices wasting space saves significant amount space 
report wasted space flavors system classes fields technique versus wasted space conventional index assignment algorithm 
pugh technique couched terms laying instance variables object allow field accesses single memory indirection approach easily applied laying entries class specific array member function addresses allow virtual function calls extra memory indirections normal direct procedure call single inheritance implementations 
unfortunately usefulness techniques limited need examine complete class hierarchy prior assigning indices message names compiling code 
additionally adding new class may require system struct int virtual int virtual int virtual int struct virtual int virtual int virtual int struct virtual int virtual int virtual int struct virtual int virtual int vtbl vtbl virtual function implementation multiple inheritance virtual base classes vtbl vtbl vtbl vtbl vtbl vtbl wide assignment recalculated scratch new conflicts may introduced invalidate previous assignment 
dixon suggest scheme index assignments treated constants compiler global variables defined separately compiled module limiting recompilation overhead just separate module defines assignments 
scheme slows message lookup extra memory indirection load appropriate constant global variable location largely eliminating performance advantage global index allocation techniques 
basic vtbl array approach implementations faster versions described rely static type checking guarantee messages sent objects expect 
techniques may extended dynamically typed languages performing array bounds checking fetching function pointer function array checking actual message name name expected function extracted array 
tests fails message understood receiver object run time type error results 
extra cost run time checks may significant combined original cost additional memory indirections may expected average cost line caching 
trellis owl trellis owl statically typed class object oriented language supporting multiple inheritance 
trellis owl equates types classes subclass required legal subtype superclasses 
trellis owl includes conventional kinds built control structures plus type case control structure tests run time type expression 
trellis owl support user defined control structures closures provide iterators exceptions 
iterator user defined procedure invoked built control structure 
normal procedure iterator may successively yield series values returning 
yielded value assigned iteration variable local loop body loop executed yielded value 
control alternates iterator procedure loop body loop exits iterator returns normally 
iterators provide way user defined data types collections iterated conveniently element time preserving abstraction boundary caller data type 
exceptions provide way procedures signal non standard situation occurred callers handle exceptional situation point separate normal return point 
exceptions help organize programs streamlining code handling normal common case situations clearly distinguishing code handling unusual cases 
type case control structures iterators exceptions common uses closures 
building language implementation trellis owl include special techniques relatively efficient 
type case iterators exceptions extend traditional control structures useful ways user defined control structures expressed easily built control structures 
example exceptions defined trellis owl automatically terminate routine raises exception 
exceptions implemented general closures terminate routine restart continue discretion caller 
example limitations built control structures trellis owl iterators provide mechanism performing action uniformly element collection user defined control structure treat middle elements collection differently closures arguments 
class closures user defined control structures expressive simpler selection built control structures 
trellis owl supports object oriented data abstraction 
pure object oriented language operations objects performed message sends statically bound procedure calls language variables accessed solely messages 
trellis owl implementation concerned primarily eliminating overhead messages kil 
principle implementation technique trellis owl automatically compiles separate version source method inheriting subclass 
version receivers class exactly assumed copy 
allows compiler know exact type receiver copied method enabling compiler statically bind messages sent receiver 
static binding possible single shared version source method subclass free provide new implementations methods defined receiver 
additionally save compiled control structures pioneered clu language lab trellis owl descendant 
operations classes may invoked class constants variables contain classes effectively 
operations classes primarily create new instances 
code space compiled code method subclass superclass subclass method share compiled code superclass trellis owl compiler generates new compiled method differs compiled methods 
customization technique described chapter similar trellis owl copy compilation strategy 
trellis owl statically bind messages situations 
receiver message compile time constant message sent constant statically bound 
static type receiver message annotated subtypes preventing programmer defining subclasses overriding methods compiler statically bind message 
unfortunately subtypes declaration violates pure object oriented model explicitly preventing inheritance classes declared subtypes declaration classic example implementation efficiency concerns compromising clean object model 
fact efficiency benefits achieved compromising language definition simply checking link time class subclasses inferring subtypes declaration modifying source code 
message send statically bound compiler uses direct procedure call speed message dispatch 
target message instance variable accessor remember instance variables accessed messages trellis owl accessor method inlined increasing speed messages 
currently trellis owl inline methods common methods integer arithmetic 
message remains dynamically bound trellis owl uses line caching technique pioneered deutsch schiffman smalltalk system 
cache misses external hash table specific class receiver consulted target method 
trellis owl calling overhead dynamically bound messages compare favorably fast smalltalk systems 
unfortunately performance data available trellis owl 
implementation little traditional optimization optimization fast smalltalk systems copying methods receiver class 
implementors openly admit trellis owl performance near traditional languages report performance users 
emerald emerald statically typed pure object oriented language distributed programming hut hrb jul 
emerald unusual lacking classes implementation inheritance emerald objects completely self sufficient 
emerald include separate subtyping hierarchy versions include powerful mechanism statically type checked polymorphism bh 
emerald data structures objects way manipulate access object send message 
emerald just pure trellis owl 
unfortunately emerald sacrifices complete purity elegance sake efficiency manner similar subtypes declaration trellis owl 
common object types int real bool char vector string built implementation modified 
allows compiler statically bind inline messages sent objects statically declared built types improve efficiency cost reduced reusability extra temptation programmers misuse lower level data types 
messages user definable single system wide definition programmers redefine override definition 
restriction allows compiler generate line code generating full message send 
implementation techniques developed emerald address distributed systems including run time tests distinguish local objects remote objects optimizations eliminate tests 
emerald compiler includes flow insensitive type inferencer determine representation level concrete type expression expression literal variable known assigned expressions particular concrete type message send receiver known particular concrete type enabling compiler statically bind message send particular method bound method result literal 
simple interprocedural concrete type inference statically bind message sends eliminating run time message lookup support analyses determining object guaranteed remain local current node 
emerald compiler performs inlining user defined methods statically bound 
eiffel eiffel statically typed class object oriented language supporting multiple inheritance mey mey mey 
trellis owl eiffel equates classes types treats subclassing subtyping 
eiffel verify subclasses legal subtypes superclasses fact provides features violate standard subtype compatibility rules assumed statically typed object oriented languages 
eiffel provides support user defined control structures include exception mechanism assertion mechanism optionally checks assertions run time generating exception assertion violated 
eiffel uses dynamically bound message passing procedure calls includes explicit variables overridden subclasses eiffel allow methods overridden variables case accessed dynamically bound messages trellis owl 
speeds variable accesses cost reduced reusability polygon rectangle example section illustrate problems smalltalk applies equally eiffel 
unfortunately information published implementation techniques run time performance eiffel 
summary widely described efficient object oriented language 
efficiency comes embedded non object oriented language program achieves efficiency primarily avoiding object oriented features relinquishing performance overhead programming benefits associated features 
trellis owl emerald eiffel purely object oriented compromise purity restrictions designed enable efficient implementation 
implementations virtual function table implementation reduces overhead message passing normal direct procedure calls memory indirections cases add instruction depending system supports single multiple inheritance virtual base classes involved function table indices assigned local information globally system wide information 
unfortunately performance implementations relies static type checking verify messages sent objects understand directly applicable dynamically typed language self adding extra run time checking detect illegal messages sacrifice performance advantage held virtual function table implementation system line caching 
scheme systems scheme dynamically typed function oriented language descended lisp rc 
scheme supports language features described chapter desirable including closures generic arithmetic 
consequently insights construction efficient scheme implementations may help building efficient implementations object oriented languages particularly ones user defined control structures generic arithmetic 
conversely techniques developed implementing object oriented languages may useful implementing scheme 
section describes efficient scheme implementations 
scheme language scheme includes number built control structures data types operations 
addition scheme supports lexically scoped closures scheme programmers may build wide variety user defined control structures 
closures class data values may passed scheme programs stored data structures invoked time blocks self smalltalk 
scheme closures smalltalk blocks defined upwardly mobile may invoked lexically enclosing scope returned refer local variables defined enclosing scope 
closures key ingredient functional programming style higher order lexically scoped functions 
scheme supports class continuations 
continuation function object encapsulates rest program time created 
continuations may programmers build powerful control structures exception handlers coroutines backtracking searches 
non local returns self smalltalk specialized form continuation creation invocation 
lisps scheme supports generic arithmetic 
scheme directly object oriented includes sizable object oriented subsystem albeit extensible programmer 
primitive operations scheme robust performing necessary error checking detect programmer errors 
variables untyped checking general performed statically incurring additional run time cost 
unchecked versions common primitives available speed conscious programmer corresponding loss safety 
type specific unchecked versions arithmetic functions exist programmers willing sacrifice safety reusability quest speed 
language orbit compiler object oriented extension scheme ra sla ram 
includes features scheme including class closures continuations adds ability declare dynamically bound generic operations object structure types 
operation called object argument invokes implementation operation associated object default implementation operation may provided built data types objects implement operation 
pure object oriented language built data types procedures scheme available better hybrid languages built procedures inherited scheme defined dynamically bound operations programmer override statically bound procedure dynamically bound version 
programmer may turn system nearly pure object oriented language common practice relies heavily traditional built data types operators lisp cons car cdr redefined operations 
orbit compiler kranz respected scheme compiler kkr kra 
orbit compiler analyzes closures continuations attempts avoid heap allocation closures possible 
early measurements indicate orbit performance comparable lisps closures traditional languages pascal 
rule lisp benchmarks unsafe versions arithmetic operators achieve fast performance compiler optimize performance true generic arithmetic kra 
additionally user defined control structures benchmarks measured built control structures 
orbit optimize object oriented features kra 
shivers control flow analysis type recovery olin shivers developed set algorithms constructing relatively large control flow graphs scheme programs interprocedural analysis presence higher order functions closures shi 
resulting large control flow graph amenable traditional optimizations original small control flow graphs potentially boosting performance scheme programs achieved optimizing compilers traditional languages 
shivers developed technique type recovery scheme attempts infer types variables programs shi 
algorithm begins assignments constants known type propagates information extended interprocedural control flow graph subsequent variable bindings traditional data flow analysis described section 
proposes type information eliminate runtime type tests type checking primitive operations 
type analysis described chapter common proposal 
unfortunately techniques developed practical working system example system generate machine code small examples examined compiler quite slow 
scheme programs contain higher order functions object oriented interprocedural analysis construction extended control flow graph type recovery performed easily accurately presence dynamically bound message passing characteristic object oriented systems 
summary scheme supports seemingly expensive features self closures generic arithmetic robust primitives includes inexpensive alternatives programmers avoid expensive features 
scheme includes built control structures avoid overhead closures scheme systems include unsafe primitives type specific arithmetic operators avoid overhead generic arithmetic robust primitives 
course avoiding overhead expensive features programmer loses advantages flexibility simplicity safety 
techniques scheme systems relevant self including analysis closures avoid heap allocation analysis type information avoid run time tests 
traditional compiler techniques attempting self competitive performance traditional languages optimizing compilers need include possibly extend traditional optimizing compiler techniques self compiler 
section discuss conventional techniques relate techniques self 
data flow analysis done developing techniques optimize traditional statically typed non object oriented imperative programming languages fortran bbb pascal asu pw gup 
techniques revolve data flow analysis framework information computed procedure propagating information procedure control flow graph 
information computed data flow analysis may improve running time compiled code space consumption programs 
examples optimizations performed results data flow analysis include constant propagation common subexpression elimination dead code elimination copy propagation code motion loop invariant code hoisting induction variable elimination range analysis optimizations eliminating array bounds checking overflow checking 
techniques self compiler akin data flow analysis particular type analysis described chapter 
data flow analyzers propagate information usually form sets variables values control flow graph altering information propagates nodes control flow graph affect information computed 
analysis may propagate forwards backwards control flow graph depending kind information computed 
data flow analysis examines single piece straight line code basic block called local data flow analysis examines entire procedure called global 
data flow analysis complicated join points control flow graph merge nodes forward propagation branch nodes backward propagation 
join points information derived join predecessors may different 
data flow analysis algorithms combine information predecessors conservative approximation meaning matter path execution takes program information data flow analysis computes correct conservative precise possible may approximation 
conservativeness data flow analysis algorithms required order optimizations performed computed information preserve semantics original program 
ideally approximations close truth achieved compile time expense 
data flow analysis gets complicated presence loops 
loop entry point head loop forward data flow analysis acts join point time loop entry point reached information looping backwards branch computed 
common approach handling problem assumes best possible information loop branch analyzes loop assumption keeps loop information computed looping branch matches information assumed looping branch 
iterative data flow analysis finds best fixpoint information computed loop relatively expensive operation body loop times fixpoint 
iterative type analysis technique works similarly described chapter 
iterative data flow analysis extracts information arbitrary control flow graphs 
certain restricted kinds control flow graphs called reducible control flow graphs typically produced programmers traditional languages structured programming asymptotically faster complex techniques interval analysis extract similar kinds information 
unfortunately control flow graphs manipulated self compiler reducible especially splitting loops described chapter 
interpretation interpretation semantics approach data flow analysis problem cc 
framework semantics original programming language abstracted capture relevant information new semantics called non standard semantics 
program analyzed interpreting program non standard semantics 
course conservative approximation usually required able interpret non standard semantics original program bounded amount time approximation frequently captured formally way non standard semantics defined 
interpretation data flow analysis techniques statically analyzing programs particular analysis problem described elegantly interpretation 
partial evaluation partial evaluation technique optimizing program partial description input ss 
partial evaluator takes input program say circuit simulator written large class potential inputs particular input program say circuit description produces output new program optimized particular input simulator optimized particular circuit 
partial evaluation intended allow programming style single general program written optimized particular cases improving alternative style writing specialized programs 
partial evaluators produce optimized programs form interprocedural analysis propagating description input program entire program call graph advantage extra information heavy constant folding procedure inlining 
partial evaluation systems commonly produce multiple versions particular procedure called residual functions optimized particular calling environment procedure calls branch appropriate optimized residual function general presumably slower original function 
customization technique described chapter viewed partial evaluation run time information discussed section 
common subexpression elimination static single assignment form global common subexpression elimination important traditional optimizations 
standard approach common subexpression elimination uses data flow analysis propagate sets available expressions computations performed earlier control flow graph asu 
computing expressions available control flow graph node arithmetic instruction node compiler eliminate node result computed node available 
example result add control flow graph node added available expressions set node graph calculates value result earlier node available compiler replace second redundant calculation simple assignment node result earlier node result eliminated node common subexpression elimination comparing available expressions relies heavily determining expressions expressions equivalent 
expressions include add available add available available available variable names expect expressions 
assignment variable referenced available expression cast equivalence doubt expression removed available expression set variables expression assigned 
example assigned new value second computations expression considered unavailable second expression eliminated 
hand value assigned third variable second calculation referenced computations compute result available expression system need sophisticated track assignments detect computations produce result 
researchers developed techniques improve effectiveness determining expressions equal 
best current approaches static single assignment ssa form 
invariant maintained program ssa form variable assigned exactly definition variable reaches variable 
arbitrary programs transformed equivalent program ssa form replacing definition variable original program definition fresh new variable named adding subscript original variable name 
original variable changed appropriate subscripted variable 
preserve invariant exactly definition reaches merge points reached different subscripted variables original variable fresh new subscripted variable created merge assigned result function incoming subscripted variables pseudo assignments generate code simply preserve ssa invariant 
ssa form supports optimizations common subexpression elimination better traditional approach original variable names ssa form need take account assignments variables ssa renamed subscripted variables assigned 
assignments variables kill existing available expressions assignments guaranteed different variables renaming 
addition functions ssa form track expressions flow control structures supporting better identification constant expressions equivalent expressions turn enable common subexpressions eliminated 
described section self compiler performs global common subexpression elimination 
self compiler precisely ssa form values described section quite similar 
wegman node distinction mark wegman describes generalization standard code duplication code motion optimizations called node distinction weg 
differentiating criterion computable data flow analysis wegman technique splits nodes downstream potential merge point merging paths different values differentiating criterion 
traditional code motion techniques code hoisting novel techniques splitting nodes value boolean variable expressed framework 
node distinction remarkably similar splitting technique described chapter 
drawback node distinction described wegman differentiating criterion known advance prior data flow analysis node distinction 
splitting hand require advance knowledge suitable practical compiler 
relationship splitting node distinction explored section 
procedure inlining researchers worked improving performance procedure calls inline expansion bodies callees place calls technique known inlining procedure integration beta reduction 
languages including provide mechanisms programmer tell compiler inline calls particular routines 
sophisticated systems attempt determine automatically routines inlined sch aj hc rg mcf 
inlining particularly difficult transformation harder devise set heuristics control automatic inlining balancing compiled code space compilation time increases projected run time performance improvements operating correctly presence recursive routines 
chapter describe heuristics self compiler guide automatic inlining 
inlining possible target call known compile time inlining directly applicable purely object oriented systems messages dynamically bound 
techniques included self compiler designed enable common messages inlined away 
register allocation important techniques included virtually optimizing compilers global register allocation 
modern register allocators treat problem allocating fixed number registers larger number variables instance graph coloring interference graph 
nodes interference graph variables candidates allocation registers 
interference graph contains arc nodes corresponding variables simultaneously live point procedure allocated 
coloring graph assigning node color adjacent nodes color corresponds register allocation color represents distinct register 
fixed number registers register allocation goal coloring process find coloring interference graph colors registers 
unfortunately determining graph colorable colors np complete problem implementing graph register allocators real compilers involves developing heuristics usually find coloring interference graph reasonable amount time handling spilling variables memory coloring quickly cac cha ch lh ch 
practice nodes interference graph may portions variable lifetime particularly portions represent disconnected regions real data flow regions separate def chains allowing different parts variable lifetime allocated different registers 
additionally variables may coalesced single node variables simultaneously live variable assigned 
subsumption process eliminate unnecessary register moves graph harder color 
section describe implementation global register allocation self compiler discussing current strengths weaknesses 
summary implementation techniques developed exploited various language implementations objectoriented languages relevant quest efficient implementation self 
techniques speed dynamic binding 
effective techniques reduce dynamically bound message send statically bound procedure call determining class receiver compile time 
ts typed smalltalk compiler hurricane compiler type declarations determine types receivers messages statically bind possibly inline away messages sent receivers known single type compilers case analysis receiver may small set types 
trellis owl uses copy scheme additionally statically bind inline away messages sent self particular instance variable accesses 
static binding inlining away extra layers abstraction especially important pure object oriented languages 
techniques developed cases type receiver determined statically 
smalltalk systems trellis owl line caching technique speed message sends class receiver remains fairly constant resulting speed message send times slower normal procedure call 
cases line cache misses hash table locate target method quickly 
implementations implement message passing indirect array accessing technique 
approach exploits information class hierarchy produce mapping message name array index reducing cost message passing times cost normal procedure call single inheritance case times cost multiple inheritance case 
extensions approach reduce cost multiple inheritance scheme just cost single inheritance scheme cost wasted space significant extra compile time 
languages include built control structures data types operations ease burden implementation message passing 
smalltalk supposedly pure object oriented language built control structures includes critical compromises language design speed performance 
languages attempt purity processing takes place languages base non objectoriented sublanguage 
consequently speed object oriented features heavily optimized programmers concerned speed may code problems faster non object oriented facilities 
summary existing technology overhead dynamic binding eliminated limited number cases type receiver identified precisely 
message send statically bound target method current techniques imply direct overhead times slowdown speed message send statically typed language dynamically typed language 
techniques impose additional indirect overhead preventing inlining reduce cost extra abstraction boundaries introduced designed factored code user defined control structures cases indirect overhead damaging direct slowdown procedure calls 
impediments performance user defined control structures generic arithmetic robust language primitives significantly optimized implementations described 
consequently performance existing object oriented languages lags far performance conventional languages 
chapter self language self dynamically typed prototype object oriented language multiple dynamic inheritance originally designed david ungar randy smith xerox parc hcc cuch successor smalltalk programming language 
smalltalk self intended exploratory programming environments rapid program development modification primary goals 
self affording greater flexibility ease development modification cost reduced reliability existing implementation technology reduced run time performance 
additionally self includes features described chapter desirable object oriented language data types pure object oriented model dynamic binding messages including variable accesses closures user defined control structures exceptions robust primitives support generic arithmetic 
readers familiar self may choose skim chapter 
basic object model self object consists set named slots contains object 
slots may designated parent slots 
objects may self source code associated case object method 
new object self existing object called prototype simply cloned produce new object name value pairs prototype 
example picture portrays self objects 
bottom left object represents cartesian point instance containing slots parent slot named parent identified parent slot asterisk slot name containing point traits object slots named containing integer objects slots named contain assignment primitive method notated symbol described 
second cartesian point object lies right 
top left object labeled point traits inherited cartesian point objects 
contains parent slot named parent containing object shown diagram slot named print containing method object 
method objects differ objects attached self code addition slots 
method object parent slot named self argument slot contents filled receiver message method invoked described 
method additional argument slot named arg filled right hand argument message method invoked 
integer objects slots conciseness omit diagram 
kinds objects appear self object arrays byte arrays 
arrays just normal data objects additionally contain variable number array elements indexed number name 
parent cartesian point 
print parent point traits print 
clone arg print 
print 
self print method self arg arg method parent cartesian point names suggest object arrays contain elements arbitrary objects byte arrays contain integer objects range compact form suitable interacting external character byte stream systems 
primitive operations support fetching storing elements arrays determining size array cloning new array particular size 
object syntax programmer may describe self object textual form listing object slots code inside parentheses 
slots listed vertical bars object code components object may omitted 
slot declaration begins slot name asterisk slot parent slot left arrow equal sign depending respectively assignment slot desired expression evaluated determine slots contents 
assignable slot initialized nil may declared concisely omitting left arrow initializer expression 
slots separated periods 
example cartesian point object defined follows comments double quotes parent traits point 
evaluates point traits object example illustrates define single data slot define data slot assignment slot pair name assignment slot computed appending colon name data slot 
point traits object defined follows parent 
code evaluate parent point traits print print 
print 
print 
arg clone arg arg 
print method objects defined directly contents slots 
method objects look just object declarations specify code addition slots 
argument slots prefixed colons may initialized 
self defines syntactic sugar argument slots allows written part slot name slot declaration written follows arg clone arg arg 
self includes forms object literals including integer floating point literal expressions evaluate corresponding integer floating point objects string literals delimited single quotes 
message evaluation message sent object called receiver message receiver object scanned slot name message 
matching slot contents object parent slots searched recursively self multiple inheritance rules disambiguate duplicate matching slots 
example message sent cartesian point object pictured system search cartesian point slot name locating slot referring object 
print message sent cartesian point object system scan cartesian point object slot named print unsuccessfully 
system search object stored parent slot cartesian point example point traits object system find matching print slot parent object 
matching slot object referred slot evaluated result returned result message send 
object code evaluates slot holding acts variable 
example sending message cartesian point system locates slot point extracts contents current version self supports prioritized parents differing numbers asterisks different parent priorities 
details may cuch 
integer object evaluates case just returning object contains code evaluates returns result result original message 
object code method treated prototype activation record 
evaluated system clones method object fills clone self slot receiver message fills clone argument slots arguments message executes code 
example print message sent cartesian point system locate print slot point traits object extract print method object referenced slot evaluate method object 
evaluating method involve cloning method object create fresh activation record filling contents self slot new activation record receiver cartesian point object executing messages specified code associated print method 
result message print method returned result print message 
self supports assignments data slots associating assignment slot assignable data slot 
assignment slot contains assignment primitive method object takes argument 
assignment primitive evaluated result message send stores argument associated data slot 
data slot corresponding assignment slot called constant read slot opposed assignable data slot running program change value 
example parent slots constant slots 
self object model allows parent slot assignable just slot simply defining corresponding assignment slot 
assignable parent slot permits object inheritance change fly run time instance result change object state 
call run time changes object inheritance dynamic inheritance facility practical value self programming 
information uses dynamic inheritance may 
message syntax self message syntax smalltalk message syntax 
languages define classes message distinguished syntactically unary messages 
unary message takes arguments receiver 
syntactically unary message name written receiver expression postfix form distinguished forms message name sequence letters digits begins lower case letter colon 
print valid unary message names 
unary messages highest precedence associate left right 
binary messages 
binary message takes receiver argument binary message name separating 
binary message easily distinguished sequence punctuation characters excluding reserved sequences 
legal binary message names 
binary messages medium precedence 
associativity defined binaries programmers explicitly add parenthesis disambiguate sequences binary messages binary messages left associate binary message 
expressions legal evaluated expressions illegal explicit parenthesized 
arguments evaluated left right 
parent cartesian point 
print parent point traits print 
clone arg print 
print 
self print method self arg arg method print 
print 
print 
self print activation record clone keyword messages 
keyword message takes receiver arguments 
keyword message names unusual message name written interspersed arguments message 
piece keyword message name sequence letters digits letter colon unary messages colon 
aid limiting number parentheses required parsing keyword piece lower case letter subsequent keyword pieces upper case letter 
receiver written keyword message argument colon keyword message pieces 
name message concatenation various name pieces 
iftrue iftrue false legal keyword message names argument receiver third arguments iftrue iffalse 
keyword messages lowest precedence associate right left 
example message iftrue false sends iftrue false message result message arguments message iftrue iffalse sends message iffalse message argument iftrue message result message result iffalse message argument original message parenthesized iftrue iffalse 
code part method simply sequence period separated messages 
blocks self allows programmers build control structures blocks self version closures 
block self object slot named value contains special kind method 
invoked sending value block object special block method runs child lexically enclosing activation record activation record executing block object created 
block method include self parent slot anonymous parent slot refers lexically enclosing activation record object value self inherited enclosing method activation 
differences normal methods enable blocks block methods act lexically scoped closures self uses normal inheritance implement lexical scoping 
syntactically blocks identical method definitions enclosed square brackets parentheses 
particular variables local block activation record declared normal data slots slot list block literal 
example suppose method added point traits object 
method tests components receiver point positive returns string literal st quadrant 
string literal quadrant returned 
diagram shows state system invoking method creating new activation record 
block object corresponds block literal enclosed square brackets method 
block value slot refers block method object anonymous lexical parent slot refers block lexically enclosing activation record object 
parent cartesian point 

parent point traits self method clone iftrue st quadrant 
quadrant self activation record iftrue st quadrant 
quadrant parent value block 
parent block method st quadrant block method may terminate non local return prefixing result expression symbol reminiscent arrow causing result returned caller block method sender value caller lexically enclosing normal non block method 
non local returns effect return statement example executing non local return example block return sender value inside iftrue user defined control structure caller lexically enclosing method case returning st quadrant string object sender 
implicit self sends local variables arguments accessed self implicit self sends 
sends self receiver message search matching slot current activation record self 
search follow lexical chain activation records anonymous parent slots nested block methods 
arguments local variables simply normal slots method prototype objects cloned activation records implicit self message sends support argument local variable accesses mechanisms access data slots methods normal objects 
self parent slot outermost method activation record implicit self sends access slots receiver ancestors 
implicit self sends termed self receiver elided message send syntax message explicit receiver implicitly send self 
example point method contains fragment arg code sends message self implicitly 
lookup starts current activation record activation record contain slot system scan contents activation record parent slots 
activation record self slot parent slot system search contents self slot receiver cartesian point slot 
search successful system evaluate contents slot compute result message 
example code fragment send arg message self implicitly 
lookup current activation record time system find matching arg slot activation record 
contents arg local slot accordingly evaluated returning argument original message send 
implicit self messages allow self syntax local slot accesses slot accesses receiver concise syntactic expression local instance global variable accesses smalltalk powerful semantics full message sends 
particular code looks accessing instance variable originally access instance variable reused situations message invokes method 
possibility enables self system solve thorny reuse problems polygon rectangle example section cartesian polar point example described section 
primitives real self program performed primitive operations provided virtual machine implemented level language 
integer arithmetic array accessing input output provided primitives self programmer 
primitive operations invoked syntax send message message name begins underscore 
instance invokes standard integer addition primitive 
call primitive operation may optionally pass block invoked primitive fails appending message name passing block additional argument 
invoked block passed error string identifying nature failure overflow divide zero incorrect argument type 
example abc code 
passes failure block addition arguments added block invoked object primitive arguments primitive fixed precision integers 
loops implemented self restart primitive 
call restart transfers control back scope containing restart call creating loop 
programmer uses non local return break loop 
programmers combine restart non local returns closures build arbitrary user defined looping control structures 
self uses restart implement loops explicitly 
languages scheme perform tail recursion elimination automatically transform recursion iteration introducing extra language construct explicitly iteration 
unfortunately tail recursion elimination generally tail call elimination violates user execution debugging model eliminating activation records user expects see 
scheme language definition specifies tail recursive calls transformed iterations effectively introduces special language mechanism looping 
self looping code explicit easy recognize restart creates loop 
scheme hand procedure call happens tail recursive transformed iterative loop programmer desired expected identifying procedure call tail recursive tricky 
example cartesian polar points presents example collection self objects 
bottom objects dimensional point objects left ones represented cartesian coordinates right ones polar coordinates 
cartesian point traits object immediate parent object shared cartesian point objects defines methods interpreting cartesian points terms polar coordinates polar point traits object reverse polar point objects 
point traits object shared ancestor point objects defining general methods printing adding points regardless coordinate system 
point traits object inherits turn topmost object diagram defines general behavior copy objects 
sending message leftmost cartesian point object finds slot immediately 
contents slot integer evaluates associated code producing result message 
sending rightmost polar point object find matching slot immediately 
consequently object parent searched finding slot defined polar point traits object 
slot contains method computes polar point coordinate rho theta coordinates 
method gets cloned executed producing floating point result 
print message sent point object print slot defined point traits object 
method contained slot prints point object cartesian coordinates 
point represented cartesian coordinates messages implicitly sent self access corresponding data slots cartesian point object 
print method works fine points represented polar coordinates messages find conversion methods defined polar point traits object compute correct values 
example illustrates conventional self programming practice 
self code structured hierarchies traits objects objects hold behavior inherited refined child objects 
traits objects 
parent rho theta rho theta parent rho theta rho theta print arg parent parent parent 
clone 
cartesian point polar point cartesian point traits polar point traits point traits general traits rho theta cos rho theta sin sqrt arctan print 
print 
print clone arg arg parent cartesian point parent rho theta rho theta polar point play role similar roles classes class languages 
concrete objects inherit traits objects filling missing implementation assignable data slots holding object specific state information 
initial concrete objects prototypical instances data type cloned create new instances 
example illustrates challenges facing self implementation 
frequency message sends high print example nearly source token corresponds message send 
instance variables accessed message sends 
challenges facing implementation illustrated short example include user defined control structures generic arithmetic support 
chapter overview compiler chapter introduces design self compiler 
section describes goals 
section describes primary purpose new techniques developed part self compiler extraction representation level type information 
section relates novel parts self compiler traditional front back division compiler outlines topics covered chapters 
goals immediate goal build efficient usable implementation self stock hardware 
willing compromise self pure object model expressive features 
want preserve illusion system directly executing program programmer wrote user visible optimizations 
constraint consequences distinguish optimizing language implementations programmer free edit procedure system including basic ones definition integers iftrue booleans provide overriding definitions data types desired 
programmer able understand execution program errors program solely terms source code source language constructs 
requirement debugging monitoring interface system disallows internal optimizations shatter illusion implementation directly executing source program written 
programmers unaware programs get compiled optimized 
programmer isolated mere fact programs getting compiled 
explicit commands compile method program programming changes 
programmer just runs program 
illusion hiding compiler break programmer distracted mysterious pauses due compilation analysis optimization 
ideally pauses incurred implementation system imperceptible order fraction second interactive 
longer running batch programs interrupted longer pauses long total time program slowed programmer aware pauses 
constraints user visible semantics system main objective excellent run time performance 
wish self pure object oriented languages similar powerful features competitive performance traditional non object oriented languages pascal 
particular self program advantage object oriented features inner loop written just easily self want performance self close performance optimized performance goals met programmers may able switch traditional languages pure object oriented languages reap benefits afforded pure object oriented programming user defined control structures generic arithmetic robust primitives 
goals secondary constraint source level execution model goal rapid execution 
particular run time compile time space overheads concern run time speed 
modern computer platforms especially workstations typically equipped large amount physical main memory amount increasing rapid rate 
willing space straightforward implementation order meet execution speed goals 
began techniques available implement pure object oriented languages self efficiently relying special purpose hardware support cheating implementation objectoriented model introducing non object oriented constructs language 
main part involved developing implementing new techniques implementing object oriented languages efficiently stock hardware 
new techniques enabled largely meet goals faithfulness source code run time execution speed 
course wish implement self efficiently larger class self languages 
fortunately new techniques specific self language 
object oriented languages including eiffel trellis owl smalltalk clos benefit varying degrees techniques developed 
languages object oriented subsystems benefit including languages supporting form generic arithmetic lisp apl ive gr postscript ado icon gg languages logic variables prolog languages futures multilisp hal mul 
approach section describes approach achieving efficient implementation self similar languages 
representation level type information key dynamically typed object oriented programming languages historically run slower traditional statically typed non object oriented programming languages 
performance gap attributable largely lack representation level type information dynamically typed object oriented languages 
representation level information object embodied object class class system 
section describe maps internal implementation structures embody representation level type information prototype languages self 
compiler infer classes maps objects compile time eliminate run time overhead associated dynamic typing object orientation 
dynamically typed language compiler insert extra run time type checking code type safe primitives extra run time type casing code support generic arithmetic 
compiler infer classes arguments type checking primitive perform type checks compile time run time 
similarly compiler infer classes arguments generic arithmetic primitives perform type casing compile time generating code type specific arithmetic operation slower generic operation 
object oriented language compiler insert extra run time message dispatching code implement dynamic binding message names target methods run time class receiver 
compiler infer class receiver message perform message lookup compile time run time replacing dynamically bound message statically bound procedure call statically bound call subsequently amenable optimizations inlining described chapter significantly boost performance 
interface level type declarations help clearly run time performance dynamically typed object oriented programs dramatically improved compiler infer representation level type information form objects classes maps 
surface imply statically typed object oriented languages lots type information available compiler huge advantage performance dynamically typed counterparts 
surprisingly advantage fact quite small 
non object oriented language type variable specifies representation implementation contents variable 
static information corresponds knowing exact class contents variable supports optimizations described reduce gap dynamically typed object oriented languages statically typed non object oriented languages 
object oriented language interface level type declarations type variable specifies set operations guaranteed implemented objects stored variable 
interface level type deliberately specify objects stored variable implement operations order maximize generality reusability code 
interface level type information compiler perform optimizations require type information 
example knowing object understands message help compiler generate efficient code message knowledge object implements message executing method integers enables optimizations inlining markedly improve performance 
interface level type information useful special cases system wide knowledge 
compiler examine possible implementations system satisfy interface infer useful representation level type information 
example object class implements particular interface static interface level type information implies representation level information 
kinds optimizations speed operations basic data structures numbers collections implementations interface norm 
transforming polymorphic monomorphic code lack static representation level type information limits run time performance object oriented languages dynamically typed statically typed 
consequently new compilation techniques strive infer missing representation level type information compiler perform optimizations eliminate overhead dynamic typing object orientation 
optimizations performed task compiling dynamically typed object oriented program reduces task compiling traditional statically typed procedural program 
perform inlining self compiler prove receiver message single representation receiver expression monomorphic 
general self code polymorphic expressions may denote values different representations different times source code works fine representations 
polymorphism central power object oriented programming 
cases self program exploit full power polymorphism 
message receiver member single clone family 
exploit cases compiler includes techniques type analysis described chapter identify monomorphic expressions subsequently optimize 
cases compiler task easy self expressions really potentially polymorphic 
compiler frequently optimize polymorphic messages 
compiler includes techniques customization described chapter type casing described section type prediction described section splitting described chapter transform kinds polymorphic expressions monomorphic expressions monomorphic expressions suitable optimization 
techniques duplicating code transforming single polymorphic expression possible representations separate monomorphic expressions 
monomorphic case optimized independently separation optimization possible 
techniques trading away compiled code space gain run time speed form heart self compiler key contribution compilation technology object oriented languages 
identifying creating monomorphic sections code fairly time consuming self compiler seeks conserve efforts 
particular compiler attempts compile parts self program executed 
compiler performs customization demand exploiting self dynamic compilation architecture described section 
additionally cases arise principle rarely arise practice integer overflows array accesses bounds illegally typed arguments primitives compiled self compiler saving lot compile time compiled code space allowing better optimization parts programs executed 
lazy compilation uncommon branches described section 
organization compiler traditional compilers typically divided front performs lexical analysis parsing backend performs optimizations generates code 
self parser described section performs functions traditional front translating self source byte coded representation 
self compiler performs functions traditional back chapter describes self compiler version traditional functions 
bulk self compiler effort lies halves traditional compiler 
middle half self compiler performs representation level type analysis inlining bridges semantic gap high level polymorphic program input self compiler lower level monomorphic version program suitable optimizations performed traditional compiler back 
chapter describes supporting services provided rest self system architecture including description map data structures convey representation level type information objects compiler 
middle half self compiler described chapters 
chapter describes inlining detail 
chapter presents customization self compiler important new techniques 
chapter describes type analysis technique self compiler infer propagate representation level type information control flow graph 
presents type prediction technique guessing types objects names messages built profile information 
chapter describes splitting primary technique self compiler turn polymorphic pieces code multiple monomorphic pieces code 
chapter describes lazy compilation technique compiling parts methods compiler judges executed 
chapter concludes discussion middle describing type analysis splitting presence loops 
inlining chapter customization chapter type analysis chapter type casing section type prediction section splitting chapter loop analysis splitting chapter register allocation code generation chapter front back traditional compiler lexical analysis parsing type checking data flow analysis optimizations register allocation code generation self compiler parser section front back middle chapter system architecture self compiler operate isolation 
integral part self system depending facilities provided rest system constrained satisfy requirements imposed rest system 
place compiler context chapter describes architecture self system focusing impact parts system design compiler 
chapters describe compiler 
object storage system object storage system called memory system represents self objects relationships 
provides facilities creating new objects automatically reclaims resources consumed inaccessible objects 
supports modifying objects programming scanning objects locate occurrences certain kinds 
memory system design exploits technology proven existing high performance smalltalk lisp systems 
minimal overhead common case self system represents object direct tagged pointers indirectly object table smalltalk systems 
early version self memory system documented lee lee version described cul 
subsections describe techniques efficient object storage systems pioneered self implementation 
subsection describes constraints placed compiler self garbage collection algorithm 
appendix describes object formats detail 
maps traditional class languages class object contains format names locations instance variables methods superclass information instances instances contain values instance variables pointer shared class object 
self uses prototype model object self sufficient defining format behavior inheritance 
straightforward implementation self represent class format method inheritance information instance state information self object 
representation consume twice space traditional class language 
fortunately storage efficiency classes regained self prototype object model observing self objects totally unique format behavior 
objects created cloning object possibly modifying values assignable slots 
wholesale changes format inheritance object induced programmer accomplished invoking special primitives 
prototype objects cloned identical way values assignable slots form call clone family 
self implementation uses maps represent members clone family efficiently 
self object storage system objects represented values assignable slots pointer object map map shared members object clone family 
slot object map contains name slot slot parent slot offset object slot contents assignable slot slot contents constant slot non assignable parent slot 
object code techniques described chapter designed self group implemented various members self group including lee urs hlzle david ungar author viewed contributions solely attributable author 
method map stores pointer self byte code object representing source code method byte code objects described section 
maps immutable may freely shared objects clone family 
user changes format object value object constant slots map longer applies object 
case new map created changed object starting new clone family 
old map applies members original clone family 
members exist modified object member clone family old map garbage collected automatically 
implementation point view maps look classes achieve sorts space savings shared data 
addition map object conveys static properties self compiler instance class class language 
maps completely invisible self programmer 
programmers operate world populated self sufficient objects principle unique 
implementation simply optimizing representation execution existing usage patterns presence clone families 
segregation memory system frequently scans object meet criterion scavenger scans objects objects space part garbage collection 
programming primitives find redirect object size changes moved 
browser may need search objects contain particular object interests self user backpointers 
support functions self implementation designed rapid scanning object 
ideally system just sweep memory word word find object matching desired criterion 
unfortunately elements byte arrays represented packed bytes tagged words see appendix byte array elements may masquerade object byte arrays scanned blindly 
systems handle problem scanning heap object object word word 
scan object system examines header object locate part object containing object skip part containing packed bytes non pointer data masquerade pointer 
pointer parts object scanned parts ignored 
scanner proceeds object 
procedure avoids problems associated scanning byte arrays slows scan overhead parse object headers compute object lengths 
self system avoids problems associated scanning byte arrays degrading object scanning speed segregating packed untagged bytes self objects 
generation scavenging memory space described section divided areas bytes parts byte arrays parent rho theta rho theta 
parent parent parent rho theta rho theta 
parent offset offset 
cartesian point traits cartesian point traits map cartesian point traits cartesian points cartesian points cartesian point map maps maps rest data including object part byte arrays 
scan object object area space needs scanned ignoring scans integers range require special support occur 
optimization speeds scans eliminating need parse object headers 
avoid slowing tight scanning loop explicit space check word space temporarily replaced sentinel matches scanning criterion 
enables scanner check space matching word 
early measurements sun showed self system scanned memory rate approximately megabytes second 
measurements fastest smalltalk implementation machine parcplace smalltalk indicated scanning speed non segregated memory spaces megabytes second 
current measurements indicate scanning speed self megabytes second sparc sun workstation 
kinds scans finding objects refer particular object backpointers scanner needs find objects contain matching 
system scans object object task difficult searching 
system scans word word may difficult locate object containing matching 
support kinds scans resorting object object scanning self system specially tags header word object called mark word identify object 
find objects containing particular scanner proceeds normally searching matching 
scanner locates object containing scanning backwards object mark word converting mark word address object adding right tag bits address 
garbage collection self implementation reclaims inaccessible objects version generation scavenging ung ung demographic feedback mediated uj augmented traditional mark sweep collector reclaim garbage 
self heap currently configured kb eden memory space newly allocated objects pair kb survivor memory spaces objects survived scavenge mb old space objects 
implementation algorithm imposes certain constraints compiler 
run time system able locate object embedded compiled instructions scavenge garbage collection occurs 
conversely garbage collector protected examining data values falsely masquerade object 
particular self compiler produce derived pointers interior object 
restriction allows garbage collector assume data tagged object really points design slightly different segregation described lee cul byte arrays stored completely bytes area 
design changed byte array user defined objects addition array bytes 
segregated self memory space bytes area grows downward contains packed bytes parts object object area grows upward contains object confusing untagged packed bytes object heap speeding collector hopefully small cost run time efficiency certain kinds programs notably iterate arrays 
generation scavenging requires compiler generate store checks 
store data slot heap needs checked see creating object old space object new space need recorded special table 
current self implementation uses card marking scheme similar systems wm 
card corresponds region self memory space currently bytes long records words corresponding region contain pointers new space 
compiler generates store object old space object new space compiler generate code mark appropriate card modified data word 
stores objects need fast self compiler attempts generate code fast possible 
compiler prove target stored integer floating point immediate value pointer store check needs generated store create old 
compiler generates sequence sparc assembly syntax st dest source offset store add source offset temp compute address modified word sra temp log base card size temp compute card index stb card base temp mark card zeroing compiler generates code shift address modified data slot right number bits equal log card size adds contents dedicated global register sparc global variable motorola zeros byte address 
system uses byte card single bit required record card marked store checking code slowed bit manipulation operations 
space cards allocated objects new space store checking code doesn check see object modified old space scavenger simply ignores cards objects new space 
byte cards space required store card mark bytes total space self heap adding kb standard heap size mb 
amount space overhead varied cost scanning card scavenging time changing size cards 
dedicated global register named card base code represents base address array bytes cards 
initialized address lowest memory word heap shifted right appropriate number bits added global register contents yields address byte array cards card base cards heap log base card size store checking design imposes relatively small overhead instructions store memory support generation scavenging aware store checking designs impose overhead 
parser minimize parsing overhead textual self programs parsed entered system generating byte code objects smalltalk instances gr 
method object represents source code storing pre parsed byte code object method map cloned invocations method share byte code object share map 
byte code object contains byte array holding byte codes source object array holding message names object literals source byte code object records original unparsed source file name line number method defined user interface purposes 
earlier self implementations including described lee traditional remembered set record old objects containing pointers new objects 
byte code byte array represents single byte sized virtual machine instruction divided parts bit opcode bit object array index 
opcodes represent self programs index extension extend index prepending index extension self push self execution stack literal push literal value execution stack non local return execute non local return lexically enclosing method activation direct message send resend named parent send send message popping receiver arguments execution stack pushing result implicit self send send message implicit self popping arguments execution stack pushing result message lookup current activation record resend send message self lookup parents object containing sending method popping arguments execution stack pushing result super send smalltalk opcodes specified direct evaluation stack oriented interpreter reality self system dynamically compiles machine code simulates interpreter 
index specified opcodes index byte code object accompanying object array 
bit index allows message names literals referenced directly indices larger constructed extra index extension instructions 
primitive operations invoked just normal messages albeit leading underscore message name normal send byte codes represent primitive operation invocations simplifying byte codes facilitating extensions set available primitive operations 
example diagram depicts method object associated byte code object point print method originally section 
top left object prototype activation record containing placeholders local slots method case just self slot plus byte code object representing source code stored method map 
byte code object contains byte array byte codes separate object array constants message names source code 
resends directed resends described detail cuch hcc 
code literals codes 
method object byte code object prototype activation record implicit self send send print literal send print implicit self send send print byte code array literal array self print source code representation filename source 
print 
print 
print 
source code string run time system stacks running self program collection lightweight processes process sharing self heap address space set activation records 
traditional language implementations activation records implemented stack frames linked stack pointers frame pointers 
machine hardware provides support efficiently managing stack frames 
example motorola architectures provide special instructions link jsr managing linked stacks activation records stack pointers mot sun sparc architecture provides hardware register windows support fast procedure calls returns little register saving restoring overhead sun 
garbage collection places requirements design implementation run time system compiler 
garbage collector able locate object stored registers stack 
current self implementation compiler places saved locations mask word fixed offset call instruction trigger scavenge message sends primitives identifying scavenger registers stack locations may contain tagged heap object scanned 
sparc incoming local registers contain valid object outgoing registers handled frame calls incoming registers global registers contain valid object requiring bits saved locations mask word mark registers scan 
remaining bits bit mask word indicate stack temporary locations need scanned 
additional stack temporaries assumed need scanning compiler zeros excess temporaries entering method contents acceptable garbage collector 
mask word design allows compiler freedom allocating data registers alternative approach fixing registers contain object contain non pointer data 
slow scavenging overhead extract interpret mask word stack frame fortunately noticed potential problem performance bottleneck practice 
keeping self robust implementation stack overflow detected reported signal self code 
current implementation stack overflow detected explicit check compiled method requires new stack frame 
sparc dedicated global register maintains current stack limit 
entrance compiled method global register compared current stack pointer register current stack pointer past stack limit stack overflow code invoked 
system similar current stack limit stored global variable memory dedicated register 
stack overflow detection imposes small run time overhead check overflow method invocation 
polling stack overflows methods self system signal handling keyboard interrupt handling memory scavenging requests 
running self process needs interrupted signal arrived scavenge needs performed current stack limit reset back base stack 
causes execution interrupted message send point 
stack overflow handler invoked checks see caused overflow pending signal scavenge request real stack overflow branches appropriate handler 
unfortunately polling approach handling interrupts loops message sends inlined away 
support interrupts loops compiler generates extra code check stack limit value current stack pointer loop body restart primitive call described section 
ensures interrupt handlers invoked relatively quickly interrupt posted 
earlier memory system implementations saved locations mask word registers stack locations 
forced methods stack temporaries execute lengthy prologue zero stack locations 
new design avoids overhead nearly cases arising practice 
alternate implementation avoid run time overhead hardware page protection protect memory page upper limit stack 
page accessed run time system interpret subsequent memory access trap stack overflow error 
interrupting self programs message send boundaries allows compiler freedom generating code 
execution environment stack registers needs consistent state interrupt caught message send boundaries instruction boundary 
debugging information describe state execution saved locations mask word mapping variable names register assignments described section need generated interruption points message sends machine instruction 
blocks current self implementation blocks outlive lexically enclosing scope 
block may passed called routine block part user defined control structure exception handler 
restriction included activation records may stack allocated additional special implementation techniques 
blocks outlive lexically enclosing scope contents implicit lexical parent slot block represented simple untagged pointer stack frame representing lexically enclosing activation record 
stack frames aligned double word byte boundary sparc half word byte boundary untagged address stack frame representation block fear unfortunate interactions garbage collector 
self implementation prevent block object returned lexically enclosing scope stored long lived heap data structure disallows block value method invoked lexically enclosing scope returned zombie block termed non lifo block lifetime follow normal lifo stack discipline 
enforce restriction compiler generates code zap block lexically enclosing scope returns zeroing block frame pointer 
subsequent accesses block null frame pointer cause segmentation faults caught self implementation interpreted non lifo block invocation errors signalled back self program 
summary self memory system provides important facilities compiler 
maps capture essential similarities clone families embodying representation level type information crucial compiler optimizations 
garbage collector run time system place constraints design implementation compiler 
constraints impose extra overhead run time execution programs current design polling interrupts restrict possible optimizations included compiler disallowing derived pointers confuse garbage collector 
fortunately restrictions severe parts system architecture ease burden compiler limiting interrupts defined points compiled code placing restrictions allocation pointer non pointer data registers 
returned upwards self blocks powerful blocks smalltalk closures scheme 
power self programming heap allocated objects created line object literals hold long lived state shared lexically enclosing method nested blocks allow line object returned upwards 
restriction lifetime blocks ways reduces elegance language self implementation may relax restriction believe accomplished seriously degrading performance 
chapter inlining chapter describes inlining methods primitive operations 
people researched problem improving performance procedure calls inlining described section 
chapter describe approach taken self compiler detail heuristics guide inlining automatically 
message inlining self compiler reduces overhead pure object orientation user defined control structures primarily message inlining 
compiler infer exact type map receiver message compiler perform message lookup compile time run time 
lookup successful absence dynamic inheritance message lookup errors program compiler statically bind message send invoked method reducing message normal procedure call 
static binding improves performance direct procedure call faster dynamically dispatched message send help reduce high call frequency 
message send statically bound compiler may elect inline copy target message caller eliminating call entirely 
illustrate sparc call return sequence dynamically bound message send takes minimum cycles current self implementation ignoring additional overhead lru compiled method reclamation support described section interrupt checking described section 
call return sequence statically bound procedure call hand takes just cycles 
inlined call takes cycles usually takes inlined body called method optimized particular context call site 
example register moves added get arguments message right locations dictated calling conventions avoided inlining optimizations common subexpression elimination performed inlining call boundary 
inlining situations additional benefits derived inlining greater initial benefits derived static binding 
reported section inlining self run times slower 
effect message inlining depends contents slot evaluated result statically bound message slot contains method compiler inline expand body method call site method short inlined recursive call 
slot contains block value method compiler inline expand body block value method call site short 
inlining remaining uses block object compiler optimize away code created block object run time 
slot constant data slot slot contains normal object code corresponding assignment slot compiler replace message send value slot message acts compile time constant expression 
kinds messages typically access languages special constant identifiers global variables true rectangle 
slot assignable data slot slot contains normal object code corresponding assignment slot compiler replace message send code fetches contents slot load instruction 
kinds messages typically access languages instance variables class variables 
slot assignment slot slot contains assignment primitive method compiler replace message send code updates contents corresponding data slot store instruction 
kinds messages typically assign languages instance variables class variables 
subsections discuss interesting aspects compile time message lookup inlining self compiler 
assignable versus constant slots described self compiler treats constant assignable data slots differently 
compiler inlines contents constant non assignable data slot inlines offset access path assignable data slot 
reflects compiler expectations remain constant execution programs may change frequently run time 
object mutations running programs expected perform assignments assignable data slots 
object formats contents non assignable data slots method slots changed special programming primitives expected invoked frequently execution programs 
accordingly compiler different compile time run time trade decisions rarely changing information frequently changing information rarely changing information formats objects definitions methods contents slots embedded compiled code 
normal programs fast possible incurs significant recompilation cost information change 
frequently changing information contents assignable slots embedded compiled code 
allows programs change information cost may sacrifice opportunities optimization 
assignable parent slots particularly vexing compiler 
parent slots non assignable data slots 
consequently compiler feels free assume contents change run time 
assumption enables compile time message lookup result depends contents parents searched part lookup turn enables static binding inlining keys run time performance 
contrast encountering assignable parent slot blocks compile time lookup receiver type known 
parent slot assignable compiler assumes parent change run time potentially invalidating compile time message lookup results 
consequently current self compiler statically bind inline message assignable parent encountered compile time message lookup 
decision allows assignable parent slots changed relatively cheaply imposes significant cost dynamic inheritance slowing messages looked assignable parent 
currently exploring techniques reduce cost 
summary self compiler heavy distinction assignable constant slots exploits fact slot assignable determined examining object containing data slot 
slots self implicitly assignable data slot assignable adding corresponding assignment slot child object case early design self compiler longer able treat parent slots unchanging 
absence appropriate new techniques alternative language designs serious performance problems 
issues specific self reductionist object model 
facilities supported self constant slots supported languages special language mechanisms 
example smalltalk uses different language mechanisms instance variables global variables superclass links methods self uses slots 
situations self compiler takes advantage slot constant correspond situations smalltalk compiler similar techniques assume feature constant semantics language feature 
dynamic inheritance self specific problem arising directly self reductionist orthogonal object model aware language similar implementation issue 
heuristics method inlining message statically bound single target method compiler inline message speed execution 
unrestricted inlining drastically increase compiled code space requirements slow compilation 
consequently compiler inline methods costs inlining great 
compiler decide statically bound message inline message 
self system compiler responsible making inlining decisions self programmers involved aware inlining 
compiler uses heuristics balance benefits inlining costs compiled code space compilation time electing inline messages benefits significantly outweigh costs 
course compiler avoid performing costly analysis decide method inlined significantly increase compilation time 
subsections describe principle heuristics self compiler deciding inline message method length checks recursive call checks 
length checks ideally calculate benefits costs inlining method compiler inline method optimize context call calculate performance improvement attributable inlining extra compile time compiled code space costs inlined version 
compiler accurate information base inlining decision 
compiler decided inline message compiler back earlier decision reverting control flow graph state inlining 
unfortunately compilation time returned backing unwise inlining decision ideal method impractical directly 
compiler approximates ideal approach calculating length target method inlining method method shorter built length threshold value compiler inlines statically bound messages access constant data slots assignable data slots assignment slots inlined code load store instruction frequently smaller original message send 
approach seeks predict compile time compiled code space costs inlining method definition method fraction cost ideal method 
length calculation reasonably accurate predicting methods inlined approach achieve run time performance results similar produced ideal approach similar costs compiled code space little cost compile time 
formula calculating method length plays central role deciding inline method developing formula extremely important effectiveness compiler 
length calculations self compiler evolved time hope effective distinguishing methods inline bad methods inline 
approximation compiler calculates method length counting send implicit self send resend byte codes 
length metric assumes number sends measure cost terms compiled code space compile time inlining message literal administrative byte codes relatively free 
sounding reasonable assumption glaring problems 
problem assumes sends equally costly assigns equal weight 
assumption grossly inaccurate considering sends access local variables recall self uses implicit self messages access local variables described section sends access data slots instance variable accesses 
improve accuracy length calculation self compiler excludes length count implicit self send byte codes access local variables data slots receiver compiler penalize method accessing local variables instance variables 
compiler reasonably exclude send byte codes access data slots ancestors receiver sends access self equivalent class variables globals cost checking send byte code accesses data slot relatively high involving call compile time message lookup system current self compiler perform additional checks 
increase accuracy possible performing additional checks may someday deemed important outweigh additional cost compile time 
excluding messages access local instance variables greatly reduces spread cost messages 
remains fairly significant range costs messages left 
compiler include additional heuristics name message indicator cost 
example message iftrue probably cheaper compile important optimize message unknown compiler 
compiler reflect expectation incrementing length count unknown message recognized message iftrue 
distinction mesh static type prediction described section 
original assumption literal byte codes free compared send byte codes frequently mistaken literal question block 
block gets inlined cases compiler hard inline blocks optimize away block creation code cost compiling method include cost compiling block plus cost compiling inlined methods outer method block 
correct deficiency self compiler adds length nested blocks length method 
rule errs side inlining method nested blocks inlining reasonable possibly reducing run time performance better direction inlining methods inlined possibly drastically lengthening compile times 
exception nested block rule failure blocks blocks passed argument primitive added method total length techniques lazy compilation uncommon cases described section ensure failure blocks inlined 
optimizing away block creations important performance compiler uses generous inlining length cut offs methods block value methods methods take blocks arguments methods user defined control structure 
method block value method compiler uses high length threshold currently intended inline reasonable block value methods 
method passed block arguments compiler uses medium length threshold currently 
seeks preferentially inline methods part user defined control structures hope uses block inlined away block creation code removed 
compiler uses low threshold currently 
additionally uncommon branches control flow graph compiler uses drastically reduced length thresholds currently prevent inlining methods run time pay expected low 
message uncommon branch probably sent effort expended optimize wasted 
heuristics reasonable job methods inlining right messages 
example common userdefined control structures loops get inlined primitive operations enabling self compiler generate code similar generated traditional compiler 
unfortunately length calculation heuristics serious mistakes leading overly long compile pauses compiler underestimates cost inlining messages missed opportunities optimization compiler errs opposite direction 
improved heuristics support automatic inlining remain promising area research 
recursion checks compiler inline forever analyzing recursive routine 
example factorial function factorial compiler inline recursive call factorial infer type receiver factorial integer 
requires compiler include mechanism prevent unbounded inlining recursive methods 
approach sufficient prevent unbounded inlining compiler record internal call graph inlined methods inline method twice particular path root leaves call graph recursive message send statically bound inlined 
internal call graph data structure maintained compiler support source level debugging described section easy include part recursion testing 
code may look strange self novice self keywords smalltalk version elided self implicit self message syntax 
receivers predecessor messages implicitly self 
factorial value predecessor factorial inline precise approach check see type receiver calls method allow method inlined long receiver maps different 
finite number maps system new maps created compilation receiver map check adequate prevent unbounded inlining 
practice different maps encountered run compiler check place quite tight bound amount recursive inlining allowed 
unfortunately precise recursion check restrictive handle user defined control structures blocks desired 
consider simple code fragment test iftrue iftrue 
code compiler inlining statements 
similarly calls iftrue inlined recursion check described disallow second call iftrue occurs existing call iftrue 
example typical similar situations implementations common user defined control structures solution order achieve run time performance 
self compiler solves problem augmenting approach described special treatment lexically scoped block value methods 
traversing call stack searching pre existing invocations method compiler follows lexical parent link block value methods dynamic link normal methods 
revised rule allows example inlined desired outer iftrue method skipped lexical chain nested iftrue message prevents unbounded recursive inlining finite number recursive invocations message lexical scope 
iftrue value test iftrue 
inline inlining desired 
iftrue value test iftrue 
value 
lexical parent links inline 
recursion checking conservative 
compiler able inline recursive call getting infinite loop information available compiler processing method invocations different 
illustrate possibility consider print method defined cons cells simply sends print receiver subcomponents shared behavior cons cells traits cons print left print 
right print 
representation individual cons cell cons parent traits cons 
left 
right 
shared behavior collections collection 
concatenate collections creating cons cell collection cons clone left self right collection 
test program index index index printstring bounds print 
compiling method compiler inlines concatenation messages low level consing code retaining intimate knowledge contents left right subcomponents cons cells 
compiler inlines initial print message sent top cons cell knows receiver map inlines nested left right messages 
point compiler inline nested print messages knows types contents subcomponents cons cell just constructed fact compiler information inline example series primitive calls plus call index printstring optimizing away cons ing completely 
unfortunately recursion detection system prevents compiler inlining second print message sent nested cons cell receiver maps print messages invoked directly performing inlining send compiler infinite loop 
current self compiler outermost level cons ing outermost level print ing eliminated example 
unfortunately detecting recursion lead infinite looping difficult 
self compiler remains conservative inlining method receiver map twice mixed lexical dynamic call chain 
extension compiler inline recursive method small number times 
change speed tightly recursive programs unrolling recursion times catch cases cons cell example recursion fact bounded 
course benefits need balanced extra compile time space needed unroll recursive calls 
left right left right index bounds index printstring speeding compile time message lookup compile time message lookup turns bottleneck self compiler consuming total compilation time 
speed compile time message lookups compiler maintains cache lookup results run time system includes cache message results speed run time message lookups 
compile time message cache important certain classes message sends sends accessing global slots nil true false require lot searching inheritance hierarchy common messages iftrue sent frequently 
unfortunately compiler internal memory allocation scheme current compile time lookup cache cache message lookup results single compilation cache flushed compile 
significantly reduces hit rate cache cache new compilation 
example benchmark suites performed approximately compile time message lookups 
accessed slots local slots argument local variable accesses go compile time lookup cache 
real messages compile time lookup cache caused misses hit rate 
consequently compile time lookup cache reduces compile time reported section 
long lived compile time lookup cache presumably higher hit rate cache filling overhead associated compilation cache amortized compilations reduce compile time costs compile time message lookup 
possible implementation strategy cache interact change dependency links outlined section 
inlining primitives addition user defined methods compiler inlines bodies primitive operations 
primitive invocations strictly messages analogous statically bound procedure calls compiler inline calls primitive operations 
implementations certain commonly primitives integer arithmetic comparison primitives object equality primitive array accessing sizing primitives built compiler 
known primitives called compiler generates code line implement primitive 
non inlined primitives implemented call function virtual machine executes primitive 
compiler inlines commonly primitives achieve performance 
call return overhead small primitives frequently larger cost primitive 
primitives self robust check types values arguments legality instance arguments integer addition primitive integers index array access primitive bounds array 
checks optimized away type information available primitive call site 
compiler allocates internal data structures including compile time lookup cache special region memory 
compilation completes entire region emptied 
approach relieves compiler burden manual garbage collection virtual machine written support automatic garbage collection internal data structures cost able easily allocate data structures outlive single compilation 
arguments side effect free idempotent primitive constants known compile time compiler constant fold primitive executing primitive operation compile time run time replacing call primitive compile time constant result 
constant folding especially important optimizing user defined control structures arguments may frequently simple constants control behavior control structure 
example control structure self form simple integer loop defined terms general control structure step value block block 
body routine tests sign step value see loop stepping step block step compare step block equal error step zero loop greater step block 
step block step self self 
block value step 
step negative gets smaller 
step block step self self 
block value step 
compiler constant fold comparisons compare equal greater method knows receiver argument compare arg equal greater false arg iftrue false 
constant folding critical optimizing away overhead general loop just loop 
summary inlining methods primitives slashes call frequency pure object oriented languages languages user defined control structures opening door traditional optimizations global register allocation common subexpression elimination code procedure calls 
inlining message requires static knowledge map message receiver requires sophisticated techniques inferring types objects 
techniques subject chapters 
chapter customization customization self compiler main techniques provides type information enabling compiletime message lookup inlining 
chapter describes customization discusses important related issues 
customization programmers object oriented languages receive expressive power applying inheritance organize code factoring common fragments code shared ancestors 
cases factored code parameterized specific information available inheriting objects subclasses 
factored code gain access specific information sending message self relying inheriting objects subclasses provide specific implementations message take care specific computation 
example point print example section uses sends self access behavior specific cartesian polar points sending messages implicit self allows single print method kinds points irrespective implement messages 
object oriented systems generate compiled code method source code method 
single compiled method general handle possible receiver types inherit single source method 
particular send self implemented full dynamically bound messages different inheriting objects provide different implementations message 
implementation architecture places factored objectoriented code performance disadvantage relative factored code 
parent rho theta rho theta parent rho theta rho theta print arg parent parent parent 
clone 
cartesian point polar point cartesian point traits polar point traits point traits general traits rho theta cos rho theta sin sqrt arctan print 
print 
print clone arg arg parent cartesian point parent rho theta rho theta polar point self compiler avoids penalizing factored code compiling separate version source code method receiver type receiver map method invoked 
version invoked receivers particular map 
method compiler knows precise type self single receiver map self perform compile time message lookup inlining sends self 
example point print example compiler generates compiled version print cartesian point receivers compiled version polar point receivers 
version type self known statically messages statically bound target methods inlined 
self common messages sent self including instance variable accesses global variable accesses control structures extra type information huge difference performance self programs shown section customization self run average times slower 
customization completely overcomes apparent performance disadvantage accessing instance variables global variables messages self special restrictive linguistic constructs smalltalk languages 
customization dynamic compilation customization potentially lead explosion compiled code space consumption 
single source method inherited different receiver types compiled customized different ways 
fortunately potential space explosion controlled cases integrating customization dynamic compilation strategy deutsch schiffman smalltalk system described section 
described section self source code parsed byte code objects compilation takes place run time 
method invoked compiler generates code method byte coded description source code 
compiler stores resulting generated code cache called compiled code cache jumps generated code execute method 
deutsch schiffman smalltalk system generates single compiled method executed source method 
self compiler dynamic compilation integrated customization method custom compiled invoked particular receiver map 
approach usually limits code explosion potentially created customization customizing inheriting receiver type system customizes inheriting receiver types currently manipulated part user working set programs 
section describes pathological cases dynamic customized compilation wasteful compiled code space suggests approaches handling rare situations 
impact dynamic compilation dynamic compilation marked effect flavor system 
dynamic compilation naturally incremental enabling effective programming environment 
turnaround time programming changes short code needs executed compiled change code affected change need recompiled 
dynamic compilation compilation speed important 
programmers unwilling accept lengthy compilation pauses interleaved execution programs total compile time point source code customize customize compiled code point cartesian point version compiled code point polar point version dynamic compilation system traditional batch compilation system 
ideally programmers unaware compilation entirely implying compilation series compilations take second long running non interactive program small fraction second interactive program program real time needs animation play back program 
traditional batch compilers especially optimizers normally labor compilation speed restrictions probably users expect compilation fast unnoticeable 
sense dynamic compilation created problem raising level expectation users 
self compiler takes special pains reduce compilation time lazy compilation uncommon branches described section 
compiled code cache dynamic compilation systems require compiler source code system available runtime possibly compact form self byte code objects 
needs imply dynamically compiled system take space run time corresponding statically compiled system 
self system deutsch schiffman smalltalk system dynamically compiled code cached fixed sized region 
code cache overflows methods flushed cache room new compiled code flushed methods recompiled needed 
caching advantage working set compiled code needs exist compiled form methods exist compact byte code form 
organization save space similar statically compiled systems statically compiled system compiled code exist time dynamically compiled system compact byte codes need kept time 
hand dynamically compiled system caches results compilation may incur compilation overhead dynamically compiled system caching cache grows larger flushing code unnecessarily statically compiled system assuming compiled code eventually needed 
size compiled code cache unbounded important parameter controlling behavior system dynamic compilation small compiled code cache lead excessive compilation overhead akin thrashing large compiled code cache lead excessive paging systems virtual memory 
current self system compiled code cache sized mb machine instructions additional space reserved information output compiler instructions hold commonly compiled code prototype self user interface currently largest self application 
lru cache flushing support current implementation dynamic compilation caching requires support compiler implement replacement algorithm compiled code cache 
select compiled method flush room new compiled methods code cache uses lru approximation strategy 
compiled method allocated word memory record method 
compiled method compiled code zero word mark method 
partial sweeps compiled code cache check methods words zeroed transferring information separate compact data structure 
scanning examined words reset non zero time interval 
clock lru detection strategy imposes small run time overhead clear word memory fixed address method invocation 
customization static compilation self customization incurring huge blow compiled code space self compiler relies dynamic compilation limit customization receiver type source method combinations occur practice 
object oriented language implementations traditional static compilation 
environments customization appear practical compiled versions source methods compiled front possible receiver type source method combinations irrespective combinations occur practice 
trellis owl system described section automatically compiles customized versions methods inheriting subclasses statically 
trellis owl self accesses instance variables messages consequently trellis owl implementors developed similar optimization overcome potential performance problems 
system includes techniques apparently keep costs static customization 
trellis owl compiler conserves space generating new compiled version method differs compiled code superclass version 
technique solve problem having non string classes share compiled code default definition 
trellis owl keeps compiled code space costs compile time costs performing little optimization inlining methods primitives messages self accessing instance variables inlined 
global variables constants accessed directly messages objects false accessed directly sending messages self false accessed normal implicit self message 
trellis owl includes suite built control structures special declarations easier compile code common types integers booleans 
doubt static customization remain practical aggressively optimizing system self pure language model research verify belief useful 
customization partial evaluation customization viewed kind partial evaluation introduced section customizing compiler partially evaluates source method respect type receiver produce residual function customized compiled code 
partial evaluation systems self compiler heavy type analysis inlining optimize routines 
important distinctions self compiler partial evaluators 
self compiler partially evaluates customizes methods type information extracted run time dynamic compilation user type data declarations partial evaluation systems typically extensive static description input program program partially evaluated 
partial evaluators primarily propagate constant information self compiler typically propagates general information representation level types expressions 
partial evaluators typically unroll loops inline recursive calls long constant folded away terminate non terminating input programs programs containing errors lead infinite recursions 
self compiler robust compiling code reasonable amount time programs contain errors 
accordingly self compiler unroll loops arbitrarily sacrificing opportunities optimization process recursion detection rules described section elaborate partial evaluators 
line caching line caching customization customized methods message lookup system additional job locating particular customized version source method applies receiver type 
fortunately selection folded line caching additional run time cost 
deutsch schiffman smalltalk system described section self system uses line caching speed non inlined message sends 
traditional line caching compiler verifies cached method checking receiver map method cached line 
check extends naturally handle customized methods verifying receiver map method customized 
test hit rate traditional line caching receiver map map method customized vice versa 
modified test takes compiled code space cached receiver map longer needs stored line call site 
new check faster perform cached receiver map longer needs fetched line memory location required map value compile time constant embedded instructions method prologue 
sparc instructions implement check methods invoked dynamically dispatched message sends customized integer receivers receiver test low order bits integer tag bz hit instruction rest method prologue delay slot sethi hi call line cache handler jmp lo hit rest method prologue customized floating point receivers receiver test second low order bit float tag mark hit instruction rest method prologue delay slot sethi hi call line cache handler jmp lo hit rest method prologue customized receivers receiver test low order bit memory tag mark map test ld receiver map load receiver map delay slot sethi hi call line cache handler jmp lo map test sethi hi constant load bit map constant add lo constant cmp map compare receiver map customized map constant beq hit instruction rest method prologue delay slot ba branch back call line cache handler hit rest method prologue messages modified line cache compiled code table consulted find appropriate customized version target method 
support customization map receiver object included key indexes table 
version method right receiver map compiled compiler invoked produce new customized version resulting version added compiled code table uses source method receiver type 
line cache overwritten call newly invoked method executions message send test invoked method 
bulk research reported dissertation completed urs hlzle members self group designed implemented extension normal line caching called polymorphic inline caching hcu 
polymorphic inline caches roughly act dynamically growing chains normal monomorphic line caches eventually increasing hit rate polymorphic inline cache 
performance data chapter includes improvements polymorphic inline caches 
line caching dynamic inheritance presence dynamic inheritance outcome method lookup depends just map receiver depends run time contents assignable parent slots traversed lookup 
consequently simple line cache receiver map check insufficient guarantee cached method correct receiver 
approach early self implementation simply disable line caching messages affected dynamic inheritance full lookup performed message involving assignable parents 
unfortunately approach places severe run time overhead dynamic inheritance 
current self system extends line caching check state assignable parents part checking line cache hit 
compiler generates extra code method prologue receiver type check verifies contents assignable parent slots 
cases compiler check map assignable parent statically known constant cases compiler check parent object identity cases relate certain aspects self inheritance rules depend relative identities objects involved message lookup 
assignable parents correct cached method line cache hits body method executed 
line cache misses additional processing needed resolve potentially involving full message lookup 
implementation dynamic inheritance better simple approach disabling line caching altogether fast desired presence dynamic inheritance currently blocks compile time message lookup message inlining 
dynamic inheritance truly competitive performance messages involving static inheritance system need include means statically binding inlining messages influenced dynamic inheritance 
line caching performs self users may send message name computed run time value static compile time string perform primitive 
example self code implement loop control structure succinctly current way section step block step compare step sending block equal error step zero loop greater step sending step sending name block step self self 
perform name block value step 
version loop control structure passes name message test loop done 
implement perform ed messages efficiently generalize notion message generalize line cache prologue handle general kind message 
general message involves number parameters control message lookup including receiver map name message 
parameters may compile time constant run time computed quantity 
normal message send simply special case generalized message message name compile time constant 
perform message name may run time computed value 
addition receiver map normally run time computed value compiletime constant instance message statically bound inlined recursive call 
generalized line cache responsible checking run time computed parameters message lookup guaranteed compile time constants checked compile time receiver map message name 
variants perform allow aspects message lookup object start search computed passed run time values 
compiler attempts determine statically parameters message possible compiler generate better code knows message 
example compiler infer value message name argument perform primitive statically replaces perform primitive normal message send compiler attempts optimize 
way compiler integrates treatment normal messages perform ed messages kinds techniques run time mechanisms improve performance 
logical extension current system customize types arguments addition type receiver 
theoretical reason customization apply arguments addition receiver performance programs improve argument customization 
practical standpoint singly dispatched language self receiver important arguments message lookup depends type receiver types arguments 
customizing receiver comes additional run time cost line caching handle customized methods easily non customized methods 
contrast argument customization require additional run time checks method prologue 
argument customization pays practice depends benefits knowing types arguments outweigh run time costs associated checking types customized arguments method costs additional compiled code space 
successful system customize arguments received heavy body method customization arguments methods surely lead significant compiled code space compilation time overheads 
customization usually improves performance significantly greatly increasing compiled code space usage customization may appropriate source methods 
methods extra space cost associated customization may worth improvement run time performance method send messages self method called different receiver types 
example current self system testing arbitrary objects equality implemented double dispatching ing 
implementation strings example traits string 
anobject anobject self 
arguments strings compare characters 
arguments strings version strings called proceeds compare individual characters strings 
argument string default version called traits defaults false 
version just returns false string equal string 
compiler generates separate customized version method non string receiver types compared strings practice 
normally small set self program iterated objects heap comparing particular string program caused compiler generate customized version default message non string type system 
clearly single shared version method better 
prevent pathological cases investigating approaches compiler elect customize methods costs customization outweigh benefits 
syntax precisely self syntax intuitive syntax dissertation pedagogical reasons 
summary self compiler performs customization main techniques improve performance 
customization provides compiler precise static knowledge type self enabling statically bind inline messages sent self 
kinds messages extremely common self instance variables global variables accessed sending messages self special purpose language mechanisms limited expressive power 
customization directly overcomes performance disadvantages self expressive approach clearing way languages rely messages variable accesses adverse performance impact 
coupling customization dynamic compilation space overhead customization kept reasonable 
coupling customization line caching extra run time required select right customized version invoked source method 
chapter type analysis type information plays critical role improving performance object oriented languages self 
obtain maximum benefit type information compiler infer compiler uses sophisticated flow analysis propagate type information control flow graph 
propagation called type analysis subject chapter 
simplify exposition type analysis straight line code loops discussed type analysis loops described chapter 
internal representation programs type information control flow graph compiler represents method compiled control flow graph data structure different kinds nodes control flow graph different kinds operations 
nodes include high level nodes message send nodes instruction level nodes add instructions control flow nodes merge nodes conditional branch nodes bookkeeping nodes assignment nodes 
passes compiler involve sort traversal graph 
diagrams dissertation success arc branch node exit left 
add names compiler uses names capture data dependencies nodes 
name corresponds source level variable name argument local variable compiler generated temporary name name referring result subexpression source code 
compiler treats kinds names way 
names represent flow data nodes graph having nodes bind names computed results nodes referring bound names arguments 
instance message send node passes data values bound names receiver argument message binds result data value temporary name 
illustrate control flow graphs names inlining consider message send node 
compiler infer type receiver message say integer lookup message integers compile time locate method 
compiler inline method 
inlining method entails constructing new control flow graph inlined method splicing control flow graph main graph place message send node 
name assignment nodes inserted assign names actual receiver arguments names formals inlined control flow graph likewise assign name result inlined control flow graph name result eliminated message send node 
names self correspond formals inlined methods 
new names created inlined copy method 
result self 

self result result self compiler inline expand call primitive 
transformations control flow graph performed self compiler flavor 
values view names merely mechanisms programmers compiler refer underlying data values run time existence 
underlying data values referred names represented explicitly self compiler values 
value data structure compiler represents particular run time object 
names may refer value particular point program single name may refer different values different points program 
example assignment node refer value object value referred assignment node may refer different value assignment assignment 
assignment nodes simply affect compiler internal mappings names values directly generate machine code 
primary invariant relating names values names map value point program names guaranteed refer object run time point 
hand names map different values compiler tell names refer object runtime 
values immutable new value created compiler needs representation run time object potentially different object 
example receiver argument names method self int 
int 
add self result value overflow 

self result result compiled initialized new unique values results non inlined message sends primitives integer arithmetic 
contents assignable data slots heap instance variable accesses bound new unique values 
merges control flow graph pose interesting problem maintaining invariant relating names values 
name bound value predecessor branch merge node bound different value predecessor branch value name bound merge 
chosen paths program run time compiler inferred incorrect information potentially leading incorrect optimizations cause optimized program misbehave crash 
incoming value acceptable brand new value created represent run time data value referred name merge 
new values created merge nodes called union values 
union values similar functions ssa form described section 
currently name needs new union value merge node unique union value 
accurate analysis locate names predecessor branch bound value assign new union value merge 
example merge node guaranteed refer run time object union value 
self compiler analysis currently sophisticated recognize situation new union value merge 
values provide better base names certain kinds analysis optimizations 
consequence aggressive inlining user defined control structures operations new names created high rate trivial assignment nodes introduced merely assign name name actual name formal 
values provide explicit representation objects flowing names closer variables traditional language compiler 
techniques optimizations normally variable names traditional compiler register allocation common subexpression elimination naturally values self compiler 
common subexpression elimination values described section global register allocation described section respects values serve purposes similar served subscripted variables ssa form described section 
feel explicitly separating values names propagating values control flow graph independently names testing values equality simpler transforming program ssa form name replaced 
uv uv uv uv uv subscripted names merging subscripted names equivalence classes testing subscripted names membership equivalence class 
values improve effectiveness type analysis 
run time type tests particular name implementing run time type checking arguments primitives verifying guesses part type prediction described section alter type associated value bound tested name just name happen simpler system mapped names directly types 
allows single type test alter inferred type names currently bound aliased tested value 
example values type tested value updated values aliased names updated improved type information 
type tests occur deeply nested inlined control structures operations names part test just local temporary names 
values critical communicating important type information local scope inlined control structure operation 
shown section values self run average slower 
types types primary data structures compiler represent type information support various kinds type related optimizations compile time message lookup constant folding 
type describes set runtime objects usually sharing common property 
particular kinds sets compiler capable describing concisely types motivated primarily optimizations currently performed compiler new sorts optimizations require new kinds type information represented propagated control flow graph 
types currently self compiler described subsections 
map types map type specifies objects share particular map objects single clone family 
kind type important kind type general type enables compiler perform message lookup compile time perform type checking primitive arguments compile time 
map types introduced sources type self map type result customization described chapter 
run time type test testing object integer marks tested object particular map type branch test successful 
primitive operations known return objects particular map types 
example integer addition primitives known return integers primitive succeeds clone primitive returns object map receiver 
class language kind type specify objects instances particular class called class type 
int 
unknown int int int 
unknown unknown unknown unknown unknown int unknown unknown int values values constant types constant type specifies single object compile time constant value 
constant types support sorts optimizations map types plus additional optimizations constant folding primitives 
constant types introduced sources literal self source code constant type 
result inlined message accesses constant data slot true message constant type 
run time value test testing true object run time marks tested object particular constant type success branch 
integer subrange types integer subrange type specifies range integer values lower bound upper bound 
integer subrange types allow compiler perform kinds range analysis optimizations 
example compiler eliminate array bounds check range index guaranteed bounds array 
similarly compiler eliminate overflow check integer arithmetic operation ranges arguments prove result overflow normal bit integer representation 
compiler constant fold integer comparison ranges arguments overlap 
integer map type integer constant types may viewed extreme cases integer subrange types represented concisely integer subrange types 
integer subrange types introduced sources result successful integer arithmetic primitive integer subrange integer constant integer map type computed ranges arguments operation 
integer comparison operation narrows types arguments depending outcome comparison 
example comparing true branch compiler lower upper bound type upper bound type similarly raise lower bound type false branch analogous narrowing occur 
narrowing convert integer map type integer subrange type 
primitive operations known return integers particular range 
example array size primitive returns non negative integers upper limit bounded maximum size heap 
unknown type unknown type specifies possible objects conveys information compiler 
compiler associates unknown type incoming arguments method compiled customization performed arguments results non inlined messages contents assignable data slots heap instance variables results primitive operations 
union types union type specifies union objects specified component types 
name known particular union type contents variable run time objects possible component types 
component types covered map type component types specify objects single clone family union type allows sorts optimizations map type 
common component types different clone families union type typically provides information map type information generic unknown type 
union types guide run time type casing described section splitting described chapter 
union types created primarily result merges control flow graph value associated different types different predecessor branches analogously union values 
example value associated integer map type predecessor branch merge associated floating point map type predecessor branch merge value associated union type components integer map type floating point map type 
union types created result certain primitive operations 
example type result comparison primitive success branch union true constant type false constant type true false 
exclude types exclude type specifies set component types associated value known 
exclude types introduced result unsuccessful run time type tests recording object integer failure branch integer type test 
compiler uses exclude types avoid repeated tests possibilities guaranteed occur 
exclude types expressive full fledged difference types 
difference type specify objects type set difference objects specified types 
exclude type equivalent difference unknown type union excluded types differences unknown type expressed exclude type exclude type express differences precise type 
example current type system record possibilities map type excluded failed run time value test object map type known 
full fledged difference type describe type difference known map type union excluded possibilities map type 
unfortunately generalizing exclude types difference types create new problems 
general difference types single type represented multiple ways significantly complicating type equality testing sorts comparisons types type covers 
problem exists current type system extent integers integer subrange type represent type union adjacent overlapping integer constant types integer subrange types 
example types considered equivalent union integer constant types union integer subrange integer constant types integer subrange type adding general difference types exacerbates problem difference type equivalent types int maxint current self compiler detect types types equivalent 
chose worsen situation consequently include fully general difference types type algebra 
luckily exclude types sufficient practice rarely compiler perform value tests objects map known 
self compiler eventually include generalized approach equality type comparisons presence union types difference types integer subrange types defining canonical representation sets integers translating canonical form prior comparing types 
inaccuracy lead incorrect compilation side treating things different types safe lead poorer generated code 
int float int float type analysis perform optimizations compile time message lookup elimination run time type checks compiler needs able determine type associated name particular point program 
support determination compiler maintains mapping names values mapping values types 
mappings propagated control flow graph type analysis proceeds 
performing type analysis compiler visits node control flow graph topological order 
control flow graph node implements type analysis routine 
routine typically examines type mappings propagated node predecessor performs optimizations node type information produces new type mappings node successor 
subsections describe type analysis operations detail 
discussion type analysis presence loops deferred chapter 
assignment nodes assignment node alters name value mapping assigned name 
name value mapping assignment node value associated assigned name left hand side value associated assigned name right hand side 
bindings unaffected 
merge nodes merge node combines name value value type mappings predecessors form new pair mappings merge 
new union values union types may constructed 
branch nodes branch node copies mappings successor branch subsequent alterations mapping successor branch affect mapping successor branch 
message send nodes message send node looks type bound receiver name 
type map type specific map type compiler performs compile time message lookup 
lookup successful compiler elects inline target method message send node replaced control flow graph representing inlined target method type analysis begins analyze new inlined method 
message inlined new value created represent result message send 
name message result bound new value new value bound unknown type 
altered mappings passed message send successor 
primitive operation nodes primitive operation node checks see inlined compiler implementation primitive built 
primitive operation node replaced nodes implement primitive line 
type information associated arguments primitive optimize inlined primitives eliminating unnecessary argument type checks overflow checks array bounds checks 
primitive inlined type analysis continues node primitive 
new value created primitive result primitive result name bound new value 
result value turn bound unknown type precise type depending primitive argument types 
altered mappings passed primitive success continue type analysis 
compiler implemented 
control flow graph node instance class different classes different kinds control flow graph nodes 
control flow graph nodes define virtual function implements node specific type analysis 
type casing compiler infer type receiver message covered map type compiler perform message lookup compile time statically binding inlining message 
type receiver frequently simple map type union different types 
occur result primitive known compiler return different types comparison primitives returning true false merge control flow graph single name associated different types merge 
type casing simple technique optimizing message sends receiver bound union type 
example iftrue message frequently sent expression type union true constant type false constant type optimizing messages type casing involves testing element type union branching code specific type 
specifically component union type covered map type compiler inserts run time type test message checks type successful case branches copy message 
iftrue example compiler perform type casing inserting run time check true branched copy iftrue message copy type cased message statically bound inlined away type receiver known successful run time type test 
furthermore components union type covered map types iftrue example component union type need run time type check failed type checks excluded possible types 
hand components union type covered single map type union type contains unknown type components message inlined map types union final dynamically dispatched version message needed handle remaining case 
control flow re merges copies message 
type casing simple technique take advantage union type information 
effect transforming single polymorphic message monomorphic messages optimized 
example general theme self compiler trading away space improved run time performance 
type casing similar case analysis performed ts compiler typed smalltalk see section 
type casing incurs run time overhead extra type tests various component types union 
resulting code typically faster original message send faster iftrue example fast frequently possible 
chapter discusses splitting sophisticated technique exploiting union type information created merges control flow graph run time overhead 
compiler apply splitting unions created result primitive operations falls back type casing technique 
type casing illustrates union type provides information just specifying set union objects specified component types 
divisions various component types important expr true false result iftrue expr block expr true false result iftrue expr block expr true 
result iftrue expr block expr true expr false component types drive type casing 
example prior merge node name bound integer map type predecessor unknown type predecessor part type analysis compiler bind name union types merge integer unknown 
naive implementation simplify union type noting unknown type covers integer map type integer map type removed union altering set objects specified type unknown simply unknown 
simplification lose significant amount important information 
compiler longer information integers component type compiler longer separate integer case type casing 
faced blank unknown type compiler information decide subset possible types worth type casing 
accordingly union types self compiler simplified eliminating component types covered component type preserve useful information possible 
type prediction customization type analysis enable compiler infer representation level types expressions turn enables compiler statically bind inline messages optimize away run time type checks 
messages remain receiver types inferred customization type analysis 
example types arguments unknown compiler currently customize types arguments types contents assignable data slots heap instance variables messages sent objects inlined customization type analysis type casing 
enable compiler infer type information self compiler performs type prediction 
compiler infer type receiver message customization type analysis compiler tries name message hint type receiver 
example compiler predicts receiver message integer receiver message chance array receiver iftrue message certainly true false 
predictions embedded compiler form small fixed table mapping message names receiver types 
message name table type predicted implemented full message send absence optimizations 
compiler uses predicted type transform original receiver type union type components original receiver type component predicted type 
example iftrue message sent expression type unknown compiler perform type prediction replace receiver type contains true false constant types original type part union type receiver type remains just general type prediction 
form receiver type suitable optimization type casing run time type test inserted check predicted type branch separate statically bound copy message suitable inlining fall back dynamically bound version message remain case predictions expr unknown result iftrue expr block expr true false unknown result iftrue expr block correct 
iftrue example type prediction compiler apply type casing insert run time checks true false branched separate copy iftrue message copies iftrue message inlined away receiver types compile time constants third copy remains dynamically bound receiver type unknown 
control flow re merges type prediction type casing transformations 
rate successful prediction high performance predicted message faster original unpredicted message 
cost type test cost dynamically dispatched procedure call inlining predicted messages lead opportunities additional time saving optimizations 
course unsuccessful branch slowed extra run time type test type prediction cases low success rate slow performance system 
current self system type prediction high success rate 
benchmarks measure performance self predictions correct 
benchmarks originally written traditional languages pascal bcpl data types predicted type prediction integers booleans arrays normally ones traditional languages mispredictions occur benchmarks translated lisp overload compare integers cons cells 
large smalltalk systems receiver message integer time ung type prediction useful programs written heavily object oriented style 
reported section type prediction speeds self programs factor average object oriented self programs traditional numeric benchmarks 
type prediction similar inspired technique smalltalk systems hard wires implementations certain common messages compiler described section 
insert run time type tests verify static predictions embedded compiler 
smalltalk hard wiring type prediction embed definition predicted messages compiler inlining compiler impose restrictions predicted messages iftrue 
programmers free change definitions predicted messages add new definitions exist compiler implemented 
type prediction coupled inlining enables self implementation achieve run time performance hard wired messages preserve self pure message passing model 
type prediction currently implemented static technique message names predicted receiver types fixed compiler changed users 
better technique dynamic type prediction message names predicted receiver types automatically adapt self source code currently 
example dynamic profile data augment replace static table built compiler 
actively investigating techniques type prediction adapting changing usage patterns hcu 
expr true false unknown result iftrue expr block expr true 
result iftrue expr block expr true expr false expr false true expr false 
result iftrue expr block expr true false block analysis blocks common self code primarily central role implementation user defined control structures 
straightforward self implementation invocation control structure self version traditional loop involve creating blocks run time invoking blocks repeatedly execution loop 
approach compiling user defined control structures blocks competitive run time performance traditional languages built control structures compiler generate instructions implement control structure 
consequently self compiler efforts directed eliminating overhead user defined control structures blocks 
deferred computations important block related optimizations mentioned section inlining statically bound block value methods eliminating block creation operations uses block eliminated preferentially inlining methods block arguments increase likelihood block arguments getting inlined away 
blocks may remaining uses eliminated requiring block created uses control flow paths executed rarely 
example consider inlined integer addition primitive operation passed failure block 
primitive fails block sent value message message inlined current self system block remains block creation code completely eliminated 
primitive invocations fail large majority invocations primitive created block 
overhead creating block primitive invocation especially ones simple integer arithmetic quickly bring system knees 
self int 
int 
add self result value overflow 

result limit overhead block creation just parts program need block compiler defers creation blocks needed run time value 
paths control flow graph need block pay expense creating block paths penalized 
compiler needs way tell block created point program 
information encoded type associated block value block type deferred block type created block type 
place original block creation code compiler generates assignment node binds fresh value object deferred form block type 
uses require block real run time value non inlined message send block argument check type block deferred insert additional nodes control flow graph create block object run time 
block creation node value object representing block deferred block type corresponding created block type name value binding created block remains unchanged names aliased block value simultaneously see block created avoiding duplicate block creations 
deferred computations block creation handled time type analysis optimizations performed expensive computation performed influence control flow graph gets built 
especially important optimizing block zapping code see section paths control flow graph compiler knows block created 
earlier version self compiler deferred computations handled pass compiler bulk control flow graph transformations performed 
design led poor performance extra run time tests inserted check block creation performed zapping block 
example self int 
int 
add self result value overflow 

result created graph includes code zap failure block block creation deferred primitive failure branch block zapping code remains paths primitive succeeds 
handling deferred computations part type analysis allows techniques cooperate type analysis splitting described chapter help optimize treatment blocks block zapping blocks self int 
int 
add self result value overflow 

result created created 
store created need zapping code 
enables block zapping code executed branches block created 
optimizing away block creations unnecessary deferring remaining block creations just prior uses important optimizations performed self compiler 
reported section deferred block creation self run average times slower times slower programs user defined control structures loops 
computations arithmetic deferred uses 
required deferred computation externally visible side effects 
opportunities deferring computations block creations occur far produce sorts dramatic speed ups deferring block creations 
analysis exposed blocks blocks support lexical scoping local variables nested block contain accesses assignments arguments local variables lexically enclosing scope 
accesses assignments called level accesses assignments 
block value method inlined level accesses assignments value method treated local accesses assignments type analysis infer types variables 
inlined block value method non local return implemented direct branch part control flow graph block outermost lexically enclosing scope inlined compiled method 
block optimized away passed receiver argument non inlined message compiler conservative 
non inlined message invoke block types inferred local variables potentially level assigned block value method weakened include unknown type current self compiler interprocedural analysis infer types expressions level assigned non inlined blocks 
non inlined method store block long lived global data structure subsequent non inlined messages assumed invoke block requiring types level assigned variables weakened 
call blocks invoked methods call site exposed blocks exposed outside world longer tight control 
self int 
int 
add self result value overflow 

result store exposed blocks dilute type information potentially level assigned local variables compiler works hard limit number blocks treated exposed 
addition maintaining name value value type bindings type analysis compiler maintains set blocks exposed 
block added current exposed blocks set passed non inlined message send stored data structure heap 
addition blocks level accessible newly exposed block added exposed blocks set accessed invoked original block invoked 
block removed exposed blocks set lexically enclosing scope returns block unusable 
merge points compiler unions exposed blocks sets merge predecessors form set exposed blocks merge successor compiler forms union types merge nodes 
exposed block set non inlined message send node alter type bindings potentially level assigned variables exposed blocks set 
calculating mappings message send successor potentially level assigned name rebound new unique value object modelling assignment name unknown object exposed block 
compiler generalize type associated new value object assignment occur compiler know type object assigned local variable 
simple strategy simply bind new value object unknown type 
unfortunately naive approach sacrifice type information compiler inferred local variable prior message 
self compiler tries limit damage type information caused potential level assignment heuristics 
potential level assignments performed information accumulated contents local variable true message send 
second assignments performed type variable assignment similar type variable assignment say clone family programs normally assign completely unrelated objects local variable 
exploit trends self compiler assigns type local variable potential level assignment union unknown type case assignment occurs original type local variable message send generalized enclosing map type case assignment occur assignment member clone family occurs 
example type potentially level assigned variable float message send message send type changed unknown integer float 
union type information exploited type casing see section 
local variable assigned assigned member clone family strategy incurs overhead type test subsequent accesses local variable requiring full message send naive strategy 
exposed block sets improve quality type analysis limiting damage level assignments parts control flow graph blocks created exposed outside world 
blocks created exposed relatively uncommon branches primitive operation failure blocks effect exposures limited parts graph 
containment allows common case branches execute conservative assumption local variables assigned 
practice exposed block analysis effective shown section exposed block analysis self programs run faster compiler treated blocks exposed 
common subexpression elimination compiler performs common subexpression elimination cse part type analysis 
cse eliminates redundant computations detecting computation performed earlier result reused 
self compiler performs cse kinds computations arithmetic calculations memory loads stores 
eliminating redundant arithmetic calculations compiler discovers redundant arithmetic calculations searching name value mappings existing value result value potentially redundant calculation 
value exists name value mapping compiler replaces arithmetic calculation simple assignment name bound existing value name result eliminated calculation 
support equality comparisons values values representing results arithmetic calculations structured containing subcomponents receiver argument values kind arithmetic calculation 
arithmetic values guaranteed equal equal subcomponents 
example add control flow graph node produces structured result value node graph calculates value compiler replace second redundant calculation simple assignment node compiler typically avoid generating code assignments arranging names related simple assignment allocated register register allocation described section 
currently compiler detects equal calculations operations equal operands equal order operands operation operand trees isomorphic 
sophisticated value equality systems take account arithmetic identities commutative property addition relationships addition subtraction 
weakness current self compiler rules difference quality generated code large difference 
additionally values extended enable equality testing presence conditional branches algorithms associated subscripted names ssa form described section support flow sensitive equality testing 
remains open question practical benefit received additional analysis 
reported section cse arithmetic expressions relatively minor effect run time performance usually improvement performance 
redundant arithmetic computations probably common self programs traditional programs part array accesses self require multiplication array index scaling factor languages self tagged integer representation appropriately scaled indexing self built dimensional object vectors 
calculations eliminated redundant currently limitations garbage collector treatment derived pointers 
eliminating redundant memory redundant memory fetches stores detected fashion similar detecting redundant arithmetic calculations 
compiler maintains mapping cells values propagated control flow graph part type analysis 
cells compiler internal representations memory locations heap assignable data slots instance variables array elements lexically enclosing frame pointers blocks 
value object associated cell represents current contents cell 
cell addressed component values add arith add arith arith arith arith base value offset value 
base value value object accessed memory offset value constant fixed offset memory accesses assignable data slots frame pointers normal computed value computed indexes arrays 
cells guaranteed refer physical memory location corresponding base values offset values equal 
course cells different base offset values refer memory location values guaranteed refer run time object see section 
cse memory works way cse arithmetic calculations 
memory load instructions construct new value object representing contents cell new cell object addressed base offset load instruction new contents value object 
bindings added mappings maintained type analysis mapping name result memory load new value object added name value table mapping value unknown type added value type table mapping new cell new value added cell value table 
memory store instructions similarly add binding new cell object addressed base offset store instruction value bound name object stored addressed memory location 
memory fetch node cell cell value mapping table compiler replace redundant memory fetch node simple assignment node binding result name redundant memory value associated existing cell 
addition eliminating unnecessary memory optimization preserves type information compiler able infer contents memory cell 
cases benefits preserving type information outweigh benefits merely eliminating instruction 
cse performed stage compilation process done compilers traditional languages ability preserve type information memory cells lost 
effect cse memory accesses supports type analysis locations heap 
load base offset base offset cell unknown store arg base offset base offset cell arg load cell cell compiler eliminate memory store node cell assigned cell value mapping current contents value bound cell mapping value stored 
store eliminated compiler updates cell value mapping store node reflect compiler knowledge cell new contents 
additionally cells mapping refer memory location stored cell potentially aliasing cells base offset values removed cell value mapping contents ambiguous 
similarly message sends assumed conservatively assign global heap cells assigned 
non inlined message send cell value bindings cells changed self program including instance variables array elements excluding frame pointers array sizes removed cell value mapping cells called exposed cells analogy exposed blocks 
avoid losing precious type information potentially modified cells treated way potentially level assigned local variables described section 
potentially modified cell new unique value object added back cell value mapping new value object bound union type constructed generalizing previous inferred type enclosing map type combining type unknown type 
way damage type information assignments potential alias cells potential assignments exposed cells limited 
cse memory cells purpose self compiler eliminating unnecessary array bounds checks 
array access compiler checks see cell corresponding accessed array element cell value binding 
compiler omits code checked array access bounds 
optimization legal program able access array cell error cell available bounds check performed part previous access 
theoretically optimization unnecessary compiler able eliminate bounds check directly remembering bounds check performed array index 
current implementation self compiler record information 
checking cse memory cells cheap way eliminating redundant array bounds checks additional mechanism 
self compiler include information able determine array bounds checks performed better array bounds checks required run time technique removed redundant 
reported section common subexpression elimination redundant memory effective self common subexpression elimination redundant arithmetic 
average performance improvement cse memory benchmarks speed technique 
effect able track type information assignments subsequent fetches memory cells accounts sizable fraction total contribution cse memory eliminating array bounds checking available cell information accounts smaller fraction 
eliminating unnecessary object creations initializations ideally compiler eliminate object creations stores uses object memory fetches created object eliminated 
example situation throw away objects get created quadratic formula function return multiple roots creating object pair assignable data slots store result roots object return object result 
temp 
temp squared sqrt 
result 

clone 
create object hold roots result negate temp 
result negate temp 
result 
caller quadratic formula routine extract roots result object data slots throw result object away result result 
result printstring result printstring 
called routine inlined calling routine memory fetches extract data slots returned object optimized away redundant compiler recorded earlier stores memory cells 
intermediate object longer useful execution program compiler eliminate object creation memory stores unnecessary 
example consider standard looping control structure 
control structure takes integers block arguments iterates integers integer arguments invoking block integer 
better way defining loop apply standard control structure iterates arbitrary collection interval object represents collection integers lower upper bounds 
self interval created sending message integer integer argument loops written follows upperbound body loop 
intervals implemented follows traits interval parent traits collection 
lowerbound 
value successor 
self 
interval parent traits interval 
lowerbound 
upperbound 
interval creation code defined integers traits integer upperbound interval clone lowerbound self upperbound upperbound 
design number concepts needed normal self programming known operation self intervals useful data structure right 
programmers need special iterator methods just integer loops 
typical usage style loop create new interval object invocation loop 
performance comparable traditional language loop built control structure desired overhead creating interval object reduced 
fortunately interval object created immediately thrown away method intervals extracts lower upper bounds iteration remaining run time uses hope compiler optimize away object creation overhead entirely 
unfortunately self compiler currently optimize away object creations stores 
eliminating object creates complicated source level debugging 
debugger invoked object scope visible stack dump compiler provide information debugger create illusion debug time real object created initialized 
problem avoided cases compiler able determine object visible debug time analysis determining created object exposed outside world 
compiler eliminate stores un exposed object stores seen routines self programmer subsequently eliminate object creation code uses created object gone 
current self coding style optimization crucial performance helpful 
new style loops intervals implemented efficiently aspects self programming style change optimization needed maintain current high level run time performance 
summary self compiler uses type analysis propagate information types variables expressions control flow graph maximize benefits received relatively scarce type information compiler infer 
compiler maintains data structures part type analysis 
mappings names values values types central determine type receiver message support compile time message lookup inlining type arguments primitive support eliminating runtime type checking overhead 
type testing code alters value type mapping implicitly altering induced name type mapping names aliased tested value necessary system relying aggressive inlining 
type casing exploits information contained union types inserting run time type tests branch monomorphic versions code amenable compile time message lookup inlining 
type prediction uses context information form names message sent expression prediction type expression 
prediction exploited replacing original type predicted expression union type contains predicted type original type 
type casing employed read new union type information insert appropriate run time type tests verify prediction branch optimized code case prediction correct 
compiler optimizes blocks part type analysis primarily deferring creation block runtime 
compiler calculates blocks exposed outside world weakens types variables potentially level assigned exposed blocks 
name value mapping support common subexpression elimination redundant arithmetic calculations result values arithmetic calculations structured compared equivalence 
additional mapping cells values supports common subexpression elimination redundant memory fetches stores turn allowing type analysis track types values stored fetched assignable data slots heap 
chapter splitting chapter describes splitting technique transforming polymorphic code multiple copies monomorphic code amenable optimization inlining 
splitting resembles type casing described section introduces additional run time overhead type tests 
chapter discusses splitting straight line code splitting loops discussed chapter 
motivating example splitting originally motivated attempting generate code expression expressions occur virtually programs imperative generate code example 
reasonable compiler faced similar code int produce control flow graph intermediate step techniques far come close graph quite 
self compiler control flow graph representation original conditional expression simplify example assume self compiler able infer type analysis integers 
compiler lookup definition integer receiver find method handle failure 
compiler inline expand method place message produce control flow graph iftrue iftrue handle failure result self self result int int type analysis proceeds eventually reaching call primitive expanding line 
compiler eliminate type checks arguments primitive compiler knows type analysis self integers 
subsequently compiler eliminate creation self result true int int self result false int self int result iftrue self int 
int 
handle failure result value handle failure handle failure block longer needed run time value 
optimizations produce control flow graph self result true int int self result false int self int result iftrue type analysis starts analyzing body primitive eventually reaching iftrue message knowledge type receiver iftrue true false self result true int int self result false int self int result iftrue result false result true true false result true false point compiler type casing insert run time type test separate true false cases compiler perform message lookup compile time copies iftrue message locating methods true iftrue block value 
false iftrue nil 
self result true int int self result false int self int result result false result true true false result true false true 
true false iftrue iftrue compiler inline methods control flow graph compiler restarts type analysis determines type particular cloned block literal inlines block value message just body block block creation code eliminated self result true int int self result false int self int result result false result true true false result true false true 
true false result value self result self result result nil remaining uses block 
produces final control flow graph removing assignment nodes correspond generated instructions unfortunately type casing approach produces code efficient single compare branch sequence generated compiler true false true 
inefficiencies stem control flow merging primitive split apart part type casing iftrue message 
merge primitive simply postponed versions iftrue message inefficiencies disappear self compiler generate code compiler 
example inlining primitive reach graph self result true int int self result false int self int result iftrue compiler delayed merge iftrue message copying control flow graph nodes primitive result iftrue message get graph compiler directly inline iftrue messages inserting run time type tests type analysis infer type left hand iftrue message true type right hand message false 
inlining assignments result optimized away longer needed runtime 
leads directly control flow graph eliminating assignment nodes generate machine code graph produced compiler 
achieve level performance competitive traditional languages self compiler needs mechanism postpone merges selectively avoid falling back run time type casing code 
words self compiler needs splitting mechanism 
splitting general term techniques lead multiple versions compiled parts control flow graph version optimized different situations different type bindings 
kinds splitting implemented self compiler different trade compilation speed execution speed 
main discriminating characteristics various splitting strategies decide postpone merge split nodes downstream postponed merge decide postponing merge 
sections describe different approaches 
self result true int int self int self int result iftrue result iftrue result false reluctant splitting reluctant splitting form implementations self compiler 
splitting variant compiler initially merges control flow reverse decision desired undoing merge copying parts control flow graph 
kind splitting called reluctant compiler splits merges demand lazy splitting demand driven splitting equally appropriate names 
example consider generalized example reluctant splitting reaching potential merge point compiler merges control 
compiler remembers merge forming union types names bound different types merge 
example name bound union type 
control flow node optimized name bound union type bound component union type compiler reverse earlier decision merge control duplicating control flow graph nodes premature merge node demands split 
duplication approximates control flow graph generated original merge taken place 
example message send node optimized receiver bound component type joint union type message send node demands merge postponed message send 
demand satisfied duplicating nodes message send back original merge point 
subsequent splits may postpone merge farther 
copying parts control flow graph different type compiler transformed single polymorphic message send independent monomorphic optimizable message sends inserting extra run time overhead type tests 
arbitrary subgraph copied part reluctant splitting restricted empty nodes generate instructions call splitting algorithm local reluctant splitting splitting takes place locally control flow graph 
arbitrary subgraphs allowed call splitting algorithm global reluctant splitting 
splitting turns crucial technique achieving performance 
detailed section form splitting self programs run half fast 
cases termed local message splitting extended message splitting cu 
splitting splitting message arbitrary subgraph message arbitrary subgraph message arbitrary subgraph splittable versus unsplittable union types compiler detects reluctant splitting possible checking type expression union type merges create union types component types component types may split apart 
union types created merges results primitives known return set possible types 
example type result floating point comparison primitive union true constant type false constant type 
compiler inline primitive invoking calling external function built self implementation merge node split apart separate true result false result 
splitting applicable case compiler fall back type casing described section optimize messages sent result primitive 
compiler distinguishes union types created result merges control flow graph amenable splitting created externally amenable splitting 
kind called splittable union types called unsplittable union types 
merge nodes create splittable union types floating point comparison primitive returns unsplittable union type 
compiler attempts splitting splittable union types 
splitting multiple union types simultaneously compiler uses splitting transform single piece polymorphic code independent pieces monomorphic code 
splitting step typically operates single name single union type name type receiver message 
splitting operation compiler update type name split branches appropriate component type 
precise types enable inlining split branches 
important function splitting break apart union types component types enables optimizations 
dividing union types works fine single name split improve types names precise split 
example consider control flow graph fragment splitting message integer map type case compiler alter type bindings split branches get split control flow graph unfortunately techniques described far compiler notice type narrowed split branches 
cause compiler insert unnecessary integer type check argument primitive eventually get inlined part implementation integers 
unknown int arbitrary subgraph int unknown int unknown int unknown int unknown int unknown unknown int arbitrary subgraph int int unknown int int unknown int unknown arbitrary subgraph unknown unknown int unknown int unknown paths self compiler solves problem augmenting type information information possible paths control flow graph lead various name value value type bindings 
possible paths control flow graph represented internally compiler path objects 
path objects serve link pieces type information guaranteed occur components different union types 
path object represents possible flow control control flow graph 
type analysis compiler keeps track set path objects represent possible paths control flow graph lead node analyzed 
initial node control flow graph associated initial path object 
code basic blocks set paths propagated predecessor node successor node unchanged 
branch nodes incoming path compiler creates new path object outgoing branch capturing fact paths control flow graph possible incoming path merge nodes compiler forms union set paths incoming predecessor merge compiler associates type information just node control flow graph path node 
conceptually compiler records name value value type mappings plus kinds type information propagated cell value mappings path control flow graph 
compute type expression point control flow graph compiler forms union information expression associated path reaches node 
illustrate graph fragment shows compiler associates type information paths just nodes earlier example compiler reaches message elects split integer case 
separate paths bound int paths 
leads graph paths elegantly solve problem narrowing type appropriately split 
compiler splits set paths case splitting path path types associated split paths get narrowed implicitly 
type automatically narrowed int split type associated directly individual paths just control flow graph nodes 
splitting couched terms separating subset paths remaining paths splitting union type 
splitting subsystem compiler operates solely terms splitting paths apart knows split performed affect split various kinds type information 
parts compiler responsible deciding split order break apart splittable union type expression available common sub expression elimination combination 
compiler decided split calculates path subset satisfies desired criteria invokes splitting subsystem perform split 
unknown int int unknown int int unknown unknown int int unknown unknown arbitrary subgraph int int unknown unknown int int unknown unknown int int unknown unknown arbitrary subgraph arbitrary subgraph int int unknown unknown separation concerns simplifies organization compiler narrowing interface splitting system rest compiler 
enables certain kinds splitting operations costly awkward 
example compiler needs split branches expressions particular types normal case splitting single expression type 
occur splitting branches primitive primitive arguments right types primitive fail type error connecting loop tails loop heads described section 
compiler perform splits simply calculating paths satisfy right requirements calling splitting subsystem path subset 
splitting merge nodes splitting subsystem compiler performs actual splitting operation walking backwards control flow graph copying control flow graph node traversed 
merge node compiler examines merge node predecessors decide split paths come predecessor 
example consider control flow graph fragment say compiler wants split apart paths paths compiler split nodes merge node copied nodes left compiler examines left predecessor 
paths set paths split predecessor simply redirected copied merge node processed subset middle predecessor paths split rest remain unsplit 
compiler continues split middle predecessor branch eventually producing graph compiler examines right predecessor 
paths split half predecessor simply remains connected unsplit merge node 
splitting branch nodes branch nodes require special treatment ensure branch node predecessor split independently successors split 
consider control flow graph fragment branch node split say splitting path path left hand successor compiler copies branch node 
compiler inserts new merge node successor branch split connects copied branch node successor arm merge node compiler splits branch node predecessors branch successor split say splitting path path normal splitting rules merge nodes break apart freshly inserted merge node completely severing link branch nodes current implementation branch splitting optimizes strategy lazily creating inserting extra merge nodes 
branch node split creating merge node join unsplit successor branches compiler simply marks branch node partially split links branch node copies 
branch successor split appropriate control flow graph links simulating step breaking apart merge node inserted 
handle case successor branch node split compiler visits partially split branches splitting operation complete creates inserts necessary merge nodes 
optimization speed splitting especially branch nodes get split sides introduce complexity 
implementation paths direct implementation type information paths described quite inefficient paths similar type information 
path complete copy type information lots duplication paths 
avoid potential problem current self compiler splittable union types associates component splittable union type set paths generate type 
example compiler represents type binding variable bound type paths type paths 
values bound types splittable union types interpreted bound type paths 
information vary path path stored just concisely paths introduced 
representation type information compact straightforward representation complete copy type information path 
supports style splitting including narrowing appropriate type bindings split 
supports efficient detection paths lead different types split apart checking values bound splittable union types efficient calculation paths lead desired types examining set paths associated desired component types splittable union types 
unfortunately representation drawbacks straightforward representation 
chief drawback currently type information connected paths 
types bound values value type mapping depend path information automatically narrowed splittable union types relate path information information name value mappings cell value mappings exposed blocks lists assumed apply paths 
lack precision means information types associated values lost paths merge split apart 
example graph paths merging different information combining information merge compiler creates new union values represent bindings paths associated components splittable union types 
compiler treat value longer available available path compiler associate available value exposed blocks available value int int unknown unknown information particular paths consider exposed blocks path exposed merge compiler associated exposed block information individual paths reaching message send compiler splits apart type optimize integer receiver case producing graph compiler able narrow type path information splittable union types unfortunately compiler reclaim lost information available value path compiler restore original value bindings allow compiler eliminate redundant calculation compiler exclude unnecessary blocks exposed blocks list path 
differences lead incorrect code legal fewer available values exposed blocks required 
sacrifice information lead better code 
better implementation type information paths enable compiler reclaim type information split merge occurred 
current self compiler handles reclaiming type bindings value bindings available expressions exposed blocks lists reclaimed 
type binding case far important inlining message important optimization cases instance common subexpression eliminating single instruction 
hope opportunities optimization lost current imperfect implementation relatively minor 
exposed blocks available value int int int unknown int unknown exposed blocks unknown unknown exposed blocks available value int int unknown unknown exposed blocks unknown unknown exposed blocks int int heuristics reluctant splitting splitting lead increase compiled code space compilation time compiler includes heuristics determine splitting performed balance expected improvement run time execution speed expected costs increased code size compile times 
current self compiler factors influence decision split part control flow graph cost split terms number copied control flow graph nodes machine instructions weight split paths terms relative likelihood execution 
compiler avoids splitting cost split high weight split paths low stricter limits splits enable important optimizations 
costs cost splitting set paths remaining paths calculated sum costs control flow graph nodes copied part split 
node control flow graph associated cost determined follows kinds nodes zero cost generate machine instructions copied free 
instance name binding assignment nodes typically generate instructions register allocator arrange left right hand sides assignment register 
kinds nodes generate machine instructions arithmetic nodes compare nodes cost appropriate 
non inlined message send nodes cost account extra space required send line cache extra instructions frequently needed move send arguments locations defined calling convention 
difficult determine efficiently control flow graph nodes copied part split 
compiler determine simulating split traversing graph manner splitting operation expensive 
current self compiler represents paths connected path segments enable efficient computation paths overlap 
straight line sequence code control flow graph basic block associated single path segment object 
path objects represented list path segments traversed path 
paths pass segment corresponding basic block share single path segment object 
example control flow graph sports path segments linked different paths path segments listed path segments consed head path branch merge point code code code code code code seg seg seg seg seg seg seg seg seg seg seg seg seg seg seg seg seg seg seg seg seg seg seg seg seg seg seg path segments concise abstractions straight line chunks control flow graph 
straight line chunks code split unit cost splitting nodes chunk computed nodes type analyzed stored path segment object 
determine cost total split compiler determine path segments get copied part split sum costs path segments 
compiler compute control flow graph segments get copied part split finding path segment objects shared set paths split set paths left point split paths unsplit paths diverge 
example paths split paths compiler calculates segments seg seg seg seg shared split unsplit paths 
segments seg seg shared copied split seg shared lies point split unsplit paths diverge copied part split 
mechanism path segments efficient straightforward approach just simulating splitting operation effectively caches results operations simulation perform costs nodes basic block 
path graph smaller control flow graph traversing path graph faster traversing main control flow graph 
course optimization increases complexity compiler 
weights weight node represents compiler estimate likelihood particular node executed 
weight composed measures loop nesting depth amount 
loop nesting depth component records number loops entered exited method 
initial loop nesting depth zero 
component records compiler considers reaching point control flow graph 
example compiler considers having non integer receiver message absence information contrary consequently increases component failure branch downstream run time type test inserted part type predicting message 
type prediction described section compiler considers primitive failure instance index argument primitive integer lies bounds compiler increases component failure branch 
initial value zero higher indicating branches 
weight value zero called common case weights positive values called uncommon cases 
weights attempt record expected execution frequencies finer grain 
example branch nodes correspond normal comparisons source code successor branches equal weight weight weight predecessor branch 
precise weighting system mark downstream branches half weight branch predecessor 
unfortunately approach rapidly leads trouble 
consider series conditional branches corresponding series elseif tests 
time leaves decision tree reached weight individual leaf original weight entering decision tree 
compiler uses weight information decide splitting inlining optimizations worthwhile exponentially reducing weight calculation rules compiler decide leaves decision tree merit optimization execution decision tree leaf executed 
clearly behavior weights undesirable 
problem fined grain weights combine weights merge nodes 
basic principle computation weights obey conservation weight sum weights leaving arbitrary part control flow graph equal sum weights entering part graph execution frequency lost gained 
implies weights halved branches weights summed merges 
unfortunately fine grained approach computing weights difficult implement presence loops 
loop head node merges loop entrance branch loop tail branches created restart calls 
weight loop head defined terms loop tail weight depends loop head weight recurrence equation solved calculate weight loop 
simple approach just increment loop depth counter ignore weight loop tail branch 
approach violates restart looping primitive described section 
conservation weight weight path loop eventually connects back loop head unaccounted lost summing weights branches exit loop 
fine grained weight propagation rules difficult support weight loss extra precision frequently unneeded counter productive current self compiler uses coarser grained weights measure loop depth 
successors branch nodes equal weight predecessor branch node merge nodes take maximum weight predecessors sum weights 
rules satisfy conservation weight weight loop exit branches weight loop entrance branch ignoring effect failed type predictions primitives loop exit branches 
leaves decision tree weight entrance decision tree leaves optimized executed decision tree executed usually desired effect 
weights associated paths nodes 
weight individual paths maintained type analysis progresses adjusted loop entered loop exited uncommon branch taken 
weights paths remain unchanged paths flow merge nodes split branch nodes 
weight node maximum weights paths reaching node 
weights associated paths addition nodes splitting operation easily compute weight node split maximum weights split paths 
splitting new weight calculated way branch left 
cost weight thresholds costs weights control paths split compiler detects opportunity splitting 
compiler includes threshold values 
specify largest cost splitting operation compiler decide perform split 
defines threshold value selects relatively common path generous threshold split relatively inexpensive go ahead 
split endif uncommon path threshold split relatively inexpensive go ahead 
split endif endif currently global reluctant splitting set 
means common case nodes nodes zero weight compiler willing duplicate instructions part split limit rarely reached practice 
uncommon nodes compiler willing duplicate instructions splits increase size compiled code allowed uncommon paths 
local reluctant splitting mode enabled simply changing preventing compiler duplicating instructions part split 
current self compiler heuristics deciding perform reluctant splitting fairly crude 
weigh expected cost split terms additional machine instructions generated likelihood split branch executed split 
pieces information improve results splitting 
example current heuristics take account expected increase compilation time split indirectly dependence expected number duplicated machine instructions 
heuristics include measures expected improvement run time performance caused split weight split paths 
better set heuristics differentiate various splitting opportunities characterization expected pay example giving high pay value splitting enables inlining message relatively low pay value splitting allows constant folding common subexpression elimination performed 
profile information gathered previous executions program employed direct compiler attention important parts program 
serious problem current heuristics examine local information 
example deciding split message compiler looks costs benefits single message 
better approach take account messages operations receiver 
operations going performed expression compiler willing split operation cost split amortized subsequent uses split 
similarly compiler duplicated nodes part earlier splits reluctant split messages avoid spending compile time compiled code space splitting 
current localized view influences aspects compiler deciding type predict type case message 
global perspective compiler lead significantly better trade offs compiled code space compilation time run time performance 
related reluctant splitting similar redirecting predecessors ts compiler typed smalltalk described section 
approaches duplicate node merge postponing merge duplicated node take advantage extra information available prior merge 
differences 
reluctant splitting performed type information part type analysis redirecting predecessors performed compilation process lower level conditional branch information 
performed part type analysis inlining splitting considerations influence control flow graph gets constructed information conditional expressions split 
global reluctant splitting self compiler split arbitrary amounts code redirecting predecessors splits nodes immediately merge 
reluctant splitting similarities wegman node distinction described section 
approaches copy control flow graph nodes lead different properties code wants optimize independently 
node distinction primarily theoretical framework explaining variety code motion optimizations traditional ones novel ones practical mechanism type splitting 
main disadvantage node distinction currently formulated distinguishing criteria duplicate nodes known advance duplication takes place 
words compiler know splitting merges information 
practice compiler know merges split apart postponed merges merge normally 
information available compiler reaches message send node wants merge split apart decision split merge 
reluctant splitting practical advantage operates demand merging branches splitting information needs split apart cost effective 
potential avenue combine approaches produce theoretical framework demand driven splitting algorithms 
eager splitting self compiler supports alternate splitting strategy diametrically opposed philosophy reluctant splitting 
reluctant splitting initially merges branches demand separated eager splitting initially separates branches merge source code 
possible path control flow graph leads sequence control flow graph nodes control flow graph tree merge points 
example compiling example compiler reaches control flow graph inlining primitive self result true int int self result false int self int result iftrue compiler reaches merge node eager splitting compiler merge control pursues branch independently duplicating rest control flow graph predecessor merge processing branches independently compiler reaches final control flow graph self result true int int self int self int result iftrue result iftrue result false general eager splitting transforms control flow graphs tree shaped control flow graphs arbitrary subgraph test arbitrary subgraph arbitrary subgraph arbitrary subgraph test arbitrary subgraph arbitrary subgraph arbitrary subgraph test arbitrary subgraph arbitrary subgraph arbitrary subgraph arbitrary subgraph arbitrary subgraph test arbitrary subgraph arbitrary subgraph arbitrary subgraph arbitrary subgraph test arbitrary subgraph arbitrary subgraph eager splitting number advantages 
backtracking compilation type analysis process compiler changes mind paths split apart compiler generate best possible code relatively simple type analysis system 
path data structures required control flow graph isomorphic path data structure 
graph merges paths splittable union types needed 
consequently inaccuracies approximations induced representation path type information occur 
parts compiler type information message inliner primitive operation inliner need constantly concern path dependent type information split get specific information 
rest compiler simpler 
reluctant splitting world complexity testing splittable union types electing perform split spread parts compiler exploit type information 
eager splitting rest compiler isolated splitting issues centralized system making splitting decisions split 
eager splitting number serious drawbacks 
obvious potential exponential code explosion path control flow source code turned separate physical branch control flow graph 
cut reducing space problem self compiler performs eager splitting common case control flow graph branches uncommon branches alternate splitting technique reluctant splitting 
heuristic attempts balance better benefits costs splitting paying high code space price pay common case paths paying reduced price pay uncommon case paths 
problem remains control flow graphs containing loops graph merge loop tails merge loop heads programs containing loops suddenly infinite tree shaped expanded control flow graphs eager splitting 
solution current self compiler implementation eager splitting treat loop heads differently merge nodes allow loop tails connected loop heads form merges 
modifications pure eager splitting model compiler duplicate far code 
example common case branches merge original unsplit graph eager splitting compile completely independent copies rest control flow graph branches 
branches type information merge copies rest control flow graph completely identical 
performance measurements reported section appendix show self compiler takes order magnitude compiled code space compile time version eager splitting reluctant splitting 
observation suggests approach reducing code explosion tail merging 
general terminal branches control flow graph combined save code introducing merge connect branches 
course save compilation time compiler able detect terminal branches going identical compiling branches comparing 
approach called tail merging similar traditional optimization combines ends arms conditional statement 
easy way detecting branches generate code check type information branches 
branches generate code merged 
prevents unnecessary duplication diamonds control flow graph conditional branches corresponding merges arms conditional sides diamond affect type information entering diamond type information exiting diamond merge branches 
unfortunately simple approach overly conservative consequently save compiled code space compilation time desired compilation speed compiled code density improve factor eager splitting tail merging measurements reported section appendix fall short performance reluctant splitting 
form tail merging misses cases branches start different type information producing identical control flow graphs rest control flow graph depend available type information 
enable merging control flow graph enabled simple form tail merging self compiler includes technique call reverse requirements analysis identifies subset available type information compiling terminal branch control flow graph 
compiler allows tail merging branches required type information ignoring unused type information 
integrating forward type analysis splitting tail merging decisions backwards requirements analysis somewhat tricky 
compiler type analyzes control flow graph depth manner 
tip control flow graph reached sort return node graph successors compiler scans backwards graph branch point successor forward analyzed 
part backwards traversal compiler accumulates assumptions requirements generated nodes type information 
way compiler determines subset type information forwards compilation branch control flow graph 
determining generated branch merged previously compiled branch compiler checks see type information available generated branch compatible required previously compiled branch computed reverse requirements analysis 
compatible compiler merges branches 
compiler continues generating compiled branch separately checking potential merges points compilation process 
loops complicate reverse requirements analysis complicate forward data flow analysis traditional compilers 
requirements loop head computed requirements imposed loop body branches downstream loop body turn depend requirements loop head requirements imposed loop tail depends requirements imposed loop head 
tail merging loop bodies complicated merging depends requirements analysis performed arbitrary subgraph test arbitrary subgraph arbitrary subgraph arbitrary subgraph arbitrary subgraph arbitrary subgraph test arbitrary subgraph arbitrary subgraph arbitrary subgraph arbitrary subgraph test arbitrary subgraph arbitrary subgraph return return return return forward type analysis backward requirements analysis loop completely constructed 
current implementation requirements analysis side steps problems assuming conservatively body loop depends type information available loop head 
eliminates problem iteratively computing requirements sacrifices opportunities tail merging 
know important sacrifice 
unfortunately tail merging requirements analysis eager splitting takes compile time results reported section appendix sophisticated form eager splitting consumes twice compile time compiled code space local reluctant splitting 
ultimately eager splitting may deemed unusable inherently inflexible relative strategies reluctant splitting 
reluctant splitting compiler writer trade compilation speed execution speed varying cost weight splitting threshold values 
eager splitting difficult compiler generate optimized code speed compilation 
eager splitting compiler assumed certain piece type information generating code control flow graph node decision easily changed cost decision deemed great 
consider example compiler pursues branch control flow graph node assumes value variable constant 
second branch generated identical branch node assumes value variable constant 
cost depending value constant may high branches long identical 
compiler hard time detecting cost particular particular constant type high generating branches completion 
reluctant splitting problem compiler decide point compiling node split earlier branches apart 
eager splitting errs side splitting branches apart reluctant splitting errs side keeping branches merged 
behavior reluctant splitting probably preferable eager splitting 
divided splitting divided splitting practical compromise reluctant splitting eager splitting 
description simple compiler eagerly separates common case paths uncommon case paths separates individual common case paths 
general divided splitting transforms control flow graphs control flow graphs common case paths completely separated paths half split apart desirable arbitrary subgraph test arbitrary subgraph arbitrary subgraph arbitrary subgraph test arbitrary subgraph arbitrary subgraph arbitrary subgraph arbitrary subgraph arbitrary subgraph test arbitrary subgraph arbitrary subgraph arbitrary subgraph test arbitrary subgraph arbitrary subgraph test arbitrary subgraph arbitrary subgraph divided splitting double size control flow graph straight reluctant splitting exponentially growing size control flow graph characteristic pure eager splitting 
divided splitting engineering compromise motivated practical concerns empirical observations programs 
cases compiler want split common case paths apart uncommon case paths 
common case paths usually sends inlined away leading precise type information values available common subexpression elimination fewer exposed blocks 
divided splitting splits apart common case uncommon case paths 
splitting apart various common case paths nearly cut conservative reluctant splitting algorithm cases 
divided splitting improves current path reluctant splitting algorithm effectively placing firewall type information common case paths type information uncommon paths sets type information mix firewall 
prevents uncommon case type information common case type information 
reluctant splitting perfect loss degradation type information decision merge branches changed opportunities optimizations missed process divided splitting interesting 
reality reluctant splitting perfect especially currently implemented path objects value type bindings 
common case uncommon case paths continually merged split apart typically case pure reluctant splitting loss type information caused imprecision implementation reluctant splitting take toll 
typically mean type information contain available values heap cell information uncommon case type information normally current reluctant splitting implementation parameterize information path 
contrast divided splitting common case type information mixed uncommon case information uncommon case paths cause type information common case paths lost 
cases common case path divided reluctant splitting approach code quality pure eager splitting compile time costs pure reluctant splitting 
results reported section appendix divided splitting speeds self programs base local reluctant splitting algorithm additional compile time compiled code space costs 
divided splitting allows self implementation imperfect faster implementation reluctant splitting sacrificing code quality 
lazy compilation uncommon branches techniques self compiler distinguish common case parts control flow graph uncommon case parts 
techniques divided splitting designed specifically separate common case paths uncommon case paths 
burden compiling uncommon case paths fairly high parts control flow graph quite large handling unusual events happen 
furthermore uncommon branches frequent compiled code 
example primitive operations arithmetic fail way leading uncommon case branch needs compiled 
add injury uncommon cases expected uncommon occurring rarely possible uncommon events occur practice 
compiler effort wasted 
self compiler takes advantage skewed distribution execution frequency different parts control flow graph compiling uncommon parts methods uncommon events occur call technique lazy compilation uncommon branches 
compiler compiling method time pursues common case paths control flow graph 
entrances uncommon case paths instance failed type prediction conditional branch branch overflow conditional branch lazy compilation uncommon branches suggested john maloney graduate student university washington 
compiler generates code call routine run time system 
simple integer addition primitive generating graph compiler generate graph result value failure block result add self arg self int 
arg int 
overflow 
self int arg int arg unknown self unknown self unknown int arg unknown int result int result unknown self int unknown int arg int unknown int result int unknown result add self arg self int 
arg int 
overflow 
self int arg int arg unknown self unknown self unknown int arg unknown int result int self int arg int lazy compilation number advantages 
foremost dramatically reduces compile time compiled code space compiling uncommon branches practice 
uncommon branches typically larger normal common case code uncommon branch code far run time type tests space consuming message sends common case branches 
expectations borne practice measurements reported section show lazy compilation uncommon branches speeds compilation factor factor compiled code space costs fall amount 
lazy compilation subtle advantages 
lazy compilation automatically provides advantages divided splitting 
uncommon branches effectively split eagerly common case paths just divided splitting compiling 
providing advantages divided splitting fringe benefit lazy compilation presents rest type analysis splitting system simpler control flow graph frequently remaining path 
allows compiler rely simpler accurate type analysis splitting implementation sacrificing performance needing implement divided splitting 
subtle advantage lazy compilation eases problem register allocation 
described section self compiler performs global register allocation allocates name single location entire lifetime 
names survive calls allocated certain locations caller save registers 
uncommon case branches contain far calls common case branches eliminating uncommon case branches eliminates register allocation restrictions allows self compiler simple allocation strategy produce reasonable allocation 
improved type analysis lazy compilation improves execution speed self programs results section 
procedure called compiler starts compiles additional chunk code called uncommon branch extension method handle control flow graph point failure led uncommon branch original method 
current implementation uncommon branch extension method takes original common case method run time stack frame returns original common case method caller completing original method charter 
uncommon branch extension method return middle common case version common case version freed handling uncommon cases merging back main common case flow control 
allows compiler generate excellent code common cases cost somewhat larger compiled code space usage 
analogously line caching described section compiler call routine replacing call newly compiled uncommon branch extension code 
compiler beliefs uncommon proved incorrect compiler distinguish common uncommon branches uncommon branch extension 
compiler applies conservative splitting strategy currently local reluctant splitting compiling uncommon branch extension compiler perform lazy compilation uncommon branch extension method 
strategy limits amount compile time compiled code space spent uncommon branches 
current implementation compiling uncommon branches treats extensions new methods ways subroutines called common case version method 
alternative strategy recompile original method uncommon branch taken time uncommon branches compiled line stub routines replace old common case code general cases code modify existing stack frame accordingly restart new method point entrance uncommon branch 
current strategy significantly simpler implement alternative strategy may take compiled code space compile time cases different uncommon branch entrances taken different invocations original method 
current strategy separate uncommon branch entry point lead separate uncommon branch extension method alternative strategy original common case method generalized handle uncommon branches step 
fortunately observed multiple uncommon branch extensions common case method content remain simpler strategy 
operations performed uncommon branch extension adjust stack frame size execute register moves required shift common case method register allocation uncommon extension register allocation 
summary self compiler uses splitting transform single polymorphic piece code multiple monomorphic versions optimized independently trading away compiled code space significantly improved execution speed 
reluctant splitting records information merge points merge reversed specific type information available merge helpful merge simply postponed point desires specific information 
path objects enable compiler narrow multiple union types split keeping type information fairly accurate face repeated merging splitting 
compiler includes heuristics estimated space cost split estimated execution frequency split nodes decide benefits potential split outweigh costs 
demand driven nature reluctant splitting works practice balancing compilation time quality generated code exploiting splitting opportunities single forward pass control flow graph 
eager splitting alternative splitting strategy initially splits merges apart assuming merges split apart anyway 
eager splitting offers simpler possibly faster implementation splitting backtracking loss type information merges split apart 
unfortunately pure eager splitting leads exponential increase size control flow graph compilation time 
avoid unnecessary code duplication eager splitting augmented tail merging technique merges branches second branch generate control flow graph generated branch 
compiler implements forms tail merging forward computed available type information precise reverse computed required type information 
tail merging extensions reduce compilation time eager splitting somewhat compilation time long eager splitting currently implemented practical splitting alternative 
divided splitting hybrid approach strives provide precision eager splitting costs reluctant splitting 
divided splitting compiler eagerly splits apart common case paths uncommon case paths uses reluctant splitting split apart common case paths 
divided splitting overcomes weaknesses current implementation reluctant splitting placing firewall relatively precise type information available common case paths relatively imprecise type information uncommon case paths 
compilation speed divided splitting roughly compilation speed pure reluctant splitting significantly better code quality 
self compiler saves great deal compilation time compiled code space improves execution performance compiling uncommon branches lazily 
method compiled common case paths compiled stubs generated branches uncommon case paths simply call routine run time system 
uncommon branches taken compiler generate code uncommon branches 
division automatically provides effect divided splitting additional effort lazy compilation 
lazy compilation enables self implementation cake eat 
common case paths typically correspond paths ones supported semantics traditional languages compile time run time speed self close traditional languages systems generating code self supports extra power example generic arithmetic programmer pays advanced features 
chapter type analysis splitting loops loops pose special problems type analysis 
basic problem loop head kind merge node type information predecessors loop head loop tail backwards branch depends type information loop head creating circularity type analysis 
chapter describes type analysis presence loops discusses synergistic interactions loop type analysis splitting 
pessimistic type analysis approach breaking circularity assume general possible value unknown value type unknown type head loop names potentially assigned loop 
bindings guaranteed compatible bindings subsequently computed loop tail type analysis remain conservative produce legal control flow graphs 
example graph type automatically generalized unknown type loop head assignment loop unfortunately approach call pessimistic type analysis sacrifice type information precisely places program need information generate best possible code inner loops programs 
loop head loop entry arbitrary subgraph loop tail loop exit int arbitrary subgraph arbitrary subgraph unknown traditional iterative data flow analysis traditional compilers resolve similar circularities data flow analysis algorithms performing analysis iteratively 
iterative data flow analysis begins assuming data flow information loop head usually just information computed loop entry branch analyzes loop body recomputes data flow information loop head time including results loop tail 
information unchanged fixed point analysis reached analysis done 
information loop head changed previous analysis results incorrect compiler loop body new loop head information 
iteration eventually terminate final fixed point domain computed information finite lattice partial ordering unique greatest elements representing best worst possible fixed points data flow propagation functions monotonically increasing 
traditionally applied iterative data flow analysis operates fixed control flow graph 
compiler constructs control flow graph performs iterative flow analysis compute property interest nodes graph performs optimization transformation graph computed information 
optimizations need applied modified control flow graph data flow analysis frameworks require recomputing interesting information scratch new control flow graph 
incrementally updating data flow information modification control flow graph open research problem 
self compiler computed type information loops naively applying iterative flow analyzer control flow graph prior type information perform optimizations inlining splitting computed type information poor virtually bad computed pessimistic type analysis 
surprising result consequence self messages computation computation simple arithmetic instance variable accesses 
messages loop inlined iterative flow analyzer applied compute types variables accessed loop variables assigned results messages including messages conservatively assumed unknown type compiler know type result non inlined message 
example naive iterative flow analysis compute type graph unknown type result non inlined message unknown iterative type analysis self compiler uses technique call iterative type analysis compute precise types variables modified loops 
iterative type analysis extension traditional iterative data flow analysis designed cope changing control flow graphs 
iterative data flow analysis self compiler begins compiling loop assuming certain set types loop head 
traditional flow analysis self compiler goes ahead compiles loop initial assumed types 
self compiler analysis may change body loop int arbitrary subgraph arbitrary subgraph unknown unknown part analysis inlining messages splitting apart sections loop body 
loop tail reached types computed analysis loop tail compared types assumed loop head 
compatible loop tail connected loop head compiler done 
analysis iterate compiling new version loop general types 
example faced control flow graph iterative type analysis compiler assume set types loop head derived loop entry types case integer compiler compile body loop assumed types 
compiler reaches message receiver determined integer assuming assignments loop int arbitrary subgraph arbitrary subgraph int arbitrary subgraph arbitrary subgraph int compiler inline message contained primitive add machine instruction overflow check analyzing rest control flow graph determined integer loop tail compatible types assumed loop head compiler done type information computed iterative type analysis better type information computed pessimistic type analysis naive iterative data flow analysis 
message inlined away extra overhead iterative type analysis compute type remains integer loop 
approaches compute type unknown top loop best run time type test need inserted check integer receiver 
messages optimized type prediction compiler prevented inlining iterative type analysis 
int arbitrary subgraph arbitrary subgraph int add int overflow 
int int arbitrary subgraph arbitrary subgraph int add int overflow 
int int int earlier description iterative type analysis algorithm purposely vague points type information assumed loop head initially 
type information computed loop tail compatible type information assumed loop head allowing loop tail connected loop head 
exactly analysis iterate loop tail type information compatible loop head type information 
questions reasonable answer different combinations answers result different trade offs execution speed compilation speed compiler simplicity 
iterative type analysis revealed framework family algorithms 
subsections provide current self compiler answers questions different answers earlier self compilers 
question compatibility taken answer question impacts answers questions 
compatibility central question types computed loop tail considered compatible types assumed loop head 
extreme approach consider loop tail type compatible loop head type loop head type contains loop tail type treating types sets values described section 
approach loop tails compatible loop heads possible helping iterative analysis reach fixed point quickly saving compile time 
unfortunately approach sacrifice type information available loop tail form precise types connecting loop tail loop head compiled assuming specific type information 
example variable bound unknown type loop head integer map type loop tail compatibility rule consider bindings compatible allow loop tail connected loop head 
legal sacrifice type information available loop tail lead better generated code loop tail connected loop head 
want type compatibility rule sacrifice compiler hard won type information 
opposite extreme position compatibility consider types compatible exactly 
position avoid loss type information loop tail exactly type information loop head connect easily lead iterations part iterative type analysis 
example different types relating integers power set different integer constants empty set type analysis iterate times definition compatibility 
impractical define type compatibility narrowly 
current self compiler uses type compatibility rule midway extreme positions 
type loop tail considered compatible type loop head loop head type contains loop tail type loop head type lose map level type information 
loop tail type constant type integer subrange type loop head type general enclosing map type compatible 
loop tail type union type components different maps loop head type general union corresponding map types 
compatibility rule implements heuristic map level type information important kind type information provides lion share optimization opportunities map level type information general kind supports static binding inlining messages 
practice compiler encounter different map types single variable fixed point iterative type analysis reached fairly quickly 
strategy amended situations common case uncommon case branches exist uncommon branch extension methods described section simulating version system lazy compilation uncommon branches 
compiler conservative compiling uncommon branches connecting common case loop tail uncommon case loop head sacrifice performance common case path 
compiler considers loop tail compatible loop head weight loop head great weight loop tail 
additionally minimize compilation time spent uncommon branches compiler uses conservative strategy type compatibility considering true mainly currently ways compiler infer map level type information 
characteristic may true adaptive recompilation polymorphic line caches hcu type compatibility rule may need revised keep number iterations needed reach fixed point 
uncommon case loop tail compatible uncommon case loop head types loop head merely contain types loop tail 
uncommon branches conserving compilation time space important preserving map level type information 
initial loop head type information question answered exactly types assumed initially loop head 
obvious strategy assume loop head type information loop entry type information 
approach simple precise loop entry type information enable compiler compute precise fixed points 
unfortunately approach frequently cause algorithm iterate cases name assigned type body loop different initially assumed entrance loop 
example loop typical standard loop written self loop counter initialized zero incremented body loop loop head types assumed loop entry types iteration inlining message constant folding primitive compiler produce graph type counter computed loop tail compatible type assumed loop head type assumed loop head contain corresponding type loop tail 
consequently analysis required iterate general type constant type 
loops modify loop counter initial constant value initial throw away iteration expected loop structures 
counter arbitrary subgraph counter counter counter counter arbitrary subgraph counter counter counter counter counter assignable local variables assigned body loop value different initial value computed entrance loop 
avoid extra iteration current self compiler generalizes types assignable names loop head corresponding types loop entrance 
preserve map level type information generalization enclosing map type union map types loop entrance type union type 
loop example compiler generalize initial type loop counter integer map type compiling loop body compiler reach graph type loop tail compatible type loop head iteration additional iterations needed 
counter arbitrary subgraph counter counter int counter counter counter arbitrary subgraph counter counter int counter int counter add counter overflow 
counter int generalization heuristic works cases assignments local variables map initial value case integer loop counters 
usually saves iteration iterative type analysis version iterative type analysis earlier compiler 
situations generalization assignable names overly pessimistic 
variables may assignable assigned loop analyzed generalizing types may lose type information 
example loop loop counters assignable outer loop counter assigned inner loop 
inner loop head current self compiler generalize type outer loop counter integer subrange type computed initial comparison outer loop upper bound general integer map type 
unnecessary generalization sacrifice possibilities optimization applying integer subrange analysis eliminate unneeded array bounds checks arithmetic overflow checks involving outer loop counter 
better approach lose kind type information determine variables assigned particular loop generalize variables 
analysis complicated fact assignments occur blocks making body loop control structure accurately determining variables assigned loop require computing sort transitive closure blocks invoked messages sent loop 
implemented approach solve doubly nested loop problem 
minimize compilation time uncommon case loops self compiler generalizes types assignable variables way unknown type uncommon case loop heads just enclosing map type common case loop heads 
conjunction lenient rules type compatibility uncommon case loop tails generalization ensures uncommon case loop tail connects uncommon case loop head pass iterations needed uncommon case loops 
iterating analysis final question remaining answered complete description iterative type analysis happens loop tail compatible loop head forcing analysis iterate 
traditional iterative data flow analysis information previously computed nodes loop thrown away new information computed loop head combining information assumed previous iteration information computed loop tail information body loop recomputed new general loop head information 
approach directly applied iterative type analysis part analyzing body loop control flow graph optimized information 
type information assumed loop head determined overly optimistic optimizations performed information longer valid 
backing optimizations inlining splitting difficult 
approach taken self compiler iterative type analysis create fresh new copy part control flow graph representing loop body reapply type analysis fresh unoptimized copy 
fresh loop head simply connected downstream previous iteration incompatible loop tail effect unrolling loop new type information 
example reaching loop tail iteration proves incompatible loop head compiler simply create new copy loop connect previous copy loop tail unrolling loop bodies way important fringe benefits 
type tests code get hoisted normal loop body earlier versions 
example type variable unknown top initial loop body loop head assume type variable unknown 
variable treated integer inside loop sending messages compiler type prediction insert run time type test splitting compile separate path loop body variable known bound integer 
loop tail reached integer type compatible unknown type loop head type separate version loop body compiled just variable bound integer 
second version loop contain type tests loop body variable bound integer 
turns common case expected control pass initial version loop remain faster integer specific version loop 
unrolling approach quite simple implement 
strategy may waste compiled code space compile time unrolled copies similar 
compiled code space reclaimed generalizing types assignable names iteration described section usually saving iteration correspondingly copy loop body 
alternative approach replace original overly optimized loop body fresh new loop body redirecting predecessors original loop head flow fresh new loop head 
allow compiler conserve compiled code space compile time sharing similar parts loop body separate copies 
unfortunately alternative approach complex implement require mechanism ensure new loop head starts general type information just predecessors indicate 
path type information exacerbates problems 
research explore designs iterating type analysis conserve compiled code space compile time path type information minimize compiler complexity 
arbitrary subgraph arbitrary subgraph arbitrary subgraph iterative type analysis splitting iterative type analysis loops splitting carefully crafted enable self compiler compile multiple versions loop version optimized different combinations run time types 
inner loops programs important optimize compiling separate versions loops different run time types self compiler able generate code loops 
loop heads just special kind merge node split apart just merge nodes 
example branches merge loop head branch variable type integer branch variable type floating point number compiler split apart loop head merge node variable sent message loop splitting create multiple loop heads single loop loop head different types 
current self compiler unrolling approach iteration creates multiple loop heads different types 
consequently loop tail may loop heads choose trying connect type compatible loop head 
compiler tries find highest weight loop head compatible types connect loop tail 
int arbitrary subgraph arbitrary subgraph int float float int arbitrary subgraph arbitrary subgraph int float arbitrary subgraph float int float loop head compatible compiler tries split loop tail match loop head 
example inlining away messages compiling rest loop compiler reaches graph loop tail type int float directly match loop head types 
compiler tries find subset paths loop tail split connected loop head 
compiler determines left hand path producing integer type binding compatible left hand loop head split int arbitrary subgraph arbitrary subgraph int float arbitrary subgraph float int float add overflow 
int float int float int float compiler splits loop tail accordingly assuming cost splitting low connects split loop tail get graph splitting loop tail connecting copies compiler turns attention remaining loop tail copy 
loop tail checked available loop heads possibly split connected directly loop head 
process continues loop tail successfully connected loop head case compiler finished compiling loop loop tail split connected loop head case compiler falls back unrolling strategy creating fresh copy loop new loop head loop tail combination types 
process splitting loop tails match loop heads guaranteed terminate successful connection unrolling loop split number paths leftover loop tail decreases 
int arbitrary subgraph arbitrary subgraph int float arbitrary subgraph float int float add overflow 
int float int int arbitrary subgraph float int float int float case example loop compiler determines leftover loop tail compatible right hand loop head connects get control flow graph effect splitting loop heads subsequently splitting loop tails leads compiler generate multiple versions single source loop compiled version different type bindings 
example compiler produced completely independent compiled versions original source loop version case integer separate version case floating point number 
version loop optimized particular combination types faster single general version loop 
example compiler constrained generate single version loop compiler forced type casing optimize message integer floating point cases introducing extra run time overhead type test 
extra overhead significant number operations loop split apart increases 
splitting loops different types enables self compiler compete optimizing compilers traditional languages execution performance sacrificing extra power available self programmers pure message passing generic arithmetic support 
typically version versions loop split optimized common case types types include equivalent program traditional language integers fixed length arrays 
common case version loop achieve quite performance compiler compiling version loop specific common case types 
self compiler gets nearly information available compiler traditional language generate code runs nearly fast generated compiler traditional language 
self compiler generate extra version loop handle types situations arise fall category common case types situations extra version loop implements additional semantics available self form pure message passing generic arithmetic 
lazy compilation extra version uncommon branch extension method usually compiled 
separation common case version uncommon case version primary mechanism self compiler rival performance traditional languages 
int arbitrary subgraph arbitrary subgraph int float arbitrary subgraph float int float add overflow 
int float int int arbitrary subgraph float int float int float summary self compiler uses iterative type analysis infer relatively precise types presence loops 
algorithm performs optimizations inlining splitting body loop part iteration analysis 
result compiler computes precise type information possible standard data flow analysis algorithm 
self compiler uses various heuristics reach fixed point quickly possible sacrificing significant amount type information 
heuristics include automatically generalizing types assignable names enclosing map type loop heads connecting loop tails loop heads long loop head sacrifice map level type information 
loop tail match available loop head loop unrolled appending fresh copy loop body loop tail continuing analysis 
self compiler treats loop heads merge nodes split loop head apart node downstream loop head wants type information diluted loop head merge node merge node loop head 
splitting creates multiple loop heads choose connecting loop tail loop head 
unrolling loop loop tail compatible available loop head creates loop heads loop tail choose 
loop tail split apart directly match loop head subset paths reach loop tail match loop head 
loop heads loop tails split apart self compiler frequently ends creating completely independent versions loop loop head leading matching loop tail 
versions faster single general version loop consequently generating multiple versions loops different run time types key sources self execution speed 
combined lazy compilation uncommon branches self compiler frequently compiles common case versions loop handle standard situations additional general version loop compiled needed 
chapter back compiler type analysis inlining splitting performed simultaneously pass self compiler 
chapter describes remaining passes self compiler describing optimizations included competitive traditional optimizing compilers 
global register allocation ways register allocation important optimization traditional compilers 
effect register allocation felt compiled code optimizations poor allocator hurt performance improvements caused optimizations inconsequential 
register allocation historically important implementations pure object oriented languages kinds high level languages primarily high level languages contained procedure calls register allocation secondary importance 
register allocation algorithms flush caller save registers home stack locations procedure calls language implementations procedure calls straight line code calls justify simplest allocators 
contrast self compiler designed eliminate procedure calls frequently compiles versions loops internal calls 
global register allocation important especially light goal rival performance traditional optimizing compilers 
straightforward register allocator assign variable source program register stack location simultaneously live variables assigned register stack location 
naive approach poorly environment self method inlining commonplace formal variable name inlined method simultaneously live corresponding actual parameter consequently variables allocated separate locations variables contain value 
layers inlined methods duplication conceptually require single location introduce huge overhead register copying layers inlined methods entered exited 
better system perform copy propagation prior register allocation replacing uses formal variables inlined routines uses corresponding actuals 
unfortunately straightforward application copy propagation supporting complete source level debugging difficult variable name replaced different variable names different points lifetime 
self compiler incorporates alternative strategy introducing new abstraction name assigned location called variable 
names aliases actuals formals mapped variable object variable object assigned single run time location register stack location marked compile time constant need run time location 
assigning locations variables names self compiler eliminates impact artificial name distinctions caused inlining reduces self register allocation problem similar faced traditional optimizing compilers built control structures limited inlining 
earlier version self compiler included ambitious register allocator 
allocating locations names variables earlier compiler allocated locations values 
perfect sense values precisely run time bit patterns manipulated program names merely convenient handles programmers compiler refer values 
earlier compiler went extreme allowing value straight line inter segment value lifetime independently allocated location marked compile time constant run time existence 
design provided allocator lot freedom implement various parts value lifetime different ways 
unfortunately earlier register allocation design main drawbacks earlier allocator slow 
primarily caused extremely fine granularity allocation single lifetime segment 
current implementation register allocators allocates lifetimes single step runs faster 
earlier allocator hard sure long sequences lifetime segments uses single value allocated location register moves inserted middle value lifetime unnecessarily 
additionally name bound variables lifetime counter gets incremented time loop get values allocated location unnecessary register moves inserted assignments name 
naturally allocator imperfect introducing register moves awkward places generated code consequently earlier self compiler achieve level code quality traditional register allocator traditional register allocators easier job usually support complete source level debugging massive inlining user defined control structures 
current self compiler register allocator requiring name bound single variable entire lifetime variable allocated single location lifetime 
earlier register allocator supported source level debugger 
intention compiler generate tables describing mappings names values values registers part information generated debugger 
current version debugging information described detail section mappings indexed hardware program counter mappings depended compiled code program stopped 
unfortunately design space representation mappings proved difficult completed 
current self compiler avoids complexity time varying allocations restricting mappings position independent 
compared earlier ambitious register allocation strategy current strategy simple fast reasonably effective 
assigning variables names compiler constructs name variable mapping part type analysis 
compiler internal representation name object refers corresponding variable object variable keeps track names refer called variable alias set 
name assigned initial value name simply added alias set variable associated right hand side assignment 
name subsequently assigned new value longer considered alias initializing expression consequently name removed old alias set allocated fresh variable object 
type analysis completes name associated single variable represents name alias set names contain value allocated location 
type analysis compiler records extra information names occurrence name operand run time expression 
information records preferred location particular register stack location required part parameter passing kind location address register architecture plus weight control flow graph node containing weights described section 
extra information helps register allocator avoid unnecessary register movement concentrate efforts frequently executed parts control flow graph 
live variable analysis eliminating redundant constants described section unneeded computations described section compiler begins actual register allocation phase 
global register allocators self compiler computes register interference graph determine lifetimes names overlap 
nodes interference graph variable objects name objects 
compute register interference graph compiler backwards pass control flow graph 
compiler passes graph maintains set live variables computed set variables downstream control flow graph initial definitions 
compiler reaches variable initial definition compiler removes variable live variable set 
compiler reaches variable live variable set variable compiler adds newly live variable live variable set adds interference links newly live variable variables live variable set 
scanning backwards control flow graph compiler detect variable lifetime encounters occurrence variable backwards pass 
variable lifetime detected corresponding forward scan initial assignment name known statically represented different kind control flow graph node control flow graph constructed 
merges branches loops complicate backwards traversal 
compiler reaches merge node backwards traversal places merge predecessors special stack live variable set pairs representing pending compiler processes merge predecessor 
compiler reaches particular branch node time live variable analysis compiler processing path wait branch successor analyzed compiler process branch predecessors 
compiler records current live variable set branch node pops pair pending nodes stack resumes processing node corresponding live variable set 
branch node reached second time successor branch branch predecessor processed live variable set branch predecessor union current live variable set live variable set recorded part earlier visit branch node 
absence loops search pattern guarantees node processed successors processed traverses graph reverse topological order ensuring correctness computed live variable sets 
unfortunately presence loops nodes processed successors sort iterative algorithm needed compute fixed point live variable sets 
compiler runs pending pairs process remains branch node predecessor processed successor eventually leads restart backward branch earlier loop head compiler simply forces branch node process predecessor early 
compiler initially assumes live variables unprocessed successor subset live variables processed successor 
branch node successor processed compiler verify assumption correct 
compiler done branch node find nodes process 
second successor live variables successor live variable set earlier analysis branch node predecessor inadequate live variable analysis iterate reprocessing branch node predecessor larger live variable set 
completely repeating analysis case normal iterative data flow algorithm self compiler uses special mode live variable analysis update earlier results incrementally 
mode compiler propagates difference previous live variable set new live variable set 
incremental live variable set initialized set difference live variable sets second predecessor predecessor 
incremental analysis differs normal non incremental analysis variable needs added incremental live variable set variable previous normal live variable set difference previous new live variable sets 
variables removed incremental live variable set initial definitions reached longer appear new live variable set 
incremental live variable set empty previous new live variable sets incremental analysis path 
incremental re analysis faster nonincremental re analysis incremental live variable sets smaller faster check incremental live variable set empty check previous current live variable sets 
compiler represents live variable sets variable sets interfering variables dual bit vector linked list data structure 
representation sets supports constant time set member testing bit vectors variable allocated unique integer index position bit vectors linear time size set iteration elements set linked lists 
execution profiling self compiler shows extra space cost representations set extra compilation time cost copy representations set merge nodes significant 
dual representation supports set operations better traditional representation 
register selection register interference graph built compiler visits variable allocates location 
compiler uses weights uses names associated variables determine order allocate variables registers highest weight variables allocated 
variable run time uses variable compile time constant variable source level variable visible outside world allocated run time location 
preferences specified uses names associated variable pick register stack location different chosen interfering variable 
allocation strategy simple fast 
limited allocating single location compile time constant variable 
name associated single variable completely disjoint portions name lifetime created splitting allocated different registers importantly associated different compile time constants 
especially common names bound true constant false constant results comparison operations 
contents name visible debugger current register allocation limitation name allocated run time location location initialized run time true false 
true program need run time value splitting value encoded position program just traditional compilers encode result boolean tests statements positions compiled code 
fortunately common circumstances particular problem side stepped creating new names formals inlined methods reusing names actuals natural concise specially handling debugger view expressions arguments inlined methods 
example iftrue message sent result comparison typically iftrue methods get inlined receivers type true receivers type false 
name formal name corresponding actual case single name bound result comparison reused names receiver formals inlined versions iftrue 
formal visible debugger approach cause register allocator allocate run time location hold value result comparison 
current compiler creates new names formals assigns actuals formals 
receivers iftrue methods names correctly allocated compiletime constants 
original comparison result name longer debugger left unallocated 
common case compiler introduce extra run time overhead 
general limitation single compile time constant location name problem 
earlier register allocator able solve problem allowing allocation different different parts control flow graph proved inefficient practical 
register allocators compilers frequently allocate disjoint subregions variable lifetime different registers split variable lifetime separately regions ch ch systems simultaneously support complete source level debugging 
continue search happy medium register allocator combines form flexible position dependent allocation high allocation speed compact debugging information 
inserting register moves allocation pass control flow graph inserts register move control flow graph nodes needed 
places compiler inserts register moves include assignments nodes left right hand sides allocated different registers message send nodes move arguments locations required calling convention 
compiler computation variables set aliased names large possible 
example consider simple sequence appearing looping control structures 
executing message executing message original value result simultaneously live holding different values 
consequently compiler assigns expressions different variables 
compiler knows integer inlines primitive call simple add instruction ignoring overflow check moment add original value add instruction 
actual run time lifetimes result message overlap usually allocated register 
say different variables adjacent lifetimes overlap variable assigned 
adjacent variables usually allocated location minimize register moves variable assigned 
way achieving combine variables single larger variable transformation called subsumption 
unfortunately combining performed part forwards type analysis second variable lifetime begins message compiler know additional uses variable 
may possible augment live variable analysis pass detect adjacent variables mark extend register allocator allocate adjacent variables location 
alternative approach compute variables separate backwards pass type analysis providing information analysis implicitly coalescing different adjacent variables 
common subexpression elimination constants type analysis compiler performs pass control flow graph eliminate redundant loads constants 
pass creates extra names represent available constants replaces subsequent redundant loads constants simple assignments new names 
unfortunately technique interact current register allocation strategy 
new variable objects created phase newly created names constants 
distinct variables allocated independently frequently different registers described section extra register moves may inserted point initial constant load instruction register new variable different register result load instruction eliminated constant new variable allocated register different original variable 
extra register moves diminish benefits common subexpression elimination constants 
problems avoided performing common subexpression elimination constants pass type analysis name variable mapping construction eliminating extra register move saved constant unfortunately subtle interactions parts compiler complicate approach 
techniques allocating adjacent variables location described section help 
ignoring problems common subexpression elimination constants fact common subexpression elimination relatively cheap computation arithmetic operation slow programs variable constant allocated stack location register 
storing subsequently fetching constant stack location significantly redundantly loading bit constant value register 
common subexpression elimination register allocation avoid sorts inadvertent 
unfortunately kind inter pass cooperation notoriously difficult manage 
practice common subexpression elimination constants mixed effect 
results shown section appendix benchmarks speed optimization slow 
clearly technique redesigned reimplemented improve effectiveness 
eliminating unneeded computations third phase self compiler scans names find value computed 
computation exists side effects simple assignment load constant instruction memory fetch instruction arithmetic instruction object clone primitive call call side effect free primitive compiler simply splices unnecessary operation control flow graph 
optimization improves performance self programs percent average measurements shown section 
filling delay slots register allocation sparc version self compiler attempts fill branch call delay slots 
sparc architecture branch instructions instruction delay slot executed executed branch taken branch 
self compiler attempts fill delay slots useful instructions preferably branch instruction successor remaining branch successor 
call jump return instructions similarly instruction delay slots self compiler attempts fill instruction 
nodes control flow graph generate machine instructions assignment nodes bookkeeping nodes searching instruction move delay slot may scan control flow graph nodes locating node generates single instruction finding node fit delay slot message send node 
node put delay slot nodes branch call target node spliced control flow graph adjusting predecessor successor links branch call appropriately removed chain nodes stored branch call node code generation 
searching instruction branch node compiler encounters merge node compiler attempts locate candidate instruction merge node 
finds compiler duplicates control flow graph merge located node delaying merge located node 
normal mechanism splicing nodes branch target node applies branch target automatically adjusted jump instruction original merge point part splicing operation 
splitting merges filling delay slots important fill delay slots possible 
additional code space needed kind splitting real instruction copied inserted delay slot wasted 
filling delay slots important performance 
results shown section delay slot filling improves performance benchmarks reduces compiled code space consumption average 
code generators risc machines perform additional kinds scheduling instructions reordering memory loads minimize stalls caused waiting result load 
current self compiler currently instruction scheduling 
code generation final pass compiler traverses control flow graph generates native machine code 
instruction selection nearly trivial self compiler compiler designed primarily modern risc machines simple regular instruction sets 
peephole style optimizations performed cases needed 
compiler generates debugging information described section pass 
generation complete compiler passes buffers hold instructions debugging information compiled code cache manager creates new compiled method adds cache possibly throwing old methods compacting cache room new method 
compiler returns starting address compiled code run time message lookup routine invoked compiler 
lookup routine completes message send jumping newly generated machine code 
portability issues self compiler designed specifically retargetable new machine architecture programming changes designed implemented porting compiler related architecture relatively straightforward 
example compiler describes set registers target machine register stack related calling conventions table compile time constants easily changed architecture similar system registers 
parts compiler minimal number ifdef isolate machine dependencies 
control flow graph nodes intended map single target machine instruction allow low level optimization delay slot filling possible dependence kind instructions provided target architecture kinds control flow graph nodes 
approach works best risc style architectures self compiler primary intended target cisc machine styles sort peephole optimizer generalized instruction selector needed support kinds architectures effectively 
largest machine dependency compiler instruction formats dependency isolated machine subsystem compiler responsible producing machine specific instruction formats higher level control flow graph nodes add branch 
control flow graph nodes architectures sethi control flow graph node sparc phases delay slot filling executed architectures need 
difference machines operand instructions machines operand instructions handled register allocator constraining destination register sources operand machines 
expect self compiler ported new risc architecture generate reasonably code week parts self implementation memory system low level assembly code support take longer port 
cisc machines supported amount time peephole optimizer instruction selector run time performance may compilers machine 
just compilers self compiler benefit current table driven compiler compilers improve portability 
summary type analysis inlining splitting phase compiler executes additional phases way generating machine code compiler eliminates redundant loads constants 
currently interact register allocation variable assignment benefit wasted unnecessary register traffic 
compiler eliminates unneeded computations arithmetic memory loads 
compiler performs live variable analysis allocates registers variables inserts needed register moves 
compiler uses variables represent sets aliased names response heavy inlining self compiler 
register allocator extended avoid inserting register moves adjacent variables support finer grained position dependent allocation 
compiler fills delay slots branches calls 
extended schedule load delays 
compiler generates native machine instructions debugging information adds new compiled method compiled method cache 
porting self compiler new architecture overly difficult trivial 
chapter programming environment support self system designed interactive exploratory programming environment self implementation support rapid turn programming changes complete source level debugging 
features fairly easy support interpretive environment difficult achieve highperformance optimizing compiler environment particularly aggressive inlining 
researchers investigated problem enabling compilation optimization coexist gracefully programming environment hen cmr zj 
chapter describes techniques self implementation support programming environment focusing support provided compiler 
compiled method contains just instructions 
includes list offsets instructions embedded object garbage collector modify compiled code referenced object moved 
second compiled method includes descriptions inlined methods find values local slots method display source level call stacks 
third compiled method contains bidirectional mapping source level byte codes actual program counter values 
kinds debugger related information described section 
compiled method includes dependency links support selective invalidation methods programming changes described detail section 
header native machine code scavenging info 
scope descriptions pc byte code mappings caller scope slot locations compiled method pc address scope desc 
ptr 
byte code index scope description byte code mapping desc 
ptr 
parts compiled method dependency links support source level debugging programming environment include source level debugger 
self debugger presents program execution state terms programmer execution model state source code interpreter optimizations 
requires debugger able examine state compiled optimized self program construct view state virtual state terms byte coded execution model 
examining execution state complicated having activation records virtual call stack inlined activation records physical call stack allocating slots virtual methods registers stack locations compiled methods 
compiler generated debugging information allow debugger reconstruct virtual call stack physical call stack self compiler appends debugging information compiled method 
scope compiled initial method plus methods block methods inlined compiler outputs information describing scope place virtual call tree compiled method single physical stack frame 
argument local slot scope compiler outputs value slot constant known compile time slots register stack location allocated hold value slot run time 
subexpression compiled method compiler describes compile time constant value subexpression register stack location allocated subexpression 
information reconstruct stack evaluated expressions waiting consumed message sends 
example consider simple method compute minimum values min arg min sent integer compiler generate compiled version min source method customized integers 
customization subject chapter 
method integers looked compiletime locating method stack compiled methods return address stack stack grows source methods return address virtual source level view physical machine level view method contained call primitive inlined 
result message true object false object common case leading compiler split succeeding iftrue false message possible outcomes 
splitting subject chapter 
compiler looks definition iftrue false true iftrue false value 
false iftrue false value 
methods inlined nested value messages method 
generating code compiler method compiler outputs debugging information represent tree inlined methods scope description refers calling scope description black arrows diagram block scope lexically enclosing scope description gray arrows diagram 
slot scope debugging information identifies slot compile time value run time location 
min example initial arguments run time locations registers case slot contents known statically compile time 
expression stack debugging information omitted illustration 
additional debugging information fairly space consuming 
described section appendix scope descriptions take times space compiled machine instructions depending degree inlining performed compiling routine 
fortunately information paged associated routine debugged 
possible avoid storing debugging information regenerating debugging information demand re executing compiler 
virtual physical program counter translation self compiler outputs debugging information support translation source level virtual program counter virtual stack frame pair scope description byte code index scope description machine level physical program counter physical stack frame 
information translate hardware physical return address stack frame byte code index virtual stack frame physical frame displaying current virtual execution stack 
mapping locate physical program counter corresponding particular virtual program counter setting breakpoints particular source positions 
byte code mapping 
virtual program counter addresses map physical program counter address sequence source level messages get inlined optimized away completely messages sent execution user defined control structure generating machine code eliminated messages messages mapping physical machine address 
additionally physical program counter addresses may map virtual program counter address single source level message get split compiled place leading physical program counter addresses map source level message 
consequently compiler treats mapping simple relation generates long list word tuples self min self arg value block arg iftrue self true self arg false iftrue self false self arg false value block self tuple consisting physical program counter address physical view pair pointer virtual scope description byte code index scope virtual source level view 
reported section appendix byte code mapping fairly concise requiring space taken compiled instructions 
reason comparatively small size compiler generates tuples correspond call sites compiled code places system suspend method examine byte code mapping 
current debugging primitives current self implementation includes partial support interactive debugging 
system supports displaying virtual execution call stack complete current values local variables virtual activation records optimizations including inlining completely invisible 
system supports manipulating individual activation records directly self objects querying contents local slots examining expression stack navigating dynamic static call chains 
current implementation support modifying contents local variables activation records think adding facility difficult 
system supports breakpoints single stepping process control primitives 
programmer set breakpoint editing call user defined self code eventually invokes process suspend primitive 
suspended process single stepped invoking process control primitives 
interactions debugging optimization optimizing compilers support complete source level debugging optimizations perform prevent virtual source level state completely reconstructed 
example tail call optimizations prevent programmer examining elided stack frames dead variable elimination dead assignment elimination prevent programmer examining contents variable scope longer needed compiled code 
self compiler performs optimization prevent debugger completely reconstructing virtual source level execution state optimizations performed 
self compiler perform effective optimizations including inlining splitting common subexpression elimination optimizations undone debug time appropriate debugging information 
self compiler job balancing debugging support various optimizations eased requiring debugger support places debugging primitive invoked message sends restart loop tails 
compiler required support debugging arbitrary instruction boundaries required interrupt occur point program source level byte code boundaries required user single step optimized code single stepping implemented recompiling methods optimization stopping call site 
debugger invoked defined locations compiled code compiler perform optimizations potential interruption points difficult impossible perform instruction level byte code level debugging information required 
example compiler reuse register dead variable long subsequent call sites interruption points variable scope 
unfortunately current representation debugging information places restrictions compiler hurt performance 
mentioned section current self register allocator allocate particular name single location entire lifetime mark name bound particular compile time constant entire lifetime 
restriction allocation constant name entire lifetime primarily caused limited abilities debugging information describe allocation easy way different allocations different subranges name lifetime 
system able get added flexibility constraints current representation making copies virtual scope descriptions name different allocations different parts lifetime physical virtual program counter mapping select appropriate virtual scope description physical program counter 
approach support form allocation lead lot duplicated debugging information 
better approach redesign debugging information representation scratch efficiently support names position varying allocations 
urs hlzle implemented process control activation record manipulating primitives bay wei chang integrated primitives graphical user interface 
debugger run restart points points check interrupts described section userdefined interrupt handler call debugger 
support programming changes described section self compiler assumes certain hard change parts objects remain constant compiler performs optimizations assumptions 
example compiler assumes set slots particular object integer true object remain allows compiler perform message lookup compile time 
similarly compiler assumes contents data slot change offset assignable data slot change 
assumptions enable compiler inline bodies methods replace data slot access methods load store instructions generating faster code 
assumptions correct object mutations available programs programmers normal assignments assignable data slots compiler explicitly avoids depending contents assignable slot 
additional operations available programming environment mutate objects ways adding removing slots changing contents non assignable data slots 
modifications may invalidate assumptions compiler compiling optimizing methods 
executed compiled code lead incorrect behavior system crashes 
ways supporting programming changes traditional batch compiling systems support programming changes requiring programmer recompile manually files date relink program restart application 
best process automated utilities determine files need recompiled set programming changes 
turn time single programming change quite long tens seconds typically minutes tens minutes 
programmer productivity suffers greatly turn times length simple programming changes 
interactive systems designed support rapid programming turn times order seconds 
usually achieve level interactive performance limiting dependencies components system individual components replaced simply easily possible requiring complex timeconsuming system relinking recompilation components directly altered programming change 
execution performance tends lower traditional optimizing environment intercomponent information optimizations 
self compiler clearly violates basic assumptions style system self compiler performing optimizations inlining create intercomponent dependencies 
self run time compilation architecture offers possible solution dilemma 
programming change invalidated assumptions compile method system simply flush compiled code compiled code cache 
new code compiled needed possibly changed source methods new correct assumptions relatively unchanging parts object structure 
compiled code cache flushing fast solve programming turn time problem 
unfortunately approach simply shifts cost programming change flushing operation immediately flushing 
flush methods compiled nearly message send require new compiled code generated leading long sequence compiler pauses immediately flush 
compiler pauses spread somewhat chunk program execution self compiler architecture allow code generated relinked faster traditional file environment turnaround time usually longer second supported interactive system 
avoid lengthy recompilation pauses self system maintains inter component dependency information selectively invalidate compiled methods affected programming change 
set small number pauses recompile invalidated methods kept small perceived turn time kept short 
situations selective invalidation enables self system support fast turn programming changes fast run time execution changes 
selective invalidation cure 
extremely common method inlined places changed definition integers selective invalidation approach reduces expensive total flush approach producing lengthy compilation pauses system gets recompiled new definition 
fortunately problem practice common methods system changed rarely systems sweeping changes 
current trade run time performance programming turn time favors run time performance turn time kinds short commonly system methods 
dependency links support selective invalidation compiler maintains way change dependency links compiled method cache information compiler assumed remain constant 
information compile code set slots objects offsets assignable data slots contents non assignable slots precisely information stored maps 
maps described section appendix 
system needs maintain dependency links maps compiled methods 
compiled methods may depend particular map compiled method may depend maps dependency links support mapping maps compiled methods 
dependency links created result message lookup record aspects objects traversed lookup modified potentially change result lookup consequently correctness compiled code 
clearly compiled code depends result message lookup system leaves dependency link matching slot lookup method compiled 
matching slot changed changing contents constant slot method parent slot changing offset object contents assignable data slot removed altogether linked compiled methods flushed compiled code cache 
lookup system scans parts objects affect outcome lookup need dependencies 
message lookup system fetches contents parent slot searching object parents 
parent slot changed removed outcome message send change 
record fact system creates dependency link parent slot compiled code method eventually result lookup 
parent slot modified removed linked compiled methods flushed appropriately 
subtle kind link handles problem slot may added object affects outcome message send 
message lookup system frequently searches object matching slot unsuccessful object parents searched turn matching slot 
matching slot parent slot inherits matching slot added object results earlier message changed possibly invalidating compiled code 
handle problem compiler creates special add dependency link compiled code maps objects unsuccessfully searched particular slot dependency associated slot map map 
slot added map compiled methods linked add dependency flushed added slot affect lookup results 
slot specific dependency links add dependency links imprecise 
record exactly message names unsuccessfully scanned previously methods may flushed need flushed 
significantly reduce flushing possibly leading long compile pauses programming change slots added 
hand recording exactly message names searched unsuccessfully map consume lot space maps similar long lists unsuccessful matches 
currently exploring alternative mechanisms support selective invalidation slot additions 
diagram illustrates dependency links created compiling min method described earlier chapter 
gray line represents separate dependency links link connecting slot map map case add dependency links compiled code min 
invalidation programming change compiler traverses dependency links invalidate compiled methods linked updated information 
invalidation normally quite straightforward simply requiring invalid compiled method thrown compiled code cache 
compiled method currently running stack frame suspended compiled method invalidation complicated 
compiled methods just flushed executing returned 
remain untouched optimized information longer correct 
approach taken self system recompile date compiled method rebuild stack frame data stored old stack frame 
self system performs conversion lazily recompilation easier intrusive hcu 
compiled method suspended stack invalidated part execution programming primitive system marks method invalid removes lookup cache message sends bound invalid compiled method system flush compiled method compiled code cache 
system adjusts return address stack frame returned invalid data stack frame valid 
compiled code invalidated method suspect 
root integer root map integer traits map true true map integer map min parent parent add dependency add dependency parent add dependency iftrue false add dependency false false map parent iftrue false add dependency traits compiled code integer min dependency lists case min changed case changed case iftrue false changed case min parent added case parent changed case parent changed case min parent added case iftrue false changed method stack frame return special support routine run time system 
system returns control programming primitive eventually return self process invoked 
eventually stack frame invalid compiled method return calling special run time support routine 
routine invalid compiled method builds new stack frames represent state invalidated compiled method stack frame lazy recompilation bottom stack self activation records 
easy fill state new stack frame keep recompilation pauses short new method compiled optimization 
invalidated compiled method probably compiled optimization including inlining system may need compile unoptimized methods represent state invalidated compiled method unoptimized method physical stack frame virtual stack frame inlined single physical stack frame invalidated compiled method point call 
complete conversion process recompiling routine returns appropriate point new compiled method topmost stack frame 
invalidated compiled method flushed compiled code cache old invalid stack frame activation method 
lazy conversion spreads load recompilation longer period time reducing perceived pauses programming change 
programming changes occur returning invalid method may performed method recompiled programming change 
lazy urs hlzle implemented mechanisms lazily recompile invalid methods stack 
stack stack grows compiled methods return address invalid 
recompile stack compiled methods old invalid unoptimized stack frame stack grows conversion simplifies speeds conversion process limiting recompilation stack frame creation top stack 
eliminates need copy stacks adjust interior addresses recompiling rebuilding stack frame buried middle stack 
summary self compiler designed coexist interactive exploratory programming environment 
kind environment requires complete source level debugging available times time caused programming changes limited seconds 
self compiler supports complete source level debugging face optimizations inlining splitting generating additional information allows debugger view single physical stack frame source level virtual stack frames 
fast turn time programming changes supported selective invalidation mechanism dependency links flushes date compiled methods compiled code cache 
invalidation performed lazily compiled methods currently executing stack 
chapter performance evaluation illustrated new techniques described dissertation small examples previous chapters context appeared quite effective 
chapter explore effectiveness implementation techniques actual self programs measuring execution speed compilation time compiled code space consumption suite self programs ranging size lines lines 
evaluate perspectives effectively combination new techniques narrow gap performance pure object oriented languages self traditional languages optimized optimized lisp 
new techniques effective 
costly 
promising areas 
answers questions sections respectively 
section results description measurement methodology 
methodology benchmarks analyzed performance self implementation selection benchmark programs 
benchmarks include micro benchmarks gathered various sources stanford integer benchmark suite hen richards benchmark deu self programs originally written benchmarking mind 
source code benchmarks available author request 
eleven micro benchmarks short typically stress aspects implementation speed integer calculations speed procedure calls 
stanford integer benchmark suite composed perm towers queens quick bubble tree puzzle benchmarks originally collected john hennessy peter help design measure risc microprocessors compilers 
stanford integer benchmarks larger micro benchmarks exercise integer calculations generic arithmetic userdefined control structures particularly style loops tree stress array accessing 
self smalltalk stanford integer benchmarks puzzle written versions similar version possible advantage message passing features self smalltalk associating core benchmark code data structures manipulated code 
versions benchmark perform algorithm source level optimizations version object oriented version sends messages self fewer messages objects procedure oriented version 
results versions benchmarks object oriented style identified oo prefix benchmark names 
section appendix describes individual micro benchmarks stanford integer benchmarks detail 
micro benchmarks stanford integer benchmarks particularly object oriented 
expected translated self traditional non object oriented languages lack implies may drawn measurements benchmarks limited effectiveness self implementation running smaller traditional programs 
performance self object oriented programs performance benchmarks tentative speculative 
object oriented programs contain traditional portions scan array perform simple arithmetic calculations small non object oriented sections fact may comprise running time application 
improvement performance non objectoriented code improve system performance 
richards benchmark line operating system simulation benchmark originally written martin richards test bcpl compilers 
benchmark maintains queue tasks spends time primarily removing task queue processing usually appending different queue processing benchmark begins initializing queue tasks ends main task queue empty 
richards benchmark different previous ones large overly stress particular operations tight loops recursions 
richards benchmark manipulates data structures representative typical object oriented programs previous benchmarks 
richards benchmark unusual including message send invokes different methods different calls message sent task removed head queue defined differently different kinds tasks 
measure effectiveness new techniques truly object oriented programs measured self programs originally written useful right benchmarks 
pathcache primmaker programs active today 
pathcache computes mapping objects names inferred object structure part self user interface 
pathcache lines long excluding code supporting data structures dictionaries self key value mapping data structure 
primmaker generates self wrapper functions user defined primitives textual description primitives 
benchmark run test file exercises different possible descriptions ends dominated compile time 
primmaker lines long 
parser parses earlier version self 
benchmark run relatively short expressions 
parser lines long 
benchmarks features self prototype design dynamic inheritance available languages versions benchmarks exist self 
benchmarks compare performance self languages systems measure effectiveness various techniques developed self comparing configuration self system 
table summarizes benchmarks measured section appendix describes measurement procedures detail 
appendix contains raw data measurements 
hardware measurements sun workstation configured mb main memory 
sun workstation sparc risc style microprocessor hardware register windows delayed branches calls hp 
implementation sparc sun ns cycle time register windows cycle loads target register instruction cycles cycle stores assuming cache hits 
sparc provides limited hardware support tagged arithmetic systems measured including self system currently exploit hardware 
charting technique charts bigger bars better sense correspond efficient implementation 
execution compilation performance reported terms speed taller bars corresponding faster systems 
compiled code space costs reported terms density inverse space usage taller bars corresponding systems systems requiring space compiled code 
compiled code space numbers include space machine instructions exclude space costs debugging information 
section appendix reports measurements additional space costs self system 
small eleven micro benchmarks lines long stanford stanford integer benchmarks written traditional procedure oriented style lines long oo stanford stanford integer benchmarks written object oriented style different stanford self smalltalk puzzle largest stanford integer benchmark written traditional procedure oriented style lines long richards medium sized operating system simulation benchmark lines long pathcache short self program frequent today lines long primmaker larger self program occasional today lines long parser medium sized self program originally written programming exercise lines long performance versus languages determine effectiveness new techniques described dissertation compared performance self implementation implementations languages 
want compare self implementation traditional optimized language implementation 
comparison tell techniques narrowing gap performance previously existed pure object oriented languages traditional optimized languages 
performance traditional optimized language places upper bound performance reasonably expect self implementation 
measured optimizing compiler provided standard sunos unix invoked flag 
version richards benchmark really written version cfront translator message implemented virtual function call 
want compare self implementation performance best existing implementation pure object oriented language compare effectiveness techniques previous techniques implementing pure object oriented languages 
measured performance parcplace smalltalk version fastest commercial implementation smalltalk 
parcplace smalltalk implementation includes deutsch schiffman techniques constructing fast smalltalk implementation described section 
unfortunately execution speed results available smalltalk compilation speed compiled code space usage measured 
interested relative performance self system sort lisp system 
lisps provide features self support generic arithmetic closures dynamic type checking 
self lisps provide direct procedure calls direct variable accesses built control structures 
comparing self lisp system help determine self handles generic arithmetic compared existing techniques lisp systems self optimizes away extra overhead message passing completely user defined control structures 
measured orbit compiler version dialect scheme described section 
system widely regarded highquality lisp implementation supposedly competes traditional language implementations 
lisps include special low level primitive operations avoid overhead corresponding general safe primitive operations 
example includes fx operation assumes arguments lisp jargon fixed precision integers fit machine word equivalent objects smalltalk int 
fx efficient generic operation fx assumes arguments compiled single machine add instruction operator fx verify assumption arguments fact adds check overflows addition consequently fx unsafe 
distinction slow safe operations fast unsafe operations forces programmers choose explicitly normal generic arithmetic willing pay cost better semantics faster fx style arithmetic willing take responsibility assumptions 
lisps typically include directives allow programmer invoke additional optimizations inlining 
includes define integrable form works just define binding name value function compiler inline bound value function name evaluated 
inlining optimization invoked explicitly programmers 
expect programmers normally define 
programmers may feel need unsafe operations explicit directives get better performance 
fact nearly benchmark performance results lisps reported hand optimizations replaced fx possible define integrable sprinkled desired 
capture styles usage wrote versions benchmark expect normal programmer written benchmark measuring expected performance average programmer hand optimized version benchmark fx define integrable measure measuring best possible performance seen expert 
versions richards benchmark structure facility implement task objects declare operation different methods invoked different types task objects 
table summarizes language implementations measured charts page compare execution speed compilation speed compiled code space efficiency systems benchmarks compilation speed compiled code space efficiency results unavailable smalltalk 
results reported percentage results optimized self benchmark suites programs run speed optimized level performance times better performance parcplace smalltalk despite fact self harder compile efficiently smalltalk primarily self accesses variables messages smalltalk accesses directly 
surprisingly self runs times faster compiled orbit compiler running normal programs benchmarks direct procedure calls excluding operation invocation site richards built control structures 
self outperforms cases comparing specific hand optimized programs 
comparisons provide evidence self compiler doing excellent job eliminating run time overhead associated message passing user defined control structures generic arithmetic 
compilation speed self implementation quite reasonable compared optimizing language implementations 
self compilation speed better optimized cases puzzle self compiles richards times faster 
self compiles faster orbit compiler normal benchmarks orbit usually compiles integer specific version benchmarks faster self compiler 
measure compilation speed parcplace smalltalk believe significantly faster implementations principally smalltalk compiler virtually optimization compilation 
unfortunately performance self compiler slow go unnoticed self programmer original goals project 
speeding compiler sacrificing execution performance continues active area research 
self space efficient optimized self uses times space micro benchmarks times space puzzle benchmark 
stanford integer benchmarks self incurs space overhead compared richards largest data structure oriented benchmarks self consumes twice space optimized 
relatively low space overheads majority benchmarks remarkable considering self compiler techniques splitting trade away compiled code space get faster execution times 
falling costs memory self system extra space requirements quite reasonable 
self uses compiled code space normal programs compiled orbit compiler space integer specific programs 
results confirm practicality compilation techniques terms space costs 
sunos standard optimizing compiler smalltalk parcplace systems smalltalk fastest commercial smalltalk implementation normal orbit compiler version normal coding style int orbit compiler version integer specific hand optimized coding style self self implementation small stanford oo stanford puzzle richards run speed opt 
small stanford oo stanford puzzle richards compile speed opt 
small stanford oo stanford puzzle richards code density opt 
smalltalk normal int self language implementations typically trade execution speed compilation speed compiler usually harder produce code runs faster 
practicality particular language depends large part implementation balances competing goals 
chart summarizes performance language implementations scatter plotting execution compilation speed results system averaging benchmarks 
assigned smalltalk implementation compilation speed times faster optimizing compiler 
guess intended roughly illustrate smalltalk implementation position chart execution speed accurate 
self performs better execution speed version orbit system compiles faster orbit normal version benchmarks 
self runs benchmarks average speed optimized faster compilation speed optimizing compiler 
paste chart page ps relative effectiveness techniques understand effect new techniques explore detail relative effectiveness accompanying costs new techniques developed part self compiler 
results help identify techniques worth including implementation language self 
calculate benefits costs technique comparing performance standard self configuration performance configuration technique disabled 
charts effect optimization shown reporting performance normal self system including optimization relative performance self system optimization disabled approach remains consistent general visual theme bigger bars indicating better results case bigger bars indicating important effective optimization 
charts page summarize effect performance various optimizations implemented self compiler 
detailed information optimization may sections appendix optimizations clear winners inlining deferred block creations type prediction customization 
type analysis presumably winner impact directly measured 
value type analysis exposed block analysis lazy compilation local reluctant splitting delay slot filling significant contributions lazy compilation particular huge improvement compilation speed compiled code space utilization 
course register allocation important impact particular details self register allocator allocating variables names measured 
common subexpression elimination eliminating unneeded computations range analysis line caching caching compile time lookups appear particularly important 
range analysis improves compilation speed compiled code space density slim execution speed improvement 
described section line caching appear effective larger object oriented benchmarks performance bug run time system halves performance pathcache benchmark 
common subexpression elimination provides significant improvements benchmarks oo bubble see appendix performance improves 
nearly cases optimizations bigger difference smaller numeric benchmarks larger object oriented benchmarks 
part disparity probably stems concentration smaller benchmarks self compiler designed implemented 
concentrated objectoriented benchmarks trend reverse 
styles program important view difference effectiveness opportunity 
inlining def block creation type prediction customization type analysis exposed block analysis lazy compilation local 
splitting delay slot filling cse elim 
unneeded comp range analysis line caching compiletime lookup cache run speed optimization compile speed optimization space density optimization numeric object oriented remaining sources overhead self system run fast traditional language implementation optimized know sources remaining gap performance guide 
section examine sources overhead traditionally slowed pure object oriented languages extra run time type tests overflow checks array bounds checks 
results see new techniques applied self reduce traditional sources overhead direct research reducing cost overhead remains significant 
measure cost particular source overhead constructing version system source overhead comparing speed version standard version system 
frequently new version system legal self implementation self programs run correctly benchmarks measure altered system runs correctly 
unfortunately possible source overhead measured way 
key sources overhead difficult produce configurations simulate absence 
example overhead message passing inheritance dynamic typing user defined control structures nearly impossible remove measure directly system internals benchmark source code relies features 
overhead introduced result deficiencies implementation self compiler measured way 
charts page summarize cost sources overhead able measure displaying performance configuration source overhead removed relative performance standard self system bigger bars mean overhead costly self current implementation technology 
report performance additional configurations feasible excludes sources overhead avoided implementing parts system differently block zapping interrupt checking polling lru compiled method reclamation support 
excludes language implementation overhead measured try produce system fast possible 
report execution speed optimized comparison 
detailed analysis sources overhead may section appendix surprisingly single traditional source overhead measure imposes significant execution speed cost 
array bounds checking type testing overflow checking interrupt checking lru compiled method reclamation support measured sources overhead non trivial execution time cost incurs cost 
low cost reinforces earlier performance measurements illustrating techniques implemented self compiler reducing eliminating sources overhead historically plagued implementations pure object oriented languages 
fairly sizable gap performance fastest version self optimized remains unaccounted 
gap stems solely poorer implementation self compiler traditional optimizations global register allocation loop invariant code motion improvements speed pure object oriented languages await additional insight 
compilation time costs compiled code space costs sources overhead measure fairly small 
polling style interrupt checking block zapping compile time compiled code space consuming features support 
interestingly worst offenders related particular system architecture self implementation required language environment features hope overhead reduced redesigning various parts implementation 
integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping primitive failure checking names interrupt checking calls interrupt checking restarts lru compiled method support feasible optimized run speed normal self compile speed normal self space density normal self numeric object oriented summary large new techniques developed efficiently implementing self self runs half fast optimized benchmarks measured fold improvement best previous implementation similar pure object oriented language parcplace smalltalk 
self performance double speed respected implementation similarly dynamically typed language supporting generic arithmetic direct procedure calls built control structures orbit compiler 
compile time compiled code space costs comparable language implementations exception parcplace smalltalk believe compiles faster language implementations 
bulk self compiler performance attributed optimizations 
certain techniques simply applied hope decent performance including inlining deferred block creations 
type prediction customization improve execution performance factor 
value type analysis exposed block analysis splitting lazy compilation uncommon branches delay slot filling significant contributions run time performance 
optimizations modest benefits common subexpression elimination integer subrange analysis particular fairly complex implement expected greater pay offs 
important techniques measured effectiveness type analysis space costs customization techniques integral system disable 
measurements remaining cost traditional sources overhead implementations pure objectoriented languages indicate traditional bottlenecks continues incur significant cost 
result confirms effectiveness new techniques unfortunately provide help guiding research profitable avenues 
chapter results demonstrate pure object oriented languages efficient stock hardware suitable implementation techniques 
achieve high efficiency design implement new implementation techniques including customization type prediction iterative type analysis splitting 
customization type prediction extract representation level type information untyped source programs type analysis splitting preserve valuable information long possible 
accumulated type information statically bind inline away messages especially involved user defined control structures generic arithmetic leading dramatic performance improvements 
lazily compiling uncommon cases arithmetic overflows primitive failures compiler concentrate efforts common case parts program supporting uncommon events occur 
strategy resolves tension fast execution powerful language features providing best worlds execution compilation speed common cases support powerful common cases 
techniques current self implementation runs small medium sized benchmarks half speed optimized compile times comparable optimizing compiler compiled code space usage double optimizing compiler 
new execution speed times faster fastest previous implementation similar language parcplace smalltalk 
general themes underlie 
techniques frequently trade away space speed compiling multiple specialized versions single piece source code customization splitting exemplify approach 
minimize compile time compiled code space costs approach techniques applied lazily 
methods compiled specialized lazily invoked uncommon parts control flow graph compiled lazily taken 
lazy compilation appears saving grace specialization practical 
applicability techniques techniques developed self optimize programs heavy message passing 
techniques apply languages share properties 
clearly pure dynamically typed object oriented languages smalltalk benefit directly techniques 
discussed section techniques apply relatively pure statically typed object oriented languages trellis owl eiffel 
hybrid languages object oriented lisps need techniques performance critical parts programs written lower level non object oriented subset language 
techniques useful extent implementations wish support encourage object oriented features languages 
researcher proposed extending support form customization lea 
techniques improve performance languages claim object oriented 
languages include powerful features different representations objects interchangeably programs 
ability essentially message passing set possible representations usually user extensible argue languages contain object oriented subsets 
techniques useful improving performance languages extent object oriented features programs 
example non object oriented languages supporting generic arithmetic lisps icon significantly benefit inclusion type analysis type prediction splitting lazy compilation uncommon cases extract preserve representation level information optimization 
illustrate self implementation generates code benchmarks generic arithmetic runs twice fast code generated orbit compiler dialect scheme benchmarks message passing user defined control structures 
version benchmarks rewritten unsafe arithmetic giving semantics generic arithmetic self runs faster 
result argue language designers implementors users abandon unsafe integer specific arithmetic favor safe expressive generic arithmetic combined optimization techniques 
language features generic arithmetic benefit techniques 
apl allows programs manipulate scalars arrays matrices arbitrary dimension interchangeably techniques lazily compile dimension specific code speed apl programs 
implementations logic programming languages prolog benefit knowing certain branches logic variable guaranteed instantiated knowledge come techniques related type analysis splitting 
similarly implementations programming languages supporting futures multilisp hal mul distinguish known unknown futures compiling specialized code case just common case known futures 
techniques may broadly applicable variety modern programming languages pure object oriented languages 
significant progress moving self pure object oriented programming languages realm practicality remains complete task 
applications require maximum efficiency scientific numerical applications traditionally written fortran 
self currently implemented probably efficient demanding users 
avenue research push upper limits performance achieved traditional languages extend current implementation techniques handle floating point representations efficiently integer representations currently handled self implementation 
related direction attempt validate techniques scale larger systems measured far 
techniques rely trading away compiled code space run time speed 
systems measured line program range potential space explosion problem practice larger programs order lines concern remains 
research done ensure techniques robust face large systems 
third direction focus improving performance object oriented programs 
benchmarks measured far significant extra power self language available traditional languages 
question remains techniques fare programs heavy advanced features language 
ideally object oriented programs written faster easier change extend equivalent non object oriented programs run just fast oriented versions 
goal met current self implementation richards benchmark runs third speed optimized version 
initial begun direction hcu 
final direction address programming environment issues 
current self compiler compiles fast optimizing compiler small medium sized benchmarks compiles twice fast optimizing compiler fact compilation takes place run time self holds system higher standard 
users tend distracted pauses fraction second garbage collection run time compilation productivity drops correspondingly 
pauses dozen seconds bring severe distraction decline productivity 
current self implementation meet second level performance unfortunately level fraction compile pauses 
maintain high productivity environment research needed reconcile unnoticeable compiler pauses run time performance 
fortunately problem actively pursued hcu early results quite promising 
believe demonstrated feasibility new techniques consequently practicality pure object oriented languages wide range applications 
hope demonstration convince language designers avoid compromises designs motivated solely concerns efficiency pure message passing model 
hope language users demand simple flexible languages 
appendix object formats appendix details representation objects self memory system 
section presents overview self memory system 
tag formats self memory space organized linear array aligned bit words 
word contains low order bit tag field interpret remaining bits information 
integer floating point number encodes number directly 
self objects map objects embed address object object table 
remaining tag format mark header word object required scanning scheme discussed section 
pointers virtual machine functions objects self heap represented raw machine addresses addresses bit half word aligned scavenger interpret immediates integers floats try relocate 
tag formats chosen speed common operations 
tagged integers may added subtracted compared directly integers weren tagged tag field zero integers 
overflow checking free additional overhead tag bits low order overflows tagged format detected hardware just overflows normal untagged integers 
integer multiplies divides require extra shift instruction prior invoking corresponding untagged operation 
conversions tagged untagged integers require arithmetic shift instruction 
nice benefit integer tag format object array accesses may tagged index directly access elements array extra multiplies shifts required convert index offset required untagged integers traditional language 
tag format heap objects relatively free overhead 
self objects word boundaries byte addressed architectures bit low order tag format reduce available address space tag simply replaced tag turn tagged raw word aligned address 
additionally machines register offset addressing mode tag stripped automatically accessing field referenced object constant offset folding decrement offset instruction 
example access th word object origin compile time constant compiler simply generate sparc assembly syntax ld object bytes word offset load instruction compile time constant 
bit signed integer integer immediate virtual machine address top bits word aligned address self heap object bits ieee floating point number floating point immediate virtual machine address scavenging fields hash field mark header word begins self heap object testing integer immediate requires simple object bz integer sequence check low order bits tag 
testing heap object requires object heap sequence test low order bit non zero sequence assumes mark word case object manipulated user programs 
testing floating point immediate similarly requires object float sequence test second lowest order bit assuming mark word 
object layout heap object begins header words 
word mark word identifying object 
mark word contains scavenger self primitive 
second word object tagged object map 
self object assignable slots contains additional words represent contents 
array objects include length tagged self integer prevent interactions scavenging scanning assignable slot contents 
object arrays include elements tagged object assignable slot contents assignable slot contents constant offsets clone family byte arrays include untagged pointer word aligned sequence bit bytes assignable slot contents segregation packed bytes parts described section 
bytes bytes bytes plain object object array byte array map mark map mark map mark array length array length element element element bytes pointer segregated slot contents slot contents slot contents slot contents slot contents slot contents slot contents slot contents slot contents bytes part objects map objects mark map words 
map objects share map called map map map map map 
maps new space linked third words scavenge system traverses list perform special finalization inaccessible maps 
fourth word map contains virtual machine address array function pointers functions perform type dependent operations objects maps 
maps objects slots fifth word specifies size object words 
sixth word indicates number slots object number slot descriptions map 
words contain change dependency link map described section 
words tagged integers 
map method ninth word byte code object representing method source code byte code objects described section 
map includes word description object slots 
word points self string object representing name slot 
word describes type slot constant data slot assignable data slot assignment slot slot parent slot 
third word slot description contains contents slot slot constant slot offset object contents slot slot assignable data slot index corresponding data slot description slot assignment slot 
words slot contain change dependency link slot described section 
representations pair cartesian points map displayed page 
function pointer array virtual function array generated compiler 
self parents prioritized priority parent slot stored second word slot description 
map map slot description method data object map map mark function array link slot count object length link dependency map map mark function array link slot count object length link dependency byte code slot description slot description slot description constant slot data slot assignment slot description description description slot type slot name link dependency slot offset slot type slot name link dependency slot contents slot type slot name link dependency data slot index parent 
assignment slot assignment slot representation cartesian point objects 
objects left point instances containing values assignable data slots 
object right shared map cartesian points containing value constant parent slot offsets assignable slots 
slot name slot type mark map scavenging link function array link map dependency link slot dependency const 
parent slot slot contents object length slot count slot name slot type link slot dependency slot offset slot name slot type link slot dependency slot offset slot name slot type link slot dependency data slot index slot name slot type link slot dependency data slot index data slot data slot 
map map cart 
point traits mark map slot contents slot contents 
mark map slot contents slot contents 
appendix detailed performance evaluation sections appendix contain additional information benchmarks measurement procedures 
appendix includes detailed analyses effectiveness various techniques included self implementation sections remaining sources overhead slow self compared optimized section 
section discusses space costs extra information generated self compiler just native machine instructions 
detailed description benchmarks measured micro benchmarks recur tiny recursive benchmark stresses method call integer comparison subtraction adapted smalltalk micro benchmark kra 
sumto adds numbers receiver argument times 
sumto body sumto inlined outer loop manually smalltalk versions automatically self version 
similar sumto adds numbers argument second initializing accumulator receiver 
benchmarks stress generic arithmetic applied integers user defined control structures 
benchmark measure ts typed smalltalk compiler described section 
increments counter inside doubly nested style loop loop iterating times test run times 
test stresses generic arithmetic applied integers user defined control structures benchmark measure ts typed smalltalk compiler 
stores integer elements element long vector stresses iterating storing arrays 
originally suggested peter deutsch interesting microbenchmark deu 
adds elements element long vector 
test stresses iterating arrays generic arithmetic integers benchmark ts typed smalltalk compiler 
increments elements element long vector stressing iterating arrays storing arrays generic arithmetic integers 
benchmark ts typed smalltalk benchmark 
sieve finds primes eratosthenes sieve algorithm stresses integer calculations integer comparisons accessing arrays booleans 
tak executes recursive tak benchmark gabriel lisp benchmarks gab stresses method calling integer arithmetic 
performs algorithm uses lists cons cells represent integers additionally stresses list traversals memory allocation 
micro benchmarks lines long 
sieve lines long tak lines long lines long 
measured stanford integer benchmarks exercises integer calculations generic arithmetic array accessing user defined control structures particularly style loops perm oo perm recursive permutation programs lines long 
towers oo towers recursively solve towers hanoi problem disks lines long 
queens oo queens solve queens placement problem times lines long 
oo multiply matrices random integers 
dimensional matrices supported directly self smalltalk languages matrix represented array arrays contrast contiguous representation version 
benchmarks lines long 
quick oo quick sort array random integers quicksort algorithm lines long 
bubble oo bubble sort array random integers bubblesort algorithm lines long 
tree oo tree sort random integers insertion sorted binary tree data structure lines long 
benchmarks stress memory allocation data structure manipulation array accessing 
puzzle solves time consuming placement problem lines long 
benchmark unusual source code size dominated code initialize puzzle data structures demands compiler speed benchmarks 
source code benchmarks available author request 
measurement procedures measured speed self implementations milliseconds cpu time user plus system time reported unix system call self time function smalltalk implementation reports elapsed real time measurements performance smalltalk programs milliseconds real time cpu time 
smalltalk real time numbers fluctuate significantly run run indicating extra overhead included real time smalltalk relatively constant real time measurements self corresponding cpu time measurements indicating extra overhead included real time self small believe cpu time smalltalk close real time measurements 
feel comparisons smalltalk real time measurements language cpu time measurements reasonably valid 
measured run time benchmark executing benchmark loop iterations reading elapsed time loop dividing resulting difference 
measurement iterations helps increase effective resolution hardware clock nearest ms sun smooth variations run run 
compile times version benchmark calculated unix time command compilation optimization source file containing benchmark measured 
compilation time includes time reading writing files time linking output object file executable program 
compile times orbit compiler computed similarly time function invocation orbit function name file containing benchmark 
compile time includes time read write files time load resulting output file running system 
separating compile time run time self smalltalk complicated dynamic compilation 
execution piece code includes execution time compilation time 
technique calculating compilation execution time self benchmark flush compiled code cache sure code benchmark compiled cache 
benchmark run run includes compilation time run time 
benchmark run times runs include execution compiled code compiled code cache run 
final execution time calculated time runs divided just compilation time calculated time run minus average execution time runs 
approach assumes execution time run benchmark equal average subsequent execution may completely accurate hardware caching paging effects probably close calculated compile times rough measure actual compile time overhead implementation 
unfortunately smalltalk system provide mechanism flush compiled code cache compilation speed numbers unreliable 
consequently process measuring self performed smalltalk ignoring cache flush step average execution time numbers retained 
compiled code space figures available smalltalk 
frequently reduce number data points displayed charts results individual benchmarks combined form result suite similar benchmarks 
average result benchmark suite calculated individual benchmark result normalizing result typically performance optimized performance standard self configuration geometric mean normalized results benchmarks suite 
appendix contains original raw data measurements 
relative effectiveness techniques section explores effectiveness individual optimizations detail exception splitting strategies subject section 
techniques covered order described dissertation 
summary information may section 
inlining self compiler relies aggressive inlining achieve performance described chapter 
verify self system slow inlining measured version self inlining messages calls primitives disabled 
results displayed charts 
inlining huge difference performance 
smaller numeric benchmarks run orders magnitude faster inlining larger non numeric benchmarks run times faster inlining 
results support contention aggressive inlining key achieving performance pure objectoriented languages 
surprisingly inlining frequently speeds compilation saves compiled code space 
counterintuitive result illustrates difference inlining traditional language inlining self system 
self compiler uses inlining optimizing user defined control structures variable accesses resulting inlined control flow graph usually smaller original un inlined graph 
sorts inlined constructs inlined traditional language environment 
inlining larger user level methods procedures usually increase compile time compiled code space observed traditional environments self compiler simply spends time inlining things shrink control flow graph things expand 
course system designed perform inlining place compile faster compiler perform inlining results probably benefits inlining compilation speed 
inlining run speed optimization inlining compile speed optimization inlining code density optimization small stanford oo stanford puzzle richards parser pathcache caching compile time message lookups self compiler includes message lookup cache speed compile time message lookups described section 
chart reports effectiveness cache speeding compilation 
compile time message lookup cache speeds compilation percent 
poor showing result able cache single compilation calls compiler 
investigating system designs support longer lived compile time message lookup cache believe save significant amount compile time 
customization self compiler uses customization provide extra type information self enables inlining described chapter 
implementation self system assumes compiled methods customized difficult completely disable customization order measure impact performance system 
fortunately aspect customization disabled relatively easily contribution type self 
method prologue disabled compiler forgets type self rebinding unknown type 
run time performance configuration closely approximate run time performance version system customization dominating effect customization extra type information duplication methods line caches 
compilation speed compiled code space efficiency disabled configuration probably worse version system customize disabled configuration compile multiple versions source methods report accurate compile time costs compiled code space costs customization configuration 
compile time lookup cache compile speed optimization small stanford oo stanford puzzle richards primmaker pathcache chart presents customization impact run time execution performance 
customization improves performance significantly enabling self benchmarks run times faster 
extra type information provided customization put compiler speeding runtime performance 
line caching line caching speeds message sends inlined statically bound described section 
self compiler effective inlining messages question remain line caching important performance 
chart reports impact line caching execution performance self system line caching affects compilation speed compiled code density 
line caching little impact smaller benchmarks sends inlined benchmarks remain frequently statically bound single target method require line caching 
primmaker parser benchmarks benefit line caching 
result customization run speed optimization small stanford oo stanford puzzle richards parser primmaker line caching run speed optimization small stanford oo stanford puzzle richards primmaker pathcache surprising line caching importance deutsch schiffman smalltalk system serves confirmation effectiveness inlining static binding 
amazingly pathcache benchmark runs twice slowly line caching 
unexpected result caused performance bug run time system compiler slows messages different receiver types 
measurements problem discovered implementation line caches receiver types improved line cached sends reportedly faster uncached sends hl 
type analysis self compiler relies type analysis propagate type information control flow graph inline away messages avoid unnecessary type tests 
type analysis subject chapter chapter 
customization type analysis difficult disable self compiler type analysis compiler entire design 
directly measure effectiveness type analysis version self performs type analysis 
value type analysis fortunately measure effect parts type analysis self compiler value objects link names types improve effectiveness optimizations type prediction described section 
approximate version self values creating new dummy value object name assigned 
consequently names share value 
execution speed compiled code space efficiency configuration close version self values compilation speed compiler written values run faster written values rendered useless 
charts report impact value type analysis execution speed compiled code space density 
value type analysis run speed optimization value type analysis code density optimization small stanford oo stanford puzzle richards parser primmaker pathcache results show value type analysis significant improvement execution performance 
smaller benchmark suites run twice fast value type analysis 
values improves smaller numerical benchmarks larger benchmarks reasons 
numerical benchmarks rely type prediction performance corroborated results section values improve effectiveness type prediction 
second smaller benchmarks send messages single expression example accumulator receives message time loop values allow type information propagated name exploited optimizing sequences messages 
value type analysis improves density compiled code eliminating need repeated redundant type tests take additional compiled code space 
integer subrange analysis integer subrange types support optimizations depend range analysis eliminating unnecessary overflow checks array bounds checks described section 
effect integer subrange types determined easily general integer map type integer subrange type introduced 
charts report effectiveness integer subrange analysis 
integer subrange analysis little effect execution performance improving speed benchmarks 
range analysis impact greater limiting factors 
current register allocator ensure adjacent variables get allocated location described section frequently register move instruction remains integer subrange analysis eliminates overflow check 
second range analysis currently limited requiring integer constants upper lower bounds 
allowing form symbolic bounds constraints unknown values improve effectiveness range analysis especially eliminating array bounds checks iteration arrays unknown size 
range analysis run speed optimization range analysis compile speed optimization range analysis code density optimization small stanford oo stanford puzzle richards parser pathcache integer subrange analysis frequently large improvement compilation speed compiled code space costs 
optimizing away possible uncommon branches integer subrange analysis big improvement compilation speed execution speed saves lot compiled code space 
type prediction self compiler uses type prediction described section increase amount type information available compiler certain common message sends 
effectiveness technique easily measured simply performing type prediction 
charts report type prediction contribution execution performance effect compilation speed compiled code density 
type prediction self programs run faster times faster 
type prediction important customization achieving performance benchmarks 
type prediction mixed effect compilation speed speeding slowing compilation 
compiler compile faster type prediction lazy compilation uncommon branches common case type predicted version message send simpler faster compile original unpredicted message send 
type prediction reduces compiled code space costs benchmarks reason 
type prediction run speed optimization type prediction compile speed optimization type prediction code density optimization small stanford oo stanford puzzle richards parser pathcache block analysis blocks key part self system virtually control structures self compiler incorporates optimizations designed reduce cost blocks 
optimizations described section 
deferring block creations self compiler defers creating block needed real run time value 
disabling optimization easy 
charts report effect deferring block creations 
delaying eliminating unneeded block creations clearly boosts execution speed frequently customization type prediction 
course effect attributable primarily eliminating unneeded block creations entirely simply delaying block creations latest possible moment 
case measurements dramatically display importance optimizing block creation get performance language relies user defined control structures 
eliminating block creations saves significant amount compile time speeding compiler smaller benchmarks 
surprisingly eliminating code created unnecessary blocks significantly improves compiled code space density factor smaller benchmarks 
deferred block creations run speed optimization deferred block creations compile speed optimization deferred block creations code density optimization small stanford oo stanford puzzle richards parser primmaker pathcache exposed block analysis self compiler tracks blocks created passed run time arguments methods reduce number local variables arguments considered level accessible 
analysis disabled treating blocks exposed 
unfortunately simulation strategy reclaim compile time manipulating lists exposed blocks impact exposed block analysis compilation time determined 
charts report impact exposed block analysis execution speed compiled code space efficiency 
analyzing blocks exposed significantly speeds self programs making benchmarks run twice fast 
exposed block analysis greatly reduces number local variables arguments considered visible send points restart points uncommon branch entry points turn eliminates unnecessary register moves 
exposed block analysis save lot compiled code space factor smaller numerical benchmark suites 
savings comes extra register moves stack accesses exposed block analysis proves may eliminated 
common subexpression elimination self compiler performs common subexpression elimination described sections 
distinct kinds calculations eliminated redundant self compiler loads stores arithmetic operations constants 
aspects common subexpression elimination self compiler improve performance 
simply computations repeated traditional benefit common subexpression elimination 
benefit unique self compiler additional type information associated result original exposed block analysis run speed optimization exposed block analysis code density optimization small stanford oo stanford puzzle richards parser pathcache computation propagated result redundant computation described section 
example compiler eliminates load instruction redundant type information known contents loaded memory cell propagated result eliminated load instruction 
third benefit unique self compiler corresponding array bounds check may eliminated load store array element may eliminated 
described section benefit exists primarily symbolic range analysis implemented self compiler 
charts page display impact common subexpression elimination 
easy disable effects simply recording checking available values results configuration shown columns labelled cse 
individual contributions effects may measured recording checking available values advantage particular aspect information 
columns labelled cse constants cse arithmetic operations cse memory report incremental effect common subexpression elimination constants arithmetic instructions load store instructions respectively 
sum incremental effects columns ideally equal incremental effect cse shown column 
cse memory includes impact effects eliminating memory instructions propagating information types contents memory cells eliminating redundant bounds checks 
effects components shown columns labelled cse memory cell type information cse memory cell array bounds checking respectively 
difference columns cse memory column incremental effect just eliminating memory instructions 
propagated type information ignored introducing new value memory load replaced assignment node value initially bound unknown type 
common subexpression elimination relatively small effect execution speed performance improvement occasionally performance degradation 
cause poor performance common subexpression elimination redundant constants implemented current self system described section elimination constants frequently replaces instruction sequence sparc loads bit constant register instruction register move eliminating loading constant entirely 
cases saved constant allocated stack location register performance slow presence common subexpression elimination constants 
common subexpression elimination arithmetic operations helps stanford integer benchmarks average 
common subexpression elimination memory loads stores provides best improvements boosting performance object oriented versions stanford integer benchmarks nearly 
common subexpression elimination usually imposes small penalty compilation speed 
bulk extra cost compilation time comes common subexpression elimination memory loads stores partially extra required propagate mapping available memory cells contained values type analysis partially additional inlining enabled extra type information 
fortunately compilation speed drops execution speed rises common subexpression elimination tends pay cost compile time savings execution time 
common subexpression elimination usually saves small amount compiled code space expected 
parser benchmark presence common subexpression elimination increases amount code generated 
increase may attributed extra inlining enabled type information available common subexpression elimination memory 
register allocation self compiler maps names aliases variables allocates run time locations variables 
register allocation design described section 
unfortunately measure effectiveness register allocation strategy 
cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking run speed optimization compile speed optimization space density optimization small stanford oo stanford puzzle richards parser pathcache eliminating unneeded computations self compiler performs additional pass control flow graph eliminate unneeded computations memory arithmetic operations results described section 
disabling optimization easy complications 
pass required parts self compiler run correctly eliminating unnecessary assignment nodes prior constructing variable lifetime conflict graph part register allocation 
parts compiler expect pass executed get sort reasonable performance 
example initial type analysis phase expects unnecessary loads constants removed pass compiler feels free insert loads prior message sends variables may turn constant sends 
variables constants loads eliminated technique technique eliminate unnecessary loads 
avoid importance eliminating unneeded computations version self system constructed eliminated assignments loading constants eliminate unused arithmetic operations memory loads 
compilation speed relatively unaffected change compiler pass eliminate unnecessary constant loads assignments 
charts display impact eliminating unneeded arithmetic memory loads execution speed compiled code space density 
eliminating unnecessary arithmetic memory load instructions saves percent execution time 
eliminating unnecessary loading constants disabled difference execution speed greater 
virtually compiled code space saved optimization indicating arithmetic memory instructions eliminated benchmarks 
run speed optimization code density optimization small stanford oo stanford puzzle richards parser pathcache filling delay slots described section self compiler fills delay slots sparc speed generated code 
effect delay slot filling performance system computed constructing version system fill delay slots leaving nop instruction delay slot 
charts display effect delay slot filling 
filling delay slots sparc speeds benchmarks costs little compile time reduces compiled code space requirements 
execution speed compiled code space benefits delay slot filling worth nominal compile time expense 
summary effectiveness techniques techniques stand crucial achieving performance self similar languages 
inlining deferred block creations type prediction customization provide major improvements performance 
value type analysis exposed block analysis splitting lazy compilation uncommon branches delay slot filling significant contributions run time performance 
optimizations modest benefits 
number place upper bound average size basic block self system 
assuming reduction space instructions delay slot filled 
gives average basic block size self instructions filling delay slots 
delay slot filling run speed optimization delay slot filling compile speed optimization delay slot filling code density optimization small stanford oo stanford puzzle richards parser primmaker pathcache splitting strategies splitting important techniques self compiler complex implement 
different splitting strategies devised implemented self compiler 
section explore effectiveness techniques 
splitting strategies broken main approaches splitting reluctant splitting described section eager splitting described section 
reluctant splitting divided local reluctant splitting global reluctant splitting depending code compiler willing duplicate split 
eager splitting combined tail merging strategies tail merging tail merging forward computed type information tail merging reverse computed requirements 
base splitting strategies combined divided splitting described section lazy compilation uncommon branches described section 
subsections explore effectiveness various combinations 
measurements reported relative performance standard self splitting configuration local reluctant splitting lazy compilation uncommon branches divided splitting 
reluctant splitting strategies charts page illustrate effectiveness local reluctant splitting global reluctant splitting comparison configuration splitting 
results include effect lazy compilation uncommon branches 
results relative performance standard configuration local reluctant splitting lazy compilation uncommon branches 
local reluctant splitting clearly best strategy performing execution speed compilation speed compiled code density 
performing splitting saves compile time usually incurs significant run time speed penalty consumes additional compiled code space 
local reluctant splitting compares favorably splitting compilation speed code density splits performed part local reluctant splitting separate true results false results comparisons control structures performing kind splitting significantly simplifies control flow graph illustrated section paying savings compile time compiled code space 
global reluctant splitting compensate larger compile time compiled code space costs significantly improved execution speed 
benchmarks programs typically just single common case path case global reluctant splitting needed pair common case paths true result false result 
true false cases result comparison boolean function produced paths usually consumed immediately message iftrue message local reluctant splitting adequate handle kind situation 
benchmarks global reluctant splitting provides unneeded extra power 
argue subsection global reluctant splitting important lazy compilation uncommon branches devised lazy compilation splits uncommon case paths split albeit effectively global reluctant splitting 
global reluctant splitting useful today system local reluctant splitting lazy compilation multiple common case paths may frequent implementation techniques incorporated hcu global reluctant splitting may show significant improvement local reluctant splitting 
splitting local reluctant splitting global reluctant splitting compile speed normal self splitting local reluctant splitting global reluctant splitting space density normal self splitting local reluctant splitting global reluctant splitting run speed normal self small stanford oo stanford puzzle richards parser pathcache lazy compilation uncommon branches charts page report performance various reluctant splitting strategies lazy compilation uncommon branches 
results relative performance standard configuration local reluctant splitting lazy compilation uncommon branches 
lazy compilation uncommon cases improves execution performance splitting configurations benchmarks seen comparing results 
lazy column results corresponding 
lazy column 
speed may attributed major factors providing effect divided splitting simplifying register allocation problem 
largest effects occur splitting local reluctant splitting strategies benefit additional divided splitting effect 
strategies benefit better register allocation enabled lazy compilation 
largest impact execution speed occurs numerically oriented benchmarks 
expected benchmarks messages get inlined primitives introduce primitive failure uncommon branches messages getting inlined primitive calls may fail overflow error type prediction type tests get inserted integer type tests messages create uncommon branch 
larger number uncommon branch entries benchmarks provides opportunities lazy compilation 
lazy compilation dramatic improvement compilation speed compiled code space efficiency board base splitting strategies benchmarks 
expected lazy compilation improves performance smaller numerical benchmarks 
lazy compilation uncommon cases obviates need sophisticated base splitting techniques global reluctant splitting 
lazy compilation divided splitting implemented global reluctant splitting offer performance improvements local reluctant splitting seen comparing local reluctant splitting lazy results global reluctant splitting lazy results 
system common case path normally generated global reluctant splitting provide additional functionality 
vector type prediction configuring self system choice today set message names corresponding receiver types type predicted 
messages choice fairly obvious predict true false receiver iftrue predict integer receiver instance 
unfortunately choices clear cut 
example compiler predict message 
currently compiler predicts receiver integer consequently inserts type test integer receiver kinds objects integers compared equality non integer branch treated uncommon gives rise second common case path 
compiler faces similar problem messages put 
example compiler predict receiver messages built fixed length vector type 
arrays manipulated micro benchmarks stanford integer benchmarks type prediction improve performance benchmarks 
parts system receivers put dictionaries kinds keyed collections simple fixed length vectors 
current standard self configuration type prediction put consideration object oriented programs currently run self system kinds programs including benchmarks benefit type predicting fixed length vector receivers put 
determine improvement gained vector type prediction measured performance additional configurations 
extended standard configuration local reluctant splitting additionally type predict vectors version treats non vector cases uncommon treats non vector cases giving rise second common case path 
global reluctant splitting shine presence multiple common case paths measured configuration global reluctant splitting type predicts vectors treats non vector cases additional common case paths 
charts page report performance vector type prediction configurations relative standard configuration local reluctant splitting vector type prediction 
splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy run speed normal self compile speed normal self space density normal self small stanford oo stanford puzzle richards parser pathcache type predict vectors local splitting type predict vectors global splitting vectors common local splitting run speed normal self type predict vectors local splitting type predict vectors global splitting vectors common local splitting compile speed normal self type predict vectors local splitting type predict vectors global splitting vectors common local splitting space density normal self small stanford oo stanford puzzle richards parser pathcache type prediction vectors improves performance benchmarks 
individual benchmarks unchanged benchmarks oo puzzle improve raw data may appendix 
treating non vector cases uncommon difference benchmark boosting performance 
global reluctant splitting just effective treating non vectors uncommon verifying global reluctant splitting provide improved performance presence multiple common case paths 
unfortunately type predicting vectors slows larger object oriented benchmarks predictions wrong 
non vector receivers considered uncommon forces compiler generate optimized uncommon branch extensions slowing execution performance parser benchmark runs slower mode normal configuration 
expect self programs parser puzzle normal self system type predict vectors 
environments applications vector receivers common vector type prediction significantly improve performance 
developing system parts type predict vectors active area current research hcu 
type predicting vectors uniformly slows compilation 
usually slow mispredictions force compiler generate uncommon branch extension methods non vector cases considered uncommon compile times twice long normal configuration 
fact potentially lengthy compile pauses primary reason currently type predict vector receivers 
surprisingly vector type prediction takes compiled code space usually non predicting configuration 
mispredictions lazy compilation cause uncommon branch extensions created code space consumption double 
eager splitting strategies charts page report execution performance various eager splitting configurations relative standard configuration local reluctant splitting lazy compilation uncommon branches 
various tail merging strategies designed save compile time compiled code space producing execution speed 
expectations borne results 
different tail merging strategies achieve roughly execution speeds requirements analysis tail merging sophisticated tail merging technique achieves better compilation speeds compiled code densities simpler techniques extra reverse pass control flow graph part requirements analysis easily pays reduced compile time 
lazy compilation slightly improves execution performance eager splitting 
eager splitting incorporates divided splitting potential benefit lazy compilation unneeded 
compilation speed improves dramatically lazy compilation uncommon cases 
speed pronounced tail merging strategy sophisticated 
lazy compilation impact simpler tail merging strategies strategies spend compile time compiling common case parts control flow graph compile time spent uncommon branches eliminated 
similarly lazy compilation dramatically improves compiled code space efficiency especially eager splitting requirements tail merging 
unfortunately eager splitting provides execution speed benefits compared reluctant splitting best doubles compilation time costs cases benchmarks compiled eager splitting needed compiler temporary data space 
clearly eager splitting practical currently implemented 
eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy run speed normal self compile speed normal self space density normal self small stanford oo stanford puzzle richards parser pathcache divided splitting charts page report performance various reluctant splitting strategies divided splitting 
results relative performance standard configuration local reluctant splitting lazy compilation uncommon branches divided splitting 
divided splitting difference conjunction lazy compilation uncommon branches seen comparing results columns labelled 
lazy results columns labelled divided 
lazy 
expected lazy compilation achieves beneficial effect type analysis divided splitting efficient manner 
strategies lazy compilation divided splitting speeds execution significantly seen comparing results columns labelled 
lazy results columns labelled divided 
lazy 
advantages provided divided splitting decrease sophistication base splitting technique 
divided splitting usually slows compilation consumes compiled code space expected 
results divided splitting provided additional benefits lazy compilation uncommon cases 
environments lazy compilation uncommon cases impossible divided splitting offer improved performance 
summary splitting strategies local reluctant splitting combined lazy compilation uncommon cases effective splitting strategy optimizing execution speed compilation speed compiled code space efficiency 
lazy compilation provides dramatic near order magnitude improvements compilation speed code density boosts execution speed obviates need divided splitting 
global reluctant splitting provides slight improvements performance benchmarks noticeable decrease compilation speed 
eager splitting presently implemented provides significant performance improvements sacrifices half compilation speed local reluctant splitting 
vector type prediction improve performance benchmarks slows benchmarks 
chart page summarize effectiveness various splitting strategies trade offs execution speed compilation speed plotting execution speed compilation speed splitting strategies dimensional scatter chart relative compiled code space efficiency nearly directly proportional relative compilation speed third dimension necessary 
chart readable selected configurations included 
cluster strategies upper right corner chart region reluctant splitting lazy compilation uncommon branches 
strategies produce better execution speeds poor compilation speeds 
splitting lazy divided splitting lazy splitting lazy divided splitting lazy local reluctant splitting lazy divided local reluctant splitting lazy local reluctant splitting lazy divided local reluctant splitting lazy global reluctant splitting lazy divided global reluctant splitting lazy global reluctant splitting lazy divided global reluctant splitting lazy run speed normal self compile speed normal self space density normal self small stanford oo stanford puzzle richards parser pathcache paste chart page ps remaining sources overhead section explores detail remaining sources overhead able measure self system traditional optimized language implementation 
results summarized section 
type tests self compiler inserts extra run time type tests output optimizing compiler 
tests inserted part type prediction messages iftrue part type checking arguments primitives 
language features message passing dynamic typing generic arithmetic safe primitives self incur extra overhead 
measure cost run time type tests constructing version self generate type checks assumes type tests succeed 
configuration obviously unsafe benchmarks happen fail type tests break configuration 
charts report costs execution speed compilation speed compiled code space run time type tests checks report costs integer type tests boolean type tests individually 
type tests increase execution time benchmarks take little extra compile time add additional compiled code space 
type analysis splitting lazy compilation uncommon cases largely responsible relatively low number type tests remain compiled self code 
integer type tests boolean type tests run speed normal self integer type tests boolean type tests compile speed normal self integer type tests boolean type tests code density normal self small stanford oo stanford puzzle richards parser pathcache overflow checking self compiler generates overflow check integer arithmetic instruction check primitive failure corresponding integer arithmetic primitives 
handling primitive failure self programs support generic arithmetic 
generic arithmetic traditionally expensive language feature section showed performance programs generic arithmetic half programs avoid generic arithmetic 
added cost generic arithmetic involves extra type tests associated checking integer arguments generic arithmetic operations overhead measured previous section 
remaining cost generic arithmetic incurred extra overflow checks generated integer specific arithmetic supported traditional languages 
measure cost overflow checking self simply checking overflows 
configuration unsafe benchmarks overflow arithmetic operations 
charts report cost overflow checking 
overflow checking slows execution smallest benchmarks 
register allocator frequently leaves extra register move overflow checks removed cost overflow checking may results factor 
overflow checking imposes negligible compiletime cost small space cost 
overflow checking run speed normal self overflow checking compile speed normal self overflow checking code density normal self small stanford oo stanford puzzle richards parser primmaker pathcache array bounds checking self array accessing primitives verify accesses lie bounds array 
measure cost array bounds checking self checking access bounds generated code 
charts display cost array bounds checking 
array bounds checking imposes modest run time performance cost overhead benchmarks manipulating arrays 
think overhead eliminated applying sophisticated integer subrange analysis symbolic bounds complexity technique may worth apparently modest improvement execution time 
little compile time generating code check bounds array accesses 
compiled code space required support array bounds checking self implementation space overhead 
cost higher compiler compile uncommon cases lazily 
block zapping current self implementation allow block invoked lexically enclosing activation record returned 
enforce restriction compiler generates extra code blocks described section 
zapping cost fairly expensive 
block zapping involves extra run time code needed zero frame pointer block created extra registers stack locations hold blocks zap code 
block value methods require explicit run time code test zero frame pointer relying machine addressing protection hardware trap illegal addresses zapping architecture imposes run time overhead block invocation 
array bounds checking run speed normal self array bounds checking compile speed normal self array bounds checking code density normal self small stanford oo stanford puzzle richards parser pathcache gauge cost design measured configurations self system different rule zapping blocks zapping early zapping block lifetimes extend message initially argument late zapping block lifetimes extend scope created standard configuration 
early zapping proposed alternative late zapping expensive particularly terms register usage 
charts report performance zapping early zapping relative late zapping standard configuration 
surprisingly block zapping negligible impact execution speed benchmarks 
slight slow downs benchmarks may caused unlucky interactions parts system performance improve alternative zapping implementations 
expected self compiler faster bother generating zap code 
early zapping slows compiler relative late zapping data structures algorithms compiler generate early zapping code complex late zapping 
zapping early zapping slightly better compiled code space efficiencies late zapping percent 
results safety greater flexibility late zapping preferable block zapping strategy 
implementation self support true non lifo blocks fully upward closures dispense entirely need block zapping simplifying part compiler implementation process 
zapping early zapping run speed normal self zapping early zapping compile speed normal self zapping early zapping code density normal self small stanford oo stanford puzzle richards parser primmaker pathcache primitive failure checking self compiler implements primitive calling routine virtual machine generating special inlined code primitive compiler generates code primitive call checks primitive failure testing special return value 
run time overhead avoided alternate calling convention primitives different return offsets successful returns failing returns 
determine change useful need know cost current design 
compute cost simply checking primitive failures assuming primitives succeed 
charts report cost external primitive failure checking currently implemented 
primitive failure checking non inlined primitives effect execution performance benchmarks 
slight slow oo stanford primmaker benchmarks may unfortunate interactions register allocator removing run time check improve performance 
primitive failure checking imposes slight cost compile time increases space costs 
costs significant justify optimizing return sequence external primitives 
primitive failure checking run speed normal self primitive failure checking compile speed normal self primitive failure checking code density normal self small stanford oo stanford puzzle richards parser pathcache debugger visible names section argued language implementation support debugging entire program level source code optimizations implementation artifacts hidden programmer 
requirement restricts kinds optimizations performed possibly degrading performance system support source level debugging 
costs measured easily system cost performing tail call elimination reordering code 
fortunately costs attributable need debugging measured 
self system programmer get complete view state suspended process including values data slots activation records contents local variables arguments source level stack frames described section 
requires compiler ensure contents variable date available long debugger access compiled code need variable 
imposes cost terms registers reallocated expressions possibly causing expressions spilled stack 
measured cost variable lifetime extension configuring compiler ignore effect debugger computing lifetime variables freeing registers soon compiled code variable report effects charts 
supporting debugger ability view source level visible names executing code requires name imposes execution time cost benchmarks 
partially attributable presence hardware register windows sparc leaving unused variable register free saving restoring unused variable calls partially attributable presence interruption points uncommon branch entry points force names maintained anyway partially attributable variable names unused prior scope methods self typically shorter procedures traditional language 
ensuring names stay live visible lifetimes takes negligible amount extra compile time negligible amount compiled code space 
clearly aspect programming environment supported cost 
debugger visible names run speed normal self debugger visible names compile speed normal self debugger visible names code density normal self small stanford oo stanford puzzle richards parser pathcache interrupt checking stack overflow checking self run time system handles interrupts polling compiler generating code method entry restart points loop tails check interrupts described section 
cost associated checks avoided alternative interrupt architecture polling instructions ahead current program counter call interrupt handler exactly point polling code detected interrupt 
interrupt check method entry doubles check stack overflow polling need eliminated read protecting page maximum stack size page protection hardware detect stack overflow 
costs current associated interrupt polling probably avoided sophisticated run time system design important determine costly polling overhead current self implementation 
measure cost polling simply generating polling code ignoring interrupts 
charts report costs sources polling overhead 
interrupt checking imposes moderate cost execution time benchmarks 
numerical benchmarks slow interrupt checking restart percent interrupt checking method entries larger object oriented benchmarks slow percent interrupt checking method entries virtually interrupt checking restart 
difference reflects fact smaller benchmarks contain tight loops larger benchmarks contain calls 
generating code handle interrupts imposes fairly substantial compile time cost compared measured sources overhead 
interrupt checking restart costly calls involves ensuring debugger visible names properly set debugger display virtual source level call stack invoked interrupt point 
supporting interrupts polling imposes fairly significant compiled code space costs extra compiled code generated smaller benchmarks 
interrupt checking interrupt checking calls interrupt checking restarts run speed normal self interrupt checking calls interrupt checking restarts compile speed normal self interrupt checking calls interrupt checking restarts code density normal self small stanford oo stanford puzzle richards parser primmaker pathcache restart expensive calls may require extra code support debugger 
compile time costs avoided alternative run time mechanism handling interrupts 
lru compiled method reclamation support compiled methods stored fixed sized cache 
compiled methods flushed cache room new compiled methods cache full 
system attempts flush compiled methods soon keeping track compiled methods invoked flushing compiled methods 
compiler supports lru replacement strategy generating extra code compiled method prologue mark compiled method described section 
overhead exist implementation threw away compiled code replaced compiled methods different way fifo replacement lru replacement usage information computed periodic interrupts 
measured overhead current lru implementation disabling marking code method prologue 
charts display costs current lru compiled method reclamation support 
lru support imposes run time cost system support 
cost directly proportional call frequency program measured 
lru reclamation support takes little additional compile time little additional compiled code space 
summary remaining sources overhead remaining sources overhead able measure single source overhead imposes significant execution speed cost 
array bounds checking type testing overflow checking interrupt checking lru compiled method reclamation non trivial execution time cost incurs overhead 
remaining gap performance self optimized remains unaccounted 
run speed normal self lru compiled method reclamation support compile speed normal self code density normal self small stanford oo stanford puzzle richards parser primmaker pathcache additional space costs previous compiled code space efficiency measurements compared number machine instructions generated self compiler number machine instructions generated configurations self language implementations 
self compiler generates additional information compiled methods takes space 
additional pieces information include descriptions inlined scopes tables mapping physical program counters source level byte code position 
information reconstruct virtual call stack physical call stack debugging described section support lazy compilation uncommon branches compile block methods perform level accesses lexically enclosing stack frames 
compiler generates dependency links selective invalidation compiled methods programming changes described section 
compiler generates information identifying locations tagged object embedded compiled code scope debugging information enable system update pointers scavenge garbage collection 
chart breaks space consumed output self compiler categories information standard configuration 
chart illustrates machine instructions take relatively small fraction space consumed data space numerical benchmarks space larger objectoriented programs 
scope debugging information dependency links take lion share space generated information total generated space 
relocation information requires roughly constant total space 
program counter byte code mappings relatively concise total space 
fortunately information needs remain main memory times 
paged virtual memory brought main memory needed 
compiled instructions need main memory methods active 
location information needs main memory accessed relatively small stanford oo stanford puzzle richards parser primmaker pathcache space usage total debugging info pc mapping info relocation info dependency links instructions frequently 
scope byte code debugging information needed compiling nested blocks uncommon branch extensions debugging methods debugged scope information paged relatively quickly warming compiled code cache 
dependencies needed programming flushing invalid methods space paged program development mode 
working debugged programs extra data generated compiler paged main memory minimizing real memory requirements 
ultimately machine instructions locations need main memory garbage collections keeping real memory space costs fraction total virtual memory space costs 
tagged object pointers embedded compiled method refer objects old space objects scanned part scavenging location information needed full garbage collection normally paged 
optimization system automatically tenure objects reachable compiled methods allow location information paged immediately speed 
appendix raw benchmark data appendix includes raw data performance measurements reported chapter appendix row left right tables report running time benchmark seconds compile time benchmark seconds size bytes compiled instructions generated compiler benchmark 
smalltalk compile time compiled code space measurements unavailable 
blank rows correspond configurations compiler consumed internal memory mb compiling benchmark 
note local reluctant splitting lazy late block zapping configurations identical normal self configuration 
recur language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest sumto language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest tak language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest sieve language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest perm language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest towers language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest queens language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest quick language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest bubble language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest tree language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest oo perm language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest oo towers language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest oo queens language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest oo language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest oo quick language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest oo bubble language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest oo tree language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest puzzle language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest richards language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest parser language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest primmaker language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest pathcache language configuration run compile space optimized smalltalk normal integer normal self splitting lazy splitting lazy local reluctant splitting lazy local reluctant splitting lazy global reluctant splitting lazy global reluctant splitting lazy divided splitting lazy divided splitting lazy divided local reluctant splitting lazy divided local reluctant splitting lazy divided global reluctant splitting lazy divided global reluctant splitting lazy eager splitting lazy eager splitting lazy eager splitting tail merging lazy eager splitting tail merging lazy eager splitting requirements analysis lazy eager splitting requirements analysis lazy type predict vectors local splitting type predict vectors global splitting vectors common local splitting inlining line caching compile time lookup caching customization value type analysis range analysis type prediction deferred block creation exposed block analysis cse cse constants cse arithmetic operations cse memory cse memory cell type information cse memory cell array bounds checking eliminating unneeded computations delay slot filling integer type tests boolean type tests overflow checking array bounds checking block zapping early block zapping late block zapping primitive failure checking debugger visible names interrupt checking calls interrupt checking restarts lru compiled method reclamation support fast fastest bibliography harold abelson gerald jay sussman 
structure interpretation computer programs 
mit press cambridge ma 
ado adobe systems postscript language manual 
addison wesley reading ma 
asu alfred aho ravi sethi jeffrey ullman 
compilers principles techniques tools 
addison wesley reading ma 
aj randy allen steve johnson 
compiling vectorization parallelization inline expansion 
proceedings sigplan conference programming language design implementation pp 
atlanta ga june 
published sigplan notices july 
bowen alpern mark wegman kenneth zadeck 
detecting equality variables programs 
conference record fifteenth annual acm symposium principles programming languages pp 
san diego ca january 
atk robert atkinson 
hurricane optimizing compiler smalltalk 
oopsla conference proceedings pp 
portland september 
published sigplan notices november 
bbb backus best goldberg nelson sheridan stern hughes nutt 
fortran automatic coding system 
western joint computer conference pp 

andrew black norman hutchinson eric jul henry levy 
object structure emerald system 
oopsla conference proceedings pp 
portland september 
published sigplan notices november 
bh andrew black norman hutchinson 
typechecking polymorphism emerald 
technical report tr department computer science university arizona december 
bdg bobrow demichiel gabriel keene kiczales moon 
common lisp object system specification 
sigplan notices september 
preston briggs keith cooper ken kennedy linda torczon 
coloring heuristics register allocation 
proceedings sigplan conference programming language design implementation pp 
portland june 
published sigplan notices july 
cac gregory chaitin marc auslander ashok chandra john cocke martin hopkins peter markstein 
register allocation coloring 
computer languages pp 

cha chaitin 
register allocation spilling graph coloring 
proceedings sigplan symposium compiler construction pp 
boston ma june 
published sigplan notices june 
cu craig chambers david ungar 
customization optimizing compiler technology self dynamically typed object oriented programming language 
proceedings sigplan conference programming language design implementation pp 
portland june 
published sigplan notices july 
cul craig chambers david ungar lee 
efficient implementation self object oriented language prototypes 
oopsla conference proceedings pp 
new orleans la october 
published sigplan notices october 
published lisp symbolic computation kluwer academic publishers june 
cu craig chambers david ungar 
iterative type analysis extended message splitting optimizing dynamically typed object oriented programs 
proceedings sigplan conference programming language design implementation pp 
white plains ny june 
published sigplan notices june 
published lisp symbolic computation kluwer academic publishers june 
cuch craig chambers david ungar bay wei chang urs hlzle 
parents shared parts inheritance encapsulation self 
published lisp symbolic computation kluwer academic publishers june 
cu craig chambers david ungar 
making pure object oriented languages practical 
oopsla conference proceedings pp 
phoenix az october 
published sigplan notices october 
ch frederick chow john hennessy 
register allocation priority coloring 
proceedings sigplan symposium compiler construction pp 
montreal canada june 
published sigplan notices june 
ch fred chow john hennessy 
priority coloring approach register allocation 
acm transactions programming languages systems pp 
october 
cc cousot cousot 
interpretation unified lattice model static analysis programs construction approximation fixpoints 
conference record fourth acm symposium principles programming languages pp 
january 
cmr deborah coutant sue michelle 
doc practical approach source level debugging globally optimized code 
proceedings sigplan conference programming language design implementation pp 
atlanta ga june 
published sigplan notices july 
db peter deutsch daniel bobrow 
efficient incremental real time garbage collector 
communications acm october 
deu peter deutsch 
dorado smalltalk implementation hardware architecture impact software architecture 
glenn krasner editor smalltalk bits history words advice 
addisonwesley reading ma 
ds peter deutsch allan schiffman 
efficient implementation smalltalk system 
conference record eleventh annual acm symposium principles programming languages pp 
salt lake city ut january 
deu peter deutsch 
richards benchmark source code 
personal communication october 
deu peter deutsch 
benchmark suggestion 
personal communication september 
dixon mckee schweitzer vaughan 
fast method dispatcher compiled languages multiple inheritance 
oopsla conference proceedings pp 
new orleans la october 
published sigplan notices october 
gab richard gabriel 
performance evaluation lisp systems 
mit press cambridge ma 
gr leonard gilman allen rose 
apl interactive approach 
wiley new york 
gr adele goldberg david robson 
smalltalk language implementation 
addisonwesley reading ma 
gra justin owen graver 
type checking type inference object oriented programming languages 
ph thesis university illinois urbana champaign 
gj justin graver ralph johnson 
type system smalltalk 
conference record seventeenth annual acm symposium principles programming languages pp 
san francisco ca january 
gg ralph griswold griswold 
icon programming language 
prentice hall englewood cliffs nj 
gup rajiv gupta 
fresh look optimizing array bounds checking 
proceedings sigplan conference programming language design implementation pp 
white plains ny june 
published sigplan notices june 
hal robert halstead 
multilisp language concurrent symbolic computation 
acm transactions programming languages systems pp 
october 
har samuel harbison 
modula 
prentice hall englewood cliffs nj 
hdb robert kent dybvig carl bruggeman 
representing control presence class continuations 
proceedings sigplan conference programming language design implementation pp 
white plains ny june 
published sigplan notices june 
hei richard louis heintz jr low level optimizations object oriented programming language 
master thesis university illinois urbana champaign 
hen john hennessy 
symbolic debugging optimized code 
acm transactions programming languages systems july 
hen john hennessy 
stanford benchmark suite source code 
personal communication june 
hp john hennessy david patterson 
computer architecture quantitative approach 
san mateo 
hcc urs hlzle bay wei chang craig chambers ole agesen david ungar 
self manual version 
unpublished manual february 
hcu urs hlzle craig chambers david ungar 
optimizing dynamically typed object oriented programming languages polymorphic inline caches 
ecoop conference proceedings pp 
geneva switzerland july 
hl urs hlzle 
line cache improvements 
personal communication august 
hcu urs hlzle craig chambers david ungar 
debugging optimized code dynamic deoptimization 
appear proceedings sigplan conference programming language design implementation san francisco ca june 
hut norman hutchinson 
emerald object language distributed programming 
ph thesis university washington 
hrb norman hutchinson raj andrew black henry levy eric jul emerald programming language report 
technical report department computer science university washington october 
hc wen mei hwu chang 
inline function expansion compiling programs 
proceedings sigplan conference programming language design implementation pp 
portland june 
published sigplan notices july 
ing daniel ingalls 
smalltalk programming system design implementation 
conference record fifth annual acm symposium principles programming languages pp 
tucson az 
ive kenneth iverson 
programming language 
wiley new york 
jw kathleen jensen niklaus wirth 
pascal user manual report 
springer verlag new york 
joh ralph johnson 
type checking smalltalk 
oopsla conference proceedings pp 
portland september 
published sigplan notices november 
joh ralph johnson 
workshop compiling optimizing object oriented programming languages 
addendum oopsla conference proceedings pp 
orlando fl october 
published sigplan notices may 
ralph johnson justin graver lawrence 
ts optimizing compiler smalltalk 
oopsla conference proceedings pp 
san diego ca october 
published sigplan notices november 
eric jul henry levy normal hutchinson andrew black 
fine grained mobility emerald system 
acm transactions computer systems pp 
february 
jul eric jul object mobility distributed object oriented system 
ph thesis university washington december 
kr brian kernighan dennis ritchie 
programming language 
prentice hall englewood cliffs nj 
kil michael kilian 
trellis owl runs fast 
unpublished manuscript march 
kkr david kranz richard kelsey jonathan rees paul hudak james philbin norman adams 
orbit optimizing compiler scheme 
proceedings sigplan symposium compiler construction pp 
palo alto ca june 
published sigplan notices july 
kra david andrew kranz 
orbit optimizing compiler scheme 
ph thesis yale university 
david kranz robert halstead jr eric mohr 
mul high performance parallel lisp 
proceedings sigplan conference programming language design implementation pp 
portland june 
published sigplan notices july 
kra david kranz 
optimization object oriented features orbit compiler 
personal communication june 
kra david kranz 
optimization generic arithmetic orbit compiler 
personal communication june 
kra glenn krasner editor 
smalltalk bits history words advice 
addison wesley reading ma 
lh james larus paul hilfinger 
register allocation spur lisp compiler 
proceedings sigplan symposium compiler construction pp 
palo alto ca june 
published sigplan notices july 
lea douglas lea 
customization 
proceedings usenix conference pp 
san francisco ca april 
lee lee 
object storage inheritance self prototype object oriented programming language 
engineer thesis stanford university 
lz barbara liskov stephen zilles 
programming data types 
proceedings acm sigplan conference high level languages pp 
april 
published sigplan notices 
lsas barbara liskov alan snyder russell atkinson craig schaffert 
abstraction mechanisms clu 
communications acm pp 
august 
lab barbara liskov russell atkinson toby bloom eliot moss craig schaffert robert scheifler alan snyder 
clu manual 
springer verlag berlin 
lg barbara liskov john guttag 
abstraction specification program development 
mit press cambridge ma 
mcc carl mcconnell 
design rtl system 
master thesis university illinois urbanachampaign 
mcf scott mcfarling 
procedure merging instruction caches 
proceedings sigplan conference programming language design implementation pp 
toronto ontario canada june 
published sigplan notices june 
mey bertrand meyer 
genericity versus inheritance 
oopsla conference proceedings pp 
portland september 
published sigplan notices november 
mey bertrand meyer 
object oriented software construction 
prentice hall new york 
mey bertrand meyer 
eiffel language 
prentice hall new york 
mth robin milner mads tofte robert harper 
definition standard ml 
mit press cambridge ma 
mot motorola 
mc bit microprocessor user manual second edition 
prentice hall englewood cliffs nj 
nel greg nelson editor 
systems programming modula 
prentice hall englewood cliffs nj 
pw david padua michael wolfe 
advanced compiler optimizations supercomputers 
communications acm pp 
december 
pey simon peyton jones 
implementation functional programming languages 
prentice hall new york 
pw william pugh 
directional record layout multiple inheritance 
proceedings sigplan conference programming language design implementation pp 
white plains ny june 
published sigplan notices june 
ra jonathan rees norman adams 
dialect lisp lambda ultimate software tool 
proceedings acm symposium lisp functional programming pp 
august 
rc jonathan rees william clinger editors 
revised report algorithmic language scheme 
sigplan notices december 
ram jonathan rees norman adams james 
manual fifth edition yale university october 
rg steve richardson mahadevan ganapathi 
interprocedural analysis vs procedure integration 
information processing letters pp 
august 
ros john rose 
fast dispatch mechanisms stock hardware 
oopsla conference proceedings pp 
san diego ca october 
published sigplan notices november 
craig schaffert cooper carrie 
trellis object environment language manual 
technical report dec tr november 
craig schaffert cooper bruce mike kilian carrie 
trellis owl 
oopsla conference proceedings pp 
portland september 
published sigplan notices november 
sch robert scheifler 
analysis inline substitution structured programming language 
communications acm pp 
september 
ss peter sestoft harald sndergaard 
bibliography partial evaluation 
sigplan notices pp 
february 
shi olin shivers 
control flow analysis scheme 
proceedings sigplan conference programming language design implementation pp 
atlanta ga june 
published sigplan notices july 
shi olin shivers 
data flow analysis type recovery scheme 
technical report cmu cs march 
appear topics advanced language implementation peter lee ed mit press 
sla stephen 
programming language 
prentice hall englewood cliffs nj 
ste guy lewis steele jr lambda ultimate declarative 
ai memo mit artificial intelligence laboratory november 
ss guy lewis steele jr gerald jay sussman 
lambda ultimate imperative 
ai memo mit artificial intelligence laboratory march 
ste guy steele jr common lisp 
digital press 
ss leon sterling ehud shapiro 
art prolog 
mit press cambridge ma 
str bjarne stroustrup 
programming language 
addison wesley reading ma 
str bjarne stroustrup 
multiple inheritance 
proceedings european unix users group conference pp 
helsinki may 
sun sun microsystems 
sparc architecture manual version 
january 
ung david ungar 
generation scavenging non disruptive high performance storage reclamation algorithm 
proceedings acm sigsoft sigplan software engineering symposium practical software development environments pp 
pittsburgh pa april ung david michael ungar 
design evaluation high performance smalltalk system 
mit press cambridge ma 
david ungar randall smith 
self power simplicity 
oopsla conference proceedings pp 
orlando fl october 
published sigplan notices december 
published lisp symbolic computation kluwer academic publishers june 
david ungar craig chambers bay wei chang urs hlzle 
organizing programs classes 
published lisp symbolic computation kluwer academic publishers june 
weg mark wegman 
general efficient methods global code improvement 
ph thesis university california berkeley 
weg peter wegner 
dimensions object language design 
oopsla conference proceedings pp 
orlando fl october 
published sigplan notices december 
wik ke 
functional programming standard ml 
prentice hall london 
wm paul wilson thomas moher 
design opportunistic garbage collector 
oopsla conference proceedings pp 
new orleans la october 
published sigplan notices october 
wh patrick henry winston berthold klaus paul horn 
lisp 
addison wesley reading ma 
wir niklaus wirth 
program development stepwise refinement 
communications acm pp 
april 
zellweger 
interactive source level debugging optimized programs 
ph dissertation computer science department university california berkeley 
published xerox parc technical report csl may 
zj lawrence ralph johnson 
debugging optimized code expected behavior 
unpublished manuscript 
