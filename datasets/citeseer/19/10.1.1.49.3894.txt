adaptive optimization self reconciling high performance exploratory programming dissertation submitted department computer science committee graduate studies partial fulfillment requirements degree doctor philosophy urs lzle august copyright urs lzle rights reserved iii certify read dissertation opinion fully adequate scope quality dissertation degree doctor philosophy 
david ungar principal advisor certify read dissertation opinion fully adequate scope quality dissertation degree doctor philosophy 
john hennessy advisor certify read dissertation opinion fully adequate scope quality dissertation degree doctor philosophy 
peter deutsch certify read dissertation opinion fully adequate scope quality dissertation degree doctor philosophy 
john mitchell approved university committee graduate studies iv object oriented programming languages confer benefits including abstraction lets programmer hide details object implementation object clients 
unfortunately crossing abstraction boundaries incurs substantial run time overhead form frequent procedure calls 
pervasive abstraction desirable design standpoint may impractical leads inefficient programs 
aggressive compiler optimizations reduce overhead abstraction 
long compilation times introduced optimizing compilers delay programming environment responses changes program 
furthermore optimization conflicts source level debugging 
programmers caught horns dilemmas choose abstraction efficiency responsive programming environments efficiency 
dissertation shows reconcile seemingly contradictory goals performing optimizations lazily 
new techniques achieve high performance high responsiveness type feedback achieves high performance allowing compiler inline message sends information extracted runtime system 
average programs run times faster previous self system compared commercial smalltalk implementation medium sized benchmarks run times faster 
level performance obtained compiler simpler faster previous self compilers 
adaptive optimization achieves high responsiveness sacrificing performance fast compiler generate initial code automatically recompiling heavily parts program optimizing compiler 
previous generation workstation sparcstation fewer pauses exceeded ms minute interaction pauses exceeded second 
workstation pauses exceed ms dynamic deoptimization shields programmer complexity debugging optimized code transparently recreating non optimized code needed 
matter program optimized stopped inspected single stepped 
compared previous approaches deoptimization allows debugging placing fewer restrictions optimizations performed 
polymorphic inline caching generates type case sequences fly speed messages sent call site different types object 
significantly collect concrete type information optimizing compiler 
better performance interactive behavior techniques exploratory programming possible pure object oriented languages application domains requiring higher ultimate performance reconciling exploratory programming ubiquitous abstraction high performance 
vi vii acknowledgments ph 
thesis written help am deeply indebted people dissertation possible 
fortunate find great advisor david ungar taught research equally important write 
asked right questions early persisted answer 
dissertation readers peter deutsch john hennessy john mitchell advice comments drafts thesis 
peter dave smalltalk implementations originally sparked interest field years ago 
craig chambers bay wei chang helped get started self patiently answering questions joined self group 
members project ole agesen lars bak john maloney randy smith mario wolczko sure ran topics discuss created unique atmosphere working self fun 
guys 
learned interesting discussions including walter nther bob cmelik amer diwan claus robert rel barry hayes david keppel peter kessler ole lehrmann madsen boris magnusson eliot moss stephan jens palsberg peter michael schwartzbach paul wilson ben zorn 
robert comments earlier drafts proofreading final draft 
cis stanford sun stay enjoyable friends dinner club dessert chore pleasant 
am deeply indebted changed life graduate study 
wish organizations generously provided financial support especially sun microsystems laboratories swiss fulbright program 
viii ix table contents table contents ix list figures xiii list tables xvii 

background related self language self system overview implementation efficiency source level semantics direct execution semantics overview compilation process benefits related dynamic compilation customization previous self compilers smalltalk compilers deutsch schiffman system soar smalltalk compilers 
polymorphic inline caching line lookup caches dispatch tables inline caches handling polymorphic sends polymorphic inline caches variations implementation results execution time space overhead summary 
non inlining compiler simple code generation compilation speed execution speed overview lookup cache misses blocks primitive calls garbage collection register windows instruction cache misses nic vs deutsch schiffman smalltalk smalltalk hardwired control structures nic inlined control structures predicting performance self interpreter nic interactive performance summary 
type feedback adaptive recompilation type feedback adaptive recompilation recompile recompile overview recompilation process selecting method recompiled type feedback information adding type feedback conventional system applicability languages related static type prediction customization program optimizations inlining profile compilation summary 
optimizing self compiler front finding receiver types code inlining heuristics splitting uncommon traps ignoring uncommon cases helps uncommon branch extensions uncommon traps recompilation back intermediate code format computing exposed blocks def information copy propagation register allocation runtime system issues type tests store checks block zapping missing xi peephole optimization repeated type tests naive register allocation summary 
performance evaluation methodology type analysis improve performance object oriented programs type feedback works comparison systems different inputs stability performance type analysis exhibits unstable performance static type prediction fail non predicted messages slower mispredictions costly performance variations high level languages detailed performance analysis type feedback eliminates type tests type feedback reduces cost type tests type feedback reduces time spent type tests type analysis vs type feedback reoptimization effectiveness size compiled code summary 
hardware impact performance instruction usage register windows hardware support tagged arithmetic instruction cache behavior data cache behavior possible improvements 
responsiveness pause clustering compile pauses pauses interactive session pauses faster systems starting new code performance variations start behavior large programs performance stability influence configuration parameters performance variation influence configuration parameters final performance xii compilation speed summary 
debugging optimized code optimization vs debugging displaying stack single stepping changing value variable changing procedure deoptimization recovering unoptimized state transformation function lazy deoptimization interrupt points updating active methods common debugging operations single step finish breakpoints discussion benefits current limitations generality implementation cost impact responsiveness impact runtime performance memory usage related 
applicability summary glossary detailed data xiii list figures 
customization 
splitting 
type prediction expression 
overview dynamic recompilation self 
inline cache send 
empty inline cache 
inline cache 
size distribution degree polymorphism inline caches 
polymorphic inline cache 
execution time saved polymorphic inline caches 
polymorphism vs ratio 
profile nic compilation 
nic compile time function source method size 
histogram nic compile time 
timeline interactive session cold start scenario 
timeline interactive session warm start scenario 
self compilation process 
overview recompilation process 
type feedback statically compiled system 
organization optimizing self compiler 
class hierarchy expressions 
finding type feedback information 
finding type feedback information ii 
finding type feedback information iii 
splitting 
code handling cases 
code handling common cases 
sequence events handling uncommon traps 
execution speed self relative self nofeedback 
number sends self relative self nofeedback 
execution speed self relative self nofeedback 
calls performed self relative self nofeedback 
call frequency relative unoptimized self 
call frequency reduction vs execution time reduction 
reasons self improved performance 
self execution speed compared languages 
performance self stanford benchmarks 
performance self stanford benchmarks 
number type tests executed self nofeedback self 
sends inlined additional inline type test 
path length type tests 
arity type tests dynamic averages 
execution time consumed type tests 
percentage execution time spent type tests self xiv 
performance stanford integer benchmarks 
analysis inner loop sieve benchmark 
self type testing overhead stanford benchmarks 
code size self self 
dynamic instruction usage specint vs self benchmarks 
dynamic instruction usage specint vs self benchmarks excluding ops unconditional branches 
differences self spec programs 
register window overhead 
time overhead instruction cache misses self 
time overhead instruction cache misses self 
time overhead data cache misses self write allocate subblock placement 
time overhead data cache misses self write cache 
write ratio self write cache 
object allocation self 
individual pauses resulting pause clusters 
distribution individual vs combined compile pauses 
distribution compile pause length 
compile pauses minute interaction 
long term clustering compilation pauses 
compilation pauses faster cpus 
overview performance development 
start phase selected benchmarks 
correlation program size time stabilize performance 
performance variations self 
performance variations subset benchmarks self 
alternate visualization data 
performance variations subset benchmarks self worst run 
performance variations invocation counters don decay 
influence recompilation parameters performance 
influence recompilation parameters performance cecilcomp 
compilation speed self nofeedback 
compilation speed self 
profile self nofeedback compilation 
displaying stack 
pseudo code declarations scope data structures 
recovering source level state 
transforming optimized stack frame unoptimized form 
lazy deoptimization stack frames 
cache ratios self 
data read ratios self 
data write ratios self 
data read ratios self write cache xv 
data write ratios self write cache 
data read ratios self eden xvi xvii list tables table 
lookup timings cycles sparcstation table 
inline cache ratios benchmark programs table 
space overhead pics table 
performance unoptimized code relative optimized code table 
software profile unoptimized code table 
hardware profile unoptimized code table 
performance parcplace smalltalk inlined control structures table 
performance richards various system configurations table 
performance improved nic versions relative original nic table 
benchmark programs table 
systems benchmarking table 
implementation characteristics benchmarked systems table 
performance long running benchmarks table 
performance variation related static type prediction table 
performance variations caused failing static type prediction table 
time taken unoptimized code table 
differences table 
summary main differences self specint table 
arithmetic operation frequency estimated benefit tagged instructions table 
cache parameters current workstations table 
sources possible performance improvements table 
speed workstation pcs table 
ui interaction sequence table 
configuration parameters impacting compilation pauses self table 
start behavior dynamic compilation table 
space cost debugging information relative instructions table 
distribution degree polymorphism call sites table 
execution time saved pics table 
execution time comparison table 
number dynamically dispatched calls table 
performance relative times ms simulated time table 
performance relative smalltalk lisp table 
time saved inlined call table 
time spent performing type tests table 
type tests self nofeedback table 
type tests self table 
number comparisons type test sequence table 
performance stanford integer benchmarks table 
specint instruction usage table 
self instruction usage table 
instruction usage richards deltablue table 
allocation behavior benchmarks self table 
self compiler configuration parameters table 
pause length histogram data xviii 
object oriented programming increasingly popular programming easier 
allows programmer hide implementation details object clients turning object data type operations state accessed message sends 
late binding greatly enhances power data types allowing different implementations data type interchangeably runtime 
code invoking operation object aware exactly code executed result invocation late binding called dynamic dispatch selects appropriate implementation operation object exact type 
late binding essential object oriented programming implementations need efficiently support 
ideally object oriented languages late binding basic operations instance variable access 
pervasively encapsulation dynamic dispatch flexible reusable resulting code 
unfortunately late binding creates efficiency problems 
example instance variable accesses performed message sends compiler translate access attribute object simple load instruction objects implement differently 
example cartesian point just return value instance variable polar point compute rho theta 
variation precisely late binding means binding operation implementation instance variable access computation delayed runtime 
operation compiled dynamically dispatched call called indirect procedure call virtual call selects appropriate implementation runtime 
cycle instruction cycle call 
increased flexibility reusability source code significant run time overhead encapsulation efficiency coexist 
dissertation shows reconcile 
similar efficiency problem arises desire exploratory programming environments 
exploratory programming environment increases programmer productivity giving immediate feedback programming actions pause free interaction allows programmer concentrate task hand distracted long compilation pauses 
traditionally system designers interpreters non optimizing compilers exploratory programming environments 
unfortunately overhead interpretation combined efficiency problems created dynamic dispatch slows execution limits usefulness systems 
dissertation describes reduce overhead dynamic dispatch preserving responsiveness required interactive exploratory programming environment 
research vehicle object oriented language self 
self pure semantics exacerbate implementation problems faced object oriented languages self single operation assignment involves late binding 
language ideal test case optimizations reduce overhead late binding 
equally importantly self designed exploratory programming 
exploratory programming environment provide quick turnaround programming changes easy debugging partially complete programs order increase programmer productivity 
optimizing compiler system overcome performance problems created dynamic dispatch compatible compilation quick non intrusive system support full source level debugging times 
implemented system self 
contributions new optimization strategy type feedback allows dynamically dispatched call inlined 
example implementation self type feedback reduces call frequency factor improves performance compared system type feedback 
despite radically different language models self new system brings self half performance optimized object oriented programs 
recompilation system dynamically hot spots application 
system quickly generates initial code fast non optimizing compiler time critical parts slower optimizing compiler 
introducing adaptive recompilation dramatically improved interactive performance self system making possible combine optimizing compilation exploratory programming environment 
previous generation workstation sparcstation fewer pauses exceeded ms minute interaction pauses exceeded second 
extension inline caching polymorphic inline caching speeds dynamically dispatched calls polymorphic call sites 
addition improving performance median polymorphic inline caches source type information type feedback 
debugging system dynamically code provide source level debugging globally optimized code 
optimized code supports restricted debugging resulting system hide restrictions provide full source level debugging including debugging operations implementation relies dynamic compilation techniques described thesis require 
type feedback straightforward integrate conventional compiling system see section similar profile optimizations 
source level debugging code implemented keeping precompiled unoptimized code separate file see section 
polymorphic inline caches require simple stub generator full fledged dynamic compilation 
reoptimization system described section nature specific dynamic compilation 
techniques described thesis specific self language 
debugging system largely language independent discussed section 
type feedback optimize late binding language object oriented languages non object oriented languages heavy late binding apl generic operators lisp generic arithmetic profit optimization 
system dynamic compilation profit adaptive recompilation improve performance reduce compile pauses 
remainder dissertation describe design implementation techniques evaluate performance impact self system 
techniques fully implemented stable part public self distribution 
chapter presents overview self described subsequent chapters discusses related 
chapter discusses dynamic dispatch optimized runtime system outside compiler 
chapter describes non optimizing self compiler evaluates performance 
chapter describes type feedback reduces overhead dynamic dispatch chapter describes implementation optimizing self compiler 
chapters evaluate new self compiler performance relative previous self systems languages investigate impact hardware features performance 
chapter discusses optimizing compilation impacts interactive behavior system 
chapter describes system provide full source level debugging despite optimizations performed compiler 
hurry familiar previous smalltalk self implementations recommend skim chapter overview read summaries chapter plus 
glossary page contains short definitions important terms thesis 
available anonymous ftp self stanford edu www self stanford edu 

background related chapter presents background related dissertation 
briefly introduce self language goals followed overview self system 
describe compilation process new system review related 
self language self dynamically typed prototype object oriented language originally designed david ungar randall smith xerox parc 
conceived alternative smalltalk programming language self attempts maximize programmer productivity exploratory programming environment keeping language simple pure reducing expressiveness malleability 
self pure object oriented language data objects computation performed message sends including instance variable accesses receiver object 
self merges state behavior syntactically method invocation variable access indistinguishable sender know message implemented simple data access method 
consequently code representation independent code reused objects different structure long objects correctly implement expected message protocol 
words self supports fully data types interface object set messages responds visible implementation level details object size structure hidden code object 
self main highlights listed 
self dynamically typed programs contain type declarations 
self prototypes classes 
object self describing changed independently 
addition flexibility approach prototype systems avoid complexity introduced metaclasses 
self multiple inheritance 
inheritance design underwent changes years 
self single inheritance self introduced prioritized multiple inheritance combined new privacy mechanism better encapsulation sender path rule disambiguating equal priority parents 
pendulum back simplicity self eliminated sender path tended hide ambiguities self eliminated prioritized inheritance privacy language 
control structures user defined 
example self statement 
control structures implemented message sends blocks closures just smalltalk 
smalltalk self implementation control structure words programmer change definition method including implementing statements loops integer addition system faithfully reflect changes 
objects heap allocated deallocated automatically garbage collector 
features designed harness computing power modern hardware programmer life easier 
example representation independence easier reuse reorganize code creates implementation problems data access involves message send 
concentrating minimizing program execution time self concentrates minimizing programming time 
self system goal self implementation reflects language maximize programming productivity 
features contribute goal source level semantics 
system behavior explained source level terms 
programmers confronted error messages segmentation fault arithmetic exception denormalized float errors explained diving low level implementation details outside language definition hard understand 
self primitives safe arithmetic operations test overflow array accesses perform index bounds checks 
direct execution semantics interpreter semantics 
system behave directly executed source methods source change immediately effective 
direct execution semantics frees programmer worrying having explicitly invoke compilers linkers having deal tedious details makefile dependencies 
fast turnaround time 
making change programmer wait slow compiler linker delays kept short possible ideally just fraction second 
efficiency 
programs run efficiently despite self programmer penalized choosing self conventional language 
goals easy reach 
particular emphasis right language programmer machine self hard implement efficiently 
key features language create particularly hard problems computation performed sending messages call frequency naive implementation extremely high 
simple frequent computations usually require single instruction conventional languages instance variable access integer addition involve dynamically dispatched procedure calls programs run dozens hundreds times slower conventional languages 
similarly built control structures simple loop involves dozens message sends creation blocks closures 
definitions control structures changed user compiler take shortcuts translation hand optimized code patterns 
goal maximize programmer productivity system highly interactive provide immediate feedback user 
long compilation pauses occur optimizing compilers unacceptable restricting implementor freedom searching efficient self implementations 
sections give overview current self implementation attempts overcome problems described 
describing base system outline new solutions topic thesis fit existing system 
overview implementation self virtual machine consists subsystems memory system handles allocation garbage collection 
objects stored heap generation scavenging garbage collector reclaims unused objects 
object tagged bit tag lower bits bit word tag integers pointers floats object headers 
objects integers floats consist words header word pointer object map :10.1.1.56.2990
map describes object format viewed low level type object 
objects format share map object layout information stored 
preserve illusion self describing objects mandated language maps copy write example slot added object object gets new map 
parser reads textual descriptions objects transforms real objects stored heap 
methods represented set simple byte codes essentially send push literal return 
message name receiver lookup system determines result lookup matching slot 
lookups check hash table code table see compiled code exists lookup 
code executed system performs actual object lookup traversing receiver parent objects necessary invokes compiler generate machine code resulting method data access 
compiler translates byte codes method machine code stores code code cache 
isn room code cache newly compiled method existing methods flushed room 
code cache keeps approximate lru information determine compiled methods flush 
virtual machine contains numerous primitives invoked self programs perform arithmetic graphics 
new primitives dynamically linked system runtime 
contain details system :10.1.1.30.1652
efficiency self pure semantics threatened programs extremely inefficient early implementation effort went compiler techniques optimizing self programs 
techniques successful dynamic compilation 
compiler dynamically translates source methods compiled methods demand 
separate compilation phase execution compilation interleaved 
self dynamic compilation inspired deutsch schiffman smalltalk system 
customization 
customization allows compiler determine types message receivers method :10.1.1.56.2990
extends dynamic compilation exploiting fact messages method sent self 
compiler creates separate compiled version source method receiver type 
example create separate methods min source method computing minimum numbers 
duplication allows compiler customize version specific receiver type 
particular knowing type self compile time enables compiler inline sends self 
customization especially important self messages sent self including instance variable accesses global variable accesses kinds user defined control structures 
type prediction 
certain messages exclusively sent particular receiver types 
messages compiler uses optimization originally introduced early smalltalk systems predicts type source method finding minimum numbers compiled method specialized integer receivers compiled method specialized floating point receivers customization 
customization receiver message name inserts runtime type test message send test expected receiver type 
similar optimizations performed lisp systems optimize generic arithmetic 
branch type test succeeds compiler precise information type receiver statically bind inline copy message 
example existing self smalltalk systems predict sent integer measurements indicate occurs time 
type prediction improves performance cost test low likelihood successful outcome high 
splitting way turn polymorphic message separate monomorphic messages 
avoids type tests copying parts control flow graph :10.1.1.56.2990
example suppose object known integer branch statement floating point number branch 
object receiver message send statement compiler copy send branches 
exact receiver type known branch compiler inline copies send 
optimizations elevated self performance reasonable level 
example chambers ungar reported self significantly outperformed parcplace smalltalk implementation suite small integer benchmarks 
source level semantics combination language implementation features ensures behavior programs erroneous ones understood solely source level terms 
language guarantees message send finds matching slot accessing data running method results message understood error 
consequently errors level lookup errors 
furthermore implementation guarantees primitives integer 
int add send 
type prediction expression 
splitting send area inlined code area method circles inlined code area method squares splitting splitting safe 
primitives check operands results fail defined way operation performed 
example integer arithmetic primitives check overflow array access primitives check range errors 
self allow pointer arithmetic uses garbage collection system possible accidentally overwrite random pieces memory dereference dangling pointers 
combination features easier find program errors 
safe primitives efficient implementation harder :10.1.1.30.1652
example integer addition check overflow slowing 
importantly result type integer operations unknown primitives take failure block argument operation fails message sent argument 
result send result primitive call 
example method integers current self system invokes primitive failure block converts arguments arbitrary precision integers adds 
exact result type expression unknown known integers overflow result integer result big represented machine level integer result arbitrary precision number 
known integers compiler know statically second expression invoke method integers method arbitrary precision numbers 
safe primitives help programmer potentially slow execution 
direct execution semantics self mimics interpreter dynamic compilation 
source method invoked corresponding compiled code compiler invoked automatically generate missing code 
conversely user changes source method compiled code depending old definition invalidated 
accomplish system keeps dependency links source compiled methods :10.1.1.30.1652
explicit compilation linking step traditional edit compile link run cycle collapsed edit run cycle 
programs changed running application debugged need restarted scratch 
overview compilation process shows compilation process new self system detail 
self source methods stored objects heap just data objects 
methods code encoded string byte codes instructions simple stack machine 
byte codes directly executed interpreter source methods unoptimized code call counters method invoked optimized code executed called optimized code uncommon cases happen optimizations needed debugging 
overview dynamic recompilation self numbers diagram refer sections discussing part compilation process 
current self system 
byte codes translated native machine code demand 
source method executed time machine code generated fly 
usually compiled code generated fast dumb compiler 
unoptimized object code straightforward translation byte codes quite slow chapter describes non optimizing compiler detail 
unoptimized methods contain invocation counters 
counter exceeds certain threshold system invokes recompilation system determine optimization needed methods recompiled 
recompilation system calls optimizing compiler 
chapter explains recompilation process detail chapter describes optimizing compiler 
part developed new variant inline caches polymorphic inline caches pics 
pics speed runtime dispatching polymorphic call sites provide valuable information receiver types compiler 
chapter describes pics chapter explains optimizing compiler takes advantage type feedback information contained pics 
cases optimized methods may recompiled 
example recompiling system decide optimized version method contains calls inlined wasn type information optimizing time 
optimized method recompiled encounters uncommon cases compiler may predict certain situations occur integer overflow omit generating code cases 
omitted uncommon case happens anyway original compiled method recompiled extended code needed handle case see section 
remainder thesis discuss individual components system order usually come play polymorphic inline caches pics non inlining compiler recompilation optimizing compiler 
benefits techniques described thesis implemented self system improving areas higher performance realistic programs 
automatically recompiling sections program optimizing compiler take advantage type information collected runtime 
additional information allows compiler produce efficient code previously possible compiler performs compile time analysis program 
deltablue constraint solver example runs times faster previous optimizing compiler see chapter 
stable performance 
new compiler relies dynamically observed program behavior static analysis susceptible breakdowns static analysis techniques 
fragile performance major problem previous self compiler small source changes result dramatic performance loss particularly important optimization disabled change 
example set small changes stanford integer benchmarks slowed factor compiled previous self compiler compiled new compiler see section 
faster simpler compilation 
approach relying dynamic feedback runtime system employing sophisticated static type analysis techniques additional advantages better runtime performance 
results considerably simpler compiler vs non comment source lines 
second new compiler runs times faster previous self compiler see section 
user compile pauses reduced see section system needs optimize parts application 
support source level debugging 
previous self system users print stack optimized programs change programs running perform common debugging operations single stepping 
new system provides functionality automatically code needed debugger job easier see chapter 
related section compares self system generally related detailed comparisons chapters 
dynamic compilation self uses dynamic compilation generates code fly runtime 
idea dynamically creating compiled code traditional batch style compilation grew quest faster interpreters compiling native code interpreter overhead especially decoding pseudo code instructions avoided 
example mitchell proposed convert parts dynamically typed interpreted programs compiled form assuming types variables remained constant 
compiled code generated side effect interpreting expression time 
similarly threaded code early remove interpretation overhead 
interpreters dynamic compilers traditionally reasons languages hard efficiently compile statically usually source code contain low level implementation type information generate efficient code 
second languages strongly emphasize interactive implemented interpreters slow compilers 
apl language hard compile statically operators polymorphic emphasizes interactive 
surprisingly apl systems exploit dynamic compilers 
example johnston describes apl system dynamic compilation efficient alternative interpretation 
systems mechanisms similar customization guibas wyatt inline caching weiss 
deutsch schiffman pioneered dynamic compilation object oriented systems 
smalltalk implementation dynamically translates byte codes defined smalltalk virtual machine native machine code caches compiled code deutsch schiffman estimate just simple dynamic compilation interpretation speeds system factor sophisticated compiler gains factor 
franz describes variation dynamic compilation generates machine code load time compact intermediate code representation 
byte codes contained smalltalk snapshots intermediate code architecture independent addition intended language independent current implementation supports oberon 
compilation intermediate code machine code fast loader competitive standard loaders 
dynamic compilation useful applications language implementation wide variety ways 
example kessler implement fast breakpoints debuggers 
pike speed bit blt graphics primitives dynamically generating optimal code sequences 
operating systems dynamic compilation efficiently support fine grain parallelism eliminate overhead protocol stacks dynamic linking 
dynamic compilation areas database query optimization microcode generation fast instruction set emulation :10.1.1.13.6511
customization idea customizing portions compiled code specific environment closely related dynamic compilation environment information available runtime 
example mitchell system specialized arithmetic operations runtime types operands 
type variable changed compiled code depended type discarded 
language support user defined polymorphism object oriented main motivation scheme reduce interpretation overhead replace generic built operators simpler specialized code sequences replace generic addition integer addition 
similarly apl compilers created specialized code certain expressions 
systems hp apl compiler comes closest customization technique self 
hp apl system compiles code statement statement basis 
addition performing apl specific optimizations compiled code specialized specific operand types number dimensions size dimension element type storage layout 
called hard code execute efficiently general versions computation performed apl operator may vary wildly depending actual argument types 
preserve language semantics specialized code preceded prologue verifying argument types conform specific assumptions compiling expression 
compiled code safely reused executions expression 
types hopefully common case compilation necessary types differ new version restrictive assumptions generated called soft code 
descriptions system clear old hard code retained multiple hard versions exist time specialized different case 
customization applied traditional batch oriented compilers 
example cooper describe fortran compiler create customized versions procedures enable certain loop optimizations 
nicolau describes fortran compiler dynamically selects appropriate statically generated version loop 
saltz delay loop scheduling runtime 
generate specialized cache simulator different set simulation parameters 
keppel discuss value specific runtime specialization variety applications 
theoretically oriented areas computer science customization known partial evaluation 
previous self compilers self compiler described thesis predecessors 
self compiler introduced customization splitting achieved reasonably performance :10.1.1.56.2990
stanford integer benchmarks richards benchmark programs ran order times slower optimized times faster fastest smalltalk system time 
compiler intermediate code tree hard significantly improve code quality explicit representation control flow 
example hard find successor control flow terms node optimizations spanning nodes difficult implement 
compilation speed usually vary widely compiler algorithms superlinear size source methods 
compile pauses noticeable code compiled full optimization 
compiler implemented lines 
second compiler called self designed remove restrictions compiler increase runtime performance 
introduced iterative type analysis sophisticated back 
result achieved excellent performance stanford integer benchmarks half speed optimized performance richards benchmark improve significantly 
unfortunately users experienced similar discrepancies programs programs particularly small integer loops showed excellent performance larger programs perform 
discuss reasons discrepancy detail section sophisticated analyses optimizations performed compiler price compile time compared previous self compiler compilation times slower 
compared standard sun compiler compiler improved ratios directly compared current numbers 
excluding empty lines comments preprocessor lines usually statement line 
pauses distracting cause users new compiler 
compared compiler self compiler considerably complex consuming lines code 
smalltalk compilers smalltalk probably object oriented language closest self projects investigated techniques speed smalltalk programs 
deutsch schiffman system deutsch schiffman system represents state art commercial smalltalk implementations 
contains simple fast dynamic compiler performs peephole optimizations inlining 
self smalltalk implementation certain important methods integer addition messages implementing statements certain loops 
result deutsch schiffman compiler generate efficient code constructs achieved optimizations similar performed self compilers 
deutsch schiffman compiler uses order instructions generate compiled machine instruction compile pauses virtually unnoticeable 
addition dynamic compilation deutsch schiffman system pioneered optimization techniques 
inline caching speeds message lookup caching lookup result call site see chapter 
result cost sends reduced cost call type test 
self uses inline caching polymorphic inline caches extend usefulness polymorphic call sites see chapter 
type prediction speeds common sends predicting receiver type see section 
self compilers non optimizing self compiler described chapter type prediction various degrees 
optimizing self compiler extends static type prediction dynamically predicting receiver types type feedback information chapter 
soar goal soar smalltalk risc project speed smalltalk combination hardware software 
software side soar non dynamic native code compiler methods compiled native code immediately parsed inline caching type prediction generation scavenging garbage collector 
deutsch schiffman compiler soar compiler perform extensive global optimizations method inlining 
hardware side soar variation berkeley risc ii processor important hardware features register windows tagged integer instructions 
combination soar software hardware features successful compared smalltalk implementations cisc machines ns cycle time soar ran fast ns xerox dorado workstation faster deutsch schiffman smalltalk system running sun cycle time ns 
see chapter optimization techniques self compiler greatly reduces performance benefit special hardware support 
smalltalk compilers smalltalk systems attempted speed programs annotating type declarations 
atkinson partially implemented smalltalk compiler type declarations hints generate better code 
example programmer declared local variable type class compiler look message sends variable inline called method 
code safe compiler inserted type test inlined code current value wasn instance class unoptimized untyped version method invoked 
atkinson compiler completed run small examples reported speedups factor deutsch schiffman system sun 
course obtain speedup cycle time sun ns instructions hit small chip cache ns instructions programmer carefully annotate code type declarations invocations right types sped 
ts compiler took similar approach 
types specified sets classes compiler statically bind calls receiver type single class 
methods inlined programmer marked 
receiver type small set classes compiler inserted type test class optimized individual branches separately 
optimization viewed form type prediction programmer supplied type declaration list messages expected types hardwired compiler 
type prediction atkinson compiler programmer type declarations just hints firm promises 
validity promises checked type checker checker completed couldn analyze significant portions smalltalk system 
back ts compiler rtl intermediate form performed extensive optimizations 
written smalltalk compiler slow needing seconds compile benchmarks short 
published data efficiency generated code available ts compiler fully completed compile small benchmark programs 
data comparing ts tektronix smalltalk interpreter workstation indicate ts ran twice fast deutsch schiffman system small benchmarks annotated type declarations ts 
comparison entirely valid ts implement full semantics smalltalk constructs 
example integer arithmetic checked overflow 
significant impact performance small ts benchmarks 
example loop sumto benchmark adding integers contains instructions overflow check add significant overhead 
importantly type variable accumulating result sum specified result operation necessarily overflow occurs smalltalk primitive failure code converts arguments arbitrary length integers returns sum 
declared type sum upper bound general significantly slow generated code 
typed smalltalk system second kind type called signature type information compiler optimizations 

polymorphic inline caching object oriented programs send messages sends fast 
chapter reviews existing wellknown techniques improving efficiency lookups dynamically typed object oriented languages describes polymorphic inline caches extension standard inline caching developed 
line lookup caches object oriented languages substitute message sends procedure calls 
sending dynamically bound message takes longer calling statically bound procedure program find correct target method runtime type receiver inheritance rules language 
early smalltalk systems simple inheritance rules relatively slow interpreters method lookup known message lookup responsible substantial portion execution time 
lookup caches reduce overhead dynamically bound message passing 
lookup cache maps pair receiver type message name target method holds lookup results 
message sends consult cache indexing receiver type message name 
cache probe fails call expensive lookup routine traverses inheritance graph find target method stores result lookup cache possibly replacing older lookup result 
lookup caches effective reducing lookup overhead 
berkeley smalltalk example slower lookup cache 
dispatch tables statically typed languages implement message lookup dispatch tables 
common variant message name encoded integer index type specific dispatch table contains address target function 
message send consists loading address dispatch table stored word object indexing table get address target function calling function 
dispatch tables simple implement statically typed languages range table index set possible messages sent object known statically sizes contents tables easy compute 
possible dispatch tables dynamically typed languages albeit added complications 
major drawback methods hard integrate interactive system dispatch tables need reorganized periodically introducing new messages new types large system smalltalk system take minutes dispatch table consume kbytes 
discuss dispatch tables 
inline caches lookup cache sending message takes considerably longer calling simple procedure cache probed message sent 
ideal case cache lookup involves order instructions retrieve receiver type message name form cache index example xoring shifting receiver type message name fetch cache entry compare entry actual receiver type message name verify cached target method correct 
lookup cache effective accesses hit sends relatively slow lookup cache probe adds roughly instructions send 
various complications ensue multiple inheritance discussion see 
aside unclear dispatch tables compete inline caches terms performance especially modern superscalar processors indirect call introduces costly pipeline stalls 
fortunately message sends sped observing type receiver call site rarely varies message sent object type particular call site time send executed receiver type example studies shown smalltalk code receiver type call site remains constant time 
locality type usage exploited caching looked method address call site 
lookup result cached line call site case hit separate lookup cache accessed technique called inline caching 
shows empty inline cache calling method simply contains call system lookup routine 
time call executed lookup routine finds target method 
branching target lookup routine changes call instruction point target method just 
subsequent executions send jump directly target method completely avoiding lookup 
course type receiver changed prologue called method verify receiver type correct call lookup code type test fails 
inline caches faster line lookup caches reasons 
call site separate cache message name need tested verify cache hit test done misses system lookup routine 
second inline caching doesn need execute load instructions fetch cache entry function implicitly performed call instruction 
explicit indexing table omit xor shift instructions hash function 
overhead remains check receiver type usually accomplished instructions loads compare branch typical smalltalk system load comparison constant self system 
inline caching effective reducing lookup overhead hit rates high hit cost low 
example soar smalltalk implementation risc processor slower inline caching 
compiled implementations smalltalk know incorporate inline caches self system 
similar technique weiss apl interpreter specialized routines operations generic arithmetic speculatively previous specialization call 
course inline caching resembles hardware mechanisms branch target buffers direct mapped processor caches 
inline cache system lookup routine call lookup routine 
empty inline cache calling method check receiver type method code system lookup routine 
inline cache send call method calling method method body method prologue target method handling polymorphic sends inline caches effective receiver type call target remains relatively constant call site 
inline caching works majority sends speed polymorphic call site equally receiver types call target switches back forth different methods 
example suppose method sending display message elements list list element rectangle element list circle control passes call inline cache display method rectangles 
receiver test method prologue detects type mismatch calls lookup routine inline cache circle display method unfortunately list contains circles rectangles particular order inline cache receiver type changes 
case inline caching may slower lookup cache time higher 
particular code includes changing call instruction requires invalidating parts instruction cache modern processors 
handler includes considerable additional overhead 
example self system links inline caches calling particular method list updated method relocated different address discarded 
performance impact inline cache misses severe highly efficient systems longer ignored 
example measurements self system showed richards benchmark spent time handling inline cache misses :10.1.1.56.2990
term polymorphic call sites polymorphism 
consequently monomorphic call sites polymorphism potentially polymorphic 
call display method check receiver type code display rectangle calling method rectangle display method method body method prologue call display method check receiver type code display rectangle check receiver type code display circle calling method rectangle display method circle display method 
inline cache informal examination polymorphic call sites self system showed cases degree polymorphism small typically 
degree polymorphism sends distribution sends monomorphic receiver type polymorphic receiver types receiver types 
shows arity distribution non empty inline caches degree polymorphism exhibited call sites 
distribution taken prototype self user interface minute 
overwhelming majority call sites receiver type normal inline caching works 
quite call sites different receiver types frequent case boolean messages true false different types self 
call sites receiver types 
call sites receiver types invoked blocks section explains behavior 
data suggests performance polymorphic calls improved flexible form caching non monomorphic sends receiver types 
polymorphic inline caches order reduce inline cache overhead designed implemented polymorphic inline caches new technique extends inline caching handle polymorphic call sites efficiently 
merely caching lookup result polymorphic inline cache pic caches lookup results polymorphic call site specially generated stub routine 
example sending display message elements list system pics initially works normal inline caching send inline cache bound rectangle display method 
see table appendix detailed data 
williams obtained similar results analyzing smalltalk traces 
reports monomorphic call sites sites sites 

size distribution degree polymorphism inline caches number receiver types call display method check receiver type code display rectangle calling method rectangle display method method body method prologue circle encountered simply switching call target circle display method handler constructs short stub routine call stub routine stub type case checks receiver rectangle circle branches corresponding method 
stub branch directly method body skipping type test method prologue receiver type verified 
methods need type test prologue called monomorphic call sites standard inline cache 
pic caches rectangle circle case misses occur list contains just rectangles circles 
sends fast involving comparisons find target 
cache misses receiver rectangle circle stub routine simply extended handle new case 
eventually stub contain cases seen practice cache misses lookups 
variations scheme described works cases reduces cost polymorphic send machine cycles 
section discusses remaining problems possible solutions 
coping sends 
send sites may send message large number types 
example method send message object system 
building large pic send wastes time space 
inline cache handler extend pic certain number type cases mark call site adopt fallback strategy possibly just traditional monomorphic inline cache mechanism 
self system pics marked exceed certain size currently cases 
pic misses grow include new type 
cases picked random replaced new case 
earlier version system move front strategy inserted new case front shifting cases back dropping case 
strategy abandoned cost high entries needed changed just ratio high occasionally types changed circular fashion 
murphy law holds real programs exhibited behavior 
improving linear search 
dynamic usage frequency type available pics reordered periodically order move frequently occurring types pic reducing average number type tests executed 
linear search efficient sophisticated algorithms binary check receiver type code display rectangle check receiver type code display circle rectangle display method circle display method type rectangle jump method type circle jump method call lookup pic stub call display pic calling method 
polymorphic inline cache search form hashing cases types 
number types small average see optimization probably worth effort pic linear search probably faster methods situations 
improving space efficiency 
polymorphic inline caches larger normal inline caches stub routine associated polymorphic call site 
space tight call sites identical message names share common pic reduce space overhead 
scenario pics act fast message specific lookup caches 
average cost polymorphic send higher call site specific pics number types pic increase due loss locality shared pic contain receiver types particular message name call specific pic contains types occur call site 
implementation results designed implemented polymorphic inline caches self system measured effectiveness 
measurements section done lightly loaded sun mb memory base system comparison self system september 
base system uses inline caching send takes instructions cycles method specific code reached 
inline cache takes microseconds cycles 
time reduced optimizations recoding critical parts assembly 
estimate optimizations reduce overhead factor 
measurements may direct performance advantage pics factor 
hand measurements commercial smalltalk implementation parcplace smalltalk system version indicate takes microseconds handle current implementation unreasonably slow 
monomorphic sends experimental system inline caching scheme base system 
polymorphic sends stub constructed tests receiver type branches corresponding method 
stub fixed overhead cycles load receiver type jump target method type test takes cycles 
pics implemented described section 
variations mentioned previous section implemented call site treated receiver types calls occur benchmarks 
table summarizes lookup timings 
cost polymorphic send base system depends inline cache ratio polymorphic call site 
example receiver type changes third time send executed cost approximately cycles 
system pics average polymorphic lookup cost function number cases cases equally average dispatch involve type tests cost cycles 
measurements done september current self system 
actual time taken may vary somewhat depending hardware influences processor cache misses algorithmic details scope discussion 
microsecond estimate obtained profiling execution benchmark gprof profiler 
base self pics smalltalk inline cache hit unknown inline cache approx 
approx 
polymorphic send table 
lookup timings cycles sparcstation order evaluate effectiveness polymorphic inline caches measured suite self programs 
programs exception considered fairly typical object oriented programs cover variety programming styles 
detailed data benchmarks 
parser 
recursive descent parser earlier version self syntax lines 

program generating self stub routines description primitives lines 
ui 
self user interface prototype lines running short interactive session 
sun measurements special graphics hardware runtime dominated graphics primitives polygon filling full screen bitmap copies 
tests expensive graphics primitives turned ops remaining primitives account total execution time 
pathcache 
part self system computes names global objects stores compressed form lines 
time spent loop iterates collection containing different kinds objects 
richards 
operating system simulation benchmark lines 
benchmark schedules execution different kinds tasks 
contains frequently executed polymorphic send scheduler sends message task 

artificial benchmark lines designed show highest possible speedup pics 
consists loop containing polymorphic send degree send executed times 
normal inline caches rate benchmark consecutive sends receiver type 
short artificial benchmark include computing averages entire set benchmarks 
execution time obtain accurate measurements benchmarks run times row average cpu time computed 
process repeated times smallest average chosen assumption longer execution times caused external influences unix processes 
garbage collection performed measurement order reduce inaccuracies 
shows execution time saved system pics see table appendix details 

execution time saved polymorphic inline caches richards parser primmaker ui execution time savings vary ui substantial richards parser spectacular median saving benchmarks 
speedup observed individual benchmarks closely corresponds time required handle inline cache misses base system 
example base system spends execution time handler execution time eliminated pics 
close correlation indicates pics eliminate virtually overhead inline cache misses 
table shows ratios benchmarks base system 
self implementation optimizing compiler quite different smalltalk systems ratios agree observed previous studies smalltalk systems observed ratios order 
ratios directly correlate speedups observed introducing pics benchmarks different call frequencies differing factor 
expect inline cache ratio correlated degree polymorphism exhibited program measured fraction messages sent polymorphic call sites inline caches encounter different receiver types 
example assume program sending messages polymorphic call sites higher inline cache ratio program sends messages polymorphic call sites 
interestingly case benchmark programs 
example messages sent pathcache polymorphic call sites vs parser pathcache inline cache ratio slightly lower parser ratio 
apparently receiver type dominates polymorphic call sites pathcache receiver type rarely changes receiver type frequently changes parser inline caches 
ordering pic type tests frequency occurrence suggested section win programs pathcache 
benchmark ratio richards parser ui pathcache table 
inline cache ratios benchmark programs 
polymorphism vs ratio 
inline cache ratio messages sent polymorphic call sites richards parser primmaker ui pathcache space overhead space overhead pics low typically compiled code see table 
main reason low overhead call sites need pic monomorphic 
element pic stub occupies bytes space overhead modest 
summary traditional inline caching works sends 
truly polymorphic call sites call sites receiver type changes frequently cause significant inline cache overheads consuming total execution time programs measured 
extended traditional inline caching polymorphic inline caches pics cache multiple lookup targets 
pics effective removing inline cache overhead speeding execution benchmark programs median 
discussed pics way speed message sends primary purpose self system provide type information optimizing compiler 
performance improvements gained type information far outweigh speedups observed chapter 
discuss aspect pics detail chapter 
benchmark overhead richards parser primmaker pathcache ui available table 
space overhead pics 
non inlining compiler long compile pauses caused previous self compilers distracting users threatened compromise goal increasing programmer productivity 
example starting prototype user interface took seconds seconds compile time 
self compiler fast standard compiler slow 
simple code generation improve responsiveness system decided implement non inlining compiler nic 
task compile methods quickly possible attempting optimizations 
consequently code generation strategy extremely simple 
compiler translates byte codes source method directly machine code building intermediate representation 
byte codes translated follows source literals foo loaded register 
literal block primitive called create block primitive failure blocks handled specially see 
primitive sends translated calls corresponding assembly primitive 
sends translated dynamically dispatched calls generating inline cache send accesses local variable method case corresponding stack location accessed 
register allocation equally simple expression stack entries expressions evaluated consumed send expressions need evaluated allocated registers bit mask keeps track available registers 
locals stack allocated accessed nested blocks incoming arguments passed registers flushed stack reason 
areas little bit analysis source code example see method contains blocks speed generated code 
non inlining compiler consists lines 
compiler performs optimizations simple implement known significant benefit 
creation primitive failure blocks delayed needed 
example integer addition primitive takes failure block argument 
block invoked primitive fails example overflow 
primitive calls usually succeed failure block usually isn needed 
compiler delays creation failure blocks primitive fails returns special error value 
optimization speeds programs greatly reduces allocation rate programs 
second optimization accesses slots receiver inlined replacing send load instruction 
resulting speedup modest usually percent optimization reduces size compiled code replaces word inline caches word load instructions 
addition nic uses customization profit inlining receiver instance variable accesses optimization advantage customization 
customization assumption code customized parts self system especially lookup system nic easier integrate system way 
removing customization nic advantageous reduce code duplication 
times sparcstation workstation rated specint noted 
optimization added lars bak 
nic perform optimizations commonly performed similar compilers languages 
notably optimize integer arithmetic way special case inline control structures iftrue 
performance impact decisions examined section 
compilation speed nic uses roughly instructions generate sparc instruction 
significantly slower simple compiler described deutsch schiffman uses instructions generated instruction 
reasons difference 
self byte codes higher level byte codes smalltalk virtual machine 
example self uses send byte code general sends accesses instance variables accesses local variables primitive calls syntactically equivalent self 
contrast smalltalk machine oriented byte code format fact byte codes directly interpreted microcode early smalltalk implementations 
example smalltalk special byte codes instance variable accesses specifying slot number local accesses frequent primitives arithmetic control transfer 
smalltalk byte code compiler equivalent self parser performs compilation replacing simple control structures jump byte codes replacing arithmetic operations special arithmetic byte codes 
result deutsch schiffman compiler perform translate smalltalk byte codes machine code 
contrast information isn explicit self byte codes designed interpretation fast compilation mind 
shows coarse profile nic compilation obtained profiling compilations gprof profiler 
interestingly actual compilation takes twice long allocation compiled method 
surprisingly handling message sends takes portion actual compilation compilation time total 
compiler spends time total tasks handled parser smalltalk system searching slots determining send local slot access computing byte codes statement boundaries 
allocator fairly efficient vast majority allocation requests filled free lists kept frequently requested sizes 
time listed profile includes time flushing unused compiled methods necessary room new method copying compiled code auxiliary data dependencies debugging information newly allocated space relocating object code dependency pointers 
total compilation time allocating initializing compiled method total time nic compiling method compiling send generating real send generating local slot access generating instance variable access generating method prologue epilogue compiling methods slot access assignment determining statement boundaries source method determining send accesses local slot searching slot receiver object 
profile nic compilation source overhead compiler uses data abstraction liberally order simplify implementation 
example uses code generator object code generation details front porting compiler simplified 
similarly code generator uses assembler object generate instructions manipulating instruction words 
assembler quite efficient compiler calls methods load base reg offset destination passing string load base offset dest organization prevents optimizations precomputed patterns certain frequent instructions instruction sequences 
hand simplifies system compilers share assembler 
way characterize compile time measure compile time byte code source code unit 
shows compile time approximately linear source method length time methods equal size vary somewhat different kinds byte codes take different amounts time compile 
example loading constant faster compiling send directly generates machine instruction generates dozen instructions check send accesses local variable instance variable receiver 
average compiler uses ms byte code start overhead ms linear regression correlation coefficient 
compilations short mean ms 
execution speed unoptimized code quite slow table shows performance relative optimized self programs 
large programs unoptimized code runs times slower code generated optimizing self compiler smaller programs tight loops slow 
extreme case bubblesort slows orders magnitude 
earlier version nic spent time assembler compiler generating inefficient code bit field operations compose instructions 
current overhead hard estimate assembler functions inline functions don show gprof profile 
time taken functions say code generation takes total compilation time 
time measured elapsed time unloaded system system cpu timer resolution coarse measure short compile times 
see table page details benchmarks 

nic compile time function source method size time spent unoptimized programs 
section analyzes questions cecilint large object oriented program richards smaller polymorphic program examples discusses main reasons register windows lookups instruction cache misses primitive calls garbage collection 
overview table shows rough profile benchmarks execution time 
actual compiled code accounts half total execution time 
time spent handling inline cache misses see section goes primitives integer addition block allocation garbage collection 
benchmark size benchmark slowdown factor tiny bubblesort small deltablue richards primmaker large cecilcomp cecilint mango typeinf ui ui geometric mean table 
performance unoptimized code relative optimized code 
histogram nic compile time table shows hardware oriented view programs includes data entire program execution compiled self code 
register windows instruction cache misses contribute programs low performance 
factors discussed sections 
lookup cache misses unoptimized code call sites receiver type changes frequently large number receiver types 
example iftrue method true object sends value argument iftrue value iftrue sent places passing different argument blocks 
value send different receiver type iftrue program 
pics cache different receiver types current system send misses pic causing costly lookup handler execution 
misses account third richard execution time cecil 
optimized code iftrue inlined allowing compiler inline value send argument usually block literal 
optimized code value send incurs overhead 
blocks primitive calls garbage collection nic creates blocks closures optimized self code optimize control structures involve blocks matter 
example blocks created iteration loop 
block creation accounts execution time benchmarks 
block objects created partial garbage collections necessary 
fortunately fast objects blocks survive gc overhead programs 
unoptimized code calls primitives simple operations integer arithmetic comparisons 
primitives including block allocation gc total execution time 
register windows sparc architecture defines set overlapping register windows allows procedure save caller state switching new set registers 
long call depth exceed number available self block different type value slot block method different 
contrast blocks type class smalltalk 
category richards cecilint discussed section 
compiled self code primitives gc lookup handler table 
software profile unoptimized code category richards cecilint discussed section 
register windows instruction cache misses table 
hardware profile unoptimized code register sets save performed cycle memory accesses 
free register set available save instruction executed register window overflow trap occurs trap handler transparently frees set saving contents memory 
similarly window underflow trap reload flushed register set memory needed 
unfortunately unoptimized self code high call density high call depth example loop implemented messages blocks call depth 
unoptimized code incurs frequent window overflow underflow traps spends significant amount time handling traps 
section analyze overhead detail suffices say register window overhead unoptimized code high total execution time current sparc implementations 
instruction cache misses code generated nic consumes lot space send takes bit words instructions loading parameter registers call inline cache 
additionally method prologue words prologue tests receiver map increments tests invocation counter recompilation establishes stack frame tests stack overflow 
result unoptimized code relatively high cache overhead richards spends time waiting instructions fetched memory cecilint spends time cache misses 
nic vs deutsch schiffman smalltalk nic similar deutsch schiffman smalltalk compiler respects 
parcplace smalltalk version runs richards benchmark seconds times faster unoptimized self version 
smalltalk system faster 
performance disparity caused differences language differences implementation 
differences explain performance discrepancy particular reasons come mind language differences implementation difference smalltalk small set performance critical methods 
meaning certain messages fixed source code messages ignored 
particular smalltalk methods implementing control structures plus loop control structures integer arithmetic 
doing system special case greatly speed frequent operations implementing general inlining optimizations 
contrast nic special case operation performs message sends operations 
self programs execute message sends access instance variables performed messages smalltalk methods directly access instance variables receiver sending messages 
nic inlines accesses instance variables receiver shouldn extra sends certainly account order magnitude performance difference 
code generated nic inefficient code generated parcplace smalltalk compiler 
example smalltalk compiler uses type prediction arithmetic messages generates inline code integer case nic doesn type predict inlines primitives 
nic uses sparc register windows parcplace smalltalk 
special casing integer arithmetic differences local code quality account near factor performance difference 
smalltalk hardwired control structures cursory examination language implementation differences appears smalltalk hardwiring control structures drastic performance impact 
verify suspicion hardwiring responsible performance difference disabled optimizations parcplace smalltalk system version control structures generated real message sends 
furthermore changed definitions resemble self equivalents 
step necessary methods recursive standard smalltalk system execution unnecessarily inefficient 
implemented new control structure recognized open coded changed compiler 
example implementation follows repeat long receiver block yields true self value iffalse nil 
test condition exit false value 
changed smalltalk compiler transforms message simple backward jump method just restart primitive self uses implement iteration 
changes implementation important smalltalk control structures similar self counterparts generally optimized 
example implemented self directly smalltalk 
interested rough estimation performance impact non inlined control structures bother reimplement smalltalk control structures exact equivalent self counterparts 
changes profound effect smalltalk performance original system executed richards seconds changed system needed seconds 
programs slowed similar amounts table 
appears hardwiring simple control structures speeds smalltalk roughly factor 
somewhat surprisingly single change closes performance gap unoptimized self code smalltalk 
nic inlined control structures surprisingly pronounced result experiment suggests similar optimizations nic dramatically speed code 
validate assumption configured optimizing self compiler generate code resembling hypothetical nic inlined control structures 
intention inline control structures involve block creations sends 
example expression richards deltablue self benchmarks translated smalltalk 
included subset commonly smalltalk macro benchmarks comparison 
benchmark slowdown factor richards 
deltablue 
compiler decompiler table 
performance parcplace smalltalk inlined control structures translated machine code resembling sequence load true load false cmp result beq cmp result beq 
experiments various parts optimizing compiler turned quality generated code resembled nic code possible 
example local variables register allocated back code optimization performed 
table compares execution times standard nic compiler nic richards benchmark execution time versions smalltalk system included comparison 
clearly inlining control structures tremendously improves performance compiled self code compiled code efficient call overhead fewer block creations lookup misses caused non inlined control structures eliminated 
category time seconds nic nic smalltalk smalltalk inlined control structures 
compiled self code unknown unknown primitives gc unknown unknown lookup handler unknown unknown unknown unknown total table 
performance richards various system configurations benchmark inlining inlining better code generation deltablue parser cecilint richards bubblesort puzzle geometric mean table 
performance improved nic versions relative original nic table shows inlining speeds programs average factor 
better compiler additionally performs optimizations register allocation copy propagation achieves speedup factor 
simulated compiler enabling back optimizations optimizing self compiler keeping restrictions 
additional speedup gained better code quality modest richards performance real implementation compiler inlined just fairly similar numbers code quality differed somewhat experimental setup 
parcplace smalltalk benefit inlined control structures factor see table self table 
differences contribute discrepancy 
example sets benchmarks different base smalltalk system optimizes control structures just 
furthermore systems differ implementation details example nic uses customization smalltalk implementation doesn 
smalltalk system intended run inlining central control structures properly tuned non inlining case 
details smalltalk implementation available determine exact source discrepancy known closely non optimizing self compiler approximate non optimizing smalltalk compiler 
predicting performance self interpreter relatively slow speed unoptimized code relatively large space overhead storing compiled code couldn interpreter provide similar performance virtually space cost 
self interpreter faces big challenge interpret message sends efficiently self involves message sends 
problems hard inline caching sends expensive 
straightforward interpreter inline caching lookup cache 
unfortunately lookups extremely frequent example richards sends messages counting accesses local variables 
want interpreter run nic speed allow time go lookups lookup take instructions assuming cpi sparcstation see chapter 
certainly lookup cache probe clear cover amortized cost misses 
cost real message lookup fairly high current system probably order 
relatively small ratio result amortized cost send 
byte code encoding 
discussed send byte code local variable accesses expressed byte code 
efficient interpretation probably necessary redesign byte code format separate trivial sends real message sends cache intermediate representation discussed 
redesigned byte code format carefully optimized lookup code large lookup cache probably possible easy achieve nic speed interpreter 
hardwiring important control structures interpreter possibly surpass current nic 
unclear big space savings redesigned method representation space lookup cache fairly big misses expensive 
appears interpreter significantly space compiled code 
alternate interpreter implementation approach promises better performance expense higher space usage 
interpreter dynamically translate source methods interpreted methods interpret 
main advantage approach format tuned efficient interpretation 
self interpreter implemented author reports execution times times slower oberon code small integer benchmarks 
interpreter especially optimized 
example inline caches significantly speed sends 
time space overhead kept low cached expanded representation source methods continue space efficient representation 
side approach introduces problems overheads cached code translation cost source method cost maintaining cache allocation deallocation compaction cost keeping track source changes source method changed flushed 
depending speedup simple interpreter additional complexity space cost may justified 
alternatively interpreter just add inline caching straightforward interpreter adding pointer send byte code cache method invoked send method cache receiver type 
similar organizations smalltalk interpreters 
byte codes sends approach require roughly word byte code 
nic interactive performance goals adding non optimizing compiler self system reduce compile pauses improve responsiveness system 
current nic achieve goal 
discussed previous sections variety trade offs compilation speed simplicity execution time 
better double compile speed double execution speed 
shows timeline minute interaction self user interface 
interaction sequence designed stress initial responsiveness interactions performed time cold code cache new code generated fairly 
shows similar interaction sequence system compiled code 
obtained timelines sampling current activity system compiling running self code times second writing resulting trace file 
timelines observations cold start timeline compilation time outweighs execution time nic 
example seconds system spends considerable fraction time nic generating hundreds new compiled methods 
timelines unoptimized self code accounts small fraction total execution time runs long periods time 
naive code generated nic quite surprising little impact code quality 
observations suggest faster compiler interpreter improve responsiveness system compromised execution speed 
example appears interpreter beneficial executed programs twice slowly 
summary non inlining compiler nic stage compilation process 
compiles code quickly typically ms compilation generates relatively slow code significantly slower generated non optimizing parcplace smalltalk compiler 
experiment parcplace smalltalk compiler shows hardwiring common control structures smalltalk execution slows order magnitude 
experiment showed sophisticated compiler inlined hardwired control structures achieve significant speedup factor self expense added implementation variant cache receiver type caller callee adding second word send bytecode 
organization increase inline cache hit ratio expense roughly doubling space overhead 
nic unoptimized self code 
timeline interactive session cold start scenario complexity loss language simplicity control structures hardwired 
alternatively self interpreter possibly achieve current nic speed time reducing memory usage 
contrary intuition relatively slow speed unoptimized self code problem practice system doesn spend time executing unoptimized code 
timelines interactive self sessions show improvements compilation speed benefit system responsiveness improvements speed compiled code especially cold start situations hundreds methods compiled quickly 
nic unoptimized self code 
timeline interactive session warm start scenario 
type feedback adaptive recompilation object oriented programs harder optimize programs written languages fortran 
objectoriented programming style encourages code factoring differential programming 
result procedures smaller procedure calls frequent 
furthermore hard optimize procedure calls dynamic dispatch procedure invoked call known runtime depends dynamic type receiver 
compiler usually apply standard optimizations inline substitution interprocedural analysis calls 
consider example written class point virtual float get get coordinate virtual float get ditto virtual point distance point compute distance receiver compiler encounters expression get declared type point optimize call know exact runtime type 
example subclasses point cartesian points polar points class point float virtual float get return methods omitted class point float rho theta virtual float get return rho cos theta methods omitted refer instance runtime compiler type information precise optimize call compiler knows type set operations invoked signatures concrete type object size format implementation operations 
important realize problem optimizing dynamically dispatched calls result dynamic typing seen example occurs statically typed language 
result encapsulation polymorphism hiding implementation details objects object clients providing multiple implementations type 
compiler statically typed language better generating dynamically dispatched call message send general case access program source 
static typing enables compiler check get message legal implementation guaranteed exist receiver detect implementation invoked 
problem eliminating dynamically dispatched calls little consequence study dynamically typed language self statically typed language 
pure object oriented languages exacerbate problem operation involves dynamically dispatched message send 
example simple operations instance variable access integer addition array access conceptually involve message sends self 
result pure object oriented language self offers ideal test case optimization techniques tackling problem frequent dynamically dispatched calls 
course beating strawman statement strictly true see section details 
developed simple optimization technique type feedback feeds back type information runtime system compiler 
feedback compiler inline dynamically dispatched call 
implemented type feedback self system combined adaptive recompilation compiled code initially created unoptimized form save compile time time critical code recompiled optimized type feedback optimization techniques 
implemented type feedback self technique language independent applied statically typed non pure languages 
rest chapter describes basic ideas going self specific detail 
chapter discuss implementation details new self system ideas described 
type feedback main implementation problems object oriented languages languages supporting form late binding apl arise paucity information statically available compile time 
exact meaning operations determined statically dependent dynamic runtime information 
hard optimize late bound operations statically program text 
approaches solving problem 
dynamic compilation called lazy compilation moves compilation runtime additional information available better optimize operations 
approach taken self previous systems van dyke apl compiler 
compared systems self takes laziness step trying best possible job right away compiled code needed 
system generates unoptimized code optimizes clear code 
obvious savings compile time approach possible generate better code eager systems compiler information available 
possible desired move compilation runtime second conventional approach moving additional runtime information compiler 
typically information collected separate run written file compiler re invoked additional information generate final optimized program 
type feedback works approaches 
concentrate approach self second approach briefly outlined section 
key idea type feedback extract type information runtime system feed back compiler 
specifically instrumented version program records program type profile list receiver types optionally frequencies single call site program 
obtain type profile standard method dispatch mechanism extended way record desired information keeping table receiver types call site 
having obtained program type profile information fed back compiler optimize dynamically dispatched calls desired predicting receiver types inline call types 
example type feedback information predict receivers get call expression get compiled class inline case don inline polar point case method big branch covers receiver types get dynamically dispatched call see section related page details 
receivers code sequence execute significantly faster reduces original virtual function call comparison simple load instruction 
type feedback substantially altered understanding problem optimizing object oriented languages 
traditional view implementation level type information scarce program text usually contain information tell 
new viewpoint comes realizing implementation type information initially available compiler scarce information running system abundant program running 
self inline cache pic contains exact list receiver types encountered send 
words additional instrumentation needed type feedback system uses polymorphic inline caches 
program type profile readily available compiler just needs inspect program inline caches know receiver types encountered far call site 
example inline cache get send contain types 
compiler access type information inline send receiver types known 
type information provided type feedback substantially simplify compilation 
lacking type feedback self compiler performed extensive type analysis attempt preserve propagate scarce type information available compiler 
type feedback main problem longer inline dynamically dispatched call choose calls inline 
relieved arduous burden performing type analysis optimizing compilers object oriented languages simpler write sacrificing code quality 
new self compiler type feedback half size previous compiler inline calls produce faster code 
adaptive recompilation self system uses adaptive recompilation take advantage type feedback determine parts application optimized 
shows overview compilation process self system source method invoked time compiled simple completely non optimizing compiler chapter order generate code quickly 
having fast compiler essential reduce compile pauses interactive system dynamic compilation see chapter 
method executed recompiled optimized type feedback 
optimized method take advantage additional type information adapt changes program type profile 
self system discover opportunities recompilation programmer intervention 
particular system decide recompile long wait type information accumulate recompile compiled code benefit additional type information sends inline sends leave dynamically dispatched calls 
sections discuss questions 
solutions employ simple heuristics shall see 
call sites course 
source methods unoptimized code method executed needed debugging invoked optimized code 
self compilation process recompile self system uses counters detect recompilation candidates 
unoptimized method counter 
method prologue code method increments counter compares limit 
counter exceeds limit recompilation system invoked decide method recompiled 
method overflowing counter recompiled counter reset zero 
done method eventually reach invocation limit recompiled execute times second optimization hardly bring benefits 
invocation counters decay exponentially 
decay rate half life time time counter loses half value 
decay process approximated periodically dividing counters constant example process adjusting counters wakes seconds half life time seconds constant factor 
decay process converts counters invocation counts invocation rates invocation limit decay factor method execute times decay interval recompiled 
originally counters envisioned step better solution 
course experiments discovered trigger mechanism important recompilation results selection mechanism 
simple counter approach worked extensively investigate mechanisms 
interesting questions relating invocation counter decay exponential decay right model 
ideally system recompile methods optimization cost smaller benefits accrue invocations optimized method 
course system know method executed rate measure ignores past method executes minimum execution rate trigger recompilation executed times 
invocation limit constant depend particular method 
counters really trying measure execution time wasted running unoptimized code 
method benefit optimization count faster lower limit method benefit optimization 
course may hard estimate performance impact optimization particular method 
half life times adapted executing faster slower machine 
suppose original half life parameter seconds system executes new machine twice fast 
half life parameter changed 
view faster machine system real time runs half fast twice operations completed second reduce half life seconds 
argue invocation rate limit absolute method executes times second worth optimizing 
assume method count just decaying process wakes 
decayed value execute times reach count decay process wakes 
method eventually needs reach recompiled execute times decay interval 
consider alternatives 
placed counters edges call graph nodes providing information recompilation system 
unfortunately space cost edge counts prohibitive self system call graph unoptimized code extremely large 
second approach pc sampling discover time consuming methods similar profilers 
unoptimized self methods short numerous control structures implemented message sending coarse timer profile information helpful making recompilation decisions 
statement quite true system theoretically minimize total execution time ignores fact optimization type feedback information accumulated 
ignores interactive behavior clustering compilations 
similarly half life time measured real time cpu time machine specific unit number instructions executed 
intuitively real time wrong user think pauses coffee pauses influence recompilation 
cpu time problems example time spent vm garbage collection compilation graphics primitives half life time effectively shortened compiled methods get time execute increase invocation counters 
hand effect may desirable time spent compiled self code optimizing code may increase performance course optimizations reduce vm overhead reducing number block closures created reducing allocation costs garbage collections 
questions raise interesting issues addressed simple scheme works time limitations 
believe interesting area research investigating may ultimately lead better recompilation decisions 
recompile counter overflows recompilation system invoked decide method recompile 
simple strategy recompile method counter overflowed obviously invoked 
strategy 
example suppose method overflowing counter just returns constant 
optimizing method gain method inlined caller 
general find candidate recompilation need walk call chain inspect callers method triggering recompilation 
overview recompilation process shows overview recompilation process 
starting method overflowed counter recompilation system walks stack find candidate recompilation section explain means 
compiler invoked method old version discarded 
execution continues normally 
optimizing compilation compiler marks restart point point execution resumed computes contents live registers point 
computation successful method replaces corresponding unoptimized methods stack possibly replacing unoptimized activation records single optimized activation record 
newly optimized method isn top stack recompilation continues stack grows downwards method overflows invocation counter triggers recompilation recompilation system walks stack determine method recompile 
calls compiler generate new code 
recompilation system replaces old unoptimized stack frames activation newly compiled method 
example replaces unoptimized frames optimized frame 
recompilation system continues remaining stack optimized 
performs optimization replaces bottom frames 

overview recompilation process newly optimized method callee 
way system optimizes entire call chain top current execution point 
usually recompiled call chain compiled methods deep 
unoptimized methods replaced stack left finish current activations subsequent invocations new optimized method 
main effect failing replace unoptimized methods additional recompilations may occur unoptimized code continues execute 
example optimized method contains loop placed stack immediately reoptimization system may try replace just loop body optimized code 
selecting method recompiled self system selects method recompiled examining metrics 
compiled method values defined size size instructions 
count number times invoked 
sends number calls directly version records times recompiled 
search outlined follows 
trip method counter current candidate recompilation 

start trip 

closure arguments choose closure lexically enclosing method meets conditions described 
rule eliminates closures inlining closure closure home 
inlining succeeds closure usually optimized away completely 

choose caller meets conditions 
rule walk stack encountering method large appear cause message sends executed 

repeat steps doesn change anymore 
recompilation system considers new steps accept new meets conditions count version 
clause ensures method executed times consider type information representative 
second clause prevents endless recompilation method 
sends size unoptimized 
clause accepts methods sending messages accept methods combined callee inlining 
compiler describe register contents source level terms track effects optimizations order keep compiler simple 
detect situation signal recompilation system 
recall self implements control structures blocks closures message sends body loop method invoked message send optimized send 
note count sends incomplete data system count invocations optimized methods edge counts 
values parameters current system uses instructions bytes 
assumptions underlying rules frequently executed methods worth optimizing inlining small methods eliminating closures lead faster execution 
rules simple appear finding hot spots applications shown chapter 
exploring accurate ways estimating potential savings estimated cost recompilation remains interesting area 
course system making decisions dynamically run time additional cost making accurate estimates considered 
rules recompilation system finding recompilation candidate aspects mirror rules compiler choosing inlining opportunities 
example rule skipping tiny methods equivalent rule compiler causes tiny methods inlined 
ideally recompilation system consult compiler decision walk upwards stack caller sure compiler inline send 
system probably unrealistic inlining decisions compiler needs context size caller combined inlining candidates see section page 
recompilation decisions system expensive approach rejected 
recompilation system compiler share common structure self system essentially criteria walking stack subset compiler criteria inlining 
recompilation system checks see recompilation effective improved code 
previous new compiled methods exactly non inlined calls recompilation really gain new method marked won considered recompilations 
type feedback information compiling particular message send locating corresponding type feedback information relatively simple 
determining trust information difficult method type information may merged types callers 
example caller may method polar point arguments caller uses cartesian points 
case type feedback information sends argument contain types 
information inlining caller compiler specialize send kinds points polar points occur wasting code space compile time 
compiled method callers type information ignored compiler inline cache contains types 
larger code size compile time increase predicted receiver type sets large smaller effectiveness type feedback declines receiver types unknown type feedback information considered polluted 
implementation currently uses attempt find balance extremes 
fortunately inline caches contain just type information trusted regardless number callers 
course precautions predicting receiver types past receiver types educated guess 
similar guesses optimizing compilers base decisions execution profiles taken previous runs 
program type profile stable time profile usually new types created runtime 
application run new types appear exceptional case error occurs programmer changes application 
adding type feedback conventional system type feedback require exotic implementation techniques self dynamic compilation adaptive recompilation 
techniques harder optimize programs dynamic compilation interactive system places high demands compilation speed space efficiency 
reasons compiled methods linked inline caches call relatively simple determine number callers 
self implementation type feedback cope incomplete information partial type profiles inexact invocation counts refrain performing optimizations achieve compilation speed 
believe type feedback probably easier add conventional batch style compilation system 
system optimization proceed phases 
executable instrumented record receiver types example gprof profiler 
standard gprof profiler collects information needed type feedback data caller specific call site specific separate calls foo come function 
application run test inputs representative expected inputs production 
collected type profiling information fed back compiler produce final optimized code 
mentioned static compilation advantage compiler complete information complete call graph type profile optimization starts complete program execution 
contrast dynamic recompilation system decisions incomplete information 
example afford keep complete call graph recompilations may necessary program initialization phases type profile representative 
hand dynamic recompilation system significant advantage dynamically adapt changes program behavior 
applicability languages obviously type feedback object oriented languages smalltalk languages generic operators optimized type feedback information apl lisp 
effective 
give definitive answer require measurements actual implementations available 
discuss applicability type feedback smalltalk examples 
type feedback directly applicable smalltalk expect resulting speedups similar achieved self 
despite language differences prototype vs class inheritance languages similar execution characteristics high frequency message sends intensive heap allocation closures implement user defined control structures similar sources inefficiency 
execution behavior language philosophy away self believe benefit type feedback 
measurements large programs shown calls times frequent programs programs average size virtual function instructions times smaller average function 
second programs measured see section slowed factors virtual functions demonstrating current compilers optimize calls 
third expect programmers virtual functions familiar object oriented programming styles example versions interviews framework virtual functions frequently previous versions 
instrumented program source program optimized program 
type feedback statically compiled system compiler compiler type feedback data give concrete example doc document editor measured performs virtual call instructions virtual call uses instructions usually incurs load stalls stall indirect function call estimate program spends roughly time dispatching virtual functions 
type feedback eliminate large fraction calls indirect benefits inlining similar measured self total savings times higher call overhead see page substantial speedups appear possible 
type feedback dynamic number receiver types call site close receiver types dominate 
large fraction call sites property holds object oriented programming languages smalltalk self sather eiffel reason inline caching works languages implementation dynamic dispatch 
expect type feedback languages higher frequency dynamically dispatched calls beneficial type feedback 
type feedback applies languages type dependent generic operators apl lisp 
languages operations expensive exact implementation types arguments unknown cheap types known 
example lisp systems type feedback predict floating point data common case arithmetic operations floating point intensive program statically predicting integers handling types call runtime routine current lisp systems usually 
related static type prediction previous systems static type prediction inline operations depend runtime type operands 
example lisp systems usually inline integer case generic arithmetic handle type combinations call routine runtime system 
deutsch schiffman smalltalk compiler objectoriented system predict integer receivers common message names 
systems predicted types dynamically system feedback 
customization systems mechanisms similar customization limited form runtime type feedback 
example mitchell system specialized arithmetic operations runtime types operands 
similarly apl compilers created specialized code certain expressions 
systems hp apl compiler flexible 
system compiled code statement statement basis 
addition performing apl specific optimizations compiled code specialized specific operand types number dimensions size dimension element type 
called hard code execute efficiently general versions cost apl operator varies wildly depending actual argument types 
code invoked incompatible types new version restrictive assumptions generated called soft code 
similarly chambers ungar introduced customization self compiled methods specialized type receiver source method different translations different receiver types 
type feedback surpasses customization type feedback provide receiver type information sends customization provides type self customized method 
type feedback optimize polymorphic calls customization 
type feedback optional compiler may send 
customization implemented self mandatory code customized receiver type 
systems mentioned runtime type information code initially compiled 
compared system described section compilation eager compiler tries best job runtime information time compilation revisits code exceptional happens type prediction fails compiler generate specific code 
systems customize code runtime type information employ type feedback 
result systems information available compiler 
compiler obtain receiver argument types method procedure compiled optimize operations method runtime type information method executed 
contrast type feedback system uses complete runtime information system achieved deferring optimization code executed batch style type feedback system achieved executing complete training runs optimizing 
program optimizations previous systems attempted eliminate dynamic dispatch means 
example apple object pascal linker turned dynamically dispatched calls statically bound calls type exactly implementation system contained class class 
disadvantage system leaves procedure call overhead simple callees optimize polymorphic calls precludes extensibility dynamic linking 
forms type inference infer concrete receiver types enabling compiler inline sends 
example inferencer described agesen infer set concrete possible receiver types expression self program 
compared type feedback information type inference may compute better smaller sets sends guarantee absence unknown type 
sends information may inferior example type occurs practice types theoretically possible 
type inference systems real object oriented languages just emerge early assess value optimization purposes 
main disadvantages program optimizations concrete type inference link time optimizations program may available optimizer 
programs today workstations dynamically linked compiler know exact set classes program link time deferred runtime 
program hardwired dynamic link library version refused execute version optimization practical compiled version library available 
problem solved optimizations prevent extensibility dynamically loaded user defined libraries add modules available generic tools word processors spreadsheets 
second program optimizations expensive large amounts code analyzed hard integrate rapid turnaround programming environment 
third unclear program optimizers perform profile information guide optimizations 
soon profile optimizer profile type feedback information performing global type flow analysis 
words remains seen global program analyses significantly improve performance type feedback system 
inlining inlining studied extensively procedural languages :10.1.1.14.9668
languages inlining simpler rarely dynamic dispatch 
furthermore inlining candidates easier analyze space time cost language constructs known 
contrast source reveals implementation information pure object oriented language single operation involves dynamic dispatch including arithmetic boolean operations control structures 
profile compilation dean chambers describe system self compiler uses compilation experiment records inlining decisions benefits database 
environment send known information receiver arguments compiler searches database compilations 
system dynamic reoptimization type feedback take advantage recorded information method inlined times compiled code discarded 
compared code heuristics system able better decisions records information context call optimizations enabled inlining 
measurements programs calder grunwald argue type feedback beneficial 
unfortunately apparently aware previous proposed conversion appears identical inline caching proposed extension conversion identical type feedback 
modern compilers conventional languages runtime feedback execution profiling information perform branch scheduling reduce cache conflicts 
systems profile information assist classic code optimizations procedure inlining trace scheduling register allocation 
hansen describes adaptive compiler fortran 
compiler optimized inner loops fortran programs runtime 
main goal minimize total cost running program presumably executed 
optimizations applied statically hansen system tried allocate compile time wisely order minimize total execution time sum compile runtime 
self tries achieve similar goal quickly optimize important parts program time minimizing compile pauses perceptible user 
summary optimizing compiler object oriented language obtain information source code 
example compiler generally determine exact receiver types message sends program spends time 
lazy compilation solves problem optimizing programs system initially information job optimizing program defers dealing hard problems information known 
information available optimization simpler effective additional information optimizing compiler generate better code time 
support contention empirical data chapters 
words laziness wins 
specifically self realizes lazy compilation adaptive recompilation type feedback adaptive recompilation optimizes hot spots application 
initially code generated fast compiler 
unoptimized method frequently recompiled execution slowed running unoptimized code long periods time 
type feedback allows compiler inline dynamically dispatched call exact receiver type known statically 
type information collected previous executions calls type feedback compiler optimize dynamically dispatched call inserting type test common case inline substituting callees 
words dynamically dispatched call target method class replaced type test verify receiver type inlined code callee 
technique relies specific features self language believe useful optimizing statically typed dynamically typed object oriented languages clos smalltalk languages type dependent generic operators apl lisp 
addition type feedback implemented static dynamic compilation 
type feedback adaptive recompilation improve performance object oriented programs removing overhead message passing 
designed implemented techniques self chapter details implementation subsequent chapters evaluate performance 

optimizing self compiler idea type feedback information contained inline caches optimization purposes developed new optimizing compiler self 
compiler design goals threefold high compilation speed 
self system uses dynamic runtime compilation compiler fast possible order minimize compilation pauses 
possible avoided costly global optimizations analyses tried restrict algorithms time complexity linear size source program 
stable high performance large programs 
second design goal surpass performance previous self compiler object oriented programs programs data types integers arrays especially provide performance large applications 
addition performance reasonably stable minor changes source program changes affect complexity algorithm intuitively performance neutral significantly affect performance 
simplicity 
previous self compiler consisted lines code employed complicated optimizations :10.1.1.30.1652
example control flow graph kept changing result type analysis turn influenced results analysis 
consequently compiler relatively hard understand change 
armed new type feedback information decided omit type analysis new compiler anticipated benefits additional type information compensate missing analysis 
know receiver type anyway type analysis eliminate type checks 
lines code resulting compiler considerably simpler old 
generally opted risc style approach designing new compiler included new optimization inspection generated code showed worthwhile 
experience previous self compilers included customization splitting block creation optimizations start shown clear winners :10.1.1.30.1652
shows overview optimizing compiler 
compiler divided phases 
phase performs high level optimizations type feedback inlining splitting resulting graph intermediate code nodes 
second phase performs common optimizations copy propagation dead code elimination register allocation 
third phase completes compilation process generating machine code intermediate control flow graph 
front phase compiler generates intermediate control flow graph method objects byte code representation source methods 
basic code generation strategy straightforward relies second pass eliminate redundancies 
main complexity front lies inlining splitting algorithms 
sections highlight specific parts front obtaining type feedback information program inline caches deciding methods inline handling uncommon cases 
finding receiver types compiler encounters send tries determine possible receiver type inline send possible desirable 
expression objects represent source level values message receivers argu noted lines code excludes blank comment lines 
ments results 
expression objects propagated message inlining keep track known types 
example constant expressions generated numerical literals expression receiver argument expressions method inlined 
context compilation type means implementation type type describing exact object layout set methods 
self explicit types language level implementation maintains type descriptors called maps object contains pointer type descriptor 

organization optimizing self compiler method objects inline caches compiler phase self specific optimizations runtime system intermediate code tree inlined scopes intermediate code source mapping second compiler phase classical optimizations third compiler phase code generation compiled method including scope descriptors pc source mapping source level debugging lookup system receiver constant self receiver type known trivially case customization creates separate compiled method specific receiver type 
receiver expression contains unknown type compiler examines corresponding inline cache previous version compiled code 
finds inline cache judges information reliable see compiler adds corresponding inline cache receiver maps expression object 
resulting expression object contains types obtained type feedback 
created type information merged result control flow merge example method possible result types 
order find inline cache corresponding send currently inlined compiler keeps track current scope current position old compiled code 
compiled method contains description inlined scopes compiler find new callee information method debugging information callee inlined old compiled code inspecting inline cache 
example illustrate process 
suppose method foo sends bar turn sends baz suppose compiler currently recompiling foo 
initial situation shown data structures runtime systems shown left compiler data structures shown right 
generating intermediate code foo source method compiler encounters send bar 
looking current position old compiled code foo compiler finds inline cache bar 
inlining send compiler follows inline cache single target starts generating intermediate code bar 
map system internal representation implementation type objects having exactly structure list slot names constant slot contents share map :10.1.1.56.2990
scope descriptors needed support source level debugging see chapter 

class hierarchy expressions expression superclass known type set expressions unknown type known value code foo compiled method foo old version call bar scope currently compiled runtime system compiler foo scope descriptor 
finding type feedback information compiler encounters send baz bar 
time inline cache corresponding baz old compiled code send inlined 
compiler obtain send receiver type scope descriptors old compiled method bar continue inlining baz 
send chain inlined old version compiled code compiler extract type feedback information little overhead 
foo compiled method foo old version call bar scope currently compiled bar baz bar compiled method bar inlined baz foo runtime system compiler code scope descriptor code scope descriptors 
finding type feedback information ii code scope descriptor code scope descriptors foo compiled method foo old version call bar scope currently compiled bar baz bar compiled method bar inlined baz foo baz runtime system compiler 
finding type feedback information iii code inlining heuristics compiler determined receiver types performs lookups find target methods 
lookup receiver type successful compiler decide inline target method 
compile time lookup may fail reasons dynamic inheritance 
difficult decision reasons hard estimate impact decision method inlining method may cause methods inlined compiler easily determine methods 
self pure message language model predictions especially hard source token dynamically dispatched send target cost known 
reasons hard determine method source execution time saved send inlined 
method small fewer sends inlining won cost terms code size give speedups 
accurately estimate impact inlining particular send performance impact depends result inlining decisions 
example inlining particular send may beneficial case may hurt performance case inlined sends increase register pressure important variables stack allocated 
important constraint compile time inlining longer compile pause 
true single compilations method called places inlined erroneously slow compilations involving method just single compilation 
uncertainties set simple heuristics suffice best approach try 
example compiler optimistic assumptions build possibly large tree inlined scopes look costs benefits individual decision come pruned tree 
words try inlining set methods back inlining beneficial 
strategy promises maximize quality generated code probably suboptimal terms system performance system complexity 
inlining phase consumes compile time see chapter 
building optimistic large call tree expensive terms compile time 
second undoing inlining decision tricky implement significantly complicate design compiler 
reasons pursue strategy 
dean chambers propose strategy compilation optimistic experiment record decisions benefits inlining database 
environment send known information receiver arguments compiler search database experiences previous compilations better informed inlining decision 
dean chambers report encouraging results compilation sped factor execution slowed factor study included programs consider type feedback inlining usefulness technique compiler described unknown 
inlining database add considerable amount code compiler dean chambers implementation consists lines code increase size compiler 
risc style design strategy decided include mechanism necessity proven 
dynamic inheritance lets self programs change inheritance graph runtime method invoked may vary send send receiver type constant 
study programs showed reduction compile time performance degradation compile time reduction execution performance penalty factor 
may mechanism benefit system self system dean chambers system inlining opportunities 
self compiler uses simple local inlining heuristics previous self compilers extended additional feedback runtime system 
currently compiler uses sets heuristics inlining decisions code heuristics source heuristics 
code heuristics look previous version compiled code available determine size impact inlining decision 
advantage approach doesn look single source method bigger picture compiled method source method includes code inlined calls 
furthermore lets observe size real compiled code vaguely defined notion source method size 
size better approximation code size impact inlining decision estimate source code metrics 
sense code heuristics employ compiled methods primitive inlining databases 
naturally optimized methods size unoptimized methods bad predictor size optimized code send implemented word inline cache 
currently code heuristics consist rules 
estimated total size method compiled exceeds threshold currently instructions reject method inlining 
rule designed limit maximum size compiled method 
size estimate sum node costs intermediate control flow graph 

callee small instructions instructions method block arguments accept inlining 

callee larger threshold reject estimated total size method compiled remains small callee inlined 
exception allows large methods inlined called small wrapper method total size won larger callee 

rules apply optimized callee available compiler uses source heuristics fallback 
source length defined number message sends corrections cheap sends local slot accesses 
methods exceeding threshold inlined long estimated total size allows see rule 
known control structures treated specially example message implementing loop 
general recompilation source heuristics old code completely unoptimized code heuristics 
code heuristics frequently stages recompilation replacing optimized method newer 
possible improve effectiveness code heuristics keeping record previous compiled method sizes inlining database just containing size optimized compiled methods try implement mechanism 
compilers conventional languages gnu size intermediate code guide inlining decisions making decision optimizing intermediate code 
approach depends having intermediate code inlining candidates available compile time case self inlining candidates compiled earlier time 
splitting splitting avoids type tests copying parts control flow graph :10.1.1.56.2990
main motivation splitting allows compiler generate code boolean expressions statements relatively especially important object code remotely resembles source code sends expand zero machine instructions may expand hundreds instructions 
additional rule limits inlining recursive calls 
strictly speaking rule isn essential inlining maximum estimated method size reached decided include anyway results better compile time execution performance 
little effort 
chambers terminology compiler uses extended reluctant splitting may split send send immediately follow merge control flow extended splitting see split send profitable reluctant splitting :10.1.1.30.1652
current system splits amount copied code code small limit currently instructions 
previous self compiler compiler perform loop splitting create copies loop body specialized specific types 
include loop splitting reasons 
convinced lead significant speedups nontrivial programs programs usually dominated small monomorphic loops loop splitting really pay 
second loop splitting relatively expensive implement requires type analysis realize benefits :10.1.1.30.1652
performing splitting front obtain similarly code back optimization 
heintz proposes optimization called rerouting predecessors smalltalk compiler similar back optimizations conventional languages known 
opted splitting achieve code extensive analysis helps building fast compiler 
uncommon traps self operations different possible outcomes compiler creates specialized code common cases order improve efficiency 
example compiler specializes message sends type feedback information generates optimized code predicted common cases 
uncommon cases compiled 
integer addition overflows send receiver type seen 
ignoring uncommon cases helps obviously correct strategy include clause type tests overflow checks include unoptimized code case 
typically code perform non inlined message send 
example send optimized type feedback look receiver type simple strategy serious drawbacks 
splitting code inlined code area method circles inlined code area method squares splitting splitting send area code code code size 
non inlined sends inline caches consume lot code space instructions 
sends inlined type predictions code handle infrequent cases consume significant portion total code space 
compile time 
similarly larger control flow graph containing branches slow compilation 
slowing common case 
section show including code uncommon cases may slow common case especially send block arguments 
drawback severe 
shows code expression generated including uncommon cases assuming max vector local variables predicted integers 
code detect uncommon cases required handle simpler 
optimization suggested john maloney implemented self compiler 
integer 
max integer 
temp 
inlined primitive temp int lt max true false primitive fails code omitted message send temp send initialize may may created inlined code argument block goes shown uncommon case probably programming error create block optimization block creation delayed send iftrue temp false true rest method goes 
method destroy block code code iftrue 

code handling cases self compiler code cases compiler considers happen replaced trap instruction 
example compiles code shown 
version code advantages example eliminates branches assignments sends closure creation 
result static code size significantly reduced execution sped 
uncommon cases don merge back common case code uncommon traps improves accuracy type information 
example compiler knows temp true false result types primitive unknown 
compiler perform extensive type analysis advantage pronounced previous compiler essential :10.1.1.30.1652
code eliminates overhead related argument block created 
addition direct savings initializing destroying block exit method eliminating block trap instruction compiler call jump trap saves instructions uncommon case 
integer 
max integer 
temp true 
inlined primitive temp int lt max true false primitive fails type prediction failed need initialize created inlined code argument block goes shown false case rest method goes 
method code needed destroy iftrue argument code code iftrue 

code handling common cases significant benefits code generation 
block possibly created variables accessed block vector register allocated may accessed block lexical link memory 
current values variables written memory non inlined send send invoke block 
eliminating block compiler eliminated memory accesses 
similarly block assigned local variable variable stack allocated value reloaded memory non inlined send 
clear requiring compiled code handle uncommon cases significant performance advantages 
improved code handle uncommon cases non integer language semantics preserved code verifies type predictions 
happens uncommon case happen 
cases may rare possible need handled 
uncommon branch extensions self compiler replaced code uncommon cases call runtime routine 
uncommon case executed runtime routine created uncommon extension patched call refer extension 
extension continuation original compiled code contained code point failure method 
taken uncommon branch got extension code 
main method extensions unoptimized order create extensions 
compiler predicted integer prediction failed code look follows integer int add goto uncommon extension rest method separate method uncommon extension send unoptimized code rest method type prediction failed code jump uncommon extension send message execute rest extension unoptimized code 
common case fast uncommon case fairly slow 
approach problems 
compiler strike careful balance deciding uncommon meant 
compiler conservative making extremely rare cases uncommon common case code slowed 
treated cases uncommon cases occur frequently result poor performance 
example previous compiler type predicted sends having integer receivers non integer case uncommon 
programs floatingpoint arithmetic frequently executed uncommon extensions performed poorly 
system performance fragile compiler predictions right performance performance drop dramatically wrong see section page 
second approach space efficient 
new extension created uncommon case method encountering uncommon cases need uncommon extensions duplicating code 
uncommon extensions tended large relatively unoptimized uncommon case happened near method send extensions include original method 
uncommon traps recompilation avoid pitfalls self compiler method extensions uncommon cases 
views uncommon cases form runtime feedback shows summary process 
times uncommon trap executed assumed rare case need optimized 
example programmer passed object wrong type method resulting message understood error 
compiled code remains unchanged stack frame containing uncommon trap merely replaced unoptimized frames execution continues 
deoptimization described detail chapter won describe 
uncommon trap executed probably rare result compiler misprediction 
example program integers started floating point data arithmetic operations start fail 
traps occur uncommon trap handler discards offending compiled method addition creates new version 
new version conservative approach cases uncommon run slowly 
eventually new method system marks uncommon method lets execute 
new compiled method executed eventually selected recompilation 
point compiler aggressively uncommon traps 
method executing inline caches corresponding uncommon cases filled cases occurred 
status inline caches reveals considered uncommon inline cache empty send executed 
recompilation method specialized new set types program 
self system handles uncommon cases replacing overly specialized code specialized code necessary extending specialized code general extension 
handling uncommon cases way advantages 
compiler aggressive uncommon traps compiler writer bet turns wrong recompilation correct mistake 
second eliminates reduces potentially large performance difference programs compiler writer bets bad programs bets lose 
section revisit topic measurements 
astute reader may noticed pitfall scheme learning period second final recompilation short uncommon method inline caches properly reflect program type usage recompiled code may soon incur uncommon trap specialized 
system keeps track compiled code version uses exponential back exact limit user definable currently 
optimized method encounters uncommon trap trap happens infrequently method continue execution trap happens frequently recompile method replace traps real code uncommon method executed recompile traps feedback uncommon method 
sequence events handling uncommon traps strategy determine minimum number invocations reoptimization allowed 
time method recompiled uncommon traps minimum number invocations multiplied constant 
learning period eventually long capture method type usage 
back compiler back performs optimizations intermediate code generates machine code 
compiler perform full fledged dataflow analysis coloring register allocation order maximize compilation speed 
remainder section discusses key aspects self back 
intermediate code format intermediate code control flow graph consists different kinds nodes 
node types represent simple risc operations assignment heap loads stores arithmetic 
nodes typically translate machine instruction 
contrast nodes represent self specific operations translate code patterns machine instructions long 
examples macro nodes prologue node expands method prologue code including receiver type check stack overflow test stack frame allocation 
nodes implement tagged integer arithmetic generated code checks tags arguments necessary tests result overflow 
tagged arithmetic nodes successors success case arguments integers result didn overflow failure cases 
nodes implement accesses object byte arrays including bounds check value check byte array stores integers stored byte arrays 
tagged arithmetic nodes array access nodes successors 
node implements non inlined message send generates inline cache send 
message node successors normal return non local returns 
node implements way type test successors types tested case 
expressing way branch single node sequence simple way branches simplifies control flow graph allows efficient testing code example fairly simple fill branch delay slots testing sequence implementing general delay slot filling algorithm 
nodes implement creation destruction blocks 
block creation involves calling primitive block destruction simpler see section 
generally introduced special node type function expressed easily combination simpler nodes allowed compiler generate better code certain constructs generated introducing general optimizations deemed expensive reduced size control flow graph improving compilation speed 
high level nodes sacrifice code quality lower level rtl style intermediate language expose optimization opportunities 
decided trade compilation speed runtime performance case 
nodes pseudo registers arguments pseudo registers mapped machine registers stack locations register allocator 
pseudo registers divided kinds aid back optimizations incorporating front knowledge usage pseudo registers 
important pseudo register types see glossary definition non local returns 
general form pseudo register 
example front creates local variables method 
singly assigned pseudo register 
may nodes defining splitting front guarantees definition kill definition 
exactly defining node executed possible path method 
freely computing reaching definition information 
pseudo register holding block additional information needed block creation zapping nodes 
potentially definitions propagated freely 
pseudo register holding constant constant value method exactly constant twice uses pseudo register 
intermediate code nodes introduced special pseudo register types simplified task back speeding compilation allowed perform optimizations possible extensive dataflow analysis 
computing exposed blocks phase back computes set exposed blocks blocks may passed arguments compiled methods may stored heap 
information important reasons block exposed needed actual object 
compiler omit creation saving considerable time instructions including garbage collection overhead code space instructions call block creation routine instruction zap block 
block exposed may needed actual object point 
compiler mark variables accessed block local variables parameters lexically enclosing scopes read written 
non inlined call considered potential definition accessed variables block invoked callee 
computing set exposed blocks fairly straightforward 
pass nodes control flow graph constructs initial set 
blocks precisely added set assigned pseudo register source store node 
compute transitive closure initial set adding blocks lexically enclose blocks set 
side effect pass sets read written flags pseudo registers accessed exposed blocks 
inserts definition read pseudo register sure reads access correct value pseudo register allocated machine register 
currently written variables allocated stack register allocator cached register 
def information compiler pass computes definitions uses pseudo register single pass control flow graph 
definitions uses pseudo register recorded linked list grouped basic block definitions uses basic block readily available local copy propagation 
see section details block zapping 
assigned pseudo register argument non inlined send passed inlined method front omits creating pseudo register callee argument directly uses 
copy propagation copy propagation important classical code optimization implemented back divided phases local propagation basic blocks global propagation 
phase compiler tries propagate guaranteed real definition exactly definition 
compiler compute reaching definitions information detect subset possible propagations 
fortunately represent majority pseudo registers incoming parameters expression stack entries guaranteed singly assigned limitation probably severe 
furthermore information akin reaching definitions readily available basic blocks local copy propagation phase propagate multiply assigned registers 
pseudo register propagated control flow graph simplified result propagation 
example successor branch node eliminated constant propagated node 
recall front perform type analysis may generate redundant type tests 
copy propagation process compiler eliminates definitions unused pseudo registers definitions side effect free 
preserve source level semantics compiler eliminate expression result compiler prove integer overflow occur 
register allocation performing optimizations task remaining code generation register allocation 
known algorithms graph coloring produce register allocations chose implement allocator considered compile time cost high 
register allocator relies usage counts 
performing global register allocation local register allocator allocates pseudo registers local basic blocks 
local allocator scans nodes basic block determine hardwired registers registers holding outgoing arguments basic block 
pseudo register local basic block allocator tries find scratch register completely unused basic block 
usually free scratch registers successfully allocate pseudo registers local basic block local register allocation fast 
scratch registers basic block local register allocator switches slightly complicated approach 
determine pseudo registers share scratch register live ranges conflict allocator computes live ranges scratch registers basic block live ranges represented bit vectors bit node basic block 
unallocated local pseudo register allocator tries find scratch register live pseudo register live range 
register assigned live range bit vector updated 
combination allocation strategies successful local register allocator usually succeeds allocating registers allocated locally 
pseudo registers typically represent pseudo registers local allocator significantly reduces burden global allocator 
global register allocator allocates remaining unallocated pseudo registers simple usage count algorithm 
pseudo registers weight number times pseudo register overhead includes cost performing allocation cost computing information required allocator register interference graph 
compiler passes receiver arguments registers 
scratch registers caller saved registers temporary registers compiler certain hardwired code patterns global allocator uses registers 
highest weight allocated 
usage counts weighted loop depth nesting 
simplicity speed live range global pseudo register expressed source level terms scope byte codes say bit vector indexed intermediate nodes 
test live ranges overlap suffices determine scope child byte code ranges overlap scopes identical 
original live ranges easy determine locals live entire method expression stack entries live byte code producing byte code consuming 
ensure correct allocation copy propagation phase updates pseudo register live range include new 
self register allocator fast simple generates reasonably allocations demonstrated performance measurements chapter 
sophisticated coloring register allocator undoubtedly improve performance times expensive terms compile time 
compilation speed demands placed compiler interactive exploratory system see chapter believe current allocator represents compromise allocator speed allocation quality 
runtime system issues section discusses low level code generation issues related runtime system efficiently test type object maintain write barrier generational garbage collection invalidate zap blocks lexically enclosing method returns 
type tests type tests frequent operations compiler predicts receiver type information pics static type prediction insert type test inlined body respective method 
important type tests fast 
conceptually type test involves loading object type testing constant 
heap allocated objects store map type second word object 
unfortunately exceptions integers floating point numbers represented immediate bit values heap allocated explicitly contain map type encoded tag bits 
type test test receiver heap ness loading map obj temp extract object tag cmp temp compare pointer tag beq heap object 
ld obj temp delay slot load map heap object handle immediate case code shown continuation heap object case set map temp load map constant cmp temp temp compare maps beq expected map 
code cases omitted implementation simple type test consumes considerable code space execution time 
fortunately improve code speculatively loading map testing object tag 
load incur illegal address trap object immediate value memory addresses word aligned 
trap occurs trap handler fill destination register appropriate map integer map float map continue execution instruction 
sparc branch instructions delay slot branches marked suffix execute delay slot branch taken 
sequence load instruction executed object correct tag 
suffice zero destination register purpose sure comparison predicted map fail 
speculative map loads significantly speed type tests new code sequence ld obj temp speculatively load map set map temp load map constant cmp temp temp compare maps beq expected map 
code cases omitted testing repeatedly map map constant allocated register reducing test sequence just instructions load compare branch 
course type test expensive trap taken costing thousands instructions speculative load scheme sense traps occur rarely 
reason speculative loads previous systems soar 
soar trap handler faster unix trap handler cycles compared thousands cycles hardware software designed fast traps 
unfortunately fast traps available typical workstations 
speculative loads gamble performance degrade significantly traps occurred 
fortunately pics type feedback recompilation guarantee traps rare system 
speculative map load part pic encounter trap pic extended immediate case trap course new pic test tag loading map 
speculative map load part compiled method get trap previous version compiled method encountered immediate values previous invocations 
load cause repeated traps anyway corresponding compiled method recompiled way similar uncommon traps 
adaptive recompilation allows system speculative map loads speed type tests 
non adaptive systems self system pay heavy price speculation fails recompile method reverting non speculative tests speculation fail frequently 
store checks generational garbage collectors need keep track older younger generations younger generations garbage collected inspecting object older generation 
set locations potentially containing pointers newer objects called remembered set 
store system ensure updated location added remembered set store creates older newer object 
maintenance invariant usually referred write barrier store check 
stores compiler statically know store check necessary example storing integer storing old constant old objects new 
general case store check executed store operation 
stores frequent efficient write barrier implementation essential 
self write barrier implementation wilson card marking scheme 
scheme heap divided cards size words typically card associated byte separate vector 
store check simply clears byte corresponding location updated 
garbage collection time collector scans byte vector finds marked byte collector examines pointers corresponding card heap 
advantage byte marking scheme speed 
self store check involves just sparc instructions addition actual store fact traps slower hardware faster 
designs attempt reduce trap overhead example sparc architecture allows low overhead user mode traps 
similar efficient schemes proposed sobalvarro shaw 
st obj offset ptr store ptr object field add obj offset temp calculate address updated word sll temp temp divide card size shift left st byte map temp clear byte byte map code sequence assumes register byte map holds adjusted base address byte map byte map start address heap word avoiding extra subtraction computing index byte cleared 
reduced write barrier implementation extra instructions checked store previous self system 
new write barrier improves standard card marking relaxing invariant maintained card marking scheme 
invariant maintained standard card marking bit byte marked card may contain pointer old new scheme relaxed invariant bit byte marked cards may contain pointer old new small constant current implementation 
essentially gives store check leeway choosing byte mark marked byte may bytes away direction lower addresses correct byte 
relaxed invariant allows omit computing exact address updated word long offset updated word distance object mark byte corresponding object address byte corresponding updated field 
common case store check instructions st obj offset ptr store ptr object field sll obj temp calculate approximate byte index st byte map temp clear byte byte map usually leeway sufficient cover virtually stores stores array elements 
example card size bytes self system stores fields objects fast code sequence 
arrays containing pointers treated specially array element assignments mark exact card assignment may arbitrarily far away start array 
system determine object boundaries leeway restriction lifted entirely addition scanning marked card collector just scans forward finds object header 
arrays containing pointers treated specially mark exact card assignment 
scanned card contains object headers object starting card pointer array collector need scan current card 
overhead store check instructions store represent significant fraction total garbage collection overhead 
example store checks slow richards sparcstation 
detailed analysis scope thesis refer interested reader information 
block zapping consider method foo localvar localvar hello fields object system user defined field offset bytes 
example method stores block global 
possible invoke block foo returned access localvar foo activation activation heap allocated sure localvar existed 
block invoked enclosing method returned known non lifo block upward lisp parlance 
current self implementation method activations stack allocated blocks may executed enclosing scope returned 
restriction currently exists hard implement full closures slowing calls 
detect abort non lifo block invocations blocks enclosing method returns 
block object contains pointer enclosing scope stack frame lexical link zapping simply clears link 
blocks inlined away don need block object created 
blocks created simple store instruction 
third category blocks memoized blocks may may created depending execution path taken 
pseudo code expression foo illustrates situation integer compiler inlined call block block created type unknown need perform real send create block order pass argument returning method containing expression block zap 
previous self system solved problem initializing block location zero testing executing zapping store resulting instruction sequence cmp block block created 
bne cont suppress instruction created st block zap block delay slot cont eliminate compare branch instructions initialize block location dummy block object locations holding created blocks point dummy block containing zero 
result unconditionally zap block single store instruction matter created 
block created store just overwrites dummy block lexical link st block zap block real dummy drawback reducing block zapping instructions extra instruction initialize block location bit constants require instructions sparc risc architectures 
cost initialize zap lower especially branches expensive 
furthermore blocks need initialized address dummy block usually allocated register reducing initialization back instruction block 
self way block zapping clear improvement previous implementations 
missing describing implemented current compiler fitting describe implemented 
currently compiler shortcomings adversely affect code quality important ones listed sections 
course optimizations improve performance common subexpression elimination loop invariant code motion 
ignore optimizations discussion require global dataflow analysis slow compiler considerably 
restrict shortcomings removed reduced relatively little impact compile time 
section estimates potential performance impact shortcomings 
peephole optimization generated code contains inefficiencies removed simple peephole optimizer 
examples inefficiencies unfilled delay slots 
delay slots filled fixed code sequences method prologue delay slots unfilled 
branch chains 
code contains branches branch unconditional branch instructions directly branching final target 
extra loads 
values may repeatedly loaded memory basic block 
especially inefficient loaded value accessed value entire sequence loads lexical chain repeated cases 
load stalls 
compiler attempt avoid load stalls 
sparcstation load takes extra cycle result instruction 
extra cycle avoided inserting unrelated instruction load instruction loaded value 
superscalar sparcstation load stall occurs instruction load starts new instruction group executed cycle load incurring similar performance loss 
repeated type tests compiler perform type analysis value may tested repeatedly type test unnecessary 
example consider method foo 

assuming integer compiler uses uncommon branches compiler produce code integer uncommon trap integer uncommon trap 
second type test redundant guaranteed integer point weren execution left compiled method uncommon trap 
compiler remove second test dataflow analysis parameter assigned parameters read self 
naive register allocation current register allocator fast compete high quality register allocator especially register pressure high 
interference graph registers coalesced effectively resulting extra register moves 
furthermore register live range expressed source level terms coarse registers may appear busy basic block 
fortunately sparc register windows create relatively register rich environment deficiency usually result extra memory accesses 
summary self compiler geared producing code quickly 
techniques help achieving goal 
type feedback allows compiler inline sends extensive analysis type information readily extracted runtime system little overhead 
code inlining heuristics allow compiler better inlining decisions inspecting existing compiled version inlining candidate 
uncommon traps aggressively reduce code size improve code quality excluding unknown cases adaptive recompilation allows compiler back assumptions prove overly aggressive 
compiler back fairly conventional uses high level intermediate code format format order retain high compilation speed 
exploiting single assignment characteristics entities compiler perform global optimizations computing full dataflow information 
innovative solutions runtime system improve code quality type tests eliminate tag checking overhead speculative map loads store checks generational garbage collections reduced instructions heap store blocks closures invalidated efficiently 
chapters evaluate analyze performance generated code chapter evaluate compilation speed show compatible interactive programming environment 

performance evaluation chapter evaluates analyzes performance new self system measuring execution times suite small large self programs see table 
exception richards benchmark programs real applications written benchmarking mind 
applications written different programmers exhibit range programming styles application mango generated parser generator 
evaluate benefits type feedback adaptive reoptimization compared different self implementations table 
self previous implementation iterative type analysis system recompilation type feedback :10.1.1.30.1652
self current system described previous chapters dynamic recompilation type feedback 
self nofeedback self recompilation type feedback turned 
configuration normally optimizes methods type feedback estimate impact type feedback comparing self 
furthermore impact type analysis self estimated comparing self self nofeedback 
evaluate absolute performance self system compare smalltalk 
smalltalk dynamically typed language comes closest self spirit 
implementation parcplace smalltalk widely commercial implementation regarded fastest smalltalk implementation available today 
provide point compare self widely statically typed object oriented language 
gnu compiler probably widely implementation generates code sun cc typical implementations excluding blank lines time ui ui excludes time spent graphics primitives self execute cecilcomp benchmark run simulator described section 
believe bug related register window flushing explain simulated benchmark runs correctly 
benchmark size lines description small benchmarks deltablue way constraint solver developed university washington primmaker program generating glue stubs external primitives callable self richards simple operating system simulator originally written bcpl martin richards large benchmarks cecilcomp cecil compiler compiling fibonacci function compiler shares code interpreter cecilint cecilint interpreter cecil language running short cecil test program mango automatically generated lexer parser ansi parsing line file typeinf type inferencer self described ui prototype user interface animation techniques ui experimental user interface table 
benchmark programs preprocessing 
compare compilers significant performance differences 
table lists main implementation characteristics systems 
methodology execution times self programs reflect performance re optimized code include compile time 
recompiling system programs run performance stable recompilations occurred runs run involving compilations 
self self nofeedback dynamic recompilation second run measurements 
uncommon cases trigger recompilations gnu inline virtual calls circumstances data flow analysis chapter examines aspects related compilation speed 
system description self current self system dynamic recompilation type feedback methods compiled fast non optimizing compiler recompiled optimizing compiler necessary 
self nofeedback self type feedback recompilation methods optimized 
self chambers self compiler iterative type analysis methods optimized :10.1.1.30.1652
compiler shown achieve excellent performance smaller programs :10.1.1.30.1652
smalltalk parcplace smalltalk release techniques described gnu compiler version sun cc cfront highest optimization level option table 
systems benchmarking system optimizing compiler high quality back type feedback adaptive recompilation customization splitting type analysis self self nofeedback self smalltalk table 
implementation characteristics benchmarked systems symbols mentioned follow established practice geometric means summarizing performance comparisons 
example system times faster system means geometric mean benchmarks execution time ratios time taken divided time taken 
accurately measure execution times wrote sparc simulator spa shade tracing tools cache simulator 
simulator models cy implementation sparc architecture running mhz chip sparcstation ss workstation 
accurately models memory system ss exception different cache organization 
original ss cache configuration suffers large variations cache ratios caused small differences code data positioning observed variations total execution time 
cache variations apparent self compiled code data dynamically allocated small changes system change relative alignments code data changing number conflict misses 
systems measured specifically optimizes programs better cache behavior variations considered random presence harder accurately compare systems versions system 
unified direct mapped cache ss simulate machine way associative instruction cache way associative data cache write allocate subblock placement 
assumed write buffer write cache sufficiently large absorb writes 
original ss workstation cache lines bytes long cache penalty cycles 
changed cache configuration variations smaller order execution time caches way set associative reducing number conflict misses split caches cache conflicts instructions data 
simulated cache configuration reduces variability cache misses number conflict misses misses capacity misses 
larger programs general incur cache misses run slowly real machine 
words simulated cache configuration merely reduces noise data realistically models program cache behavior 
reduced noise able evaluate performance impact compiler optimizations precisely 
verified simulator comparing real execution times measured idle sparcstation workstation single user mode simulated times 
simulating actual sparcstation cache organization results agreed differing cases simulated times usually somewhat better measured times probably simulator model cpu usage cache pollution operating system 
simulated times associative cache organization real execution times depending benchmark particular measurement run recall direct mapped caches show wide variations cache effectiveness 
attribute difference better cache organization ignored operating system overhead 
type analysis improve performance object oriented programs measure baseline speed new compiler comparing self nofeedback fastest previous implementation self 
shows results table appendix contains detailed data 
surprisingly self slightly faster average despite type analysis extensive type prediction significantly ambitious back 
informal analysis generated code leads believe speedup self self nofeedback comes differences compiler write allocate subblock placement allocates cache line store instruction location currently residing cache 
organization common current workstations decstation shown effective programs intensive heap allocation 
sparcstation write buffer organization somewhat unfortunate incurs high write costs compared common designs 
avoid skewing data chose model ss write buffer assume perfect write buffer 
previous shown writes absorbed virtually cost diwan report write buffer cpi contribution element deep buffer cache organization similar decstation 
back ends 
example self compiles division shift richards inner loop self nofeedback perform optimization 
general code generated self performs significantly fewer unnecessary register moves 
performance difference explained self superior back mean type analysis performance impact 
confirms programs benchmark suite benefit type analysis average self inline sends self nofeedback 
apparently type analysis infer receiver type sends 
general type analysis infer result types non inlined message sends arguments non inlined sends assignable slots instance variables 
sends values fairly frequent large fraction message sends benefit type analysis 
object oriented programs benchmark suite benefits type analysis small 
section discuss type analysis detail 
alternate explanation small impact type analysis compiler chose inline additional sends type analysis infer receiver types 
ample evidence explanation accurate example half non inlined sends richards access methods just return set value instance variable 
compiler known receiver type sends inlined 

execution speed self relative self nofeedback faster geomean ui ui typeinf mango cecilint cecilcomp richards primmaker deltablue 
number sends self relative self nofeedback geomean ui ui typeinf mango cecilint richards primmaker deltablue type feedback works evaluate performance impact type feedback compared self self nofeedback 
self nofeedback competitive self reasonably sure speedup obtained type feedback result artificially self nofeedback system true performance improvement 
shows results measurements table appendix contains detailed data 
type feedback significantly improves quality generated code resulting speedup geometric mean self nofeedback self nofeedback optimizes code self optimizes parts code 
recompilation system appears highly successful exploiting type feedback provided pics generate better code finding time critical parts applications 
type feedback drastically reduces number calls executed benchmark programs 
average type feedback reduces call frequency factor 
apparently type feedback exposes new inlining opportunities self nofeedback exploit lacks receiver type information 
example self nofeedback half non inlined sends richards access methods just return set value instance variable 
sends inlined self 
see table appendix detailed data 

execution speed self relative self nofeedback faster geomean ui ui typeinf mango cecilint cecilcomp richards primmaker deltablue 
calls performed self relative self nofeedback geomean ui ui typeinf mango cecilint cecilcomp richards primmaker deltablue interesting look number calls relative completely unoptimized self 
unoptimized self message send implemented dynamically dispatched call exception accesses instance variables receiver unoptimized programs run times slower self self see chapter 
self executes calls unoptimized self 
contrast original calls remain self self nofeedback 
expected correlation eliminated calls speedup generally inlining call improves performance 
correlation fairly weak see 
table appendix shows time saved call averaged benchmark varies factor ranging microseconds call eliminated instructions eliminated call 
benchmark averages probably underestimate true variance savings inlined call inlining decisions probably result negative savings increase register pressure leading spill cycles cycles saved eliminating call overhead may result higher savings allow expensive primitive constant folded 
aggressive inlining generally significantly increases performance self programs 
effect marked contrast studies languages fortran inlining 
call frequency relative unoptimized self geomean ui ui typeinf mango cecilint cecilcomp richards primmaker deltablue self self nofeedback self 
call frequency reduction vs execution time reduction 
execution time ratio self self nofeedback call frequency ratio self self nofeedback deltablue primmaker richards cecilcomp cecilint mango typeinf ui ui geom 
mean small benefits :10.1.1.14.9668
attribute discrepancy number significant differences self conventional languages 
self procedures methods smaller average fortran operation including simple ones integer arithmetic array accesses involves message sends 
object oriented programming encourages code factoring separate commonalities related abstractions tends result shorter methods 

fortran programmers may performance conscious writing programs guided assumption calls expensive 
essence programmer hand inlines small line methods 

self uses closures implement control structures user defined iterators inlining call may enable compiler avoid creating closure saving instructions 
inlining single call potentially result significant savings source level redundancies common subexpressions exploited result inlining 
interestingly modern computer architectures create similar situations conventional languages 
example inlining single call may dramatically speed fortran program loop parallelized improved data dependence information 
speedup result reducing execution time inlined code optimizations enabled caller 
shows sources improved performance vary widely benchmark benchmark 
xaxis shows total execution time saving type feedback difference self nofeedback self 
boxes summarize importance category benchmarks contribution speedup 
depending benchmark reduced call overhead represents total savings execution time median arithmetic mean geometric mean 
reduced number type tests contributes speedup median contribution mean reduced number closure creations 
effects standard optimizations perform better increased size compiled methods greatest contribution speedup median mean show largest variation 
benchmark contribution negative slows execution 
possible reasons slowdown inferior register allocation increased register pressure higher instruction cache misses 
measurements include cache effects 
summarize measurements show performance improvement obtained type feedback means dominated decreased call overhead 
benchmarks factors call overhead dominate savings execution time 
inlining type feedback enabling optimization allows optimizations better creating indirect performance benefits addition direct benefits obtained eliminating calls 
data assumes savings cycles eliminated call measure exact savings individual call 

reasons self improved performance 
closures overhead call overhead fraction total speedup achieved comparison systems provide context self absolute performance measured versions deltablue richards benchmarks written smalltalk 
benchmarks available smalltalk 
measured versions 
version hand optimized declaring functions virtual dynamically dispatched absolutely necessary 
second version functions declared virtual implicitly smalltalk self 
mean function calls dynamically dispatched gnu compiler statically binds calls receiver type known 
possible run smalltalk simulator obtain elapsed time measurements 
discussed section estimate simulated times lower elapsed times simulation include os overhead simulates better cache organization 
comparison self estimated smalltalk time elapsed time 
benchmarks self runs times faster parcplace smalltalk generally regarded fastest commercially available smalltalk system see self language model purer harder implement efficiently :10.1.1.30.1652
furthermore self times slower optimized despite radically different language models fact self back clearly inferior compilers 
functions declared virtual approximate self semantics speed advantage reduced significantly times speed self 
compilers potentially perform classical optimizations self generate efficient code case optimize dynamically dispatched calls 
course programs support generic arithmetic arithmetic overflow checks array bounds checks user defined control structures 
summary self compiler brings self performance level competitive objectoriented languages compromising self pure semantics 
different inputs possible objection results performance self measured inputs trained 
intuitively smaller impact self conventional system 
main benefit feedback type information program type profile important time profile 
type profile remain unchanged input input time table appendix detailed data 

self execution speed compared languages faster richards deltablue relative execution speed self sun cc sun cc self smalltalk profile 
study garrett confirmed hypothesis cecil 
second system dynamically adapt changes time type profile new hot spots recompiling incorporate new types 
potential objection performance measurements far benchmark runs short 
execution times benchmarks kept relatively short allow easy simulation 
sure small inputs distort performance figures measured benchmarks large inputs 
table shows speedups achieved large inputs similar speedups smaller inputs 
stability performance resulting high performance type feedback additional advantage compared systems static analysis source provides stable performance 
minor source change system solely static analysis may lose crucial piece information disabling key optimization performance 
example suppose program inner loop adds elements vector elem sum sum elem source change prevents compiler obtaining exact type loop inlined performance drop significantly 
type feedback loop inlined anyway type information runtime system overhead added extra type test 
dynamic nature type feedback results stable performance static analysis 
type analysis exhibits unstable performance measure effect reduced static type information performance took small integer programs stanford benchmark suite evaluate self compiler :10.1.1.30.1652
inner loops microbenchmarks typically lines long perform integer arithmetic array accesses 
originally written benchmarks provide static type information compiler important values 
example vector sorted bubble benchmark constant slot compiler inline array accesses put sends 
having constant array lets compiler determine array size allows compiler infer integer additions performed loop index overflow 
model normal usage created second version benchmark eliminating sources static type information usually changing constant slots assignable slots 
changes minor way change algorithmic character benchmark 
example modification bubble benchmark change array sorted constant slot assignable slot 
shows perfor computed data table hard separate influence input size input characteristics larger inputs may exercised different parts benchmark applications 
benchmark execution time seconds speedup self nofeedback self large input small input cecilcomp cecilint mango table 
performance long running benchmarks mance self benchmarks performance shown relative performance equivalent compiled gnu compiler full optimization 
self complete type information performs reaching speed optimized self performance drops type information lost example bubble slows factor 
average modified benchmarks slow factor self 
having lost important sources static type information compiler longer able optimize benchmarks 

performance self stanford benchmarks geom 
mean tree towers sieve quick queens puzzle perm bubble performance relative optimized original modified original maximum static type information modified normal type information geom 
mean tree towers sieve quick queens puzzle perm bubble performance relative optimized original modified 
performance self stanford benchmarks self performance stable dropping average 
object type known statically type feedback provide type adding small runtime overhead form additional type test 
type feedback self performance dependent static type information performance varies information lost 
static type prediction fail systems type feedback similar problem exists static type prediction 
systems smalltalk self implementations statically predict type certain messages 
example receiver iftrue predicted true false object receiver predicted integer 
static type prediction obvious performance advantages contributes unstable performance 
specialized messages execute faster semantically equivalent code non specialized selectors 
second prediction fails cost message send significantly higher normal send 
example self fall back significantly optimized code type prediction fails discussed section 
non predicted messages slower example taken smalltalk world smalltalk programmer noted newsgroup comp lang smalltalk code sequences vastly different performance perform amount source level terms nil iftrue iffalse nil programmer puzzled fact expression ran times faster smalltalk system second expression involves sends iftrue iffalse just 
reason performance variation smalltalk optimizes iftrue iffalse message send receiver hard coding control structure words source code iftrue iffalse message ignored 
result smalltalk system avoid actual send creation actual block objects argument blocks 
get special treatment execution involves actual message send creation argument blocks runs slowly 
smalltalk special casing messages creates unstable performance 
previous self implementations similar problems type predicted iftrue 
translated example self ran expression times table 
nil self slightly faster self type feedback enables inline send self nofeedback considerably slower relatively low quality compiler back 
systems relatively correctly predict iftrue 
nil compilers static type prediction suffer exactly problem smalltalk system produce dramatically slower code example self times slower 
contrast self generates essentially code nil type feedback astute reader notice self considerably outperforms self original benchmark suite 
section examine situation detail reveal self performance advantage due better back 
self self nofeedback self nil iftrue type prediction works ms ms ms nil static type prediction ms ms ms ratio nil nil table 
performance variation related static type prediction predicts receiver message 
type feedback provides stable performance predict message send just special cases 
mispredictions costly second example shows effect mispredicted message send self 
example adds points expressions add 
expressions compute exactly result 
case static type prediction predict integer receiver message second case misprediction occurs add type predicted message name 
table shows results running expression times 
feedback recompilation self shows largest variation factor 
self nofeedback predicts integer receiver self back misprediction recompiling method encountering uncommon trap see section 
recompiling encountering uncommon cases reduces misprediction penalty 
self nofeedback runs faster self case misprediction type feedback compiler inline add send code significantly slower self 
self produces fastest code cases shows virtually performance variation 
self predicts integer receiver type feedback information unoptimized code indicates correct receiver type point recall compiler performs send 
type feedback recompilation help reduce performance variations 
type feedback helps providing stable performance allows system dynamically adapt program actual type usage 
systems type feedback show higher performance variations cope missing static type information 
systems rely static heuristics speed common sends adapt new frequently message appears heuristics fail 
performance variations high level languages unstable unexpected performance quoted argument high level languages expensive operations cost accurately predicted programmer 
example hoare argues follows language optimized general success fortran specifically designed purpose 
fortran optimization grave disadvantages 
small change optimized program may switch optimization unpredictable unacceptable loss efficiency 
solution problems produce programming language simple straightforward non compiler produce straightforward object programs acceptable compactness efficiency 
believe argument lost strength years 
advent high speed risc processors cache memories ram longer random access basic operations accessing array element widely varying costs relatively low level language fortran 
example add part standard self system usually points added 
self self nofeedback self type prediction fails ms ms ms add prediction ms ms ms ratio table 
performance variations caused failing static type prediction matrix benchmark written fortran part spec benchmark suite sped order magnitude series loop transformations improve cache performance 
programmer lacking knowledge machine architecture transformed program appears complicated efficient radically faster 
appears programming language having memory accessing operations suffer significant performance variations explained source level 
detailed performance analysis analyze sources overhead detail 
look overhead type tests 
time optimized programs spend testing receiver types type feedback influence overhead 
type feedback eliminates type tests type feedback inlines sends type test sequences choose inlined code sequences receiver type 
type feedback replaces dispatch type test pic method prologue line type test 
example assume print sent objects type sight type feedback replaces dispatch test exactly inlined test send inlined 
expect programs execute number type tests applying type feedback optimizations 
shows case programs compiled type feed temp typeof receiver temp goto print temp goto print goto lookup temp typeof receiver temp inlined code print temp inlined code print handle failed prediction type pic send print feedback optimized code send 
number type tests executed self nofeedback self geomean ui ui typeinf mango cecilint cecilcomp richards primmaker deltablue dispatch tests inlined tests self nofeedback self back execute fewer type tests 
benchmark shows number dispatch inlined type tests relative self nofeedback upper bar self nofeedback lower self 
benchmarks self performs fewer type tests average original tests 
apparently simple story effect type feedback incomplete 
type feedback replace dispatch tests inlined tests dispatch tests outnumber inlined tests self nofeedback reverse true self ratio 
reason somewhat surprising result inlining call type test may result additional type information allows sends inlined type test 
example type argument inlined call known sends argument inlined test 
frequent case arguments control structures iterators usually block literals constants 
non inlined case sends require type test type argument known 
inlining send saves type tests 
similarly situations compiler may able outcome type test send value perform general type analysis 
specifically assume non assignable slot parameter predicted type feedback type compiler decides case 
statement sequence foo 
bar send foo send bar translated code unknown inlined code foo unknown trap return inlined code bar type test needed type test verifies type leads trap known type remainder method subsequent sends inlined type tests 
measured extent effect dividing number sends inlined extra inlined type tests executed dynamic counts 
ratio indicates bonus newly inlined send requires type test 
values greater indicate sends inlined free 
average self removed calls new inlined type test executed bonus real compiler perform extensive dataflow type analysis 
programs optimized type feedback execute fewer type tests 
type feedback reduces cost type tests type feedback reduces number types tested type test path length 
example type test succeeds test object type type tested path length type test sequence contains cases 
shows path lengths dispatch inlined tests self nofeedback self 
left box category represents self nofeedback right box test mean code sequence testing value types test counts tests value types 
inlined tests include type feedback tests type prediction tests tests inlined primitives verify arguments integer addition primitive integers 
inlined tests self nofeedback prediction tests testing true false integer primitive tests 
see tables appendix detailed data 
data include type tests performed virtual machine non inlined primitives tests control self compiler 
benchmark spent time type tests virtual machine 
self methods modify parameters 
ends box correspond quartiles horizontal line shows median 
vertical lines box extend minimum maximum value 
represents self 
highlights effects 
inlined type test sequences shorter path length type test sequences method dispatch 
example inlined type tests execute comparisons geometric mean type test sequence self dispatch tests execute comparisons 
second type feedback reduces path length inlined type tests increases path length dispatch type tests 
main reason reduced path length inlined tests inlining reduces degree polymorphism code creating separate copies callee caller 
types particular inlined callee just types caller union callers 
remarkably low path length inlined type tests self equally remarkable low variance show vast majority inlined sends requiring type test need perform single comparison find target 
increased path length dispatch tests probably result inlining highly polymorphic calls 
monomorphic calls inlined remaining calls higher degree polymorphism longer path lengths 
geomean ui ui typeinf richards primmaker mango deltablue cecilint cecilcomp 
sends inlined additional inline type test 
path length type tests inlined type tests dispatch type tests path length type feedback type feedback shows average arity type tests degree polymorphism number cases type test dispatch sequence 
type test arity greater equal path length possible execution 
example test arity testing true false path length contains unknown case path length doesn contain unknown case 
data show dynamic averages 
case path length arity dispatched non inlined sends higher inlined sends 
inlined sends dominate frequency average closer arity inlined sends 
average send arity slightly higher path length 
inlined tests average arity vs path length 
path lengths observed small type tests contain types 
arity higher path length indicate case tested 
self order cases type test frequency precise edge counts low arity shows optimization probably difference 
type feedback reduces cost type tests decreases path length number type tests send 
path length arity inlined sends close indicating inlined sends specialized just type 
type feedback reduces time spent type tests type feedback reduces number type tests performed test seen previous sections 
factors help reduce overhead type tests self 
shows execution time consumed type tests self nofeedback upper bars self lower bars 
benchmarks self spends time type tests average times geometric mean 
reduction result different code sequences dispatched inlined tests faster inlined tests sequences virtually identical 
shows percentage execution time spent type tests self split dispatch inlined tests 
average self spends time type tests executing benchmark programs large benchmarks tend spend time type testing smaller ones 
comparison systems test suffices types unknown case frequent case occurs testing result primitive known return true false 

arity type tests dynamic averages geomean ui ui typeinf mango cecilint cecilcomp richards primmaker deltablue dispatch inlined type testing overhead self appears modest especially considering absolute speed systems relative overhead increases parts system faster 
comparison soar smalltalk implementation risc processor special hardware support spent time method dispatch spent integer tag checking included special hardware 
lisp systems tag checking closest equivalent type tests lisp dynamic dispatch 
steenkiste reported recall category inlined tests includes type feedback tests tests required verify arguments primitive operations index array indexing operation integer 
sparc architecture contains support soar incur additional overhead sparcstation 
virtually risc architecture includes support discuss chapter current self system profit instructions 
risc architecture soar system slowed self 

execution time consumed type tests ui ui typeinf mango cecilint cecilcomp richards primmaker deltablue milliseconds dispatch tests inlined tests self nofeedback self 
percentage execution time spent type tests self geomean ui ui typeinf mango cecilint cecilcomp richards primmaker deltablue dispatch tests inlined tests execution time spent tag handling lisp mips taylor reported instructions involved tag checks spur machine tagged architecture 
type feedback reduces number type tests amount path length type test total execution time spend type tests 
average programs spend time type tests message dispatch 
type analysis vs type feedback seen section system type feedback self outperforms system type analysis self large object oriented benchmarks measured 
valuable type analysis 
comparing self self nofeedback saw self marginally faster benchmarks inlines approximately number sends 
hard separate effects individual factors contributing performance difference type analysis static type prediction code generation result strong indication type analysis self pay programs 
valuable type analysis self combined type feedback 
self spends time type tests including type tests required method dispatch large objectoriented applications measured type analysis improve performance 
type analysis may valuable particular class programs tight loops 
program inner loop consists instructions extra type test brings significant performance hit 
example inner loop bubblesort consists instructions single test verify type array adds instructions loop halving performance 
addition impact code generation weaknesses unfilled delay slots redundant register moves tends amplified small loops 
compares self self original stanford integer benchmarks set small integer benchmarks lines long :10.1.1.30.1652
programs come versions plain oo object oriented 
oo versions rewritten main data structure receiver 
example bubble takes array sorted argument receiver bubble benchmark object bubble oo array receiver sort message 
difference large performance impact self receiver type known due customization 
self performs better small benchmarks applications main benchmark suite outperforming self factor 
recall self times faster large benchmarks 
main reasons self performance original stanford benchmarks 
benchmarks compiler statically know receiver types constants static type prediction 
discussed section self performance drops static type information removed 
self compiler inline sends isn disadvantage relative self 
second self performs variety optimizations benchmarks loop splitting range analysis eliminate array index bounds checks arithmetic overflow tests common subexpression elimination 
furthermore back considerable sophisticated 
benchmarks small weaknesses self code generator extra register moves repeated loads large performance impact 
shows number cycles executed inner loop sieve consumes cycles self vs cycles self 
back differences responsible performance difference 
unnecessary register moves unfilled delay slots account cycles missing array optimizations consume cycles self uses derived pointer step array self indexes array eliminating type tests conceivably save additional time secondary effects better cache behavior larger basic blocks 
loop 
iteration 
type analysis eliminate instructions integer type tests self self back optimizer perform type analysis performance sieve competitive 
shows adding type analysis self significantly speed benchmarks average type testing overhead total execution time 
example bubble times slower self spends type tests 
relatively poor performance self benchmarks results initial decision keep compiler small fast 
better back global dataflow analysis standard optimizations self probably reach performance level competitive self benchmarks 
standard dataflow techniques reduce type testing overhead branch type test value define 
performance stanford integer benchmarks bubble oo bubble sieve perm oo quick quick oo queens oo queens perm puzzle towers towers oo oo tree oo tree relative execution time self original benchmark self original benchmark 
analysis inner loop sieve benchmark self self number cycles inner loop necessary instructions extra moves ops type tests array access auxiliary value type represent result type test add assignment type type tested branch add kill kill type 
type tests optimized standard value propagation single definition type reaches type test type test eliminated 
standard dataflow techniques full fledged type analysis sacrifices precision value propagation loses value type definition reaches point type analysis keeps track exact union values 
recall vast majority type tests involves single type branch uncommon page 
definition type reach type tests loss information relative type analysis occur 
believe adding standard dataflow analysis compiler eliminate redundant type tests full type analysis system 
reoptimization effectiveness performance numbers self far included certain amount unoptimized code recompilation system optimize single method program 
faster programs run system optimized time spent unoptimized code 
table answers questions 
time spent optimized unoptimized code measured unix kernel profiling facility repeated benchmark times recompilation turned get accurate data 
programs unoptimized code represents total execution time 
programs spending time unoptimized code primmaker ui dynamic inheritance di reduces effectiveness optimizing compilers comparison 
dynamic inheritance object change inheritance structure fly changing set methods inherits 
current self compilers inline sends may affected dynamic inheritance lookup results aren compile time constants di 
di poses problems recompilation system result recompile method affected di 
discuss exact reason require detailed explanation way dynamic inheritance currently implemented 
problem fixed chose keep implementation simple wait reorganization implementation di di simpler handle recompilation system 
geomean tree oo tree towers oo towers sieve quick oo quick queens oo queens puzzle perm oo perm oo bubble oo bubble total execution time inlined type tests dispatch type tests 
self type testing overhead stanford benchmarks benchmarks time spent unoptimized code richards deltablue smallest benchmarks spent time compiled methods 
example richards spends time compiled methods 
larger programs top compiled methods combined usually represent execution time 
speed unoptimized code largely irrelevant benchmarks measured spend time unoptimized code 
example possible replace non optimizing compiler interpreter losing performance 
size compiled code despite additional inlining programs generated self code larger generated compilers 
shows size compiled code self upper bar benchmark self lower bar relative self nofeedback 
average self code smallest smaller self nofeedback code 
difference probably result self better back self nofeedback generates register moves redundant loads stores 
self code biggest average bigger self nofeedback bigger self 
additional inlining enabled type feedback lead larger code 
example optimized self code typeinf larger self nofeedback 
typeinf cecilcomp shows substantial increase optimized code additional inlining reduce code size 
example richards substantially smaller compiled self 
second unoptimized code dense optimized code see section 
unoptimized parts program take space optimized 
compiler configuration measurements aggressively relatively small portion program remains unoptimized 
aggressive recompilation code remain unoptimized code size grow 
summary type feedback successful optimizing self programs self compiler improves performance factor suite large self applications 
compared self previous self compiler self times faster self significantly complicated performs back optimizations 
type feedback reduces call frequency programs factor 
counter recompilation time unoptimized code cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui median table 
time taken unoptimized code system effective finding time critical parts applications applications spend time unoptimized code 
send self dynamically dispatched dispatch overhead optimized programs modest 
self implements message dispatch type test sequences pics chapter inlined code 
self compiler perform extensive optimizations aimed eliminating type tests execution time overhead type tests small benchmark suite including tests method dispatch 
type feedback reduces type testing overhead compared system feedback 
inlined type tests short path length average type test sequence performs comparisons branching target code 
type feedback help reduce performance variability experienced systems static type prediction type analysis 
systems small change large performance impact change removes important piece type information causes type prediction fail 
implementation type feedback obtains information running program dependent static information small change usually fundamentally change set optimizations compiler perform 
type feedback simple technique significantly speeds object oriented programs 
hope results lead implementors object oriented languages systems similar implementation characteristics consider type feedback attractive implementation technique 
type feedback system hear 
please drop line urs cs stanford edu 
geomean ui ui typeinf mango cecilint cecilcomp richards primmaker deltablue code size relative self nofeedback optimized code unoptimized code 
code size self self bigger self self 
hardware impact performance dedicated hardware improve performance object oriented programs 
previous studies indicated various hardware features improve performance implementations objectoriented systems relied heavily hardware support 
example ungar reported soar system slower register windows slower instructions tagged arithmetic 
williams wolczko argue software controlled caching improved performance locality objectoriented systems 
xerox dorado smalltalk long time fastest smalltalk implementation available contained microcode support large portions smalltalk virtual machine 
systems optimizing compiler comparable self results previous studies may valid current self implementation 
order evaluate architectural implementation features benefit execution analyzed instruction usage programs compiled self 
compare self instruction mix specint benchmark suite consists programs written evaluate features sparc architecture register windows tagged arithmetic commonly believed improve performance object oriented programs 
examine cache behavior benchmark programs 
results indicate programs compiled self differ remarkably little optimized programs suite programs 
previous studies find object oriented hardware feature improve performance 
instruction usage designing architecture tasks identify frequent operations optimize 
previous studies execution characteristics object oriented languages different argued need object oriented architectures 
example smalltalk studies shown calls frequent languages 
hybrid language core shouldn different significant differences 
table shows data study calder measured large applications compared specint suite programs 
study gnu compilers mips architecture 
similar spirit differences execution behavior pronounced example executes times calls executes half conditional branches 
numbers expect pure object oriented language self away language radically different 
recall example integer addition statements involve message sends 
execution behavior function source language characteristics compiler technology big difference 
shows dynamic instruction usage specint integer benchmarks written self programs measured 
data summarized box plots appendix contains detailed data 
leftmost box category represents self execution compiled self code excluding time spent runtime system written 
middle box category filled gray represents complete self programs specint ratio spec basic block size call return frequency instructions conditional branch table 
differences user mode instructions 
measured self programs call routines runtime system example allocate objects call functions defined libraries 
programs spend third time routines wanted sure data biased execution behavior non self code 
hand showing self misleading data may representative instructions processor executes 
benchmarks specint suite individual data points shown directly box plots dotted 
specint data taken cmelik 
reveals interesting points 
differences spec benchmarks self 
differences individual spec benchmarks bigger difference self graph spec boxes usually larger corresponding self boxes 

average self programs execute ops unconditional branches logical instructions 
differences explained simple back current self compiler explained detail 
artifact current compiler back linked object oriented nature self 
box plot summarizes distribution showing median horizontal line box percentiles box percentiles vertical lines 
load store cond 
branch 
branch call return nop sethi arithmetic logical shift compare total instructions 

dynamic instruction usage specint vs self benchmarks self excluding runtime system self specint dots individual benchmarks 
self basic blocks similar size spec programs instructions vs spec benchmarks geometric means 
calls returns frequent occurring instructions self instructions self vs instructions specint 
interestingly self runtime system written making heavy inline functions higher call return frequency self code 
accurate comparison exclude instruction categories distorted deficiencies back current self compiler 
back fill delay slots fixed code patterns self programs contain ops vs 
optimize branch chains rearrange code blocks avoid unconditional branches code contains unconditional branches vs eliminated better back 
presence extra instructions distorts frequencies instruction categories exclude ops unconditional branches graphs 
shows adjusted execution frequencies 
comparing self self appears runtime system influence behavior 
instruction groups showing significant difference conditional branches sethi instructions 
conditional branches frequent self self uses non standard calling convention performs call direct jump perform message dispatch currently unable measure call frequency directly call return frequency 
call return frequency frequency call instructions combined 
call frequency half call return frequency self case call frequency half call return frequency calls involve call 
percentages geometric means frequencies 
load store cond 
branch call return sethi arithmetic logical shift compare total instructions excluding ops 
branches 

dynamic instruction usage specint vs self benchmarks excluding ops unconditional branches self excluding runtime system self specint dots individual benchmarks self unusually high compared spec programs 
sethi instructions load bit constants frequent compiled self code reasons values true false nil objects self represented bit code constants contrast programs short immediates 
second object types represented address type descriptor implementation message dispatch compares receiver type expected type 
message dispatch frequent bit constants 
compared spec benchmarks self shows differences 
differences conditional branches sethi instructions mentioned logical instructions comparisons show significant difference 
logical instructions frequent self reasons 
compiler back uses simple register allocator result generates unnecessary register moves 
move instructions account roughly half logical instructions self 
second self uses tagged object representation lower bits tag 
dispatch tests involving integers integer arithmetic operations test argument tag instruction similarly runtime system garbage collector extracts object tag 
instructions account logical instructions 
unable explain remaining difference self programs detailed data logical instructions available 
self executes fewer comparisons spec integer benchmarks 
result surprised expected self execute comparisons message dispatch involves comparisons quite frequent 
self indirect function calls implement message dispatch explain lower frequency conditional branches object oriented programming style typically replaces switch statements dynamic dispatch calder observed effect comparing programs spec programs 
self implementation indirect function calls explain difference way 
possible self optimizer eliminates dispatch type tests lower frequency comparisons 
table summarizes main differences instruction usage 
differences self programs spec programs surprising result considering languages different 
surprising self closer 
summarizes data measurements 
category self closer spec 
example basic block size times higher spec self times higher spec 
difference due differences sparc mips compilers calder compiler technology gnu 
apparently execution behavior function compiler technology source language characteristics 
believe difference object oriented languages disappear compilers oo specific optimizations 
course systems implemented moves implemented instruction included logical instructions 
instruction category frequency relative spec reasons difference call return higher self different programming styles possible compiler deficiencies self logical instructions higher extra register moves inferior back integer tag tests instructions unknown third factor see text sethi load bit constant higher true false nil bit constants message dispatch involves comparisons bit constants comparison instructions lower unknown see text table 
summary main differences self specint certain better optimization bring object oriented languages closer light data claims regarding necessity architectural support object oriented language regarded caution system study employs state art optimization techniques optimizations shown ineffective particular language 
register windows sparc architecture defines set overlapping register windows allows procedure save caller state switching new set registers 
long call depth exceed number available register sets save performed cycle memory accesses 
free register set save instruction executed register window overflow trap occurs trap handler transparently frees set saving contents memory 
similarly window underflow trap reload flushed register set memory needed 
register windows originally proposed part berkeley risc architecture intention making procedure calls fast possible 
variation procedure call depth significantly exceed number available register windows usually current sparc implementations calls executed memory accesses 
register windows self 
shows box plots relative overhead register window overflows underflows unoptimized self self richards deltablue function number register windows available 
overhead measured percentage time spent program user mode instructions 
base time excludes cache overhead time spent window trap handlers 
overhead means execution time twice long infinitely large set register windows 
truncated graph order lose resolution interesting points corresponding actual system self machine windows available user programs 
expected window overhead decreases available register windows fewer overflows underflows 
windows number windows available sparc implementations median overhead self 
overhead programs varies widely 
versions richards overhead zero exceeds call depth compiled sun cc overhead apparently cfront compiler performs inlining gnu 
similarly overhead deltablue varies depending compiler version virtual 
surprisingly unoptimized self programs extremely high overhead routinely call depths deeper 
register windows median overhead unoptimized self code 
instructions conditional branch call return frequency basic block size value normalized spec spec self 
differences self spec programs comparison soar architecture windows registers ungar reports calls caused register window overflow 
unoptimized self ratio 
reason significant difference probably self doesn common control structures smalltalk 
statements loops contribute call depth self smalltalk 
numbers overflow cost cycles underflow cost cycles measured empirically sparcstation running sunos 
window overflow underflow handlers expensive execute supervisor mode leads complications 
example trap handler sure loads stores executed memory access permissions user process kernel 
sparc architecture corrects flaw window trap handlers execute user mode 
modification reduces window handling overhead factor 
instructions trap 
change register window handling overhead small 
reduction factor remaining overhead self negligible overhead unoptimized code modest despite extremely high call frequency call chain depth 
register windows cheap programs high call depths overflow underflow traps reasonably fast 
register windows potential benefits 
simplify compiler somewhat keep track registers need saved function calls minimize resulting register spills 
effect small especially optimizing compiler 
second register windows potentially save load store instructions register spills 
register windows increase self performance 
example equivalent loop nesting depth implemented messages closures 
optimizing self compilers usually eliminate levels 

register window overhead relative overhead number register windows self unoptimized self self compiler lacks register allocator consider register spills measure number memory eliminated register windows 
get rough estimate assuming register windows call store values stack receiver arguments average number arguments taken smalltalk 
non leaf calls save return pc frame pointer words assume save locals 
lacking data assume calls non leaf calls 
average call stores reloads words 
calls representing roughly instructions self increase number instructions loads stores call 
machine loads take cycles stores take overhead terms cycles ignoring cache effects 
unoptimized code relative overhead roughly twice high calls twice frequent 
rough estimate appears sparc register windows high trap overhead improve self performance vs overhead storing restoring values significantly increase execution time unoptimized code vs overhead 
sparc register windows times lower trap overhead performance improved cases optimized code unoptimized code 
machine probably offer loads stores reducing performance advantage register windows optimized code disadvantage unoptimized code 
contrast ungar reports soar slower register windows 
discrepancy stems non optimizing compiler soar perform inlining resulting higher call frequency 
furthermore ungar numbers overestimated benefits register windows number registers saved call assumed number non nil registers method return 
estimates fairly rough appears register windows dramatically improve performance self 
scenario speed optimized code slow unoptimized code 
hardware support tagged arithmetic tagged integer addition subtraction instructions unique sparc feature motivated results soar project 
soar smalltalk slower 
useful system optimizing compiler self 
answer question need look code generated integer addition subtraction self 
assuming integers expression compiled code values saved receiver arguments caller callee assume arguments passed registers argument passing involves memory operations 
data average depth expression stack number arguments sends evaluated consumed ignore possible contribution category values 
number locals probably uncertain factor estimate 
hand optimized methods locals consumed outgoing arguments 
hand variables updated calls need saved restored 
non nil registers held expression stack temporaries evaluated consumed arguments message send live part method need saved call 
estimates rough estimates machine configuration fast traps cycle loads stores fairly stable 
underestimated save restore memory traffic factor advantage register windows optimized code unoptimized code long way reported ungar 
integer instructions test branch unfilled delay slot tagged add temp instruction overflow flag set instruction cond 
branch handle error case overflow non integer argument result temp assign result instruction delay slot branch handle non integer case tagged add instruction sets overflow bit wrong tags nonzero lowest bits addition overflows 
type test type feedback test message may omitted type known general may instructions type test tagged add depending definition method integers 
currently instructions optimized away standard definition 
general integer addition takes instructions self 
ideal code sequence addition tagged addition instruction instruction tagged add result variant tagged add instruction sparc syntax assigns result register integer tags addition overflow 
trap occurs 
self currently variant runtime system need additional mechanisms handle traps recompile offending compiled code necessary avoid frequent repeated traps integer type prediction longer valid 
support hard add 
get upper bound benefits tagged instructions assume ideal system maximal tagged arithmetic support execute integer additions subtractions cycle 
code sequence self sparc tagged instructions integer instructions test cond 
branch unfilled delay slot integer instructions fill delay slot add add temp instruction overflow flag set instruction cond 
branch handle error case overflow non integer argument result temp assign result instr delay slot branch handle non integer case handle non integer case sequence needs additional instructions extra type test total instructions 
tagged arithmetic instructions save maximum instructions arithmetic operation 
code sequences assume unsophisticated back self better optimization code sequences shortened 
example better code sequence tagged instructions fill delay slots tag test oring eliminate extra assignment savings instructions 
comparison assume instruction sequence integer addition subtraction 
addition integer additions subtractions tagged instructions useful integer comparisons integer tag tests verify index array indexing operation integer 
integer tag test operations assume overhead cycles test branch unfilled delay slot tagged add 
number cycles saved tagged arithmetic instructions number adds number subtracts number integer tag tests 
table shows frequencies operations self assuming take cycle 
integer arithmetic instructions rarely represent instructions average integer tag tests frequent median 
hardware support tagged integers estimate predicts self slower system making maximal tagged instructions 
true savings smaller estimate simplifying assumptions tend inflate benefits tagged arithmetic instructions counting instructions cycles overestimate savings code sequences free memory accesses execute close cpi cycles instruction cpi closer machine 
sequences replacing tagged instructions consume instructions cycles 
exact extent overestimation course machine dependent believe non tagged instructions execute average cpi superscalar architectures high branch penalties branches extremely predictable integer type test succeed overflows rare 
furthermore existing superscalar sparc implementations execute tagged instructions parallel instructions trap tagged instruction consumes time instructions 
assumed compiler type information arguments integer additions subtractions explicitly test tags integer operations 
overestimates cost explicit tag checking argument may constant incrementing loop index known 
reasons consider estimate upper bound benefits tagged instruction support programs benchmark suite 
system accurate upper bound execution replacing integer addition sequence trapping tagged add requires optimization folding tag test compiler recognize instructions dispatch test integer tag test addition argument tag test 
fact instructions tests result current definition self method addition definition changed user compiler code sequence 
assume instruction sequence system tagged instructions assume instruction sequence folded tag tests system tagged instructions 
comparisons differ subtractions check arithmetic overflow 
frequencies relative system making maximum tagged instructions 
integer additions integer subtractions integer tag tests estimated slowdown hardware support cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui median table 
arithmetic operation frequency estimated benefit tagged instructions time 
appears removing instructions tagged addition subtraction sparc architecture significantly reduce self performance 
result stands marked contrast ungar measurements showing soar slower instructions tagged arithmetic 
big difference 
analyzing ungar data find reasons higher estimate ungar data includes speedups tagged instructions load load class sparc needed self 
including tagged arithmetic instructions ungar estimate 
soar code sequences integer arithmetic tagged instructions slowed soar branch overflow instruction 
instruction code sequences shorter reducing estimated slowdown 
integer arithmetic frequent soar self 
ungar benchmarks represent instructions vs self 
know difference result compiler differences differences benchmarks 
factors explain difference estimates soar estimates 
self significantly benefit tagged addition subtraction instructions sparc architecture self system making ideal instructions save total execution time 
instruction cache behavior part performance analysis measured cache behavior self variety cache configurations 
caches organization machine way associative byte lines write allocate subblock placement data cache cycles penalty 
interesting result analysis medium sized caches self spends significant amount time instruction cache misses relatively little time data cache misses 
shows execution time overhead relative system infinitely fast memory system example overhead means system runs times slower ideal system 
instruction cache system instruction cache overheads range virtually zero small benchmarks richards deltablue cecilint median 
sizes measured doubling cache size roughly halves median overhead 
comparing results simulation way associative cache determined misses dominated capacity misses conflict misses increasing cache associativity help 
programs benchmark suite large kbytes code appears cache big hold programs working sets 
cache reduce overhead modest levels median 
self code larger self self nofeedback seen section 
increased code space contribute cache overhead 
obtain approximate answer measured cache behavior self system generating smallest code 
instruction cache overhead self half self cache size lower larger caches 
course programs higher frequency integer arithmetic stanford integer benchmarks benefit tagged arithmetic instructions 
class programs amenable optimizations eliminate type checks see section 
example load class instruction handled equally fast self normal load instruction see section 
time overhead ratios comparing ratios misleading 
write misses free assume absorbed write buffer write cache 
furthermore data ratios directly related execution time customary relative number data density loads may vary program program 
appendix starting page contains ratios programs 
cache system self slower system ideal memory system opposed slowdown self 
despite increased cache overhead self runs considerably faster 

time overhead instruction cache misses self execution time overhead cache size cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui 
time overhead instruction cache misses self execution time overhead cache size cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui summarize instruction cache misses substantial performance impact self 
instruction cache base system self runs slower infinitely large cache 
doubling cache size halves performance penalty halving cache size doubles penalty 
size instruction cache larger influence self performance register windows tagged instructions combined 
emphasize large cache overhead part result conservative parameters cache design machine 
particular main memory latency cycles consistent sparcstation cache upper typical caches current workstations table 
felt conservative cache parameters helpful predicting performance systems gap processor cache main memory speeds expected widen 
studies shown cache overhead scientific commercial workloads similar overhead measured self 
example report cache overhead total execution time specint benchmarks tcp benchmarks running dec alpha system 
data cache behavior shows data cache overhead smaller 
data cache median overhead relative infinite cache 
doubling cache size usually reduces median overhead factor 
data cache overhead modest median 
quite surprising programs allocate mbytes data 
data consistent diwan measured allocation intensive ml programs low data cache overheads cache organization write allocate subblock placement 
data confirms reinhold investigated cache performance large lisp programs 
diwan measured data cache overhead ml programs increases substantially policy cache allocate cache line write true self see 
cache write increases cache overhead median factor factor increases larger caches factor cache 
write write ratio truly staggering ranging 
cache barely decreasing larger caches see 
cost associated write misses model interesting look extremely high ratios help explain write inferior cache policy systems self 
write ratio high way objects allocated system copying generational garbage collector see 
typically systems allocate objects consecutively creation space cycles mbyte external cache approximately cycles main memory see table appendix machine cache parameters memory latency cycles instruction data machine way way sparcstation unified way sparcstation way way decstation way way hp way way table 
cache parameters current workstations 
scavenge creation space empty 
system allocate object 
time overhead data cache misses self write allocate subblock placement execution time overhead cache size cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui 
time overhead data cache misses self write cache execution time overhead cache size cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui increment pointer marking start available space pointer points creation space scavenge initiated live objects creation space area memory 
creation space larger caches kbytes self 
memory locations newly allocated object virtually guaranteed cache allocator touched contiguous block kbytes time object allocated address 
stores initializing object cache write cache words object initialized read time read cause additional cache allocates cache line 
contrast write allocate cache allocate cache line write subsequent accesses object cached 
cache uses subblock placement cache penalty incurred current contents corresponding newly allocated line need read memory subblocks containing invalid obviously cache behavior improved prefetching executing load bring line cache 
non blocking loads prefetching possibly allow write caches approach ratios write allocate caches 

write ratio self write cache ratio cache size cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui creation space free space objects allocated object allocated 
object allocation self direction allocation data marked 
subblock placement old contents memory read unnecessarily overwritten initializing stores 
wilson argue allocation behavior result especially high cache overheads direct mapped caches allocation pointer sweep entire cache evict single cache line 
caches associative size creation space smaller cache size 
test hypothesis measured version self kbyte creation space determine cache impact 
result showed desirable run self small creation space execution time increased roughly factor increased garbage collection overhead 
kbyte creation space times frequent times expensive scavenge root processing time remains constant object survival rates increase shorter time intervals 
furthermore experiment confirm basic assumptions wilson data read ratio substantially higher system benchmarks cache sizes 
example caches median ratio doubled 
attribute increase increased data scavenger pollute program cache data prove hypothesis 
consistent fact instruction cache ratio half system change cache parameters 
reduced cache misses result frequent occur scavenger instructions may remain cache resident 
scavenger fairly small better cache behavior system 
data cache behavior self fairly despite high allocation rates 
data accesses go objects created sequential nature allocation accesses hit cache 
kbyte data cache median cache overhead 
possible improvements results performance evaluation possible improvements speed self 
table shows list improvements self 
gains hardware improvements entries speedups listed table rough estimates measured overheads estimate fraction eliminated 
main areas improvement better back incorporating standard known code optimizations 
estimate inspecting code generated current compiler speedup optimizations typically bring compilers languages 
frequency unfilled delay slots extra register moves branch chains estimate relatively simple postpass generated code bring speedup 
see appendix detailed figures page 
source improvement see section estimated speedup software improvements better back register allocation peephole optimization eliminating unnecessary type tests reducing code size better inlining decisions interpreter hardware improvements instruction cache sparc register windows table 
sources possible performance improvements extensive optimizations may compatible interactive system compile pauses increase see chapter 
reducing instruction cache misses 
self spends time cache misses cache reducing misses bring significant performance benefits 
technique help reducing code size system making better informed inlining decisions similar proposed dean chambers account size inlining candidate potential speedup 
system possibly reduce code size significantly increasing number instructions executed reduce execution time reduced cache overhead 
data judge approach successful 
taken speedup estimates reveal speed self improved significantly expense complicated compiler slower compilation 
course estimated speedups additive example software techniques succeed halving code size cache bring additional speedup 
combined speedup software techniques substantial appears promising area research 
self compiler reduces self pure message passing language model programs instruction usage similar spec integer benchmarks 
factoring effects relatively naive code generator generating ops register moves significant difference self call frequency times higher 
basic block sizes similar 
surprisingly programs compiled self differ programs spec suite programs 
believe difference object oriented languages disappear compilers oo specific optimizations 
data cache behavior self small caches data cache memory latency cycles relatively high latency median overhead compared ideal system infinitely fast memory 
reducing size allocation area fit cache size improve performance occur smaller allocation area 
reason data cache misses decrease smaller allocation area 
instruction cache overhead substantially higher median overhead instruction cache 
doubling halving cache size roughly halves doubles overhead 
hardware features smaller impact performance 
register windows simplify compiler probably speed compilation improve performance sparcstation window handler traps expensive handle 
cheaper window traps specified upcoming sparc implementations register windows improve self performance estimated 
performance benefit tagged integer arithmetic instructions smaller compared system making ideal instructions require sophisticated compiler back self special instructions slower 
compiled self code similar code run efficiently standard hardware usually designed programs 
light results believe arguments favor architectural support specific object oriented language regarded caution system study employs state art optimization techniques optimizations shown ineffective impractical particular language 

responsiveness previous chapter examined self performance terms speed compiled code 
interactive environment intended rapid prototyping raw runtime performance aspect performance 
aspect responsiveness quickly system react programming changes quickly programmer try new piece code typing 
self uses runtime compilation interpretation slow compile pauses impact system 
example time menu popped code drawing menu compiled 
runtime compilation may create distracting pauses situations 
similarly adaptive recompilation may introduce pauses runs code gets optimized 
chapter explores severity compilation pauses variety measurements compilation speed distribution compile pauses pauses experienced actual interactive session 
part definition pause clustering new method characterizing pauses takes account way pauses experienced user 
addition measure influence parameters recompilation system performance variations final performance 
measurements previous chapters performance numbers chapter obtained simulator measurements involve interactive system 
measured cpu times idle sparcstation mbytes memory 
due cache related performance fluctuations discussed chapter measurements probably accurate 
obtain measurements ran instrumented versions self 
individual compilation times measured comparing process cpu time compilation 
data section obtained pc sampling interrupting program times second inspecting program counter determine system compiling executing code time interrupt 
results samples written log file 
instrumentation slowed system 
pause clustering main goals chapter evaluate self interactive performance measuring compile pauses 
constitutes compile pause 
tempting measure duration individual compilations measurements lead overly optimistic picture compilations tend occur clusters 
compilations occur back back perceived pause user counted single long pause shorter pauses 
individual pauses short user may notice distracting pauses compilations occur quick succession 
goal characterize pause behavior experienced user pause defined way correctly handles non uniform distribution pauses time 
pause clustering attempts define pauses interaction centered physiological way 
pause cluster time period satisfying criteria 
cluster starts ends pause 

pauses consume cluster time 
small pauses occur short succession lumped long pause cluster long pauses consume half cpu time interval 
believe limit conservative system making progress half normal speed user may notice temporary slowdown 
course latency vs throughput problems known areas operating systems 

cluster contains pause free interval longer seconds 
groups pauses grouped rules separated half second assume perceived distinct pauses lump 
clear events separated half second distinguished 
shows example 
pauses clustered total execution time time period rule 
similarly short pauses grouped long pause forming long pause cluster second 
clusters won fused big second cluster resulting cluster satisfied rule example separated pause free period seconds rule 
example illustrates pause clustering quite conservative may overestimate true pauses experienced user 
believe pause clustering realistic measuring individual pauses 
furthermore hope approach strengthen results measured pause behavior despite conservative methodology 
hope inspire example implementors incremental garbage collectors similar approaches characterizing pause times 
shows effect pause clustering measuring compile pauses 
graph shows number compile pauses exceed certain length sparcstation 
ignoring pause clustering reported pauses self exceed milliseconds exceed seconds 
pause clustering combined pauses exceed seconds 
clustering pauses magnitude difference 
reporting individual pauses result distorted picture 
course parameter values pause clustering cpu percentage intergroup time affect results 
example increasing pause percentage results optimistic 
results fairly insensitive changes parameter values 
particular varying pause percentage qualitatively change results doubling intergroup time second 
compile pauses section measures pauses occurring interactive session self user interface 
pauses measurements pause clustering specifically mentioned 
term pause means clustered pause 
pauses interactive session measured compilation pauses occurring minute session self user interface 
session involved completing self tutorial includes browsing editing making small programming changes 
tutorial discovered bug cut paste code session includes real life debugging 
pause clustering may conservative compilation pauses ignores execution speed self interpreter slow causes distracting interaction pauses 
sake simplicity assume interpreter fast interactive 

individual pauses resulting pause clusters time seconds individual pauses pause clusters shows distribution compile pauses experiment 
thirds measurable pauses tenth second second 
shows data absolute terms bars truncated show detail rest histogram 
ms lower threshold perceptible pauses pauses exceeded threshold 
similarly second lower threshold distracting pauses pauses minute run 
see table appendix details 
obtained data sampling system hz short compilations omitted counted pause second 

distribution individual vs combined compile pauses compilations exceeding length pause length seconds sparcstation individual pauses combined pauses 
distribution compile pause length compile pauses pause length seconds sparcstation pause clustering addresses short term clustering compile pauses 
pauses non uniformly distributed larger time scale 
page shows pauses distributed minute interaction 
pause represented spike height corresponds clustered pause length axis shows elapsed time 
note axis range larger axis range orders magnitude graph visually spikes height proximity 
run substantial programs started scratch precompiled code 
working set changes caused flurry compilations visible groups spikes 
initial group includes longest pause corresponds starting user interface group represents phase 
compile pauses minute interaction number occurrences pause length seconds sparcstation 
long term clustering compilation pauses pause time seconds sparcstation elapsed time seconds starting tutorial program hits bug debugger comes debugger find bug starting system tutorial user interface code exercised time 
groups correspond invoking debugger discovering bug inspecting stack find cause error 
system pause behavior sparcstation adequate especially considering workstation considered low today summer 
interactive behavior isn non optimizing deutsch schiffman smalltalk compiler pauses shorter traditional programming environments batch style compilation 
pauses faster systems practicality optimizing compilation interactive system strongly dependent cpu speed 
system significantly slower specint sparcstation probably pause times distracting 
system impractical machines commonly deutsch schiffman smalltalk compiler developed order magnitude slower 
hand system interactive behavior improve faster cpus 
today workstations pcs significantly faster sparcstation measurements see table 
investigate effect faster cpus re analyzed trace parameters chosen represent current generation workstation pc times faster sparcstation workstation times faster 
compares sparcstation pauses simulated systems 
pause length graph shows number pauses exceeding length 
note graph faster machine just original graph shifted left may combine compilations combined slower system 
example groups compilations second apart sparcstation seconds apart current workstation combined single pause see rules section 
impact effect quite small confirms earlier observation pause clustering fairly insensitive value time parameter 
current generation workstation pauses exceed seconds 
numbers confirm informal experience self running current generation sparcstation machines pauses noticeable rarely distracting 
faster generation workstation eliminate virtually noticeable pauses pauses longer seconds 
machine current self system feel workstation vendors expected announce specint workstations year workstations available 
see table appendix details 
system specint higher faster absolute relative ss sparcstation mhz pentium pc mhz powerpc macintosh sparcstation high workstation expected table 
speed workstation pcs optimizing interpreter 
pauses may short real time animation video human eye spot pauses seconds situations 
arguments kind faster machine better misleading especially assume problem size program size remains constant 
believe argument escapes fallacy length individual compilations depend program size program input size compilation unit size method methods inlined optimization 
people programming styles change average size methods remain individual pauses shorter faster processors 
furthermore noted simply divide pause times speedup factor trace combining individual pauses longer pauses correctly accounting shorter times 
larger programs may prolong time needed recompilation settle see section experience program size influence clustering individual compilations 
words larger programs may cause pauses lengthen pauses 
believe safe predict system perceived user improve faster processors shown 
summarize characterize compilation pauses self system noticeable occasionally distracting previous generation systems noticeable distracting current generation systems virtually unnoticeable generation systems 
combined adaptive recompilation optimizing compiler coexist interactive programming environment 
starting new code responsive system able quickly start new programs program parts 
example time user positions cursor editor corresponding editor code compiled 
starting precompiled code similar continuing programming change change invalidates previously compiled code 
example programmer changes methods related pop menus tries test change corresponding compiled code generated 
measuring time needed complete small program parts user interface interactions precompiled code characterize behavior system typical debugging session programmer changes source code tests changed code 

compilation pauses faster cpus number pauses exceeding length pause length seconds sparcstation current faster faster order evaluate benefits combining fast non optimizing compiler slower optimizing compiler measured time taken execution common user interactions displaying object opening editor 
typical actions long compilation pauses especially distracting 
sequence system started empty code cache 
table shows individual interactions sequence 
measurements interactions executed sequence shown table activities trivial actions placing cursor text editor 
sequence starts empty code cache interactions may reuse code compiled previous interactions 
interactions measure quickly system reacts changes definition methods worst case redefining integer addition method 
points worth noting 
self usually executes interactions fastest average times faster self times faster self nofeedback geometric means medians 
fairly large variations tests run faster self number times faster respectively tests number run slower systems recompilation 
adaptive recompilation introduces certain variability running times slowing interactions recompilations 
factors contribute results 
self usually fastest non optimizing compiler saves compilation time 
dynamic recompilation introduces certain variability running times slowing interactions 
happen recompilation timid time spent unoptimized code aggressive methods recompiled early recompiled 
description execution time sparcstation self self nofeedback self start user interface display initial objects dismiss standard editor window dismiss lobby object show slot point object open editor slot comment dismiss editor sprout coordinate integer sprout parent object traits integer display slot sprout method dismiss method open editor method change method changing definition integer addition reopen editor method undo previous change changing definition integer addition back original definition dismiss traits object geometric mean ratios relative self median table 
ui interaction sequence 
self faster self type feedback allowed design kept simpler compromising performance compiled code 
rows show quickly system recover massive change cases large amount kbytes compiled user interface code needed regenerated definition integer addition changed 
integer addition method small frequently inlined compiled methods compiled methods discarded change 
adaptive recompilation allowed system recover seconds recall table page user interface consists lines code counting general system code collections lists 
time included time accept parse changed method dismiss editor update screen react mouse click 
compared self dynamic optimization buys speedup case compared self speedup factor 
course subsequent interactions may slower result change code needs recreated 
example opening editor row takes times longer change row 
adaptive optimization small pieces code compiled quickly small changes program handled quickly 
compared previous self system self incurs significantly shorter pauses average interactions run times faster 
performance variations program part program run repeatedly performance vary time 
initially run slowly code unoptimized compilation recompilation occurs 
time performance stabilize asymptotic value 
section investigate execution behavior benchmark programs changes time different compiler configuration parameters affect behavior 
questions addressed performance stabilize programs differ respect 
configuration parameters influence quickly performance stabilizes slope performance curve 
configuration parameters influence final performance asymptote performance curve 
table shows main configuration parameters function 
parameter function max 
compiled method size limits maximum size method limits maximum length compilation inlining limits determining calls inline see section smaller values reduce individual compilation times individual compiled methods smaller compilation time inlining usually means code duplication recompilation limit determines methods trigger recompilation section half life time determines quickly invocation counts decay time section max 
compilation percentage limits total cpu time compilation may percentage sliding second interval low values limit recompilation start situations table 
configuration parameters impacting compilation pauses self start behavior large programs previous section characterized pauses caused re compiling small pieces code 
section investigates happens large programs compiled scratch 
see performance develops time benchmark started empty code cache repeatedly executed times 
benchmarks fairly large test runs kept short seconds optimized code 
runs dominated compilation time large body code compiled optimized 
example typeinf run takes minute tenth run takes seconds 
runs compilations die execution time stable programs ui experiences flurry compilation run 
initial peak substantial typically execution runs times slower th run 
initial peak execution time 
page breaks start phase benchmarks compilation execution 
initial execution time ui consumed non optimizing compilations slow running unoptimized code ui optimizing compilation dominates execution time 
reduce initial peak execution time ui non optimizing compiler substantially faster generate better code 
contrast optimizing compilation dominates start phase typeinf lesser extent cecilcomp 
cecilint lies optimizing compilation consumes minor portion run major portion second run 
summary reasons high initial execution time vary benchmark benchmark single bottleneck 
programs start time correlate program size expect larger programs take longer reach stable performance code re compiled 
page shows stabilization time benchmarks plotted programs sizes 
stabilization time compilation time incurred program reaches knee initial start peak 
expected correlation exist general larger programs take longer start 
correlation perfect expected 
example large program spends time small inner loop quickly reach performance small portion needs optimized 
knee determined informally interested qualitative picture precise quantitative values 

overview performance development execution time seconds sparcstation iteration cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui start compilation correlated program size measurements characterize startup behavior system depending program size execution time table 
programs small start time start behavior small programs 
programs run long time start behavior initial compilations hidden long execution time 
large program size small big execution time short long table 
start behavior dynamic compilation 
start phase selected benchmarks execution time seconds sparcstation iteration number ui execution time seconds sparcstation iteration number cecilint execution time seconds sparcstation iteration number cecilcomp execution time seconds sparcstation iteration number unoptimized code optimized code primitives fast compiler optimizing compiler typeinf programs execute short time start costs dynamic compilation hidden current self system 
benchmarks fall category inputs chosen keep execution times short simulated reasonable effort 
real life expect large programs run longer start behavior better benchmarks 
performance stability initial flurry compilations tailed programs experience performance variations caused compilations 
remainder section clip axis graphs seconds order show detail performance variations runs 
shows development performance benchmarks run standard system 
benchmarks show little variation execution time run converge run 
benchmarks converge especially quickly stable performance level show subsequent variations 
group consists richards deltablue smallest benchmarks mango 
behaved configurations measured benchmarks excluded graphs reduce visual clutter 
remaining group benchmarks shows peaks execution time start phase variations usually small 
cases peaks correspond recompilation overhead 
iterations ui cecilcomp reached stable performance iterations benchmarks reached stability 
remaining peaks fairly small order second example ui shows peaks second iterations 
performance fairly stable start phase 
graphs tends exaggerate performance volatility system individual execution times time taken single run short small absolute compilation overhead registers significant peak 
shows difference measurement interval presents data groups runs simulating effect longer benchmarks 
longer benchmark runs performance stable exception initial slowdown appear smaller 
believe keeping improve graphs legibility separately show execution compilation time 
small variations caused sources compilation significantly alter appearance graphs 

correlation program size time stabilize performance 
stabilization time seconds sparcstation program size lines code cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui individual measurement intervals short right way measure variability reflects time taken typical actions interactive system 
intuitively visual height peaks indicates disturbing peak interactive small peak seconds barely noticeable peak seconds certainly measurements shown chapter entirely reproducible exact event sequence varies run run 
example invocation counter decay implemented process wakes cpu seconds 
clock interrupts arrive invocation counter values recompilation decisions may vary run run 
shape different runs usually similar 
graphs section repeated measurement times ranked resulting graphs third ranked middle graph 
shows graph ranked worst configuration 
ui graphs similar 
ui behaves differently large recompilations run 
performance variations self execution time seconds sparcstation iteration cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui 
performance variations subset benchmarks self execution time seconds sparcstation iteration cecilcomp cecilint primmaker typeinf ui ui 
difference apparently extreme case tried investigate cause difference reproduce behavior 
summary large programs initially run slowly aren optimized time spent compilation 
compilation time spent start period roughly proportional program size 
minutes sparcstation programs reached stable performance 

alternate visualization data execution time seconds sparcstation iteration cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui 
performance variations subset benchmarks self worst run execution time seconds sparcstation iteration cecilcomp cecilint primmaker typeinf ui ui seconds influence configuration parameters performance variation searching configuration standard system experimented wide range parameter values 
establish hard rules rules exceptions predict influence parameter changes trends invocation count decay significantly reduces variability reducing number recompilations 
half life time influences variability closer infinity decay variable execution times 
decay performance really stabilizes 
increasing inlining limits leading aggressive inlining tends produce higher peaks longer recompilation pauses 
effect pronounced 
increasing recompilation limit invocation count value recompilation triggered start phase time spent unoptimized code 
reduce performance variability 
reducing percentage cpu time available compilation tends lengthen start periods somewhat flattening initial peaks 
counter decay influence stability particular difference decay decay striking 
shows variability system half life time infinity seconds 
programs show significantly higher variations 
programs converge stable performance level iterations 
intuitively reason behavior clear decaying invocation counts single method eventually exceed recompilation threshold optimized 
performance stabilizes methods left recompile 
influence configuration parameters final performance influence configuration parameters performance fairly clear cut general changes improve performance increasing inlining limits reason reducing invocation limit 
performance variations invocation counters don decay execution time seconds sparcstation iteration cecilcomp cecilint primmaker typeinf ui ui reducing decay factor moving closer 
measure impact factors varied invocation limit half life time measured resulting execution time 
combination parameters chose best execution time repetitions benchmark normalized time best time benchmark 
parameter configuration resulting best performance particular benchmark receives value 
value parameter combination mean combination results execution time times longer best parameter combination 
times measured sparcstation 
shows resulting performance profile averaged benchmarks data clipped true value worst parameter combination 
parameters behave expected increasing invocation limit decreasing half life increase execution time smaller part application optimized fewer methods recompiled 
performance profile bumpy showing performance vary monotonically parameter varied 
bumps partly result measurement errors recall variations caused cache effects sparcstation high partly result element randomness introduced timer decaying 
effect particularly strong bad parameter combinations short half life high invocation limit 
shows performance profile cecilcomp profiles measured shows strong variations area half life times high invocation limits 
timer interrupts governing counter decaying routine arrive exactly points program execution loop may optimized run half life time seconds run half life time seconds counters decayed time trigger recompilation 
explanation consistent fact bumps exactly reproducible phenomenon reproducible exact location height bumps 
liked measure performance simulation possible simulations consumed cpu time experiment consumed days cpu time simulation 
furthermore simulator currently simulate timer interrupts 

influence recompilation parameters performance infinity relative execution time invocation limit half life time seconds summary recompilation parameters influence performance predictable ways generally reducing invocation limit increasing counter half life time improve performance 
performance curve fairly flat range parameter values allowing system tuned best compromise interactive responsiveness performance 
compilation speed section measures aspect related interactive performance raw compilation speed 
figures show compilation speed self nofeedback self compilers function total number byte codes processed compilation byte code corresponds source code token 
data points represent compilations occurring run benchmark suite 
example compilation processed total byte codes including byte codes inlined methods ms register dot coordinates 
compilers compile time approximately linear function source size measured number byte codes correlation coefficients self nofeedback self 
compilers speed linear function source length negligible start overhead relative compilation speed expressed single number 
average self nofeedback takes ms byte code self takes ms words self nofeedback compiles times faster 
faster compilation attributed simpler compiler front perform type analysis ambitious back 
addition eliminating message sends type feedback speeds compilation 
compared self nofeedback self order obtain data points optimizing compiler methods compiled non optimizing compiler 
recall systems optimizing compiler 
cecilcomp execute correctly self see footnote page measured self 
submitted journal results observe compilation speed vs ms byte code perfectly correlates compilers source code sizes vs lines 

influence recompilation parameters performance cecilcomp infinity relative execution time invocation limit half life time seconds compiler described chapter self nofeedback times slower byte code see page details 
seen page start phase programs dominated optimizing compilations desirable improve compiler speed 
shows compile time spent self nofeedback 
spent front half time spent performing message lookups making inlining decisions 
lookup time eliminated lookup cache cache mapping lookup keys slot descriptors 
cache implemented message lookups currently perform functions find target slot record set slots objects lookup result depends 
needed invalidate compiled code source code changes 
compiled method includes dependency set containing union lookup dependency sets lookups performed compilation 
object changes compiled methods containing object discarded 
lookup cache store dependency sets lookup results require reorganization dependency system 

compilation speed self nofeedback 
compilation speed self lookup cache compiler third faster back start dominate compilation time perform optimizations 
compiler probably sped somewhat standard programming tricks significant speedups require fundamental design changes 
important source inefficiency compiler perform lot common operations repeated times 
example compiler inline message sends control structure method creating intermediate nodes pseudo registers 
eventually nodes optimized away course process consumes valuable compile time 
promising approach reducing compile time partially evaluate compilation common message sends iftrue 
possible generate templates pre optimized intermediate code messages compilation effort avoided 
unfortunately implementing templates involves nontrivial problems parameterize templates pre optimize independently code embedded example optimization method implementing loop depends heavily knowing type integer value positive negative arguments 
summary discussing pause times imperative measure pauses experienced user 
defined pause metric pause clustering 
pause clustering combines consecutive short pauses longer pause just counting individual pauses 
applying pause clustering compilation pauses self system changes pause distribution order magnitude emphasizing importance pause clustering 
believe pause clustering pause length important example evaluating incremental garbage collectors 
languages object oriented languages need runtime performance interactive performance 
pure object oriented languages task harder need aggressive optimization run acceptable speed compromising interactive performance 
adaptive recompilation system provide runtime performance interactive performance pure object oriented language self 
problems similar problems encountered dean chambers better inlining decisions results previous compilations 

profile self nofeedback compilation message lookup inlining decisions intermediate code generation construct basic blocks def information copy propagation dead code elimination register allocation source level debugging support code generation back previous generation workstation sparcstation fewer pauses exceeded ms minute interaction pauses exceeded second 
faster cpus compilation pauses start unnoticeable current generation workstation pauses exceed ms generation workstation available pause exceed ms pauses exceed ms addition speeding execution programs type feedback speeds compilation 
type feedback allows compiler simpler individual compilations shorter compilation speed increases compared self optimizing self nofeedback compiler compiles times faster source code unit 
recompilation reduces average individual compile pauses compilations performed fast non optimizing compiler take milliseconds 
adaptive recompilation helps improve system responsiveness programming changes 
example takes seconds sparcstation self user interface start responding user events radical change redefining integer addition method invalidates compiled code inlined integer addition 
compared self system adaptive recompilation self improves responsiveness factor case 
large programs initially run slowly started precompiled code 
time required reach stable performance correlates source length longer program longer initial start phase 
general recompilation process settles fairly quickly benchmarks reach stable performance running seconds sparcstation 
worst case large benchmarks execute short time current system hide initial overhead dynamic compilation large programs execute longer minute relative overhead diminishes 
system save precompiled optimized code disk load demand reduce start overhead 
pauses may significant making far reaching changes large programs program development need problem production mode precompiled code 
possible hide compilation pauses better current self system 
dynamic recompilation optimization optional sense optimized code needed immediately 
system decides certain method optimized actual optimizing compilation deferred desired 
example system enter optimization requests queue process user think pauses similar opportunistic garbage collection 
alternatively optimizing compilations performed parallel program execution multiprocessor machine 
increasing speed hardware interpreters dynamic compilers current smalltalk systems may longer represent optimal compromise performance responsiveness 
today practical push better performance widening applicability systems responsiveness 
adaptive recompilation exploits speed contemporary hardware reconcile run time performance compile time responsiveness 
hope encourage implementors object oriented languages explore new region design space resulting new high performance exploratory programming environments object oriented languages 

debugging optimized code self pure message model computation requires extensive optimization achieve performance 
interactive programming environment demands rapid turnaround time complete source level debugging 
merely providing correct execution semantics despite optimization optimal programmer productivity system provide illusion directly executing programmer source code 
words system provide interpreter semantics compiled code speed combining global optimization expected behavior 
existing systems support debugging optimized code 
programs optimized full speed compiled optimizations full source level debugging 
techniques developed strive possible debug optimized code 
systems able provide full source level debugging 
example generally possible obtain values source level variables single step program change value variable 
optimization priority debugging consequently systems provide restricted forms debugging 
contrast willing compromise source level debugging maximize programmer productivity system provide full source level debugging times 
chose restrict optimizations preserve debugging tried keep performance impact debugging small possible 
unfortunately source level state program recoverable point program virtually instruction boundary support single stepping existing techniques severely restrict optimizations performed effectively disabling common optimizations 
solve dilemma relaxing debugging requirements placed optimized code 
requiring source level state available point program optimized code need able recover state relatively points essentially non inlined calls 
programmer restriction invisible code needed provide finer grain debugging 
programs inspected single stepped completely unoptimized 
best knowledge system describe chapter practical system providing expected behavior presence global optimizations compared previous techniques dynamic deoptimization interrupt points permits place fewer restrictions kind optimizations performed preserving expected behavior 
optimization vs debugging code transformations performed global optimization hard debug optimized code source level 
optimizations delete change rearrange parts original program visible user tries debug optimized program 
section presents problems solved provide source level debugging optimized code 
displaying stack optimizations inlining register allocation constant propagation copy propagation create methods activation records direct correspondence source level activations 
example single physical stack frame may contain source level activations message sends inlined 
variables may different locations different times variables may runtime locations 
example shows effects inlining 
physical stack contains activations 
contrast source level stack contains additional activations inlined compiler 
example activation inlined appear physical stack trace 
single stepping single step debugger find execute machine instructions belonging source operation 
optimizations code motion instruction scheduling hard problem instructions statement may interspersed neighboring statements statements may reordered execute source level order 
contrast single stepping simple unoptimized code code statement contiguous 
changing value variable consider code fragment expression compile time constant compiler eliminated computation generated code 
program suspended just assignment programmer changes 
execution optimized code resumed produce unexpected value unoptimized code course problem addition performed compiled code 
changing procedure similar problem arises inlined procedure changed debugging 
suppose program suspended just executing inlined copy function programmer changes bug 
obviously execution simply continue old definition hardwired caller 
hand easy provide expected behavior unoptimized code definition simply replaced subsequent call execute correct code 
deoptimization problems exist unoptimized code 
optimized code converted unoptimized code demand programs debugged easily running full speed time 
self debugging system transformation 
compiled code exists states 
displaying stack physical stack source level stack optimization optimized code suspended relatively widely spaced interrupt points 
interrupt point source level state reconstructed 
unoptimized code suspended arbitrary source level operation supports debugging operations single stepping 
section explains data structures recover source level state optimized program state 
sections describe optimized code transformed unoptimized code demand section discusses interrupt points lessen impact debugging optimization 
recovering unoptimized state display source level stack trace perform deoptimization system needs reconstruct source level state optimized machine level state 
support reconstruction self compiler generates scope descriptors scope contained compiled method initial source method methods inlined :10.1.1.56.2990
scope descriptor specifies scope place virtual call tree physical stack frame records locations values arguments locals see 
compiler describes location value subexpression compiled method 
information needed reconstruct stack evaluated expressions waiting consumed message sends 
find correct scope physical program counter debugger needs know virtual program counter source position pair scope description source position scope 
debugging information generated compiled method includes mapping physical virtual program counters 
help information debugger hide effects inlining splitting register allocation constant propagation constant folding user 
example compiler eliminates variable value compile time constant variable descriptor contain constant 
straightforward extension descriptor structure handle variables values computed values eliminated induction variables 
scope descriptor mechanism originally developed chambers ungar lee reimplemented lars bak reduce memory requirements :10.1.1.56.2990
struct oop method pointer method object caller scope scope inlined int source position caller lexically enclosing scope args descriptors receiver arguments locals descriptors locals descriptors subexpressions struct enum const loc tag compile time constant run time value union oop value constant value location location run time location 
pseudo code declarations scope data structures shows method suspended different times 
method suspended time physical pc corresponding source position line method stack trace display called hiding fact inlined compiler 
similarly time source level view show called called displaying virtual stack frames single physical stack frame 
display complete stack trace process simply repeated physical stack frame 
transformation function needed debugging system transforms optimized method equivalent unoptimized methods 
moment assume topmost stack activation needs transformed stack frames easily removed added section explains remove restriction 
transformation performed follows 
save contents physical activation stack frame transformed remove runtime stack 

mechanisms described previous section determine source level virtual activations contained physical activation values locals virtual pc 
pc scope line scope descriptors physical virtual pc mapping source level stack program suspended time scope descriptors physical virtual pc mapping source level stack program suspended time pc scope line pc scope line pc scope line 
recovering source level state physical stack physical stack pc pc 
stack frame optimized method stack frame optimized method 
virtual activation create new compiled method corresponding physical activation 
simplify transformation function subsequent debugging activities new methods target methods completely unoptimized message send corresponds call optimizations constant folding common subexpression elimination performed 

virtual activation find new physical pc corresponding compiled method 
target method unoptimized exactly physical pc virtual pc 
necessarily case target methods optimized 
initialize stack frames created previous step filling return pc fields needed runtime system frame pointer 

virtual activation copy values parameters locals expression stack entries optimized unoptimized activation 
unoptimized method straightforward translation source method variables mapped locations target activation copied values unambiguous destination 
necessarily case target methods optimized 
furthermore target method unoptimized contain hidden state need initialized value common subexpression 
step new stack frames completely initialized virtual activations transformation complete 
illustrates process 
transformation expands optimized stack frame containing virtual activations sequence unoptimized stack frames creating correspondence virtual physical frames 
parts program actively debugged stepping need transformed 
parts parts program running unoptimized code parts run full speed 
transformations necessary just inspect program state described section 
lazy deoptimization stack frame middle stack new stack frames inserted easily 
solve problem current implementation transforms stack frames lazily 
transforming optimized stack frame unoptimized form optimized method unoptimized methods optimized stack frame stack grows downwards unoptimized stack frames tion deferred control return frame see 
example virtual activation vf inlined frame middle stack immediately 
return address stack frame called changed point routine transform returns 
point topmost frame 
transforming activation simplifies transformation process stack frames need adjusted deoptimization causes stack frames grow size 
lazy deoptimization simplify system considerably may restrict debugging functionality additional steps taken 
self system currently allow contents local variables changed debugging variable runtime location 
order create runtime location variable necessary transform activation middle stack system currently 
fundamental problem example transformed stack frames described 
simpler solution allocate stack locations eliminated variables 
locations unused normal program execution spring life programmer manually changed value eliminated variable 
compiled code depended old supposedly constant value invalidated programmer changed method source code see section 
interrupt points optimized programs interrupted instruction boundary debugging optimized code hard source level state recoverable single point program 
ease restrictions impose optimization optimized self program interrupted certain interrupt points state guaranteed consistent 
notification asynchronous event occurring interrupt points delayed interrupt point reached 
currently self system defines kinds interrupt points method including process control primitives loop bodies backward branches 
definition implies maximum interrupt latency bounded length longest code sequence containing call loop typically dozen instructions 
latency short interrupt points noticed programmer 
sends interrupt points loops calls interrupted 
backward branches need check interrupts loop contains execution path calls loop iteration performs call checks interrupts 
interrupt points need cover possible points program suspended need handle synchronous events arithmetic overflow 
self possible runtime errors interrupt points primitives safe requested operation performed primitive calls user defined error handler usually invokes debugger 
interrupt points systems see section discussion deutsch schiffman smalltalk system 
hennessy similar mechanism define stopping points statement zellweger 

lazy deoptimization stack frames real stack frame includes virtual activations vf vf vf stack grows downwards returns vf real stack frame deoptimization vf vf vf vf vf virtual activation contained real stack frame vf vf vf optimized program suspended current activation necessary carry debugging requests 
unoptimized method source point interrupt point program point 
debugger invoked interrupt points debugging information need generated points 
optimization reduces space debugging information importantly allows extensive optimizations interrupt points 
essentially compiler may perform optimization effects reach interrupt point undone point 
example compiler reuse dead variable register long subsequent interrupt points variable scope 
widely spaced interrupt points fewer restrictions source level debugging imposes optimization 
interrupt points lessen impact garbage collection compiler optimization 
garbage collections occur interrupt points compiler generate code interrupt points temporarily violates invariants needed garbage collector 
updating active methods debugging programmer change value variable definition method 
invalidate compiled code affected change self system maintains dependency links compiled code objects representing source code methods :10.1.1.56.2990
example compiled method contains inlined copies method changed compiled method discarded 
compiled method containing inlined copy changed method active activation simply discarded 
compiled method replaced new compiled method execution continue 
fortunately deoptimization purpose 
active compiled method longer contain inlined methods 
execution continues subsequent calls changed method correctly invoke new definition 
changed method currently active updating activation hard 
fortunately self don solve problem self language model activations created cloning method object 
created clone independent original changes original affect clone 
lazy transformation elegantly solves problem invalidated compiled methods middle stack simply wait invalid method top stack transform 
lazy transformation desirable interactive system spreads repair effort time avoiding distracting pauses 
furthermore handles sequences changes example reading file containing new definitions group related objects 
eager transformation new definition cause affected compiled methods recompiled methods recompiled times affected changes 
lazy transformation compiled methods invalidated repeatedly problem invalidation cheap transformed 
debugging mechanisms trivial support changing running programs 
current implementation consists lines code top previously described debugging functionality code maintaining dependencies 
common debugging operations section describes debugging operations implemented self system outlines possible implementations additional operations 
deoptimization relatively easy implement common debugging operations single step finish operations simple perform unoptimized code deoptimization supply unoptimized code program piece demand 
contrast single step finish generally provided previous systems debugging optimized code 
single step source point interrupt point associated method implementation single stepping trivial 
system current activation restarts process interrupt flag set 
process relinquish control reaching interrupt point executing single step 
finish finish operation continues program execution selected activation returns 
implemented changing return address selected activation stack frame special routine suspend execution activation returns 
program slowed finish operation run optimized code 
selected activation physical stack frame inlined method stack frame lazy deoptimization 
case program run optimized code time lazy deoptimization performed run unoptimized code 
operation called step executes source operation stepping calls 
program source operation completed 
synthesized performing single step possibly followed finish operation call 
consequently implemented lines self code system 
breakpoints self currently supports breakpoints source transformation programmer inserts breakpoint simply inserting send halt source method halt explicitly invokes debugger 
implement breakpoints explicit changes programmer debugger perform source transformation transparently 
value variable changes easy provide self pure objectoriented language accesses performed message sends conceptually compiler usually optimize away sends 
monitor accesses object instance variable rename variable private install new methods monitor accesses assignments respectively return change private 
remember self prototype model allows change single object affecting monitor specific objects easily 
dependency system invalidate code inlined old definition directly accessed changed 
requiring programmer explicitly changes system perform transparently support breakpoints 
general case code need recompiled accommodate new interrupt point system need additional basic mechanisms support functionality 
discussion section discuss strengths weaknesses approach assess generality 
benefits debugging technique important advantages 
simple current implementation transformation process consists lines code top code implementing debugging information described section 
second allows loose coupling debugger compiler know 
third places additional restrictions described section kind optimizations performed compiler 
common optimizations inlining loop unrolling common subexpression elimination instruction scheduling interrupt points affecting 
method suited interactive system incremental usually stack frame needs converted result user command 
current limitations unoptimized code debugging introduces performance problem user decides continue execution 
execution proceed full speed stack frames may unoptimized 
problem usually severe frames running unoptimized code unoptimized code discarded soon frames return 
parts system run full speed 
methods containing loops pose problem remain stack unoptimized form indefinitely 
frames automatically recompiled optimization recompilation system described chapter 
certain optimizations cause problems handled system 
provide full debugging self compiler perform optimizations 
general dead stores eliminated registers dead variables reused spilling variable memory 
similarly optimizations code motion induction variable elimination may performed debugger recovery techniques powerful hide optimizations effects interrupt points 
extent problems depends particular recovery techniques average distance interrupt points 
optimizations performed interrupt point affected variable scope see section 
self compiler perform tail recursion elimination tail call elimination supported transparently general possible reconstruct stack frames eliminated optimizations 
generality debugging approach specific self exploited languages 
system appears require runtime compilation deoptimization systems runtime compilation include unoptimized copy procedure executable dynamically link needed 
pointer safe languages lisp smalltalk pointer safe subset approach directly applicable 
estimate performance impact source level debugging systems depends characteristics particular language compiler techniques recover values interrupt points 
general system interrupt points deoptimization allow faster code system providing full source level debugging deoptimization allows optimized code support subset logically required interrupt points 
pointer unsafe languages allow pointer errors interrupt points closely spaced debugger potentially invoked load store compiler prove address fault occur 
interrupt points caused unsafe loads stores frequent approach allow optimizations approaches source level debugging 
pointers stack require special care deoptimization locations pointers unknown 
case address stack variable potentially referenced pointer may changed 
problem probably solved expense stack space requiring layout optimized unoptimized stack frames identical 
true source level debugging unsafe languages programs overwrite arbitrary memory regions produce behavior explained language source level 
example integer erroneously stored location floating point variable resulting behavior explained referring particular integer floating point representations system 
implementation cost providing full source level debugging presence optimizing compiler come free 
section examine impact techniques responsiveness runtime performance memory usage 
impact responsiveness deoptimization process interrupt points perceptible users 
compiler typically creates unoptimized methods millisecond sparcstation pauses introduced dynamic deoptimization negligible 
interrupt points increase latency user system interrupts microseconds interrupt point usually reached dozen instructions runtime system set interrupt flag 
summary providing full source level debugging self system reduced responsiveness 
impact runtime performance ideally performance impact full source level debugging measured completely disabling system 
possible source level debugging fundamental design goal self system 
disabling debugging support require major redesign compiler runtime system better performance achieved 
furthermore garbage collector imposes constraints optimizer requirement live registers may contain derived pointers pointers middle objects 
cases optimizations inhibited garbage collection similar inhibited debugging requirements dead store elimination forms common subexpression elimination 
difficult separate impact garbage collection optimization impact full debugging 
measure full performance impact debugging scheme inspection generated code indicated obvious debugging related inefficiencies 
measured effects source level debugging self system 
determine impact debugger visible names self compiler changed release registers allocated dead variables visible interrupt point 
performance improvement changed compiler insignificant wide range programs :10.1.1.30.1652
extension variable lifetimes needed support debugging incur virtually cost system 
reason self methods typically short variables unused significant portions scope 
system currently detects interrupts testing special register test takes cycles sparc 
polling slows typical programs numerical programs tight loops slowed :10.1.1.30.1652
complicated runtime system conditional traps overhead reduced cycle check loop unrolling reduce problem tight loops 
alternatively system employ non polling scheme interrupt handler patch code currently executing procedure cause process switch interrupt point 
summarize exact performance impact source level debugging self system hard determine 
data points believe source level debugging slows typical programs 
memory usage table shows memory usage various parts compiled methods relative space machine instructions 
example relocation information needed garbage collection size study performed self fully functional 
self measurements :10.1.1.30.1652
compiler better back results probably indicative true performance impact source level debugging self performs fewer optimizations 
actual machine code 
data obtained running benchmark programs sequence represent mbytes compiler generated data 
unoptimized optimized methods different characteristics data optimizing compilations shown second data column 
data column represents space distribution configuration practice second data column shows characteristics pure optimized code 
space consumption split main groups 
group contains method headers machine instructions represent information needed execute programs 
second group contains dependency links needed invalidate compiled code programming changes see section 
third group contains debugging related information scope descriptors pc mapping see section relocation information garbage collector update object pointers contained debugging information 
includes information needed recompile methods source code 
space consumed debugging information varies degree optimization 
optimized methods show higher relative space overhead unoptimized methods debugging information inlined method typically larger inline expanded code 
debugging information grows faster aggressive inlining compiled code 
total space overhead debugging reasonable 
standard system debugging information half size instructions system optimizes overhead 
words adding debugging information increases space usage excluding dependencies factor 
order conservative left space method headers needed system debugging 
headers contain lookup key various control information 
include headers debugging increases space usage factor 
cost supporting changes running programs slightly smaller dependency information occupies times size instructions 
current representation dependencies contains significant redundancies alternate implementation probably reduce space usage significantly 
rough comparison compiling self virtual machine gnu gnu specific pragmas generates executable mbytes text size mbytes 
debugging information times grouping slight simplification 
example compiler occasionally generates instructions just support debugging 
small portion relocation information attributed code debugging information 
simplifications significantly distort numbers 
category self self optimized machine instructions actual machine instructions control information method headers dependency links invalidate code programming changes scope descriptors physical pc mapping recreate state optimized code recompile methods relocation information gc table 
space cost debugging information relative instructions larger actual code total overhead current self system factor including support programming changes garbage collection 
comparison taken grain salt indicates despite increased functionality space overhead data structures supporting source level debugging probably higher systems 
related smalltalk system described deutsch schiffman pioneered dynamic compilation interrupt points 
hide effects compilation native code compiled methods included mapping compiled code source position 
activations normally created stack runtime efficiency converted demand full fledged activation objects required language definition converted back needed execution 
system interrupts delayed call backward branch 
compiler performed global optimizations system provide expected behavior deoptimization 
johnson describe model debugger developed concurrently closely resembles inspection points dynamic deoptimization provide expected behavior optimized code 
system lazy conversion 
furthermore definition inspection points allows asynchronous events user interrupts delayed arbitrarily 
ideas implemented typed smalltalk compiler system apparently run small programs practice system daily 
debugging optimized code orthogonal addresses problem recovering source level value point program 
contrast addresses source level values need recovered recovered 
interrupt points deoptimization amplifies benefits techniques aimed recovering source level values 
techniques discussed combined support optimizations 
hennessy addresses problem recovering values variables presence selected local global code reordering optimizations 
algorithms detect variable incorrect value terms source program reconstruct source level value 
hennessy stopping points similar interrupt points debugger invoked stopping points statement 
code sequences restricted stores completed entire statement runs completion 
error occurs execution statement debugger display program state program stopped statement 
adl tabatabai investigate problem detecting recovering values source level variables presence instruction scheduling 
recovery mechanisms described coutant warren 
zellweger describes interactive source level debugger cedar handles optimizations procedure inlining cross jumping provide expected behavior cases 
techniques recover source level values variables hide certain code location problems example optimized code difficult compiler performed optimizations considered zellweger instruction scheduling dead code elimination 
self system switch unoptimized code able avoid problems 
uses transparent incremental recompilation debugging purposes 
example user sets breakpoint procedure procedure converted unoptimized form debugging easier 
self system debugging information virtual memory unix debugging information consumes memory program debugged 
gnu allows source level debugging optimized code offers restricted functionality 
optimizations transparent programmer 
compiler version measurements 
perform transformations active procedures 
program suspended optimized procedure generally possible set breakpoint procedure continue execution single stepping 
mitigate problem users able specify amount optimization performed possibly impacting amount debugging transparency needed possibly affecting code quality 
far know support optimized code implemented 
tolmach appel describe debugger ml compiler performs optimizations program automatically annotated debugging statements compilation 
debug optimized program programmer manually recompile re execute program 
unoptimized programs annotated programs run significantly slower fully optimized programs 
self system increases programmer productivity providing full source level debugging presence global optimizations constant folding common subexpression elimination dead code elimination procedure inlining code motion instruction scheduling 
systems severely restrict optimization order achieve full source level debugging debugging operations single stepping breakpoints require source level state recoverable virtually instruction boundary 
contrast debugging system gives optimizing compiler freedom require full debugging instruction 
full source level debugging need preclude global optimization 
techniques possible combine lazy deoptimization interrupt points 
optimizations performed compiler hidden programmer code necessary 
deoptimization supports single stepping running method completion replacing inlined method operations affects procedure activations actively debugged code runs full speed 
debugging information needed relatively interrupt points compiler perform extensive optimizations interrupt points affecting 
debugging system allows optimized programs changed running execution resumed change 
particular system guarantees invocation old definition method change method inlined compiled methods 
best knowledge self system system provide full source level debugging particular single stepping optimized programs 
far know system provide source level semantics changing running programs 
adaptive optimization debugging system possible integrate optimizing compiler exploratory programming environment 

better late binding optimized late compilation 
late binding problem traditional compilers information generate efficient code source code 
delaying optimization necessary information available compiler generate better code 
interactive system self delaying optimization additional benefit reducing compilation pauses confining costly optimization time critical parts program 
polymorphic inline caches proven attractive mechanism collecting type information needed type feedback 
addition recording receiver types send speed dynamic dispatch 
words programs instrumented type feedback run faster programs instrumentation 
polymorphic inline caches attractive implementation dynamic dispatch systems type feedback 
combined type feedback adaptive recompilation improve better runtime performance interactive behavior 
average programs run times faster compiled type feedback execute times fewer calls 
execution characteristics optimized self programs surprisingly close specint benchmark suite 
measurements indicate object oriented programs execute efficiently special hardware support right compiler technology 
compared previous self compiler new compiler significantly simpler vs lines code compiles times faster source code unit 
adaptive recompilation improves apparent speed compilation compilations performed fast non optimizing compiler optimizing recompilations spread time 
self system adaptive recompilation manages hide optimization large extent current generation workstation faster machines compilation pauses virtually unnoticeable 
visible effect dynamic re compilation programs initially run slowly starting scratch compiled code 
experiments indicate line program expected get speed half minute current generation workstation 
similarly program change temporarily slows execution proportionally impact existing code 
usually changes small local execution proceed virtually delay 
dynamic deoptimization allows reconcile global optimization source level debugging 
debugging information provided compiler needs support reading source level program state points program may interrupted instruction boundary 
interrupt points optimization allowed 
debugging requests go reading current state handled transparently compiled code performing request unoptimized code 
programmer forced chose speed source level debugging programs debugged time optimized 
dynamic compilation regarded complicated hard implement 
hope shows dynamic compilation implementor life easier 
underlying mechanisms place new functionality dynamic compilation added relatively easily 
example source level debugging system chapter handling exceptional situations section implemented relatively little effort lines code respectively system supported dynamic compilation 
believe dynamic compilation attractive choice interactive development environments languages compiler static information self compiler generate reasonable code debugging purposes runtime information 
applicability implementation relies dynamic compilation techniques described thesis require 
type feedback straightforward integrate conventional compiling system similar profile optimizations 
source level debugging code implemented keeping precompiled unoptimized code separate file 
polymorphic inline caches require simple stub generator full fledged dynamic compilation 
dynamic reoptimization nature specific dynamic compilation 
techniques described thesis specific self language 
debugging system largely language independent works best pointer safe languages interrupt points spaced apart languages 
type feedback optimize late binding language object oriented languages non object oriented languages heavy late binding apl generic operators lisp generic arithmetic profit optimization 
system dynamic compilation profit adaptive recompilation improve performance reduce compile pauses 
possible areas improvement compilation overhead 
speeding compilation reduce compilation pauses start times 
increasing processor speeds help reduce pauses today workstations outperform system measurements factor 
increasing processor speed comes increasing problem size start times large applications may smaller applications may larger 
software techniques reducing compile pauses remain interesting area research 
discussed section non optimizing compiler possibly replaced interpreter little loss execution speed 
speeding optimizing compiler harder introducing compiletime lookup cache improve compilation speed relatively little effort 
possible approaches resulting significant speedups include improving inlining system precomputing intermediate code frequently inlined methods section 
multiprocessor workstations recompilations executed separate processor reducing recompilation pauses virtually zero 
alternatively optimization performed opportunistically user think pauses 
current system parameters recompilation system defined statically 
result values represent compromise ultimate performance 
example half life time invocation counters low ultimate performance usually better compile pauses worse 
better system extend adaptiveness compilation varying half life parameter situation 
start situations characterized compilations half life time short prevent premature optimization 
initial flurry compilations died half life time increased dynamically allow application optimized 
similarly longer term recompilation mechanism optimize program parts execute long time system gather information periods minutes seconds example inexpensive form profiling 
results extended combining type feedback aggressive global optimizer 
inlining enlarges scope global optimizations optimizer may significantly increase benefits inlining 
context exploratory programming aggressive optimization may warranted increase compile pauses 
optimizations useful application extractor generating stand executable application system batch style compilation 
optimizer additional information profilers type generate better code 
self inherits problem previous self systems 
system customizes compiled code exact receiver type duplicates code better share compiled code different receiver types code identical optimization benefits customization negligible 
originally customization introduced self choice customize system non adaptive 
adaptive recompilation possible customize lazily performance critical parts program customization benefit code messages sent self 
similarly system choose perform method cloning method inlining 
system simultaneously reduce code size improve performance reduced instruction cache overhead represents interesting area 
summary self proposed people including author placed hopeless category asked judge feasibility efficient implementation 
years chambers ungar showed self achieve excellent performance restricted set programs 
unfortunately large object oriented programs didn perform 
furthermore runtime performance achieved expense considerable compiler complexity compilation speeds slow interactive 
thesis third generation self system simultaneously improves execution compilation speed decreasing implementation complexity 
lesson certainly learned past years hopeless isn 
hope progress implementing self encourage find better solutions implementation challenges posed object oriented languages systems heavily late binding abstraction minimalistic languages 
hope contributes exploratory programming environments popular 
past systems suffered performance problems limited acceptance 
example programs executed interpreted smalltalk systems run times slower equivalents 
adaptive recompilation type feedback combined speed modern hardware significantly extend applicability exploratory programming environments 
better performance interactive behavior techniques exploratory programming possible pure languages application domains requiring higher ultimate performance reconciling exploratory programming high performance 
glossary adaptive recompilation 
technique adaptively runtime selecting heavily methods recompiling optimizing better performance see chapters 
block 
block object representing lexically scoped closure similar smalltalk block 
self blocks executed enclosing scope returned 
compiled method 
compiled method contains native code self source method plus various additional information dependency links invalidate compiled code source code changes relocation information garbage collector debugging information source level debugging 
customization single source method may compiled methods see chapter 
customization 
idea generating versions compiled code single source method version customized particular receiver type see section page 
dynamic deoptimization 
process transforming optimized code unoptimized code demand source level debugging see chapter 
inline cache 
dispatch mechanism dynamically typed languages see chapter 
map 
internal data structure describing implementation type object exact format contents constant slots :10.1.1.56.2990
things maps message dispatch customization 
send 
highly polymorphic send receiver types occur 
method 
object representing source method function 
method object contains byte coded version source code created parser 
self compilers byte codes input parsing source 
message 
message request object perform operation 
object request sent called receiver 
message send action sending message receiver 
monomorphic send 
non polymorphic send calls receiver type 
nic 
non inlining compiler 
non optimizing compiler generate initial compiled code self 
see chapter 
non local return 
non local return return method activation resulting performing return evaluating expression preceded operator lexically enclosed block 
non local return forces returns activations method activation activation block performing return 
pause clustering 
technique combining clusters pauses occurring short succession longer pause order better characterize pauses experienced user see section 
pic 
polymorphic inline cache extension inline caching handles polymorphic call sites efficiently 
see chapter 
polymorphic send 
send encounters receiver type execution 
self 
previous self implementation described chambers :10.1.1.30.1652
self 
self implementation described thesis 
splitting 
idea delaying control flow merge loss type information comes merge copying code see section page 
type 
types come flavors 
types interface types specify operations object supports 
concrete types called implementation types specify implementation details number order instance variables methods specific object 
self explicit types say type thesis usually mean implementation type 
words terms type map interchangeably 
type feedback 
optimization technique feeding back concrete receiver type information compiler 
type information collected program execution fed back compiler execution dynamic compilation static compilation 
see chapter 
type prediction 
technique optimizing send predicting receiver types specializing send predicted type 
see section page chapter 
appendix detailed data appendix contains detailed data graphs abbreviated tables main text 
number types inline caches table 
distribution degree polymorphism call sites benchmark time saved richards parser primmaker ui pathcache table 
execution time saved pics benchmark execution time ms self nofeedback self self cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui table 
execution time comparison benchmark unoptimized self self self nofeedback cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui table 
number dynamically dispatched calls benchmark self gnu sun cc gnu sun cc deltablue richards table 
performance relative times ms simulated time cpu time sparcstation benchmark self smalltalk lisp deltablue richards table 
performance relative smalltalk lisp table shows reduction execution time eliminated non inlined call time self nofeedback time self sends self nofeedback sends self 
benchmark time saved eliminated call microseconds cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui table 
time saved inlined call execution time spent type tests self self nofeedback cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui median table 
time spent performing type tests message dispatch inline type tests tests compares cycles tests compares cycles cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui table 
type tests self nofeedback message dispatch inline type tests tests compares cycles tests compares cycles cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui table 
type tests self average number comparisons type test sequence dispatch feedback tests cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui median table 
number comparisons type test sequence benchmark execution time ms self self original modified original modified bubble perm puzzle queens quick sieve towers tree bubble oo oo perm oo queens oo quick oo towers oo tree oo table 
performance stanford integer benchmarks spec gcc espresso li eqntott geomean load store cond 
branch 
branch call return jump nop sethi arithmetic logical shift compare table 
specint instruction usage cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui geomean load store cond 
branch 
branch call return jump nop sethi arithmetic logical shift compare table 
self instruction usage graphs show cache ratios caches discussed section 
ratios relative number category 
example data read ratio relative total number data reads performed program 
richards gcc richards cc richards virtual gcc richards virtual cc deltablue gcc deltablue cc deltablue virtual gcc deltablue virtual cc load store cond 
branch 
branch call return jump nop sethi arithmetic logical shift compare table 
instruction usage richards deltablue bytes allocated cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui table 
allocation behavior benchmarks self 
cache ratios self ratio cache size cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui 
data read ratios self ratio cache size cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui 
data write ratios self ratio cache size cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui 
data read ratios self write cache ratio cache size cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui 
data write ratios self write cache ratio cache size cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui 
data read ratios self eden ratio cache size cecilcomp cecilint deltablue mango primmaker richards typeinf ui ui performance measurements obtained instruction level simulator effectively infinite half life time simulator simulate timer interrupts 
parameter value max 
size compiled method instructions max 
size inlined method block arguments instructions max 
size inlined method block arguments instructions invocation limit recompilation invocation counter half life time seconds table 
self compiler configuration parameters pause length seconds sparcstation current faster faster table 
pause length histogram data mark abbot larry peterson 
language approach protocol implementation 
computer communications review 
ali reza adl tabatabai thomas gross 
detection recovery endangered variables caused instruction scheduling 
pldi conference proceedings pp 

published sigplan notices june 
ali reza adl tabatabai thomas gross 
evicted variables interaction global register allocation symbolic debugging 
popl conference proceedings 
alfred aho ravi sethi jeffrey ullman 
compilers principles techniques tools 
addison wesley 
ole agesen jens palsberg michael schwartzbach 
type inference self analysis objects dynamic multiple inheritance 
ecoop conference proceedings 
kaiserslautern germany july 
frances allen john cocke 
catalogue optimizing transformations 
design optimization compilers ed 

prentice hall 
pascal andr jean claude royer 
optimizing method search lookup caches incremental coloring 
oopsla conference proceedings vancouver bc october 
apple computer object pascal user manual 
cupertino 
borning ingalls 
type declaration inference system smalltalk 
conference record ninth annual symposium foundations computer science 
frank mueller david whalley 
avoiding unconditional jumps code replication 
proceedings sigplan conference programming language design implementation 
published sigplan notices july 
robert atkinson 
hurricane optimizing compiler smalltalk 
oopsla conference proceedings portland september 
published sigplan notices november 
bell 
threaded code 
communications acm 
borning 
classes versus prototypes object oriented languages 
proceedings acm ieee fall joint computer conference dallas tx november 
brian bray flynn 
write caches alternative write buffers 
technical report csl tr computer systems laboratory stanford university april 

polyvariant mixed computation analyzer programs 
acta informatica 
brad calder dirk grunwald benjamin zorn 
quantifying behavioral differences programs 
technical report cu cs university colorado boulder january 
brad calder dirk grunwald 
reducing indirect function call overhead programs 
st annual acm symposium principles programming languages january 
chaitin register allocation coloring 
proceedings sigplan symposium compiler construction pp 
june 
chamberlin support repetitive transactions ad hoc queries system acm transactions database systems march 
craig chambers 
cost garbage collection self system 
oopsla gc workshop october 
craig chambers design implementation self compiler optimizing compiler objectoriented programming languages :10.1.1.30.1652
ph thesis stanford university april 
craig chambers david ungar 
customization optimizing compiler technology self object oriented programming language 
proceedings sigplan conference programming language design implementation portland june 
published sigplan notices july 
craig chambers david ungar lee :10.1.1.56.2990
efficient implementation self dynamically typed object oriented language prototypes 
oopsla conference proceedings new orleans la october 
published sigplan notices october 
published lisp symbolic computation kluwer academic publishers june 
craig chambers david ungar 
iterative type analysis extended message splitting optimizing dynamically typed object oriented programs 
proceedings sigplan conference programming language design implementation white plains ny june 
published sigplan notices june 
published lisp symbolic computation kluwer academic publishers june 
craig chambers david ungar bay wei chang urs lzle 
parents shared parts inheritance encapsulation self 
published lisp symbolic computation kluwer academic publishers june 
craig chambers david ungar 
making pure object oriented languages practical 
oopsla conference proceedings phoenix az october 
craig chambers 
cecil language specification rationale 
university washington technical report cs tr 
bay wei chang david ungar 
animation cartoons user interface 
user interface software technology conference proceedings atlanta ga november 
chang scott mahlke william chen wen mei hwu 
profile guided automatic inline expansion programs 
software practice experience may 
chang scott mahlke hwu 
profile information assist classic code optimizations 
technical report eng university illinois urbana champaign april 
david chase 
garbage collection optimizations 
ph dissertation computer science department rice university 
andrew chien vijay karamcheti john plevyak 
concert system compiler runtime support efficient fine grained concurrent object oriented programs 
university illinois urbana champaign technical report uiuc dcs 
robert cmelik shing kong david edmund kelly 
analysis mips sparc instruction set utilization spec benchmarks 
asplos iv santa clara ca april 
robert cmelik david keppel 
shade fast instruction set simulator execution profiling 
sun microsystems laboratories technical report tr 
thomas eduardo 
assessment method lookup caches smalltalk implementations 

cooper hall ken kennedy 
procedure cloning 
ieee intl 
conference computer languages oakland ca april 
deborah coutant sue michelle 
doc practical approach source level debugging globally optimized code 
proceedings sigplan conference programming language design implementation 

characterization alpha axp performance tcp spec workloads 
isca conference proceedings may 
jack davidson anne holler 
study function inliner 
software practice experience 
jeffrey dean craig chambers 
training compilers better inlining decisions 
university washington technical report 
jeffrey dean craig chambers 
better inlining decisions inlining trials 
acm conference lisp functional programming orlando fl june 
morishita 
design implementation glue nail database system 
sigmod conference proceedings pp 

published sigmod record june 
peter deutsch 
dorado smalltalk implementation hardware architecture impact software architecture 

peter deutsch alan schiffman 
efficient implementation smalltalk system 
proceedings th symposium principles programming languages salt lake city ut 
peter deutsch 
private communication 
dixon mckee schweitzer vaughan 
fast method dispatcher compiled languages multiple inheritance 
oopsla conference proceedings new orleans la october 
published sigplan notices october 
david 
private communication 
amer diwan david tarditi eliot moss 
memory subsystem performance programs intensive heap allocation 
st annual acm symposium principles programming languages january 
amer diwan david tarditi eliot moss 
private communication august 
karel 
selector table indexing sparse arrays 
oopsla conference proceedings pp 
washington 
published sigplan notices october 
eric van dyke 
dynamic incremental compiler interpretative language 
hp journal july 
peter feiler 
language oriented interactive programming environment compilation technology 
ph dissertation carnegie mellon university 
john ellis 
bulldog compiler vliw architectures 
mit press 
margaret ellis bjarne stroustrup annotated manual 
addison wesley reading ma 
michael franz 
technological steps software component industry 
international conference programming languages system architecture springer verlag lecture notes computer science march 
free software foundation 
gnu compiler 
boston ma 
charles garrett jeffrey dean david grove craig chambers 
measurement application dynamic receiver class distributions 
technical report cse tr university washington february 
adele goldberg david robson smalltalk language implementation 
addison wesley reading ma 
susan graham peter kessler marshall mckusick 
execution profiler modular programs 
software practice experience 
justin graver ralph johnson 
type system smalltalk 
conference record th annual acm symposium principles programming languages san francisco ca january 
leo guibas douglas wyatt 
compilation delayed evaluation apl 
fifth annual acm symposium principles programming languages 
mary hall 
managing interprocedural optimization 
technical report comp tr ph thesis computer science department rice university april 
gilbert hansen adaptive systems dynamic run time optimization programs 
ph thesis carnegie mellon university 
richard heintz 
low level optimizations object oriented programming language 
sc 
thesis university illinois urbana champaign 
john hennessy 
symbolic debugging optimized code 
acm transactions programming languages systems july 
mark hill 
aspects cache memory instruction buffer performance 
technical report ucb csd computer science division university california berkeley november 
wilson ho ronald olsson 
approach genuine dynamic linking 
software practice experience april 
hoare 
hints programming language design 
annual acm symposium principles programming languages 
urs lzle bay wei chang craig chambers ole agesen david ungar 
self manual version 
unpublished manual february 
urs lzle craig chambers david ungar 
optimizing dynamically typed object oriented languages polymorphic inline caches 
ecoop conference proceedings geneva 
published springer verlag lecture notes computer science springer verlag berlin 
urs lzle craig chambers david ungar 
debugging optimized code dynamic deoptimization 
proceedings sigplan conference programming language design implementation 
published sigplan notices july 
urs lzle 
fast write barrier generational garbage collectors 
proceedings oopsla workshop garbage collection washington september 
antony hosking eliot moss darko 
comparative performance evaluation write barrier implementations 
oopsla proceedings pp 

hwu chang 
inline function expansion compiling programs 
proceedings sigplan conference programming language design implementation portland june 
published sigplan notices july 
daniel ingalls 
simple technique handling multiple polymorphism 
oopsla conference proceedings portland 
published sigplan notices november 
gordon 
spa sparc analyzer toolset 
available ftp cs adelaide edu au 
joel gregory peter 
leaf morphology environmental gradients hawaiian 

ralph johnson ed 
workshop compiling optimizing object oriented programming languages 
addendum oopsla conference proceedings orlando fl october 
published sigplan notices may 
ralph johnson justin graver lawrence 
ts optimizing compiler smalltalk 
oopsla conference proceedings san diego ca october 
published sigplan notices november 
ronald johnston 
dynamic incremental compiler apl 
proceedings apl conference 
published apl quote quad 
norm jouppi 
cache write policies performance 
isca conference proceedings pp 
san diego ca 
published computer architecture news may 
david keppel susan eggers robert henry 
case runtime code generation 
technical report uw tr department computer science engineering university washington seattle 
david keppel susan eggers robert henry 
evaluating runtime compiled value specific optimizations 
technical report uw tr department computer science engineering university washington seattle 
peter kessler 
fast breakpoints design implementation 
proceedings sigplan conference programming language design implementation white plains ny june 
published sigplan notices june 
gregor kiczales luis rodriguez 
efficient method dispatch pcl 
technical report ssl xerox parc 
glenn krasner ed smalltalk bits history words advice 
addison wesley reading ma 
douglas lea 
customization 
proceedings usenix conference san francisco ca april 
lee 
object storage inheritance self prototype object oriented programming language 
engineer thesis stanford university 
henry lieberman carl hewitt 
real time garbage collector lifetime objects 
communications acm 
henry lieberman 
prototypical objects implement shared behavior object oriented systems 
oopsla conference proceedings portland september 
published sigplan notices november 
henry lieberman lynn andrea stein david ungar 
treaty orlando 
addendum oopsla conference proceedings orlando fl october 
published sigplan notices may 
mark linton john vlissides paul calder 
composing user interfaces interviews 
ieee computer february 
cathy may mimic fast system simulator 
sigplan symposium interpreters interpretive techniques june 
scott mcfarling 
procedure merging instruction caches 
proceedings sigplan conference programming language design implementation 
published sigplan notices june 
scott mcfarling 
program analysis optimization machines instruction cache 
technical report stanford university 
mips computer systems mips language programmer guide 
mips computer systems sunnyvale ca 
mitchell design construction flexible efficient interactive programming systems 
ph thesis carnegie mellon university 
morris 
prototype code generator 
proceedings sigplan conference programming language design implementation 
published sigplan notices june 
alexandru nicolau 
run time disambiguation coping statically unpredictable dependencies 
ieee transactions computers may 
john ousterhout 
aren operating systems getting faster fast hardware 
dec western research laboratory technical note tn 
david patterson david 
case reduced instruction set computer 
computer architecture news october 
david patterson 
reduced instruction set computers 
communications acm january 
rob pike bart john reiser 
hardware software trade offs bitmap graphics 
software practice experience february 
steven mark horowitz john hennessy 
performance trade offs cache design 
proceedings th international symposium computer architecture may 
calton pu henry massalin 
overview synthesis operating system 
technical report cucs department computer science columbia university new york 
william pugh 
directional record layout multiple inheritance 
proceedings sigplan conference programming language design implementation white plains ny june 
published sigplan notices june 
rau 
levels representation programs architecture universal host machines 
proceedings micro asilomar ca november 
mark reinhold 
cache performance garbage collected programming languages 
technical report mit lcs tr mit september 
stephen richardson 
evaluating interprocedural code optimization techniques 
ph thesis computer systems laboratory technical report csl tr stanford university feb 
john rose 
fast dispatch mechanisms stock hardware 
oopsla conference proceedings san diego ca october 
published sigplan notices november 
weiss 
software high performance apl interpreter 
apl quote quad 
joel saltz harry janet wu 
multiprocessors runtime compilation 
proceedings international workshop compilers parallel computers paris december 
michael sannella john maloney bjorn freeman benson alan borning 
multi way versus way constraints user interfaces experience deltablue algorithm 
software practice experience 
warren 
design fds interactive debugging system 
ibm research report rc ibm yorktown heights july 
self group 
self manual version 
unpublished manual august 
robert shaw 
empirical analysis lisp system 
stanford university computer systems laboratory technical report csl tr 
patrick sobalvarro 
lifetime collector lisp systems general purpose computers 
thesis eecs dept massachusetts institute technology 
sparc international 
sparc architecture manual version 
prentice hall nj 
sparc international 
sparc architecture manual version 
prentice hall nj 
richard stallman 
gnu compiler 
free software foundation 
peter steenkiste john hennessy 
tags type checking lisp hardware software approaches 
asplos ii conference proceedings october 
markus str 
implementierung von oberon self 
diploma thesis institute computer systems eth rich march 
sun microsystems 
viking microprocessor tms user documentation 
part november 
suzuki 
inferring types smalltalk 
proceedings th symposium principles programming languages 
suzuki minoru 
creating efficient systems object oriented languages 
proceedings th symposium principles programming languages salt lake city january 
taylor hilfinger larus patterson zorn 
evaluation spur lisp architecture 
proceedings th symposium computer architecture tokyo 
andrew tolmach andrew appel 
debugging standard ml reverse engineering 
proceedings acm conference lisp functional programming nice france june 
david ungar david patterson 
berkeley smalltalk knows time goes 

ungar blau foley samples patterson 
architecture soar smalltalk risc 
eleventh annual international symposium computer architecture ann arbor mi june 
david ungar david patterson 
price smalltalk 
ieee computer january 
david ungar 
generation scavenging non disruptive high performance storage reclamation algorithm 
sigplan symposium practical software development environments pittsburgh pa april 
david ungar 
design evaluation high performance smalltalk system 
mit press cambridge ma 
david ungar randall smith 
self power simplicity 
oopsla conference proceedings orlando fl october 
published sigplan notices december 
published lisp symbolic computation kluwer academic publishers june 
david ungar craig chambers bay wei chang urs lzle 
organizing programs classes 
published lisp symbolic computation kluwer academic publishers june 
jan vitek nigel horspool 
taming message passing efficient method lookup dynamically typed languages 
ecoop proceedings springer verlag lecture notes computer science july 
david wall 
global register allocation link time 
sigplan symposium compiler construction published sigplan notices july 
david wall 
predicting program behavior real estimated profiles 
proceedings sigplan conference programming language design implementation 
published sigplan notices june 
paul wilson thomas 
design opportunistic garbage collector 
oopsla conference proceedings new orleans la october 
published sigplan notices october 
paul wilson thomas moher 
card marking scheme controlling intergenerational generation gc stock hardware 
sigplan notices 
paul wilson michael lam thomas moher 
caching considerations generational gc case large set associative caches 
university illinois chicago technical report ui eecs december 
mario wolczko 
private communication april 
williams mario wolczko 
object memory architecture 
proc 
th intl 
workshop persistent object systems martha vineyard ma september 
zellweger 
interactive high level debugger control flow optimized programs 
xerox parc technical report csl january 
zellweger 
interactive source level debugging optimized programs 
ph dissertation computer science department university california berkeley 
published xerox parc technical report csl may 
benjamin zorn 
barrier methods garbage collection 
technical report cu cs university colorado boulder november 
lawrence ralph johnson 
debugging optimized code expected behavior 
unpublished manuscript 
