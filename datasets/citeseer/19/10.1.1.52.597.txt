active learning natural language parsing information extraction cynthia thompson mary elaine califf raymond mooney february csli ventura hall stanford university stanford ca csli stanford edu department applied computer science illinois state university normal il edu department computer sciences university texas austin tx mooney cs utexas edu natural language acquisition difficult gather annotated data needed supervised learning unannotated data fairly plentiful 
active learning methods attempt select annotation training informative examples potentially useful natural language applications 
existing results active learning considered standard classification tasks 
reduce annotation effort maintaining accuracy apply active learning non classification tasks natural language processing semantic parsing information extraction 
show active learning significantly reduce number annotated examples required achieve level performance complex tasks 
keywords active learning natural language learning learning parsing learning information extraction email address contact author csli stanford edu phone number contact author electronic version postscript icml ijs si active learning emerging area machine learning explores methods relying benevolent teacher random sampling actively participate collection training examples 
primary goal active learning reduce number supervised training examples needed achieve level performance 
active learning systems may construct examples request certain types examples determine set unsupervised examples usefully labeled 
approach selective sampling cohn atlas ladner particularly attractive natural language learning abundance text annotate informative sentences :10.1.1.119.2797
language learning tasks annotation particularly time consuming requires specifying complex output just category label reducing number training examples required greatly increase utility learning 
increasing number researchers successfully applying machine learning natural language processing see brill mooney overview 
utilized active learning addressed particular tasks part speech tagging dagan engelson text categorization lewis catlett liere tadepalli 
fundamentally classification tasks tasks address semantic parsing information extraction 
language learning tasks require annotating natural language text complex output parse tree semantic representation filled template 
application active learning tasks requiring complex outputs studied 
research shows active learning methods applied problems demonstrates significantly decrease annotation costs important realistic natural language tasks 
remainder organized follows 
section presents background active learning section introduces systems apply active learning 
sections describe application active learning parser acquisition experimental results 
sections describe application active learning learning information extraction rules experimental results task 
section suggests directions research 
section describes related research section presents 
background active learning relative ease obtaining line text focus selective sampling methods active learning 
case learning begins small pool annotated examples large pool unannotated examples learner attempts choose informative additional examples annotation 
existing area emphasized approaches certainty methods lewis catlett committee methods freund seung shamir tishby liere tadepalli dagan engelson cohn 
certainty paradigm system trained small number annotated examples learn initial classifier 
system examines unannotated examples attaches predicted annotation examples 
examples lowest user annotation retraining 
methods attaching typically attempt estimate probability classifier consistent prior training data classify new example correctly 
committee paradigm diverse committee classifiers created small number examples 
committee member attempts label additional examples 
examples annotation results disagreement committee members user annotation retraining 
diverse committee consistent prior training data produce highest disagreement examples label uncertain respect possible classifiers obtained training data 
presents pseudocode certainty committee selective sampling 
ideal situation batch size set intelligent decisions choices efficiency reasons retraining batch learning algorithms frequently set higher 
results number classification tasks demonstrated general approach effective reducing need labeled examples see citations 
current explored certainty approaches committee approaches tasks learning parsers information extraction rules topic research 
apply learner bootstrap examples creating classifier committee 
examples annotator unwilling label examples learned classifier committee annotate unlabeled instance 
find instances lowest annotation certainty disagreement committee members 
annotate instances 
train learner bootstrap examples examples annotated point 
selective sampling algorithm natural language learning systems parser acquisition chill system set training sentences paired meaning representation learns parser maps sentences semantic form zelle mooney 
uses inductive logic programming ilp methods muggleton lavrac dzeroski learn deterministic parser written prolog 
chill solves parser acquisition problem learning rules control step step actions initial overly general parsing shell 
initial training examples sentence representation pairs examples ilp system positive negative examples states parser particular operator applied 
examples automatically constructed analyzing sequence operator applications shift reduce leads correct parse 
learning task user feedback provided classification task 
focus application chill tested learning interface geographical database 
domain chill learns parsers map natural language questions directly prolog queries executed produce answer 
sample queries database geography paired corresponding prolog query capital state biggest population 
answer capital largest state population 
state located 
answer state eq loc 
sufficient corpus sentence representation pairs chill able learn parser correctly parses novel sentences logical queries 
information extraction developed system rapier learns rules information extraction califf 
goal system find specific pieces information natural language document 
specification information extracted generally takes form template list slots filled substrings document lehnert sundheim 
particularly useful obtaining structured database unstructured documents growing number web internet applications 
rapier bottom relational learner acquires rules form sequence patterns identify relevant phrases document 
patterns similar regular expressions include constraints words part speech tags semantic classes extracted phrase surrounding context results simplest version system words 
part speech tags may useful domains words provide power 
semantic parsing classification task parsing chill mapped series classification subproblems freitag bennett aone lovell 
rapier approach problem manner case example annotations provided user form filled templates class labels 
active learning research focused tasks rapier extensively tested extracting information computer related jobs netnews postings 
shows example part corresponding filled template 
task extract information slots appropriate development jobs database 
posting newsgroup telecommunications 
solaris systems administrator 

immediate need leading telecommunications firm need energetic individual fill position atlanta office solaris systems administrator salary full benefits location atlanta georgia relocation assistance provided filled template title solaris systems administrator salary state georgia city atlanta platform solaris area telecommunications sample message filled template slots vary applicability different postings 
relatively postings provide salary information provide information job location 
number slots may filler example slots platform language prospective employee 
active learning semantic parsing applying certainty sample selection systems requires determining certainty complete annotation potential new training example despite fact individual learned rules perform part annotation task 
general approach compute individual decision processing example combine obtain certainty example 
systems learn rules explicit uncertainty parameters simple metrics coverage training examples assign rule decisions 
chill approach complicated slightly fact current learned parser may get stuck complete parse potential new training example 
happen control rule learned operator may overly specific preventing correct application operator required parsing sentence may needed training examples parser include 
sentence parsed annotation obviously uncertain candidate selection 
sentences batch size distinguish 
done counting maximum number sequential operators successfully applied attempting parse sentence dividing number words sentence give estimate close parser came completing parse 
sentences lower value metric preferred annotation 
number examples remaining examples selected annotation chosen parsable ones 
certainty parse potential training example obtained considering sequence operators applied produce 
recall control rules operator induced positive negative examples contexts operator applied 
simple approximation number examples induce specific control rule select operator measure certainty parsing decision 
believe reasonable certainty measure rule learning shown holte acker porter small disjuncts rules correctly classify examples error prone large ones 
average certainty operators parse sentence obtain metric rank example 
increase diversity examples included batch include sentences vary known names database constants city names chosen examples sentences contain subset words chosen sentence 
experimental results semantic parsing experimental results general methodology 
trial random set test examples system trained subsets remaining examples 
bootstrap examples randomly selected training examples step active learning best examples remaining training examples selected added training set 
result learning set evaluated round 
comparing random sampling examples round chosen randomly 
initial corpus evaluating parser acquisition contains questions geography paired prolog queries 
domain chosen due availability existing hand build natural language interface simple geography database containing facts 
original interface geobase supplied turbo prolog borland international 
questions collected uninformed undergraduates mapped logical form expert 
examples corpus section 
parser learned training data process test examples resulting queries submitted database answers compared generated correct representation percentage correct answers recorded 
tests data test examples chosen independently trials bootstrap examples batch size 
results shown chill refers random sampling chill active refers sample selection geobase refers hand built benchmark 
training examples chill active chill geobase chill active learning initially advantage small insufficient information intelligent choice examples examples advantage clear 
eventually training set exhausted active learner choice picking remaining examples approaches full training set converge performance 
number examples required reach level significantly reduced active learning 
get final accuracy requires selected examples random examples savings 
surpass performance geobase requires selected examples versus random examples savings 
test differences active random choice training examples statistically significant level better 
ran experiments larger diverse corpus geography queries additional examples collected undergraduate students introductory ai course 
set questions previous experiments collected students introductory german instructions complexity queries desired 
ai students tended ask complex diverse queries task give interesting questions associated logical form homework assignment 
new sentences total 
data split training training examples chill active chill geobase active learning larger geography corpus sentences test sentences random splits 
corpus 
results shown 
savings active learning examples reach accuracy close maximum annotation savings 
curve selective sampling reach examples elimination sentences vary database names contain subset words chosen sentence 
obviously difficult corpus active learning able choose examples allow significant savings annotation cost 
active learning information extraction similar approach certainty sample selection rapier 
simple notion certainty individual extraction rule coverage training data pos gamma delta neg pos number correct fillers generated rule neg number incorrect ones 
small disjuncts account examples deemed certain 
rapier prunes rules prevent overfitting may generate spurious fillers training data significant penalty included errors 
notion rule certainty rapier determines certainty filled slot example evaluated annotation certainty 
case single rule finds filler slot certainty slot certainty rule filled 
slot filler certainty slot defined minimum rules produced fillers 
minimum chosen want focus attention certain rules find examples confirm deny 
final consideration determining certainty empty slot 
tasks slots empty large percentage time 
example jobs domain salary half time 
hand slots filled absence fillers slots decrease confidence example labeling 
consequently record number times slot appears training data fillers count confidence slot filler 
confidence slot determined confidence example summing confidence slots 
order allow desirable option actively selecting single example time incremental version rapier created 
version requires remembering training examples reuses updates existing rules new examples added 
resulting system incrementally incorporate new training examples reasonably efficiently allowing chosen example immediately effect result choice example 
experimental results information extraction computer related job posting corpus test active learning rapier consists postings local newsgroup austin jobs illustrated section 
training test sets generated fold crossvalidation learning curves generated training random actively selected subsets training data trial 
active learning bootstrap examples subsequent examples selected time remaining examples 
training examples rapier rapier active results selective sampling versus random selection examples information extraction standard measurements performance precision percentage items system extracted extracted recall percentage items system extracted extract 
order combine measurements simplify comparisons common measure delta precision delta recall precision recall 
possible weight measure prefer recall precision weight equally 
active learning results measured performance example intervals 
results random sampling measured frequently 
shows results rapier uses random sampling rapier active uses selective sampling 
examples rapier active consistently outperforms rapier 
difference curves large represent large reduction number examples required achieve level performance 
examples average fmeasure exactly average measure random examples 
represents savings examples 
differences performance examples significant level tailed paired test 
curve selective sampling go way examples performance randomly chosen examples reached information available data set exploited curve just level useful examples added 
experiments additional semantic parsing information extraction corpora needed test ability approach reduce annotation costs variety domains 
interesting explore active learning natural language processing problems syntactic parsing word sense disambiguation machine translation 
current results involved certainty approach proponents committee approaches convincing arguments theoretical advantages 
initial attempts adapting committee approaches systems successful additional research topic indicated 
critical problem obtaining diverse committees properly sample version space cohn 
quite certainty metrics chill rapier quite simple somewhat ad hoc 
principled approach learning probabilistic models parsing information extraction result better estimates certainty improved sample selection 
intelligent method choosing batch sizes needed 
initial informal experiments chill observed optimal batch size vary total amount training data 
small batches beneficial learning larger batches better 
converting chill incremental version done rapier sidestep issue allow efficient learning step increments 
related cohn 
discuss certainty active learning methods detail 
focus neural network approach actively searching version space concepts 
liere tadepalli apply active learning committees problem text categorization 
show improvements active learning similar obtain committee winnow learners traditional classification task 
dagan engelson apply committee learning part ofspeech tagging 
committee hidden markov models select examples annotation 
lewis catlett heterogeneous certainty methods simple classifier select examples annotated powerful classifier 
methods applied text classification 
researcher applied active learning information extraction 
soderland whisk system uses unusual form selective sampling soderland 
committees whisk divides pool unannotated instances classes covered existing rule near misses rule covered rule 
system randomly selects set new examples classes adds training set 
soderland shows method significantly improves performance management succession domain soderland unclear traditional sample selection methods perform comparison 
active learning new area machine learning exclusively applied classification tasks 
demonstrated successful application complex natural language processing tasks semantic parsing information extraction 
wealth unannotated natural language data difficulty annotating data selective sampling potentially invaluable technique natural language learning 
results realistic corpora semantic parsing information extraction indicate example savings high achieved employing active sample selection simple certainty measures predictions unannotated data 
improved sample selection methods applications important language problems hold promise continued progress machine learning construct effective natural language processing systems 
research supported national science foundation iri 
bennett aone lovell 

learning tag multilingual texts observation 
proceedings second conference empirical methods natural language processing pp 

borland international 
turbo prolog guide 
borland international valley ca 
brill mooney 

overview empirical natural language processing 
ai magazine 
califf 

relational learning techniques natural language information extraction 
ph thesis university texas austin tx 
cohn atlas ladner 

improving generalization active learning 
machine learning 
dagan engelson 

committee sampling training probabilistic classifiers 
proceedings twelfth international conference machine learning pp 
san francisco ca 
morgan kaufman 
freitag 

multi strategy learning information extraction 
proceedings fifteenth international conference machine learning pp 

freund seung shamir tishby 

selective sampling query committee algorithm 
machine learning 
holte acker porter 

concept learning problem small disjuncts 
proceedings eleventh international joint conference artificial intelligence pp 
detroit mi 
lavrac dzeroski 

inductive logic programming techniques applications 
ellis horwood 
lehnert sundheim 

performance evaluation text analysis technologies 
ai magazine 
lewis catlett 

heterogeneous uncertainty sampling supervised learning 
proceedings eleventh international conference machine learning pp 
san francisco ca 
morgan kaufman 
liere tadepalli 

active learning committees text categorization 
proceedings fourteenth national conference artificial intelligence pp 
providence ri 
muggleton 
ed 

inductive logic programming 
academic press new york ny 
soderland 

learning information extraction rules semi structured free text 
machine learning 
zelle mooney 

learning parse database queries inductive logic programming 
proceedings thirteenth national conference artificial intelligence portland 
