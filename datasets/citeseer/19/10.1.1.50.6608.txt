array algorithm simultaneous multidimensional aggregates zhao computer sciences department university wisconsin madison zhao cs wisc edu prasad deshpande computer sciences department university wisconsin madison pmd cs wisc edu jeffrey naughton computer sciences department university wisconsin madison naughton cs wisc edu computing multiple related group bys aggregates core operations line analytical processing olap applications 
gray proposed cube operator computes group aggregations possible subsets specified dimensions 
rapid acceptance importance operator led variant cube proposed sql standard 
efficient algorithms relational olap rolap developed compute cube 
knowledge literature compute cube multidimensional olap molap systems store data sparse arrays tables 
molap algorithm compute cube compare leading rolap algorithm 
comparison interesting computing function value rolap algorithm position molap algorithm 
tests show appropriate compression techniques molap algorithm significantly faster rolap algorithm 
fact difference pronounced molap algorithm may useful rolap systems molap systems cases cubing table directly faster convert table array cube array convert result back table 
computing multiple related group bys aggregates core operations line analytical processing olap applications 
gray proposed cube operator computes group aggregations possible subsets specified dimensions 
rapid acceptance importance operator led variant cube proposed sql supported nsf iri ibm university partnership program arpa contract standard 
efficient algorithms relational olap rolap developed compute cube 
knowledge date literature compute cube multidimensional olap molap systems 
concreteness consider simple multidimensional model dimensions product store time measure data value sales 
compute cube compute sales grouped subsets dimensions 
sales product store date sales product store sales product date sales store date sales product sales store sales date sales 
multidimensional applications system called compute aggregates large subset response user query part load process aggregates speed queries 
challenge course compute cube far efficiency naive method computing component aggregate individually succession 
molap systems different sort challenge computing cube rolap systems 
main reason fundamental difference data structures systems store data 
rolap systems example ms informix mc information advantage ia definition relational tables data structure 
means cell logically multidimensional space represented system tuple attributes identify location tuple multidimensional space attributes contain data value corresponding data cell 
returning example cell array represented tuple shoes july 
computing cube table requires generalization standard relational aggregation operators 
prior main ideas rolap computation efficient 
sort grouping operation dimension attributes bring related tuples sorting hashing 
grouping performed behalf sub aggregates partial grouping speed computation sub aggregate 
compute aggregate aggregate presumably larger base table 
contrast molap systems example arbor software ccs rj express oracle oc pilot psw store data sparse arrays 
returning running example storing tuple shoes july molap system just store data value position sparse array encode fact sales volume shoes west town store july 
consider computing cube data stored arrays rolap trick computing aggregate 
techniques developed rolap cube computations apply 
importantly equivalent reordering bring related tuples dimension values 
data values stored fixed positions determined dimension values trick visit values right order computation efficient 
similarly concept order generated computation trick simultaneously compute spatially delimited partial aggregates cell revisited sub aggregate 
minimal memory requires great deal care attention size dimensions involved 
complicated fact order store arrays efficiently disk chunk small memory sized pieces perform sort compression avoid wasting space cells contain valid data 
molap algorithm incorporating ideas 
algorithm succeeds overlapping computation multiple available main memory 
prove number theorems algorithm including specification optimal ordering dimensions cube reading chunks base array upper bound memory requirement pass computation cube general smaller size original base array 
implemented algorithm performance results wide range dimension sizes data densities buffer pool sizes 
show algorithm performs significantly faster naive algorithm computing aggregates separately naive algorithm smart computing sub aggregates base array 
compared algorithm implementation rolap cube algorithm molap algorithm significantly faster 
clearly molap cube algorithm multidimensional database system 
believe may applicability relational database systems part support multidimensional database applications reasons 
relational database systems provide richer richer type systems feasible implement arrays storage device rdbms data 
explored performance implications approach consolidation operations study adds weight including array storage relational systems significantly enhance rdbms performance certain workloads 
second application algorithm rolap systems came surprise retrospect foreseen result 
simply put molap algorithm relational system step procedure 
scan table load array 

compute cube resulting array 

dump resulting cubed array tables 
result directly cubing table surprising step approach faster direct approach cubing table 
step approach array internal data structure hash table hash join standard relational join processing 
rest organized follows 
section introduce chunked array representation discuss compressed arrays algorithm loading chunked compressed arrays tables 
basic array algorithm section 
new algorithm multi way array method described section theorems show predict minimize memory requirements algorithm 
performance results section conclude section 
array storage issues section discuss basic techniques load store large sparse arrays efficiently 
main issues resolve 
highly multidimensional application array far large fit memory 
case array split chunks small fit comfortably memory 
second chunking cells array empty meaning data combination coordinates 
efficiently store sort data need compress chunks 
third cases array may need loaded data array format relational table external load file 
conclude section description efficient algorithm loading arrays compressed chunked format 
chunking arrays mentioned high performance large arrays stored broken smaller chunks 
standard programming language technique storing array row major column major order efficient 
consider row major representation dimensional array dimensions store date store forms row date forms column 
accessing array row order order stores efficient representation disk page read contain stores 
accessing order columns dates inefficient 
store dimension big disk page read contain data date 
get data date require disk access fact disk access date required 
simple row major layout creates asymmetry dimensions favoring 
data accessed disk units pages 
uniform treatment dimensions chunk array suggested sarawagi sm 
chunking way divide dimensional array small size dimensional chunks store chunk object disk 
array chunk dimensions correspond blocking size disk 
chunks size dimension 
compressing sparse arrays dense chunks define array cells valid value compress array simply storing cells array asis assigning null value invalid array cells 
chunk fixed length 
note storing dense multidimensional data set array significant compression storing data relational table store dimension values 
example running example store product store date values array 
sparse chunk data density storing array compression wasteful space devoted invalid cells 
case call chunk offset compression 
chunk offset compression valid array entry store pair data 
integer computed follows consider chunk normal uncompressed array 
cell chunk defined set indices example working dimensional chunk cell address chunk 
access cell memory convert triple offset start chunk typically assuming chunk laid memory standard order 
offset integer store 
representation chunks variable length meta data hold length chunk store meta data data file 
experimented compressing array chunks lossless compression algorithm lzw compression wel far effective couple reasons 
compression ratio chunk offset compression 
intuitively lzw compression uses domain knowledge chunk offset compression fact storing array cells minimize storage 
second important lzw compression necessary materialize possibly sparse full chunk memory operated 
contrast chunk offset compression operate directly compressed chunk 
loading arrays tables designed implemented partition loading algorithm convert relational table external load file possibly compressed chunked array 
input algorithm takes table dimension size predefined chunk size 
briefly algorithm works follows 
know size full array chunk size know chunks array loaded 
available memory size size resulting array partition set chunks partitions data partition fits memory 
partitioning logical phase 
example chunks need partitions put tuples corresponding cells map chunks partition map chunks partition 
partitions determined algorithm scans table 
tuple algorithm calculates tuple chunk number offset element chunk 
possible examining dimension values tuple 
algorithm stores chunk number offset data element tuple inserts tuple buffer page corresponding partition 
buffer page partition full page written disk resident file partition 
second pass partition algorithm reads partition tuple assigns bucket memory chunk number 
bucket corresponds unique chunk 
assign tuples buckets algorithm constructs array chunks bucket compresses necessary chunk offset compression writes chunks disk 
optimization compute chunks partition pass 
allocate partition buffer page allocate rest available memory buckets partition 
similar techniques hybrid hash join algorithm keep bucket memory 
basic array cubing algorithm introduce algorithm compute cube chunked array multiple passes minimum memory 
algorithm attempt overlap computation computing group separate pass 
section modify simple algorithm minimize cost overlap aggregation related group bys 
consider compute group simple non chunked array 
suppose dimensional array dimensions suppose furthermore want compute aggregate ab want project aggregate values 
seen projecting ab plane logically done sweeping plane dimension aggregating go array swept 
suppose abc array stored number chunks 
computation viewed sweeping array aggregating away dimension 
sweeping entire plane size jaj jbj sizes dimensions chunk chunk basis 
suppose dimension chunk size ac dimension chunk size bc think orienting array looking ab face array going back chunk upper left hand portion array sweep plane size back chunk aggregating away values go 
finished upper left hand chunk continue sweep plane chunk immediately front array upper left corner 
continue fashion swept way array 
point computed portion ab aggregate corresponding upper left hand sub plane size store plane disk part ab aggregate move compute sub plane corresponding chunk immediately right initial chunk 
note way chunk read computation ab aggregate disk collection planes size memory computation hold chunk plus hold plane swept chunks 
generalization algorithm higher dimensions straight forward sweeping planes arrays higher dimensions say dimensional ar rays sweeps gamma dimensional subarrays array 
discussed computing single aggregate array 
mentioned cube array requires computing aggregates array 
example array dimensions abc need compute ab bc ac total aggregate 
naive approach compute aggregates initial abc array 
moment thought shows bad idea far efficient compute ab compute abc 
idea explored rolap cube computation literature 
look entire cube computation aggregates computed viewed lattice abc root 
abc children ab bc ac ac children forth 
compute cube efficiently embed tree lattice compute aggregate parent tree 
question arises tree computation 
rolap cube computations difficult question sizes tables corresponding nodes lattice known computed heuristics 
chunk array algorithm fortunate knowing dimension sizes array size chunks store array compute exactly size array corresponding node lattice storage needed arrays compute child 
define minimum size spanning tree lattice 
node lattice parent minimum size spanning tree node minimum size computed 
state basic array cubing algorithm 
construct minimum size spanning tree cube 
compute group cube parent minimum size 
read chunk dimension aggregate chunk chunk chunk complete output chunk disk memory chunk note need keep chunk memory time 
dimensional array example 
array abc theta theta array theta theta array chunks laid dimension order abc see 
order layout indicated chunk numbers shown 
chunks numbered 
cube array consists group bys ab ac bc 
example compute bc group read chunk number order aggregate abc chunks bc chunk output bc chunk disk reuse memory bc chunk 
algorithm fairly careful hierarchy aggregates compute cube minimal memory step somewhat naive computes independently 
detail suppose computing ab ac bc abc example 
basic algorithm compute ab abc re scan abc compute ac scan third time compute bc 
sections discuss modify algorithm compute children parent single pass parent 
multi way array algorithm multi way array cubing algorithm 
algorithm overlaps computations different avoiding multiple scans required naive algorithm 
recall data cube dimensional array contains multiple related group bys 
specifically consists group bys subset dimensions 
group bys represented arrays 
ideally need memory large hold group bys overlap computation group bys finish cube scan array 
unfortunately total size group bys usually larger buffer pool size 
algorithm tries minimize memory needed computation achieve maximum overlap 
describe algorithm steps 
initially assume sufficient memory compute group bys scan 
extend case memory insufficient 
single pass multi way array cubing algorithm showed naive algorithm necessary keep entire array memory group keeping relevant part array memory step suffice 
reducing memory requirements keeping parts group arrays memory 
computing multiple group bys simultaneously total memory required depends critically order input array scanned 
order reduce total amount memory algorithm special logical order called dimension order 
dimension order dimension order array chunks row major order chunks dimensions dn order 
different dimension orders lead different orders reading array chunks 
note logical order reading independent actual physical layout chunks disk 
chunks array may laid disk order different dimension order 
see dimension order determines amount memory needed computation 
memory requirements assuming read array chunks dimension order formulate general rule determine chunks group cube need stay memory order avoid chunk input array 
array illustrate rule example 
array chunks read dimension order abc chunk chunk 
suppose chunk read 
group ab chunk aggregated dimension get chunk ab 
similarly ac bc chunk aggregated dimensions respectively 
chunk ab group aggregated chunk ab chunk ac aggregated chunk ac chunk bc aggregated chunk bc 
read new chunks aggregate chunk ab ac bc group corresponding chunks group bys ab ac bc 
compute chunk ab ac bc group may dimension dimension dimension array naively allocate memory chunk group bys memory 
exploit order chunk brought memory reduce memory required group minimum compute group bys ac ab bc scan array abc 
look compute chunk group bys detail 
notice read chunks dimension order layout linear order chunk chunk 
chunk chunk complete aggregation chunk bc aggregating chunk bc group chunk bc 
chunk completed write chunk reassign chunk memory chunk computed chunks abc chunk chunk 
chunk bc memory compute entire bc group 
similarly allocate memory chunks ac group scanning chunks abc 
finish aggregation chunk aggregate ac chunks chunk aggregate chunks ac chunks ac aggregation ac chunks done 
output ac chunks disk order reassign chunks memory ac group 
compute ab group scan array abc need allocate memory chunks ab 
chunks abc aggregate chunk ab corresponding ab chunks 
aggregation ab complete aggregate chunks ab ab chunks 
aggregation ab chunks done output chunks order 
notice generate bc chunk dimension order 
write bc chunk disk bc chunks compute chunks read bc chunk dimension order 
generally chunks group bys cube generated proper dimension order 
fact key apply general memory requirement rule recursively nodes minimum memory spanning tree overlap computation cube group bys 
explain idea detail discuss 
example computing bc need memory hold chunk bc ac need memory hold chunks ac ab need memory hold chunks ab 
generalizing allocate ju memory bc group ju ac group ju ab group stands size dimension stands chunk size dimension stands size chunk element 
size chunk element array element size depends type array 
integer array array element takes bytes 
pattern allocating memory ab ac bc group bys dimension order 
xy contains prefix abc length allocate theta gammap theta memory xy group bys 
dimension size chunk dimension size 
generalize group bys dimensional array rule 
rule group gamma array dn read dimension order dn gamma contains prefix dn length gamma allocate jd theta gamma jc units array element gamma group jd size dimension jc chunk size dimension jc smaller jd dimensions 
rule allocate amount memory size group 
benefit reducing memory allocated group compute group bys cube simultaneously overlap computation group bys higher degree 
need kind structure coordinate overlapped computation 
spanning tree lattice group bys purpose 
dimension order different spanning trees require different amounts memory 
define minimum memory spanning tree section 
minimum memory spanning tree cube dn dimension order levels root level tree node level level may computed nodes level dimensions contain dimensions node node level may node level computed 
choose node node require minimum memory rule 
words prefix parent node contained node minimum length 
dimension order minimum terms total memory requirement dimension order 
node contains minimum prefix upper level nodes size nodes break tie choose node minimum size parent node build cube dimension order overlap computation subtrees 
example array abc explain 
assume memory allocate node required memory 
array abc dimension order shown 
mentioned chunks bc ac ab calculated dimension orders memory read abc chunks dimension order produce chunk bc ac ab 
node equivalent reading chunks group ab ac dimension order abc ac ab bc level level level level array dimension order 
similar nodes level chunks nodes generated proper dimension orders 
generalize nodes level level chunks tree node generated proper dimension order 
recursively apply rule nodes level level allocate minimum number chunks nodes chunks 
furthermore compute chunks tree node simultaneously 
example aggregate chunk ac dimension compute chunk aggregate chunk abc chunk write chunk disk 
generally allocate node required memory compute chunks tree nodes top level level simultaneously 
give way calculating memory required dimension order dn 
assume array element takes bytes 
addition numbers memory size sections units array element size 
memory requirements assume chunk size dimension jc calculate memory required tree node level rule 
root level allocate root dn level gamma level root dn nodes dn gamma dn gamma dn gamma dn delta delta delta dn node omits dimension root dimensions dn node contains prefix root dn 
length prefix node gamma gamma delta delta delta 
rule sum memory required nodes gamma jd gamma jd gamma jd delta delta delta gamma level gamma classify tree nodes types length prefix root contained nodes dn gamma dn gamma delta delta delta wn gamma wn gamma type dk wn gamma gammak nodes start prefix dk root followed dimensions included delta delta delta dk dk gamma gamma gamma nodes belonging type choosing gamma gamma dimensions gamma dimensions 
gamma choose dimension dk node type dk gamma gamma type wn gamma gammak sum memory required nodes level gamma jd gamma jd gamma jd delta delta delta gamma gamma gamma similarly calculate total memory required nodes level gamma 
sum gamma jd gamma jd gamma jd delta delta delta gamma gamma gamma general get rule 
rule total memory requirement level dimension order dn gammaj jd gammaj gamma jd gammaj gamma jd delta delta delta gamma gamma gammaj example sum memory level nodes gamma level node requires amount memory 
different dimension orders array dn may generate different may profoundly different memory requirements 
illustrate dimension array abcd chunks 
sizes dimensions 
dimension order dimension order shown figures 
number group node figures number units array element required node 
adding numbers find order requires approximately gb pass computation tree order requires mb 
investigating reason difference trees find switching order changes amount memory required tree node 
clearly important determine dimension order require memory 
optimal dimension order optimal dimension order dimension order requires amount memory 
prove optimal dimension order dn jd jd delta delta delta jd denotes size dimension dimensions ordered incrementally dimension order abcd abc abd acd bcd ab ac bc ad bd cd dbc dba dca ca da bca dc ba bc db dimension order abcd total memory required mb dimension order total memory required gb theorem consider chunked multidimensional array size jd having chunks size jc jc 
read chunks logical order dn jd jd jd delta delta delta total amount memory required compute cube array scan minimum 
question naturally follows upper bound total amount memory required theorem corollary answer question 
theorem chunked multidimensional array size jd jd array chunk size jc jc total amount memory compute cube array scan gamma corollary chunked multidimensional array size jd jd jd delta delta delta array chunk size jc jc total amount memory compute cube array scan gamma gamma jd gamma note indicates bound independent size largest dimension dn single pass multiway algorithm assumes memory required optimal dimension order 
memory group bys computed recursively single scan input array described previously example abc 
memory insufficient need multiple passes 
need multi pass algorithm handle case described section 
multi pass multi way array algorithm optimal dimension ordering mt memory required calculated rule 
mt allocate required memory subtrees 
call subtrees incomplete subtrees 
need extra steps compute group bys included incomplete subtrees 
problem allocating memory optimally different subtrees similar described np hard 
heuristic allocating memory subtrees root right left order 
example order subtrees considered bc ac ab 
heuristic bc largest array want avoid computing multiple passes 
multi pass algorithm listed create dimension order add list 
tree list create working subtree incomplete subtrees allocate memory subtrees scan array chunk root order aggregate chunk generate intermediate results write complete chunks disk write intermediate results partitions generate chunks partitions write completed chunks disk add incomplete subtrees exist case mt compute cube case need multiple passes 
divide working subtree set incomplete subtrees 
allocate node working subtree memory required finish aggregation group bys contained working subtree scan array 
incomplete subtree gamma allocate memory equal chunk size group gamma aggregate input array chunk group gamma write intermediate result disk 
intermediate result aggregation gamma group chunk dn intermediate result incomplete intermediate results different dn chunks map chunk gamma group 
need aggregate different chunks produce chunk gamma group 
possible amount memory required gamma group larger divide chunks gamma group partitions dimension order chunks partition fit memory 
output intermediate chunks gamma write partition belong 
example partition may decided values gamma chunk 
different ranges values gamma go different partitions 
step partition read intermediate result aggregate corresponding chunk gamma group 
finish processing intermediate result chunk gamma group memory complete output dimension order gamma done partition complete computation group gamma compute subtrees gamma node repeat loop finish aggregation node subtree 
performance results section performance results molap cube algorithm previously published rolap algorithm 
experiments run sun sparc machine running sunos 
workstation mb memory gb local disk sequential read speed mb second 
implementation uses unix file system provided os 
data sets synthetic data sets study algorithms performance 
number factors affect performance cubing algorithm 
include ffl number valid data entries 
fraction cells multidimensional space contain valid data 
note number valid data entries just number tuples rolap table implementing multidimensional data set 
ffl dimension size 
elements dimension note molap array implementation dimension size determines size array 
rolap implementation table size remains constant vary dimension size range values dimension attributes drawn changes 
ffl number dimensions 
obvious just mention keeping number valid data cells constant varying number dimensions impacts rolap molap implementations differently 
adding dimensions molap causes shape array change adding dimensions rolap adds subtracts attributes tuples table 
data density number array dimensions array size affect algorithm performance designed data sets 
data set keep number valid data elements constant vary dimension sizes 
data set consists dimension arrays 
arrays dimensions sizes fixed fourth dimension array second third 
array valid elements 
results data density arrays fraction valid cells ranging 
size input compressed array array method turned mb 
input table size rolap method mb 
data set keep dimension sizes fixed vary number valid data elements 
members data set logically dimensional arrays size 
varied number valid data elements array data density ranges 
input compressed array size varied mb mb mb mb 
corresponding table sizes rolap tables mb mb mb mb 
data set data set contains arrays number dimensions ranging 
goal keep density number valid cells constant data set arrays sizes theta theta theta theta theta theta theta theta theta 
array data density 
array valid array cells 
size input array mb 
table size rolap changed mb mb mb due added attributes tuples 
generated uniform data data sets 
data sets small proportionately small buffer pool mb experiments 
indicate available memory size tests memory size 
array cube algorithms section compare naive multi way array algorithms study effect compression algorithm performance multi way algorithm investigate behaviour buffer pool size decreases test scale number dimensions increases 
naive vs multi way array algorithm ran tests naive multi way array algorithm dimension arrays 
dimension sizes fixed fourth dimension varied 
array data density 
see naive array algorithm slower multi way array algorithm due multiple scans parent group bys 
response time seconds fourth dimension size naive array alg uncompressed multi way array alg uncompressed naive vs multi way array alg 
response time seconds data density mult way array alg 
mult way array alg 
offset comp compression methods compression performance compare array compression array offset compression data set 
shows data density multi way array algorithm performed input array compressed offset algorithm faster uncompressed input array 
reasons 
lower densities compressed array size smaller 
reduces cost reading input array 
multi way array algorithm processes valid array cells input array computing data cube input array compressed offset algorithm 
uncompressed input array multi way array algorithm handle invalid array cells 
multi way array different buffer sizes ran experiments data set density varying buffer pool size 
see performance multi way array algorithm step function available memory size 
test increased available memory size kb mb 
step right caused generating incomplete subtrees scan input array due insufficient memory hold required chunks subtrees 
algorithm goes second pass produce incomplete subtree computes contained subtrees 
available memory size increases kb incomplete subtree generated causes second step right 
available memory kb algorithm allocates memory entire computes cube scan input array 
flushed os cache process working subtrees partitions 
theorem predicts bound kb memory required data 
graph shows kb entire fits memory 
bound quite close actual value 
response time memory size kb multi way array alg 
offset comp multi way array alg 
various memory size varying number dimensions discuss varying number dimensions compare array algorithm rolap algorithm 
rolap vs multi way array algorithms section investigate performance molap algorithm previously published sort rolap algorithm cases 
overlap method benchmark comparison 
rolap data stored tables 
computing cube table produces set result tables representing group bys 
hand molap data stored sparse multidimensional arrays 
cube array produce array group bys 
different formats table array possible input output data ways comparing methods 
described sections 
tables vs arrays way compare array vs table algorithms examine expected perform native systems 
consider multi way array algorithm performs system stores data array format table algorithm performs system stores data tables 
argue arrays order data way facilitate cube computation tables may 
accordingly tests began table sorted order desired table algorithm 
slightly unfair array algorithm table stored specific order table cube algorithm large sort 
response time seconds number valid cells rolap alg multi way array alg loading rolap vs multi way array data set response time seconds number dimensions multi way array alg loading rolap alg 
rolap vs multi way array data set graphs figures compare methods data sets 
data set density increases size input table increases 
leads bigger group bys result table sizes increase 
rolap method need memory due increase size 
memory kept constant rolap method multiple passes performance progressively worse 
shall see growing cpu cost due multiple passes dominates cost 
array method array dimension sizes changing 
memory requirement single pass computation array method depends dimension sizes number valid data cells response time seconds size fourth dimension rolap alg multi way array alg loading rolap vs multi way array loading data set response time seconds size fourth dimension rolap alg rolap alg sorting multi way array alg loading rolap vs multi way array loading data set cases array method finish computation pass 
see smaller increase time required array method 
similarly data set size fourth dimension increased sizes group bys containing fourth dimension rolap computation grow 
input table size constant increase size group bys leads greater memory requirements 
due memory available constant mb rolap method reverts multiple passes performance suffers 
turning array algorithm array sizes increase due increase size fourth dimension 
optimal dimension order theorem biggest fourth dimension kept 
furthermore corollary size dimension affect memory required fo ra single pass computation 
memory requirements array algorithm remains constant mb computes pass 
running time array method increase significantly 
data set vary number dimensions 
number group bys computed exponential number dimensions 
algorithms compute group bys running time methods increases number dimensions 
molap algorithm rolap systems designed molap systems array method applied rolap system 
array method faster table method viable convert input table array cube array convert back resulting arrays tables 
approach persistent storage structure array query evaluation data structure hash table join 
experiments study performance approach 
comparison multi way array method loads data input table array step 
rolap method just computes cube input table previous case 
input table unsorted rolap method specifically sort input 
times data set shown 
seen array method loading faster rolap method 
repeated experiments sorted input table rolap method initial sorting step avoided 
times shown graph 
turns case multi way array method turns faster 
drilling performance section try explain multi way array method performs better rolap method 
experiments showed rolap method time spent cpu computations remaining rolap method reads writes data tables 
table sizes significantly bigger compressed arrays multi way array method 
rolap method reads writes data meaning running time due dominates time molap algorithm 
turning cpu usage profiling code significant percentage time spent sorting intermediate results time spent copying data 
sorts costly largely due large number tuple comparisons 
tuple comparisons incur lot cost multiple fields compared 
copying arises rolap method copy data generate result tuples 
copying expensive tuples bigger array cells molap algorithm 
hand multi way array method position method 
different cells array aggregated position incurring cost multiple sorts multidimensional nature array captures relationships dimensions 
array built computing different group bys incurs little cost 
potential problem array sparsity array size grow data sparse 
offset compression method effective 
compresses array different compressed chunks directly aggregated having decompress 
leads better performance array 
turns multi way array method cpu intensive rolap algorithm cpu time 
time spent doing aggregation spent converting offset index values processing compressed chunks 
multi way array method cube computation 
method overlaps computation different group bys minimal memory group 
proven dimension order algorithm minimizes total memory requirement algorithm 
performance results show multi way array method performs better previously published rolap algorithms 
fact performance benefits multi way array method substantial tests faster load array table cube array dump cubed array tables cube table directly 
suggests algorithm valuable rolap molap systems necessary system support arrays persistent storage type order obtain performance benefits algorithm 
agarwal agrawal deshpande naughton sarawagi ramakrishnan 
computation multidimensional aggregates 
proceedings nd international conference large databases mumbai bombay 
arbor software 
role multidimensional database data warehousing solution 
white arbor software 
www com papers html ccs codd codd 
providing olap line analytical processing user analysts mandate white codd associates 
www com papers html dewitt katz olken shapiro stonebraker wood 
implementation techniques main memory database systems 
proceedings sigmod boston 
gray bosworth layman pirahesh 
data cube relational aggregation operator generalizing group cross tabs sub totals 
technical report msr tr microsoft research advance technology division microsoft redmond 
gc 
olap relational multidimensional database systems 
sigmod record vol 

september 
ia information advantage 
olap scaling masses 
white information advantage 
www com mc stanford technology group 
product brochure 
www informix com informix products new plo brochure html ms incorporated 
case relational olap 
white incorporated 
www strategy com wp html oc oracle 
oracle olap products 
white oracle 
www oracle com products pdf psw pilot software 
olap 
white pilot software 
www com olap olap htm rj arbor software robert patent sm sunita sarawagi michael stonebraker efficient organization large multidimensional arrays 
proceedings eleventh international conference data engineering houston tx february 
wel welch 
technique high performance data compression 
ieee computer 
zhao tufte naughton 
performance array adt olap workloads 
technical report cs tr university wisconsin madison cs department may 
