analytical approach file prefetching hui lei dan duchamp computer science department columbia university new york ny cs columbia edu file prefetching effective technique improving file access performance 
file prefetching mechanism line analytic modeling interesting system events transparent higher levels 
mechanism incorporated client file cache manager seeks build semantic structures capture intrinsic correlations file accesses 
heuristically uses structures represent distinct file usage patterns exploits prefetch files file server 
show results simulation study working implementation 
measurements suggest method predict file accesses accuracy reduce cache rate application latency 
method imposes little overhead antagonistic circumstances 
reports effectiveness predictive file prefetching technique operates automatically sort information supplied applications users 
technique incorporated client cache manager extra requests server hopefully advance actual need data 
prefetched data placed client cache 
hypothesize pronounced regularity file access patterns relatively simple algorithms identify access pattern quickly spot re emerges run application 
particular build semantic data structures called access trees capture potentially useful information concerning interrelationships dependencies files 
access tree program records files referenced execution program 
program maintain number access trees virtual memory represent distinct file usage patterns 
program re executed compare access tree formed current activity saved access trees determine usage pattern recurring prefetch files remaining saved access tree 
file prefetching brings major advantages 
applications run faster hit file cache 
second burst load placed network prefetching done network bandwidth available demand 
hand main costs prefetching 
cpu cycles expended client determining prefetch 
cycles spent overhead gathering information necessary prefetch decisions carrying prefetch 
second cost network bandwidth server capacity wasted prefetch decisions inevitably prove perfect 
conducted study unix environment ubiquitous academia research community 
recognized unix files accessed entirety sequentially 
take advantage phenomenon ways 
effectively model file system events level files look access patterns files 
second file predicted prefetch initial portion file letting standard sequential readahead mechanism bring rest file demanded 
section details prefetching mechanism 
section reports results trace driven simulation 
section describes implementation presents initial performance data 
section discusses related 
example access trees 
graphs show access trees generated events described section 
graph shows version compression 
graph vertical horizontal compression assuming shell program 
mechanism data abstraction access tree program invoke programs 
unix style operating systems usually realized executing program forking child processes turn execute programs 
programs may open data files 
file application formulated tree data structure dubbed access tree 
files program files data files nodes edge drawn parent child program invokes program program opens data file order siblings reflects chronology file accesses 
depicts access tree application includes activities 
program invokes program 
opens data files order 
opens 
invokes programs 
opens 
invokes program 
opens access tree subject kinds compression 
vertical compression draws edges unix shells effectively cutting access tree 
necessary role shells play command interpreters 
shells invoked variety circumstances generate large number file usage patterns 
perform file prefetching program file access tree infeasible prefetch accurately shells choose ignore 
horizontal compression removes consecutive accesses data file 
detail consecutive accesses offers help prefetching safely omitted 
non consecutive accesses data file preserved phase 
assuming program shell previous example shows access tree compressions 
access tree describes context file incurred application 
feel context information available prefetching mechanism decrease apparent randomness file system events permit accurate predictions file 
application proceeds construct access tree intercepting fork execve open chdir exit system calls 
execve open calls deal program files data files respectively 
fork provides information processes program executions associated 
information chdir calls resolve relative pathnames files 
access tree completed exit program execution 
access tree constructed current activity called working tree finished access tree saved exemplify file usage pattern called pattern tree 
prefetch algorithm heuristic function compatibility tree tree returns indication degree working tree tree pattern tree tree resemble 
function value constant match threshold say trees match meaning similar considered belong access pattern 
shall explain definition compatibility function detail discuss basic operation mechanism 
mentioned earlier number pattern trees saved virtual memory program working tree constructed course program execution 
program file new child node added working tree program analysis performed find saved pattern trees prefetched 
analysis follows simple guideline previously prefetched pattern tree current working tree longer matches prefetched pattern tree compute compatibility working tree pattern trees program 
match prefetch pattern tree highest compatibility 
pattern tree bears highest compatibility prefetch saved 
analysis carried executable file inside working tree executable initiates file 
point pattern tree may prefetched executable result prefetch analysis incurred earlier file accesses form subtree larger tree prefetched higher level 
executable prefetch pattern tree analysis result 
effectively allows minor prefetch corrections reducing cost bad guess 
complications may arise pattern tree selected prefetching large prefetched files may evicted cache referenced cache considerably destroyed prefetching guess bad 
diminish extent problems place upper limit prefetch capacity number files pattern tree shall prefetch time 
pattern tree selected large prefetch files initial portion tree prefetch limit exceeded 
record immediate child node pattern tree prefetch 
working tree extends child prefetch remaining portion pattern tree latest working tree matches 
measure prefetch capacity number files number bytes measure goes access tree structure tree node represents file 
illustrated initial portion predicted file prefetched cost incorrect prediction proportional number prefetched files 
program exit compare newly completed working tree saved pattern trees 
doesn match pattern trees working tree saved new information 
substituted pattern tree matches best 
set algorithmic parameters values ffl match threshold computation compatibility 
graph shows pattern tree 
graph shows finished working tree bearing compatibility pattern tree 
graph shows unfinished working tree far bearing compatibility pattern tree pivot pattern tree corresponds latest child node working tree formed 
ffl prefetch capacity behavior mechanism insensitive value parameters 
shall illustrate 
compatibility computation take closer look compatibility function 
function essentially similarity metric access trees abstracts complexities prefetch analysis pattern tree maintenance 
illustrate definition example 
pattern tree shown 
consider case working tree finished 
try pair immediate child nodes working tree identical child nodes pattern tree preserving order nodes 
recall child node represent executable file may root access tree data file 
trees considered describe file access pattern require correspondence executable files data files 
executable file may root different access subtree working tree pattern tree 
define percentage data files paired percentage pairs executables root access subtree 
intuitively suggests compatible data files suggests compatible executable files 
choose average values compatibility trees question 
consider finished working tree 
data files working tree pattern tree 
files appearances paired making 
executable pairs root identical access subtrees 
compatibility 
case unfinished working tree similar child pattern tree determined pivot node 
pivot corresponds added child node working tree 
child nodes pattern tree appear pivot involved compatibility computation 
pattern tree selected prefetch files follow pivot sequence pattern tree pivot probably accessed 
unfinished working tree node pivot pattern tree 
giving rise compatibility 
decide prefetch pattern tree files subtrees tf tg prefetched 
compatibility function invoked important expensive 
examine immediate child nodes 
time complexity computation proportional number child nodes 
simulation initial assessments mechanism obtained trace driven simulation 
method gathered file traces sunos :10.1.1.52.4504
version sunos offers secure computing facility includes ability produce system call audit trail 
feature gathered traces volunteer user performing normal activity period weeks 
trace contains invocations previously enumerated system calls captured hours 
second trace contains invocations captured hours numbers third trace respectively 
hours activity varied widely included compilations document production data analysis display large file searches news reading printing operations 
simulated cache manager lru replacement policy 
stepped traces maintaining cache accordance user file accesses 
trace data lacks file sizes defined cache size number files 
varied cache size cache size compared results prefetching prefetching 
results metric cache rate 
results traces appears 
prefetching delivers substantially better rate cache sizes 
results worse second trace includes recursive directory traversal unix find program hierarchy includes thousands files accessed 
addition monitored cache behavior finer grain mechanism worked run accesses 
run checked see prefetching beaten measuring number misses run 
shown table prefetching won comparison easily consistently 
shows mechanism superiority steady stable simply result exceptionally fruitful prefetch sequences 
losses due bad prefetching guesses offset wins 
table indicate increased intelligence mechanism effective smaller caches 
expected 
consider extreme case infinitely large cache file appearing access tree cache 
room improvement prefetching algorithm solely information past 
measured accuracy prefetch decisions defined percentage file access predictions 
calculated overhead mechanism percentage file fetches initiated due bad prefetch decisions 
reflects network bandwidth server capacity wasted 
table shows results 
shall address overhead cpu cycles expended prefetcher section 
lru policy bigger caches give better accuracy results prefetched entries better chance survive cache entry replacements 
similarly bigger caches give better overhead results bad predictions bet pf pf cache rate trace pf pf cache rate trace pf pf cache rate trace cache rate 
charts compare accumulative cache rate prefetching pf prefetching pf 
comparison performed different cache sizes measured number files 
ter chance existed cache case fetch performed 
compatibility function plays critical role mechanism 
assumption underlying trace cache size wins ties losses table finer grained comparison 
run file accesses determined prefetching beaten prefetching measuring number cache misses run 
trace cache size accuracy overhead table prefetching accuracy overhead 
accuracy percentage predictions 
overhead percentage file fetches due bad guesses 
function initial portions access trees bear high compatibility trees entirety 
order test assumption examined number loads total number times prefetched pattern trees 
classified loads proper improper 
load improper different pattern tree selected prefetcher take place current finished working tree match pattern tree loaded 
load proper 
results suggest compatibility function effective 
note algorithmic parameters stable small variations settings produce large performance degradations 
proper improper trace trace trace effectiveness compatibility function 
illustrates trace split pattern tree loads proper ones improper ones 
illustrate stability algorithmic parameters measure mechanism wide range parameter values 
show results trace cache size 
measurements cache rate predication accuracy prefetch overhead 
results traces cache sizes similar 
simulation demonstrates algorithm accurately predict file accesses past file usage 
major limitation simulation study account relative timing events due absence information traces 
result prefetched files assumed appear cache instantaneously 
remains determined real system resources exploit information file accesses 
limitations simulation motivated conduct full implementation evaluations 
implementation implemented mechanism ux bsd unix server running mach 
ux resides user space organized collection threads 
threads handle bsd system calls 
nfs async daemons handle asynchronous nfs block requests 
expect network file accesses performance bottleneck client server architecture prefetch nfs files opened read 
prefetch capacity match threshold rate cache rate prefetch capacity match threshold accuracy prediction accuracy prefetch capacity match threshold overhead prefetch overhead mechanism stability 
presents measurements prefetching mechanism wide range parameter values 
match threshold varied 
prefetch capacity prefetching 
measurements taken trace cache size 
structure shows basic structure implementation box stands thread shaded area constitutes prefetcher 
prefetcher consists pieces code 
system independent prefetch engine handles prefetch analysis working tree construction pattern tree maintenance 
code simulation 
implementation bsd service threads modified provide prefetch engine information fork execve open chdir exit system call 
prefetch engine processes information prefetch decisions enters files prefetched queue 
second piece prefetcher code added thread called prefetch daemon 
prefetch daemon consumes file queue produces block read requests queue 
requests satisfied async daemons 
read operation prefetch ends block placed system buffer cache 
copy user space necessary 
files prefetch blocks read prefetch daemon nfs async daemons bsd service threads info sys calls prefetch engine ux structure implementation 
prefetch daemon added collection threads ux 
bsd service thread extended call prefetch engine relevant system call serviced 
prefetch daemon takes advantage nfs readahead logic prefetching block file 
requested block number number block read nfs readahead logic speculates file accessed sequentially initiates asynchronous read block requested block 
requested blocks needed establish sequential access pattern nfs starts readahead 
accordingly removes entry file queue prefetch daemon generates read request file block block 
sets 
block accessed request hit cache readahead block issued 
scheme moves file read block block norm 
prefetch daemon issues block read file prefetched cost prefetching minimal 
ensured prefetch yields regular nfs asynchronous nfs async daemons regular certain number prefetch prefetch started limit reached pending regular regular asynchronous serviced promptly prefetcher refrain issuing prefetch 
ensures prefetching halts system capacity limit reached add extra load overload 
implementation consists approximately lines code 
lines added existent ux source files lines separate files lines files 
controlled experiments started evaluation implementation simple controlled experiments 
questions motivated experiments ffl potential benefits prefetching ffl antagonistic circumstances bearing prefetching performance 
ffl cpu overhead due prefetching 
experiment shell script consists tens filter programs reads parametric input file performs transformation writes output file 
known unix programs filters included script awk compress sed sort strings uniq members grep family 
input files size reside remotely 
second experiment composed program builds 
builds may ideally suited prefetching antagonistic reasons 
hardware platform build cpu intensive leaving prefetch mechanism relatively little excess cpu cycles prefetch decisions 
second compilations access header files rapid fire meaning time prefetch decision actual need cache rate file size buffer cache name cache latency blocks pf pf pf pf pf pf speedup cache rate file size buffer cache name cache latency blocks pf pf pf pf pf pf speedup table filters experiment 
table summarizes performance results filters experiment 
part presents results mb sec wired link part mb sec wireless link 
data may sufficient complete prefetch experiments run standalone 
client server processors mb memory 
client mb unix buffer cache 
interested find mechanism behaves relation different network bandwidth ran tests hardwired wireless links directly connecting client server 
hardwired connection ethernet mb sec wireless link ncr wavelan radio link maximum mb sec data rate 
combination experiment network link ran tests prefetching 
number reported mean trials 
table shows results filters experiment run wired link 
varied load different input file sizes file size column 
size measured multiples server preferred nfs block size kb 
load list unix buffer cache rate directory name lookup cache rate 
mechanism deals exclusively network files cache rates remote entries 
reduction rates due prefetching substantial 
measurements application latency total elapsed time couldn simulation 
user standpoint latency important performance metric 
time measurements seconds standard deviations included parentheses 
speedup column ratio prefetching latency prefetching latency 
speedups significant particularly input files relatively small delay caused sequential file access mainly lies block reads 
remaining blocks brought cache nfs readahead logic needed 
table shows results experiment run wireless link 
application latency larger slower link speedups comparable wired setup 
cache rates prefetching slightly higher wireless link wired link small amount prefetch operations completed time demanded 
bandwidth wireless link adequate perform nfs readahead timely basis suffices prefetcher read file block prediction 
graphically illustrates application latency filters experiment 
second experiment consists builds unix utility programs 
table contains results tests 
application latency illustrated 
large number header files opened quick succession common compilation prefetcher file size blocks pf pf file size blocks pf pf filters experiment comparison application latency 
graphs illustrate latency data table 
part compares latency wired setting part wireless setting 
prefetching results significant speedup experiment 
time available cpu cycles fruitful prefetch speculate accurately files soon referenced 
relatively small name cache rate improvements confirm 
prefetching manages reduce buffer cache rate 
significant latency enhancement suggesting total file read time dominant application latency 
pleased see negative effects observed adverse case 
simulation traces demonstrate prefetch overhead terms wasted network bandwidth server capacity 
implementation enables collect data type cost extra cpu consumption 
table presents results experiments 
cpu time includes system time user time 
time measurements seconds standard deviations parentheses 
cpu time overhead negligible cases 
show ratio cpu time application latency cpu latency 
comes surprise prefetching increases ratio substantially experiment 
prefetcher reduces total elapsed time increasing parallelism cpu processing discussion necessary conditions prefetching useful 
spare capacity data pipe extends server disk client cache 
pipe consists client network interface network server cpu server disk network paths 
second system resources client side prefetcher 
special importance cpu cycles 
prefetcher fulfill duty doesn acquire cpu cycles promptly amount cpu time needed low 
third workload allow interval file accesses prefetched started completed ahead demand 
prefetching proves feasible wired wireless network connections 
experiment results suggest speedups situations comparable speedup may better appreciated client wireless link 
slower link job lasts longer real time saved prefetching 
ratio cpu speed network speed increases prefetching provide benefit 
subtlety implementation scheme 
ratio lowered certain point longer able depend nfs readahead function bring subsequent file blocks quickly 
difficulty tackled simple generalization initial approach 
prefetcher prefetch file blocks growing cpu network ratio nfs readahead code modified read th file block immediately block 
currently simply set 
cache rate buffer cache name cache latency utility pf pf pf pf pf pf speedup hash info cache rate buffer cache name cache latency utility pf pf pf pf pf pf speedup hash info table builds experiment 
table summarizes performance results builds experiment 
part presents results mb sec wired link part mb sec wireless link 
related prefetching old idea 
studied extensively various areas including prefetching files prefetching database objects 
major impact computer architecture tight time complexity constraints paging hardware software 
prefetching files database objects promising endeavor reasons 
file database accesses frequent page accesses speed decision prefetch essence 
secondly resource needed arrange intelligent prefetching client cpu cycles resource excess distributed systems 
researchers looked prefetching blocks files 
sequential readahead primitive successful approach 
kotz ellis see example focuses uses prefetching increase bandwidth mimd shared memory architectures 
prefetching methods geared patterns scientific database applications running multiprocessors 
contrast methods looks access patterns files 
file prefetching methods require application inform operating system demands 
includes tip project patterson cao karlin li :10.1.1.118.6273
researchers consider interaction prefetching caching carefully 
tip uses application disclosed access patterns dynamically allocate file buffers competing demands prefetching caching model 
cao allow applications pass prefetching hints caching hints 
employ integrated algorithm prefetching caching shown theoretically near optimal 
informed approaches possess advantage prefetching driven deductions snooping certain knowledge provided advance higher levels 
danger incorrect speculative prefetching trash cache 
hand approaches require re coding applications 
prefetching mechanism act interval higher level learns need initiates interval may sufficient perform prefetch number prefetching methods completely transparent clients 
past accesses predict accesses 
seek build semantic structures access trees endowed application level meaning approaches probabilistic method model user behavior 
general form probabilistic method regards file string events uses information pf pf hash info latency secs pf pf hash info latency secs builds experiment comparison application latency 
graphs illustrate latency data table 
part compares latency wired setting part wireless setting 
observable negative effects antagonistic experiment 
events estimate frequency distributions events called context order lookahead size respectively 
simplicity existing methods single context order single lookahead 
tree approach attempt build understanding files referenced 
examples probabilistic methods prefetching objects object oriented database griffioen appleton kroeger long prefetch files 
adapt context modeling techniques data compression predict access 
inspired idea data compressor able predict data 
griffioen appleton comparison employs probability graph file accumulates frequency counts files accessed lookahead window 
initial stages considered probabilistic modeling 
implemented simulator extremely simple probabilistic methods called stupid pairs smart pairs ffl stupid pairs file accessed prefetch file accessed immediately time accessed 
sample series accesses remember fg prefetch remember remember prefetch remember cache hit 
ffl smart pairs keep track files accessed immediately accessed time choose frequency distribution 
sample series accesses pairs shown prefetch prefetch weighting prefetch weighting 
stupid pair scheme worked better smart pair approach applied traces 
unexpected result explained considering locality accesses user works file group files time moves similar operations different files 
stupid pairs equipped handle usage pattern invariably prefetch successor file 
seemingly superior intelligence smart pairs liability locality strong files longer active may undue weight heavily accessed past 
phenomenon information better provided identifies common weakness existing probabilistic methods don address problem aged information 
predictions total history accesses 
addition single lookahead methods widely applicable latencies grow terms cpu cycles prefetches advance complete time 
hand single context order methods griffioen appleton fail full historical file access information unable confidently wired link mb sec wireless link mb sec cpu time cpu latency cpu time cpu latency file size pf pf pf pf pf pf pf pf wired link mb sec wireless link mb sec cpu time cpu latency cpu time cpu latency utility pf pf pf pf pf pf pf pf hash info table cpu time consumption 
table presents cpu time prefetching different network links 
ratio cpu time application latency 
part give results filters experiment part builds experiment 
fer accesses far 
similar palmer zdonik fido explicitly recognizes maintains access patterns 
important aspects different 
conducted context object oriented database systems context file systems 
second represent access patterns strings object identifiers semantics involved 
represent patterns access trees 
third employ specialized pattern memory store pattern trees virtual memory 
notably fido requires separate training phase user session mechanism line line computation periodic analysis needed 
issue related prefetching hoarding 
prefetching hoarding involve anticipatory file fetches bringing files remote servers local cache needed 
exactly techniques 
hoarding scheme designed increase likelihood mobile client able continue working periods total disconnection file servers 
hoarding relatively infrequent operation performed client request prior disconnection timing critical 
hand prefetching mainly concerned improving performance timing important 
prefetching file server assumed accessible network connectivity may weak 
cache catastrophic disconnected operations hoarding typically willing substantially order enhance availability files 
despite differences idea uncovering exploiting semantic structure underlying file applies hoarding problem shown 
technique transparent online file prefetching 
technique analytically models interesting system calls builds semantic structures capture intrinsic correlations file 
accurate predictions file accesses imposes little cpu overhead defers demand delivers substantially lower client cache rates elapsed time intensive applications 
central trait algorithm spends client cpu cycles return effective client cache space fewer demand network operations 
distinguishing aspect algorithm lookahead ability potentially greater previous 
traits help couple application performance closely cpu speed device speed addressing fundamental longstanding problem operating systems 
initial performance evaluation encouraging 
intend extend evaluation directions 
run experiments model user behavior realistically 
process synthesizing workloads actual file system traces 
second plan conduct experiments wider spectrum network client capacities 
hope reach understanding prefetching feasible 
third examine applicability prefetching algorithm different kinds file systems local file systems remote file systems caching file blocks main memory remote file systems file caching client disks 
supported part advanced research projects agency arpa order number contract monitored office naval research part center telecommunications research nsf engineering research center supported number ecd 
wish carl tait constructive conversations insightful comments 
wish anonymous reviewers usenix shepherd carl staelin remarks 
accetta baron bolosky golub rashid tevanian young 
mach new kernel foundation unix development 
proc 
usenix summer conf pages june 
baker hartman kupfer shirriff ousterhout 
measurements distributed file system 
proc 
thirteenth symp 
operating system principles pages 
acm october 
cao felten karlin li 
study integrated prefetching caching strategies 
proc 
acm sigmetrics pages june 
cao felten karlin li 
implementation performance integrated application controlled caching prefetching disk scheduling 
proc 
usenix symposium operating systems design implementation pages november 
cooper draves 
threads 
technical report cmu cs carnegie mellon university june 
krishnan vitter 
practical prefetching data compression 
proc 
acm sigmod pages may 

multics input output system 
proc 
third symp 
operating system principles pages 
acm 
golub dean forin rashid 
unix application program 
proc 
summer usenix conf usenix pages june 
griffioen appleton 
reducing file system latency predictive approach 
proc 
usenix summer conf pages june 
griffioen appleton 
performance measurements automatic prefetching 
parallel distributed computing systems pages ieee september 
honeyman 
disconnected operation afs 
proc 
usenix symp 
mobile location independent computing pages august 
kistler satyanarayanan 
disconnected operation coda file system 
proc 
thirteenth symp 
operating system principles pages 
acm october 
korner 
intelligent caching remote file service 
proc 
tenth intl 
conf 
distributed computing systems pages 
ieee may 
kotz ellis 
practical prefetching techniques parallel file systems 
proc 
intl 
conf 
parallel distributed information systems pages 
acm december 
kroeger long 
predicting file system actions prior events 
proc 
usenix annual technical conf pages january 
kuenning 
design seer predictive caching system 
proc 
workshop mobile computing systems applications acm ieee pages december 
mckusick joy leffler fabry 
fast file system unix 
acm trans 
computer systems august 
ousterhout 
aren operating systems getting faster fast hardware 
proc 
usenix summer conf pages june 
ousterhout da costa harrison kunze kupfer thompson 
trace driven analysis unix bsd file system 
proc 
tenth symp 
operating system principles pages 
acm december 
palmer zdonik 
fido cache learns fetch 
proc 
th intl 
conf 
large data bases pages september 
patterson gibson satyanarayanan 
status report research transparent informed prefetching 
operating systems review april 
patterson gibson zelenka 
informed prefetching caching 
proc 
fifteenth symp 
operating system principles pages 
acm december 
sandberg goldberg kleiman walsh lyon 
design implementation sun network filesystem 
proc 
usenix summer conf pages june 
tait duchamp 
detection exploitation file working sets 
proc 
eleventh intl 
conf 
distributed computing systems pages 
ieee may 
tait lei acharya chang 
intelligent file hoarding mobile computers 
proc 
intl 
conf 
mobile computing networking pages acm november 
tuch 
engineer story wavelan 
proc 
virginia tech symp 
wireless personal communications june 
