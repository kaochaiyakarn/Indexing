bayesian density estimation inference mixtures michael escobar mike west department statistics institute statistics university toronto decision sciences toronto ontario duke university canada durham nc describe illustrate bayesian inference models density estimation mixtures dirichlet processes 
models provide natural settings density estimation exemplified special cases data modelled sample mixtures normal distributions 
efficient simulation methods approximate various prior posterior predictive distributions 
allows direct inference variety practical issues including problems local versus global smoothing uncertainty density estimates assessment modality inference numbers components 
convergence results established general class normal mixture models 
keywords kernel estimation mixtures dirichlet processes multimodality normal mixtures posterior sampling smoothing parameter estimation michael escobar assistant professor department statistics department preventive medicine biostatistics university toronto canada 
mike west professor director institute statistics decision sciences duke university durham nc 
michael escobar partially financed national cancer institute ro ca national research service award nimh mh national science engineering research council canada 
mike west partially financed national science foundation dms dms 
authors steve maceachern helpful discussions 
bayesian density estimation july 
models uncertain data distributions mixtures standard components normal mixtures underly mainstream approaches density estimation including kernel techniques silverman nonparametric maximum likelihood lindsay bayesian approaches mixtures dirichlet processes ferguson 
provide theoretical bases traditional nonparametric methods kernel techniques modelling framework various practical problems local versus global smoothing smoothing parameter estimation assessment uncertainty density estimates may addressed 
contrast nonparametric approaches formal model allows problems addressed directly inference relevant model parameters 
discuss issues data distributions derived normal mixtures framework mixtures dirichlet processes essentially framework ferguson 
west discusses models special case framework studied 
concerned developing approximations predictive distributions clustering algorithm motivated model structure draws obvious connections kernel approaches 
current develops general framework computational method allow evaluation posterior distributions models parameters direct evaluation predictive distributions 
natural product develop approaches inference numbers components modes population distribution 
computational method developed direct extension method escobar example gibbs sampler markov chains monte carlo method gelfand smith 
earlier markov chain monte carlo methods include geman geman metropolis tanner wong 
besag green smith roberts review markov chain monte carlo methods 
basic normal mixture model similar ferguson described follows 
suppose data conditionally independent normally distributed means variances determining parameters suppose come prior distribution theta 
having observed data fy observed value distribution case mixture normals relevant density function mixed respect posterior predictive distribution jd common prior distribution uncertain modelled part dirichlet process data come dirichlet mixture normals ferguson escobar west 
important special case studied widely appear west considers common setup uncertain prior modelled dirichlet process normal base measure see west cao 
connections kernel estimation techniques explored papers analytic numerical approximations predictive distributions derived models 
analysis covers problems estimating escobar considers similar models differing uniform dirichlet process base bayesian density estimation july measure assuming known 
ferguson monte carlo techniques kuo considers generally case possibly distinct uncertain suitability model form density estimation argued earlier 
suitable dirichlet process prior structure described model produces predictive distributions qualitatively similar kernel techniques catering differing degrees smoothing sample space possibly differing variances structure posterior distribution strongly support common values individual parameters data points close combining information locally sample space estimate local structure 
proceed general model noting similar discussion analysis applies restricted global smoothing version section completes model specification reviews implications 
section develops computational technique monte carlo analysis extending technique escobar 
improves importance sampling simulation analysis ferguson kuo provides efficient sampling posterior distribution model parameters section discusses prior posterior inference number components discrete mixture multi modality developed application section problem astronomy considered roeder 
section discusses advanced techniques related smoothing parameter dirichlet process illustration 
summary discussion contained section 
appendix discuss convergence issues monte carlo analysis 

normal mixture models prediction suppose normal means variances come prior distribution theta 
uncertain modelled dirichlet process data come dirichlet mixture normals escobar ferguson west 
particular suppose ffg dirichlet process defined ff positive scalar specified bivariate distribution function theta prior expectation efg theta ff precision parameter determining concentration prior 
write key feature model structure analysis relates discreteness dirichlet process assumption 
details may ferguson 
briefly sample size positive probability coincident values 
see follows 
gamma conditional prior ffa gamma gamma ffi ffi denotes unit point mass ff positive integers bayesian density estimation july similarly distribution ffa ffi sample size case represents new distinct value probability ffa drawn uniformly values 
values behave described positive probability reduce distinct values 
write distinct values elements suppose occurrences index set occurrences immediately reduces mixture fewer components ffa ffi theory summarised antoniak gives prior induced dirichlet process model 
prior distribution critically depends ff stochastically increasing ff 
instance moderately large ff ln ff 
practical density estimation suitable values ff typically small relative ff corresponds initial prior receiving weight observation posterior 
say prior heavily favours single digit values 
proceed need specify prior mean 
convenient form normal inverse gamma conjugate normal sampling model assume gamma gamma prior shape scale jv mean scale factor moment assume prior parameters specified 
generically jd represent distribution quantity respect predicting clear may evaluated dp 
component integrand normal sampling distribution second imply ffa student distribution degrees freedom mode scale factor equivalently reduced form ffa discussed ferguson strong relationships standard kernel density estimates silverman 
standard kernel density estimator normal kernel estimate jd jd gamma window width bayesian density estimation july addition obvious data estimation smoothing parameters inducing varying sample space involves types shrinkage shrunk means density estimate shrunk initial prior 
bayesian prediction density estimation problem solved summarising unconditional predictive distribution jd dp jd direct evaluation computationally extremely involved small sample size due inherent complexity posterior jd antoniak escobar lo west 
fortunately monte carlo approximation possible extensions iterative technique escobar described 

computations recall gamma note jd conditional structure 
conditional posterior mixture ffi bivariate normal inverse gamma distribution components gamma gammam jv xv weights defined ffc gamma sm gamma expf gamma gamma gamma subject gamma gamma gamma gamma gamma just posterior distribution jy prior weight proportional ff times marginal density evaluated datum prior model proportional ff times density function evaluated weight proportional likelihood data sample normal distribution just density function point conditional distribution weighted mixture best guess prior single atom distributions values conditioned 
weights determined relative predictive densities data value conditional distributions easily sampled straightforward sample fact important iterative resampling process provides single approximate draw joint posterior jd follows 
bayesian density estimation july algorithm 
choose starting value reasonable initial values samples individual conditional posteriors 
ii sequentially sample elements drawing distribution relevant elements sampled values inserted conditioning vectors step 
iii return ii proceed iteratively convergence 
sampling process computationally straightforward 
note implementation required computations reduced fact mixtures reduce typically fewer apparent components due clustering elements elements earlier superscript denote distinct values suppose conditioning quantities concentrate gamma distinct values common value 
reduces ffi weights include viz expf gamma gamma gamma sampling process results approximate draw jd 
escobar discusses theoretical aspects convergence simpler case known 
unfortunately proof simple case extend easily model get arbitrarily close 
results violation condition required escobar feller pp tanner wong 
results tierney press monograph 
theorem stated proof additional discussion convergence issues contained appendix 
probability initial value iteration algorithm produces sample value contained measurable set probability initial value iterations algorithm produces sample value contained measurable set markov chain implied algorithm delta delta called transition kernel markov chain 
explicit representation transition kernel similar algorithms involving dirichlet processes see escobar 
fixed value delta delta probability measures fixed measurable set delta delta measurable functions 
metric delta total variation norm defined tierney press 
posterior distribution 
theorems follow conditions surely respect measure generated posterior distribution 
theorem 
starting values algorithm probability measure converges total variation norm posterior distribution goes infinity 
lim kq delta gamma initial prior variance plays critical role determining extent smoothing analysis 
distinct values elements larger value leads increased dispersion group means fixed leads greater bayesian density estimation july chance multi modality resulting predictive distribution 
restricted models choice relates choice window widths traditional kernel density estimation 
typically information content data estimating small prior reasonably informative 
relevant general setting 
conditionally conjugate structure built model easily allows extension sampling analysis include learning prior parameters suppose independent priors form gamma specified hyperparameters follows conditionally independent normally distributed moments mj gamma gamma mj gamma gamma sums conditionally independent inverse gamma posterior gamma jm gamma incorporating iterative resampling scheme provides sampling complete joint posterior jd 
steps iii may modified follows algorithm ii 
generate initial conditional preliminary chosen values 
sample order relevant distributions just described 
ii ii sampled values 
iii return proceed iteratively convergence 
extending notation introduced algorithm algorithm ii get convergence theorem 
proof involves straightforward extension arguments proof theorem 
theorem 
starting values algorithm ii probability measure ii converges total variation norm posterior distribution goes infinity 
lim kq ii delta gamma specified initial values iterate sampling procedure burn process approximate convergence 
burn successively generated values assumed drawn posterior denote values specified simulation sample size required 
approximate predictive inference follows monte carlo approximation jd gamma summands mixtures notation explicitly recognizes dependence sampled values additional information available includes sampled values fk ng directly provide histogram approximation bayesian density estimation july interest assessing number components 
posteriors may approximated mixture conditional posteriors noted general principles expounded gelfand smith leads mixture normals gamma mj mixture inverse gammas jd gamma jm sums case 
theorem tierney press shown path averages bounded functions converge surely posterior expectations 
estimates cumulative distribution functions estimates probability functions discrete random variables histogram estimates probability density functions converge surely posterior expectations 
theorems state estimates probability density functions converge surely posterior expectations 
theorem 
estimate predictive density evaluated fixed point strongly consistent starting values algorithm 
starting values fixed value gamma gamma gamma gamma gamma 
jd theorem 
estimate posterior density evaluated fixed point strongly consistent starting values algorithm 
starting values fixed point gamma gamma gamma gamma gamma 
jd theorem 
estimate posterior density evaluated fixed point strongly consistent starting values algorithm 
starting values fixed point gamma gamma gamma gamma gamma 
jd 
mixture deconvolution common closely linked objectives density estimation assessment number components discrete mixture inference number modes population distribution 
roeder example nonparametric inference number modes mixture 
various methods exists inference modality mixtures hartigan hartigan silverman roeder approaches direct inference numbers components developed 
framework prior posterior distributions bayesian density estimation july number components underlying observed data set readily derived shown illustrate 
inference modality questions derived 
consider generating sample size model resulting predicting observation mixture 
knowledge mixture bayesian estimate population distribution 
number distinct components realised observations arise generated process drawing 
leading component allows observation come distinct component 
noted earlier dirichlet structure imposes prior depends ff 
problems number mixture components small relative say moderate sample sizes non negligible prior probabilities vary dramatically decay rapidly increases 
table illustrates ff sample sizes probabilities computed results antoniak 
cases model may considered proxy finite mixture model fixed uncertain conditions ff fairly small leading high prior probabilities small values implied prior sample size problem interest acceptable representation available prior information number components 
section assume conditions analysis astronomical data roeder 
table 
prior probabilities computations posterior predictive distributions described section information generated provides monte carlo approximation posterior observed data 
generating draw leads product value say posterior histogram approximation posterior induced may address question number components 
issues numbers modes numbers components secondary consideration practical perspective remain interest number modes provides conservative estimate lower bound number components rely heavily normal distributional assumption estimate number components 
particular point interest concerns implied prior distribution number modes expected predicting observation sample sample size 
bayesian density estimation july may calculated specified model extent predicted number modes satisfactorily represent informed prior opinion provides way assessing prior suitability model assumptions 
explored data analysis 
model permit easy analytic calculation prior number modes resort simulation follows 
conditional prior distributions gamma ffa gamma gamma gamma ffi 
may trivially sample joint prior drawing joint prior generating values sequentially sample remaining elements density prior predictive distribution may evaluated fine grid values searched count number modes 
repeating procedure provides random sample prior distribution number modes histogram estimate prior 
similarly calculate posterior distribution number modes simply counting recording number modes predictive density sample point 
simpler version strategy employed cases specified constant variances known unknown obvious modification 
note parameter ff determines number mixture components variance determining modality characteristics increases smoothing decreases prior favours larger numbers modes 

initial illustration roeder describes data representing measured velocities relative galaxy identifiable galaxies separated conic sections space 
roeder considers estimation density velocities represented finite mixture normals focuses effects uncertainty density estimates assessment multi modality particularly hypothesis 
scientific interest hypothesis galaxy clustering consistent big bang theory 
roeder states galaxies clumped distributions velocities multimodal mode representing cluster moves away speed 
purposes assessing scientific issue clustering appropriate focus number mixture components multi modality 
galaxies may clustered clumped components number modes exceed number components may lower data distribution possibly exhibiting inflection points skewness induced distinct heavily overlapping components 
related issues raised reviewed titterington 
sec 
sec 

cautionary note calculate posterior distribution number components remains inferential leap normal component represents galactic cluster 
bayesian density estimation july underlying assumption galactic cluster normal component 
distribution galactic cluster skewed light heavy tail may normal components fit galactic cluster component 
see titterington 
sec 
discussion 
detail elements results analysis ff distinct components may sampling cases probability observation drawn new component small 
prior number distinct components table 
recall determined ff note prior differs marginally table 
prior appreciable fairly diffuse smaller larger values positive probability 
table 
prior probabilities assist prior specification consider modality issue discussed section 
number modes predictive density observation sample size 
initial simplification consider model loss generality fixing value simple modification discussion section provides way compute prior number modes predictive density 
assumptions smoothing parameter critical modality issue unspecified quantity simulation exercise may performed various assess effect predictions 
simulation exercise performed values appearing table 
monte carlo sample size case estimated prior probabilities displayed numerical standard errors cross check accuracy note prior produced simulation cases summarised table monte carlo estimates prior probabilities agree exact values table decimal places 
exemplified table larger values lead increased chances larger values priors sensitive lower values practical viewpoint differences small 
bayesian density estimation july table 
prior probabilities standardised model information helps choosing parameters prior gamma 
choose low value shape setting prior degrees freedom parameter 
defines imprecise initial prior 
represents prior point estimate fact prior harmonic mean gamma gamma prior mode 
harmonic mean mode corresponding prior density appearing dashed line 
diffuse prior chosen analysis addition suitably diffuse note consistency predictive assessments modality issue 
fact analysis possible traditional improper prior proportional gamma noted reported essentially unchanged analysis prior 
prior conditional variances gamma note roeder citing original data source postman states error observed velocities estimated km second 
uncertainty current authors interpretation phrase fairly high indicating small initial precision parameter take 
location prior km second may variously interpreted estimate standard deviations take baseline standard deviation accounting experimental error velocity records 
reflect identifiable cluster galaxies may expected subject additional variation velocities 
roeder states expansion scenario universe points furthest galaxy moving greater velocities 
specify prior favours larger values degrees freedom recall units thousands kilometers second 
corresponding equal tails interval roughly km second 
proceeding analysis explore implied prior specification prior mean taken loss generality 
prior simulations described section carried full sampling joint prior prior number modes replications prior model appears table 
note relative various cases table prior diffuse due additional uncertainty probabilities quoted decimal places positive rapidly decreasing smaller larger values bayesian density estimation july table 
prior probabilities chosen priors posterior predictive analysis detailed framework additional final assumption diffuse prior limiting form prior gamma 
clearly informative priors 
analysis techniques section monte carlo sample size number iterations burn convergence set 
values supported experimentation different starting values suggest initial iterations adequate achieve stability estimated posterior distributions 
analyses varying sample sizes lead substantially similar inferences see section 
fact draws inference actual run times number saving draws iterations induces posterior samples consecutive values series negligible autocorrelations 
wasteful analysis reported order approximate posterior probabilities reported table acceptably accurate second decimal place assuming exactly independent draws represents upper bound posterior standard deviations posterior probabilities reported 
analyses coded risc fortran running ultrix standard numerical algorithms random variate generation ran press 
substance scientific issue galaxy clustering addressed posteriors corresponding monte carlo approximations analysis table 
prior table provides heavy support clusters whilst reasonably diffuse wider range 
posterior supports larger values 
prior centered lower values posterior distribution likelihood function puts weight large values alternative priors giving support larger values produce posteriors shifted upward 
typical inference overlapping mixtures clearly great deal uncertainty number components 
traditional approaches density estimation computations provide formal assessment uncertainty 
posterior heavily favours modes evident 
crude summary table conclude strong support components 
bayesian density estimation july table 
posterior probabilities roeder data ijd ijd varying ff repeating analysis provides insight just sensitive results ff sensitivity marked 
example repeat analyses ff increasing ff ff correspondingly shifts posterior smaller somewhat larger values differences predictive distribution functions undetectable 
effects varying ff inferences predictably smaller ff values shift posterior favouring higher values effects great due marked lack information analyses small numbers observations 
pursue sensitivity analyses defer section formally subsume sensitivity studies extended analyses incorporating learning ff 
learning ff illustration central analysis precision parameter ff underlying dirichlet process critical smoothing parameter model 
learning ff data may addressed view incorporating ff gibbs sampling analysis 
assume continuous prior density ff may depend sample size implied prior results antoniak ff gamma ff gamma ff involving ff 
required factors easily computed recurrence formulae stirling numbers details available request second author 
important example considering implications priors specific choices priors ff vice versa initial prior elicitation process 
aside note great deal flexibility representing prior opinions choices prior ff elaborated explored greater generality 
suppose sampled values parameters sampling parameters fact sampled value number distinct components sampled specific configuration data groups 
model data initially conditionally independent ff configuration known parameters conditionally independent ff configuration known 
deduce ff bayesian density estimation july likelihood function sample size appear conditioning course omitted clarity notation 
gibbs sampling analysis extended ff sample parameters usual conditional posterior jff 
iteration include ff analysis sampling conditional posterior previously sampled value information needed 
sampling may involve rejection method depending form prior ff alternatively may range ff provides discrete approximation posteriors called gibbs approach ritter tanner 
sampling exact continuous posterior possible gibbs iterations prior ff comes class mixtures gamma distributions 
develop results single gamma prior leave generalisations mixtures reader refer west 
suppose ff gamma prior shape scale may extend include prior uniform log ff letting case may expressed mixture gamma posteriors conditional distribution mixing parameter ff course simple beta 
see follows 
ff gamma functions written gamma ff gamma ff ff fi ff ff gamma fi usual beta function 
ff ff gamma ff fi ff ff ff gamma ff ff gamma gamma dx definition beta function 
implies marginal distribution joint ff continuous quantity ff jjk ff ff gamma ff ff gamma gamma ff 
conditional posteriors determined follows 
firstly prior ff ff gamma ff gammaff gammalog ff gamma gammaff gammalog nff gamma gammaff gammalog ff reduces easily mixture gamma densities viz gamma log gamma gamma gamma log weights defined gamma gamma fn gamma log note distributions defined gamma priors unit interval 
secondly ff gamma gamma ff beta distribution mean ff ff 
clear ff sampled stage simulation gibbs iteration currently sampled values ff allow draw new value ff bayesian density estimation july sampling value simple beta distribution conditional ff fixed values ii sampling new ff value mixture value just generated 
completion simulation ffjd estimated usual monte carlo average conditional forms viz ffjd gamma sampled values develop convergence theorems new algorithm 
proofs straightforward extensions results 
example prove new versions theorems bound expected posterior distributions bounds proven appendix bounds constants respect ff 
astronomical velocities data gamma prior ff density appears dashed line 
note fair degree support values near ff previous section 
assumptions details analysis previous section 
analysis summarised graphically 
displays histogram data table roeder graph estimated predictive density function equation 
density similar optimal density estimate roeder modes 
give qualitative indication uncertainty figures display graphs random selection just sampled predictive densities summands 
plots corresponding cumulative distribution functions appears figures 
nice way exhibit uncertainties density distribution functions live animated graphical display sequentially sampled functions tierney 
restricted static plots prefer displaying sampled curves bands mapping pointwise interval estimates functions define density distribution functions 
results summarised attest robustness ff values noted previous section far issues predictive density estimation concerned 
predictive distributions density functions substantially similar obtained various analyses noted ff fixed 
estimated posterior jd appears full line prior quite diffuse having long tail right plotted region 
presents corresponding prior posterior densities ff 
typical pictures information available small datasets smoothing parameters ff typically limited relates difficulties smoothing parameter estimation traditional approaches 
addressing substantive issue galaxy clustering inference table provides posteriors accounting estimation ff 
similar ff table diffuse larger values supporting components 
marked residual uncertainty number components 
inferences number modes similar table 
bayesian density estimation july table 
posterior probabilities hjd ff ijd ijd 
comments described illustrated bayesian density estimation mixture deconvolution classes models analyses routinely implementable stochastic simulation methods 
key contributions lie development computational techniques hierarchical mixture models models known years real utility realised 
problems hierarchical prior specification inference layers parameters hyperparameters particularly variance precision parameters control define degrees local smoothing addressed may currently various application areas incorporated routine data analyses models 
addition developing methodology demonstrating utility density estimation mixture identification provide theoretical results proving convergence implemented simulation schemes showing provide consistent numerical approximations exact bayesian posterior predictive distributions interest 
current areas active research extensions include generalizations elaborate multivariate linear models non linear models 
originally written resulting research refining basic computational methods particular maceachern press introduced important algorithms improve convergence characteristics see west muller escobar 
appendix convergence issues proofs helpful notion configuration defined west follows definition integer integer elements take values value appearing 
define configuration elements exactly distinct values called number equal fc kg 
bayesian density estimation july definition please note implies definition configurations posterior distribution produced algorithm fixed written jd jd jd sum unique configurations parameters prior distribution notation 
posterior distribution algorithm ii jd jd jd parameters prior distribution notion 
configuration associated dimensional subspace 
theta example configuration associated dimensional subspace jg 
define ck lebesgue measure subspace associated ck configuration posterior predictive distributions behave standard hierarchical normal models 
example jd ck mutually absolutely continuous 
posterior distribution mutually absolutely continuous 
conditioning configurations model reduces standard normal inverse gamma hierarchical model 
number configurations finite proofs consistency markov chain monte carlo estimates posterior predictive densities simple extension proofs standard normal inverse gamma hierarchical model 
authors know published proofs consistency estimates standard model 
standard normal inverse gamma hierarchical model model fixed configuration convergence density estimates standard model corollaries theorems 
proof theorem 
arguments proof theorems identical formally argue proof theorem 
theorem tierney press need show posterior distribution invariant distribution markov chain defined algorithm markov chain aperiodic irreducible respect posterior distribution 
proof invariance posterior distribution similar proof invariance contained theorem escobar 
construction markov chain see set starting points 
irreducible mutually absolute continuity irreducible respect posterior distribution 
implies aperiodic 
proof theorem 
order show convergence path averages theorem tierney press requires transition kernel markov chain converge total bayesian density estimation july variation norm posterior distribution chain harris recurrent posterior expectations bounded equal jd 
theorem know markov chain converges 
straightforward show path averages right expectation 
recurrent markov chain harris recurrent throwing away set starting values measure zero posterior distribution 
see follows 
know markov chain positive recurrent theorem tierney press 
theorems tweedie state state space divided disjoint sets set transient null set set absorbing set property markov chain restricted set harris recurrent 
starting values start chain absorbing set state space recurrent chain harris recurrent 
fuller discussion argument please see discussion theorems tweedie 
need show posterior expectations finite 
density function distribution degrees freedom mode scale factor evaluated value function bounded values gamma gamma gamma gamma sm gamma gamma gamma gamma gamma sm gamma sm gamma gamma gamma gamma theta gamma gamma gamma gamma gamma gamma gamma gamma density function normal distribution mean variance evaluated value function posterior expectation bounded values see note gamma gamma note fixed configuration posterior distribution gamma gamma distribution shape parameter scale parameter jl gamma jl th value gamma jl jc gamma gamma jc gamma gamma gamma gamma gamma gamma gamma gamma bayesian density estimation july density random variable defined equations 
bounds show posterior estimation bounded finite constant jd dp jc dp jd ffa dp jc dp jd ffa dp jc dp jd dp jd posterior expectation finite bound theorem tierney press proof complete 
proof theorem 
proof similar proof theorem 
proving theorem remains shown posterior expectation bounded 
comments statement algorithm ii gamma jm gamma fixed positive parameters prior distribution density function value jm 
density maximized 
evaluating jm mode get inequality stirling formula gamma function johnson kotz get second inequality jm exp gamma gamma gamma gamma exp 
jm bounded constant values posterior expectation jm jd bounded theorem proven 
proof theorem 
proof similar proof theorems remains shown posterior expectation bounded 
comments statement algorithm ii random variable normally distributed variance mj gamma gamma density function bayesian density estimation july value 
values density bounded gamma 
values gamma jm gamma gamma 
define gamma gamma gamma gamma gamma gamma gamma 
define constant gamma gamma ns gamma gamma ws gamma gamma jd dp dp jc dp jd gamma dp dp jc dp jd gamma gamma gamma dp dp jc dp jd gamma gamma gamma gamma dp jc dp jd gamma gamma gamma gamma dp jc dp jd gamma gamma ks gamma gamma ws gamma gamma dp jd posterior expectation bounded theorem proven 
antoniak 
mixtures dirichlet processes applications nonparametric problems annals statistics 
besag green 
spatial statistics bayesian computation journal royal statistical society ser 

escobar 
estimating means normal populations nonparametric estimation distribution means unpublished ph dissertation yale university dept statistics 
comment bayesian analysis mixtures results exact identification bayesian statistics eds bernardo berger dawid smith oxford oxford university press pp 

bayesian density estimation july estimating normal means dirichlet process prior journal american statistical association 
press nonparametric bayesian methods hierarchical models journal statistical inference planning 
feller 

probability theory applications vol 
nd ed new york john wiley sons 
ferguson 
bayesian analysis nonparametric problems annals statistics 
bayesian density estimation mixtures normal distributions advances statistics eds 
rizvi new york academic press pp 

gelfand smith 
sampling approaches calculating marginal densities journal american statistical association 
hartigan hartigan 
dip test annals statistics 

monte carlo sampling methods markov chains applications biometrika 
kuo 
computations mixtures dirichlet processes siam journal scientific statistical computing 
lindsay 
geometry mixture likelihoods part general theory annals statistics 
lo 
class bayesian nonparametric estimates 
density estimates annals statistics 
maceachern 
press estimating normal means conjugate style dirichlet process prior communication statistics 
metropolis rosenbluth rosenbluth teller teller 
equations state calculations fast computing machines journal chemical physics 
tweedie 
markov chains stochastic stability new york springer verlag 
general irreducible markov chains non negative operators cambridge cambridge university press 
postman 
probes large scale structure corona region astronomical journal 
press teukolsky flannery 
numerical recipes fortran nd edition cambridge cambridge university press 
ritter tanner 
facilitating gibbs sampler gibbs gibbs sampler journal american statistical association 
roeder 
density estimation confidence sets voids galaxies journal american statistical association 
bayesian density estimation july silverman 
kernel density estimates investigate multimodality journal royal statistical society ser 

density estimation statistics data analysis new york chapman hall 
smith roberts 
bayesian computation gibbs sampler related markov chain monte carlo methods journal royal statistical society ser 

tanner wong 
calculation posterior distributions data augmentation discussion journal american statistical association 
tierney 
exploring posterior distributions markov chains computer science statistics rd symposium interface ed pp 
tierney 
press markov chains exploring posterior distributions annals statistics 
titterington smith makov 

statistical analysis finite mixture distributions 
chichester john wiley sons 
west 
bayesian kernel density estimation discussion institute statistics decision sciences duke university 
modelling mixtures discussion bayesian statistics eds bernardo berger dawid smith oxford oxford university press pp 

hyperparameter estimation dirichlet process mixture models isds discussion duke university 
cao 
assessing mechanisms neural synaptic activity bayesian statistics science technology case studies eds hodges kass new york springer verlag pp 

muller escobar hierarchical priors mixture models applications regression density estimation aspects uncertainty tribute lindley eds smith freeman 
