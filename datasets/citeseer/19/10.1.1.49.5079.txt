introducing statistical dependencies structural constraints variable length sequence models sabine deligne fran cois fr ed eric bimbot enst dept signal dept informatique cnrs ura rue paris cedex france european union 
field natural language processing domains efficiency pattern recognition algorithms highly conditioned proper description underlying structure data 
hidden structure usually known learned examples 
multigram model originally designed extract variable length regularities streams symbols describing data concatenation statistically independent sequences :10.1.1.56.6619
description especially appealing case natural language corpora natural language syntactic regularities clearly variable length sentences composed variable number turn variable number words contain variable number morphemes 
previous experiments model revealed inadequacy independence assumption particular context phoneme transcription task 
goal twofold demonstrate theoretically possibility relax important assumption original multigram model suggest related model dependent variable length sequences relaxing hypothesis effective 
organized follows section briefly describe original multigram model multi level generalization 
sections original stochastic extensions mono dimensional model independence assumption relaxed show theoretically estimation new models feasible 
section introduces approach modeling dependencies adjacent sequences time non stochastic framework 
section presents experiments suggesting effectiveness dependencies account 
give section indications regarding extensions various models 
original multigram model formulation model multigram approach string symbols assumed result concatenation non overlapping sequences having maximum length symbols 
note fo dictionary emitted symbols drawn observable string symbols 
denote possible segmentation sequences symbols 
note resulting string sequences ts dictionary distinct sequences formed combining symbols noted fs possible segmentation assigned likelihood value likelihood string computed sum likelihoods segmentation gr flg decision oriented version model parses segmentation yielding approximation gr max flg model fully defined set parameters theta needed compute particular segmentation known 
computing likelihood independence assumption assume multigram sequences independent likelihood segmentation expressed multigram model defined set parameters theta consisting probability sequence theta fp ds 
maximum likelihood estimation model parameters estimation set parameters theta training corpus obtained maximum likelihood ml estimation incomplete data observed data string symbols unknown data underlying segmentation iterative ml estimates theta computed em algorithm 
auxiliary function computed likelihoods iterations flg log shown 
set parameters theta maximizes iteration leads increase corpus likelihood 
reestimation formula probability sequence iteration derived directly maximizing auxiliary function theta constraint parameters sum 
denoting number occurences sequence segmentation corpus rewrite joint likelihood group probabilities identical sequences auxiliary function expressed flg log function subject constraints maximum flg theta flg theta total number sequences equation shows estimate merely weighted average number occurences sequence segmentation 
iteration improves model sense increasing likelihood eventually converges critical point possibly local maximum 
reestimation implemented means forward backward algorithm 
set parameters theta initialized relative frequencies occurences symbols length training corpus 
theta iteratively reestimated training set likelihood increase significantly fixed number iterations 
practice pruning technique may advantageously applied dictionary sequences order avoid learning 
straightforward way proceed consists simply discarding iteration sequences probability value falling prespecified threshold 
joint multigram model formulation model multigram model easily generalizes joint multigram model deal case observable streams symbols drawn distinct alphabets 
strings assumed result parallel concatenation sequences possibly different lengths 
model allows matched sequences unequal length assumes alignment strings 
consider case streams omega 

omega assumed result concatenation pairs sequences oe model restricting length sequence length sequence oe omega refered joint multigram model 
note lo resp 
omega segmentation resp omega sequences corresponding joint segmentation omega paired sequences lo omega 
likelihood omega computed sum joint segmentations omega flg omega assuming subsequent pairs sequences independent likelihood joint segmentation product pair probability omega oe estimation joint multigram model parameters parameter estimation joint multigram model principles string multigram model 
oe denotes dictionary contains pairs sequences oe formed combinations symbols vocabulary oe formed combinations symbols vocabulary omega joint multigram model fully defined set theta pair probability oe 
replacing oe omega directly write parameter reestimation formula iteration oe flg oe theta omega flg theta omega oe number occurences pair oe total number matched sequences forward backward algorithm implementing detailed 
training procedure jointly parses strings maximum likelihood criterion 
produces dictionary pairs sequences automatic transduction purposes explained 
basic multigram model additional pruning dictionary may advantageously order avoid learning 
done posteriori discarding pairs sequences priori account pairs sequences compatible known possibly approximate pre alignment symbols streams 
application grapheme phoneme transduction task joint multigram model assigns probability pair sequences stochastic transducer 
study application task grapheme phoneme transduction 
assume instance joint multigram model estimated training set omega respectively string graphemes string phonemes 
training process consists mapping variable length sequences graphemes variable length sequences phonemes 
resulting set matched sequences probability occurence infer sequence sequence decoding process string phonemes omega corresponding test string graphemes pronunciation unknown 
transduction task stated standard maximum posteriori decoding problem consisting finding phonetic string omega stream omega argmax omega omega argmax omega omega assuming omega joint segmentation strings accounts likelihood inferred pronunciation results maximization approximated likelihood defined omega argmax omega omega omega argmax omega omega omega omega omega application bayes rule 
omega omega measures sequences segmentation match inferred phonetic sequences omega computed oe conditional probabilities deduced probabilities oe estimated training phase 
term omega omega measures likelihood inferred pronunciation estimated omega omega language model 
decoding strategy way impose constraints string omega constraints 
maximization rewrites omega argmax omega omega omega omega omega experiments reported sect 
component omega omega computed modeling succession phonetic sequences bigram model 
conditional probabilities attached successions estimated parsed version phonetic training stream iteration estimation algorithm 
grams multigrams computation observation likelihood relax independence assumption multigrams model correlations sequences gram model 
multigram sequence depends gamma previous sequences likelihood value particular segmentation computed gamman gamma original model likelihood computed sum possible segmentations see 
comparative example independent multigram model bi gram multigram model tab 

cd bc bcd ab ab cd abc cd bc bc bcd ab ab ab cd ab abc abc table 
likelihood abcd independent multigram model left bi gram multigram model right 
refers empty sequence 
gram model multigrams fully defined set parameters theta consisting gram conditional probabilities relative combination sequences theta fp gamma ds gamma 
estimation model parameters ml estimates parameters gram model multigrams derived exactly way case independent multigrams leading reestimation formula gamma flg gamma theta flg gamma theta number occurences combination sequences segmentation checked equal reduces derived estimation independent multigram model 
equation implemented means forwardbackward algorithm 
embedded multigrams computation likelihood section dependencies multigram sequences modeled multigram model recursive way 
multigram model applied ts resulting segmentation observed string symbols 
string multigram sequences assumed result concatenation higher level sequences formed sequences ds note resulting possible segmentation accordingly denotes set sequences formed elements ds assuming multigram structure model dependencies sequences produce noted sequence consist elements 
recursive process stops successively embedded multigrams eventually grouped single highest level sequence spanning string observation 
embedded multigram model observations structured tree see example fig 
leaves consist symbols observed non terminal nodes maximum number leaving branches equal depending depth node tree 
assume rest section second level sequences independent resulting model level embedded multigram model 
framework likelihood string computed sum likelihood values possible level associated second level segmentations flg flg level scheme likelihood segmentation computed example level embedded multigram model fig 

set parameters theta level embedded multigram model set probabilities sequence theta fp 
estimation model parameters ml estimates embedded multigram model derived method case independent multigrams 
noting number occurences sequence second level segmentation performed flg flg theta flg flg theta cd cd cd cd bc bc bc bc abc abc bcd phi phi phi bcd phi phi phi phi fig 

likelihood abcd level embedded multigram model possible segmentations left tree representation particular segmentation bcd right 
total number sequences equation shows estimate merely weighted average number occurences sequence possible second level segmentation reestimation formula implemented means forward backward algorithm 
non stochastic models overlapping sequences section alternative transduction procedure relying concatenation sequences having variable length 
main characteristic approach comparison stochastic model previous sections captures dependency successive sequences imposing overlapping constraints 
constraints suggest conjunction joint multigram model bi dimensional case 
show constraints allows define kind sequence modeling notion concatenation extended handle overlapping parts adjacent pairs 
overlapping constraints case joint multigram model pronunciation test string obtained selecting dictionary matched pairs sequence maximizing likelihood 
assuming dictionary available overlapping model adds constraints overlapping constraints candidate sequence 





pairs 





ds 

resp 


prefix respectively suffix existing lexical item 
assume instance wish transcribe word concatenating pairs aba bae supposedly concatenation second pair allowed pair bas pair aem warrants transition second concatenated pair corresponds attested pronunciation 
expected advantage constraints reduce set candidate pronunciations ruling solutions including transition adjacent pairs 
overlapping constraints appear way introduce dependencies crude way multigram model overlap pairs lexicon likelihood sequence adjacent simply zeroed 
doing somewhat unsatisfying 
turn back example feel intuitively transition aba bae better warranted observation bas aem ground larger overlap pair 
consequently appealing observed overlap size clue transition likelihood possible constraints boolean fashion 
additional assumption phonemic strings aligned inserting null symbols needed 
shall noted assumption means necessary 
overlapping models alternative view overlapping constraints allows take account 
fact overlapping model model allowing concatenation overlapping pairs sequences expression concatenation refer extended notion concatenation 
consider previous example transcription defined non overlapping pairs aba bae constraint pairs bas ash aem ds may stated concatenation overlapping pairs aba bae bas ash aem new formulation possible distinguish identical segmentations produced different overlaps correspond distinct concatenations 
stage rank various concatenations version model performed ad hoc scoring function expresses formally concatenation relies simultaneously long pairs large overlaps preferred solution relying shorter pairs smaller overlaps 
suppose word graphemes viewed concatenation ts ts sequences 
denoting number symbols sequence define scoring function ts theta form possible compromise size overlapping portions successive sequences longer overlap larger value numerator overlapping symbols counted twice number sequences involved pronunciation greater number sequences larger value denominator 
secondary criterion average frequency pairs involved solution break possible ties 
advantage modeling dependencies section briefly review results obtained grapheme phoneme transduction task joint multigram model overlapping model 
purpose complete evaluation models see instance provide reader couple results demonstrating effectiveness modeling dependencies variable sequence model 
models evaluated database french words 
approximately database training model remaining test 
transcription judged correct exactly matches pronunciation listed database phonemic level 
number correct phonemes transcription computed basis string string edit distance target pronunciation 
table reports best results obtained complete evaluation model including point obtained decision tree learning technique dec 
main outcome evaluation model words phonemes joint multigram joint multigram overlapping model decision tree dec table 
comparative evaluation joint multigram model overlapping model grapheme phoneme conversion task 
version joint multigram model decoding procedure additionally uses bigram model phonetic sequences see 
overlapping model significantly outperforms non overlapping model 
obvious fact readily account difference overlapping model uses database phonemic entries aligned kind pre processing joint multigram model 
believe superiority overlapping model results fact efficiently constraints transitions adjacent pairs sequences 
conversely basic joint multigram model suffers assumption independence successive pairs sequences 
far joint concerned evaluation encouraging provides completely unsupervised learning procedure task far mainly treated supervised see review 
confirms major weakness basic model experiments assumption independence successive joint sequences 
illustrated previous sections assumption means required multigram approach 
expect dependencies bigram models joint sequences instance improve results 
far overlapping model specifically concerned worth mentioning overlapping constraints may cause input remain 
fact may desirable feature lack better evidence virtually impossible transcribe accurately unusual forms typically words borrowed foreign language 
perspectives variable length sequence modeling schemes offers new framework description real world data observed fields natural language speech processing biological sequences modeling generally discrete event systems 
original independent multigram model generalized introducing various kinds statistical dependencies grams multigrams embedded multigrams 
alternative way account contextual effects structural constraints 
instance overlapping constraints appear efficient grapheme phoneme transcription 
contributes highlighting links approaches 
expression common formalism step integration statistically structurally descriptions symbolic streams 
particular models strongly relate goals grammatical inference describe underlying structure data unsupervised manner 

fr ed eric bimbot roberto pieraccini esther levin atal 
variable length sequence modeling multigrams 
ieee signal processing letters 

sabine deligne fr ed eric bimbot 
language modelling variable length sequences theoretical formulation evaluation multigrams 
proceedings international conference acoustics speech signal processing icassp 

sabine deligne fr ed eric bimbot fran cois 
phonetic transcription variable length sequences joint multigrams 
proceedings european conference speech communication technology eurospeech madrid sept 

dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 

guy de calm es isabelle jean marie 
le projet de base de donn ees du fran cais parl 
actes du toulouse 


kari 
efficient way learn english grapheme phoneme rules automatically 
proceedings international conference acoustics speech signal processing icassp volume minneapolis apr 

fran cois 
grapheme phoneme conversion multiple unbounded overlapping chunks 
proceeding ii ankara turkey 

fran cois 
par motivations formalisations evaluations 
phd thesis ecole nationale sup erieure des el paris 
