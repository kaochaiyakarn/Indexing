performance neocognitron various cell cell transfer functions david lovell ah chung tsoi intelligent machines laboratory department electrical engineering university queensland queensland australia april neural network solution problem handwritten character recognition proposed important know structure network function component neurons suited task 
research examined structure fukushima neocognitron effect classification distorted input patterns 
results assess classification performance neocognitron function component neurons altered 
tests describe demonstrate cells sigmoidal transfer function modified activation function significantly enhances classification performance neocognitron 
fukushima neocognitron received attention past decade partially shift invariant distortion tolerant handwritten character classifier 
performance novel structures neocognitron investigated certain extent 
aware studies assess effectiveness different implementations component cells neocognitron 
compare classification performance fukushima neocognitron identically structured networks employ modified cells 
empirically fukushima original implementation performs shown classification ability noise tolerance network significantly improved sigmoidal transfer function cell 
employ variety cell transfer functions necessary modify activation function cells 
modification propose allows supervised training cells take place single presentation training vectors 
performance modified versions neocognitron completed hildebrandt published revealing analysis function cell method tuning selectivity parameter cells 
discuss relation results hildebrandt show parameter tuning algorithm presents improve classification performance neocognitron 
initially experiments performed aimed find modified cell networks yield better results fukushima original neocognitron 
investigated modified versions neocognitron network sigmoidal cells weighted average cells exhibited best classification performance significantly better original network 
proved conjecture enhanced performance modification component cells parameters sigmoid weighted average network tuned maximize performance network 
resulting network exhibited correct classification rate compared original neocognitron 
place experiments proper context section gives general overview operation neocognitron 
sections examine propose modifications functions cells respectively 
section outline methods compare performance number versions neocognitron results experiment section 
remainder discusses results significance 
author acknowledges support australian postgraduate research award 
second author acknowledges support australian research council 
comments suggestions downs bartlett young gratefully acknowledged 
basic operation neocognitron neocognitron performs classification input succession functionally equivalent stages 
stage extracts appropriate features output preceding stage forms compressed representation extracted features 
compressed representation preserves spatial location extracted features input stage 
classification achieved steadily extracting compressing feature representations input reduced vector element corresponds similarity measure input different classes input neocognitron trained classify 
stage stage input stage cell receptive field cell plane basic structure neocognitron general structure cell planes 
rms output stepwise linear piecewise linear sinusoid exponential threshold sigmoid threshold threshold linear structure cell transfer functions experiment 
shows structure neocognitron 
feature extraction performed arrays cells called planes trained respond certain features characterize input patterns 
cell connected rectangular region cells known receptive field preceding stage 
receptive fields see cells array uniformly cover input cell plane 
plane connection strengths cell receptive field replicated 
ensures invariant response features input cell plane 
cell planes planes shown compress activity previous planes smaller representation 
doing cells provide degree translational invariance responses preceding cells 
ultimately compression activity reaches stage input pattern represented set single cells corresponding input class neocognitron trained recognize 
stage cell highest activity represents class input belongs 
cells feature extractors fundamental operation performed cell see weighted sum inputs 
connections cell receptive field inputs adaptive elements neocognitron learning rule employs local hebbian reinforcement 
neocognitron trained supervised unsupervised fashion 
supervised training completely reproducible deterministic comparative experiments detailed sections 
reader referred description unsupervised learning neocognitron 
supervised learning involves training plane stage respond certain input features 
different planes trained extract different features 
training cell exposed inputs required extract training completed weight vector cell equal sum inputs appeared receptive field 
cell receives inhibitory signal proportional root mean square rms activity receptive field 
amount inhibition signal causes adjusted learning rule alters equation maximize cell output feature similar features cell trained recognize 
behaviour cell mathematically formulated function delta cell activation output delta delta delta rms gamma vector activities receptive field input vector weights learned cell selectivity parameter 
value set experimenter closed form training algorithm value determines closely cell input correspond inputs trained order elicit response 
root mean square activity input cell defined rms vector describes gaussian kernel serves accentuate inputs centre cell receptive field implementing arithmetic mean inputs 
noted previously factor set learning rule maximize cell response training feature 
modifying cell transfer function concerned effect function delta classification accuracy neocognitron 
original neocognitron fukushima delta threshold linear function form ae ensuring output cell non zero excitatory component input exceeds inhibitory component 
preserves separation ground neocognitron 
ground context denotes background character 
investigate effectiveness transfer functions delta examined functions see threshold sigmoid commonly connectionist networks sinusoidal approximation sigmoid piecewise linear transfer function stepwise approximation exponentially tailed threshold function 
thresh ae sig sin gamma sin gamma gamma gamma gamma ac gamma ae gamma threshold function equation may directly applied activation equation transfer functions equations require modified activation function incorporated neocognitron 
transfer functions equations odd functions devised activation function sat jxj cos gamma threshold gamma threshold cos jj angle dimensional space input vector weight vector weight vector sum training patterns cell respond represents th training vector cell 
similar direction cos approach 
threshold sets value cos required activation positive satisfies threshold 
function sat delta causes cell respond maximally input features comparable size average size training vector jx avg define jx avg jx avg jx sat delta simple linear threshold function sat ae prevents cell responding small spurious inputs similar direction weight vector case zero length input vector cos undefined define product sat avg cos formulation modified activation function similar respects approach taken fukushima 
noted cell equation trained single presentation training vectors 
learning rate parameter associated cell ambiguity presentations training vectors required adequate learning 
cells compressing representation input cell receptive field subsampling activity preceding plane 
subsampling activity compressed representation plane output obtained 
cells blur activations preceding planes performing weighted sum inputs time fixed weights suppose input cell sigmoid activation transfer function 
activation equation equal zero sigmoid transfer function result output ground separation lost 
describe gaussian kernel 
denote subsampled input vector gaussian kernel activation cell fukushima original description neocognitron written weighted mean passed transfer function limits output cell mean modifying cell transfer function researchers remarked blurring plane activity cells important allowing neocognitron tolerant considerable degree input distortion 
algorithms blurring spreading activation include ffl convolution algorithms equation fukushima original cell description ffl morphological algorithms dilation incoming activations ffl order statistic filtering methods including median filtering order statistic filters straightforward incorporate structure neocognitron developed modified ranked order filter version cell 
allows compare effectiveness fukushima approach standard blurring method 
output modified cell max max xn subsampled input vector wn gaussian kernel 
refer equation weighted max operation 
experimental framework performance neocognitron undergone supervised learning primarily dependent feature set trained general structure network 
feature set network structure described experiment proper comparison performance statistics obtained 
performed sets tests 
test estimates classification performance networks set digits test ii takes networks highest classification performance test examines performance data corrupted salt pepper noise test iii looks performance networks trained hildebrandt closed form algorithm 
network parameters versions neocognitron threshold linear threshold transfer functions implemented parameters detailed 
networks experiment gaussian kernels weight inputs cells calculated method parameters described 
value fi equation set allow activation range linear nonlinear parts sigmoid 
tests ii implementations neocognitron cell transfer functions equations threshold parameters values stages respectively 
threshold parameters extensively tuned maximize network performance time tests performed quantitative method doing 
tests ii substantially complete learned feature dependent approach setting threshold parameter neocognitron style networks section iii 
performance neocognitron style networks trained closed form algorithm investigated test iii 
test set digits 
experimental results estimate success rate classifier accuracy confidence necessary perform trials 
performed trials network digits shown 
results shown tables partitioned categories percentage trials input correctly classified trials input misclassified trials input unclassified activity final stage units 
test cell transfer function cell transfer function classified misclassified unclassified sigmoid weighted average sigmoid weighted average threshold linear weighted average exponential threshold weighted max sigmoid weighted max sinusoid weighted average stepwise linear weighted average piecewise linear weighted average threshold linear weighted max threshold weighted average piecewise linear weighted max threshold weighted max sinusoid weighted max stepwise linear weighted max exponential threshold weighted average table classification performance statistics various implementations neocognitron 
high classification rate achieved sigmoid weighted average network counterbalanced increase misclassification 
transfer functions non zero equations leave input unclassified class chosen basis highest output activity 
impose requirement highest activity exceed threshold ffi positive identification input choosing ffi gives performance statistics indicated 
discussion discuss results obtained examine significance modifications cells 
fukushima original method training cell involved repeated presentations training vectors required learning rate parameter set appropriate value 
fukushima stated value large supervised training cell take place test set digits available email request author elec uq oz au salt pepper noise test ii best case sigmoid weighted average sigmoid weighted average exponential threshold weighted average threshold linear weighted average sigmoid weighted average classification performance statistics top models table input corrupted salt pepper noise 
best case version sigmoid weighted average model discussed section 
test iii cell transfer function cell transfer function classified misclassified unclassified sigmoid weighted average sigmoid weighted average threshold linear weighted average exponential threshold weighted max table classification performance statistics implementations neocognitron trained hildebrandt closed form algorithm 
presentation training vector 
immediately apparent constitutes large value fukushima suggests feel advantage formulation cell requires learning rate parameter set guaranteeing single presentations training vectors needed training proceed 
formulation cell activation function equation explicit relation cell parameters response 
feel approach intuitive appeal fukushima equation 
modified cell weighted max algorithm equation offers computationally efficient method blurring compressing feature representations neocognitron 
indications results test see table shows possible obtain statistically significant improvement classification performance reformulating behaviour cells neocognitron 
noted section improvement achieved sigmoidal transfer function accompanied corresponding increase misclassification input due fact input categorized 
argued amounts improvement 
aim experiment establish performance neocognitron enhanced alterations component neurons 
results test show coarsely tuned network employing sigmoidal cells achieve correct classification rate significantly higher original neocognitron show results improved adjusting threshold parameters cells point misclassification rate sigmoidal network similar fukushima neocognitron 
modification cells network effective 
generally poor performance networks presume parameters original neocognitron described set handwritten digits reasonably classified 
implies making fair comparison modified networks neocognitron close maximum classification ability 
weighted max blurring algorithm implies effective blurring algorithm respond average activity cell largest input 
role cells neocognitron subtle simply spreading activation preceding layers 
test ii see shows performance networks highest test classification rates input data corrupted salt pepper noise 
networks showed graceful degradation performance presence noise 
adding threshold ffi output sigmoidal cell neocognitron gives similar performance fukushima original network low noise environment 
hildebrandt closed form training algorithm cell cell cell cell cell cell direction cosine cell response dimensions dc projection output unit vector dc output jxj cell response curves response curves optimal closed form training 
third test performed evaluate closed form training algorithm neocognitron proposed hildebrandt section iii 
theoretical basis algorithm allows selectivity parameter cell chosen optimal manner 
point optimality method imply high classification performance 
shall briefly outline hildebrandt method notation section describe difficulty practical application 
correlation function implemented cell written jj see equation input weight vector cell 
define behaviour cell zero length input vector fukushima employs modified correlation function jj done assumption jj jae 
hildebrandt shows output cell equation defined terms equation output gamma note equation rearranged form similar equation substitute cos choose threshold 
plot locus direction cosine cell response dimensional input get curve 
direction cosine shows response cell varies depending direction input vector equation defines acceptance region input space input vector lie order elicit response cell 
acceptance region conical apex origin axis weight vector boundary acceptance region angle arccos region axis shown dotted lines 
stage neocognitron contains number planes described direction weight vector value selectivity parameter dimensional example direction cosines responses planes seen 
put simply hildebrandt optimal closed form training algorithm adjusts thresholds cells layer acceptance regions cells large possible overlapping 
arrangement optimal sense maximizes generalization cells coverage input space compromising discriminatory ability cells 
shows response cells selectivities adjusted manner 
test practical merit algorithm applied hildebrandt methods training networks highest test classification performances 
results obtained test iii shown table 
order understand poor performance networks test iii examined response patterns trained extract 
networks frequently responding certain original training vectors 
reason training vector rejection stems fact weight vector cell vector sum training vectors cell 
imply direction training vectors 
possible selectivity cell adjusted closed form algorithm training vectors lie outside acceptance region cell 
closed form training algorithm produces network sense optimal follow network high classification performance 
suggestion hildebrandt 
felt important critically assess practical value approach 
adjustment selectivity parameters neocognitron remains open problem 
best performance statistics obtained aim experiments described observe modifications cells neocognitron affected classification performance network 
intention see performance fukushima original implementation improved 
demonstrated sigmoid weighted average version neocognitron correctly classify data significantly better original network decided run test see result improved different settings threshold parameters network 
threshold parameter layer adjusted response cell layer training vectors close unity 
resulting threshold parameters layers respectively 
digits tests obtained rates correct classification misclassification 
misclassification rate reasonably close fukushima neocognitron may conclude sigmoidal cell better implementation originally proposed 
shows best case version neocognitron gives superior performance noisy conditions 
superior performance sigmoidal cell network agreement performance mammalian visual system uses cells transfer functions empirically modelled sigmoid curve 
describing precursor neocognitron fukushima mentions relationship transfer function terms cell input output function biological sensory neuron 
explicit sigmoidal transfer function strong biological motivation appropriate network structure implementation heavily influenced models mammalian visual pathway 
details experiments compare classification performance fukushima neocognitron number modified versions neocognitron 
order formulate new activation function neocognitron style networks equation 
activation function explicit relation output cell cell parameters requires single presentation training patterns supervised learning 
experiments shown neocognitron uses sigmoidal cells significantly better classification performance fukushima original implementation neocognitron 
function cells neocognitron adapted ranked order filter enhance test performance neocognitron 
tests showed optimal closed form training algorithm neocognitron proposed hildebrandt improve practical performance network 
main high performance neocognitron improved modifications component cells lines described 
fukushima 

neocognitron self organizing neural network model mechanism pattern recognition unaffected shift position 
biological cybernetics 
fukushima 

neocognitron hierarchical neural network capable visual pattern recognition 
neural networks 
fukushima 

analysis process visual pattern recognition neocognitron 
neural networks 
fukushima wake 

handwritten alphanumeric character recognition neocognitron 
ieee transactions neural networks 
barnard 

shift invariance neocognitron 
neural networks 
menon heinemann 

classification patterns self organizing neural network 
neural computation 
yamaguchi 

distortion tolerance curve neocognitron various structures including pyramid 
proceedings th international conference pattern recognition atlantic city new jersey 
hildebrandt 

optimal training thresholded linear correlation classifiers 
ieee transactions neural networks 
hubel wiesel 

receptive fields binocular interaction functional architecture cat visual cortex 
journal de 
hogg 

engineering statistics new york macmillan 
pitas 

nonlinear digital filters principles applications 
massachusetts kluwer academic publishers 
lovell tsoi downs 

note closed form training algorithm neocognitron 
submitted ieee transactions neural networks 
fukushima 

self organizing neural network 
biological cybernetics 
