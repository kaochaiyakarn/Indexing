learning bayesian networks np hard david chickering dmax cs ucla edu dan geiger dang cs technion ac il david heckerman microsoft com november technical report msr tr microsoft research advanced technology division microsoft microsoft way redmond wa msr tr november algorithms learning bayesian networks data components scoring metric search procedure 
scoring metric computes score reflecting goodness fit structure data 
search procedure tries identify network structures high scores 
heckerman 
introduced bayesian metric called bde metric computes relative posterior probability network structure data 
show metric property inferring causal structure data 
show problem deciding bayesian network node parents relative posterior probability greater constant np complete bde metric 
researchers begun investigate methods learning bayesian networks including bayesian methods cooper herskovits buntine york spiegelhalter madigan heckerman methods lam bacchus suzuki methods pearl verma spirtes 
approaches basic components scoring metric search procedure 
scoring metric takes data network structure returns score reflecting goodness fit data structure 
search procedure generates networks evaluation scoring metric 
approaches components identify network structure set structures predict events infer causal relationships 
bayesian approach understood follows 
suppose domain variables fx set cases fc cm case instance variables refer database 
suppose wish determine joint distribution cjd probability distribution new case database current state information 
reason distribution directly imagine data random sample bayesian network structure unknown parameters 
denote hypothesis data generated network structure assuming hypotheses corresponding possible network structures form mutually exclusive collectively exhaustive set cjd cjb delta jd msr tr november practice impossible sum possible network structures 
consequently attempt identify small subset network structure hypotheses account large fraction posterior probability hypotheses 
rewriting equation obtain cjd cjb delta jd normalization constant jd 
equation see relative posterior probability hypotheses matter 
compute posterior probability typically computes dj djb bayes factor jd jd structure empty graph 
approach approximation cjd method learning network structure relative posterior probability plays role scoring metric 
example jh learn single network structure map maximum posteriori structure jh learn collection network structures weighted relative posterior probability corresponding hypotheses 
cooper herskovits referred ch derive bayesian metric call bd metric set reasonable assumptions learning bayesian networks containing discrete variables 
heckerman 
referred hgc introduce additional assumptions learning network structures called prior equivalence likelihood equivalence 
prior equivalence says equivalent bayesian networks receive prior probabilities jxi 
likelihood equivalence says equivalent networks receive likelihoods djb 
argue assumptions applied learning acausal network structures likelihood equivalence applied cases learning causal structures 
assumption likelihood equivalence derive bde metric 
heckerman 
show problem finding network structures highest score structure node parent polynomial decomposable metric 
examine general case search described decision problem learn instance set variables database fc cm instance variables scoring metric real value question exist network structure defined variables msr tr november node parents 
shows similar problem pac learning np complete 
results translated easily show learn np complete bd metric 
show learn np complete likelihood equivalent bde metric constraint prior equivalence 
review bayesian scoring metrics section review bayesian scoring metric learning bayesian networks containing discrete variables described hgc 
metric special case metric described ch 
ch derive bayesian scoring metric assumptions 
database multinomial sample possibly uncertain structure possibly uncertain parameters theta denote hypothesis database multinomial sample structure belief network structure pi denote parents denote number states variable pi denote number instances pi integer index instances 
write pi denote observation jth instance parents ijk denote parameter associated kth state variable jth state pi 
think ijk long run fraction cases cases pi 
theta ij denote union ijk parameters theta union theta ij instances variables 
belief network structures ae theta jb ae theta ij jb 
ae deltaj delta denote conditional probability density 
assumption corresponds local global independence assumptions spiegelhalter lauritzen 
refer assumption simply parameter independence 

parents belief network structures ae theta ij jb ae theta ij jb 
call assumption parameter modularity 
assumption implicitly ch explicit hgc 

belief network structure theta ij theta ae theta ij jb dirichlet distribution ae theta ij jb ijk gamma ijk subtleties definition explored hgc 
msr tr november assumptions prior densities theta structure determined dirichlet exponents ijk derive metric ch assumption 
databases complete 
variable case database observed 
assumption parameters theta network structure remain independent modular sense assumptions database observed 
consequently ch obtain result delta gamma ij gamma ij ij delta gamma ijk ijk gamma ijk ijk number cases pi ij ijk ij ijk gamma delta gamma function 
call expression expression proportional bd bayesian dirichlet metric 
discuss specification section 
note ch assumptions straightforward compute cjb 
particular network structure update parameters database standard dirichlet updating rule 
hgc derive special case bd metric 
consider isomorphic network structures 
network structures said isomorphic represent assertions conditional independence 
example variable domain fx network structures represent independence assertion independent isomorphic 
hgc argue assumption apply learning acausal network structures 
hypothesis equivalence isomorphic hgc examine complete belief network structures containing missing arcs 
assumption hypothesis equivalence hypotheses corresponding structures denoted sc introduce assumption 
sc words assumption says data may random sample complete network structure 
msr tr november hgc demonstrate assumptions consistent assumptions imply special case bd metric dirichlet exponents constrained relation ijk delta pi jjb sc user equivalent sample size domain 
particularly remarkable fact assumptions explicit mention form parameter densities imply assumption densities dirichlet distribution 
hgc note probabilities equation may computed prior bayesian network provided expert bayesian network jb sc 
prior network encodes user beliefs happen case sc hgc show bd metric constrained equation property likelihood equivalence says likelihoods djb djb equal isomorphic 
call special case bde metric equivalence 
addition assumption hypothesis equivalence follows immediately isomorphic 
call property prior equivalence 
hgc go discuss learning causal network structures 
case argue hypothesis equivalence appropriate cases determining likelihoods djb rarely appropriate determining prior probabilities network structure 
argue learning causal network structures constraint equation applied bd metric user allowed specify different priors isomorphic network structures 
learn np complete section show learn np complete bde metric constraint prior equivalence 
proving result careful specifying inputs learn bde metric 
inputs relative prior probabilities network structures node parents database parameters ijk node parent pairs values input need include parameters ijk score computed network structures node parents 
consequently need parameters nodes having parents nodes parent configurations zero prior probabilities values corresponding data msr tr november database 
emphasize parameters ijk derivable joint probability distribution equation 
inputs see equation bde metric network structure database computed polynomial time 
consequently learn np 
sections show learn np hard 
section give polynomial time reduction known np complete problem learn 
section show learn np hard reduction section show learn np hard reducing learn learn 
discussion omit conditioning background information simplify notation 
reduction learn show learn np hard bde metric reduction restricted version feedback arc set problem 
general feedback arc set problem stated garey johnson follows feedback arc set instance directed graph positive integer jaj 
question subset ae ja contains arc directed cycle 
shown feedback arc set remains np complete directed graphs vertex total degree degree 
refer restricted version degree bounded feedback arc set short 
instance consisting task specify polynomial time components instance learn 
variable set 
database 
relative prior probabilities network structures 
necessary parameters ijk see comment section 
value msr tr november note need specify relative prior probabilities discussed section metric arbitrary proportionality constant 
show instance learn solution instance solution 
instance assume vertex degree degree zero 
node exists incident edges participate cycle remove graph 
help distinguish instance instance learn adopt convention 
term arc refer directed edge instance term edge refer directed edge instance learn 
construct variable set follows 
node include corresponding binary variable denote subset corresponds arc include additional binary variables denote subset containing variables define jaj include variables database consists single case 
relative prior probability network structure 
assignment satisfies constraint prior equivalence 
equation database relative prior probabilities equal bde metric denoted ijk ij state equal instance pi state variable pi equal 
reduction point polynomial 
specify necessary ijk parameters specify prior network compute parameters equation 
equation pi sc demonstrate reduction polynomial show prior network constructed polynomial time equation computed prior network polynomial time pi contains variables 
establish time complexity prior network construction paragraphs 
general probabilistic inference bayesian network np hard section theorem show probability equation inferred prior network constant time due special structure network 
denote prior bayesian network 
prior network contains hidden nodes appear visible nodes appear msr tr november subgraph prior net corresponding kth arc variable corresponding visible node denoted visible nodes arc instance contains hidden binary nodes directed edges shown 
instance know node adjacent nodes 
node adjacent exactly nodes hidden node edge edges hidden nodes visible node sink exactly hidden node parents hidden node source children 
ij denote hidden node parent common visible nodes create parameters follows 
hidden node ij set ij ij visible node types 
type node defined conditional probability distribution 
node corresponding fifth variable created ith arc instance type ii node nodes type nodes 
type node conditional probability distribution shown table 
say variables prior siblings corresponding nodes prior network share common hidden parent 
denote set variables prior siblings type ii node define distinguished siblings set fa ae table shows conditional probability distribution type ii node dis msr tr november table conditional probability distribution type node 
ij ik il jh ij ik il siblings fx table conditional probability distribution type ii node fx ij ik il jh ij ik il jv jaj visible nodes visible node hidden node parents probability table constant size 
construction takes time polynomial size instance 
derive value equation obtain bs pi type ii node shaded small markers indicate distinguished siblings 
msr tr november ffi gammaj pi delta pi ffi gammaj pi sx ffi gamma gamma pi delta pi ffi gammaj pi ffi gamma gamma pi delta pi ffi positive constant shall fix remainder 
oe total number prior sibling pairs defined fl number prior sibling pairs adjacent sum pi number edges connect prior sibling pairs equal oe gamma fl 
rewriting equation get ffi gamma oe gammafl pi ffi gammaoe delta ffi fl pi delta ffi fl pi state lemmas postponing proofs section 
network structure prior sibling graph pairs adjacent nodes prior siblings 
pairs prior siblings prior sibling graph need adjacent 
lemma network structure prior sibling graph created removing edge connect pair prior siblings 
follows throughput remainder symbol ff stands constant 
lemma prior sibling graph type node pi contains element pi maximized equal 
pi pi ff delta lemma prior sibling graph type ii node pi set distinguished siblings pi maximized equal 
pi pi ff delta define instance learn jv jaj ff msr tr november defined lemma respectively constant equation 
value derived polynomial time 
consequently entire reduction polynomial 
proof np hardness section prove learn np hard reduction previous section 
prove learn np hard reduction learn 
discussion lemma explain selection equation turn facilitates proof learn np hard 
fl number prior sibling pairs fx adjacent fx argue fl fl fl total number prior sibling pairs adjacent note defined prior siblings node satisfy ae prior siblings kj node kj satisfy kj ae see 
prior sibling pair fx exists exactly fx consequently fl fl 
express equation bs pi ffi fl pi pi fl fl ffi fl pi 
lemma prior sibling graph 
node adjacent prior siblings orientation connecting edges shown fl maximized equal delta fl ff delta delta proof type node prior sibling parent single type ii node distinguished siblings parents 
lemmas score pi node maximized 
furthermore pair prior siblings adjacent 
fl ffi fl pi msr tr november optimal configuration edges incident nodes corresponding arc ffi delta delta delta delta delta suppose exists orientation edges incident nodes fl ff delta delta ffi ff pair prior siblings adjacent hypothetical configuration 
furthermore node achieve maximum score total score bounded ff delta delta lemma order achieve maximum score distinguished siblings parents 
node parents parent 
parent root nodes lemma attain maximum score 
repeating argument parent parent 
resulting configuration identical lemma follows 
theorems prove learn np hard 
theorem exists solution learn instance constructed section exists solution problem proof solution create solution learn follows arc insert edges corresponding nodes described lemma 
arc insert edges corresponding nodes described lemma edge reversed oriented complete proof show solution learn instance show greater equal msr tr november node parents know solution long acyclic 
suppose contains cycle 
cycle contained set exist sequence nodes contained cycle pair consecutive nodes sequence directed path intermediate node created instance learn directed path type exists implies cycle edges contained contradicting fact solution 
derive 
opt subset sets correspond arcs rewriting equation get pi delta opt fl delta ana opt fl node prior sibling node parent node instance degree 
furthermore lemma guarantees opt fl equals delta jvj theta delta ja opt ak ana opt fl jv theta delta ak ana opt fl consider opt prior sibling pairs node set adjacent fl zero 
furthermore node set attains maximum score type node lemma attains score ff delta conclude fl equal delta delta ff jv delta delta delta ff opt jv delta delta ja ff ja jv delta jaj ff ja msr tr november ff ja conclude theorem exists solution problem exists solution learn instance constructed section proof solution instance learn remove edges connect prior siblings 
lemma guarantees bde score decrease due transformation 
create solution follows 
recall set nodes corresponds arc instance 
define solution arc set set arcs corresponding sets edges incident nodes configured described lemma 
complete proof show solution show ja suppose solution 
means exists cycle pass arc arc cycle corresponding directed path see 
implies cycle contradicts fact solution learn 
lemma know set corresponds arc fl bounded ff delta delta conclude equation arcs 
theorem learn satisfying prior equivalence np hard integer 
proof learn np hard establish theorem showing learn problem solved instance learn 
instance learn equivalent instance learn identical instance learn relative prior probability zero structure contains node parents 
note new parameters ijk need specified 
remains shown assignment satisfies prior equivalence 
establish fact showing structure containing node parents isomorphic structure node contains parents 
hgc shown isomorphic structures exists finite sequence arc reversals reversal remains isomorphic reversals edge edge reversed parents exception parent follows reversal number parents msr tr november portions theorem applies 
elements white nodes hidden black nodes members pi separated node outside dashed line 
reversal number parents reversal 
exists node parents structure exists node parents structure isomorphic proof lemmas prove lemmas derive pi pair fx pi node 
set pi satisfy mutually exclusive collectively exhaustive assertions assertion node parent prior sibling pi prior sibling parent assertion exists node parent prior sibling prior siblings parent theorem shows derive pi pair fx pi pi satisfies assertion need compute cases pi shows examples relevant portion prior network theorem applies 
theorem node pi satisfies assertion pi pi 
proof equation pi pi sc ffi gammaj pi sx msr tr november theorem follows show separated parents prior siblings parents prior siblings known 
suppose exists parent prior sibling separated pi implies active trail condition nodes 
path active nonactive parent pass element node sink active path pass node pi pi satisfies assertion know siblings pi active path pass theorems equalities 
ij ik il ij ik il ij ik il jx ij jx ik il ij ik il jx ij jx ik jx il ij jx equation follows hidden node root equation follows path ik il pass node sink 
equation follows similar argument noting topology fx equation follows tables fact ij equals 
theorem type node pi satisfies assertion 
pi pi ff delta 
pi pi 
pi pi proof part 
table drop conditioning event simplify notation 
msr tr november ij ik il jh ij ik il ij ik il ij ik il jh ij ik il ij ik il delta ffi delta ff delta consequently equation obtain pi ffi delta ff delta part 
suppose defined table parent jx ij ik il jx ij il il deltap ij hik il jx ij ik il jh ij hik il deltap ij jx hik il delta ffi delta pi ffi delta jx symmetry table obtain score prior sibling parent part 
suppose defined table parents obtain jx xk ij ik il jx ij hik il deltap ij hik il jx xk ij ik il jh ij hik il deltap ij jx hik jxk il delta ffi delta msr tr november consequently pi ffi delta jx symmetry table obtain score parents prior siblings theorem type ii node pi satisfies assertion 
pi pi ff delta 
pi pi ff delta 
pi pi pi ff delta 
pi pi proof part 
table obtain ij ik il jh ij ik il ij ik il ij ik il jh ij ik il ij ik il delta ffi delta ff delta equation get pi ffi delta pi ff delta part 
suppose defined table parent jx ij ik il jx ij hik il deltap ij hik il jx ij ik il jh ij hik il deltap ij jx hik il delta ffi delta ff delta msr tr november pi ffi delta pi ff delta obtain score prior sibling parent part 
suppose defined table parents obtain jxk ij ik il jxk ij hik il deltap ij hik il jxk ij ik il jh ij hik il deltap ij hik jxk il jx delta ffi delta ff delta consequently pi ffi delta pi ff delta obtain score parents prior siblings part 
table jx xk ij ik il jx xk ij hik il deltap ij hik il jx xk ij ik il jh ij hik il deltap ij jx hik jxk il delta ffi delta pi ffi delta pi show assertion holds parents node remove edge parent sibling decreasing score 
theorem established lemmas follow 
shows prior network assertion holds node theorem node 
pi fx jx jx 
msr tr november portion theorem applies 
white nodes hidden black nodes members pi node parents case separated non sibling parent 
proof node jx jx jx jx prior siblings follows jx 
jx jx jx delta jx jx expressing equality terms pi noting prior sibling parent canceling terms ffi obtain jfx jfx jfx type node type ii node distinguished siblings jfx equals jfx 
equation jfx jfx msr tr november implies improve local score removing edge type ii node fx jfx equals ff delta jfx 
equation jfx ff jfx remove edge affecting score preceding arguments demonstrate theorem 
theorem pair fx pi pi value pi computed constant time state variable pi equal 
buntine buntine 

theory refinement bayesian networks 
proceedings seventh conference uncertainty artificial intelligence los angeles ca pages 
morgan kaufmann 
cooper herskovits cooper herskovits 
january 
bayesian method induction probabilistic networks data 
technical report smi section medical informatics stanford university 
garey johnson garey johnson 

computers intractability guide theory np completeness 
freeman new york 


np complete problems graphs 
proc 
th conf 
information sciences systems johns hopkins university pages 
baltimore md heckerman heckerman geiger chickering 

learning bayesian networks combination knowledge statistical data 
proceedings tenth conference uncertainty artificial intelligence seattle wa pages 
morgan kaufmann 

revised 
learning robust learning product distributions 
technical report fachbereich informatik universitat dortmund 
lam bacchus lam bacchus 

causal information local measures learn bayesian networks 
proceedings ninth conference uncertainty artificial intelligence washington dc pages 
morgan kaufmann 
msr tr november madigan madigan 

model selection accounting model uncertainty graphical models occam window 
journal american statistical association appear 
pearl verma pearl verma 

theory inferred causation 
allen fikes sandewall editors knowledge representation reasoning proceedings second international conference pages 
morgan kaufmann new york 
spiegelhalter spiegelhalter dawid lauritzen cowell 

bayesian analysis expert systems 
statistical science 
spiegelhalter lauritzen spiegelhalter lauritzen 

sequential updating conditional probabilities directed graphical structures 
networks 
spirtes spirtes glymour scheines 

causation prediction search 
springer verlag new york 
suzuki suzuki 

construction bayesian networks databases mdl scheme 
proceedings ninth conference uncertainty artificial intelligence washington dc pages 
morgan kaufmann 
york york 

bayesian methods analysis misclassified incomplete multivariate discrete data 
phd thesis department statistics university washington seattle 
