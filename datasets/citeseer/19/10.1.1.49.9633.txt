committees decision trees david heath simon kasif steven salzberg department computer science johns hopkins university baltimore md lastname cs jhu edu intelligent systems designed sift mass evidence arrive decision 
certain pieces evidence may weight may affect final decision significantly 
intelligent agent available decision form committee experts 
combining different opinions experts committee approach outperform individual expert 
show exploit randomized learning algorithms order develop committees experts 
majority vote experts decisions able improve performance original learning algorithm 
precisely developed randomized decision tree induction algorithm generates different decision trees time run 
tree represents different expert decision maker 
combine trees majority voting scheme order overcome small errors appear individual trees 
tested idea real data sets accuracy consistently improved compared decision single expert 
developed analytical results explain effect occurs 
experiments show majority voting technique outperforms alternative strategies exploiting randomization 
decision trees successfully different decision making classification tasks 
number standard techniques developed machine learning community notably quinlan algorithm breiman cart algorithm 
algorithms numerous variations improvements put forward including new pruning strategies quinlan incremental versions algorithms utgoff 
refinements designed produce better decision trees trees accurate classifiers smaller trees 
main goal research produce classifiers provide accurate model possible set data 
achieve goal combined standard method classification decision trees ideas 
idea randomization context allows generate different trees task 
second idea majority voting learning methods nearest neighbor algorithms perform classification diagnosis 
majority vote decision trees classify examples 
randomization learning algorithms previous heath introduced algorithm described 
explored generation decision trees comprised tests linear inequalities attributes 
call oblique decision trees tests node simply hyperplanes oblique angle axes attribute space 
generalization standard decision tree techniques node tree test single attribute hyperplane parallel axes attribute space 
showed generating oblique trees finding single test minimizes goodness criteria np hard problem 
turned optimization technique simulated annealing find tests generate small accurate trees 
simulated annealing learning algorithm introduces element randomness 
time run generates different trees 
led explore methods randomization advantage generating trees additional criteria choose best tree 
argument picking tree solutions produced randomized algorithm may preferable algorithm clever produces solution 
explore way randomization advantage 
single training set generate set classifiers 
choosing representative tree attempt combine knowledge represented tree new accurate classifier 
regard classifier separate expert domain hand collection classifiers committee 
committee members entirely independent generated algorithm training data identical 
combination classifiers able outperform individual 
specifically take set classifiers combine classifications plurality 
binary classification problems reduces majority 
example trees classify example classify predict example belongs class technique applied decision trees call resulting algorithm dt spirit nn nearest neighbor algorithm 
advantage majority voting premise idea tree may capture target concept completely accurately approximate error 
error differs tree tree 
trees majority hope overcome type error 
consider example test example probability correctly classified random category 
take majority vote trees probability correctly classified maj jk gamma gammaj equation represents number trees correctly classify example require half trees restrictions sum 
represents probability trees getting example correct gamma gammaj probability remaining trees get wrong 
simply counts number possible ways trees divide sets trees size shows maj individual probability tree trees trees trees trees majority classification probability vs individual classification probability varies different numbers trees majority 
note example majority vote increases probability getting correct classification decreases 
set examples test set 
advantage classifiers directly 
hand majority increase probability classify correctly 
test set points cases 
obviously tell particular example belongs know classification 
experience benefit get increasing likelihood correct classification examples outweighs loss accuracy get examples intuitively simply increasing number classifiers committee majority voting scheme continually increase expected accuracy decisions 
example illustrates intuition wrong fact ideal size committee vary depending problem 
critical factor examples domain hand difficult classify examples small committees preferable 
implication choosing appropriate value may difficult problem 
number trees accuracy accuracy combined effects majority voting mixed data sets seen examples probability correctly classified average tree majority vote lower chances correct classification trees lower resulting accuracy 
hand increasing number trees involved vote increase accuracy points classified correctly average tree 
normally expect domains mixture types examples difficult classify easy 
try majority voting scheme mixture types get mixed result 
consider examples generate trees average classified correctly time classified correctly time 
shown majority voting scheme rarely classified correctly classified correctly 
shows combined expected accuracy set fe generate series trees classify examples expect average accuracy 
majority voting expect accuracy increase trees 
trees expected accuracy goes eventually converging 
simple example optimal committee size 
set examples probability example correctly classified average tree easy show average accuracy voting jxj accuracy infinite number trees majority computation jfx gj jxj fraction examples classified correctly average tree 
extremes accuracy may dips peaks 
experiment majority voting different numbers trees 
experiments empirically choose value practice 
related dt different strategies combining multiple classifiers 
common approaches problem 
approach thought multi level learning set classifiers trained 
outputs fed learning system learns appropriate weighting scheme apply outputs hopes creating accurate classifier 
depending implementation levels trained separately simultaneously 
wolpert stacked generalization technique hybrid technique developed zhang 
examples separately trained systems 
example simultaneously trained system jacobs 
second learning level learns assign training examples different components level 
dt takes approach 
level trained second level simple easily understood fixed strategy 
majority voting study fixed strategies 
system takes approach cluster back propagation network lincoln 

sadt algorithm majority voting technique applied randomized classifier scheme conceived natural enhancement 
accordingly experiments conducted 
aid understanding dt explain workings 
basic outline decision tree algorithms 
find hyperplane partition training set run partitioning algorithm subsets result 
describe hyperplane 
implementation dimensional hyperplanes stored form fh hyperplane point represents constant term 
example plane hyperplane line represented familiar ax form 
classification done recursively 
classify example compare current hyperplane initially root node 
example non leaf node labeled follow left child descend right child 
step algorithm generate initial hyperplane 
initial hyperplane generate tailored training set 
simply wanted choose hyperplane parallel axes hyperplane passing points dimension particular initial hyperplane may written form gamma points 
choose hyperplane passes 
choices initial hyperplane equally 
annealing begins hyperplane immediately moved new position location initial split important 
hyperplane repeatedly perturbed 
denote current hyperplane fh algorithm picks randomly adds uniformly chosen random variable range 
goodness measure described compute energy new hyperplane change energy deltae 
deltae negative energy decreased new hyperplane current split 
energy increased stayed new hyperplane current split probability gamma deltae temperature system 
system starts high temperature reduced slightly move 
note change energy small relative temperature probability accepting new hyperplane close temperature small probability moving worse state approaches 
order decide perturbing split keep track split generated lowest energy seen far current node 
minimum energy change large number iterations numbers iterations experiments making perturbations split generated lowest energy 
recursive splitting continues node pure leaf node contains points category 
goodness criteria goodness criterion experimented 
detailed discussions measures see heath murthy 

experiment criteria information gain quinlan max minority mm sum minority sm measures 
define mm sm follows 
consider set examples belonging classes hyperplane divides set subsets subset find class appears 
say minority categories 
examples minority category relatively pure 
prefer splits pure splits generate small 
number examples class class number examples class class 
force generate relatively pure split define sm error measure min min mm error measure max min min 
average error rate reduction best accuracy goodness error error number criterion rate trees error rate trees mm sm ig table iris results experiments classifying irises experiment ran dt fisher iris data known dataset subject numerous machine learning studies see holte summary 
data consists examples different types irises setosa virginica 
example described numeric measurements width length petals 
performed fold cross validation trials sadt 
fold cross validation trial divides data approximately equal subsets performs experiments 
subset train learning system union remaining gamma sets test set results averaged runs 
results iris data shown table 
shown table accuracy obtained test set pair take majority vote trees classifying test set 
note accuracy majority voting scheme consistently higher single 
weiss obtained accuracies data backpropagation nearest neighbor cart respectively 
results generated leave trials fold cross validation 
classification accuracy number trees max minority sum minority information gain iris classification accuracy versus number trees choosing value choose dt trees 
intuitively may trees voting process higher combined accuracy 
example difficult classify voting example classified correctly committee trees 
plot average classification accuracy iris data set number trees voting process varied 
note big jump accuracy trees 
max minority information gain measures peak fairly early drop sum minority measure increasing accuracy trees 
compromised eleven trees appears practice 
table shows average classification accuracy eleven trees voting 
shown classification accuracy optimal choice 
optimal choice table limited number cross validation trials run trees 
choice trees worked iris dataset 
accuracy obtained number trees number trees tried goodness measures quite third 
percentage examples probability correct classification percentage iris examples achieving accuracy point worth considering results expected 
example iris data set computed percentage times correctly classified tests 
shows percentage fraction examples 
note average goodness criteria 
gives rough estimate probability average tree classifying example correctly 
note vast majority examples nearly classified correctly 
approximately examples predicted correctly half time 
examples expect classified incorrectly take majority vote large number trees 
note percentage close error rate obtained dt 
suggestion majority voting scheme obtaining close maximal accuracy possible data 
applying dt cancer diagnosis second experiment chose dataset subject experiments classified data oblique hyperplanes bennett mangasarian 
dataset contains examples patients breast cancer diagnostic task determine cancer benign malignant 
input data comprised numeric attributes average error rate reduction best accuracy goodness error error number criterion rate trees error rate trees mm sm ig table breast cancer results decision trees oblique hyperplanes 
mangasarian method uses linear programming find pairs hyperplanes partition data 
algorithm finds pair parallel hyperplanes time pair oriented angle respect pairs 
resulting model set oblique hyperplanes similar spirit different structure oblique decision tree 
mangasarian received data collected clinical setting experimental design simple 
trained algorithm initial set examples 
patients remainder confirmed 
patients enter clinic algorithm diagnosis correctly diagnosed patients 
rough estimate accuracy mangasarian method 
re trained algorithm new patients reported correctly classified patients enter clinic 
mangasarian reported program output actual clinical setting 
dataset uniform experimental design salzberg reported program nearest hyperrectangle classifier obtained classification accuracy nearest neighbor accuracy 
results tests data shown table 
average values average fold cross validation trials 
accuracy obtained tree committee classifiers consistently higher average tree 
example sm goodness criterion quite bit better average benefitted majority technique 
possible average error rate reduction best accuracy goodness error error number criterion rate trees error rate trees mm sm ig table star galaxy results majority able overcome weaknesses criteria significant sm 
see eleven trees choice dataset 
max minority energy measure noticeable difference accuracy optimal choice number trees choice eleven 
identifying stars galaxies order study performance dt larger datasets ran experiments astronomical image data collected university minnesota plate scanner 
dataset contains astronomical objects classified stars galaxies 

dataset train perceptrons backpropagation networks differentiate stars galaxies 
access exact training test set partitions cross validation technique estimate classification accuracy 
study single training test set partition 
results may exactly comparable theirs include show learning methods produce similar accuracies 
results generated averaging fold cross validation trials 
astronomy dataset consists examples 
example fourteen real valued attributes label star galaxy 
approximately examples galaxies 
classification results shown table 

obtained accuracies backpropagation percep 
appears results generated single trial single partition test training sets 
fact obtained fold cross validated accuracy perceptron 
individual runs obtained higher accuracy average results better estimate true accuracy 
majority classifier increased classification accuracy data set studies 
max minority goodness criterion able reduce error rate 
eleven trees majority classification choice dataset 
results eleven trees number trees fifteen number cross validation trials ran 
comparison methods heath explored techniques advantage randomization learning algorithms 
focus techniques generate trees additional criteria select best tree measure testing set 
section compare techniques majority classification technique 
criteria choosing best tree choose smallest trees 
intuition technique smaller trees may concise descriptions problem domain sensitive noise training data lower chance generated overtraining 
addition smaller trees easier understand domain experts adopted 
pairs training testing sets fold cross validation generated chose smallest 
averaged accuracy size chosen trees 
training testing sets smallest tree averaged averaging 
experiment heath split training set trained training set 
second test set 
test tree assign merit 
ran times choosing different splits time choosing trees highest figures merit 
tested trees real test set 
table compare dt approaches 
tech error rate goodness smallest nd dataset criterion average dt trees test set iris mm sm ig cancer mm sm ig star mm galaxy sm ig table majority classifier methods niques gave improvement accuracy method choosing trees size consistent 
cases small trees worse average trees 
dts performed better separate test set judge trees 
nearly performed better picking smallest trees 
exception goodness criteria iris data 
explored idea advantage randomization inherent machine learning techniques generating committee classifiers combining majority voting scheme 
observed committees situations produce consistently better classifiers single system operating 
optimal size committee depends domain determined empirically 
experimented technique sadt randomized oblique decision tree algorithm 
results show committees containing relatively small number trees consistently perform better average turn perform better standard axis parallel trees heath murthy 
consistency degree improvement better techniques considered increasing accuracy randomization 
early stages tried apply majority technique types randomized learning algorithms 
clear opportunity experiments 
explore combining technique ideas improving classifiers 
example try majority technique trees smaller average see get improvements accuracy 
plan explore constructing committees diverse group classifiers including decision trees memory classifiers statistical classifiers methods may appropriate 
authors wish david aha providing comments relevant 
research supported part air force office scientific research afosr national science foundation iri iri 
bennett kristin mangasarian 
robust linear programming discrimination linearly inseparable sets 
optimization methods software 
breiman leo jerome friedman olshen stone 
classification regression trees 
belmont massachusetts wadsworth international group 
heath david 
geometric framework machine learning 
ph thesis johns hopkins university baltimore maryland 
holte robert 
simple classification rules perform commonly datasets 
machine learning 
jacobs jordan nowlan hinton 
adaptive mixtures local experts 
neural computation 
lincoln 
synergy clustering multiple back propagation networks 
david touretzky editor advances neural information processing systems pages 
san mateo california morgan kaufmann 
murthy sreerama simon kasif steven salzberg 
system induction oblique decision trees 
journal artificial intelligence research 
pennington humphreys 
automated star galaxy discrimination neural networks 
astronomical journal 
quinlan ross 
induction decision trees 
machine learning 
quinlan ross 
generating production rules decision trees 
proceedings tenth international joint conference artificial intelligence pages 
san mateo california morgan kaufmann 
quinlan ross 
programs machine learning 
san mateo california morgan kaufmann 
salzberg steven 
nearest hyperrectangle learning method 
machine learning 
weiss 
empirical comparison pattern recognition neural nets machine learning classification methods 
proceedings international joint conference artificial intelligence pages 
san mateo california morgan kaufmann 
wolpert david 
stacked generalization 
neural networks 
zhang jill mesirov david waltz 
hybrid system protein secondary structure prediction 
journal molecular biology 
