probable networks plausible predictions review practical bayesian methods supervised neural networks david mackay cavendish laboratory cambridge cb 
united kingdom 
mackay cam ac uk 
bayesian probability theory provides unifying framework data modelling 
framework aims find models data models optimal predictions 
neural network learning interpreted inference probable parameters model training data 
search model space space architectures noise models regularizers weight decay constants treated inference problem infer relative probability alternative models data 
review describes practical techniques gaussian approximations implementation powerful methods controlling comparing adaptive networks 

probability theory occam razor bayesian probability theory provides unifying framework data modelling 
bayesian data modeller aim develop probabilistic models matched data optimal predictions models 
bayesian framework advantages 
probability theory forces explicit modelling assumptions inferences mechanistic 
model space defined question wish pose rules probability theory give unique answer consistently takes account information 
contrast orthodox known frequentist sampling theoretical statistics invent estimators quantities interest choose estimators criterion measuring sampling properties clear principle deciding criterion measure performance estimator criteria systematic procedure construction optimal estimators 
bayesian inference satisfies likelihood principle berger inferences depend probabilities assigned data received properties data sets occurred 
probabilistic modelling handles uncertainty natural manner 
unique prescription marginalization incorporating uncertainty parameters predictions variables 
bayesian model comparison embodies occam razor principle states preference simple models 
point expanded moment 
remainder section reviews bayesian model comparison particular emphasis automatic complexity control provides 
section bayesian interpretation neural network learning 
section discusses probability theory control complexity neural network section discusses bayesian comparison neural network models 
section important bayesian theme explored incorporation parameter uncertainty error bars neural network predictions 
sections review bayesian formulae pruning parameter deletion automatic determination relevance multiple inputs 
section reviews prior probability distribution functions implicit traditional neural networks weight decay regularization 
methods reviewed employ gaussian approximations probability distribution network parameters 
section offers computational short cuts reducing expense approximating bayesian inference 
section compares bayesian framework theories methods applied neural networks highlights differences conventional dogma learning theory statistics 
discussion concluded sketch frontiers current research bayesian methods neural networks 
background reading bayesian methods may helpful 
bayesian methods introduced contrasted orthodox statistics jaynes gull loredo 
bayesian occam razor demonstrated model problems gull mackay 
useful textbooks box tiao berger 

motivations occam razor occam razor principle states preference simple theories 
explanations compatible set observations occam razor advises buy complex explanation 
principle advocated reasons aesthetic theory mathematical beauty correct ugly fits experimental data paul dirac second reason supposed empirical success occam razor 
discuss different justification occam razor coherent inference embodies occam razor automatically quantitatively 

probability theory language coherent inference probability theory cox 
coherent beliefs predictions mapped probabilities 
notation conditional probabilities ajb pronounced probability 
statements list conditional assumptions measure plausibility 
example imagine joe test nasty disease proposition test result positive joe disease quantity ajb number expresses think test give right answer assuming joe disease assumptions reliability test 
rules probability 
product rule relates joint probability conditional probability ajb probability joe disease test positive product probability joe disease probability test detects disease got 
sum rule relates marginal probability distribution ajh joint conditional distributions ajh ajb sum marginalize alternative values example joe disease sum rule states ajh probability obtaining positive result sum probabilities alternative explanations probability result positive joe disease plus probability result positive joe fact disease 
continuous variable sum replaced integral probability probability density 
having specified joint probability variables equation rules probability evaluate beliefs predictions change gain new information change conditioning statements right symbol 
example joe test result positive wish know plausible joe disease measured probability bja obtained bayes theorem bja ajb ajh model situation conditioning statement right hand side probabilities 
sense bayesian inferences subjective possible reason data making assumptions 
time bayesian inferences objective shares assumptions draw identical inferences answer posed problem 
bayesian methods force tacit assumptions explicit provide rules reasoning consistently assumptions 
note conditioning notation imply causation 
bja mean probability joe illness caused positive test result 
measures plausibility proposition assuming information proposition true 

model comparison occam razor evaluate plausibility alternative theories light data follows bayes theorem relate plausibility model data jd predictions model data djh prior plausibility 
gives probability ratio theory theory jd jd djh djh ratio right hand side measures initial beliefs favoured second ratio expresses observed data predicted compared evidence 
bayesian inference embodies occam razor 
gives basic intuition complex models turn probable 
horizontal axis represents space possible data sets bayes theorem rewards models proportion predicted data occurred 
predictions quantified normalized probability distribution probability data model djh called evidence simple model limited range predictions shown djh powerful model example free parameters able predict greater variety data sets 
means predict data sets region strongly suppose equal prior probabilities assigned models 
data set falls region powerful model probable model 
relate occam razor simpler model ratio gives opportunity wish insert prior bias favour aesthetic grounds basis experience 
correspond aesthetic empirical motivations occam razor mentioned earlier 
prior bias necessary second ratio data dependent factor embodies occam razor automatically 
simple models tend precise predictions 
complex models nature capable making greater variety predictions 
complex model spread predictive probability djh data space case data compatible theories simpler turn probable having express subjective dislike complex models 
subjective prior just needs assign equal prior probabilities possibilities simplicity complexity 
probability theory allows observed data express opinion 
turn simple example 
sequence numbers gamma task predict numbers infer underlying process probably gave rise sequence 
popular answer question prediction explanation add previous number 
alternative answer gamma underlying rule get number previous number evaluating gammax 
assume prediction plausible 
second rule fits data just rule add 
find plausible 
give labels general theories sequence arithmetic progression add integer 
sequence generated cubic function form cx dx fractions 
reason finding second explanation plausible arithmetic progressions frequently encountered cubic functions 
put bias prior probability ratio equation 
give theories equal prior probabilities concentrate data say 
theory predict data 
obtain djh specify probability distribution model assigns parameters 
depends added integer number sequence 
say numbers 
pair values fn number gamma give rise observed data probability data djh evaluate djh similarly say values fractions take 
choose represent numbers fractions real numbers real numbers model assign relative probability real parameters norm assumed rest 
reasonable prior state fraction numerator number denominator number 
initial value sequence leave probability distribution ways expressing fraction gamma gamma gamma gamma prior similarly possible solutions respectively 
probability observed data djh theta gamma comparing djh djh prior probabilities equal odds djh djh favour sequence 
answer depends subjective assumptions particular probability assigned free parameters theories 
bayesians apologies thing inference prediction assumptions 
quantitative details prior probabilities effect qualitative occam razor effect complex theory suffers occam factor parameters predict greater variety data sets 
small example data points move larger sophisticated problems magnitude occam factors typically increases degree inferences influenced quantitative details subjective assumptions smaller 

bayesian methods data analysis relate discussion real problems data analysis 
countless problems science statistics technology require limited data set preferences assigned alternative models differing complexities 
example alternative hypotheses accounting planetary motion geocentric model simpler model solar system 
model fits data planetary motion model parameters 
coincidentally extra parameters planet identical period radius sun cycle earth 
intuitively find theory probable 

mechanism bayesian razor evidence occam factor levels inference distinguished process data modelling 
level inference assume particular model true fit model data infer values free parameters plausibly take data 
results inference summarized probable parameter values error bars parameters 
analysis repeated model 
second level inference task model comparison 
wish compare models light data assign sort preference ranking alternatives bayesian methods able consistently quantitatively solve inference tasks 
popular myth states bayesian methods differ orthodox statistical methods inclusion subjective priors difficult assign usually don difference 
true level inference bayesian results differ little outcome orthodox attack 
widely appreciated bayesian performs second level inference section focus bayesian model comparison 
model comparison difficult task possible simply choose model fits data best complex models fit data better maximum likelihood model choice lead inevitably implausible parameterized models generalize poorly 
occam razor needed 
write bayes theorem levels inference described see explicitly bayesian model comparison works 
model assumed vector parameters model defined collection probability distributions prior distribution wjh states values model parameters expected take set conditional distributions value defining predictions djw model data 
model fitting 
level inference assume model ith say true infer model parameters note levels inference distinct decision theory 
goal inference defined hypothesis space particular data set assign probabilities hypotheses 
decision theory typically chooses alternative actions basis probabilities minimize expectation loss function 
concerns inference loss functions involved 
discuss model comparison construed implying model choice 
ideal bayesian predictions involve choice models predictions summing alternative models weighted probabilities section 
data bayes theorem posterior probability parameters wjd djw wjh djh posterior likelihood theta prior evidence normalizing constant djh commonly ignored irrelevant level inference inference important second level inference name evidence common practice gradient methods find maximum posterior defines probable value parameters wmp usual summarize posterior distribution value wmp error bars confidence intervals best fit parameters 
error bars obtained curvature posterior evaluating hessian wmp log wjd wmp taylor expanding log posterior probability deltaw gamma wmp wjd wmp jd exp gamma gamma deltaw deltaw delta see posterior locally approximated gaussian covariance matrix equivalent error bars gamma approximation depend problem solving 
maximum mean posterior distribution fundamental status bayesian inference change non linear 
maximization posterior probability useful approximation equation gives summary distribution 
ii model comparison 
second level inference wish infer model plausible data 
posterior probability model jd djh notice data dependent term djh evidence appeared normalizing constant 
second term subjective prior hypothesis space expresses plausible thought alternative models data arrived 
assuming choose assign equal priors alternative models models ranked evaluating evidence 
normalizing constant djh omitted equation data modelling process may develop new models data arrived inadequacy models detected example 
inference open ended continually seek probable models account data gather 
wmp oe wjd oe wjh wjd 
occam factor 
shows quantities determine occam factor hypothesis having single 
prior distribution solid line parameter width oe posterior distribution dashed line single peak wmp characteristic width oe wjd occam factor oe wjd oe wjd oe 
reiterate key concept rank alternative models bayesian evaluates evidence djh 
concept general evidence evaluated parametric non parametric models alike data modelling task regression problem classification problem density estimation problem evidence transportable quantity comparing alternative models 
cases evidence naturally embodies occam razor 

evaluating evidence study evidence closely gain insight bayesian occam razor works 
evidence normalizing constant equation jh djw wjh dw problems including interpolation common posterior wjd djw wjh strong peak probable parameters wmp 
simplicity dimensional case evidence approximated height peak integrand djw wjh times width oe wjd jh theta oe wjd evidence best fit likelihood theta occam factor evidence best fit likelihood model achieve multiplying occam factor gull term magnitude penalizes having parameter 
interpretation occam factor quantity oe wjd posterior uncertainty suppose simplicity prior wjh uniform large interval oe representing range values possible priori 
oe occam factor oe wjd oe occam factor equal ratio posterior accessible volume parameter space prior accessible volume factor hypothesis space collapses data arrive gull jeffreys 
model viewed consisting certain number exclusive submodels survives data arrive 
occam factor inverse number 
logarithm occam factor measure amount information gain model parameters data arrive 
complex model having parameters free vary large range oe typically penalized larger occam factor simpler model 
occam factor penalizes models finely tuned fit data favouring models required precision parameters oe wjd coarse 
magnitude occam factor measure complexity model dimension abu mostafa relates complexity predictions model data space 
depends number parameters model prior probability model assigns 
model achieves greatest evidence determined trade minimizing natural complexity measure minimizing data misfit 
contrast alternative measures model complexity occam factor model straightforward evaluate simply depends error bars parameters evaluated fitting model data 
displays entire hypothesis space illustrate various probabilities analysis 
models equal prior probabilities 
model parameter shown horizontal axis assigns different prior range oe parameter 
flexible complex model assigning broadest prior range 
dimensional data space shown vertical axis 
model assigns joint probability distribution wjh data parameters illustrated cloud dots 
dots represent random samples full probability distribution 
total number dots model subspaces assigned equal prior probabilities models 
particular data set received horizontal line infer posterior distribution model say reading density horizontal line normalizing 
posterior probability wjd shown dotted curve bottom 
shown prior distribution wjh 
case model poorly matched data shape posterior distribution depend details tails prior wjh likelihood djw curve shown case prior falls strongly 
obtain marginalizing joint distributions wjh axis left hand side 
data set shown dotted horizontal line evidence djh flexible model smaller value evidence placed predictive probability fewer dots line 
terms distributions model smaller evidence occam factor oe wjd oe smaller simplest model smallest evidence best fit achieve data oe oe wjd wjh wjd wjh wjd wjh wjd djh djh djh 
hypothesis space consisting exclusive models having parameter dimensional data set data set single measured value differs small amount additive noise 
typical samples joint distribution shown dots 
nb data points 
observed data set single particular value shown dashed horizontal line 
dashed curves show posterior probability model data set 
evidence different models obtained marginalizing axis left hand side 
poor 
data set probable model 
occam factor parameters posterior approximated gaussian occam factor obtained determinant corresponding covariance matrix equation jh theta det gamma evidence best fit likelihood theta occam factor log wjd hessian evaluated calculated error bars wmp equation 
number data collected increases gaussian approximation expected increasingly accurate 
summary bayesian model selection simple extension maximum likelihood model selection evidence obtained multiplying best fit likelihood occam factor 
evaluate occam factor need hessian gaussian approximation 
bayesian method model comparison evaluating evidence demanding computationally task finding model best fit parameters error bars 

bayesian methods meet neural networks ideas neural network modelling bayesian statistics uneasy bed fellows 
neural networks non linear parallel computational devices inspired structure brain 
backpropagation networks able learn example solve prediction classification problems 
neural network typically viewed black box finds hook crook incomprehensible solution poorly understood problem 
contrast bayesian methods characterized insistence coherent inference clearly defined axioms bayesian circles ad capital 
bayesian statistics neural networks occupy opposite extremes data modelling spectrum 
common theme uniting 
fields aim create models matched data 
neural networks viewed flexible versions traditional regression techniques 
flexible non linear able model regularities data linear models capture 
problem neural networks flexible network stray correlations data discovering non existent structure 
bayesian methods play complementary role 
bayesian probability theory automatically infer flexible model warranted data bayesian occam razor automatically suppresses tendency discover spurious structure data 
occam razor needed neural networks reason illustrated 
consider control parameter influences complexity model example regularization constant 
control parameter varied increase complexity model descending going left right best fit training data model achieve increasingly 
empirical performance model test error function control parameters 
complex model overfits data generalizes poorly 
problem may complicate choice number hidden units multilayer perceptron radius basis functions radial basis function network choice input variables regression problem 
finding values model control parameters appropriate data important non trivial problem 
central message illustrated 
give probabilistic interpretation model evaluate evidence control parameters 
find bayesian occam razor 
complex models probable predict data strongly 
evidence parameters objective function optimization model control parameters 
bayesian optimization model control parameters important advantages 
test set validation set involved available training data devoted model fitting model comparison 
regularization constants optimized line simultaneously optimization ordinary model parameters 
bayesian objective function noisy contrast cross model control parameters training error test error model control parameters log probability training data control parameters 
optimization model complexity 
figures gamma show radial basis function model interpolating simple data set input variable output variable 
regularization constant varied increase complexity model interpolant able fit training data increasingly certain point generalization ability test error model deteriorates 
probability theory allows optimize control parameters needing test set 
validation measure 
gradient evidence respect control parameters evaluated making possible simultaneously optimize large number control parameters 
benefits bayesian framework neural networks reviewed 
aim advocate sole bayesian methods 
practical purposes beneficial look modelling problem perspectives 
bayesian methods typically quite sensitive erroneous modelling assumptions turn bayesian methods give different model choices empirical performance measures cross validation 
differences may give useful insights poor implicit assumptions guide modeller creation new superior models 

neural networks probabilistic models supervised neural network non linear parameterized mapping input output 
output continuous function parameters discontinuous functions may defined lend practical gradient optimization 
architecture net functional form mapping denoted networks trained perform regression classification tasks 

regression networks case regression problem mapping network hidden layer may form hidden layer jl output layer ij example tanh weights biases parameter vector non linear sigmoid function hidden layer gives neural network greater computational flexibility standard linear regression model 
network trained data set fx iteratively adjusting minimize objective function sum squared error ed gamma minimization repeated evaluation gradient ed backpropagation chain rule rumelhart 
regularization known weight decay included modifying objective function fie ffe example ew additional term favours small values decreases tendency model overfit noise training data 

neural network learning inference neural network learning process probabilistic interpretation 
error function interpreted minus log likelihood noise model djw fi zd fi exp gammafie sum squared error ed corresponds assumption gaussian noise target variables parameter fi defines noise level oe fi 
similarly regularizer interpreted terms log prior probability distribution parameters zw ff exp ew quadratic defined corresponding prior distribution gaussian variance oe ff 
probabilistic model specifies functional form network equation likelihood prior 
objective function corresponds inference parameters data wjd ff fi djw fi djff fi zm exp gammam locally minimizing interpreted locally probable parameter vector wmp natural interpret error functions log probabilities 
error functions usually additive 
example ed sum squared errors 
probabilities hand multiplicative independent events joint probability 
logarithmic mapping maintains correspondence 
interpretation log probability adds little new stage 
new tools emerge proceed inferences 
establish probabilistic interpretation classification networks tools apply 

binary classification networks targets data set binary classification labels natural neural network output bounded interpreted probability jx 
example network hidden layer described equation gammaa 
error function fie replaced log likelihood log gamma log gamma total objective function gammag ffe note includes parameter fi 

multi class classification networks classification problem classes represent targets vector single element set indicating correct class elements set 
case appropriate softmax network bridle having coupled outputs sum interpreted class probabilities jx 
part equation replaced exp exp log likelihood case log 
output simple neural network function input 
case regression network minimization objective function gammag ffe corresponds inference form 

probabilistic interpretation neural network learning simple example assume studying binary classification problem minimalist neural network output function jx gammaw deltax form function known statisticians linear logistic neural networks single neuron 
view output neuron defining parameters specified probability input belongs class alternative 
shows output neuron function input vector 
possible value defines different sub hypothesis probability class relative class function different depicted dimensional weight space parameter space network 
point space corresponds function selection different values parameter vector inset figures show function performed network 
notice gradient sigmoid function increases magnitude increases 
imagine receive data shown left column 
data point consists dimensional input vector value indicated theta 
traditional view learning single parameter vector evolves learning rule initial starting point final optimum way minimize objective function measuring data misfit plus regularizer ff 
objective function measures parameters predict observed data probability assigned observed values ftg model parameters set likelihood function shown function second column 
product functions form djw fxg jw functions viewed functions fx tg fixed 
gamma gamma gamma gamma 
weight space 
traditional view product learning point space estimator minimizes objective function 
contrast bayesian view product learning ensemble plausible parameter values bottom right 
choose particular sub hypothesis evaluate posterior probabilities bayes theorem wjd fxg fxg wjh posterior distribution obtained multiplying likelihood prior distribution space shown broad gaussian upper right 
data set likelihood probability parameters constant 
evolution probability distribution parameters data arrives 
posterior ensemble multiplicative constant shown third column contour plot fourth column 
amount data increases top bottom posterior ensemble increasingly concentrated probable value illustrates bayesian interpretation generalization traditional neural network learning 

implementation study variety useful results built bayesian interpretation neural net learning 
results refer regression models corresponding results classification models obtained replacing fie gammag zd fi 
bayesian inference data modelling problems may implemented analytical methods monte carlo sampling deterministic methods employing gaussian approximations 
neural networks analytic methods 
sophisticated monte carlo methods gradient information applied model problems neal 
methods reviewed gaussian approximations posterior distribution 

setting regularization constants ff fi return general case specified equations 
control parameters ff fi determine complexity model 
term model refers set assumptions network architecture form prior parameters form noise model 
different values hyperparameters ff fi define different sub models 
infer ff fi data simply apply rules probability theory ff djff fi ff djh data dependent factor djff fi normalizing constant previous inference call factor evidence ff fi 
assuming weak prior knowledge noise level smoothness interpolant evidence framework optimizes constants ff fi finding maximum evidence ff fi 
approximate posterior probability distribution equation single gaussian wjd ff fi exp gammam wmp gamma gamma wmp gamma wmp gamma rr log wjd ff fi wmp evidence ff fi written log djff fi log zw ff fi gamma wmp gamma log det gamma log zw ff gamma log zd fi terms gamma log det gamma log zw ff constitute log volume factor penalizes small values ff 
maximum evidence elegant properties allow located efficiently line re estimation techniques 
technically may multiple evidence maxima common model space matched data 
shown gull mackay maximum evidence ff ff mp satisfies implicit equation ff mp mp fl mp parameter vector minimizes objective function fie ffe fl number determined parameters fl gamma gamma total number parameters matrix gamma variance covariance matrix defines error bars parameters fl parameters determined relation prior range defined ff 
quantity fl lies recalling ff corresponds variance oe ff assumed distribution fw equation specifies intuitive condition matching prior data variance estimated oe hw average fl effective determined parameters gamma fl effective parameters having set zero prior 
similarly regression problem gaussian noise model maximum evidence value fi satisfies fi mp ed gamma fl ed sum squared residuals expression recognized variance estimator number degrees freedom set fl 
equations re estimation formulae ff fi 
computational overhead bayesian calculations severe necessary evaluate properties error bar matrix gamma argue matrix evaluated anyway order compute uncertainty model predictions 
matrix may evaluated explicitly mackay bishop hassibi stork take significant time number parameters small 
large problems calculations performed efficiently algorithms evaluate products av explicitly evaluating skilling pearlmutter 
combines equations single re estimation formula ratio ff fi 
ratio matters best fit parameters interest 
advantage keeping ff fi distinct knowledge sources bounds value noise level example explicitly incorporated 
move noise models sophisticated gaussian separation hyperparameters essential 
wish construct error bars generate sample posterior parameter distribution monte carlo estimation procedure separate values ff fi relevant 

relationship ideal hierarchical bayesian modelling bayesian probability theory optimize hyperparameters ff fi 
procedure setting hyperparameters model data known circles generalized maximum likelihood empirical bayes 
ideally integrate nuisance parameters obtain posterior distribution parameters wjd predictive distribution jd optimization hyperparameters viewed accurate approximation ideal procedure 
hyperparameter determined data integrating estimating hyperparameter data estimate equations bretthorst gull mackay 
intuition predictive distribution jd dff jd ff ffjd probability ffjd sharply peaked ff ff mp width oe log ffjd distribution jd ff varies slowly log ff scale oe log ffjd ffjd effectively delta function jd jd ff mp error bars log ff log fi differentiating log djff fi twice mackay oe log ffjd fl oe log gamma fl error introduced optimizing ff fi expected small fl ae gamma fl ae 
large fl needs depends problem neural network problems value fl small may suffice predictions optimized network insensitive fold change ff 
possible integrate ff fi early calculation obtaining true prior true likelihood bretthorst gull 
authors recommended procedure buntine weigend wolpert counterproductive far practical manipulation concerned gull mackay resulting true posterior skew peaked distribution apart monte carlo methods currently computational techniques cope directly distributions 
correction term approximates integration ff fi predictions final step calculations 

multiple regularization constants simplicity far assumed single class weights modelled coming single gaussian prior oe ff 
dimensional terms weights usually fall distinct classes network equation example dimensional classes fw fw consistency weights different classes modelled coming single prior 
assuming gaussian prior class define ew assign prior fw zw exp gamma ff ew class scale parameter weight decay rate ff network performance enhanced division weights different classes 
automatic relevance determination model section uses prior 
evidence framework optimizes decay constants finding probable value maximum fff 
maximum evidence fff satisfy implicit equations ff mp mp fl mp parameter vector minimizes objective function fie ff ew fl number determined parameters class fl gamma ff trace gamma number parameters class trace parameters 
simplicity discussion assume single parameter ff 

model comparison evidence framework divides inferences distinct levels inference completed 
level infer parameters values ff fi wjd ff fi djw ff fi fi djff fi level infer ff fi ff djff fi ff djh level compare models hjd djh pattern applications bayes theorem higher levels data dependent factor level djff fi precisely normalizing constant evidence preceding level inference 
pattern inference continues compare different models different architectures regularizers noise models 
alternative models ranked evaluating djh normalizing constant inference 
preceding section reached level gaussian approximation wjd ff fi 
evaluate evidence separable gaussian approximation log ff log obtain estimate djh djff mp fi mp log ff mp log fi oe log ffjd oe log djff mp fi mp obtained equation error bars log ff log fi equation 
gaussian approximation ff fi holds fl ae gamma fl ae mackay 

multi modal distributions preceding exposition falls difficulty posterior distribution wjd ff fi significantly multi modal usually case multi layer neural networks 
persist gaussian approximations introduce modifications 
recognize typical optimum wmp related number equivalent optima symmetry operations interchange hidden units inversion signs weights 
evaluating evidence local gaussian approximation symmetry factor included equation take account equivalent islands probability mass case net hidden layer units equation appropriate permutation factor 
general wmp factor 
permutations hidden units factor reversal sign weights hidden unit 
second multiple optima related model symmetries 
modify framework changing goals specifically view local probability peaks distinct model 
inferring posterior ff fi entire model allow local peak posterior choose optimal value parameters 
similarly evaluating evidence entire model aim calculate posterior probability mass local optimum 
procedure natural final implementation model parameter vector set particular value small set values error bars 
case probability entire model important probability local solutions find 
method chopping complex model space unsupervised classification system autoclass hanson 
henceforth term model refer pair fh sw denotes model specification sw specifies solution neighbourhood optimum adopting shift objective gaussian integrals alteration set ff fi compare alternative solutions assuming posterior probability consists separated islands parameter space roughly gaussian 
general ff fi gaussian approximation accurate need accurate small range ff fi close probable value 
sufficiently large amounts data compared number parameters approximation expected hold 
practical experience indicates useful approximation real problems 

error bars predictions having progressed levels modelling inference task predictions adapted model 
common practice simply probable values making predictions optimal shall see 
bayesian prediction new datum involves marginalizing uncertainty levels jd dff dfi wp jw ff fi ff fi hjd evaluation distribution jw ff fi generally straightforward requiring single forward pass network 
typically marginalization affects predictive distribution significantly integration ff fi lesser effect 

error bars predictions trained regression network 
solid line gives predictions best fit parameters multilayer perceptron trained data points shown 
error bars dotted lines produced uncertainty parameters notice error bars larger data sparse 

implementation marginalization done analytically 
possible monte carlo methods neal may 
average function uncertain parameter estimated tolerable error obtaining small number samples evaluating mean observed values 
variance estimator independent dimensionality scales inversely sample size 
cheap way obtaining samples wjd ff fi described section 
methods gaussian approximations described 

error bars regression integrating fixed ff fi predictive distribution jd ff fi wp jw fi wjd ff fi take case prediction single target gaussian approximation posterior wjd ff fi noise model gaussian local linearization output function parameters wmp delta gamma wmp sensitivity output parameters fi fi fi fi wmp predictive distribution straightforward gaussian integral 
distribution mean wmp variance oe fi gamma oe log wjd ff fi 
illustrates error bars corresponding interesting term expression gamma predictions trained multilayer perceptron hidden units 

joint error bars multiple predictions 
wishes simultaneously predict targets ft single network multiple outputs sequence different inputs wish predict corresponding targets 
sensitivities outputs parameters covariance matrix values fy gamma matrix theta matrix expresses gaussian approximation expected variances covariances predicted variables 
correlated error ellipsoids demonstrated output regression network mackay 

additional uncertainty produced uncertainty hyperparameters 
integration regularization constants ff fi contributes additional variance direction leading order fl gamma jd normal variance mackay oe gamma oe log ffjd oe log mp mp oe mp log ff ffa gamma wmp oe log ffjd fl oe log gammafl 
integrating models committees multiple regression models predictive distribution obtained summing predictive distribution model weighted posterior probability 
single prediction required loss function quadratic optimal prediction weighted mean models predictions wmp 
weighting coefficients posterior probabilities obtained evidences djh 
evaluate accurately alternative pragmatic prescriptions weighting coefficients exist breiman mackay 

error bars classification case linearized regression discussed mean predictive distribution identical prediction mean wmp case classification problems 
best fit parameters give confident predictions 
non bayesian approach problem weight predictions empirically determined factor 
bayesian viewpoint helps understand cause problem provides straightforward solution demonstrably superior ad hoc procedure 
issue illustrated simple class problem 
shows binary data set modelled linear logistic function defined equation including bias decision boundary obliged pass origin 
best fit parameter values give predictions shown contours 
reasonable predictions 
consider new data arriving points best fit model assigns examples probability class 
intuitively inclined assign confident probability closer point training data 
mp samples 
integrating error bars classifier 
binary data set 
classes denoted theta ffi 
data modelled linear logistic function 
best fit model shown predictive contours 
best fit model assigns probability class inputs posterior probability distribution model parameters wjd schematic third parameter bias shown 
parameters perfectly determined data 
typical samples posterior indicated points labeled 
panels show corresponding classification contours 
sample 
sample 
notice point classified differently different plausible classifiers classification relatively stable 
obtain bayesian predictions integrating posterior distribution width decision boundary increases move away data point 
see text discussion 
precisely result obtained marginalizing parameters posterior probability distribution depicted 
random samples posterior define different classification surfaces illustrated samples figures point classified differently different plausible classifiers classification relatively stable 
obtain bayesian predictions averaging predictions plausible classifiers 
resulting contour remains similar best fit parameters 
width decision boundary region increases move away data full accordance intuition 
bayesian approach superior best fit model predictions selectively weighted different degree test case 
consequence bayesian classifier better able identify points classification uncertain 
pleasing behaviour results simply mechanical application rules probability 
binary classifier numerical approximation integral gaussian posterior distribution mackay 
equivalent approximation multi class classifier implemented 
marginalization done monte carlo methods 
disadvantage straightforward monte carlo approach poor way estimating probability improbable event close zero improbable event occur conjunction improbable parameter values 
cases temporarily add event question data set evaluate evidence jh 
desired probability obtained comparing previous evidence djh evidence assuming complementary virtual data set jh 

pruning evidence serve guide pruning changing model setting selected parameters zero 
done straightforward way parameter network tentatively pruned new model optimized evidence evaluated decide accept pruning 
alternative procedure gaussian approximation described 
pruning fact idea questioned sections 
suppose model locally linear ff fi determined model parameters prior posterior distributions exactly gaussian brevity ff fi omitted conditioning propositions section wjh zw exp gamma gammaff iw delta wjd zm exp gamma gammam mp gamma deltaw deltaw delta deltaw gamma wmp log evidence log djh gammam mp gamma log det log det ffi const interested evaluating difference evidence model alternative model subscript denotes setting zero parameter remaining parameters gaussian distribution confined constraint surface delta unit vector direction deleted parameter 
evaluate difference evidence finding location new optimum mp evaluating change mmp deltam mp gamma deltaw deltaw evaluating change log determinant distribution 
task accomplished introducing lagrange multiplier 
find mp wmp gamma oe gamma deltam mp oe marginal error bars parameter oe gamma gamma ss quantity deltam mp saliency term advocated guide optimal brain damage lecun hassibi stork 
change evidence involves second occam factor term simple calculate 
change evidence single parameter deleted log djh gamma log djh oe log oe oe oe prior variance parameter objective function select parameter delete 
tells pruning precise pruning yielding probable model positive parameters equivalent expression worked case simultaneous pruning multiple parameters 
consider pruning parameters 
obtain joint theta covariance matrix pruned parameters sigma reading appropriate submatrix gamma evidence difference log djh gamma log djh sigma gamma log det sigma ks oe bayesian formulae incorporate small additional volume terms included brain surgery literature 
opinion pruning technique superseded sophisticated regularizers discussed section 
automatic relevance determination automatic relevance determination ard model mackay neal unpublished implemented methods described previous sections 
suppose regression problem input variables irrelevant prediction output variable 
finite data set show random correlations irrelevant inputs output conventional neural network weight decay fail set coefficients junk inputs zero 
irrelevant variables hurt model performance particularly variables data 
selecting best input variables non trivial problem 
ard model define prior regression parameters embodies concept uncertain relevance model effectively able infer variables relevant switch 
achieved simple soft way introducing multiple weight decay constants ff associated input 
decay rates junk inputs automatically inferred large preventing inputs causing significant overfitting 
ard model uses prior equation 
network having hidden layer weight classes class input consisting weights input hidden layer class biases hidden units class output consisting bias weights hidden layer 
control ard model implemented equation 
automatic relevance determination useful alternative technique pruning section embodies concept relevance discrete manner 
possible advantages ard include pruning bayesian model comparison requires evaluation determinants inverses large hessian matrices may ill conditioned 
ard hand implemented evaluations trace hessian robust 
stochastic dynamics implementation ard fact matrix computations needed conditional distribution ff gamma distribution gibbs sampling procedure alternately sampling fff fff ii compared non bayesian cross validation method ard simultaneous infers utility large numbers possible input variables 
single crossvalidation measure explicitly prune variable time order estimate variables useful 
contrast ard returns vectors measuring relevance input variables regularization constants ff fl suppresses irrelevant inputs intervention 
iii ard allows large numbers input variables unknown relevance left model harm 
practical problems implementing ard model gaussian approximations irrelevant variables explicitly pruned large model computation times remain large 
ii presence large numbers irrelevant variables model hampers calculation evidence different models 
numerical problems arise calculation determinants hessians 
interfere bayesian optimization regularization constants prevents bayesian model comparison methods 
iii ard model intended embody soft version pruning approximations evidence framework lead singularities ff going signal noise ratio low causes inputs irreversibly shut 
spite reservations am confident right direction adaptive modelling methods lies replacement discrete model choices continuous control parameters 
common concern extra hyperparameters fff cause overfitting 
cause worry 
reasons 
evaluate evidence evaluate objectively new model probable data 
extra parameters penalized occam factors eventually massively increased number hyperparameters evidence maximum reached 
fact occam factors regularization constants weak error bars log ff scale fl fact relates second reason extra hyperparameters fff incapable causing overfitting data 
parameters overfit noise worst overfitting occurs regularization constants ff switched zero 
extra hyperparameters effect worst case capacity model 
effect positive damping unneeded degrees freedom model 
weak probabilistic penalty extra parameters simply increase variety simple data sets model capable predicting 
model hyperparameter ff capable realising flavour simplicity parameters small flavour complexity parameters big 
model having say hyperparameters fff predict total flavours simplicity complexity including special cases 
output input input input input input output 
samples prior input network input network 
varying number hidden units curve different number hidden units 
regularization constants input weights hidden unit biases fixed oe oe bias 
output weights oe keep dynamic range function constant 
varying oe varying oe changes characteristic scale length oscillations width region input space action occurs 
fh oe bias oe 
oe 
smaller value oe steep function 
varying oe bias oe 
seed samples weights simply scaled regularization constants movie progresses 
ratio oe oe bias cases keep action horizontal range sigma 
constant oe bias took values 
constant determines total number fluctuations function 
constant oe determines input scale fluctuations occur 
input network fh oe oe bias oe 

implicit priors interesting examine sort functions generated networks parameters set sampling prior distributions equations 
study prior distributions provides guidelines expected scaling behaviour regularization constants number hidden units identifies control parameters responsible controlling complexity function 
regression nets hidden layer tanh units gaussian priors dimensional classes weights fw fw find interesting result neal 
limit complexity functions generated prior independent number hidden units 
prior input hidden weights determines spatial scale inputs variations function 
prior biases hidden units determines characteristic number fluctuations function 
prior output weights determines vertical scale output 
figures gamma illustrate samples priors input output network large number hidden units 
illustrates number hidden units increased keeping foe oe bias oe fixed properties random sample prior remain stable 
output weights get smaller accordance oe order keep constant vertical range function sum independent random variables finite variance 
fact limit prior functions tends non trivial gaussian process neal 
illustrates effect varying oe 
illustrates effect varying oe oe bias way keep constant range action input variable oe bias oe parameter oe bias determines total number fluctuations function 
progressing multiple inputs obtain setting weights net random values plotting output net 
picture shows typical sample homogeneous random looking function hidden units activities linear functions inputs 
prior distribution functions symmetrical zero input space output space 
wise bayesian model preprocess inputs targets zero expected centre action 

cheap implementations methods solve tasks automatic optimization fff fi section calculation error bars parameters predictions section calculation hessians sophisticated monte carlo methods 
methods depend gaussian assumptions rest approximations 

cheap approximations optimization ff fi neglecting distinction determined poorly determined parameters obtain update rules ff fi cf 
equations ff fi ed easy program procedure expected give singular behaviour large number poorly determined parameters 

cheap generation predictive distributions simple way obtaining random samples posterior probability distribution parameters bayesian image reconstruction skilling 
approximate procedure accurate noise gaussian model treated locally linear 
start converged network parameters trained true data set fx estimate gaussian noise level residuals example oe gamma gamma alternatively estimate oe test set 
ii define new data set adding artificial gaussian noise magnitude oe outputs true data set fx normal oe 
noise added inputs 
iii starting train new network call converged weight vector data set changed little added noise close optimization take long 
iv repeat steps twelve times generating new data set original data set time obtain new save list vectors 
separately predictions 
example case time series continuation generate entire continuation 
predictions viewed samples model predictive distribution 
summarized measuring mean variance 
order get true sample posterior perturb prior 
weight mean weight decays ordinarily zero randomized step sampling gaussian variance oe ff 
method particularly useful obtaining error bars neural nets forecast time series bootstrapping network predictions 
full bayesian treatment time series modelling neural nets 

discussion 
applications methods sections successfully applied practical problems 
applied methods industrial problem prediction fat content data 
evidence framework yields better performance standard techniques involving cross validation 
improvement attributed fact bayesian framework needs validation set available data parameter fitting optimization model complexity model comparison 
automatic relevance determination model section win prediction competition involving modelling energy consumption building mackay 
success attributed fact evidence framework simultaneously optimize multiple regularization constants fff line 
regularization constants involved networks 

modelling insights advantage bayesian framework data modelling forces explicit assumptions constructing model 
poor modelling assumption identified probabilistic viewpoint easy design coherent modifications model 
example mackay standard weight decay model regularization constant ff applied layer regression network 
evidence different solutions poorly correlated empirical performance 
failure forced home insight dimensional consistency different ff required input hidden weights biases hidden units connections outputs 
model multiple regularizers produced solutions slightly improved empirical performance importantly evidence solutions better correlated generalization error 
automatic relevance determination model section straightforward extension larger number hyperparameters 
examples model modifications easily motivated probabilistic viewpoint 
sum squared error corresponds assumption residuals gaussian uncorrelated different target variables 
time series modelling may poor model residuals show local trends 
better model example assume gaussian correlations residuals data error fie replaced gamma fi gamma fi gamma gamma fi gamma gamma delta modify backprop algorithm propagated error signal frame weighted combination residuals neighbouring frames 
network experience urge fit local trends data 
predictions model correlations residuals able capture current trend modify net predictions accordingly 
evidence optimize correlation model parameters fi fi fi gaussian noise model modified include possibility outliers bayesian robust noise model box tiao 
probability theory allows infer data heavy tails noise model ought 
assumption output noise level input vectors 
discussed mackay chapter construct parameterized model noise level fi learned evidence maximization 
final example probabilistic motivation model modification lies image analysis 
neural net character recognition say expect trained net input weights spatially correlated 
desirable incorporate prior expectation model adaptation process priors parameters damp unnecessary degrees freedom reduce overfitting give rise probable models 
way creating correlation pre blur data linear filter feeding network traditional uncorrelated prior parameters 
equivalent keeping original inputs having correlated prior parameters correlations defined implicitly properties pre blur 
procedure fruitfully character recognition guyon 

relationship theories generalization bayesian model comparison assesses defined hypothesis space probable set alternative models 
want know model expected generalize 
empirically correlation evidence generalization error mackay 
theoretical connection established 
brief discussion similarities differences evidence quantities arising prediction generalization error 

relation moody generalized prediction error generalization akaike final prediction error non linear regularized models 
estimators generalization error derived making assumptions distribution residuals data true interpolant assuming true interpolant belongs class 
derived assumption observed distribution inputs training set gives approximation distribution inputs 
difference total number parameters replaced effective number parameters fact identical quantity fl arising bayesian analysis 
ed half sum squared error predicted error data point gamma ed oe fl delta added term oe fl intuitive interpretation terms overfitting 
parameter determined data unavoidably overfit direction noise 
effects ed smaller ought oe average means predictions vary ideal predictions infinite data prediction error input points average worse oe 
sum terms multiplied effective number determined parameters fl gives correction term 
log evidence form data error plus term penalizes complexity 
quantity fl arises bayesian analysis bayesian occam factor scaling behaviour term see 
empirically predictor generalization 
reason derivation effectively assumed test samples drawn locations received data 
consequences false assumption serious parameterized flexible models 
additional distinction evidence framework defined regression problems evidence evaluated regression classification density models 

relation effective dimension 
structural risk minimization guyon uses empirically tuned expressions form gen ed log fl fl fl effective dimension model identical quantity 
constants determined experiment 
structural risk theory currently intended applied nested families classification models absence fi ed dimensionless monotonic effective dimension evidence evaluated models 
interestingly scaling behaviour expression identical scaling behaviour log evidence subject assumptions 
value regularization constant satisfies 
second significant eigenvalues ff scale ff fl 
scaling holds various simple interpolation models 
shown scaling log evidence gamma log djff fi fie mp fl log fl fl readers familiar minimum description length mdl recognize dominant fl log term mdl bayesian inference equivalent discussed 
scaling behaviour log evidence identical structural risk minimization expression provided 
isabelle guyon personal communication confirmed empirically determined values close bayesian values 
interesting try understand develop relationship 

contrasts conventional dogma learning theory statistics 
representation theorems 
popular assert utility particular model demonstrating model arbitrary representational power 
example layer neural networks interpolation tools implement smooth function hidden units 
bayesian data modeller takes different attitude fair learning theory researchers 
objective data modelling find model data 
model match arbitrary function flexible generalize poorly bayesian terms model improbable compared simpler models fit data nearly 
probability theory fact favours models inflexible possible just flexible capture real structure data 
appreciate models universal representational power necessarily thing led astray direction second popular myth supposed need limit complexity model little data 

complexity model matched amount data 
central tenet vapnik chervonenkis theory little data model parameters model known incapable representing true function 
attempt match capacity model number data points 
motivation pruning neural network hassibi stork 
bayesian needs choice models consider matter prior belief depend amount data collected 
domain interpolation typical prior belief real underlying function smooth require large number parameters describe exactly 
data determine parameters true model 
model truly believe 
possible outcomes 
may prior knowledge smoothness included true model may constrain ill determined parameters sufficiently predictions useful 
hand may turn predictions ill determined true model predictions huge error bars 
case surely unwise simpler model predictions better determined incorrect practice deliberately simple model data scarce probably main causes dangerously predictions orthodox statistics 
second cause practice accepting null hypothesis prediction test hypothesis gives result significant 
common practice bayesians fit models parameters number data points mackay neal weir 
recommended philosophy box tiao aim incorporate imaginable possibility model space example conceivable simple model able explain data include simple models model space noise long tailed distribution include hyperparameter controls tails distribution input variable irrelevant regression include regression anyway sophisticated regularizer embodying concept uncertain relevance 
inclusion remote possibilities model space safe inferences home sub models best matched data 
inclusion initial model space bizarre models subsequently ruled data expected significantly influence predictive performance 

minimum description length mdl complementary view bayesian model comparison obtained replacing probabilities events lengths bits messages communicate event loss receiver 
message lengths correspond probabilistic model events relations gammal gamma log non integer coding lengths handled arithmetic coding procedure witten 
mdl principle wallace boulton states prefer models communicate data smallest number bits 
consider message states model communicates data model pre arranged precision ffi produces message length djh 
lengths different define implicit prior alternative models 
similarly djh corresponds density djh 
procedure assigning message lengths mapped jh djw jh djw jh djw 
popular view model comparison minimum description length 
model communicates data sending identity model sending best fit parameters model sending data relative parameters 
proceed length parameter message increases 
hand length data message decreases complex model able fit data better making residuals smaller 
example intermediate model achieves optimum trade trends 
posterior probabilities gamma log gamma log djh gamma log hjd const principle mdl interpreted bayesian model comparison vice versa 
simple discussion addressed evaluate key data dependent term djh corresponds evidence message imagined subdivided parameter block data block 
models small number parameters short parameter block fit data data message list large residuals long 
number parameters increases parameter block data message shorter 
optimum model complexity sum minimized 
picture glosses subtle issues 
specified precision parameters sent 
precision important effect precision ffi real valued data sent assuming ffi small relative noise level just introduces additive constant 
decrease precision sent parameter message shortens data message typically truncated parameters match data 
non trivial optimal precision 
simple gaussian cases possible solve optimal precision wallace freeman closely related posterior error bars parameters gamma log wjd 
turns optimal parameter message length virtually identical log occam factor equation 
random element involved parameter truncation means encoding slightly sub optimal 
care replicate bayesian results mdl terms 
earliest complex model comparison involved mdl framework patrick wallace mdl apparent advantages direct probabilistic approach 
mdl uses pedagogical tool 
description length concept useful motivating prior probability distributions 
different ways breaking task communicating data model give helpful insights modelling process illustrated 

line learning cross validation 
log evidence decomposed sum line predictive performances log djh log jh log jt log jt delta delta delta log jt gamma decomposition explain difference evidence leave cross validation measures predictive ability 
cross validation examines average value just term log jt gamma random re orderings data 
evidence hand sums model predicted data starting scratch 

bits back encoding method 
mdl thought experiment hinton van camp involves incorporating random bits message 
data communicated parameter block data block 
parameter vector sent random sample posterior distribution wjd djw wjh djh 
sample sent arbitrary small granularity ffi message length wjh gamma log wjh 
data encoded relative message length djw gamma log djw 
data message received random bits generate sample posterior deduced receiver 
number bits recovered gamma log wjd 
recovered bits need count message length optimally encoded message random bit string communicating message time 
net description cost wjh djw gamma bits back gamma log wjh djw wjd gamma log djh gamma log ffi thought experiment yielded optimal description length 

ensemble learning posterior distribution wjd may complicated density 
methods described assumed local regions contain significant probability mass posterior approximated gaussian making quadratic expansion log wjd local maximum 
brevity omit parameters ff fi 
interesting idea implemented hinton van camp try improve quality type approximation optimizing entire posterior approximation 
idea gaussian fitted mode wjd sense better approximation posterior 
consider parameterized approximation true posterior distribution wjd 
example parameters gaussian approximation mean covariance matrix 
measure quality fit variational free energy gamma dw log wjd lower bound zero realised parameters matches exactly 
measure motivated generalizing mdl bits back thought experiment section random sample drawn hinton van camp 
task minimize 
minimization called ensemble learning general challenging task 
hinton van camp shown exact derivatives respect obtained neural net non linear hidden layer linear output gaussian approximation restricted correlation weights 
weakness ensemble learning free energy minimization approximating distribution simple form free energy objective function favours distributions extremely conservative placing probability mass regions small 
example strongly correlated gaussian modelled separable gaussian free energy solution sets curvature log diagonal elements curvature log gives approximating distribution covers far small region space outcome ensemble learning essentially identical outcome traditional optimization point estimate 
possible extension hinton van camp idea complex models include adaptive linear preprocessing inputs 
denote coefficients linear mapping inputs sub inputs parameters sub inputs hidden units effective input weights product vu 
separable gaussian prior applied parameters hinton exact derivatives evaluated 
inclusion additional parameters defines richer family probability distributions effective parameters interesting see distributions powerful yield gaussian approximations superior produced evidence framework 

directions challenges face bayesian approach data modelling invention model spaces creation numerical techniques inference spaces 
non trivial tasks requiring skill ingenuity 
scaling gaussian approximation methods larger neural network problems helped implicit second order methods skilling pearlmutter algorithms properties hessian matrix value av arbitrary vector explicitly evaluating multilayer perceptrons established probabilistic models regression classification conditional modelling tasks input variables assumed condition values modelling distribution output variables model density input variables constructed 
density modelling generative modelling hand density observable quantities constructed 
multi layer perceptrons conventionally create density models belief networks spiegelhalter lauritzen neural networks boltzmann machine hinton sejnowski define density models 
researchers presently working extending multilayer perceptrons turn density models hinton zemel mackay 
acknowledgments colleagues caltech university toronto university cambridge invaluable discussions 
am grateful radford neal timothy helpful comments manuscript 
abu mostafa vapnik chervonenkis dimension information versus complexity learning neural computation berger statistical decision theory bayesian analysis springer bishop exact calculation hessian matrix multilayer perceptron neural computation box tiao bayesian inference statistical analysis addison wesley breiman stacked regressions technical report dept stat univ cal berkeley bretthorst bayesian spectrum analysis parameter estimation springer bridle probabilistic interpretation feedforward classification network outputs relationships statistical pattern recognition neuro computing algorithms architectures applications ed soulie 
springer verlag buntine weigend bayesian back propagation complex systems regression prediction shrinkage discussion statist soc cox probability frequency reasonable expectation am 
physics gull bayesian inductive inference maximum entropy maximum entropy bayesian methods science engineering vol 
foundations ed erickson smith pp dordrecht 
kluwer developments maximum entropy data analysis maximum entropy bayesian methods cambridge ed skilling pp dordrecht 
kluwer guyon vapnik boser bottou solla structural risk minimization character recognition advances neural information processing systems ed moody hanson lippmann pp san mateo california 
morgan kaufmann hanson stutz cheeseman bayesian classification correlation inheritance proceedings th international joint conference artificial intelligence sydney australia hassibi stork second order derivatives network pruning optimal brain surgeon advances neural information processing systems ed giles hanson cowan pp san mateo california 
morgan kaufmann hinton sejnowski learning relearning boltzmann machines parallel distributed processing ed rumelhart mcclelland pp 
cambridge mass mit press van camp keeping neural networks simple minimizing description length weights appear proceedings colt zemel minimum description length helmholtz free energy advances neural information processing systems ed cowan tesauro alspector san mateo california 
morgan kaufmann jaynes bayesian intervals versus confidence intervals jaynes 
papers probability statistics statistical physics ed 
kluwer jeffreys theory probability oxford univ press lecun denker solla optimal brain damage advances neural information processing systems ed touretzky pp 
morgan kaufmann loredo laplace supernova sn bayesian inference astrophysics maximum entropy bayesian methods dartmouth ed pp 
kluwer mackay bayesian methods adaptive models phd thesis california institute technology bayesian interpolation neural computation practical bayesian framework backpropagation networks neural computation evidence framework applied classification networks neural computation bayesian non linear modelling prediction competition ashrae transactions pt atlanta georgia 
ashrae bayesian neural networks density networks nuclear instruments methods physics research section hyperparameters optimize integrate 
maximum entropy bayesian methods santa barbara ed dordrecht 
kluwer moody effective number parameters analysis generalization regularization nonlinear learning systems advances neural information processing systems ed moody hanson lippmann pp san mateo california 
morgan kaufmann neal bayesian learning stochastic dynamics advances neural information processing systems ed giles hanson cowan pp san mateo california 
morgan kaufmann priors infinite networks technical report preparation univ toronto patrick wallace stone circle geometries information theory approach old world ed pp 
cambridge univ press pearlmutter fast exact multiplication hessian neural computation rumelhart hinton williams learning representations back propagating errors nature skilling bayesian numerical analysis physics probability ed jr cambridge 
robinson gull probabilistic displays maximum entropy bayesian methods ed pp dordrecht 
kluwer spiegelhalter lauritzen sequential updating conditional probabilities directed graphical structures networks ace bayes application neural networks pruning technical report danish meat research institute wallace boulton information measure classification comput 
wallace freeman estimation inference compact coding statist 
soc 
weir applications techniques hst data proceedings st ecf data analysis workshop april witten neal cleary arithmetic coding data compression communications acm wolpert evidence neural networks advances neural information processing systems ed giles hanson cowan pp san mateo california 
morgan kaufmann david mackay 
