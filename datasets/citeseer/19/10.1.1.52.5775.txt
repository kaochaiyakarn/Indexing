david andre learning hierarchical behaviors david andre computer science division university california berkeley soda hall berkeley ca cs berkeley edu www cs berkeley edu years researchers begun study introduce hierarchy reinforcement learning methods 
generally pushed envelope important directions typically avoided question learn systems hierarchical behavior experience 
preliminary system learning simple hierarchical systems constrained language behavior specification 
system learns macros choosing subgoal past experience respect subgoal build constrained macro behavior achieves 
system run offline new similar problems learning 
algorithm combines macro learning asynchronous value iteration methods prioritized sweeping 
macros learned system simple environment discussed 
additionally related literature sketch project 
nearly field artificial intelligence researchers idea computers learn learned learn 
years researchers field reinforcement learning followed maxim attempting incorporate hierarchical systems 
typically roughly grouped categories state aggregation explicit subgoal hierarchies explicit mdp decomposition behavior temporal abstraction 
note course techniques fall multiple categories 
state aggregation idea utilize approximation value function clustering values certain states treating identical related 
study function approximation techniques falls camp degree 
case values states stored updated approximation function see bertsekas tsitsiklis 
techniques hierarchical fashion represent modular structure state space 
technique explicit hard coding aggregation researchers investigated adaptive aggregation algorithms moore parti game algorithm 
mccallum area utilizes similar techniques partially observable domains learns decision tree structures encode control policy 
uther develops mccallum extends continuous state spaces 
researchers noticed similarity deterministic planning domains reinforcement learning brought ideas abstraction developed field 
dayan hinton feudal reinforcement learning system specifies explicit tree structure state spaces controllers allows hierarchical decomposition problem 
dietterich maxq algorithm generalizes idea 
somewhat falling areas subfield mdp decomposition represents area quasi hierarchical structure reinforcement learning 
idea problem environment mdp david andre contains regions largely separable problem attacked divide conquer style algorithms dean lin bertsekas tsitsiklis 
done automatically determining decomposition state space lin parr singh 
idea treating complex potentially learned actions simple ones long utilized ai fikes hart nilsson laird newell benson nilsson 
limited single step actions domain techniques utilize complex macro actions extend time state space 
researchers attempted increase power rl systems incorporating idea including singh kaelbling thrun schwartz schmidhuber takahashi precup sutton parr russell parr 
systems benefit hierarchy cases various behaviors predetermined pre learned defined user supplied subgoals learned 
applications system determine appropriate subgoals learn hierarchy behaviors automatically 
sutton precup singh addresses issue modifying parts learned macros called options method learning options somewhat undesirable requires user specify explicit subgoals 
additionally formulation allow full hierarchy compositions primitive actions 
presents method automatically learning hierarchical behaviors list possible subgoals algorithm determining subgoals 
constrained view macro behavior simple loop system able construct macro behaviors experience estimate utility macro potential computational savings 
words build concise macros experience current policy states abstracted macro 
macro learning system applied offline learned models environment learned value function applied learning 
idea complex problems agent able learn macros experience early stages solving problem allow solve problem quickly 
organized follows 
introduce mdp notation 
attempt define macro 
macro language discuss relate previous methods defining macros 
represents class project useful case discuss approaches succeed briefly describe alternative approach failed 
section describes system learning macros 
results applying macro learner simple worlds offline case 
discuss incorporation macro learner model reinforcement learning algorithms focusing prioritized sweeping 
conclude discussion description brief 
background markov decision processes prioritized sweeping assume standard markov decision process mdp framework reinforcement learning kaelbling 
denote set possible environment states set possible actions 
assume system markovian probability reaching state state executing depend system arrived state additionally assume state agent receives reward depends current transition 
assume system fully observable agent determine current state environment agent attempts maximize expected discounted accumulated reward 
problem reinforcement learning build procedures achieve objective agent initial knowledge known mdp setting agent choose optimal policy access optimal value function function measures optimal expected accumulated reward achieved state satisfies bellman equation max succ agent access agent choose optimal actions models reward transition probabilities typically assume agent learn value function experience task reinforcement learning 
review field see bertsekas tsitsiklis kaelbling 
main types methods model model free methods 
model free david andre methods learning watkins considered detail 
model methods utilize model environment perform value propagation steps planning learning 
techniques essentially repeating simple loop take step world update reward transition model number value propagations 
algorithms differ states undergo value update step world 
classic asynchronous value iteration model bertsekas tsitsiklis states undergo value updates significant changes cease take place 
dyna sutton small number random states undergo updates 
prioritized sweeping moore atkeson states undergo large changes value chosen updated 
peng william similar approach dyna architecture 
generalized prioritized sweeping andre friedman parr extends prioritized sweeping model noticing utilizing fact priorities states ordered gradient bellman equation 
macro 
order understand motivations macro scoring metrics important grasp characteristics macro shooting 
clearly want hierarchy far proved dominant technique allowing difficult problems tackled 
exactly mean context reinforcement learning 
types individual macros contribute hierarchy 
things macros 
high probability accomplish subtask useful task hand 
ideally macros useful setting context 
want away detail subproblem problem easier 
argue reasonably important macros concise full value function states covered macro 
interesting issues decomposition arise constraint generalization ability macros somewhat limited 
desiderata see dimensions macros judged 
macros specify behaviors achieve poor payoff reward structure space 
despite potential elegance simplicity macro walk hallway head wall application seeks high quality solution problem 
second macros concise away complexity problem 
macros learner fewer decisions follows policy 
noted thrun schwartz criteria represent common tradeoff expressiveness conciseness approach variety methods including minimum description length metrics priors hypotheses simple constraints expressiveness learned behaviors 
macro representation language simple iterative actions occur different environments 
simple performing action goal achieved drive store walk hallway complex write complex behaviors constructed repeated executions relatively simple behavior 
macro language constrain system simple style macros 
powerful representation parr simple hope able learn 
macros consist parts behavior specification function decides cease execution col row left 
example macro part termination condition part 
david andre macro 
example macro shown 
behavior specification macro consists decision list decision tree constrained complexity decisions depth list tree 
action specified leaves list tree actions primitive actions macros 
assume macros allowed recursive 
termination conditions consist simple conjunction constrained number literals 
literals termination conditions behavior specification set features determined state current project assuming state fully observable features guaranteed sufficient exactly pick state importantly minimal set features extra perceptual information easily added 
concept motivated fact simple behaviors consist utilizing local perceptual information hallway fact see store walking 
precise features percepts features percepts termination conditions 
features behavior specification 
appear macro feature assignment values feature take 
addition equality operator consider inequality expressions 
assumed feature assignments allowed appear macro 
macro executed takes control agent achieves goal timeout function causes early 
necessary learned macros necessarily guaranteed return sutton similar macros call options useful early depending value function 
contrary hierarchies machines parr russell options sutton macros defined explicit internal state inside macro policies expressed macros markovian 
timeout stopping required macros semi markov 
attempt iterative improvement algorithm project begun conception macro learning algorithm different primary 
addition termination conditions macros conjunction serving set preconditions specified macro applied 
precup 
uses idea set states macro potentially applied 
conditions macro precondition post condition applied determine macro start complete 
conditions defined region state space macro act 
learn macros designed system iteratively improve conditions conditions improve behavior forth 
method choosing behavior similar described section essentially idea choose condition decision list minimize policy loss set states determined macro current conditions 
determining conditions somewhat different 
algorithm loop possible pairs conjunctions termination conjunction precondition conjunction score metric attempted describe characteristics macro 
tried different metrics conditions including connectedness region covered macro goodness existing behavior states defined conditions combinations metrics 
metrics tried offline testing 
macros produced system terribly useful worse iterative algorithm converge single macro 
attempt ascertain problems system tested giving correct conditions correct behavior 
system provided correct conditions learned reasonable decision list behavior 
system provided appropriate behavior failed learn appropriate region 
factors play role failure system including tradeoff generality adherence policy value function difficult express control 
learning methods different parts macro take account 
words condition finder take account behavior learner learn behavior region specified conditions 
david andre way express utility macro terms decision points removed problem environment 
attempting find regions simple behavior accurate respect value function system tiny regions ones states hardly resembled 
words attempting define exact region macro applied system trying decide value macro limit preconditions applied matched optimal policy 
task deciding action perform state really role reinforcement learning algorithm macro learner 
macro learning second system attempted address deficiencies approach 
motivations remove preconditions definition macro utilize explicit subgoals experience agent guide selection macros metric possible utility macro 
algorithm consists components 

build path percept assignment subgoal 
choose best percept assignment simple scoring metric path 
build constrained behavior achieve chosen subgoal start loop percept possible assignment percept 
percept assignment specifies set states world 
step consists calculating path states previous experience agent 
words states achieve subgoal 
precisely percept assignment simple dynamic programming calculate discounted likelihood reaching subgoal states current exploration policy 
choice exploration policy consistent design monte carlo simulation previous history agent environment estimate values 
process keep track minimum number actions required get subgoal state 
note exploration policy learn paths subgoal states consistent current value function 
control tradeoff generality adherence value function 
varying policy fully exploration random total exploration represent wide range compromises desirable characteristics macros 
macro learning online effect earlier learned macros general exploration policy presumably 
calculating percept assignment macro learning algorithm scores percept assignments 
score percept assignment shown 
score sum states minimum actions away goal likelihoods reaching subgoal current exploration policy 
approximates number states decision action take longer taken 
states action away subgoal roughly represents number states decision taken enter macro score represents roughly number values removed problem domain macro 
actual number slightly complicated calculate entrance states macro macros actions agent 
score computational resources calculate 
example consider possible subgoals shown 
relatively deep high probability path path lengths long 
second deadend shallower path 
additionally likelihoods second subgoal lower exploratory policies assuming single goal state transition probabilities entering deadend states despite moving high 
david andre choosing percept assignment subgoal algorithm builds constrained behavior achieve subgoal 
takes place building decision list best matches value function path computed path 
step process build set states path sufficiently high likelihood getting goal experiments 
algorithm builds set number decision rules choosing step rule action minimize policy loss states remaining path 
rule added behavior states action specified rule removed calculate policy loss feature assignment uses equation min expressed max shows example state partial environment policy loss calculated percept assignments default 
set number decision rules added action perform remaining states chosen 
method building macro greedily builds decision list attempts get agent subgoal states specified percept assignment 
decision tree information theoretic methods utilized choose states split 
results macro learning perform initial tests system offline version algorithm 
version system learns macros learned value function model domain 
macros incorporated 
left highlights subgoal states right highlights subgoal states deadend 
numbers represent values 

simple state action example computing policy loss 
david andre action set learning similar world different reward structure 
stage project run macro learning algorithm examined macros produces 
section describes macro learning procedure online setting 
tested macro learner environment shown simple world single start state dead lower right single goal state dead lower left 
feature set row col deadend percepts deadend 
meant algorithm choose subgoal set deadend deadend 
algorithm assigned high scores percepts correctly judged little value world 
due small asymmetry value function score slightly better algorithm chose build macro achieve subgoal 
learned macro shown col row deadend left things note learned macro 
system forced learn decision lists exactly rules macro unnecessarily complex 
clause default rule sufficient particular macro 
second rule especially problematic removes states easy fix simply require chosen feature eliminate number states additionally information theoretic stopping criteria fixed limit 
secondly worthwhile note macro exactly behavior want go top macro world 
system learned single macro learn choosing best percept assignment chosen learned macros 
case system learned go bottom macro 
tested system worlds similar structure shown key version world shown system tended create macros go top go bottom go left go right quite reliably 
clearly testing required different environments percepts utility macros learning 
integrating reinforcement learning section describes incorporate macro learning procedure model reinforcement learning algorithms 
precup 
parr russell utilized similar macro entities reinforcement learning algorithms presentation theirs 
assume attempting solve reasonably complex problem state problem shown 
key idea macro learning procedure executed occasionally reasonably sized portion search space explored 
note macros go top discovered exploration small amount state space macros probably quite useful exploring rest state space 
basic algorithm shown 
david andre done learning take step world 
update model transitions reward 
value update 
number value updates asynchronous value iteration update states dyna randomly choose states update prioritized sweeping update states priority 
steps macro learning 
build path unused percept assignment score path choose best build decision list chosen percept assignment issues arise attempting macros model rl algorithms 
algorithm keep track transition models macro actions include expected discount reaching particular successor states 
secondly executing macro algorithm keep track current discount accumulated discounted reward order update models discount transition reward macro completes 
system choose apply macro expected discounted reward words macro included list actions 
discussion algorithm learning hierarchical behaviors 
results preliminary current continues fronts 
applying algorithm variety different worlds different types percepts 
secondly testing macros generated offline version macro learner similar worlds attempting demonstrate macros speed learning compared primitive actions 
currently implementing online version algorithm plan test variety environments 
interesting investigate heuristics choose subgoals non navigational style problems 
preliminary raised interesting issues topics discussion research 
question initialize transition tables new macros 
suggestion initialize model take agent nearest state picked subgoal high probability distribute low probability nearby states subgoal set 
macros potentially modified created macro learner 
system keep track useful macro practice subgoal states involved high quality paths 

key world 
agent starts collect keys order able receive reward david andre set subgoal states involved macro utilized indicates better macro learned 
system build new path updated policy greedily choose new behavior specification 
precup 
suggested earlier intelligent quitting criteria macros 
example probability quitting function relative goodness values macro versus actions steps taken macro 
potential problem macros perfect sense accurately reflect optimal policy final optimal policy involve macros 
introduce explicit computational penalties having decision encourage macros 
similarly penalize macros slightly information aid determining actions need modification 
set learned macros useful examine macros underlying state space 
techniques parr thesis potentially global information set choice points induced set learned macros 
suggest techniques modifying macros improve abstraction 
intend combine approach state art methods exploration friedman bayesian exploration model representation andre generalized prioritized sweeping model learning friedman sem algorithm function approximation various methods bertsekas tsitsiklis 
intend attempt extend technique partially observable domains 
simple method learning constrained hierarchical behaviors 
representation macros system utilize agent experience world choose subgoals produce useful actions 
method run offline online allows localized tradeoff policy adherence generality 
algorithm succeeded learning reasonable macros simple world 
presenting preliminary results hoped step harnessing power hierarchy automatic learning behaviors 
sebastian thrun discussing helping visiting carnegie mellon fall 
greatly helped conversations nourbakhsh jerry feldman stuart russell ron parr nir friedman andrew mccallum astro teller andrew moore 
david supported national defense science engineering graduate fellowship 
bibliography andre friedman parr 
generalized prioritized sweeping 
advances neural information processing systems 
vol 
mit press cambridge ma 
benson nilsson 
reacting planning learning autonomous agent 
furukawa michie muggleton editors machine intelligence 
oxford university press oxford 
bertsekas tsitsiklis 
neuro dynamic programming 
athena scientific belmont mass bertsekas tsitsiklis 
parallel distributed computation 
prentice hall englewood cliffs 
dayan hinton 
feudal reinforcement learning 
advances neural information processing systems volume mit press cambridge ma 

dean lin 

decomposition techniques planning stochastic domains 
proceedings fourteenth international joint conference artificial intelligence 
morgan kaufmann 

dietterich 
hierarchical reinforcement learning maxq value function decomposition 
technical report computer science dept oregon state university 
fikes hart nilsson 
learning executing generalized robot plans 
artificial intelligence 
friedman 
bayesian structural em algorithm 
proceedings th conference uncertainty artificial intelligence uai 
david andre friedman dearden russell 
bayesian learning 
proceedings th national conference artificial intelligence aaai 
gordon 
sequential update bayesian network structure 
proc 
th conf 
uncertainty ai 
kaelbling 
hierarchical learning stochastic domains preliminary results 
proceedings tenth international conference machine learning icml 
morgan kaufmann san mateo ca 

kaelbling littman moore 
reinforcement learning survey 
journal artificial intelligence research 
laird rosenbloom newell 
chunking soar anatomy general learning mechanism 
machine learning 
lin 
reinforcement learning robots neural networks 
phd thesis 
computer science department carnegie mellon university 
mccallum 
reinforcement learning selective perception hidden state 
phd thesis 
university rochester 
mcgovern sutton fagg 
roles macro actions accelerating reinforcement learning 
grace hopper women computing 
moore 
part game algorithm variable resolution reinforcement learning dimensional spaces advances neural processing systems mit press cambridge ma 
moore atkeson 
prioritized sweeping reinforcement learning data time 
machine learning 
nilsson 
hierarchical robot planning execution system 
sri ai center technical note sri international menlo park ca 
nilsson 
reactive programs agent control 
journal artificial intelligence research 
parr 
hierarchical control learning markov decision processes phd thesis computer science division university california berkeley 
parr russell 
reinforcement learning hierarchies machines 
advances neural information processing systems 
vol 
mit press cambridge ma 
precup sutton 
multi time models temporally planning 
advances neural information processing system 
vol 
mit press cambridge ma 
precup sutton singh 
theoretical results reinforcement learning temporally options 
proceedings tenth european conference machine learning springer verlag 
precup sutton singh 
planning closed loop macro actions 
working notes aaai fall symposium model directed autonomous systems pp 

singh 
scaling reinforcement learning learning variable temporal resolution models 
proceedings ninth international conference machine learning icml 
morgan kaufmann san mateo ca 

singh 
transfer learning composing solutions elemental sequential tasks 
machine learning 
may sutton precup singh submitted 
mdps semi mdps learning planning representing knowledge multiple temporal scales 
jair 
sutton precup singh 
intra option learning temporally actions 
proceedings th international conference machine learning 
morgan kaufmann 
sutton 
integrated architectures learning planning reacting approximating dynamic programming 
machine learning proc 
th int 
conf 
thrun schwartz 
finding structure reinforcement learning 
advances neural information processing systems 
san mateo morgan kaufmann 
uther veloso 
generalizing adversarial reinforcement learning 
aaai fall symposium model directed autonomous systems 
watkins 
models delayed reinforcement learning 
phd thesis 
psych 
dept cambridge university 
schmidhuber 
hq learning 
adaptive behavior 

takahashi asada hosoda 

reasonable performance learning time real robot incremental state space segmentation 
proc 
ieee rsj international conference intelligent robots systems 

