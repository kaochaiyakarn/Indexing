biological metaphors design modular artificial neural networks master thesis herman kuiper departments computer science experimental theoretical psychology leiden university netherlands preface thesis result research done departments computer science experimental theoretical psychology leiden university netherlands 
part project architecture function modular neural networks done department experimental theoretical psychology bart research comprised parts extensive reading done get familiar areas neural networks genetic algorithms formal grammars 
secondly new methods developed design modular structures transformed software number experiments done 
thesis written manuals software developed 
wish bart stimulating input excellent suggestions got stuck 
wish ida guidance project 
numerous helped write hopefully thesis sun users computer science department put simulations 
leiden august herman kuiper sponsored part dutch foundation neural networks 
iv thesis method proposed modular artificial neural network structures automatically computer program 
number biological metaphors incorporated method 
argued modular artificial neural networks better performance non modular counterparts 
human brain seen modular neural network proposed search method natural process resulted brain genetic algorithms imitate evolution systems model kind recipes nature uses biological growth 
small number experiments done investigate possibilities method 
preliminary results show method find modular networks networks outperform standard solutions 
method looks promising experiments done limited draw general 
drawback large amount computing time needed evaluate quality population member chapter number possible improvements increase speed method number suggestions continue 
vi contents preface iii contents vii research goals neural networks genetic algorithms systems overview neural networks human brain artificial intelligence neuron neurons connected brain artificial neurons artificial neural networks backpropagation simple example exclusive selecting parameters backpropagation problems backpropagation modular backpropagation implementation viii contents genetic algorithms overview selection crossover inversion mutation building blocks implicit parallelism applications implementation systems biological development fractals simple systems bracketed systems context sensitive systems implementation search modularity modularity nature modularity brain modularity genetics modularity coded imitating evolution brain graph grammars recipes combination metaphors grammar coding network string example networks absolute pointer strings binary table strings relative skip strings production rules coding production rules example repair mechanism implementation environment extended new genetic functions chromosome grammar system backpropagation main program ix experiments exclusive xor tc problem handwritten digit recognition categorization mapping problem recommendations research derivation backpropagation entropy neural networks learning entropy reduction definition entropy addendum contents quote 
modularity 
astrophysics quantum mechanics see modularity 
biology growth development living organisms modular 
human brain probably highly modular 
modularity brain identified different levels physical functional 
thesis argued modelling brain artificial neural networks order create called intelligent software exploiting modular design principles result better networks 
shown techniques biological genetics growth search optimal neural network topologies result round search topologies variety problems 
research goals preliminary results show modularity designing artificial neural networks improve performance 
set forth chapter topology network large influence performance network far method exists determine optimal topology problem high complexity large networks 
human brain large scale neural network modularity idea reverse engineering genetic search development processes led brain 
genetic search nature resulted usage kind recipes blueprints describe development organism 
goal research develop method neural network topologies computer 
excellent results nature keep method biological plausible possible topologies thesis modularity defined subdivision identifiable parts purpose function 
high degree modularity genetic search recipes blueprints 
systems seen kind recipe able encode repeated patterns modules complex internal structure efficiently 
genetic algorithms simulate evolution systems treated chapters 
chapter provides thorough treatment modularity nature discusses usefulness principle research 
thesis divided parts part chapters introductions main disciplines research 
second part chapters gives thorough treatment ideas research software developed 
part chapters results number experiments reported recommendations research 
remainder chapter disciplines briefly introduced 
neural networks computers invented humans tried imitate human intelligent behaviour computer programs 
easy task computer program able different things order called intelligent 
meaning word intelligence somewhat unclear different definitions exist 
methods achieve artificial intelligence early days computers rule systems achieved results expected far possible construct set rules capable intelligence 
reverse engineering proved successful areas researchers trying model human brain computers 
main components brain neurons relatively easy describe impossible artificial brain imitates human brain detailed complexity 
large numbers neurons involved huge amount connections neurons 
large simplifications keep needed computing power realistic bounds 
artificial neural network consists number nodes connected 
nodes receives input nodes input calculates output propagated nodes 
number nodes designated input nodes receive input nodes number designated output nodes propagate output nodes 
input output nodes means network communicate outside world 
number ways train network order learn specific problem 
method research backpropagation supervised learning train network 
supervised learning called input output pairs repeatedly net 
pair specifies input value output network supposed produce input 
achieve internal representation results wanted input output behaviour input values propagated nodes 
difference resulting output desired output error calculated output nodes 
error values internal connections nodes adjusted 
process described detail chapter 
neural networks genetic algorithms darwinian evolution fit organisms stay alive reproduce non fit organisms modelled genetic algorithms 
population strings manipulated string seen chromosome consisting number genes 
genes code parameters problem solution 
string assigned fitness indicates string solution problem 
natural selection genetics chance reproduction organism genes depends ability survive fitness strings algorithm reproduce proportional fitness 
new generation created selecting recombining existing strings pay information fitness number genetic operators 
commonly operators selection crossover inversion mutation 
described detail chapter 
chapter thorough treatment provided basic mechanisms genetic search implemented genetic algorithm 
systems development living beings governed genes 
living cell contains genetic information genotype determines way final form organism develop phenotype 
genetic information blueprint final form seen recipe 
recipe followed organism cell individually 
shape behaviour cell depends genes information extracted turn depends genes read past influences environment neighbouring cells 
development solely governed local interactions elements obey global rules 
order model development plants biologist lindenmayer developed mathematical construct called systems 
called rewriting rules system string rewritten string rewriting characters string parallel characters 
application rewriting rule depends rules applied past neighbouring characters character rewritten 
chapters describe standard systems system research 
overview method resulted combination techniques described follows 
looking network structure able learn task genetic algorithm produce production rules system 
rules result network structure 
structure evaluated looking extent network learn task 
results fitness coupled original genetic coding production rules 
fitness enables genetic algorithm evolve set production rules generate optimal network structure 
neural networks human brain enormous fish 
flat see 
fail open messages transmitted lungs don reach brain 
simple 
python disease called cough animals complicated things known universe 
quote biologist richard dawkins complexity problems seen biology 
brain complex organ animal particularly humans 
discovering brain works able intelligent probably challenging task 
human brain human brain network huge number interconnected neurons 
neuron complex bio electrical bio chemical behaviour basic computational principles believed simple adds input performs threshold operation 
words neuron capable adjusting output relatively simple function input 
simple basic unit able generate complex behaviour 
main key answer cooperation interaction neurons 
relatively slow compared modern computers lot operate parallel 
combined effort large numbers neurons believed origin human intelligence 
intelligence means unknown 
far humans ones may able provide answer little doubt discover origin intelligence owe wonderful organ head 
interesting question human brain capable understand functioning 
del theorem suggests may ideas understood brain assuming brain described formal system 
see neural networks artificial intelligence artificial intelligence community long time trying imitate intelligent behaviour computer programs 
easy task computer program able different things order called intelligent 
caused lot confusion definition artificial intelligence webster dictionary example gives definitions artificial intelligence 
area study field computer science 
artificial intelligence concerned development computers able engage human thought processes learning reasoning self correction 

concept machines improved assume capabilities normally thought human intelligence learning adapting self correction 
extension human intelligence computers times past physical power extended mechanical tools 

restricted sense study techniques computers effectively improved programming techniques 
allan turing proposed test satisfied order speak artificial intelligence 
test known turing test turi person say placed room terminal connected sites 
terminals person situated computer 
asking questions determine terminals computer situated 
turing willing accept computer program intelligent fails 
course set test impossible decide measuring response time etcetera 
turing test disputed believe possible deceive having intelligent program 
traditional ways designing intelligent systems rule systems achieved results expected time people started realize computers just calculating numbers 
far possible construct set rules capable intelligence 
expert systems able compete narrow areas program capable functioning everyday situations 
problems encountered traditional methods urge researchers look approaches 
principle proved effective occasions reverse engineering looking works trying understand rebuild 
particular case means looking natural brain processing principles computer program 
neuron neurons nerve cells brain cells responsible thinking 
artificial neural networks way neurons important idea functioning 
neuron shown divided functional parts dendrites body axon 
dendrites information collectors neuron 
receive signals neurons transmit signals form electrical impulses cell body 
body collects impulses summed charge impulses exceeds certain threshold cell body activates axon 
axon transmits neuron activation electrical impulse dendrites cells 
enable brain 
neuron 
change internal processing learn influence neuron thought variable 
learning takes place junction axons dendrites 
axon splits number junctions called synapses 
synapse responds electrical impulse axon releasing certain neurotransmitters 
chemical substances reaching dendrite cause electrical activity collected body receiving cell 
changes process causes learning development 
neurons connected brain brain neurons 
clear quite impossible fully connect neuron neuron 
estimated full connectivity result head diameter huge amount wiring 
order reduce amount connections brain divided different modules levels 
clearest division brain division right left half function large extent independently 
fully connected connected relatively small amount connections structure called corpus callosum 
smaller scale brain divided number functional areas example visual area auditory area separated sensory motor areas 
areas relatively small number connections exist 
describes patient able name colour red fruit suffering head injury 
patient able name colour object including red objects 
red fruit answers random 
apparently specific connections areas fruit colour red recognized lost 
example suggests strong modularity brain smaller scale functional areas 
lot psychological physiological research indicates strong modularization brain see chapter 
despite highly specific structure brain lot connections neurons 
average neuron receives input synapses 
neural networks huge numbers neurons connections clear researchers able computer simulation brain amount computation needed huge 
estimate computing power brain 
axon able transmit pulse milliseconds synapses unable differences amplitude impulse axon seen cable transmitting bits second 
combined total number axons results roughly bits second 
estimation just data transmission brain amount computation staggering 
jacob schwartz estimates total amount arithmetic operations needed simulate brain detail high second needing bytes memory 
probably times fast fastest supercomputer available decade 
motivation lot research massive parallel computers 
artificial neurons consequence huge complexity human brain state current hardware technology impossible build artificial brain imitates natural brain detail 
order functional principles forced large simplifications regarding computations performed neurons connectivity 
artificial neurons take input real numbers act stim stim 
basic processing element 
determine output function input 
time total stimulation processing elements simply sum individual inputs multiplied corresponding weights 
bias term added order shift sum relative origin stim analogy real neuron obvious brain activation neuron transmitted axon arrives dendrites cells strength synapses determines extent cells stimulated 
real neurons give excitatory inhibitory signals synapses model weights node positive negative 
clear distinction stimulation processing element activation usually implemented function act stim implemented function stimulation previous activation act act stim digital simulation networks generally time considered measured discrete steps 
notation indicates timestep prior time artificial neurons chosen depending kind network basic functioning processing elements neural networks assumed kind artificial neuron implements basic computational principles biologic neuron 
artificial neural networks case brain artificial neural networks number interconnected processing elements 
reader may wonder advantages neural networks offer compared traditional methods 
largest problems traditional artificial intelligence techniques programmer supply knowledge incomplete unknown wrong 
human expert knows certain decisions example medical world incapable telling exactly grounds decisions 
new area research called knowledge engineering arisen purpose find methods acquiring knowledge 
happens knowledge collected described definite rules described statistical reasoning methods fuzzy logic 
difficulty finding rules solved neural networks told able learn 
neural networks capable autonomously discovering regularities extract knowledge examples complex task domains human experts 
neural network approach promising especially areas lacking absolute knowledge 
fact traditional methods rule systems newer methods fuzzy logic seen special cases neural networks 
possible construct logical nand function neural net proves computer done neural network combining nand networks complete computer built 
shows powerful artificial neural networks examples available showing strength neural networks neural network research just 
areas neural networks successfully exhaustive overview see handwritten character recognition image compression noise filtering balancing automobile autopilot nuclear power plant control loan application scoring speech processing medical diagnosis list far complete new applications appear day 
backpropagation lot different neural network paradigms backpropagation probably best known 
formalized werbos parker park neural networks rumelhart mcclelland 

typical backpropagation network 
multi layer feedforward network trained supervised learning 
standard backpropagation network consists layers input output hidden layer 
processing elements input output layer fully connected processing elements hidden layer shown fact feedforward means recurrent loops network 
output node returns node cycles allowed network 
standard backpropagation happen input processing element comes previous layer input layer course 
large simplification compared real brain brain appears contain recurrent loops 
supervised learning means network repeatedly input output pairs provided supervisor output network produce input input output pairs specify activation patterns input output layer 
network find internal representation results wanted input output behaviour 
achieve backpropagation uses phase propagate adapt cycle 
phase input network activation nodes processing elements input layer propagated hidden layer node sums input propagates calculated output layer 
nodes output layer calculate activations way nodes hidden layer 
second phase output network compared desired output supervisor output node error calculated 
error signals transmitted hidden layer node contribution total error calculated 
error signals received connection weights adapted node cause network converge state allows training patterns input output pairs encoded 
detailed description backpropagation networks reader referred appendix short description standard backpropagation algorithm network input hidden output nodes follows 

initialize weights network random values 
denote weights hidden layer output layer ij respectively 
notation stands weight input node ij ij hidden node 
choose input output pair thesis connections figures point upwards 
input nodes bottom output nodes top 
backpropagation assign corresponding input nodes 

propagate activation input activation node stimulation node including bias 
sigmoid function 
layer hidden layer calculate stimulation activation hidden nodes 
bias see node implemented extra node standard activation weights node nodes network adaptive thresholds 
activation nodes hidden layer stim ij sigmoid function shown 
propagate activation hidden nodes output layer 
ij 
calculate deltas errors output layer 
see appendix derivation 
compute deltas hidden layer ij 
adjust weights hidden layer output layer ij ij ad bdw ij dw ij ij ij term called momentum tends keep weight changes dw ij going direction averaging changes training cycles 
usually learning rate parameter chosen momentum parameter 

adjust weights input layer hidden layer neural networks ij ij ad bdw ij 
repeat steps total error network small training vector pairs training set 
needs said algorithm correspond process learning actual brain 
resulting network training assumed employ basic computational principles 
network learning rules backpropagation closely correspond actual learning process employed natural brain 
simple example exclusive give example small problem learning logic exclusive function 
problem historical interest perceptrons marvin minsky seymour papert mins showed possible network problem hidden layer learning rule known time 
trivial proof generalized expanded 
reason researchers time working neural networks book left impression neural networks proven dead 
logical xor function function binary variables 
possible network configuration 
shows solution xor problem 
radius circle proportional activation node 
problem shown 
network trained convergence occurred 
numbers connections gure correspond weights numbers nodes biases 
pairs randomly selected different possibilities booleans represented respectively 
values really important weights biases able scale translate 
just number important sigmoid function calculate activation output nodes keeps activations interval 
output nodes hidden nodes activation exactly require infinite negative positive input 
learning xor function network takes lot training input output pairs need learn simple example exclusive problem get stuck local optimum 
probability happen depends learning parameters range random initializations weights 
selecting parameters backpropagation paragraph suggested values initial random weights learning rate momentum parameter 
optimal settings parameters strongly depend task learned 
get feeling complexity backpropagation imagine landscape hills valleys 
position point surface corresponds weights network 
height point depends total error network coordinates weights point 
generally impossible show weight space looks reality unable see dimensions simple xor network dimensions weight bias corresponds dimension 
possible take dimensional hyperplane weight space 
gives example surface 
hyperplane dimensional weight space xor example network 
xor network 
axis corresponds weight input node hidden node axis corresponds weight second input node hidden node 
weights taken example solution changed 
just possible combinations weights 
created varying weights axes picture range axes measuring performance total error resulting network 
drawn upside best weight configuration corresponds highest point drawing 
note drawing just possible dimensional hyperplanes real weight space particular xor network 
network trained initial weights drawing looked quite different 
looking clear takes long time network finds solution 
explained appendix learning backpropagation gradient descent process 
change network ignoring momentum moment equals negative gradient position weight space surface multiplied compared standing hill step downhill direction size step depends slope surface position person stands 
slope small steps consequence small 
shows need momentum 
momentum tends add small steps small steps direction gradually increase momentum term increase speed convergence 
sigmoid function backpropagation algorithm steepest slope origin steps taken learning algorithm proportional derivative neural networks sigmoid usually best take small initial weights order start process learning place 
seen steepest slope origin 
seen large slope total stimulation node 
suggests better way determine advance range random initialization chosen 
loss generality suppose values range corresponds activation range nodes input layer 
suppose node input just node 
input 
prevent input node area sigmoid function small slope give weight input value 
node input say sure total stimulation stays random weights chosen smaller range 
simply divide range random weights number inputs sure total stimulation stay 
average standard deviation sum numbers divided times small standard deviation individual number range total stimulation average times small 
propose take initial weights node range range calculate random initial weights network reasonable initial weight setting matter size network may 
best choice learning rate parameter momentum parameter 
imaginary weight space 
usually optimal setting unknown 
optimal setting result fast correct convergence network 
get impression influence learning process take look 
hypothetical dimensional error surface upside 
suppose momentum 
ball representing current state network positioned rolls stay acquire momentum 
momentum ball roll just roll back start damped oscillation local minimum momentum term large ball overshoot fall global minimum momentum large ball may roll back momentum parameter setting ball keep moving forever 
compared moving friction 
setting learning rate parameter large prevent missing wanted minimum error surface large steps 
small setting drastically increase learning time 
gives impression influence settings xor example varying parameters 
shows average number iterations selecting parameters backpropagation needed converge function 
average amount training cycles needed train network function iterations needed 
problems backpropagation mentioned paragraph possible backpropagation get stuck local minimum 
local minimum performs slightly worse global minimum may problem usually local minimum solution 
shown momentum term able overcome problem cases 
reasons backpropagation network converge random initialization weights 
initial weight settings backpropagation may able reach global minimum weight space initializations network able reach 
training network reaches global minimum network may useful 
suppose example want network able recognize handwritten digits 
clearly impossible train network possible small set examples network 
set examples may perfectly learned network say network respond 
property able respond correctly input seen training called generalization 
generalization compared interpolation mathematics 
backpropagation usually generalizes quite 
reasons may generalize effect called overtraining 
happens small set examples total task domain trained long time 
network initially learns detect global features input consequence generalizes quite 
prolonged training network start recognize individual input output pair settling weights generally describe mapping cases 
happens network give exact answers training set longer able respond correctly input contained training set 
case better training network converged 
overtraining happen large networks easily memorize individual input output pair 
usual way overcome problem train smaller network learn training set able respond correctly input vectors 
method add noise input data 
sure input output pair different preventing memorization complete training set 
unfortunately backpropagation suitable extrapolation give answers input vectors outside domain training set 
example network trained give cosine input values able give correct answers outside range 
problem mentioned problem interference 
interference occurs network supposed learn unrelated problems time 
small network may network simply able learn problems 
network principle large learn problems time may happen 
different problems way problems represented weights network problem forgotten vice neural networks versa 
example interference classifications recognition position shape input pattern see 
conducted number simulations trained layer backpropagation network input nodes hidden nodes output nodes simultaneously process form place input pattern 
binary input patterns different positions input grid 
different combinations shape position 
network encode form place stimulus output layer 
appeared network learned faster mistakes tasks processed separated parts network total amount nodes stayed 
importance number hidden nodes allocated sub networks 
networks hidden nodes combined performance worse single network hidden nodes 
optimal performance obtained hidden nodes dedicated place network apparently complex task shape network 
needs emphasized tried explain form place processed separately brain 
actual experiment showed processing tasks un split hidden layer caused interference 
failed describe removing hidden layer completely connecting input output layer directly leads better network optimum hidden nodes separate sub networks 
analysis results revealed processing strongly interfered non split model 
non split model constrained develop particular way principle developed configuration weights split network connection represented zero weight chance happen small 
problems mentioned restricted backpropagation 
neural network paradigms suffer problems 
solution problems mentioned brain hinted sections modularity 
chapter return restrict showing modularity incorporated backpropagation 
modular backpropagation 
network hidden layers 
discussed simple backpropagation network topologies consisting input layer hidden layer output layer 
learning algorithm types network topologies long networks feedforward structure 
simple changes structure addition extra hidden layer 
network just hidden layer compute function network hidden layers compute exponential number hidden nodes node assigned possible input pattern see 
learning faster multiple hidden layers especially input highly nonlinear words hard separate series straight lines 
learning algorithm modified hidden layers simply take errors calculated layer output hidden calculate errors 
modular backpropagation 
example network separated hidden sub networks 
hidden layers node layer connected nodes nodes layer weights able node layer behave differently rest 
way accomplish different nodes see different things connect nodes layer differently 
full connectivity layers specific connections left 
idea adding hidden layers greatly extended possibility splitting layers sub layers reducing number weights 
take example shows example division network separate parts connections parts dimensionality number weights weight space reduced 
decreases amount computing may take away local minima increase speed convergence 
seen vertical organization layers horizontal organization possible 
order flexible kinds different network topologies define module 
modular network 
group unconnected nodes connected set nodes 
network modules network modules 
set weights modules grouped form connection 
loss generality possible feedforward network structures built components necessary modules consisting just node 
network easily visualized 
example depends structure 
xor network 

number training cycles needed xor versus network remember xor network shown 
showed possible solution trials network converge 
rumelhart mcclelland describe occasions network got stuck local minimum 
changing network topology network times tested learned xor problem 
learn problem faster simple network 
tested networks way training network times possible combinations parameters steps 
results simulation original network shown network additional weights input nodes directly output node 
original network neural networks needed average training cycles best combination new network needs training cycles average 
large fluctuations signify dependence initial weights original network conversely shows independence initial weights new network 
intuitive idea making different network topologies different problems explained follows 
may seen weight space way local minima disappear matter start training clear path starting point global minimum weight space 
network hidden layer nodes finds solution permutation nodes hidden layer incoming outgoing weights course result solution 
means training process starts solution 
possible solutions exist 
nodes input output weights permutations result network 
different solutions respect possible permutations 
frequently observed testing standard xor example 
networks hidden layers number possible permutations subsequent layers may multiplied total number permutations possible network hidden layers nodes ignoring similar nodes different solutions respect permutation gives strong indication modular networks may better generalization fully connected networks 
network example 
permutations network shown 
permutations largely reduces amount ambiguity eventual solution training 
notion number possible solutions generalized appendix major problem arises particular problem find network topology optimal problem means converge independent arbitrary settings initial weights converge fast possible show interference able generalize 
little theory design network topology particular problem 
usually people working backpropagation rules thumb take number hidden nodes equal average number nodes input output layer network converge take hidden nodes add hidden layer network generalize take hidden nodes 
method available order find modular network 
finding structured networks main subject research 
chapter return problem offer possible solution 
modular backpropagation implementation modular backpropagation network algorithm research implemented runs ms dos unix 
allows flexible modular networks 
library easily linked programs demonstration software games simulations research 
plans extend program network paradigms object oriented approach possible networks different kinds network paradigms combined 
neural networks genetic algorithms modern men descended worm creature shows people 
problems mentioned previous chapter problem finding topology able learn specific task 
appropriate computer program relieve difficult time consuming task 
traditional search methods suitable task vast number possible connections nodes little known topology better 
search methods developed handle multiple constraint problems 
genetic algorithms treated chapter 
genetic algorithms introduced john holland biological metaphor evolution 
book david goldberg gold describes genetic algorithms search algorithms mechanics natural selection natural genetics resulting search algorithm innovative human search 
overview goldberg mentions differences gas genetic algorithms traditional search algorithms 
gas coding parameter set parameters 

gas search population points single point 

gas pay objective function information derivatives auxiliary knowledge 

gas probabilistic transition rules deterministic rules 
parameters problem coded string usually binary features analogous chromosomes biology 
coding done user ga ga knowledge meaning coded string 
problem genetic algorithms parameter string contains multiple sub strings 
sample population strings 
population sorted necessary roulette wheel selection 
bitstring fitness genes parameters 
coded string represents possible solution problem 
ga works manipulating population possible coded solutions reproduction process driven number genetic operators 
reproduction process new solutions created selecting recombining existing solutions pay information called fitness genetic operators 
process compared natural selection darwinian theory evolution biology fit organisms stay alive reproduce non fit organisms 
fitness string solution evaluated different ways 
problem example finding root mathematical function fitness inverse square function value proposed solution 
problem finding optimal neural net fitness inverse convergence time zero network couldn learn problem 
inverse error output nodes 
ga aware meaning fitness value just value 
implies ga auxiliary knowledge problem 
shows sample population strings represents knowledge ga 
starting population random strings new population generated means reproduction replaces previous generation 
time lead higher fitness better solutions original problem 
commonly genetic operators selection crossover inversion mutation 
operators random number generating string copying changing bits involved 
crossover mutation inversion applied certain probability application operator decided apply operator 
selection usually ga genetic operators applied selected string 
follows explanation genetic operators 
thorough treatment provided basic mechanisms genetic search implemented ga selection 
roulette wheel sample population 
member chance selected 
selection choose strings population reproduction 
parallel natural selection mechanism strings solutions high fitness selected fit strings 
selection methods applied research described respectively goldberg gold whitley 
roulette wheel selection gold strings selected probability proportional fitness 
selection method called rank selection chance selected defined linear function rank individual population 
population remain sorted fitness method 
advantage rank selection need fitness scaling necessary methods prevent high fitness strings dominating population may result premature convergence nonoptimal solution 
crossover 
crossover members 
crossover operator creates new members population combining different parts selected parent strings 
number crossover points usually chosen random 
new string created alternate parts parent strings 
sample crossover crossover points shown 
inversion inversion operator reorders positions genes chromosome string 
inversion points chosen random genes inversion crossover result 
genes points swap places swapped second new member constructed concatenating reordered genes 
example shown strings genes containing bit 
inversion complicates ga somewhat keep track position gene chromosome 
prevent mixing different parameters crossover strings different gen order genes put temporarily order crossover applied 
newly created string inherits gen order parents 
mutation inverted string 
bit mutated 
mutation mutation possibly simplest genetic operators 
randomly flips bits string 
purpose string mutation improve algorithm introducing new solutions population protecting algorithm accidental loss valuable information due example unfortunate crossovers 
order keep algorithm simple random search mutation rate low doesn interfere crossover inversion 
applications selection mutation ga function 
genetic algorithms building blocks far may clear simple genetic operators combine powerful robust search method 
goldberg gold describes operation genetic algorithms remarkably straightforward 
start random population strings copy strings bias best mate partially swap substrings mutate occasional bit value measure 
principle structured information exchange crossover inversion leads exponential increase high fit pieces genetic information partial solutions combined produce solutions 
explain take look example 
want maximize function 
example members fitness 
bitstring fitness range take binary strings chromosomes length coded binary representation best solution course maximal largest possible strings shown population don know fitness function best way proceed look similarities highly fit strings wanted create new fit string 
appears having positions results high fitness 
idea put positions new string 
idea similar small parts highly fit strings create new string explained precisely concepts schemata building blocks 
schema introduced john holland template describing subset strings similarities certain string positions 
take example population binary strings schemata strings strings consisting symbols 
wild card don care symbol matches 
schema matches particular string position schema matches string matches string 
take strings length binary representations numbers schema matches strings 
take look number schemata involved population strings length string built symbols binary strings different schemata positions symbols asterisk 
example different strings different schemata 
string length belongs different schemata position may take actual value wild card symbol 
strings length number population size population contains schemata 
moderately sized populations contain lot information important similarities 
schemata genetic search amount information larger looking strings 
schema assigned fitness average fitness members population corresponding particular schema 
denote average fitness schema defining length distance non wild card 
looking defining length note crossover tendency cut building blocks schemata long defining length crossover points chosen uniformly random example schema higher chance cut vs 
lower bound crossover survival probability schema defining length expressed formula crossover crossover point probability crossover occur defining length schema length schema 
formula contains schema cut survive crossover results string contains schema 
new strings schema come existence 
calculate effect selection number schemata 
examples particular schema time population expect examples time population size average fitness strings representing schema total fitness population 
rewrite formula avg average fitness population avg particular schema grows ratio average fitness schema average fitness population 
schemata fitness values average population fitness higher chance reproduced receive increasing exponential number samples new population 
carried schema population parallel 
mutation small effect number schemata mutation rate usually chosen low combined effect selection crossover expressed formula result combining avg particular schema grows decays depending multiplication factor 
selection crossover factor depends schema fitness population average fitness length schema 
especially schemata high genetic algorithms fitness short defining length propagated exponentially population schema theorem gold 
short schemata called building blocks 
crossover directs genetic search finding building blocks partial solutions combines better solutions building block hypothesis gold 
inversion facilitates formation building blocks 
complex problems consist multiple parameters coded different genes chromosome 
multiple parameter problems complex relations may exist different parameters 
defining coding problem related genes positioned close 
known relations parameters inversion automatic reordering operator 
implicit parallelism exponential propagation high fit small size schemata building blocks goes parallel special bookkeeping memory population strings 
goldberg gold presents precise count schemata processed usefully generation number turns roughly function evaluations fitness calculations done generation feature called implicit parallelism apparently unique genetic algorithms 
applications genetic algorithms described variations active topic research 
applications paragraph mentioned 
application described detail example 
goldberg book gold contains table overview history genetic algorithms 
extract table shows diversity problems genetic algorithms applied 
table shown far complete new applications continuously 
year investigators description biology rosenberg goodman simulation evolution single celled organism populations ga adapts structures responding spatial temporal food availability computer science raghavan rendell ga clustering algorithm ga search game evaluation function applications year investigators description engineering goldberg davis smith goldberg smith optimization pipeline systems vlsi circuit lay ga blind knapsack problem simple ga aircraft landing weight optimization miscellaneous brady smith de jong travelling salesman problem genetic operators search image feature detectors ga calibration population migration model ga search travelling salesman problem tsp mentioned row hypothetical salesman complete tour number cities order minimizes total distance travelled 
return starting point city may visited 
may trivial problem np complete means currently solvable deterministic polynomial time 
research done gas order find near optimal solutions reasonable time 
experimenting gas tried solve tsp problem variation inversion operator find near optimal solutions time roughly proportional number cities 
described ga knowledge problem graph theory fitness member 
variation inversion swapping sub path path propagates earlier described building blocks population 
figures show best members subsequent populations simulation cities 
starting best member optimum 
total travelling length solution 
fitness best solution length path number recombinations far 
ga converged quickly length slowly optimum 

ga quickly converged solution 

path starting solution 
genetic algorithms 
optimum solution 

path recombinations 
number recombinations distance 
length best member number recombinations 
implementation goldberg book contains programs implement genetic algorithm pascal 
changes standard algorithm proposed 
program called genitor rank selection uses atime selection replacement new solution replaces worst member population new solution higher fitness 
best solutions stay population best fitness value increases monotonously static population model 
time replacement better creating new population possible evolve string fitness string fitness global maximum passing local maxima 
assured best string population lead local maximum 
department experimental theoretical psychology leiden university research took place library written create manipulate populations binary strings goldberg whitley selection replacement happ 
functions convenient purposes changes improvements introduced original library see chapter 
systems ignorant man exceptional wise man common greatest wonder regularity nature 
george dana boardman third main real world model research biological example development growth living organisms 
biological development development living organisms governed genes 
living cell contains genetic information genotype determines way final form organism develop phenotype 
genetic information blueprint final form seen recipe 
example excellent book blind watchmaker richard dawkins describes recipe followed organism cell individually 
shape behaviour cell depend genes information extracted 
turn depends genes read past influences environment neighbouring cells 
development solely governed local interactions elements obey global rules 
principle lies basis mathematical system called fractals 
fractals fractals popular benoit mandelbrot book fractal geometry nature mand 
describing oldest examples fractal koch graph take look experiment lewis richardson 
tried measure length perimeter west coast england results depended strongly scale map 
repeating experiment just map details decreasing unit measure time find decrease length coast increase 
systems implies 
oldest examples fractal koch graph 
west coast england infinite length 
length useful method describing measure better 
mandelbrot called number fractal dimension fractional dimension 
call unit measure need times approximation length line calculating fractal dimension measured length na fractal dimension defined lim logn log koch graph example curve infinite length 
proposed von koch koch mathematicians curve part small infinite length 
mandelbrot construction follows begins shapes initiator generator 
initiator generator generator rewritten 
steps generating koch graph 
oriented broken line equal sides length stage construction begins broken line consists replacing straight interval copy generator reduced displaced points interval displaced 
resulting snowflake curve shown classical example graphical object defined terms simple rewriting rule recursively applied element 
seen fractal selfsimilar 
simple systems special class fractals called systems introduced lindenmayer lind attempt model biological growth plants 
system parallel string rewriting mechanism kind grammar 
grammar consists starting string set production rules 
starting string known axiom rewritten applying production rules production rule describes certain character string characters rewritten characters 
grammars production rules applied sequentially system characters string rewritten parallel form new string 
attach specific meaning logo style turtle characters string able visualize string 
koch graph example described system simple systems axiom production rule 
characters thought directions turtle 
associate draw line take left turn take right turn turtle looks symbols string time symbol moves draws line turns 
angle variable denoted shows axiom shows right hand side part production rule 
left hand side production rule part describes sub string replaced right hand side describes string replaced 
step axiom rewritten 
second step new replaced parallel resulting 
side represented ff ff ff ff ff ff ff fff 

side represented ff ff ff ff ff ff ff fff 
rewritten underlined 
production rule character character replaced 
shows string created axiom rewriting steps 
symbols 
example symbol tells turtle move drawing line 
difference shown figures 
bracketed systems 
steps axiom fx ff turtle symbols described previous paragraph called single line drawings contrast branching seen real life plants 
order give turtle freedom move new symbols introduced remember current position direction turtle 
restore stored position direction 
new symbols realistic drawings obtained shown figures 
systems chapter book algorithmic beauty plants prusinkiewicz lindenmayer 
symbols ignored drawing string 
context sensitive systems final extension systems called context needed model information exchange neighbouring cells described paragraph 
context leads natural looking systems plants 
context left right certain sub string 

rewriting steps axiom production rule context sensitive system form called predecessor successor earlier called left hand side right hand side production rule 
left right context respectively may absent 
technically system context called system 
production rules sided context context called system system production rules sided context 
production rule left right context replace preceded followed production rules apply certain character context context 
example production rules 
production rules context 
string abc rewritten xyz production rules string abc rewritten 
rewritten preceded rewriting try rewrite xyz rules string remains xyz production rules apply try rewrite string rules rewritten xyz preceded plant created 
rewriting steps axiom production rules 
production rules 
production rules left right context 
note turtle command rewritten create new production rules interaction different parts plant 
context matching geometric symbols ignored 
drawing string ignored 
production rules constructed hogeweg patterns generated bracketed systems 
implementation prusinkiewicz james hanan small system program macintosh 
order experiment systems ported source code pc 
fixing irregularities program implementation rewritten extensively order accept rigid input 
production rules 
files 
features added probabilistic production rules production rule ranges 
probabilistic production rules production rule fixed probability 
rewriting string rules selected random proportional probability 
results natural looking plants recognizable belonging family 
shows plants created set probabilistic production rules 
production rule ranges introduce temporal aspect system tell rules looked certain rewriting step 
example generate leafs flowers 
chapter method set forth combines gas chapter systems order obtain efficient search neural network topologies 

plants created rules probability 
systems search modularity nature gets credit truth reserved rose scent song sun radiance 
entirely mistaken 
address lyrics turn odes self excellence human mind 
alfred north whitehead chapter overview principle modularity seen nature 
explained modularity incorporated neural networks greatly improve performance 
conclude method find modular structures method modular principles 
modularity nature modularity nature possible scales living organisms dead objects 
broad definition principle modularity may subdivision identifiable parts purpose function 
course applies exactly point 
modular 
smallest possible scale quantum physics principle modularity sets 
elementary particles seen modules 
particles longer divided smaller particles lowest level modularity 
larger scale modules elementary particles form progressively larger entities atoms molecules solids fluids celestial bodies star systems galaxies clusters super clusters universe 
scales clear subdivision 
domain physics astrophysics modularity may biology modular principles 
kind progressive subdivision paragraph possible 
possible classes living organisms atoms molecules solids fluids 
mammals example subdivided cells subdivided specialized parts cell positioned nucleus cytoplasm primary living matter cell including nucleus 
cells search modularity grouped form organs turn form organism 
organism organ specific functions 
organ divided parts different functions parts cell tasks 
examples suggest recursive modularity inside modularity 
kinds modularity identified iterative modularity differentiating modularity 
iterative modularity refers kind module times differentiating modularity refers organization differing modules 
self similarity seen fractals seen recursive iterative modularity modules iterating 
modularity brain importance modularity clear looking brain 
brain shows remarkable modular structure 
modularity described levels 
lowest level course subdivision neurons 
larger scales strong modular structure see chapter 
argued anatomically brain highly specific modular organization strong functional implications cognitive information processing learning 
prominent structural characteristics brain referred horizontal structure vertical structure 
horizontal structure processing brain carried subsequent hierarchical neuron layers 
multi stage information processing example identified primary visual system simple features visual images lines arcs represented layers simple neurons combined represented neurons subsequent layers possess increasingly complex representational properties 
apart horizontal structure layered structure exist levels primate brain multiple parallel processing pathways constitute vertical structuring 
vertical structure allows separate processing different kinds information 
example visual system different aspects visual stimuli form colour motion place processed parallel anatomically separate neural systems organized cellular cellular pathways 
convergent structures integrate separately processed visual information higher hierarchical levels produce unitary percept 
presence horizontal vertical structure leads modular organization brain 
subdivision brain hemispheres mentioned chapter illustrates anatomical modularity large scale 
functionally division paralleled specialization 
groups mental functions allocated different halves brain 
split brain patients connection hemispheres corpus callosum cut live normal life showing hemispheres function large extend independently 
hemisphere individual functions organized anatomically separate regions 
analysis behavioral functions indicate complex functions brain localized extent 
example parts paragraph adapted happ 
noted ideas paragraph universally accepted researchers 
modularity brain studies localized brain damage reveal isolated mental abilities lost result local lesions leaving abilities 
warrington described subject severe impairment arithmetic skills accompanied deficit cognitive abilities 
different types aphasia language disorders indicate different functions separately localized brain 
patients aphasia able understand written spoken language speak write 
aphasia shows reverse symptoms 
ability patients understand language hardly able produce language 
important functional advantage anatomical separation different functions minimization mutual interference simultaneous processing execution different tasks needed perform complex skills example walking forest driving car 
psychology interference studies multiple task execution indicate tasks easily performed parallel strongly interfere 
tasks sufficiently dissimilar executed simultaneously interference 
striking result reported allport reynolds demonstrated subjects sight read music perform auditory shadowing task concurrently interference 
simultaneous execution similar tasks presentation auditory visual messages causes interference 
difference performance tasks explained assuming modular organization brain 
tasks processed separate modules interfere 
tasks require simultaneous processing single modules harder execute parallel 
describing modular organization cognitive functions brain went large scale grouping functions separate hemispheres modular organization individual functions 
evidence indicates individual functions split functionally different subprocesses subtasks localized anatomically separate regions 
field psychology example word matching experiments show human information processing system uses number subsequent levels encoding performing lexical tasks mars 
experiment marshall newcombe mars subjects decide simultaneously words belong category 
reaction times visually identical words table table shorter visually different words table table 
longer reaction times words belong different category table dog 
posner lewis conrad explain data follows visually identical words compared level visual encoding words 
matching visually different words take place phonological code formed 
different category words compared additional semantic code formed 
studies indicate lexical tasks separate visual phonological semantic encoding analysis words involved 
pet scan positron emission tomography studies show functionally distinguished subprocesses separately localized brain 
local changes nerve cell metabolism corresponding changes neuronal activity registered subjects execution different lexical tasks 
experiments revealed visual encoding words takes place mainly occipital lobe posterior part brain 
phonological codes formed left temporal parietal cortex 
semantic encoding takes place lateral left frontal lobe 
important argument derived studies nature information processing brain modular 
individual functions broken subprocesses executed separate modules mutual interference 
modular architecture brain form necessary neural substrate independent processing different search modularity tasks subtasks 
speculated subdivision modules smaller modules functions sub functions go extreme depth 
example modules containing neurons known proposed basic functional anatomical modular units cerebral cortex 
modules thought cooperate execution cortical functions 
question may raised moment development brain modularity arises 
important argument supporting view part structure way anatomical location different functional regions brain 
location functions situated place individuals see 
argument support view stages development language children independent language learned 
furthermore basic grammar rules world 
clearly global structure brain brain birth consequence genetically coded way 
modular coding genes capacity store specific connections 
modularity genetics genetic coding life forms earth modular 
genetic information 
genetic code rna 
phe phe leu leu ser ser ser ser tyr tyr trp leu leu leu leu pro pro pro pro arg arg arg arg ile ile ile met thr thr thr thr asn asn ser ser arg arg val val val val ala ala ala ala asp asp glu glu gly gly gly gly stored genes contained long double stranded helical strings dna cell organism copy genetic information digital coding different bases adenine 
information stored dna helix transcribed strings rna inverse copy part dna 
rna built bases dna replaced 
triplet bases rna forms coding amino acids marker see marker 
rna translated proteins ribosome reads rna connects amino acids order coded rna 
markers tell start reading rna string 
amino acids building blocks proteins built 
proteins enzymes different chemical reactions cells 
protein consists large number amino acids put specific order 
order determines shape life forms containing rna 
modularity genetics shape functioning protein 
protein exist humans order amino acids written dna protein coded gene 
way dna determines kind proteins built cell operate 
supposed growth embryo process cell differentiation takes place caused forming certain proteins generate positive feedback genes dna produced proteins 
proteins group genes positive feedback started group genes active 
experiments show certain cells embryo control differentiation adjacent cells implementing idea context 
mature cells humans produce proteins total amount 
process cell differentiation determines final shape organism detail 
argued previous paragraph brain supposed initial structure birth 
process cell differentiation forms shape brain way unknown 
initial structure coded genes genetic modules information building blocks 
modularity coded addition mentioned cortex apparently contains higher level form modular organization 
called macro modules consist aggregation forming larger processing module 
cerebral cortex envisaged mosaic columnar units remarkable similar internal structure surprisingly little variation diameter 
kind iterative modularity seen nature leaves tree lungs scales fish rods cones eye brain hairs skin ants colony 
suggests iterative modularity common principle nature 
process evolution time profitable kinds species duplicate invented 
example middle part body snake composed number segments 
segment consists vertebra set nerves set blood vessels set muscles etcetera 
individual segment complex structure exactly similar segments 
order add new segments done relatively simple process duplication 
genetic code building segment great complexity result long gradual evolutionary search 
genetic code building segment new identical segments may easily added single mutational step 
gene inserting extra segments may read simply 
snakes fossil ancestors living relatives 
sure evolution snakes numbers changed numbers 
imagine snake 
example search modularity shows complex partial architectures read modules encoded evolution repeatedly produce system 
apparently genetics small scale modules effect generating modular structures larger scales 
may drawn iterative nature translation genes organisms genotype phenotype important genetic code describe exactly final form organism describes number rules followed result final form 
words blueprint genes contain kind recipe 
means correspondence part dna part actual organism 
allows genetic search utilize discovered principles repeatedly 
resulting organisms recipes built combined evolution 
rules proteins able forming fit organism survive 
probably happened case human brain 
human brain result evolutionary process brain predecessors larger larger 
human brain compared brain example cat macaque monkey difference size case human significantly larger particular largest part brain consisting left right hemisphere responsible higher cognitive functions 
size course corrected size animal 
brain elephant larger example 
see measure brain sizes quotient 
cortex contains mentioned high amount repeating modules 
thing necessary case segments snake genes saying please 
exactly coding structure organisms done nature unknown active area research 
imitating evolution brain modularity important principle nature argued large extent brain 
question modularity construct artificial neural networks result better performances 
preliminary results show may case soll happ far results available concerning large artificial neural networks modular structure 
problem operation neural networks complex known algorithm backpropagation difficult impossible understand exactly happens inside particularly hidden layers 
difficult recurrent networks networks feedforward restriction backpropagation algorithm see chapter 
behaviour networks described large number coupled differential equations solution exists practice 
large consequences design neural networks 
possible calculate advance optimal topology network specific task 
way determining quality network specific task testing 
description behaviour network simulation 
large scale neural network functions modularity brain 
chapter explained reverse engineering brain led invention artificial neural networks reverse engineering find method able find modular structures artificial neural networks 
process imitating evolution brain invented brain course evolution 
millions years process evolution gradually resulted increased complexity aggregation information processing cells brain 
reverse engineering process logically leads genetic algorithm described chapter simulation evolution 
genetic algorithms relieves problem mentioned impossibility describing network performs better 
genetic algorithms care solution better able information 
drive reproductive functioning fitness members population 
fitness easily calculated 
just generate neural network represented members population test 
ways genetic algorithm find neural network solutions 
main possibilities genetic algorithm find weights network structure result smallest error 
method genetic algorithm learning algorithm backpropagation 
genes algorithm correspondence weights network 
slight variation method genetic algorithm find set reasonably weights leaving fine tuning learning algorithm see example 
genetic algorithm find structure network 
method genetic algorithm tries find optimal structure network weights structure 
genes genetic algorithm contain coding topology network specifying connections 
weights network trained usual see example dodd 
course combinations methods possible coding presence connections weights genes 
shown xor example chapter input output layer directly connected topology network converge training weights better done learning algorithm genetic algorithm 
done finding network topologies genetic algorithms kind blueprint method coding topology genes members population harp mari 
nature code blueprints research project trying construct method enables coding recipes genes started happ 
graph grammars recipes neural network seen collection nodes edges graph 
needed code neural network structures method graph generation 
formal languages describing graphs rewrite nodes rewrite edges perform operations adjacency matrices logical choice research biology 
describe form growth plants biologist lindenmayer mathematical construct called systems see chapter describe development multicellular organisms 
method takes genetic physiological observations search modularity account purely morphological ones lind 
biological metaphor research systems recipes obvious choice original idea coming happ 
systems offer possibility describing highly modular structures 
describing giving recipe iterative differentiating modularity describe fractals 
graph grammars shortly investigated lack flexibility offered systems 
major advantages systems usual graph grammars ease including context enabling analogy cell differentiation growth neural network 
systems specifically designed describe graphs 
just string rewriting method 
actual meaning resulting strings depends interpretation symbols 
chapter explained symbol interpretation production rules coded genetically 
combination metaphors research tried combine methods origin biology genetic algorithms 
global structure 
systems neural networks goal design method searches automatically optimal modular neural network architectures 
chapter argued concept modularity frequently nature results solutions 
working hypothesis research making modular techniques biological metaphors search method network structure eventual performance method finding optimal neural network structures better methods 
method summarized follows 
genetic algorithm generates bit string chromosome member population 
search genetic algorithm directed members high fitness measure resulting step 
system implements growth neural network results recipe coded chromosome 
chromosome decoded transformed set production rules 
applied axiom number iterations resulting string transformed structural specification network 

neural network simulator research backpropagation trains resulting network structure specified problem 
resulting error transformed measure fitness low error results high fitness 
fitness returned genetic algorithm 
method combines theory explained chapters 
chapter describes detail transformation genotype phenotype 
grammar coding worst come 
johnson chapter describes number system grammars define network structures 
system possible codings looked described 
paragraph coding rules chromosomes described 
network string described chapter ga manipulate population sets production rules 
member population binary string consisting production rules system 
determine fitness string production rules extracted string system rewrites axiom rules 
resulting string interpreted network trained case backpropagation 
backpropagation restricts possible topologies feedforward networks nodes indexed way connections ideas represent node network letter alphabet extra symbols digits implement modularity connections 
variation coding implicit connections neighbouring letters extra symbols special connections 
comma introduced note neighbouring letters connected 
letters grouped symbols 
pair brackets associated number level connections groups number 
strings called absolute pointer strings described 
idea restricted alphabet representing nodes network various levels 
connection table forward connections specific combinations 
modules introduced placing nodes different levels string 
nodes grammar coding module connected nodes level connection table 
resulting strings called binary table strings described detail 
strings described ones research called relative skip strings 
don numbered brackets brackets digits indicate relative jump string order specific connection 
variations possible neighbouring characters connected default comma denote opposite neighbouring characters connected default minus symbol indicate connection neighbouring characters 
example networks paragraph presents number connection structures evaluate string notations 
structures tried find string representation order test representational potential grammar 
basic networks 
basic elements 
shows basic elements evaluate strings 
simple elemental structures combine elements introducing modularity exploited grammar 
xor 
possible networks solving xor problem 
shows networks able solve xor problem exclusive see chapter 
extra connection results better performance complicated strings needed code 
absolute pointer strings absolute pointer strings nodes network represented characters alphabet 
character nodes characters specific contexts 
characters placed nodes represent said form module referred group nodes parts string 
pair index associated denoted 
appending digit pointer node module connection coded node module modules index corresponds pointer 
connections neighbouring nodes modules automatically separated comma 
feedforward connections connecting nodes modules nodes modules right string 
absolute pointer strings basic networks basic elements shown easy represent grammar described 
represented single character example characters example aa ab 
possible string encoding network connected third second 
better smaller solution fourth example difficult simplest representations 
xor networks xor network simple extension network shown convenient able string representing creating string representing 
modules fully connected easily written extra connection just pair brackets digit added strings able model feedforward networks single node put numbered brackets digits connections 
considerations decide grammar described form looking simple strings production rules 
system able generate numbers brackets automatically 
production rules able rewrite specific module referred pointer modules specific content pointers matched sort wild card requires changes system 
removing pair brackets production rule requires corresponding pointers removed 
done production rule requires changes system 
time experimenting grammar looked binary table strings binary alphabet 
binary table strings binary table strings nodes represented 
bit rewritten brackets automatically placed production rules 
nodes brackets said different level 
brackets indexed depending character originated 
connections depending connection table 
values characters indexes connection table character connected characters right ensuring feedforward connection connect 
column contains 
string connection table connected 
connected second 
rest paragraph connection table shown 
grammar coding context plays important role single bits 
sample connection table connect 
visualization binary string 
predecessors depending context rewritten successor string surrounded brackets indexed original character 
prod string rewritten step 
string visualized network modules highlighted rectangles 
bottom rectangle represents sub string top rectangle sub string 
fifth node represents single 
shows connections modules 
decided nodes modules may connected looking outermost level nodes brackets numbers outermost brackets 
example module number may connected module number 
single may connected module 
determine nodes connected nodes module look level 
example module compared module 
third column second node connected nodes module 
brackets connected module lowest level brackets 
applies nodes modules levels compare remaining nodes modules fully connected 
process comparing numbers connection table repeated remaining levels modules 
basic networks represented 
third element example written 
fourth example difficult need multiple levels code network 
simplest possibilities order reconstruct network string note module may connected second third module numbers brackets second third 
determine nodes connected second third module compared nodes modules connection table 
nodes second third module connected 
binary table strings xor networks xor network easy 
connection table represent 
connect 
able find string representing connection table 
connection table shown solution difficult tried table 
solution simple network tried shown 
coding 
network input nodes output node hidden layer nodes 
easy table table resulted hidden layer underlined 
decide type grammar research doubts possibility code possible feedforward networks binary table strings 
production rules contain 
nand style connection table 
connect 
brackets inserted automatically complete modules including brackets rewritten smaller modules 
time writing discovered method code feedforward networks table shown 
problem rewriting modules remains 
type strings tried returned normal alphabet ideas absolute pointer strings 
relative skip strings strings research characters alphabet 
node network represented letter alphabet 
adjoining letters automatically connected feedforward 
letters separated comma connection 
modules created grouping nodes modules square brackets 
contrast multilevel strings brackets don indexes attached 
adjoining modules fully connected 
output nodes module connected input nodes second module 
input node node receives input module 
output node output nodes module 
specific connecting modules facilitates combining independent networks larger network 
single characters comma prevent connection adjoining modules 
grammar coding order specific connections nodes modules side side string single digits denote skip string 
digit encountered interpreted skip nodes modules right connection node module 
string bc connected skips comma 
bc bc connected modules count skipping bc connected 
string visualized 
skip contained module goes closing bracket skip continued 
string connected input nodes module comma connected 
connected skips connects outside module containing 
network coded string shown 
basic networks connection method described coding networks easy letter examples aa output nodes aa xor networks xor networks difficult note similarity strings network hidden nodes written seen string order generate layers internally unconnected nodes modules production rules generated insert commas string 
wanted modular networks unconnected blocks nodes develop easily possible tried variation connect neighbouring nodes minus symbol zero skip needs explicitly specified 
strings encoding aa aa aa respectively 
xor networks written aa aa aa aa 
network written aa aaaa 
relative skip strings production rules system generating strings described previous paragraph system production rule left right context 
production rules format described chapter parts production rule arbitrary strings alphabet 
restrictions apply different parts predecessor predecessor may contain complete modules nodes 
number left right brackets equal correct order 
module complete string example discarded bc predecessor may contain empty modules 
predecessor contain letter node 
successor successor constraints predecessor 
successor absent case predecessor removed string applying production rule 
contexts context constraints successor predecessor apply 
addition loose digits allowed digit follow node module 
example string allowed leading 
deviation systems described chapter handling context 
systems chapter context matched characters left right predecessor system context compared coding characters nodes nodes predecessor receive input left context nodes connected output nodes predecessor right context 
special interpretation context context string seen enumeration number nodes modules connected predecessor time context matching 
context subset nodes connected predecessor left context nodes predecessor connected right context order match 
example look production rules 
sample production rules 
bbb shown 
take axiom rewriting process described follows axiom 
rewriting step string bbb 
grammar coding second rewriting step 
rewriting rules visualized 
rewritten rule connected second 
second rewritten rule 
third rewritten rule third connected connection 
results string shown 
rewriting goes parallel strings figures created separately directly rewritten 
figures clarity 
third rewriting step rewritten rule connected second second rewritten rule connected second rules apply final string shown 
just clarity reality rewritten parallel 
coding production rules described chapter genetic algorithms coded parameters problem 
research parameters represent set production rules 
paragraph describes coding production rules binary strings ga production rule consists possibly empty parts 
parts string alphabet 
particular alphabet implies different letters number possible symbols nodes 
goes number skip symbols 
choose arbitrary different letters different skips 
brings total numbers characters alphabet 
ways code rule decided represent character rule fixed length binary string 
binary string length code different characters 
binary codes assigned characters alphabet 
number codes determined distribution chosen 
length distribution choose loosely biological genetic code uses character code see inset 
different bases genetic code coded binary digits triplet bases translation replaced binary string length 
possible strings length distributed symbols 
symbols include characters alphabet plus special symbol asterisk separate constituent parts production rules context predecessor successor chromosome 
asterisks compared start markers transformation rna protein 
coding production rules resulting table shown 
conversion table 
short genetic code adapted textbook medical physiology 
controlling development final form biological organism genes control reproduction day day function cells 
gene nucleic acid called acid dna automatically controls formation nucleic acid acid rna spreads cell controls formation specific proteins 
proteins control function cells 
genes contained large numbers attached long double stranded helical molecules dna 
strands helix held kinds bases names represented letter alphabet hydrogen bonding 
hydrogen bonding loose strands easily split 
bases strand exposed 
exposed bases form key called genetic code 
research studies past years demonstrated genetic code consists successive triplets bases successive bases code word 
words control sequence amino acids protein molecule synthesis cell 
bases read directions reading start position positions dna located nucleus cell functions cell carried cytoplasm surrounds nucleus information dna transcribed rna transports cytoplasm 
base triplets dna transcribed base triplets rna 
rna contains bases abbreviated 
base triples rna translated amino acids start protein building protein building signal 
conversion table called genetic code 
inal genetic code table chapter 
table read follows determine character corresponding bitstring determine rows left corresponds bits string 
choose column middle bits choose row right bits 
example character corresponding table 
way character bitstring easy bitstring concatenation bits left bits bits right character 
bit string character 
mentioned extra character asterisk separate parts single production 
rules followed order extract production rule chromosome 
pick starting point bitstring chromosome 
start reading part production rule 

reading bits time 
look character corresponding long bitstring table 
grammar coding 
character asterisk add character string read far 

assign string read far empty current part production rule 
advance part production rule 
part read start reading production rule 

repeat steps bits chromosome read 
bits complete current production rule discard production rule 
real dna strands read choosing starting point bases strand reading triplets bases punctuation mark read compared asterisk chromosomes time building current protein finished building new protein starts 
research indicates different proteins coded genetic information 
different proteins build adjusting starting point reading just base compared starting point proteins bases needed code word 
genetic information seen number overlapping recipes different proteins 
order chromosomes contain kind overlapping information algorithm start bit position 
result chromosome read twelve times rules starting point bits respectively reading string forwards rules starting point bits respectively reading string backwards 
high level information contained chromosomes level implicit parallelism may eventually significantly higher genetic algorithm strings read 
example small example shown research chromosomes length longer 
example bitstring shown 
extract chromosome translations 
translations 
string contains translations shown reasons clarity 
translations shown account direction reading bbb strings forms complete production rule imagine third string part longer string example bbb example bold part third string underlined part seen complete production rule contains asterisks 
write production usual notation get bbb repair mechanism chromosomes length results characters means characters total number bits 
chromosomes bits long results string characters 
total number possible valid production rules strings length usually character translation table contains asterisks string characters contains roughly markers 
marker starting point production rule markers placed strings 
discard production rules contain invalid character combinations described earlier left production rules 
real dna hours dna replication mitosis actual process cell splitting new cells period active repair proofreading strands starts 
special enzymes cut defective areas replace appropriate nucleotides 
mistakes process cell replication happen called mutation 
developed software contains functions repair faulty strings 
functions remove spare brackets commas digits order string meet restrictions previous paragraph 
functions described chapter 
number usable production rules repair process usually 
grammar coding implementation hofstadter law takes longer expect take account hofstadter law 
douglas hofstadter software written research consists number separate programs main module controls population production rules sub programs 
chr gram translates member population set production rules 
production rules rewritten second program lsystem adjacency matrix network output 
network represented matrix trained backpropagation backprop 
chapter describes software 
programs written backpropagation module written 
elements described detail parts just briefly mentioned 
environment software written run ms dos computers unix machines 
research pc disposal 
software finished started porting sun network department computer science leiden university 
network consisting sparc sun elc lpc ipx models provided parallel computing power needed larger simulations 
murphy law liked network testing simulations network regularly 
due internal bad full hard disks system operator network items resolved time writing 
final version software easily ported computers example network transputers languages available 
implementation extended library mentioned chapter rewritten chromosomes length longer bits 

overview main datastructures 
new version called extended includes functions manipulate population disk memory long chromosomes total population size bytes large 
main datastructures functions library described shortly 
functions specific research described detail 
datastructures population described population structure typedef struct contains population info unsigned popsize nr members population nr genes member size gene bits multiple member member population pointer member array member structures typedef struct contains information member float fitness fitness member unsigned pointer array byte pointer array chromosomes member member structure contains pointers array inversion array bytes containing actual chromosome 
order keep source simple possible chromosomes length multiple number bits byte allowed 
population saved disk file starts header typedef struct structure header disk unsigned long generation generation number parameter save unsigned popsize extended member fitness position array bitstring saved 
functions library divided modules module contains functions specific task 
modules described functions 
initialization module module contains functions allocating memory population just member useful memory short 
population unsigned popsize unsigned unsigned boolean initialize member unsigned unsigned initialize parameter indicate bit strings randomly initialized cleared zero 
depending functions allocate datastructures functions free allocated memory void population void member unsigned storing retrieving members functions available save load populations disk name file pointer 
int population file fp unsigned long generation int population char filename unsigned long generation int population file fp unsigned long generation int population char filename unsigned long generation generation parameter stored retrieved file kind progress indicator 
memory short members manipulated directly disk functions int member unsigned file fp unsigned unsigned int member unsigned file fp unsigned unsigned parameter specifies member save load 
implementation selection replacement selection methods described chapter roulette wheel rank available functions unsigned select population unsigned population double pressure pressure parameter indicates larger probability highest ranked member selected member middle population 
pressure top ranking member population members twice large probability selected number 
function proposed whitley accepts pressure values 
rank selection population sorted 
done quicksort algorithm void population time replacement described whitley done unsigned population population np unsigned genetic operators genetic operators described chapter applied functions void mutate population pop unsigned member int variance double int crossover population unsigned parent unsigned parent population unsigned child double unsigned points int invert population pop unsigned member double pinv pinv parameters set probability operator applied 
crossover member receive crossover result specified functions operate change parent bitstring 
function available generates complete new population roulette wheel selection crossover inversion mutation int propagate population double pinv double double unsigned int points sample program time selection replacement 
program calls function fitness population unsigned member supply ga fitness member passed parameter 
program performs error checking 
extended include extern float fitness population pop unsigned member define popsize define define pressure define define pinv define void main void population pop contains main population population hold crossover result unsigned chosen parent strings unsigned loop counter pop popsize true false loop creates new strings replaces population 
minimal fitness wanted reached best string population loop ends 
pop member fitness pop pressure pop pressure crossover pop invert pinv mutate member fitness fitness pop pop pop sample pop program finished population saved disk named sample pop contains number iterations needed reach desired minimum fitness 
new genetic functions 
function 
order experiment pressure values larger new function written unsigned population pop double pressure accepts pressure values larger returns index selected member function popsize implementation random number range pressure 
function plotted values 
function probability selected best member population times high median member 
int population unsigned parent unsigned parent population unsigned child double unsigned points original crossover function supported maximum crossover point gene 
research just long gene wanted crossover point 
new crossover function implemented chooses crossover points uniformly random chromosome 
new member created normal concatenating alternate parts parents genes temporarily put order see chapter 
void population unsigned double pinv invert function library swaps genes described chapter 
research just gene containing variable length production rules 
order automatic reordering operator invert new function implemented acts gene chromosome consisting bit long genes 
inversion points chosen gene bits swapped bitwise 
chromosome grammar sub program chr gram reads bitstring member population translates set production rules 
done table chapter 
order ensure production rules adhere restrictions mentioned chapter clean functions called production rule 
extract function checks context 
code shown checks context irregularities concerning digits commas 
int char context unsigned count number context context string terminate context context remove extraneous context context remove empty context context remove useless context return chromosome grammar functions checking predecessor successor similar 
system file containing production rules contains axiom input lsystem sub program 
program comprises parts part reads input generates string second part translates string adjacency matrix third part reorganizes matrix order remove unnecessary connections 
extract part 
shows function returns pointer production data structure 
function called character string pointed pointer rewritten struct production char left context int length left context char pred strict predecessor int length strict predecessor char right context int length right context char succ successor int length successor calls functions prefix returns true current character pointed predecessor current production rule functions return true context current production rule subset nodes connected predecessor nodes predecessor connected 
production char production prefix pred return return null gure left chain replaced right 
second part string translated adjacency matrix function 
function connects nodes module module current module called recursively new module 
chain nodes see useful backpropagation 
backpropagation library trains output nodes regardless number needed 
extraneous nodes increase computing time part program examines matrix removes unused output nodes chains nodes 
implementation process done function 
extract function 
adjacency matrix contained dimensional array called 
number nodes matrix contained variable 
void void unsigned unsigned node counter unsigned nc nr counts node number corresponding column row unsigned nr nc nr break nc break nc nr nr removed node node connected nodes direct connection nc nr matrix done backpropagation matrix results lsystem program input backpropagation algorithm 
backpropagation algorithm written object oriented approach lent implementation modular backpropagation 
class network implements complete network classes module connection 
backpropagation class network implements backpropagation network unsigned class connection connection class module module public network unsigned unsigned size output unsigned size module specifies connections network char char load network matrix net network int save char saves network void reset selects optimal random weights void train double input double desired output double calculated output training void calc double double input output class module unsigned class connection unsigned unsigned void double double unsigned unsigned simplest way implement backpropagation network arrays specifying size module specifying connectivity 
example network chapter declared follows unsigned mod con class network net mod con parameters input output size note arrays zeros indicate element 
network net trained net train input output calc takes input desired output parameters third parameter returns output network input 
desired output needs calculated learning order backpropagation compute errors see chapter 
trained networks saved net save net net saves complete current status network including weights moment terms 
saved networks loaded new class network net net net net implementation witch dynamically creates network 
way research load specification network matrix form 
done function new class network net matrix net mat convention specifying matrix 
adjacency matrix example network 
nodes files extension mat network files net 
matrix file fact adjacency matrix nodes network implements network example shown note example module seen block ones 
possible directly modules connections enables user set values weights 
example chapter saved network loaded tested change specified weights 
code shows extract program implementing xor problem learn xor include backprop define alpha learning rate parameter define beta momentum term parameter define maximum training iterations define epsilon error training stopped void main void unsigned mod con double error input output calc int class network net mod con net alpha beta net reset error input double calculate input output pair input double output net train input output calc error output calc error break net save xor net backpropagation software backpropagation available library backprop lib linked rest program 
file backprop included contains function prototypes data structure definitions predefined constants 
library normal programs compiled compiler 
main program variations main program written implementing roulette wheel selection described goldberg gold implementing rank selection time replacement described whitley 
programs read simulation file contains necessary parameters 
file ascii file containing lines starting symbol followed keyword 
parameters separated spaces follow keyword 
example simulation file containing valid keywords shown 
sample simulation file files simulation population test pop control test ctl size sites pinv pressure steps axiom abc empty lines lines starting ignored usable comment lines 
keywords indicate location files simulation names control file population file 
size specifies number members population 
sites pinv pressure influence genetic operators 
steps maximum number rewriting steps axiom system 
multiple computers run simulation simultaneously 
roulette wheel selection running program reads specified number un processed members population file locks control file containing indicator member free processing done 
programs notices indicators done population file locked fitness values population file updated control file reset free 
fittest member generation archived new population created 
time replacement program opens main population file creates small local population file containing specified number newly created strings 
program processes local members assigned fitness program locks main population file replaces local members main population 
fills local population file new strings starts processing 
implementation cases processing done presenting member chr gram writes output file 
name file transferred lsystem matrix output file passed backprop 
population update loop time version resembles example chapter extract roulette wheel selection version 
size false null memory file line np false null memory file line file fopen rb null read error file line file gen error read error file line popsize select select parents different np copy temp 
population np np pinv np pinv np np sites mutate np np member file gen error write error file line fclose file results experiments done software chapter 
number suggestions improve expand software chapter 
experiments results 
man gotten lot results 
know things won 
thomas edison chapter presents results experiments done research 
problems tried developed software exclusive xor tc problem handwritten digit recognition categorization mapping values categories paragraphs problems described results 
exclusive xor xor function see chapter 
sample xor networks 
boolean logical function variables proven network able solve problem hidden layer mins 
standard solutions networks able solve xor shown 
experiment tried system comma separate neighbouring nodes modules system experiments minus specific connection 
networks xor 
neighbouring nodes modules see chapter 
methods worked fine method minus symbols faster finding modules groups internally unconnected nodes comma method preferred chains nodes 
decided minus symbols rest research comma symbols 
networks shown 
networks able learn xor shown 
matrix cleanup method described chapter experiment networks shown result human pruning resulting adjacency matrices 
tc problem tc problem neural network able recognize letters 
possible letters sample input grid 
grid see 
letter consisting pixels rotated grid 
total number input patterns positions put grid 
shows possible patterns sample grid input network 
black pixel represented input value white pixels input value 
output node trained respond approximation maximum possible error calculate fitness network error 
network tc problem 
fitness calculated error 
network input nodes expected pixels grid input node network shown input nodes 
input nodes expected network able learn input combinations fitness member containing network roughly recombinations 
compared network number simple backpropagation networks hidden layer 
varied number nodes hidden layer standard networks 
patterns times network 
network tested times 
results shown 
bar represents test cycles dark lower part equals number times network classify pattern correctly 
seen network outperformed hidden layer tc problem networks easily 
networks 
comparison network standard networks 
hidden layer optimum hidden layer nodes 
handwritten digit recognition problem neural network able recognize handwritten digits grid 
handwritten digit stimuli obtained instructing human subjects write digits grid computer mouse 
proportion square grid constituted activation value 
network trained half stimuli half test network generalization capabilities 
data originally test network design principles genetic algorithms calm networks happ short calm see inset 
networks short calm 
complete description reader referred 
calm model categorizing learning module proposed candidate incorporation modular information processing principles artificial neural networks 
calm modular network algorithm especially developed functional building block large modular neural networks 
number neurophysiological constraints implemented calm general architecture neocortical basic design principle 
modules contain neurons important feature modules division excitatory pyramidal cells form long ranging connections cortical regions various types short range module inhibitory interneurons 
structural principle inhibition implies main process module winner take competition 
competition enables system autonomously categorize patterns implement unsupervised competitive learning 
categorization learning incorporated calm model 
basically calm module consists number representation nodes nodes fully connected input learning connections 
number nodes module referred size 
categorization calm module operationalized association input pattern unique node said represent pattern 
different patterns categorized calm repeated subsequent presentations set input vectors 
categorization process learning takes place preserves association input pattern node adjusting learning weights 
categorization proceeds resolution competition nodes activated pattern mediated specialized inhibitory veto nodes nodes 
winning node constitutes representation pattern 
different activation patterns lead different representations 
specific learning rule calm enables discriminate non orthogonal patterns 
test shown 
particular network correct generalization score 
proved simple problem backpropagation method network hidden layer input output nodes classified test set correctly experiment training test data happ 
experiments training set 
advantages calm network ability learn new patterns interfering earlier learned patterns 
categorization 
calm network handwritten digit recognition 
problem tried proposed tc problem number patterns recognized larger grid 
problem patterns see placed grid 
recognizing form pattern network encode place pattern larger input grid possibilities 
conducted experiments attempt explain natural visual system processed separate cortical structures 
trained number different networks input output nodes hidden layer nodes 
output nodes separated groups group encoding form group encoding place 
appeared network learned faster mistakes hidden layer split appropriate separate processing resources dedicated processing see 
importance number nodes allocated form place system respectively 
shows optimal network nodes dedicated processing place remaining nodes complex task processing form 
analysis results revealed processing strongly interfere un split model hidden layer nodes 
tried problem 
network 

patterns 
method optimal network network hidden layer 
surprising difficult see network hidden layer capable learning problem 
specific split network necessary hidden layer interference occur 
mapping problem problem tried problem standard backpropagation networks hidden layer 
original problem experiments done van order investigate influence structure network ability map functions 
experiment created dimensional classification problem input space space points assigned classes colours symbols 
constructed mappings second derived misclassifying points 
second mapping shown 
misclassified points seen mapping problem noise 
van wanted networks noise ignored interested networks able learn points correctly 
van tried number networks 
input grid mapping problem 
input nodes respectively output nodes symbol 
network hidden layer nodes network hidden layer nodes able learn mapping noise failed learn changed points mapping 
network hidden layer nodes able learn misclassified points 
network hidden layers nodes connections misclassified points learned correctly 
tried experiment hoping method find small modular network able learn second mapping correctly 
experiment values various parameters population size chromosomes length whitley selection replacement pressure mutation probability crossover probability crossover points inversion probability system axiom maximum rewriting steps took days sun sparc workstations converge network shown 
roughly string evaluations 
networks shown seen ancestors earlier simulation 

networks simulation 
experiments networks production rules create network shown 
clearly production rules evolved 
shows fitness worst best 
network 
member population simulation average fitness value 
fitness determined adding total error output input stimuli subtracted 
low error resulted high fitness 
took days reach network shown roughly string evaluations 
comparison network van network network consistently higher fitness took time train network contains connections connections network 
extended training network error versus network 
shows convergence networks van vs genetically network shown training session 
number evaluations fitness best average worst 
graph showing fitness population simulation 
mapping problem network number training cycles error best network ga number training cycles error 
comparison convergence 
experiments recommendations perfection attained slow degrees requires hand time 
early stage combination search methods growth mechanisms computational principles nature biology results promising strategy designing artificial intelligent systems matter definition chooses artificial intelligence 
major problems general acceptance products principles 
systems complex functioning longer understandable 
difficult people accept artificial intelligent system knowing system came result results system just better results human experts 
get 
continuing research complex systems result new mathematical techniques able explain happening inside working neural network 
genetic algorithms find architectures neural networks best suited specific task 
genetic blueprint representation results obtained small networks 
blueprint method large size networks exponential growing number possible connections coded 
goal research design method allowed principle possible network architectures suffer exponential growth bits needed represent member population 
method resulted research uses metaphor recipe network coded just set rules produce network 
preliminary results shown chapter suggest method finds modular networks perform better networks specific structure 
idea rules grammar directs genetic search modular architectures justified results obtained 
lot experimenting needs done order optimize proposed method 
recommendations problems encountered amount computing power needed 
mapping problem described chapter needed week converge final solution sun sparc workstations parallel 
caused large amount computing needed evaluate architecture 
research research just branch tried 
usual working intensively project large numbers unanswered questions remain lot ideas implemented tested months duration project 
remainder chapter number ideas suggestions improvements possible research 
suggestion followed short explanation 
parallels nature pointed 
backpropagation network paradigms mentioned takes lot computing time train network repeated new chromosome simulation 
lot networks generated especially genetic algorithm converged topology 
database containing networks evaluated 
network evaluated say times average error calculate fitness network architecture chance chromosome coding network receiving erroneous fitness small 
population converged generating equal network structures may idea create database containing trained network architectures 
reduce amount computing time needed considerably 
works genetic algorithm generates discrete output 
soon floating point parameters coded probability members equal small 
prune extraneous input nodes 
pruning method extraneous output nodes chain ares removed 
extraneous input nodes removed network 
genetic algorithm calculate fitness initial population 
algorithm whitley initialized population random chromosomes 
fitness randomly determined initial members evaluated set zero 
caused direct loss useful information happened random initialization 
initial member tested moment placed population selection parents begins correct fitness values 
initialize population members having non zero fitness 
previous suggestion improved making sure initial member fitness 
research happened newly generated chromosome research fitness fitness zero 
happen network resulted string output nodes minimum simply large maximum number nodes 
happened frequently string contain production rules applied 
program running new chromosomes course inserted population 
selection start initialization population 
insert initial member non zero fitness 
probably result diverse population probability initial members inserted production rules small 
may prevent evolution gets stuck accidental reasonable set production rules evolve 
seen local optimum evolution 
idea gives rise comparison soup theories see 
atmosphere earth time life earth contained oxygen plenty hydrogen water carbon dioxide simple organic gases 
soup theory states environment influence lightning ultraviolet light ozone layer thousands millions years spontaneously created molecules able replicate subsequently started evolution 
started evolution slowly give rise complex creatures 
simulations 
took quite find production rule applied increasing number useful new chromosomes 
experiments described chapter compared period just start evolution small part full potential method 
longer simulations larger tasks may require full potential method 
suggestion keep soup period short possible start simulation high amount randomly rules combined may quickly result solutions 
initial population useful production rules 
previous suggestion may substantially faster initial population contains sets production rules create known standard solutions problem hand 
production rules previous simulations may taken 
needs tested method really faster waiting random members evolution initial population members wanted solution may longer process starting scratch 
particularly happen optimal solution completely different structure expected 
include axiom genetic process 
simulations axioms change simulation 
fixed parameter file genetic algorithm 
including axiom separate gene members population probably find better axioms ad hoc chosen axioms 
production rules useful may come existence specific axioms operate 
suggestion leads link biology genetic information stored nucleus cell read process transcription special enzyme rna polymerase cell 
human ovum seen axiom production rules coded genes build contains necessary initial embryo start growing 
recommendations chromosomes different size 
simulations worked chromosomes bits 
looking results noticed solutions just production rules 
decreasing size chromosomes amount freedom reduced may result faster convergence genetic algorithm 
may negative effects chance finding production rule initial population applied axiom smaller small chromosomes 
chromosomes variable length 
idea tried genetic algorithm decide useful chromosome length 
needs change genetic algorithm 
crossover changed way parts parents copied new child chosen child preferred chromosome size 
size coded separate gene coding length individual members population 
idea counterpart nature different species number chromosomes 
slight deviations number size chromosomes nature usually lead functional disorders organism syndrome humans 
safely assumed changes lead improvements 
reality individuals population eternal life 
believe accident probably necessary condition evolution real world 
tests different network structures problem noticed accidental initial weights result reasonably performing trained network 
see example chapter 
worst network hidden layer nodes correct input output mapping times 
clearly wanted solution looking network structures specific problem times possible correctly learned task 
suppose network generated early stage simulation receives high fitness accidental initialization weights 
stay population high fitness chosen parent frequently members population 
situation continues long time offspring bad network accidentally able learn task 
result increasing part population containing copies original bad member 
suppose production rules bad architecture small changes lead worse network architectures 
means genetic algorithm trapped local optimum caused randomness backpropagation 
just kind randomness trying eliminate finding right architecture 
test phase software run simulations ended local optima 
trained generated network simulations time random initialization network initial weights average error separate training measure fitness architecture 
aging 
idea came discussing problems age population 
artificial aging members static population done multiplying fitness values members value slightly just new chromosome evaluated 
course concerns static population model see chapter variant described goldberg generates completely new population generation 
result hopefully unwanted members eventually disappear research population offspring chance high fitness inserted population 
network really means network continuously generates offspring high fitness original network consequence aging process slowly leave population children carry information generations 
important choose aging factor percentage taken fitness time large chromosome high fitness stay population long generate offspring 
copy best member far saved just case 
re evaluate member 
variation suggestion regularly reevaluate member population static population model example time number new chromosomes equal population size generated 
members low probability repeatedly scoring high fitness leave population 
suggestions necessary genetic algorithms static population areas fitness large random component 
current research genetically optimizing calm networks see inset chapter group research took place idea aging 
constructing calm network able learn sequences time major problems noise calm modules occasionally resulted correct sequence 
resulted occasional high fitness resulted population high level noise 
system suggestion axioms applies section 
wild cards production rules 
wild cards improve extend production rule matching context 
example lower case letter match digit letter predecessor copy digit 
empty brackets indicate module 
different number rewriting steps 
experiments done research number rewriting steps systems 
string reached maximum length rewriting steps 
different numbers rewriting steps examined see effect resulting networks 
idea extended fixed number rewriting steps varying number rewriting 
fixed number rewriting steps 
example system continue rewriting maximum string length reached network created sofar sufficient input output nodes 
rewriting step network created sofar tested fitness values stored 
predetermined maximum string length number rewriting steps reached highest fitness far returned 
recommendations production rules coding resulting strings translation table 
table chosen translation chromosome characters production rule see chapter genetic code actual distribution characters table chosen arbitrarily 
experiments done different distributions placements characters table number characters varied asterisks digits 
symbols introduced increasing size table example allow larger skips 
network learning paradigms feedforward restriction apply provisions order strings able represent recurrent networks 
negative skips 
negative skips represented special symbols minus symbol skips currently needed allow recurrent networks 
suggestion implies changes translation table 
examine number building blocks 
research assumed overlapping information chromosomes number building blocks evaluated string evaluation higher standard genetic algorithms overlapping strings 
examined large amount building blocks processed string evaluation suggestion extended strings different density 
overlapping strings 
amount building blocks resulting suggestion compared amount building blocks strings overlapping information 
example strings read forward resulting string read times 
result experiments amount overlapping information changed 
try fourth type string 
type strings thought tried strings letter represents complete module 
size modules fixed example size size separate gene code length module 
advantage method modular networks represented shorter strings 
number production rules needed length change method may advantages 
miscellaneous large simulation tried 
experiments simulations done research small scale 
order investigate real potential methods proposed large simulation done 
preferably amount computing time needed supercomputer large network example transputers adjustments method easily having wait days simulation finish 
research exhaustive comparisons 
method proposed compared exhaustively methods ones blueprints 
methods compared convergence ga performance networks different length chromosomes etcetera 
investigate scalability 
appropriate problem possibilities scalability method proposed investigated 
convergence ga performance networks compared problem larger scales methods 
tool display network architectures 
networks get larger larger difficult get impression structure networks 
tool drawing networks adjacency matrices necessary 
recommendations derivation backpropagation appendix contains derivation backpropagation learning algorithm generalized delta rule gdr 
explanation restricted minimum interested reader find extensive treatment free 
derivation simple layer network easily generalized networks layers 
artificial neural network classes treated free offers general field neural networks 
error output node training desired output actual output jth output node 
error minimized gdr number output nodes 
determine change weights output nodes calculate negative gradient respect weights considering ij component separately get ij stim stim ij derivation backpropagation stim ij ij ij number hidden nodes ith hidden node ij stim weights changed proportional negative gradient ij ij dw ij dw ij stim factor learning rate parameter 
notice requirement function differentiable 
function usually stim stim derivative ij ij formula update weights hidden layer output layer 
stim stim rewrite formula 
ij ij ad problem calculation weights input layer hidden layer desired activation node hidden layer contrary output layer unknown 
total error related activation nodes hidden layer stim ij possible calculate gradient respect weights input layer output layer ij ij stim stim stim stim ij stim jk stim ith input node 
weights hidden layer updated negative gradient dw ij af stim stim jk dw ij af stim jk notice weight update weights hidden layer depends error terms output layer 
notion backpropagation arises 
known errors output layer propagated back hidden layer determine appropriate weight changes layer 
defining stim jk write weight update formula weights hidden layer ij ij ad form 
form delta rule training networks hidden layer see free 
derivation backpropagation entropy neural networks study claimed showed examples fact initial structure neural network greatly determines performance 
appendix gives mathematical foundation claims 
partly soll 
learning entropy reduction neural network trained performing input output mapping 
mapping implemented network depends architecture network weights 
architecture fixed mapping network solely determined weights 
set possible weight configurations weight space network determines probability distribution space possible input output mappings implemented fixed architecture 
entropy distribution quantitative measure diversity mappings realizable architecture consideration 
learning examples reduces intrinsic entropy untrained network excluding configurations realize mappings incompatible training set 
residual entropy trained network measures generalization ability 
goal research design method find network structure task residual entropy zero training examples means network converges state implements desired mapping independent initial weight configuration 
network architecture able give different mappings result training examples residual entropy zero 
probably lead bad generalization examples complete domain training possible extract set examples different mappings 
correct topology network allow mapping wanted 
residual entropy training zero small deviations input patterns training set give large errors output 
entropy neural networks seen imagining weight space large number minima implementing exactly mapping training set just representing actual wanted mapping 
shows importance selection training set 
definition entropy possible give definition entropy neural network probability network implementing mapping defined 
network architecture corresponding weight space set possible weight settings sara solla soll defines priori probability network mapping equals total volume allowed weight space dw volume initial weight space implements desired mapping implements mapping words probability network implements desired mapping arbitrary setting obviously network able implement desired mapping network unable learn desired input output mapping 
selecting network structure defines class functions realizable network 
useful measure diversity possible mappings implemented chosen architecture entropy ln probability distribution 
just desired mapping mappings distribution definitions probability entropy neural networks give mathematical notion necessity find networks initial structure 
correct initial structure network results high probability turn leads low intrinsic entropy untrained network 
intrinsic entropy untrained network needs eliminated learning process 
purpose training confine configuration space region eliminating ambiguity input output mapping implemented trained output 
addendum time thesis finished article kitano kita title designing neural networks genetic algorithms graph generation system 
addendum gives summary method kitano highlights main differences method proposed proposed thesis 
kitano uses standard genetic algorithm search artificial neural networks 
ga 
sample production rules 
production rules axiom rewritten ab ca combination kind graph grammar called matrix rewriting system 
grammar production rule single symbol non terminal left right hand side consist matrix containing non terminals 
axiom matrix 
rewriting step non terminals replaced matrix matching production rule 
replaced matrix respectively 
note matrix grows factor size rewriting step 
process repeated number times 
nonterminals remain rewriting step replaced 
binary matrix results rewriting step adjacency matrix network created 
small example shown 
chromosomes kitano genetic algorithm fixed length binary strings consisting parts part genetic operators crossover mutation performed non terminals production rule coded rewrites symbol non terminals 
second part changed genetic search predetermined non terminals matrix consisting pre encoded 
addendum precise description production rules rewriting method reader referred kita 
method proposed kitano final size network determined directly number rewriting steps 
method advantage chromosome length different size networks coded larger networks exponentially larger chromosomes needed method proposed thesis constraints resulting network size restraints necessary 
important difference method proposed fact kitano method context 
believe context plays important role biological growth context incorporated method supposed biological plausible exactly kitano tried 
results kita indicate method works method scaling problems expected called blueprint methods 
knowledge method proposed kitano 
early proposals method flexible example context 
allport pattern actions 
new directions cognitive psychology ed routledge kegan paul london 
generality functional structure neocortex 
naturwissenschaften 
dawkins blind watchmaker longman 
reprinted appendix penguin london 
denker schwartz solla howard jackel hopfield large automatic learning rule extraction generalization 
complex systems 
dodd dodd optimization network structure genetic algorithms 
proceedings international neural network conference paris widrow eds kluwer dordrecht 
free freeman neural networks algorithms applications programming techniques 
addison wesley reading 
de garis brain building gennets 
proceedings international neural network conference paris widrow eds kluwer dordrecht 
organization human brain 
science 
gold goldberg genetic algorithms search optimization machine learning 
addison wesley reading 
textbook medical physiology 
saunders philadelphia 
happ architecture function neural networks designing modular architectures 
prep 
harp harp samad guha genetic synthesis neural networks 
proceedings rd international conference genetic algorithms applications icga schaffer ed morgan kaufmann san mateo ca 
hecht nielsen neurocomputing 
addison wesley reading 
neurocomputers machines voor 

hofstadter del escher bach eternal golden braid 
basic books new york 
hogeweg model study description 
pattern recognition 
holland hierarchical descriptions universal spaces adaptive systems 
university michigan press ann harbor 
holland adaptation natural artificial systems 
university michigan press ann harbor 
van neural network genetic recognition 
unpublished student report leiden 
hubel wiesel receptive fields binocular interaction functional architecture cat visual cortex 
journal physiology 
issues brain evolution 
oxford surveys evolutionary biology dawkins eds 
kandel schwartz principles neuroscience 
elsevier new york 
kita kitano designing neural networks genetic algorithms graph generation system 
complex systems champaign il 
koch von koch une pour tude de questions de la th orie des planes 
acta mathematica 
lind lindenmayer mathematical models cellular interaction development parts ii 
journal theoretical biology 
livingstone hubel segregation form color movement depth anatomy physiology perception 
science 
mand mandelbrot fractal geometry nature 
freeman san francisco 
mari gennet systems computer aided neural network design genetic algorithms 
proceedings international joint conference neural networks washington dc 
mars marshall newcombe journal physiology 
mins minsky papert perceptrons 
mit press cambridge ma 
organizing principle cerebral function unit module distributed system 
mindful brain edelman eds mit press cambridge ma 
categorization learning neural networks 
modelling implementation modular framework 
dissertation leiden university 
origins life 
wiley new york park parker learning logic 
mit press cambridge ma 
penrose emperor new mind 
oxford university press new york 
learning natural connectionist systems experiments model 
unpublished dissertation leiden university leiden 
posner lewis conrad language ear eye eds mit press cambridge ma 
posner explorations mind 
oxford university press 
posner peterson fox localization cognitive operations human brain 
science 
prusinkiewicz hanan lindenmayer systems fractals plants 
springerverlag new york 
lindenmayer algorithmic beauty plants 
springerverlag new york 
cave kosslyn processed separate cortical visual systems 
computational investigation 
journal cognitive neuroscience 
rumelhart mcclelland eds parallel distributed processing 
volume foundations 
mit press cambridge ma 
schwartz new connectionism developing relationships neuroscience artificial intelligence 
soll solla learning generalization layered neural networks contiguity problem 
neural networks models applications dreyfus eds paris 
neural network cerebral cortex functional interpretation 
proceedings royal society london 
module concept cerebral cortex architecture 
brain research 
interpretation systems computer graphics 
science 
turi turing computing machinery intelligence 
computers thought feigenbaum feldman eds mcgraw hill new york 
warrington fractionation arithmetical skills single case study 
quarterly journal experimental psychology human experimental psychology 
werbos regression new tools prediction analysis behavioral sciences 
unpublished ph thesis harvard university cambridge ma 
whitley genitor algorithm selection pressure rank allocation reproductive trials best 
proceedings rd international conference genetic algorithms applications icga schaffer ed morgan kaufmann san mateo ca 
whitley hanson genetic synthesis neural networks 
proceedings rd international conference genetic algorithms applications icga schaffer ed morgan kaufmann san mateo ca 
functional logic cortical connections 
nature 
