connectionist sentence processing perspective mark steedman draft march appear 
geoff hinton mark liberman editors comments advice 
research supported part nsf nos 
iri iri arpa 
aro 
daah 
connectionist sentence processing perspective mark steedman cis university pennsylvania emphasis connectionist sentence processing literature distributed representation emergence grammar systems prevented connectionists alike recognizing close relations respective systems 
argues simply recurrent network srn models proposed jordan elman directly related stochastic part speech pos taggers parsers grammars recursive auto associative memory raam kind pioneered pollack incorporated hybrid connectionist parsers may useful grammar induction network conceptual structure structure building 
observations suggest interesting new directions connectionist sentence processing research including efficient devices representing finite state machines acquisition devices distinctively connectionist grounded conceptual structure 
papers special issue attest active constructive dialogue processing level spoken written words acquisition related systems phonology morphology lexicon going symbolic rule oriented theorists connectionist theorists 
dialogue symbolic rule approaches syntactic analysis despite genuine efforts reconcile positions connectionist side example hinton smolensky collected hinton 
reciprocal attempt side get dialogue going basis contributions collected 
traditional rule view syntactic processing divides problem various modules 
fairly generally applicable way doing distinguish grammar characterized rules certain class related characteristic automaton algorithm characterized properties order rules applied string bottom top certain memory resources charts tables dynamic programming oracle decision criterion deciding rule apply algorithm cases possibility 
course theoretical presentation implementation modules may combined rule theories usually distinguished functional terms 
connectionism intrinsically non modular approach connectionists including represented explicitly endorsed modular architectures various kinds emphasis connectionist literature distributed representation emergence behavior systems hard connectionists alike recognize close relations respective systems 
part difficulty reconciling stems involvement different views role grammar processing 
performance grammar module processor described theory quite different grammar linguists identify competence grammar 
linguists grammar usually derivation structures closely related intuitions interpretation sentences 
direct access interpretations practical criteria choosing linguistic analysis usually described different terms linguists chomsky fact comes 
early parsing programs algorithms requiring grammars normal form particularly linguists weakly equivalent covering grammar grammar normal form accepting string set build structure structural representation required linguists 
modern parsers compile large grammars weakly equivalent finite state covering grammars coverage course necessarily incomplete reasons efficiency 
reasons elaborated surprising find human processors covering grammar 
possible theory known poorly understood limitations human abilities process sentences involving center embedding 
exactly property led chomsky claim context free grammars represented lower bound power natural grammars claimed limitations evidence human processors incomplete finite state covering grammar 
course explanations possible 
algorithm incomplete memory limitations constructions oracle 
covering grammar involved capable specifying interpretation 
saying capable building traditional syntactic structure 
linguists tend think syntactic derivation surface structure building equally possible think structures implicit flow control parser incrementally builds interpretation directly 
linguists especially computational linguists usually think interpretations structures logical forms matter fact strictly necessary 
montague possible principle regard logical form flow control computing models sense term model theoretic semantics 
idea implicit connectionist appeals idea processor providing sort guided trajectory highdimensional semantic space kind invoked volume tanenhaus 
important get carried away possibility 
model theory really proving general properties formal systems soundness completeness 
ai natural language understanding usually interested form proof get new york city mere truth proposition proof tells get 
means practical knowledge representation systems proof theoretic model theoretic 
semantic trajectories real semantics point take entirely unproven clear support reasoning kind need 
significance point faced connectionist processor ask grammar implicit deliver interpretations right kind 
second source difficulty comparing connectionist rule systems lies tendency certain kinds connectionist architecture combine roles grammar oracle ambiguity resolution device single representation distributed single set hidden units 
hard know compare systems regard connectionist system representing disambiguating device example analog markovian part speech pos filter standard parsing input units item item context units output units hidden units copy architecture simple recurrent network srn architecture 
shall argue devices sentence processors thought restricted sense problems reliability scalability inherent mechanisms backpropagation overcome view offers way forward kind system embodies rules sub symbolic representations principled way 
recurrent networks finite state recurrent networks proposed jordan elman auxiliary bank state context units store information previous state standard level feed forward network backpropagation adjust hidden layer units 
recurrence consists copying output units hidden units context units 
context units provide inputs hidden units 
jordan applies device coarticulation problem speech uses context units represent preceding items directly copying output units previous cycle 
copying hidden units elman simple recurrent network srn shown represent properties preceding sequence 
interestingly context units come carry echoes earlier states computation represent quite distant dependencies string elements 
elman applies simply recurrent networks srn problem supervised grammar induction indirect sense device build structure exhibit internal states reveal implicit grammar indirectly predicting word word category 
elman early criticized hadley non systematicity selection test materials respect claimed implicit grammar cases implicit grammar simpler initially claimed doubt srns recurrent nets approximate covering finite state machines kind involving dependencies non neighboring elements cleeremans cleeremans 

obvious limitation approximation appears rapidly reliable number hidden units distance dependency increases 
reason precise place graded state automata automata theoretic hierarchy entirely clear 
elman extended cleeremans christiansen chater 
particular christiansen shown srns cover finitely bounded sub languages small contextfree grammars center embedding similarly bounded sub languages small trans context free grammars including crossing dependencies kind notoriously characteristic verb raising constructions bresnan shieber 
important note sole task srn required predict word word category point string 
know symbolic finite state models hidden markov models hmm part speech pos taggers jelinek church merialdo brill approximations achieve high accuracy better precision having claim whatsoever embody grammar 
put number perspective recall prediction category basis simple unigram frequency yields accuracy 
surprising things recurrent network literature little link statistical computational linguistics see williams hinton bridle 
comparison pos taggers relevant second earlier criteria covering grammar srns 
property yielding structural description meaning representation property related hadley calls semantic systematicity 
discussion srn literature principal components analysis hidden units discover implicit grammatical categories clustering factor analytic techniques quite distant structure building fact extent srns seriously exploit potential hidden units efficiently distributing representation fsm approximate correspondingly hard see associate structure building operations kind directly 
observation taken deny srns useful component sentence processors 
srns related devices may way building stochastic part speech 
role srn plays modular architectures st john mcclelland berg sharkey sharkey discussed 
interestingly srinivas srinivas joshi shown increasing set pos tags include sub categorization domain government information implicit grammars discussed section increases effectiveness devices 
virtues suggest correct regard grammar sense emergent property device presently 
worth noting distributed representations potentially allow exponentially efficiency representation stochastic finite state machines standard representations hmms williams hinton 
srn achieve efficiency see pearlmutter relevant discussion 
pos tagging srn doing may want interpret hidden unit states may prefer exploit efficiency devices discussed williams hinton 
psychological relevance srn number studies investigated fit srns human parsing performance 
studies volume dell 
tanenhaus examples kind 
tanenhaus advance theoretical model elaboration copying mechanism srn uses approximate backpropagation time btt rumelhart pearlmutter gravitational analysis attractors obtained principal components analysis patterns activation hidden units interpret fsm states 
authors point viewed parse hypotheses mapped traditional symbolic models seen analogy pos tagging srns translation general non trivial incomplete 
tanenhaus provide detailed comparison experimental findings 
concerning influence thematic fit verbs arguments line sentence comprehension minimal pairs sentences misleadingly better thematic fit cop agent patient indicative reading arrested comparison crook causes temporary increase processing load sentence reading leads grammatical analysis 
cop arrested detective guilty 
crook arrested detective guilty 
fit model terms predicting word word processing effort revealed increased reading times impressive 
authors justified claim srn cover phenomenon garden path model frazier clifton building structure 
claim amounts emergence grammar srn model premature 
remains case thing srn doing predicting category sentence 
surprising srn structural model 
learning finite state machine seen fsms category prediction homogeneous corpora 
success srn predicting processing difficulty basis frequency information justify claim human processor works basis 
earlier tanenhaus group shown structural properties sentences confounded frequency effects frequency properly controlled evidence residual structural effect 
see trueswell tanenhaus trueswell trueswell spivey knowlton 
frequency strongly confounded semantic pragmatic plausibility 
tanenhaus colleague earlier papers careful point results distinguish model processor srn resolves ambiguity entirely probabilistically ambiguity resolved basis semantic pragmatic plausibility 
evidence favors semantic interpretation 
performance low level markovian pos taggers human standards precision means error sentence standard written text 
means frequency doing humans 
doing may doing 
experimental evidence steven crain gerry alt mann author shows parsing decisions sensitive relative plausibility semantic interpretations rival analyses may depend state referential context 
fact implications sentences isolation see crain crain steedman altmann altmann steedman steedman altmann discussion 
showed example mention single policeman set discourse immediately preceding examples effect tendency assume verb indicative participial 
authors argued mentioning member set restrictive relative clause modifier pragmatically felicitous mentioning individual 
argued null context policeman mentioned referent attendant presuppositions accommodated added contextual model accommodation referent effortful accommodation distinguishing properties presupposed modifier 
preferences parsing decisions reversed preceding context setting sentences implausible argue human parser decision basis global frequencies collected large volumes input 
attempt generalize finite state approximation techniques accommodate possible discourse contexts sentential ones run sparse data problem big way technically possible 
alternative assume interpretations incrementally computed rival analyses compared leading rapid elimination plausible alternatives 
hard believe done fuller grammatical analysis implicit srn 
study syntactic priming bock dell volume appear glance encourage optimistic view 
uses variant simply recurrent architecture linking context units srn trained associate sequences words static content vector production network producing sequences content 
crucially pairs sequences corresponding active vs passive surface forms associated content 
having trained network authors able show presentation string say active pattern biases immediately subsequent productions pattern different content results comparable bock subjects 
authors claim shows syntactic priming effects emerge implicit learning rule activation structures short term memory 
observation entirely compatible idea nature implicit learning lies change probabilities implicit finite state machine 
course follow rules implicit interpretation done basis probabilities 
merely shows syntactic priming necessarily syntactic 
follow analogous priming effects humans mediated probabilities confounding probability semantics pragmatics discussed connection tanenhaus experiments 
argument support alternative view stochastic component modularly separate grammar interpretation mounted basis bock apparent grammatical priming effects related number agreement bock bock 
show errors production number nearby preceding noun overrides agreement verb number head subject bock 
know result 
striking things errors disrupt comprehension comparatively little production comprehension asymmetry plural singular agreement errors appearing rarely 
compatible idea agreement comprehension production mediated autonomous peripheral finite state processes limited buffering capacity kind implicit connectionist models kind considered suggested bock pp 
relation recurrent networks finite state machines traditional sort pos taggers hmms course evident jordan elegant application recurrent networks model coarticulation speech motor control problems suggests direction connectionist models syntactic processing evolve 
trend symbolic stochastic language processing away grammar independent pos tagging greater integration probabilistic information grammar recursive parsing algorithms particular lexicon 
requires different kind mechanism 
associative net storing single pointer recursive auto associative memory grammar number connectionist processors nets distributed representations structure networks viewed encoding thematic roles propositions 
early versions idea mcclelland kawamoto non recursive pollack showed recursively embedded structure built rule nets architecture called recursive auto associative memory raam 
efficient version simpler device called associative net willshaw willshaw 
associative net acts distributed memory associating pairs input output vectors represents grid horizontal input lines vertical output lines binary switches triangles intersections 
store association input vector output units output units hidden units parent daughter daughter daughter daughter copy architecture binary recursive auto associative memory raam left output vector top switches turned black triangles intersection lines correspond input output patterns 
retrieve associate input signal sent horizontal line corresponding input 
input signal encounters switch increments signal corresponding output line unit 
lines thresholded level corresponding number bits input 
thresholding associative memory store number associations distributed fashion interesting properties noise damage resistance 
point device purposes association input vector output vector regarded analogous storing pointers addresses 
output turn input associated output 
associative memory store recursive structures depth subject information theoretic limits dependent size 
smolensky tensor product representation generalization idea 
raam mainly differs primitive associative net hidden units observable switches efficiently encode association 
achieved realizing device level feed forward network input output units structured sectors large copy hidden units maximum branching factor nodes structure 
alternatively case associative net regard pointer responsibility separate associative devices 
shall see need ordering information implicit standard raam 
binary version device sketched 
recursive structure stored bottom raam starting leaf elements recursively auto associating vectors comprising activation patterns resulted encoding daughters 
activation pattern results auto association daughters treated address parent 
including finitely units input output layers associate node label content information nodes device store recursive parse structures thematic representations varieties logical form sentences 
device confused parser trained fully articulated structures merely efficiently stores 
hidden units regarded encoding approximation context free productions defined structures fashion similar way hinton encoded part relations 
sense device held capable inducing corresponding grammar trees pollack 
basis limited degree generalization tendency decode novel trees members training set 
theory generalization recursively productive practice achieved 
pollack proposed augment trained raam variant recursive network parsers discussed section parser interpreter lines suggested st john mcclelland non recursive predecessor related proposals including berg sharkey sharkey 
earlier remarks limitations sense srns said represent grammars opposed primitive notions current state fsm corresponding part speech show extent devices generalize truly novel sentence structures 
authors augment srn raam modules extent duplicate standard finite state control stack architecture symbolic approach lack simple virtues raam wild romance pure srn 
suggests better reserve raam device inducing grammar sense 
linguists think grammar induction problem inducing structure strings problem external source information exact solution impossible interesting cases gold 
interesting classes grammar task technically approximated degree exactness shown considered amounts data computational resources required realistically sized cases psychologically quite daunting 
led claims innate linguistic knowledge red empiricist bound bridle 
plausible source pre grammatical knowledge semantics assumption child comes language learning equipped universal conceptual structures language specific grammar directly hung pairing words sentences conceptual structures describing situation utterance 
see chomsky chomsky insisted access detailed nature conceptual structures syntax inadequate observation empirically useless 
course gleitman fisher 
rightly pointed dangers identifying situation utterance instantaneous state physical world warned assumption situation uniquely identifies relevant conceptual structure proposition 
siskind shown reasonable assumptions nature information nature ambiguity information 
relevance discussion conceptual structures reasonably regarded structural input device raam purposes induction underlying grammar 
course language acquisition conceptual structure represents universal grammar specific language conceptual structures properly regarded unordered 
regard raam related device storing word meanings logical forms sentences pair logical forms language specific categories 
frameworks discussed section categories amount specification entire language specific grammar 
categories provide input standard architecture modular parser raam associative device build interpretable structure distinctively connectionist contribution lying distributed lexical entries logical form 
interesting point representation assume training conceptual structures available result relatively directly structure connections short term memory 
structure undoubtedly result biological evolution long period 
higher levels structure may arise non linguistic network concept learning kind discussed hinton mediating symbolic forms 
networks lexicon representations built raam related distributed associative memory assumptions embody traditional local domain defined lexical entities verbs arguments including subject 
growing consensus linguistic theories lexicon main locus language specific grammatical information loosely call heads lexically specified controlling local domain lexical functional grammar lfg see bresnan combinatory categorial grammar ccg see steedman head driven phrase structure grammar hpsg see pollard sag lexical tree adjoining grammar ltag see joshi schabes certain versions government binding theory gb see hale grimshaw 
advantage theories lies closer integration lexicon syntax semantics phonology including intonation called kelly kelly martin christiansen 

example ccg word constituent associated directional syntactic type logical form phonological type 
combinatory syntactic rules combine entities produce standard constituents associated components predicates vps non standard constituents corresponding substrings 
involved phenomena coordination intonational phrasing marks intonational phrase boundary marked rise lengthening capitals indicate pitch accent stress 
lost quarter 
know lost dime 
quarter intonational phrasing example related steedman discourse information structural notions theme topic rheme comment focus new information 
non standard constituents claimed crain steedman altmann steedman provide direct grammatical support fine grain incremental interpretation proces sor implicated crain altmann experimental results 
frameworks grammar acquisition reduces decisions syntactic type corresponding walking concept looks subject left right particular language child faced ccg terms snnp np 
directionality represented value input unit categories defined finite state machine handled network lexicon learning devices raam srn 
part interest proposal lies possibility learning capture word order generalizations lexical categories point christiansen devlin 
important note assumption covering grammar transparent semantics way finite state machine implicit srn complicates task associating meanings categories greatly appears pose equally serious difficulties attempt explain evolution language faculty 
observations suggest closer relation connectionist theories usually assumed 
grimshaw particular relates forms categories take constraint satisfaction problem directly related optimality theory prince smolensky branch neural network literature discussed smolensky volume 
process ordered constraint satisfaction best seen definition notion possible human lexicon process parser goes connection level language acquisition machine learning processing discussed seidenberg 
constraints semantically related categories tensed transitive verbs directionality svo order soft constraints exceptions english auxiliary verbs main motivations optimality theory 
optimality theoretic constraint systems regarded defining finite state transducers eisner associative memory lexical acquisition device sketched suited acquiring soft constraint lexicons interesting alternative learning mechanisms proposed 
claim form possible lexicons emergent neural mechanism force 
similar tendency lexical involvement evident current statistical computational linguistic research 
proposals collins charniak move away autonomous markovian pos tagging prefiltering greater integration probabilities grammar 
collins particular uses standard dynamic programming parsing algorithm guidance probabilities dependencies heads main verbs nouns head arguments 
architecture quite directly compatible lexicalized grammars ccg hpsg tag 
interesting investigate relation collins procedure inducing probabilities neural mechanisms discussed suited capture dependencies 
semantics neural networks observe resolution syntactic non determinism human parsers language acquisition appear depend structurally explicit semantics manipulated stack automata appear sort appeal view 
want argue important contribution sub symbolic theories problem language understanding may level semantics syntax reason 
processing decisions depend semantics fact depressing need build practical parsers know semantics question poorly understood knowledge representation systems support small restricted domains 
reason expect applied computational linguists keep going statistics despite fact pretty sure tactic entirely successful 
reason connectionists inhibited way reason concentrate attention 
reason grasp semantics inadequate conceptual primitives underlie language grounded obscure ways physical social intellectual interactions real world 
cases forms take directly related physical structure sensory motor apparatus 
cases fodor claimed really little decomposition meaning level morpheme 
believes kill decomposes involving composition cause predicate die predicate far goes 
cause primitive doesn want decompose fact shows signs distinct translation word cause causative construction cooled 
fact able define inference system directly terms meaning postulates relating morpheme near morpheme level primitives directly fodor proposed 
non decomposability lexical meaning shows may ways nonexistence concept means exactly thing applies non physical entities integers fuzziness verb classification schemata levin discussed dang 
allen 
looks result principal components factor analysis semantically interpretable set features despite strong syntactic foundation 
exactly kind system sub symbolic approach best suited analyze 
remarks correct expect principal components analysis interpretable way expect results parsing happy tolerate high degree cognitive return efficiency learnability distributed representation 
really emergence worthy name 
project scientific psycholinguistics project development grounded semantics require starting earlier stage onset language learning 
recapitulate neural computational terms kind program sensory motor development outlined piaget theoretical baggage form account subsequent research lines sketched symbolic computational terms drescher siskind 
research program proceed conceptualizing primary bodily actions sensations coordinating perception primary actions reaching conceptualizing identity permanence location objects independent percepts particular actions involved amounting internalization components stable world independent child actions 
stages include conceptualization complex events including intrinsic actions objects falling translations events involving multiple participants intermediate participants including tools goals 
final stage purely development prerequisites language learning established embedded raam associative memory support program inducing similarly layered sequence linguistic categories deictic terms proximal distal dimension central place language development respect definiteness discussed lyons cf 
freud pp revealing case study markers topic comment contrast common nouns spatial path terms causal verbs modal propositional attitude verbs temporal terms 
semantic theory emerge proposed far standard logicist frameworks 
semantics view phenomena quantification modality negation variable binding new light unified theory combining symbolic neurally grounded levels 
know ambitious research program feasible current state knowledge time manifesto cognitive neuroscience freud abandoned forever favor symbolic approach 
success depend crucially involvement reliable biologically plausible network models simple types feedforward networks considered 
particular order concentrate relation entire class models alternatives passed known problems techniques backpropagation concern poor scaling data sets learning times number connections increases 
hinton interesting alternative 
papers collected convincing case attempt way 
allen joseph 
probabilistic constraints acquisition 
language acquisition knowledge representation processing edinburgh 
altmann gerry 
ambiguity parsing strategies computational models 
language cognitive processes 
altmann gerry steedman mark 
interaction context human sentence processing 
cognition 
berg george 
connectionist parser recursive sentence structure lexical disambiguation 
proceedings th national conference artificial intelligence 
cambridge ma mit press 
bock kay 
syntactic persistence language production 
cognitive psychology 
bock kay 
producing agreement 
current directions psychological science 
bock kay 
meaning sound syntax english number agreement 
language cognitive processes 
bresnan joan ed 
mental representation grammatical relations 
cambridge ma mit press 
bresnan joan kaplan ronald peters stanley zaenen annie 
cross serial dependencies dutch 
linguistic inquiry 
bridle john 
neural networks hidden markov models automatic speech recognition choice 
pietro renato de mori eds speech recognition understanding advances trends applications springer number nato asi series advanced science institutes series computer systems sciences 

brill eric 
simple rule part speech tagger 
proceedings rd conference applied computational linguistics 
trento 
charniak eugene 
statistical parsing context free grammar word statistics 
proceedings th national conference american association artificial intelligence providence ri july 

chomsky noam 
syntactic structures 
hague mouton 
chomsky noam 
aspects theory syntax 
cambridge ma mit press 
christiansen morten allen joseph seidenberg mark 
word segmentation multiple cues connectionist model 
language cognitive processes appear 
christiansen morten chater nick 
connectionist model recursion human linguistic performance 
ms christiansen morten devlin joseph 
recursive inconsistencies hard learn connectionist perspective universal word order correlations 
proceedings th annual cognitive science society conference 
mahwah nj lawrence erlbaum associates 
church kenneth 
stochastic parts program noun phrase parser unrestricted text 
proceedings nd conference applied natural language processing austin tx february 
association computational linguistics mit press 
cleeremans axel 
mechanisms implicit learning 
cambridge ma mit press 
cleeremans axel servan schreiber david mcclelland james 
graded state machines representation temporal contingencies feedback 
yves chauvin david rumelhart eds backpropagation theory architectures applications hillsdale lawrence erlbaum associates developments connectionist theory 

collins michael 
generative lexicalized models statistical parsing 
proceedings th annual meeting association computational linguistics madrid july 

crain stephen 
pragmatic constraints sentence comprehension 
ph thesis university california irvine 
crain stephen steedman mark 
led garden path context psychological parser 
dowty eds natural language parsing psychological computational theoretical perspectives cambridge university press acl studies natural language processing 

dang hoa joseph palmer martha 
associating semantic components levin classes 
proceedings workshop san diego ca oct 

dell gary chang franklin griffin 
connectionist models language production lexical access grammatical encoding 
cognitive science volume 
drescher gary 
minds 
cambridge ma mit press 
eisner jason 
efficient generation primitive optimality theory 
proceedings th annual meeting association computational linguistics th conference european association computational linguistics madrid july 
elman jeffrey 
representation structure models 
gerry altmann ed cognitive models speech processing cambridge ma mit press 

fisher cynthia hall geoffrey susan gleitman 
better receive give syntactic conceptual constraints vocabulary growth 
lingua 
fodor jerry 
language thought 
cambridge ma harvard 
frazier lyn clifton charles 

cambridge ma mit press 
freud sigmund 
project scientific psychology 
origins letters william fliess london 
pub 
standard edition xxx xxx 
freud sigmund 
pleasure principle 
london press 
standard edition xviii 
gleitman 
structural sources verb meanings 
language acquisition 
gold 
language identification limit 
information control 
grimshaw jane 
projection heads optimality 
linguistic inquiry 
hadley robert 
systematicity connectionist language learning 
mind language 
hadley robert 
systematicity revised reply christiansen chater niklasson van gelder 
mind language 
hale kenneth samuel jay 
argument structure lexical expression syntactic relations 
kenneth hale samuel jay eds view building cambridge ma mit press 

hinton geoffrey ed 
connectionist symbol processing 
cambridge mass mit press elsevier 
reprint artificial intelligence 
hinton geoffrey 
mapping part hierarchies connectionist networks 
artificial intelligence 
reprinted hinton 
hinton geoffrey 
preface special issue connectionist symbol processing 
artificial intelligence 
reprinted hinton 
hinton geoffrey zoubin 
generative models discovering sparse distributed representations 
philosophical transactions royal society london 

weak inadequacy context free phrase structure grammars 
ger de haan wim eds van kern foris dordrecht 
jelinek fred 
continuous speech recognition statistical methods 
proceedings institute electronic electrical engineers 
jordan michael 
serial order parallel distributed processing approach 
jeffrey elman david rumelhart eds advances connectionist theory speech hillsdale nj erlbaum 
xxx xxx 
joshi aravind schabes yves 
tree adjoining grammars lexicalized grammars 
nivat podelski eds definability recognizability sets trees elsevier princeton 
kelly michael 
sound solve syntactic problems 
psychological review 
kelly michael martin 
domain general abilities applied domainspecific tasks sensitivity probabilities perception cognition language 
lingua 
levin beth 
english verb classes alternations preliminary study 
chicago il university chicago press 
lyons john 
semantics vol ii 
cambridge cambridge university press 
mcclelland james kawamoto alan 
mechanisms sentence processing assigning roles constituents 
james mcclelland david rumelhart pdp group eds parallel distributed processing cambridge ma mit press 

spivey knowlton michael tanenhaus michael 
modelling influence thematic fit constraints online sentence comprehension 
journal memory language appear 
merialdo bernard 
tagging english text probablistic model 
computational linguistics 
risto 
subsymbolic natural language processing 
cambridge ma mit press 
risto 
subsymbolic parsing embedded structures 
ron sun lawrence eds computational architectures integrating neural symbolic processes dordrecht kluwer 

montague richard 
english formal language 
bruno ed milan di 

reprinted montague 
montague richard 
formal philosophy papers richard montague 
new haven yale university press 
edited richmond thomason 
pearlmutter barak 
gradient calculations dynamic recurrent neural networks survey 
ieee transactions neural networks 
piaget jean 
origins intelligence children 
new york norton 
pollack jordan 
recursive distributed representations 
artificial intelligence 
reprinted hinton 
pollard carl sag ivan 
information syntax semantics volume 
chicago csli chicago university press 
pollard carl sag ivan 
head driven phrase structure grammar 
csli chicago university press chicago 
prince alan smolensky paul 
optimality neural networks universal grammar 
science 
rumelhart david hinton geoffrey williams 
learning internal representations error propagation 
parallel distributed processing vol 
foundations cambridge ma mit press 

seidenberg mark 
language acquisition learning applying probabilistic constraints 
science 
seidenberg mark 
language processing language acquisition 
cognitive science volume 
sharkey noel sharkey amanda 
modular design parsing 
anton nijholt marc eds twente workshop language technology connectionism natural language processing enschede netherlands dept computer science university twente 

shieber stuart 
evidence context freeness natural language 
linguistics philosophy 
siskind jeffrey 
grounding language perception 
artificial intelligence review 
siskind jeffrey 
computational study cross situational techniques learning word meaning mappings 
cognition 
smolensky paul 
tensor product variable binding representation symbolic structures connectionist systems 
artificial intelligence 
reprinted hinton 
smolensky paul 
reconciling connectionism linguistics 
cognitive science volume 
spivey knowlton michael trueswell john tanenhaus michael 
context effects syntactic ambiguity resolution 
canadian journal psychology incomplete 
srinivas 
complexity lexical descriptions relevance partial parsing 
ph thesis university pennsylvania 
ircs report 
srinivas joshi aravind 
disambiguation super parts speech supertags parsing 
proceedings international conference computational linguistics coling kyoto university japan 
st john mark mcclelland james 
learning applying contextual constraints sentence comprehension 
artificial intelligence 
reprinted hinton 
steedman mark 
dependency coordination grammar dutch english 
language 
steedman mark 
structure intonation 
language 
steedman mark 
surface structure interpretation 
cambridge mass mit press 
linguistic inquiry monograph 
steedman mark altmann gerry 
ambiguity context reply 
language cognitive processes 
whitney tanenhaus michael 
dynamical models sentence processing 
cognitive science volume 
bruce 
iterative strategy language learning 
lingua appear 
trueswell john tanenhaus michael 
consulting temporal context sentence comprehension evidence monitoring eye movements reading 
proceedings th annual meeting cognitive science society 
hillsdale nj erlbaum 
trueswell john tanenhaus michael susan 
semantic influences parsing thematic role information syntactic ambiguity resolution 
journal memory language 
trueswell john tanenhaus michael kello 
verb specific constraints sentence processing separating effects lexical preference garden paths 
journal experimental psychology learning memory cognition 
williams christopher hinton geoffrey 
mean field networks learn discriminate temporally distorted strings 
david jeffrey elman sejnowski geoffrey hinton eds connectionist models proceedings summer school 
willshaw david 
association induction 
geoffrey hinton james anderson eds parallel models associative memory hillsdale nj erlbaum 

willshaw david buneman peter longuet higgins christopher 
associative memory 
nature 
