data compression dynamic markov modelling gordon cormack university waterloo nigel horspool university victoria method dynamically construct markov models describe characteristics binary messages developed 
models predict message characters basis data compression 
markov modelling technique combined guazzo coding produce powerful method data compression 
method advantage adaptive messages may encoded decoded just single pass data 
experimental results reported indicate markov modelling approach generally achieves better data compression observed competing methods typical computer data 
categories subject descriptors coding information theory data compaction compression computer communication networks data communications general terms experimentation algorithms additional key words phrases data compression text compression adaptive coding guazzo coding january authors addresses gordon cormack department computer science university waterloo waterloo ontario canada nigel horspool department computer science university victoria box victoria canada 
page 
data compression methods rely priori assumptions structure source data 
huffman coding example assumes source data consists stream characters independently randomly selected possibly unknown static probabilities 
assumption remarkably weak 
assume source data stream bits generated order markov model 
model sufficiently general describe assumptions huffman coding run length encoding compression techniques common 
survey provides data compression various algorithms employed 
assumption underlying firstorder markov model data ziv lempel coding technique :10.1.1.118.8921:10.1.1.14.2892
fact ziv lempel coding approaches optimal compression factor sufficiently long messages generated markov model 
new direction taken attempt discover algorithmically markov model describes data 
model constructed part message predict forthcoming binary characters 
markov model generates probabilities binary character zero 
probability estimate data coding scheme actual message character transfer new state markov model 
new markov state predict message bit 
probability estimates binary characters deviate reasonably accurate basis data compression method 
implementation directly control minimum redundancy coding method invented guazzo 
combination markov model generation guazzo encoding turned powerful method compressing data 
experimental results show outperforms competing compression methods wide margin 
reader may familiar guazzo approach data compression contains fairly long intuitive description method 
believe provide insights method readily available reading original 
brief description adaptive coding explain method automatically constructing markov models 
return guazzo coding method give detailed descriptions coding decoding algorithms 
give descriptions algorithms practical computer applications originally provided guazzo 
results obtained implementation data compression algorithm refer dmc dynamic markov compression 
comparisons dmc data compression techniques favourable 

guazzo coding algorithm guazzo algorithm generates minimum redundancy codes suitable discrete message sources memory 
term message source memory describe message generated order markov model 
practical situations messages exhibit degree correlation adjacent characters corresponds message source having memory 
introduce subject simple markov model example 
example assume source message string consists binary digits followed probability followed probability followed probability followed probability assume digit message equally 
messages generated model property zero digit followed zero 
similarly digit followed digit 
diagram order markov model exactly encapsulates probabilities shown 
page 
example markov model working markov model examine known huffman coding ideal proceed see guazzo coding achieve near optimal codes 

huffman coding message sources memory considering huffman coding messages generated markov model 
huffman coding directly applied binary source alphabet 
having provide separate codes zero bit bit implies compression impossible 
usual way problem create larger source alphabet 
easily done grouping message characters 
example choose groups bits yielding alphabet size 
analysis markov model involving computation equilibrium state probabilities calculated probability occurrence character new source alphabet 
probabilities derive huffman codes 
table shows probabilities corresponding codes assuming bits generated markov model grouped 
source form probability huffman code table 
huffman codes bit groups page huffman codes triples encode long message generated markov model moderate degree compression achieved 
fact long message average compacted approximately original size 
number considerably larger information theoretic lower bound 
simple calculation entropy information source shows compression original size achievable 
reasons huffman code achieve lower bound 
reason huffman codes free redundancy character frequencies source alphabet integer powers 
second reason huffman code advantage correlations bits grouped 
example source message contains adjacent groups 
huffman scheme encodes groups independently 
ignores correlation bit group bit second group 
second group encoded suboptimal manner 
problems huffman scheme ameliorated choosing larger groups bits constructing source alphabet 
choose larger larger groups coding efficiency comes closer lower bound alphabet size grows exponentially 
storage tables needed hold huffman encodings grows exponentially computational cost creating tables infeasible 
alternative expansion alphabet size multiple sets huffman codes 
choice set codes message character determined preceding message characters 
approach tuned achieve reasonable compromise compression performance total amount storage needed hold tables huffman codes 

guazzo encoding applied markov model guazzo method suffer defects noted huffman coding 
sufficiently long messages method generate encodings come arbitrarily close informationtheoretic lower bound 
step understanding binary version guazzo method consider output encoding binary fraction 
example output encoding begins digits consider binary number begins 
number value close expressed decimal fraction 
job encoding algorithm effect choose fractional number zero encodes entire source message 
guazzo algorithm access markov model shown trace algorithm see choose encoding indefinitely long source message begins 
initially guazzo algorithm freedom choose binary fractions lie inclusive 
guazzo algorithm determines markov model digit source message equally zero 
divides space binary fractions equal halves choosing fractions closed interval 
represent source message begins fractions closed interval 
represent messages 
source message begins zero pick sub interval 
binary fractions selected subrange digit effectively determines output digit zero 
source digit takes state labelled markov model 
state digit twice zero 
guazzo algorithm determines range available binary fractions subdivided parts ratio 
split sub interval page 
represents source messages sub interval 
represents messages 
sub intervals exactly twice range second 
second digit source message restricted second interval 
source digit leads back state model split available range message encodings ratio 
proceeding example level detail skip steps 
continuing lines guazzo algorithm eventually determine source message represented binary fraction interval 
lower upper bounds interval fractional digits encoding algorithm generated digits 
consider requirements practical computer implementation clear interval bounds computed binary fractions containing unlimited numbers digits 
implementation requires small finite number bits retained calculations interval bounds essential 
defer consideration practical issues section 
add intuition intuitive description th guazzo algorithm offer simple observation 
way available space binary encodings subdivided step algorithm distributes encoded messages evenly possible space 
important distribute encodings evenly encodings unnecessarily close values usually require bits differentiate values farther apart 
encoding process easily reversible 
decoding algorithm access markov model information source encoder 
starts constructing interval 
possible encodings subdividing way encoder subdivided 
inspection leading bit encoded message determines partitions encoder determines source digit 
digit known decoding algorithm select appropriate partition repeat division parts 
inspection encoded message determines second digit 

adaptive coding data compression traditionally implemented pass technique 
initial pass source message performed discover characteristics 
knowledge characteristics second pass perform compression 
example huffman coding pass count frequencies occurrence character 
huffman codes constructed second pass performs encoding 
ensure compressed data decoded fixed coding scheme details compression scheme included compressed data 
pass implementation new data compression technique easy develop prefer proceed directly pass adaptive data compression implementation 
pass implementations preferable require entire message saved line computer memory encoding take place 
pass technique data compression practice achieves compression close obtained pass techniques 
basic idea encoding scheme changes dynamically message encoded 
coding scheme th character message characteristics preceding characters message 
technique known adaptive coding 
example adaptive versions huffman coding proposed implemented 
practice adaptive huffman coding achieves data compression differs conventional pass huffman coding expense considerably computational effort 
coding adaptive coding technique 
basic idea ziv lempel coding group characters message may replaced pointer earlier occurrence character group message 
algorithm implemented way faster adaptive huffman coding achieving better data compression 
drawback relatively page large amount main memory required achieve compression 
guazzo coding algorithm eminently suitable adaptive coding scheme 
aspect algorithm need change dynamically source probability estimates message characters 
step encoding process algorithm requires estimate source digit zero 
matter guazzo algorithm probability estimates derived static markov model dynamically changing markov model 
dynamic model set states transition probabilities may change message characters seen far 
section explains method dynamically building markov model source message 
decoding message produced adaptive coding implementation guazzo algorithm prove problem 
decoding algorithm needs re create sequence changes dynamically changing markov model encoding algorithm 
decoding algorithm sees exactly sequence un encoded digits encoding algorithm difficulty 

dynamic construction predictive markov models order markov model characterized directed graph probabilities attached graph edges 
distinguish different aspects problem creating markov model automatically 
part determination suitable probabilities place edges graph 
part determination structure graph 
consider parts separately easier problem choosing probabilities transitions model 

choosing edge probabilities explanation assume markov model probabilities attached transitions 
assume starting point correlations exist message character immediately preceding characters 
reading binary digits source message corresponding transitions model counting times transition model taken 
counts provide reasonable estimators probabilities transitions taken times 
precisely transition state digit taken times transition digit taken times probability estimates reasonable prob digit current state prob digit current state visited state confidence probability estimates 
unfortunately adaptive coding technique requires probability estimates transition counts grown significant values 
furthermore formulae yield transition probabilities times visit state 
careful supply guazzo encoding algorithm probability bit immediately generate infinite length encoding bit observed 
ways probability formulae adjusted take account concerns 
method implementation simplest 
adjusted formulae prob digit current state prob digit current state positive constant 
small values equivalent having confidence probability estimates small sample sizes large values correspond having little confidence 
hand adaptive algorithm learn characteristics source file faster small page values expense making poor predictions 
large files compressed choice largely irrelevant 

building state transition graph method probabilities attached transitions markov model change dynamically just explained 
explained method set states model changes dynamically 
try explain method consideration simple scenario 
suppose partially constructed model includes states named 
drawn 
shows transitions transitions model enters state contextual information lost 
effect forget reached state quite possible choice state correlated previous state easy way learn correlation exists duplicate state call process cloning generating new state creates revised markov model drawn 
change model counts transitions updated state reached counts updated reached model learn degree correlation states states 
cloning process performed fact correlation previous state state exists little lost 
simply model complicated states probability estimates susceptible statistical fluctuations state visited 
correlations exist improvements probability estimates dramatic 

part markov model 
markov model cloning page page carrying model possible choice state correlated previous state correlated states immediately state state case cloning state cloning state cloning state state enable model discover correlations 
general cloning performed longer range correlations discovered predictive purposes 
light previous observation implementation clones states soon practicable 
practicable depends factors 
factor state visited reasonable number times 
visited times probability estimates digit unreliable 
cloning operation estimates reliable 
second factor predecessor state state cloned 
gained cloning operation help discover new correlations 
generalizing second principle little point cloning state transitions state reasonably high counts 
example suppose considering cloning state predecessors transition taken times transition taken twice 
clearly benefits cloning minimal 
account considerations discussed cloning markov model states performed algorithm 
pascal language expressing algorithm 
algorithm contains parameters control cloning process 
min cnt represents minimum number times state visited cloned 
second min cnt represents minimum number transitions state predecessors current predecessor occur cloned 
algorithm shows transition counts state cloned 
counts laws maintained 
simplifies logic needed determine state visited 

starting stopping model construction important questions answered 
question markov model 
simple answer need minimal model capable generating message sequence 
contains state 
transitions state digits loop back single state 
model diagrammed 
operation cloning algorithm single state model rapidly grows complex model thousands states 
practice benefit gained slightly trivial initial model 
computer data byte word aligned 
correlations tend occur adjacent bytes adjacent bits 
model corresponds byte structure process learning source message characteristics occurs faster leading slightly better data compression 
simple model byte structure states arranged binary tree transitions leaf node returning root tree 
general shape tree smaller number nodes diagrammed 
alternative tree structured initial model simple cycle states 
transitions state connected state cycle 
model corresponds byte structure 
achieves slightly poorer compression performance achieved tree structured model 
second question cloning process halted 
halted bound amount memory needed compression algorithm 
hand completely halted lose ability algorithm adapt characteristics source message change 
possible solution set limit number states 
limit reached markov model discarded initial model 
drastic solution effective appear 
drastic variation approach easily implemented 
retain mean count transitions state predecessors count transitions state successors 
practice counts may differ transition current state transitions taken 
page 
cloning algorithm state state reached transition digit trans cnt number observations input state state number current state state largest state number far min cnt minimum number visits state eligible cloning min cnt minimum number visits state predecessor current state eligible cloning 
eof read read input bit trans cnt state trans cnt state nxt state state nxt cnt trans cnt nxt trans cnt nxt nxt cnt min cnt nxt cnt trans cnt state min cnt state state new state obtain new state number state state new ratio trans cnt state nxt cnt state new new state nxt trans cnt new ratio trans cnt nxt trans cnt nxt ratio trans cnt nxt nxt new state nxt 
initial state markov model page 
initial markov model bit characters bytes source message read cyclic buffer 
limit number states reached model discarded 
adding encoded message new model constructed processing bytes buffer 
yield new model relatively small number states corresponds characteristics message bytes 
loss data compression performance occurs storage loss great practice compression algorithm retains adaptability 

practical usage guazzo algorithm guazzo encoding previously discussed regard problems associated practical implementation 
main problem areas addressed 
problem lower upper bounds interval rational numbers algorithm proceeds apparently calculated greater precision 
solution adopted guazzo weaken requirement bounds calculated precisely 
step encoding algorithm interval divided parts 
division performed exactly proportions ratio probabilities source digit zero great loss 
coding technique works sense decoding uniquely performed 
lost little coding efficiency 
guazzo proposed fidelity criterion determine precisely division point subranges calculated 
tighter criterion better message encoding computational expense having calculate division point accurately 
implementation algorithm chose practical approach retaining significant bits calculation division point conveniently fit computer word 
soon message bits generated bounds interval identical leading digits removed variables record interval bounds 
may removed logical shift left operations 
fairly constant degree accuracy significant bits maintained implementation 
algorithm organized lines 
algorithm expressed pascal additional operators bit page manipulation 
shl shr left right logical shifts bitwise logical operations 
code represents number significant bits algorithm uses range calculations 
implemented computer bit integers double length integer multiplication division instructions chosen 
second issue addressed guazzo 
assumed message source 
clearly suit typical computer applications data compression 

guazzo encoding algorithm meanings variables binary message interval lb hb inclusive 
mp dividing point interval 
lb hb mp scaled integers give relative probabilities message bit 
shl shl lb hb repeat point knowledge markov model external source information estimate relative probabilities source digit 
unnormalized probabilities assigned statements 


calculate range partition mp lb hb div mp lb mp mp mp mp force rightmost bit assertion lb hb eof goto read read bit lb mp pick upper part range hb mp pick lower part range lb hb write lb shr output bit lb lb shl remove bit hb hb shl false output rightmost bit mp mp mp write mp shr output bit mp mp shl remove bit page example previous section assumed source message infinite began sequence discovered encoded message infinite sequence interval 
suppose source message contains digits listed 
encoding algorithm attempts read eighth bit receives input indication 
algorithm 
encoded message finite sequence 
encoded messages correctly decoded 
decoding algorithm treats finite encoded message normal binary fraction treats synonymous messages decoded correctly 
binary fraction exact value decoding algorithm continue sub dividing intervals generating nonsensical message bits forever 
solution problem effect consider encoded message sequence representing range values 
words treat encoded message xxxx represents unknown bit value 
decoding algorithm sub divide intervals generate message bits long unambiguous choice half interval value xxx belongs 
soon choice depends value digits denoted decoding algorithm halts 
clearly responsibility encoder sure bits encoded message decoder reconstruct entire original message ambiguity 
detail taken care places algorithm 
algorithm label algorithm simply outputs remaining bits held variable mp including rightmost bit 
mp holds tail binary encoding partition point subranges 
decoding algorithm discover exact match input data partition point choose upper part lower part interval 
superfluous output bit created 
note just outputting rightmost bit mp 
bit output decoder able deduce encoded message greater equal mp cause decoder generate spurious bit 
sufficient observe superfluous output bit created decoder 
guarantee decoder entire source message losing bits 
guarantee message bits lost force calculation mp rightmost bit 
algorithm rightmost bit mp output 
maximizing number bits output encoder disambiguates pending selections interval partitions decoder 
decoding algorithm closely mirrors encoding algorithm 
delays immediately decide select upper lower half range partition calculations proceed step calculations encoder 
decoding algorithm 
implementation difficulty simplicity ignored algorithms shown figures 
algorithms assume message encoded arbitrary number bits 
practice encoded message usually contain integral number bytes words 
simply truncate encoded message dropping bits decoder may able reproduce bits original message 
append extra bits encoded message certainly converted spurious bits decoded message 
provided original source message contains integral number bytes solution problem 
intermediate results bits generated computing mp 
page 
guazzo decoding algorithm meanings variables lb hb mp meanings 
msg holds sequence encoded message bits sequence begins high order 
bit holds single bit position significant bit sequence msg 
shl shl lb hb msg bit shl repeat point estimate relative probabilities decoded bit zero 
estimation process uses exactly information available encoder point original source message 


calculate range partition mp lb hb div mp lb mp mp mp mp assertion lb hb repeat msg bit hb mp msg mp lb mp eof goto exit file read read bit bit bit shr msg msg bit write output bit lb hb lb lb shl hb hb shl msg msg shl bit bit shl false exit finished decoding page solution append extra bits source message encoding process 
source bits chosen encoding 
encoding algorithm computes mp dividing point interval lb hb 
determines lb hb leading bits common mp tie 
lb extra bit chosen zero chosen 
apart way extra bit generated treated message bit encoding purposes 
choice source bit new bit appended encoded message 
encoded message may safely truncated 
bits lost encoded message may cause bits lost decoded message 
bits lost decoded message extra bits added encoder 
extra bits chosen encoding process loss encoded message bit worst cause loss bit decoded message 
message decoded zero extra bits may message 
decoder ignores incomplete byte message genuine information lost spurious information gained 

results different data compression algorithms tested variety data files berkeley unix system running vax 
files chosen large size making prime candidates compression fairly typical files system 
file types included formatted documents unformatted documents input troff formatting program program source code language executable object files 
compared new coding algorithm dmc dynamic markov compression compression programs 
program adaptive huffman coding algorithm implemented unix compact command 
noted program yields compression results indistinguishable pass non adaptive huffman coding algorithm 
program variation due welch ziv lempel compression algorithm 
version dmc started tree structured initial model subject memory size constraints 
furthermore set parameters control cloning states markov model values give results 
say values discussing main results 
compared different compression programs data files 
resulting compression factors shown table 
compression factor computed ratio size encoded compressed file size original file 
example table indicate file compressed fifths original size 
observed results overwhelmingly favour dmc algorithm proposed 
row table demonstrates algorithm handles non homogeneous data effectively competing methods 
algorithm flexible continually adapting data 
adaptive huffman algorithm ziv lempel algorithm adapt change file characteristics take relatively long time adapt 
adaptability important applications compressing data sent communication link 
data normally formed long series short unrelated messages expect characteristics data change frequently return practical details compression algorithm 
dynamic markov modelling method described earlier free parameters min cnt min cnt algorithm 
experiments shown parameters chosen cloning occurs early 
results experiment shown table 
experiment set parameters equal tried values 
smaller values lead earlier version compress program 
program originally authored thomas university utah enhanced poor compression results adaptive huffman lzw algorithms vax unix object files due non homogeneous structure 
files sections including instruction area part data area part relocation dictionary symbol table 
page compression algorithm source file adaptive huffman ziv lempel dynamic markov compact compress dmc formatted text unformatted text object code source code heterogeneous data formatted manual entry csh command bytes 
unformatted version csh manual entry bytes 
object code csh command bytes 
source code finger command bytes 
file formed concatenating various data files text files bytes 
table 
comparative compression results cloning states markov model 
table shows small values give best compression factors 
similar results obtained parameters varied independently 
general observation promotion rapid growth model leads best results 
results involving compression algorithm reported parameter values 
detail considered exactly number states markov model limited 
table illustrated number states exceeded input file holding fewer bytes 
scheme limiting model size previously outlined 
impose upper limit number nodes graph 
graph grows reach limit entire graph discarded start initial small graph 
avoid losing compression performance compression algorithm structure source data buffer bytes source input 
bytes re build reasonably small markov model storage reclamation 
leads related practical questions 
compression performance affected number states model limited 
second large buffer 
table may helpful providing answers questions 
expect compression performance improves maximum graph size increased buffer size increased 
best choices limits depend trade offs compression efficiency storage size execution speed 
increasing maximum graph size improves compression performance reduces execution time storage frequent increasing buffer size improves compression performance increasing storage requirements execution time model rebuilding performed storage reclamation occur frequently 
buffer size large maximum graph size may reached rebuilding graph 
page parameter max 
graph size compression values number nodes performance subject file contained characters ascii text 
terminal capability database 
compression program ran storage graph nodes experiment parameter values 
program compressed source file aborted 
table 
varying cloning parameters buffer maximum number nodes graph size bytes bytes bytes bytes subject file contained characters ascii text 
terminal capability database 
results rightmost column storage occurred 
results leftmost column occurred 
table 
choosing limits markov model size 
dmc general data compression algorithm best knowledge achieves compression results better results reported literature 
text files example compressed extent character requires little bits average 
depending file observed figures range bits 
current implementation dmc algorithm requires great amount computation requires considerable amount memory 
needs done see possible find compromise time space compression efficiency 
experimentation needed study best choices values parameters control cloning process dmc algorithm 
course reason require values static 
slightly different direction research lies generalizing algorithm compress twodimensional images generated raster type devices 
problem model take account correlations adjacent scan lines adjacent page points scan line 
see applications dynamic markov modelling method problems data compression 
example computer system method predict accesses records database predictions prefetch records 
programming help experiments provided peter smith 
financial support research received natural sciences engineering research council canada 

cormack data compression data base system 
appear commun 
acm 

cormack horspool algorithms adaptive huffman codes 
inf 
proc 
letters march 

gallager variations theme huffman 
ieee trans 
inf 
theory nov 

guazzo general minimum redundancy source coding algorithm 
ieee trans 
inf 
theory jan 

huffman method construction minimum redundancy codes 
proc 
ire 

mcmaster line manual entry source code compact commands 
berkeley unix bsd 

practitioner guide data base compression 
inf 
systems 

welch technique high performance data compression 
ieee comput 
june 

ziv lempel universal algorithm sequential data compression 
ieee trans 
inf 
theory may 

ziv lempel compression individual sequences variable rate encoding 
ieee trans 
inf 
theory sept 
