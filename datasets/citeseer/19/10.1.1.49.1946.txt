trec filtering track description analysis david hull xerox research centre europe de france hull xrce xerox com article details experiments conducted trec filtering track 
filtering track extension routing track adds time sequencing document stream set evaluation strategies simulate immediate distribution retrieved documents 
introduces adaptive filtering designed simulate line sequential filtering documents 
addition motivating task describing practical details participating track document includes detailed graphical presentation experimental results attempts analyze explain observed patterns 
final section suggests ways extend current research experiments 
increasing evidence text filtering critical tool searching managing flow data information age 
new companies appearing daily offer push services intelligent agents centered core technology content filtering 
trec routing task served forum development algorithms 
uses technology changed intervening years routing problem gradually diverged common applications 
filtering track created provide realistic simulation line time critical applications technology 
start describing routing task defined trec definition starting point filtering task 
simple definition routing task topic description large collection documents sample evaluated relevant relevant topic construct query profile routing function score rank new documents likelihood relevance 
filtering builds routing task introducing concept time 
routing task traditionally evaluated average precision curves rank measures complete ranked list incoming documents determine performance 
list created documents scored ranked means system performance measured function time 
underlying model tasks stream transient documents compared fixed set query profiles 
means routing task simulates non interactive process user looks documents 
realistic situation users examine documents periodically time 
actual frequency user interaction unknown task dependent 
attempt simulate particular task allow batching partial ranking document set filtering track operates opposite spectrum assuming user wants notified potentially interesting document immediately arrives 
leads definition filtering task topic description incoming document stream possibly small historical database documents evaluated relevant relevant topic construct query profile filtering function binary decision accept reject new document arrives 
decision accept reject document independent subsequently arriving documents evaluation rank measures appropriate 
filtering results consist unordered sets documents analyzed set evaluation measures 
background motivation filtering track scenarios trec filtering track description 
recognize considerable gap trec filtering experiments goes operational systems 
particular task fairly general user interaction evaluation process 
control system performance area efficiency documents filtered second scalability 
certain unavoidable trade offs filtering track compatible standard trec experimental framework 
advantages trec approach foremost strong quantitative evaluation system effectiveness 
trec task description trec filtering task substantially revised track definition experiments conducted previous years goals remain basically 
basic goals year track move realistic filtering task retaining strong quantitative evaluation broaden range possible filtering experiments 
routing task criticized unrealistic number reasons 
average precision routing evaluation measures assume post hoc evaluation conducted entire test document collection 
second filtering thought temporal process time ordering date information associated documents 
third training documents come variety different collections correspond source new test documents 
fourth size coverage training set greater find realistic applications 
fifth training documents come variety different systems different filtering systems tested 
points addressed previous iterations filtering track 
second third point addressed year selection training test documents 
fourth fifth points addressed adaptive filtering defined section 
topics documents corpus trec filtering experiments comes foreign broadcast information service fbis selects translates text documents transcripts various broadcast print publications 
training documents date early test documents date come late 
documents date stamps attached ordered date 
date stamps represent release date fbis version release date original source document 
due practical constraints part data released year documents cleanly separated date 
test documents training documents test documents training documents 
systems required filter test documents date order allowing researchers dimension time 
addition training test documents come source viewed realistic filtering applications 
topics trec filtering experiments listed topics year routing filtering experiments topics new year listed topics built specially routing filtering task year 
new topics incomplete relevance judgements fbis training data 
particular top documents retrieved nist zprise system judged relevance 
different types topics terms relevance judgements 
old topics hundreds relevance judgements documents retrieved different sources new topics fewer relevance judgements documents retrieved source 
explore impact dichotomy performance different systems 
topics routing task 
note routing systems allowed relevance judgements topics trec subcollections filtering systems limited fbis training documents 
represents important divergence routing filtering experiments happening time year 
evaluation filtering systems expected accept reject document arrives assumed user may look accepted documents immediately 
output filtering system treated unordered set documents 
means evaluation measures ranked set documents precision recall curves appropriate 
apply set evaluation metrics utility average set precision asp 
utility average set precision utility assigns value cost document retrieved retrieved relevant relevant shown contingency table relevant relevant retrieved retrieved utility variables refer number documents category 
utility parameters determine relative value possible category 
positive utility parameter thought value document category negative utility parameter cost classifying document category 
larger utility score better filtering system performing query profile 
trec test different settings utility parameters 
retrieve rel 
retrieve rel filtering utility function equivalent filtering estimated probability relevance 
description shows appropriate probability thresholds correspond utility functions 
readers find general formula converting utility function probability threshold lewis derivation formula general book decision theory 
average set precision asp defined product precision recall asp precision recall 
think asp variant average precision 
average precision computes precision position relevant document ranked list takes sum divides total number relevant documents 
average set precision calculated unordered document sets uses precision entire retrieved place precision rank position 
calculation number multiplied number relevant documents retrieved divided total number relevant documents 
utility ideal measure judging performance filtering systems number reasons 
utility scores vary widely topic topic number relevant documents valid way normalize meaning scores easily averaged compared topics 
second utility treats relevant documents equally important matter retrieved exist topic 
sense diminishing returns counter intuitive situations 
third user probably different utility function topic topic difficulty number relevant documents 
practical administrative reasons difficult define separate utility function topic 
asp problems 
particular relevant documents returned relevant documents exist system receives score zero irrespective size retrieved set 
means retrieving documents equivalent retrieving arbitrary number non relevant documents 
clearly result preferred major part challenge text filtering knowing retrieve documents 
noted van rijsbergen measure suffers problem 
far developed general measure set retrieval effectiveness suffer drawbacks 
better approach may select particular task particular user model define model task specific measure 
pooling vs sampling general size retrieved set unbounded making accurate evaluation performance difficult topics regardless measure 
traditional pooling approach document assessment augmented random sampling strategy 
pooling top retrieved documents run topic merged create single document pool assessed 
documents appear pool assumed relevant 
strategy reasonably fair participants results performance estimates biased downwards 
filtering retrieved set unranked simply select top documents 
approach filtering select random sample size retrieved set system 
retrieved set smaller documents selected 
pooling ideal filtering topics retrieved sets larger filtering pool lower quality documents randomly sampled large retrieved set obtained selecting top ranked documents 
fortunately effect mitigated somewhat trec experiments routing runs ranked retrieval contribute pool 
second topics large retrieved sets ones tend relevant documents suffer bias due incomplete assessment 
fortunately know sample theory proportion relevant documents simple random sample unbiased estimate proportion relevant documents population 
utility function convert estimate proportion relevant documents directly estimate utility formula eq gamma size sample number relevant documents sample terms defined 
nice property sampling approach evaluation calculate standard error utility estimate eq se gamma gamma gamma gamma unfortunately sampling provide estimate recall necessary utility asp measures estimated pooling 
details mathematics sampling see lewis 
sampling provides unbiased estimates utility price 
lot information thrown away relevance judgements documents sampled systems evaluated sampled estimates tend variation 
basic task summary participating group submit filtering runs evaluation measures utility utility asp 
participating groups required submit utility run utility run 
runs classified categories automatic run uses fully automatic methods profile construction updating 
include automatic learning test documents filtered 
manual run uses manual techniques profile construction including making additional relevance judgments training documents 
manual intervention information test documents allowed automatic learning permitted 
manual feedback run uses manual techniques updating profiles previously viewed test documents 
run may may manual techniques profile construction 
practice manual run submitted submitted runs manual feedback 
previous trec routing filtering experiments consistent advantage manual techniques profile construction submitted runs treated single category 
topics evaluated documents cost terms human effort additional manual assessment justified returns 
addition evaluation measures optional subtasks groups participate described subsections 
due assessment constraints nist promised evaluate maximum documents topic participating group 
documents obtained follows 
mandatory runs contributed assessment pool 
utility set topic greater documents documents sampled assessment 
documents assessed 
furthermore sufficient number document utility set sampled bring total set size documents 
union utility utility sets documents group submitted ranked retrieval run see subsection sampling program went list rank order adding new documents sample size extracted 
final step added improve quality document pool possible assessment constraints 
comparison ranked retrieval view routing task subset filtering task routing consists defining document scoring function tends rank relevant documents non relevant ones filtering consists building thresholding function top scoring function optimize set evaluation criterion 
past systems participating filtering participated routing scoring function tasks 
systems follow model try extent separate performance thresholding function performance scoring function 
participation subtask optional priori reason system scoring algorithm filtering system 
binary rule algorithm 
addition complex systems may change scoring function view test documents meaning scores generated document stream may comparable scores generated document stream 
trec try learn issue applying different simple tests 
test take ranked list documents returned scoring function calculate score obtained selecting optimal threshold 
computed exhaustive search ranked list relevant documents 
test ask simple question system overestimate underestimate threshold 
accumulating information full topic set gain general sense system biased retrieving documents 
second test compare performance optimal thresholds observed performance determine changes relative ranking participating systems 
adaptive filtering previous versions trec filtering task viewed unrealistic provide training data training data comes previous search results different systems different search algorithms 
practice systems expect relevance judgements documents responsible retrieving 
adaptive filtering designed model situation 
task system starts topic description evaluated documents 
documents arrive sequentially system update query profile response previously viewed documents 
addition document retrieved immediately evaluated relevance information passed system 
possible trec framework provide new relevance judgements filtering systems see documents 
interactive component simulated training data previously released relevance judgements 
model relevance judgement available hidden system decides retrieve document 
relevance judgements documents revealed system 
note test documents evaluated fashion relevance judgements immediately available system user providing manual feedback 
means systems may wish evaluate document likelihood relevance value training observation improve filtering performance 
filtering interesting complex task 
year topics partially evaluated training document collection suitable adaptive filtering 
subtask trec topics fully evaluated 
systems may choose wish rest trec document collection excluding fbis training document set generate collection frequency statistics idf data structures automatically generated thesauri prior information 
systems may decide wish treat unevaluated training documents relevant assume user simply declined judgement situation 
performance judged training test document set results compared directly filtering tasks 
adaptive filtering runs operate substantial disadvantage access relevance judgements documents retrieve 
performance training set count prevent groups retrieving far documents training set increase pool relevance judgements available system improve performance test documents 
task experimental stage participants free choose evaluation measures 
trec results trec filtering track participating groups submitted total runs divided follows groups runs total asp ranked adaptive note sum runs column adds total number runs groups submitted run evaluation measure 
participating groups abbreviations run identifiers labs research att australian national university anu anu flt city university london city city daimler benz ag research center ulm daimler benz queens college cuny cuny siemens ag siemens university california berkeley berkeley university massachusetts amherst umass inq university north carolina unc isf 
summary approaches section briefly describe techniques groups filtering track information please consult individual participants papers included volume 
participants treat filtering special case routing ranked retrieval system followed thresholding document score 
vast majority systems basic technique finding threshold observe score training set evaluation measure maximized score threshold test set 
exceptions noted 
system logistic regression convert scores probability estimates filter probability estimates directly 
major differences systems come scoring algorithms 
builds feature set terms phrases adjacent pairs non adjacent pairs term weights rocchio expansion 
optimizes feature weights average precision technique called dynamic feedback optimization dfo 
experimental run att fe takes lowest scoring half relevant documents highest scoring non relevant documents initial query constructs second query methods 
final routing profile weighted combination queries 
anu selects terms phrases adjacent pairs topics training documents independent algorithms 
topic selection algorithm assigns higher weight title words words high frequency words capitals 
document selection algorithm scores terms difference probability occurrence relevant documents probability occurrence non relevant documents 
number features relative weight feature set returned algorithm passed generalized reduced gradient grg nonlinear optimization program optimizes parameters respect utility measures 
city orders terms adjacent pairs weight probabilistic model applies forward stepwise feature selection fixed increase average precision selection criterion 
term weights optimized methods deterministic adhoc weight adjustment simulated annealing procedure 
case training set partitioned halves term selection half weight optimization half 
partitions inverted procedure repeated 
additional repetitions generated splitting training set randomly different partitions 
resulting term sets merged build final query profile 
difference runs city optimizes weights half database selects thresholds half city optimizes weights halves selects thresholds entire database 
uses probabilistic model pass term selection followed second pass rocchio algorithm term selection term weighting 
thresholds chosen logistic regression utility optimal score training documents measures 
tested threshold selection methods training set picked worked best measure 
method divided training data parts applied terms selection algorithms part separately 
terms appeared sections retained final profile 
section material provided groups draft papers 
daimler benz generates different feature sets grams grams substrings extracted terms stemming variant selected measures chi square tf idf correlation relevant documents 
feature set transformed reduced dimension filtering eigen decomposition feature covariance matrix principal component analysis equivalent latent semantic indexing 
filtering treated class categorization problem topic category single classification rule built categories simultaneously linear regression 
final profile created merging decision vectors constructed feature set 
cuny combines runs terms term pairs 
extracted topics terms term pairs 
extracted training documents submission 
difference submissions lowers threshold reflect fact subsequent relevant documents similar training documents profile creation threshold selection 
siemens uses true filtering system computes document scores term correlations model 
feature set single terms topic statement 
berkeley performs term selection chi square measure positive association relevance significance cut 
terms put expanded query scored training documents 
addition terms ranked log odds relevance top terms selected 
rsv scores step log term frequencies terms extracted second step passed parameters logistic regression produces final probabilistic scoring function 
umass builds initial query topic statement extracts top scoring word passage evaluated training document 
feature set consists top terms phrases pairs word window difference probability occurrence relevant documents probability occurrence non relevant documents 
features weighted weights optimized average precision dynamic feedback optimization 
adaptive filtering umass uses true filtering system builds profile initial topic statement updates incremental rocchio algorithm 
set mid point average score relevant documents average score non relevant documents 
unc creates information space topic applying principal components analysis eigen decomposition occurrence matrix constructed query terms 
training documents query terms located space 
threshold utility chosen minimum distance evaluated document topic statement information space 
document query terms retrieved runs utility 
comparative evaluation evaluation utility measures difficult compare performance topics 
simple averaging utility measure gives retrieved document equal weight means average scores dominated topics large retrieved sets microaveraging comparative evaluation average rank measure treats topics equally 
measure computed steps topic rank systems order performance average ranks system topics 
means larger average rank score better system performing respect competitors 
table presents pseudo example ranking process 
average rank measure advantages disadvantages 
positive side utility rank table pseudo example ranking runs topics 
topics equally important determining system performance meaning scores insensitive outliers topics may high variation due random factors 
average rank scores generated set systems directly comparable different evaluation measures different retrieval tasks 
allows global comparative evaluation situations difficult case utility 
negative side topics equally important meaning topics large variation reflect real differences systems receive higher weight 
absolute standard performance scores meaningful relative performance different systems 
results depend systems compared adding removing system change results systems 
average set precision asp comparable topics evaluated traditional average score average rank measure 
mentioned previously asp distinguish systems retrieve relevant documents 
ranks allows introduce tie breaking procedure 
systems asp topic ranked inverse order number documents retrieve 
non parametric statistical tests determine significance average rank differences systems 
order keep results simple readable large number experiments competing systems limit pairwise comparisons respect best performing system 
experiment apply different tests conservative test non parametric variant newman multiple comparisons test powerful test non parametric variant significant difference lsd test 
information statistical tests performance characteristics trec style ir experiments please consult nist technical report 
alpha level set corresponds error rate pairwise comparison lsd 
true pairwise error rate newman order magnitude smaller 
evaluation results figures evaluation results trec filtering experiments 
contains experimental runs drawn page 
dashed lines drawn link runs system 
cases correspond underlying retrieval algorithm different threshold thresholding strategy 
vertical arrows link systems distinguishable top performing system statistical significance tests described previous section 
longer arrow corresponds conservative test 
mention groups problems submitted runs 
cuny optimized wrong utility thresholds utility runs accurate 
daimler benz understand sampling evaluation artificially truncated runs documents 
higher impact utility asp utility 
groups opportunity submit revised results revised unofficial comparison 
compares system performance utility 
system rankings remain small shifts 
results indicate lot interaction system performance filtering threshold 
compares system performance utility measures average set precision 
systems errors runs lot differences 
notice umass inq siemens better utility measures asp 
systems retrieved uniformly small number documents topics 
small retrieved sets utility measures rarely asp 
compares evaluation average score evaluation average rank asp 
average rank scores rescaled match points average precision scores 
find measures yield basically results 
compares pooling versus sampling strategy evaluation 
note results topics significant sampling took place 
topics retrieved sets documents pooled sampled results virtually identical 
score differences topics minimal fair conclude pooling introduces bias favor individual system 
cases pooled scores fall confidence intervals sampled results 
topics pooled scores fall outside sampling confidence bounds regularity 
cases pooled estimate significantly lower sampled estimate indicating probably fair number unevaluated relevant documents topics 
filtering runs groups allowed submit ranked list documents output filtering system thresholding 
comparing ranked list evaluation functions determine optimal threshold retrospective fashion 
compares observed performance performance choosing optimal threshold utility 
note little difference graphs 
surprising participants strategy threshold selection 
direct comparison common strategies threshold selection choosing retrieval score optimizes performance training set logistic regression 
logistic regression worked better utility optimal score worked better utility 
worked 
detailed analysis threshold bias section 
trec filtering topics divided categories old topics heavily evaluated training fbis data trec new topics evaluation training data limited roughly documents retrieved prise system 
breaks average rank scores old new topic sets utility 
performance remains roughly constant systems ignoring runs errors 
run jumps upward rank position 
topic sample small say difference statistically significant speculate approach adopted city advantage small training sets 
show jump result easily explained 
uses half training data optimization reserving half threshold selection 
smaller training sets expect ignoring half data offset advantages inherent technique 
interesting examine interaction system performance inherent difficulty topics 
experiment defined difficulty median utility score topic 
topics median utility greater zero classified easy topics median utility zero classified hard 
resulted easy hard topics remainder median utility zero 
breaks performance function easy hard topics 
note immediately systems retrieved documents topics better hard topics easy topics 
reflects fact hard topics generally correct retrieve documents 
opposite pattern emerges top ranked systems 
systems getting gain easy topics performing significantly worse remaining systems hard topics 
means systems att fc need robust hard topics 
exception att fe performs topic categories 
note new topics hard meaning size training set strongly linked topic difficulty 
shows revised comparisons daimler benz cuny submitted corrected results 
reassuring note relative positions systems remain roughly 
absolute performance threshold bias graphs previous section missing key component 
tell relative performance systems say performance absolute standard 
hard decide standard appropriate topics fall back simple approach utility measure 
system retrieves documents receives utility score zero 
system positive utility score topic providing added value 
may fairly minimal standard see hard practice measure standard 
median trec filtering system positive utility topics zero utility topics negative utility topics 
best trec filtering system positive utility topics 
means typical trec filtering system barely justify existence 
question complex utility systems penalized relevant documents retrieving documents leads negative score 
compute systems utility score greater receive retrieving documents 
median system better empty document set topics worse topics best system better topics worse topic 
respect asp better retrieve documents 
look actual utility scores left impression filtering systems general performing quite poorly 
example typical system positive utility slightly half topics positive utility topics 
possible explanations results threshold selection really hard problem filtering thresholds measure simply unrealistic standard performance 
get question looking utility scores obtained optimal thresholds 
optimal performance typical system results positive utility topics positive utility topics 
optimal performance best systems results positive utility topics positive utility topics 
results factors important 
system performing return positive utility majority topics particularly measure 
penalizing relevant documents setting high standard 
better penalize retrieved non relevant documents 
hand clearly lot room improvement threshold selection algorithms 
optimal thresholds derived ranked retrieval determine bias threshold selection algorithms 
run simply ask observed threshold lower higher optimal threshold count number topics fall category 
table presents results median trec filtering system 
median exact asp table number topics systems retrieved right number documents note immediately little bias utility systems strong tendency retrieve documents utility asp 
speculate case 
training set comprehensively evaluated missing relevant documents 
systems underestimate density relevant documents training set test set may lead select thresholds restrictive 
systems training documents building filtering profile threshold selection 
scores relevant training documents biased upwards bias may passed threshold selected systems 
example attempts reduce bias splitting training data parts merging profiles built parts independently 
results confirm threshold bias utility asp 
general commentary groups attack filtering problem building ranking algorithm applying thresholding 
partly people systems routing filtering 
best scoring systems optimize ranking completely independent measure average precision great deal success filtering track 
need optimize ranking specifically filtering evaluation measures 
key question set filtering threshold 
systems concentrate different threshold selection strategies finding score optimizes performance training set normalizing scores create probabilities logistic regression filtering probability thresholds 
ll briefly describe advantages disadvantages approach 
think method looking plot utility function retrieval score training data picking maximum 
difference logistic regression smooths curve finding maximum significant advantage 
empirical estimation suffer curve extremely bumpy observed maximum may artifact caused unusual clumping relevant documents 
consider case training data poor low density relevant documents 
desired threshold score may greater observed score relevant documents 
logistic regression allows system extrapolate threshold greater score top ranked document 
empirical methods provide direct way estimate threshold outside range observed data 
hand logistic regression fitting parametric curve data may logit distribution 
result biased estimates cases 
demonstrates logistic regression doesn better empirical methods 
alternative methods smoothing utility curve explored 
logistic regression adopt simple local smoothing algorithm 
example calculate average utility sliding window rank positions range scores choose threshold midpoint window position average utility maximized 
groups recognized danger training set profile construction threshold selection 
relevant documents construct profile greater similarity profile relevant documents appear test data 
discussed previous section may lead biased thresholds problem common methods threshold selection 
city experiment partitioning training data independent segments city uses different partitions find runs partitioned data tend successful runs full data 
probably runs partitioned data fewer relevant documents profile construction optimization 
way tradeoff 
partitioned data set estimate amount bias directly retrain system full data set apply derived bias correction factor obtain final thresholds 
factors explore filtering experiments 
example groups real time ordering training data 
possible distribution term time may related value feature filtering profile 
terms occuring frequently short period time may reflect single event enduring predictive value 
model density relevant documents function time crucial factor threshold selection 
scoring algorithm optimal distribution relevant non relevant documents training test data ideal threshold change substantially depending relative density relevant non relevant documents 
experiments adaptive filtering 
group umass tried adaptive filtering trec 
readers referred details methods results 
filtering track continue trec 
fact expanding routing task folded filtering track year 
increased emphasis adaptive filtering thought realistic problem provides new challenge sequential line learning 
routing batch style filtering continue exist support research traditional text categorization machine learning algorithms 
encourage interest subject participate filtering track year 
people contributed development trec filtering track particular david lewis karen sparck jones chris buckley paul kantor ellen voorhees trec program committee team nist 
am merely building 
david hull paul kantor 
advanced approaches statistical analysis trec information retrieval experiments 
contact author copy hull xrce xerox com 
david lewis 
trec filtering track 
th text retrieval conference trec nist sp pages 
david lewis 
evaluating optimizing autonomous text classification systems 
proc 
th acm sigir conference pages 
ellen voorhees donna harman 
overview th text retrieval conference trec th text retrieval conference trec nist sp pages 
ellen voorhees donna harman editors 
th text retrieval conference trec 
appear 
