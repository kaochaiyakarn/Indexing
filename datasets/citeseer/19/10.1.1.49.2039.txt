hinge finding algorithm hinging hyperplanes revised version department electrical engineering linkoping university sweden email liu se sjoberg department electrical engineering linkoping university sweden email sjoberg liu se revised version submitted ieee trans 
info 
theory 
concerns estimation algorithm hinging hyperplane hh models non linear black box model structure suggested 
estimation algorithm analysed shown special case newton algorithm applied quadratic criterion 
insight suggest possible improvements algorithm convergence guaranteed 
addition way updating parameters hh model discussed 
stepwise updating procedure proposed 
stress simultaneous updating model parameters preferable cases 
key words nonlinear function approximation hyperplanes numerical methods 
large activity past years field non linear function approximation 
interesting results reported connection example projection pursuit regression neural network approach see wavelets approach see 
methods closely related hinging hyperplane hh model investigated 
different approaches described basis function expansions differ choice basis 
important difference basis functions hh models projection pursuit nn models opposed basis function wavelet approach mentioned basis functions non linearity positioned certain directions 
directions function constant 
name kind functions ridge functions 
hinge hinge hinging hyperplanes hinging hyperplanes hinge function function hinge dimensional dimensional hinge hinging hyperplane hinge function 
wavelet basis localized 
data clustered subspaces preferable ridge basis functions 
nn approach basis function sigmoidal function 
new interesting approach non linear function approximation named hinging hyperplanes reported 
hh approach uses hinge functions basis functions expansion 
hinge function easily illustrated see 
assume hyperplanes gamma gamma delta delta delta xm regressor vector gamma parameter vectors defining hyperplanes 
hyperplanes joined fx gamma gamma 
joint delta gamma gamma multiples delta defined hinge hyperplanes gamma solid shaded part hyperplanes explicitly max gamma min gamma defined hinge function 
combination hyperplanes chosen min max function parameters gamma estimated 
section detailed review estimation algorithm hh model 
contribution issues penetrated 
hinge finding algorithm hfa 
shown hfa newton algorithm function minimization applied quadratic loss function suggestions improve hfa convergence guaranteed 
original hfa depending function approximated behave ways algorithm converges hinge location algorithm stuck limit cycle altering hinge location series different values hfa converge hinge located outside data support 
improvement straightforward realizing family numerical algorithms hfa belongs 
improvement guarantee global convergence algorithm means algorithm converges local minimizer non linear functional regardless initial parameter guess 
spirit 
second issue way additional basis functions hinge functions introduced hh model 
hinges introduced parameters introduced hinges fitted introduced 
fitting parameters new hinge function incorporated performed iterative way 
step taken hfa hinge function 
approach discussed compared possible estimation algorithms 
original presentation advocated hh models superior alternative nn models 
main argument efficient estimation algorithm hh models 
shown algorithms applicable model structures 
follows choice model structure hh model nn model algorithmic reasons assumptions unknown relationship modeled 
organized way 
section hfa strategy updating adding hinge functions reviewed 
section novel insights improvements 
estimation algorithm hh model consists hinge functions discussed section 
section comparisons performance different algorithms 
hinging hyperplanes function approximation general goal find model delta approximates unknown function possible 
fit parameters data available fy noisy 
choice non linear black box model delta particular problem important issue 
model basis functions manage describe data efficient way expected properties prefered 
kind prior knowledge exceptional 
choice specific black box model structure usually guided arguments 
main advantages new hh approach ffl upper bound approximation error available 
ffl estimation algorithm hh algorithm number squares algorithms executed fast computationally efficient way 
ffl may useful model structure obtained hh approximation piecewise linear models linear models proven useful large number problems 
upper bound approximation error hh models stated 
assume sufficiently smooth function sufficiently smooth means integral finite jj jj hinge functions hk jjg gamma jj radius sphere want approximate function defined 
fourier transform 
proof theorem extension barron result sigmoidal neural networks 
means hh model efficient neural networks norm 
compared best achievable convergence rate linear estimator functions class jj jjg lower rate linear estimators approximately gamma indicates largest gain nn hh models obtained dimension input space high 
hinge finding algorithm estimation algorithm proposed estimating hh models subroutine called hinge finding algorithm 
hfa reviewed 
stated hinge subspace input space satisfies equation delta delta gamma gamma data set fy hfa consists steps 
choose initial split data words choose initial hinge 
name sets data gamma 
calculate squares coefficients hyperplane fitted values fy xg denote parameters analogous fy xg gamma obtain gamma 
update gamma finding new data sets expressions fx delta gamma gamma gamma fx delta gamma gamma 

go hinge function converged 
hfa illustrated 
function want approximate samples function paired values input hfa algorithm 
state second square step algorithm gamma gamma gamma gamma gamma mentioned hh models preferably dimension input space high 
examples low dimension purely sake step hh algorithm initial split data gamma 
squares estimates gamma lines 
intersection gamma new hinge position gives new split data step hfa 
clarity presentation 
obvious hinge function chosen recall min vs max discussion 
choosing minimum hyperplanes hinge function consequence data calculate approximating hyperplane gamma unknown function hinge function shown hfa converge true hinge location 
arbitrary function different ways hfa take mentioned section 
practical applications real data involved unpredictable behavior hfa causes problems 
look example insights problems associated hinge search 
consider function 
breiman hfa applied data set resulting hinge position vary dramatically different initial values 
evolution hinge position different initial conditions depicted axis denotes initial hinge position axis represents number iterations hfa 
empty parts axis initial hinge positions tested initial values cause hinge go outside border support 
case sets gamma contains data empty 
happens algorithm stops step performed obtained function linear domain data support 
concluded particular function shown breiman hfa convergence points limit cycle 
interval taken initial hinge positions lead converged hinge 
hfa initialized region hinge position outside support area 
summary breiman algorithm guarantee convergence depending problem initial parameter value diverge 
usually assured modifying parameter steps criterion step 
cos tan pi function cos tan delta convergence hfa different initial values 
axis number iterations shown axis initial hinge position 
empty intervals axis correspond initial values converge 
breiman algorithm criterion modification straightforward 
section show algorithm modified 
hh algorithm essentially hh algorithm strategy stepwise increase number hinge functions model hfa 
procedure follows 
fy run hfa available data estimating hinge function 
introduce additional hinge function calculate difference data estimated hinge residuals gamma run hfa obtaining run hfa gamma reestimate iterate reestimation procedure converged 
third hinge function added procedure analogous calculate gamma gamma run hfa obtain reiterate hfa advice just run step hfa iteration introducing hinge function number 
clear sequential updating hinge function parameters best number variants immediately apparent hinge function parameters updated simultaneously efficient way update parameters introducing additional hinge function just simply start re initializing parameters 
hfa may converge clear hh algorithm original shape reliable algorithm 
discussed section 
globally convergent hfa section course taken quite different taken deriving original hfa 
resulting scheme alternative derivation place algorithm broader context numerical algorithms give hints algorithm improved 
approach uses gradient search minimum quadratic criterion 
differentiate function consists sum highly non linear max min elements rise worries 
problem 
respect parameters criterion smooth gradient hessian exist values parameters 
assume data set fy objective fit hinge function set data 
problem definition hfa considered general hh algorithm iteratively replaced delta sequences 
hfa remains regardless choice called hh algorithm 
input hfa data set form 
formulate objective way 
criterion fit vn gamma calculate parameter minimizes 
formally expressed arg min vn vector written gamma recall function defined max min fh gamma gamma defined half spaces respectively second argument holds 
gradient methods need derivative hinge function respect parameters 
derivative respect analogue expression derivative respect gamma dh gamma derivative just linear regression case zero 
possible data points hinges problem algorithm hinges measure zero space recall points hinge generic case 
totally defined problem hinge belong sets solution adopted breiman 
possibility define derivative zero hinge means data hinge excluded fit 
compute minimum vn standard newton procedure gradient hessian criterion vn needed 
derivative hinge function separate parameter vector gamma vn vn gamma gamma dh gamma gamma dh gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma expression derivative linear regression case modification data correct half plane included 
hessian obtained differentiating vn vn gamma gammax gamma gammax gamma gamma gamma gammax gamma gamma gamma gammax gamma gamma diagonal elements equal zero intersection gamma zero 
derivative expressions diagonal straightforward hinge function linear region summation performed 
result vn gamma apply newton algorithm find minimum see 
means iterative search algorithm 
gamma vn gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma expression newton step rule calculation step hh algorithm recognized 
rewritten obtain expression br gamma br parameter obtained hh algorithm 
newton algorithm minimization equivalent hfa 
generally newton method globally convergent precaution taken regarding decrease loss function 
conventional solutions convergence problem newton method include line search 
modified algorithm damped newton algorithm 
damped newton algorithm case give parameter update recursion br gamma strategy choosing try full newton step fails decrease loss function sequence decreasing tried 
strategies decrease suggested function evaluations performed new tested building local higher order models cost function 
higher order models base calculation test 
clarity examples simplest possible strategy 
straightforward include sophisticated algorithms 
section stating insights ffl assure convergence hfa suggested modified step length 
necessity exemplified section 
ffl single parameter update means solve squares problem 
non linear effect due subspaces gamma change parameters 
caused change apply exactly data estimated 
step length introduced limit non linear effect prevent large change subspaces gamma single iteration 
simultaneous estimation hinge function parameters previous section concluded hfa equivalent newton algorithm minimization quadratic criterion 
parameters associated hinge function changed model consists hinge functions hfa algorithm considers time 
alternative apply damped newton method parameters time give simultaneous parameter update 
section discuss possible advantageous approach 
calculate gradient hessian criterion 
consider hh model hinge functions hinge functions form 
parameters organized column gamma 
gamma index shows hinge function parameter vector belongs 
gradient criterion rv gamma gamma gamma gamma gamma 
gamma gamma gamma gamma gamma skipped index indicating number data 
notice blocks differ terms sums 
sum includes data half space 
differentiating gradient gives hessian 
vk ij gamma gamma gamma gamma component looks exactly linear regression case modification data belong intersection half spaces included 
diagonal blocks look ii gamma zero diagonal terms half spaces gamma intersection definition 
example illustrate calculations 
assume hinging hyperplane model consists hinge functions 
gradient expressed rv gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma hinge hinge example summation areas hinge functions 
shaded area gamma part hinging hyperplane model influenced gamma case illustrated dimensional example lines represent partition space half spaces gamma gradient differentiated obtain second derivative diagonal blocks contain terms type summation index goes intersections hyperplanes belonging different hinge functions 
sight calculation second derivative look messy 
software package utilizes vector matrix multiplication kind operation performed step 
having obtained gradient second derivative components applying newton type algorithm available parameters updated gamma gamma rv damped newton method implemented avoids computational demanding computation inverse hessian 
solves system linear equations delta parameter update delta gamma unknown 
done fast matlab 
shown original description hh model max min basis functions parameterized 
means pseudo inverse 
alternatively changing parameterization sparse description parameters obtained 
see 
expect obtain better performance simultaneous update hh algorithm 
clear answer question 
newton algorithm corresponds second order taylor expansion criterion 
approximation criterion expect newton step 
hfa algorithm implies diagonal elements hessian considered 
iteration faster typically compensated additional iterations 
criterion close quadratic diagonal elements importance disadvantage 
typically quadratic expansion approximation close minimum criterion narrow valley parameter space expect neglecting diagonal elements slows process considerably 
see far away minimum search quadratic expansion applicable advantageous neglect diagonal elements 
mentioned hh model viewed basis expansion 
function expansion basis functions orthonormal parameters estimated independent diagonal elements hessian zero 
hh model basis functions overlap importance overlap depends problem 
depends data current parameters simultaneous update computational expansive number parameters increase hinge functions included hh model 
interesting conjugate gradient method builds newton step series gradient steps avoiding compute hessian 
algorithm successful neural network applications 
see 
section simultaneous update compared hh algorithm simulation examples 
examples section divided parts part treats improvement hfa introducing step length parameter assure convergence 
second part deals simultaneous updating parameters subset performance compared hh algorithm 
performance modified hfa data original hfa converge initial parameter values 
hfa modified step length started different initial parameter values correspond different initial splits data 
result modified algorithm converges local minima 
evolution hinge position depicted 
compare behavior original algorithm depicted 
initial values algorithm jumps local minimum second minimum 
jumps prevented implementing advanced step length rule goldstein rule see 
compared section 
damped newton algorithm hfa converges initial values unmodified hfa evolution hinge position number initial data splits interval 
local minima 
result convergence limit cycle behavior initial hinge intervals 
simultaneous parameter updating examples illuminating practical differences stepwise updating parameters hfa simultaneous updating described section 
iteration simultaneous updating means step newton update 
iteration hfa algorithm means cycle newton updates hinge function updated 
hfa iteration faster simultaneous 
general efficient sense criterion decreases 
examples follow see hfa short cut gain speed turn fastest way 
simultaneous vs stepwise updating example compare performance hfa newton algorithm applied data generated hh model dimensions 
hh model contains hinge functions depicted 
position hinges hinge functions easier seen 
input data uniformly distributed square number samples theta 
equations hinges gamma true parameter vectors giving hinges perturbed initial vector 
equations initial hinges algorithms hh model hinge functions dimensional hh model consisting hinge functions 
hinge positions hinges hinge functions 
gamma initial parameter vectors algorithms executed 
result figures 
avoid messy plot result iterations shown plot 
initial hinge positions marked initial hinge initial hinge 
hfa algorithm hinge positions jump right true positions slowly converge true hinges 
hinge position hinge number move iterations 
simultaneous updating newton algorithm contrary hinges converged true positions iterations 
hfa iterations necessary convergence shown 
hfa iterated iterations hinge positions gamma gamma deviates true position 
evolution hinge positions hfa initial hinge initial hinge true hinge hinge position evolution iterations hfa 
recommended procedure run hfa hinge function converged introduced second hinge function 
second hinge function introduced hfa re iterated iteration time 
procedure results worse result iteration procedure example 
important calculate total time needed minimization criterion consider computationally complex parts algorithm 
example hfa iteration takes seconds iterations needed reach close minimum hfa needs minutes 
simultaneous newton algorithm iteration demands approximately seconds multiplied number iterations algorithm needs seconds minimization 
hinge evolution newton alg 
initial hinge initial hinge true hinge hinge position evolution iterations newton algorithm 
note iteration ends true hinge position 
simultaneous updating newton algorithm performed significantly better hfa diagonal elements hessian important example 
line discussion section 
hfa short cut gain speed just seen easily turn slower simultaneous newton update 
high interaction example example chosen compare algorithms simulated data 
high interaction example designed 
dimension input space distribution uniform functions formed gamma gamma gamma gamma gamma gamma gammax gamma gamma gamma gamma gamma gammax gamma gamma gamma gamma gamma sample size chosen functions normalized way upper sample equals 
example consider function obtained samples linear function 
assume samples ordered vector lowest labeling lowest sample largest labeling largest sample 
element ord sample number ordered vector read 
function multiplied factor give result multiplied read value ord 
second triple formed gamma 
non linear function function normalized factor standard deviation function white noise standard deviation added 
expression output data delta general comment regarding function belongs family named ridge functions suited approximations hh models 
shape particular function possible match exactly hh models 
words example chosen difficult easy 
concluded best fit obtained hinge hh model 
comes inspection called pe gcv criterion combination mean residual sum squares mrss penalty number hinge functions 
experience running algorithm proposed hfa variation cpu time depending chosen initial value parameter vector 
course final mrss depends initial value due local minima 
experiments performed sparcstation lx running matlab 
cpu time varies seconds seconds algorithm simultaneous updating parameters seconds seconds hfa 
mrss goes algorithms depending initial value 
mrss algorithms minimum reached algorithms 
summarized results table 
hfa newton iter time mrss iter time mrss table results comparison hh model estimation hfa method proposed 
fourth columns shows number iterations hfa newton algorithm respectively 
second fifth columns contain seconds algorithms 
third sixth column display achieved mean residual sum squares 
initial values parameter vectors picked random 
initial vectors algorithms 
table shows cpu time consumed algorithms number iterations needed mrss accomplished 
iteration hfa case complete update hh model iteration hinge function hh model 
performance simultaneous updating algorithm proposed better hfa algorithm 
average time needed iteration lower hfa newton algorithm needs half number iterations compared hfa 
estimation algorithm introduced hinging hyperplane model considered 
main building block estimation algorithm called hinge finding algorithm 
shown newton algorithm minimization quadratic loss function 
insight 
ffl difficulties convergence original hinge finding algorithm circumvented standard techniques numerical minimization theory 
introducing step length method converted damped newton algorithm assures convergence 
ffl damped newton search applied parameters simultaneously 
alternative iterative hh algorithm suggested 
difference approaches advantages disadvantages proposed algorithm discussed 
method best problem dependent general expected simultaneous update gives faster convergence close minimum 
ffl clear evident algorithmic advantageous hh model comparison example neural nets 
numerical search methods applicable model structures 
illustrated examples 
barron 
universal approximation bounds superpositions sigmoidal function 
ieee trans 
information theory may 
benveniste zhang 

wavelets identification 
mogens torsten editors preprint th ifac symposium system identification volume pages july 
breiman 
hinging hyperplanes regression classification function approximation 
ieee trans 
information theory may 
dennis schnabel 
numerical methods unconstrained optimization nonlinear equations 
prentice hall englewood cliffs new jersey 
friedman stuetzle 
projection pursuit regression 
journal american statistics association 
sjoberg 
parameterization conditioning hinging hyperplane models 
available rt gw liu se cgi bin reports year department electrical engineering linkoping university sweden 
sjoberg ljung 
neural networks system identification 
mogens torsten editors preprint th ifac symposium system identification volume pages july 
van der 
minimisation methods training feedforward neural networks 
neural networks 
