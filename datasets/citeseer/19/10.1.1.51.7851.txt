sequential simulation methods bayesian filtering arnaud doucet signal processing group department engineering university cambridge cb pz cambridge email ad eng cam ac uk technical report cued infeng tr 
report overview sequential methods bayesian filtering nonlinear non gaussian dynamic models 
includes general framework numerous methods proposed independently various areas science proposes original developments 
keywords bayesian estimation optimal filtering nonlinear non gaussian state space models hidden markov models sequential monte carlo methods 

problems statistical signal processing automatic control applied statistics econometrics stated follows 
transition equation describes prior distribution markovian hidden signal interest fx ng called hidden state process observation equation describes likelihood observations fy ng discrete time index 
aim estimate hidden state process observations 
bayesian framework relevant information fx time included posterior distribution 
applications signal processing interested estimating recursively time distribution especially marginals called filtering distribution 
problem known bayesian filtering problem called optimal filtering stochastic filtering problem 
cases including linear gaussian state space models kalman filter hidden finite state space markov chains filter impossible evaluate analytically distributions 
mid huge number papers books devoted obtaining approximations distributions see example 
popular algorithms extended kalman filter gaussian sum filter rely analytical approximations early known relying deterministic numerical integration methods performed bucy workers see example 
interesting automatic control done sequential monte carlo integration methods see 
primitive computers available time algorithms overlooked forgotten late great increase computational power allowed numerical integration methods bayesian filtering 
current research focused mc monte carlo integration methods great advantage subject linearity gaussianity hypotheses model 
main objective report include unified framework old algorithms developed independently various fields applied science 
original developments 
closest report liu chen developed independently underlines similarly central role sequential importance sampling sis sequential simulation methods bayesian filtering 
technical report translation chapter abbreviated form 
best knowledge important works cited standard article book optimal estimation current subject 
sequential simulation methods bayesian filtering report organized follows 
section briefly review bayesian filtering problem 
classical mc method bayesian importance sampling proposed solve 
sequential version method allows obtain general recursive mc filter 
algorithm probability distribution known importance function 
criterion obtain optimal importance function 
unfortunately numerous models importance function propose suboptimal distributions practical interest retrieve particular cases algorithms independently literature 
section resampling scheme limit practically degeneracy algorithm 
section apply rao blackwellisation method sis obtain efficient hybrid analytical mc filters 
section show mc filter compute prediction fixed interval smoothing distributions likelihood 
simulations section 
bayesian estimation hidden markov models importance sampling signal fx ng nx unobserved hidden markov process initial distribution transition equation gamma 
observations fy ng ny conditionally independent process fx ng marginal distribution 
sum model hidden markov model hmm described gamma denote fx xn fy yn respectively signal observations time aim estimate recursively time distribution associated features including xn expectation jy fn dx integrable fn thetan obtain straightforwardly recursive formula theta yn xn xn xn yn recursion academic sense typically compute normalizing constant marginals particular xn yn requires ability evaluate complex high dimensional integrals 
numerical solution consists monte carlo integration method 
assume know sample gamma evaluate gamma pointwise 

perfect monte carlo sampling 
assume able simulate random samples 
empirical estimate distribution dx ffi dx sequential simulation methods bayesian filtering obtains estimate fn dx fn strong law large numbers denotes sure convergence 
posterior variance fn satisfies oe fn var theta gamma central limit theorem holds gamma gamma oe fn delta denotes convergence distribution 
advantage perfect mc method clear 
set random samples easily estimate quantity speed convergence estimate depends nx theta fn unfortunately usually impossible sample efficiently posterior distribution time multivariate non standard known proportionality constant 

bayesian importance sampling 
alternative solution consists importance sampling method 
basic idea method 
choose called importance function probability distribution depends observations time easily sample 
method simple 
implies write fn dx simulate samples fx ng possible estimate fn importance weights equal fi fi fi fi fi fi fi fi fi estimate unbiased converges 
sequential simulation methods bayesian filtering bayesian framework estimate generally requires knowledge normalizing constant dx typically expressed closed form 
observe estimate ratio estimates numerator denominator obtained classical importance sampling method fn fn unnormalised importance weights equal fi fi fi means proportional normalised importance weights equal true importance weights replaced estimate method known statistical literature bayesian see example 
recall classical results mc method 
assumption set vectors distributed 
support phi nx theta psi includes support phi nx theta psi 
exists finite 
assumption jy jy 
sufficient condition verify assumption var jy cn sequential simulation methods bayesian filtering proposition 
finite biased asymptotically assumption yields assumption previous proposition implies convergence empirical distribution ffi dx dx sense convergence function fn exists finite 
result important means interpret method simulation method sample dx integration method see similar interpretation 
delta method obtain proposition 
proposition 
geweke assumptions gamma gamma oe fn delta oe fn jy gamma fn gamma jy delta show subsection possible obtain easily recursive mc filter bayesian 
monte carlo filter sequential importance sampling 
rewrite importance function follows gamma gamma probability density function conditional gamma aim obtain time estimate distribution able propagate estimate time modifying subsequently past simulated trajectories means importance function time admits marginal distribution time importance function 
possible restrict importance functions form gamma gamma xn gamma iterating yields gamma importance function allows evaluate recursively time importance weights 

assumption weakened 
example consider case interested estimate fixed lag distribution fixed 
case choose gamma sequential simulation methods bayesian filtering assumption obtain straightforwardly mc filter 
algorithm sequential importance sampling sis 
time ffl sample 
ffl evaluate importance weights normalizing constant fi fi fi ffl normalise importance weights 
times ffl sample gamma gamma ffl evaluate importance weights normalizing constant gamma fi fi fi gamma fi fi fi gamma ffl normalise importance weights numerous algorithms proposed literature special cases general simple algorithm 
particular case algorithm introduced mayne numerical complexity algorithm 
important take ae practice great advantage parallelizable 
general case memory requirements necessary keep simulated trajectories time time gamma gamma interested filtering distribution memory requirements 
general case obtains time estimate joint posterior distribution dx ffi dx estimate dx sequential simulation methods bayesian filtering assumption ensures asymptotic convergence estimates quite weak 
practice obtains poor performance estimates importance function chosen 
choice importance function topic sections 

degeneracy algorithm 
interpreting mc sampling method mc integration method best possible choice consist selecting posterior distribution interest importance function 
obtain importance weights var 
close case 
importance functions form variance importance weights increase stochastically time 
proposition 
unconditional variance importance weights observations interpreted random variables increases time 
proof proposition straightforward extension kong liu wong theorem case importance function form 
impossible avoid degeneracy phenomenon 
practically iterations algorithm normalised importance weights close zero large computational burden devoted updating trajectories contribution final estimate zero 

selection importance function 
practically time gamma importance weights gamma fixed 
limit degeneracy algorithm natural strategy consists selecting importance function minimizes variance importance weights conditional simulated trajectory gamma observations proposition 
gamma importance function minimizes variance importance weight conditional gamma proof straightforward 
implement optimal importance function gamma optimal importance function 
optimal importance function gamma introduced particular case 
importance function 
distribution obtain expression importance weight gamma fi fi fi gamma fi fi fi gamma gamma gamma 
case importance weight depend interesting practice allows parallelization simulation evaluation 
verify proposition sufficient condition ensures importance weights bounded consists assuming likelihood bounded 
unfortunately bound time dependent 
sequential simulation methods bayesian filtering optimal importance function suffers major drawbacks 
requires ability sample gamma evaluate proportionality constant gamma gamma gamma dx requires evaluation integral admit analytical expression general case 
evaluation possible important class models 
example 
partial gaussian state space models 
consider model gamma sigma cx sigma nx nx real theta nx matrix mutually independent gaussian sequences sigma sigma 
denoting sigma gamma sigma gamma sigma gamma sigma gamma sigma gamma gamma sigma gamma delta obtains gamma sigma gamma exp gamma gamma cf gamma gamma sigma sigma wc delta gamma gamma cf gamma models evaluations impossible 
suboptimal methods allow approximation optimal importance function 
proposed method second mc step 
mc approximation optimal importance function 
assume gamma evaluated analytically possible sample gamma likelihood bounded ratio gamma gamma bounded 
possible sample gamma accept reject procedure 
accept reject procedure 
sample gamma 
accept return 
unfortunately procedure requires random number iterations obtaining random sample distributed gamma framework online applications strategy avoided 
severe problem gamma evaluated 
sequential simulation methods bayesian filtering naive approach consists second mc step bayesian sample gamma evaluate gamma gamma sample random variables distributed gamma obtain approximation gamma dx gamma ffi dx gamma gamma estimate gamma gamma approximation theoretically valid 
solution simple computationally expensive 
mc methods mcmc methods proposed simulate approximately gamma evaluate gamma see :10.1.1.48.6455
iterative algorithms appear limited interest line framework lack theoretical convergence results 
fact general framework sis allows consider importance functions built approximate analytically optimal importance function 
advantages alternative approach computationally expensive mc methods previous convergence results bayesian valid 
general method build suboptimal importance functions necessary build case case basis dependent model studied 
possible base developments previous standard suboptimal filtering methods 
importance distribution obtained local linearisation 
simple choice consists selecting importance function gamma parametric distribution gamma finite dimensional parameter theta aer determined gamma nx thetar ny theta deterministic mapping 
strategies possible 
illustrate methods original methods result gaussian importance function parameters evaluated local dependent simulated trajectory local linearisation markov state space model 
propose model locally extended kalman filter 
case linearisation performed aim obtaining importance function algorithm obtained converges asymptotically optimal solution assumptions previously 
example 
consider model gamma nv theta sigma nw theta sigma sequential simulation methods bayesian filtering nx nx nx ny differentiable mutually independent sequences sigma sigma 
performing approximation order observation equation get gamma fi fi fi fi xk xk gamma gamma gamma defined new model similar evolution equation linear gaussian observation equation obtained gamma 
model markovian depends gamma form perform similar calculations obtain gaussian importance function gamma sigma mean covariance sigma evaluated trajectory formula sigma gamma sigma gamma fi fi fi fi xk xk gamma sigma gamma fi fi fi fi xk xk gamma sigma sigma gamma gamma fi fi fi fi xk xk gamma sigma gamma theta theta gamma gamma fi fi fi fi xk xk gamma gamma associated importance weight evaluated 
local linearisation optimal importance function assume ln gamma twice differentiable wrt nx define fi fi fi fi xk fi fi fi fi xk second order taylor expansion get gamma gamma gamma point perform expansion arbitrary determined deterministic mapping gamma 
additional assumption negative definite true concave setting sigma gammal gamma sigma yields gamma gamma gamma gamma gamma gamma sigma gamma gamma gamma sequential simulation methods bayesian filtering suggests adopt importance function gamma sigma gamma unimodal judicious adopt mode gamma nx theta associated importance weight evaluated 
example 
linear gaussian dynamic observations distribution exponential family 
assume evolution equation satisfies ax gamma nv theta sigma sigma observations distributed distribution exponential family exp gamma cx gamma cx delta real ny theta matrix ny ny models numerous applications allow consideration poisson binomial observations see example 
cx gamma cx gamma gamma ax gamma sigma gamma gamma ax gamma yields gamma cx fi fi fi fi xk gamma sigma gamma gammab gamma sigma gamma covariance matrix definite negative 
determine mode distribution applying iterative method initialised gamma satisfies iteration gamma theta gamma delta gamma gamma delta 
method close developed independently shephard pitt different framework 
propose mcmc algorithm line estimation non gaussian measurements time series metropolis hastings algorithms 
proposal distribution algorithm build case concave similar method simpler methods 
prior importance function 
simple choice consists selecting importance function prior distribution hidden markov model 
choice mayne seminal 
distribution adopted tanizaki 
case gamma gamma gamma method inefficient simulations state space explored knowledge observations 
especially sensitive outliers 
fact methods developed literature build clever proposal distributions metropolis hastings algorithms applied sequential framework vice versa 
convergence algorithm ensured weak assumptions number iterations simulated markov chain tends infinity sequential framework convergence algorithm ensured weak assumptions number simulated trajectories tends infinity 
sequential simulation methods bayesian filtering fixed importance function 
simpler choice consists fixing importance function independently simulated trajectories observations 
case gamma gamma fi fi fi gamma choice adopted tanizaki presents method stochastic alternative numerical integration method kitagawa 
results obtained poor dynamic model observations taken account 
leads cases unbounded importance weights 

resampling previously illustrated degeneracy algorithm sis avoided 
forgetting factor weights associated optimal importance function introduced stability regularity assumptions markov model interesting time uniform convergence result obtained 
practically regularization slows avoid degeneracy algorithm 
necessary introduce procedures 
basic idea resampling methods consists eliminating trajectories weak normalised importance weights multiply trajectories strong importance weights 
adopt measure degeneracy algorithm effective sample size 
criterion introduced liu defined variances estimates respectively obtained imaginary samples importance sampling method samples distributed 
functions vary slowly liu shows var var theta theta var effective sample size eff defined eff var evaluate exactly eff owing estimate eff eff eff eff fixed threshold thres resampling procedure 
popular resampling scheme sir algorithm sampling importance resampling introduced rubin 
scheme steps step step second step sampling step obtained discrete distribution 

sis resampling monte carlo filter 
time gamma approximation dx gamma gamma gamma ffi gamma dx gamma sequential simulation methods bayesian filtering time modified monte carlo filter proceeds follows 
algorithm sis resampling monte carlo filter 
importance sampling ffl sample gamma gamma ffl evaluate importance weights normalizing constant gamma fi fi fi gamma fi fi fi gamma ffl normalise importance weights ffl evaluate eff 

resampling eff thres ffl ffl sample index distributed discrete distribution elements satisfying prfj lg ffl eff thres algorithm section modified 
eff thres sir algorithm applied obtain approximation joint distribution dx ffi dx 
interesting resampling schemes reduce mc variation sir 

implementation resampling procedure 
eff thres necessary implement algorithm sample random variates discrete distribution elements 
straightforward application sir procedure complexity ln 
complexity important reduce proposed ad hoc methods 
fact possible implement exactly sir procedure operations noticing possible sample operations variables distributed ordered delta delta delta un classical algorithm pp 

algorithm pp 
ffl sample sequential simulation methods bayesian filtering ffl un eu ffl gamma eu deduce straightforwardly algorithm sample samples discrete distribution operations 

algorithm attributed idea algorithm carpenter clifford 

limitations resampling scheme 
resampling procedure decreases algorithmically degeneracy problem introduces practical theoretical problems 
practical point view resampling scheme seriously limits algorithm 
theoretical point view resampling step simulated trajectories longer statistically independent lose simple convergence results previously 
trajectories high importance weights statistically selected times 
numerous trajectories fact equal 
loss diversity 
established central limit theorem estimate obtained sir procedure applied iteration 
despite drawbacks sir algorithm basis numerous works 
popular bootstrap filter gordon salmond smith simultaneously developed kitagawa applies iteration resampling step gamma gamma see similar method developed closely related field bayesian networks 
limit loss diversity ad hoc procedures proposed 
trajectories artificially perturbed resampling step 
simple solution consists building semi parametric approximation dx gamma gamma delta resampling choice kernel delta difficult 
higuchi proposes various heuristic procedures taken genetic algorithms literature introduce diversity samples 
notice fact sir procedure similar mathematical structure selection step genetic algorithms 
interesting extensions sir algorithm developed shephard pitt 

rao blackwellisation sequential importance sampling propose improve sis variance reduction methods designed model studied 
numerous methods developed reduce variance mc estimates including sampling control variates 
apply rao blackwellisation method 
show possible apply method successfully important class hmm obtain hybrid filters part calculations realized analytically part mc methods 
assume partition state gamma delta denote fn dx dx thetar fn gamma delta gamma delta gamma fi fi delta dx gamma delta dx thetar dx dx gamma delta dx dx sequential simulation methods bayesian filtering fn gamma delta gamma delta gamma fi fi delta dx assumption conditional realization gamma delta evaluated analytically estimates possible 
classical obtained importance distribution gamma fi fi delta nn dn fn fi fi fi fi fi fi second rao blackwellised estimate obtained integrating analytically importance distribution gamma fi fi delta gamma fi fi delta dx estimate nn dn fi fi fi fi fi fi proposition shows integrate analytically components variance obtained estimate weaker crude estimate 
proposition 
variances importance weights numerator denominator obtained rao blackwellisation smaller obtained crude monte carlo method var jy theta gamma delta var jy theta gamma delta var jy nn var jy nn var jy dn var jy dn proof straightforward 
simple result estimate marginal distribution gamma fi fi delta sequential simulation methods bayesian filtering ffl fn gamma delta fn gamma delta fn gamma delta gamma delta fn ffl fn gamma delta fn gamma delta jy gamma fn gamma delta delta gamma delta jy gamma fn gamma delta delta cases possible mc methods developed previous sections observations independent conditional gamma delta generally longer independent conditional single process modifications straightforward 
obtain optimal importance function gamma fi fi gamma delta associated importance weight gamma gamma gamma delta important applications general method 
example 
conditionally linear gaussian state space model consider model gamma fi fi gamma delta gamma delta gamma gamma delta gamma delta gamma delta markov process nv theta nv nw theta nw 
wants estimate gamma fi fi delta gamma gamma delta fi fi delta gamma fi fi delta gamma delta fi fi fi possible mc filter rao blackwellisation 
conditional linear gaussian state space model integrations required rao blackwellisation method realized kalman filter 
introduced algorithm name rsa random sampling algorithm particular case homogeneous scalar finite state space markov chain case adopted optimal importance function gamma fi fi gamma delta possible sample discrete distribution evaluate importance weight gamma gamma delta kalman filter 
similar developments proposed 
algorithm blind deconvolution proposed liu particular case method time invariant channel gaussian prior distribution rao blackwellisation method framework particularly attractive continuous components restrict exploration discrete state space 
example 
finite state space hmm consider model gamma fi fi gamma delta gamma fi fi gamma delta gamma delta connections mayne 
framework extension time varying channel modeled linear gaussian statespace model straightforward 
sequential simulation methods bayesian filtering markov process finite state space markov chain parameters time depend want estimate gamma fi fi delta gamma gamma delta fi fi delta gamma gamma delta fi fi delta possible rao blackwellised mc filter 
conditional finite state space markov chain known parameters integrations require rao blackwellisation method done analytically 

prediction smoothing likelihood estimate joint distribution sis practice coupled resampling procedure limit degeneracy time form dx ffi dx show possible obtain distribution approximations prediction smoothing distributions likelihood 

prediction 
approximation filtering distribution dx want estimate step ahead prediction distribution gamma dx gamma replacing approximation obtained obtain gamma dx gamma evaluate integrals sufficient extend trajectories evolution equation 
algorithm 
step ahead prediction ffl ffl sample gamma gamma obtain random samples estimate dx dx ffi dx dx ffi dx 
fixed lag smoothing 
want estimate fixed lag smoothing distribution length lag 
time mc filter yields approximation dx ffi dx sequential simulation methods bayesian filtering obtain estimate fixed lag smoothing distribution dx ffi dx high approximation generally perform poorly 

estimate better importance function form gamma see 
straightforward modification criterion proposed previously optimal importance function associated importance weight respectively equal gamma gamma gamma 
usually difficult sample gamma impossible evaluate analytically gamma gamma 
possible build suboptimal importance functions example extended kalman smoother techniques remains evaluate term gamma occurs expression importance weight 
possible evaluate term mc integration 

fixed interval smoothing 
want estimate time filtering algorithm yields approximation dx ffi dx theoretically obtain distribution 
practically method soon gamma significant degeneracy problem requires resampling algorithm 
time simulated trajectories usually resampled times distinct trajectories times approximation bad 
problem severe bootstrap filter resamples time instant 
necessary develop alternative algorithm 
propose original algorithm solve problem 
algorithm formula dx seek approximation fixed interval smoothing distribution form dx ffi dx dx support filtering distribution dx weights different 
algorithm obtain weights 
algorithm 
fixed interval smoothing 

initialisation time ffl njn 
gamma sequential simulation methods bayesian filtering ffl evaluate importance weight jn fi fi fi fi fi fi ji algorithm obtained argument 
replacing approximation yields dx jn fi fi fi fi fi fi owing fi fi fi approximated fi fi fi fi fi fi dx fi fi fi approximation dx dx ffi dx jn fi fi fi fi fi fi ji jn fi fi fi fi fi fi ji ffi dx ffi dx algorithm follows 
algorithm requires storage marginal distributions dx weights supports memory requirement nn 
complexity gamma nn delta quite important ae 
complexity little lower previous developed algorithms kitagawa tanizaki require new simulation step 

likelihood 
applications particular model choice may wish estimate likelihood data 
simple estimate likelihood practice resampling steps approach impossible 
alternative decomposition likelihood gamma sequential simulation methods bayesian filtering gamma gamma dx gamma gamma gamma dx gamma estimate quantity gamma gamma samples obtained step ahead prediction approximation dx gamma gamma gamma gamma 
expression possible avoid mc integration know analytically gamma gamma gamma gamma 
simulations section apply methods developed previously linear gaussian state space model classical nonlinear model 
models simulations length evaluate empirical standard deviation filtering estimates kjk obtained mc methods ar gamma delta gamma ffl simulated state th simulation ffl mc estimate th test signal th simulated trajectory associated signal 
denote kjk calculations realized 
implemented filtering algorithms bootstrap filter sis prior importance function sis optimal suboptimal importance function 
fixed interval smoothers associated sis filters computed 
sis algorithms sir procedure eff thres 
state percentage iterations sir step importance function 

linear gaussian model 
consider model gamma white gaussian noises mutually independent gamma oe delta gamma oe delta oe oe 
model optimal filter kalman filter 
sequential simulation methods bayesian filtering optimal importance function 
optimal importance function gamma gamma oe delta oe gamma oe gamma oe gamma oe gamma oe oe associated importance weight equal gamma exp gamma gamma gamma oe oe results 
kalman filter obtain ar gamma kjk delta 
different mc filters results tab 
tab 

ar gamma kjk delta bootstrap prior dist optimal dist table mc filters linear gaussian model percentage sir prior dist optimal dist table percentage sir steps linear gaussian model trajectories estimates obtained mc methods similar obtained kalman 
sis algorithms similar performances bootstrap filter smaller computational cost 
interesting algorithm optimal importance function limits seriously number resampling steps 

nonlinear series 
consider nonlinear model gamma gamma gamma gamma cos sequential simulation methods bayesian filtering mutually independent white gaussian noises gamma oe delta gamma oe delta oe oe 
case possible evaluate analytically gamma sample simply gamma 
propose apply method described consists locally observation equation 
importance function obtained local linearisation 
get gamma fi fi fi fi xk xk gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma obtain linearised importance function gamma oe oe gamma oe gamma oe gamma gamma oe oe gamma gamma oe gamma gamma gamma results 
case possible estimate optimal filter 
mc filters results displayed tab 

average percentages sir steps tab 

ar gamma kjk delta bootstrap prior dist linearised dist table mc filters nonlinear time series percentage sir prior dist linearised dist table percentage sir steps nonlinear time series model requires simulation samples preceding 
fact variance dynamic noise important trajectories necessary explore space 
interesting algorithm sis suboptimal importance function greatly limits number resampling steps prior importance function avoiding mc integration step needed evaluate optimal importance sequential simulation methods bayesian filtering function 
roughly explained fact observation noise small highly informative allows limitation regions explored 

report overview sequential simulation methods bayesian filtering general hidden markov models 
overview includes general framework sis numerous approaches previously proposed independently literature nearly years 
original extensions 
re emerging area numerous ways improvement including new variance reduction methods efficient hybrid mcmc methods 

acknowledgments acknowledge dr higuchi pr 
kitagawa dr tanizaki sent preprints subject 
acknowledge andrieu clapp dr godsill various comments helped improve report 
state estimation systems measurements noise markov dependent statistical property algorithm random sampling proc 
th conf 
ifac 
construction discrete time nonlinear filter monte carlo methods variance reducing techniques sys 
cont vol 
pp 
japanese 
nose application monte carlo method optimal control linear systems measurement noise markov dependent statistical property int 
cont vol 
pp 

random sampling approach state estimation switching environments automatica vol 
pp 

sorenson nonlinear bayesian estimation gaussian sum approximation ieee trans 
auto 
cont vol 
pp 

anderson moore optimal filtering englewood cliffs 
andrade mendes optimal suboptimal nonlinear filtering problem discrete time systems ieee trans 
auto 
cont vol 
pp 

derin recursive algorithm bayes solution smoothing problem ieee trans 
auto 
cont vol 
pp 

stochastic simulation bayesian approach multitarget tracking iee proc 
radar sonar navigation vol 
pp 

fast weighted bayesian bootstrap filter nonlinear model state estimation ieee trans 

elec 
sys vol 
pp 

benyon monte carlo methods nonlinear non gaussian estimation math 
comp 
simul pp 

best gilks dynamic conditional independence models markov chain monte carlo methods forthcoming am 
stat 
assoc 
sequential simulation methods bayesian filtering bucy digital synthesis nonlinear filters automatica vol 
pp 

chen liu predictive updating methods application bayesian classification roy 
stat 
soc 
vol 
pp 

del moral non lin esolution la monte carlo vol 
pp 
french 
del moral esolution en traitement non lin du signal application radar sonar trait 
signal vol 
pp 
french 
doucet monte carlo approach recursive bayesian state estimation proc 
ieee 
hos june spain 
doucet implantation du paradigme bay pour estimation etat proc 
eme colloque juan les pins pp 
french 
doucet monte carlo methods bayesian estimation hidden markov models 
application radiation signals ph thesis univ paris sud orsay french chapters english 
doucet new computational method optimal estimation nonlinear non gaussian dynamic models proc 
feb 
doucet andrieu killing splitting scheme sequential importance sampling applied bayesian filtering preparation 
geweke bayesian inference econometrics models monte carlo integration econometrica vol 
pp 

gordon salmond smith novel approach nonlinear nongaussian bayesian state estimation iee proceedings vol 
pp 

gordon hybrid bootstrap filter target tracking clutter ieee trans 
aero 
elec 
sys vol 
pp 

mayne monte carlo techniques estimate conditional expectation multi stage non linear filtering int 
cont vol 
pp 

monte carlo techniques prediction filtering non linear stochastic processes automatica vol 
pp 

higuchi kitagawa monte carlo filter perspective genetic algorithm research memorandum institute statistical mathematics tokyo japan 
higuchi kitagawa monte carlo filter genetic algorithm operators research memorandum institute statistical mathematics tokyo japan 
higuchi bayesian model seasonal small count time series monte carlo filter approach technical report january 
sequential simulation methods bayesian filtering irwin cox kong sequential imputation linkage analysis proc 
nat 
acad 
sci 
usa vol 
pp 

isard blake contour tracking stochastic propagation conditional density proc 
europ 
conf 
comp 
vision cambridge pp 

stochastic processes filtering theory academic press 
kitagawa non gaussian state space modeling nonstationary time series am 
stat 
assoc vol 
pp 

kitagawa monte carlo filtering smoothing method non gaussian nonlinear state space models proc 
nd japan joint seminar statistical time series analysis honolulu hawaii pp 

kitagawa monte carlo filter smoother non gaussian nonlinear state space models comp 
graph 
stat vol 
pp 

kitagawa smoothness priors analysis time series lecture notes statistics vol 
springer 
kong liu wong sequential imputations bayesian missing data problems am 
stat 
assoc vol 
pp 

liu chen blind deconvolution sequential imputation am 
stat 
assoc vol 
pp 

liu independent sampling comparison rejection sampling importance sampling stat 
comp vol 
pp 

liu nonparametric hierarchical bayes sequential imputations ann 
stat 
liu chen monte carlo methods dynamic systems technical report department statistics stanford university 
mariano tanizaki simulation inference nonlinear state space models application testing permanent income hypothesis inference econometrics methods applications mariano weeks eds cambridge university press 
uller monte carlo integration general dynamic models contemporary math vol 
pp 

uller posterior integration dynamic models comp 
science stat vol 
pp 

ripley stochastic simulation wiley new york 
rubin sir algorithm simulate posterior distributions bayesian statistics eds bernardo degroot lindley smith oxford university press pp 

shapiro random distribution method applications solution problem nonlinear filtering discrete time radio eng 
elec 
phys vol 
pp 

shephard pitt likelihood analysis non gaussian measurement time series biometrika forthcoming 
sequential simulation methods bayesian filtering shephard pitt filtering simulation auxiliary particle filters technical report department statistics imperial college london october 
smith gelfand bayesian statistics tears perspective am 
stat vol 
pp 

stewart bayesian analysis monte carlo integration powerful methodology handling difficult problems stat vol 
pp 

stewart mccarty bayesian belief networks fuse continuous discrete information target recognition tracking situation assessment proc 
spie vol 
pp 

applying monte carlo method optimum estimation systems random disturbances auto 

cont vol 
pp 

tanizaki nonlinear filters estimation applications lecture notes economics mathematical systems springer berlin 
tanizaki mariano prediction filtering smoothing non linear non normal cases monte carlo integration app 
econometrics vol 
pp 

tanizaki mariano nonlinear nonnormal state space modeling monte carlo stochastic simulations forthcoming econometrics 
tanizaki nonlinear nonnormal filters monte carlo methods forthcoming comp 
stat 
data ana 
detection estimation abruptly changing systems automatica vol 
pp 

west mixtures models monte carlo bayesian updating dynamic models comp 
science stat vol 
pp 

west harrison bayesian forecasting dynamic models springer verlag series statistics nd edition 
monte carlo technique problems optimal data processing auto 

cont vol 
pp 

