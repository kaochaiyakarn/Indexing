stochastic versions em algorithm celeux diebolt mars programme institut national de recherche en informatique en automatique de recherche stochastic versions em algorithm celeux diebolt programme traitement du signal automatique projet rapport de recherche mars pages compare different stochastic versions em sem algorithm saem algorithm mcem algorithm 
suggest relevant contribution mcem methodology call annealing mcem algorithm turns close saem focus particularly mixture distributions problem 
thisis review available theoretical results behavior sem sample size tends infinity 
fi second part devoted intensive monte carlo real data study 
show situations sem algorithm preferable em annealing versions saem mcem em intricate mixtures algorithms confidently 
ed 
sem efficient data locating significant maxima likelihood function real data case show sem stationary contrasted view loglikelihood maxima 
key words incomplete data models stochastic algorithms mixture monte carlo experiments sum sur des versions de algorithme em resume nous diff de algorithme em les algorithmes sem saem mcem re que utilisation la plus de mcem une version de type simul qui le de saem 
le comportement de ces algorithmes un lange de lois de probabilit rob ce en revue les diff th la convergence de ces algorithmes aussi sur le comportement de sem 
une est la comparaison des la base de simulations de monte carlo sur une tude de el 
de ces il que sem est vent aux autres mais que pour des tr des algorithmes ne des 
ainsi tre vu comme un pour les maxima int de la 
de plus tude du cas el el bien en le fait que la distribution de une vue exploitable de la fonction de en diff entre les maxima de cette 
mots cles algorithmes de lois de simulations de monte carlo stochastic versions em algorithm gilles celeux didier jean diebolt inria alpes universit la vall ee cnrs universit paris vi 
compare different stochastic versions em algorithm sem algorithm saem algorithm mcem algorithm 
suggest relevant contribution mcem methodology call simulated annealing mcem algorithm turns close saem 
focus particularly mixture distributions problem 
context review available theoretical results convergence algorithms behavior sem sample size tends infinity 
second part devoted intensive monte carlo numerical simulations real data study 
show particular mixture situations sem algorithm preferable em simulated annealing versions saem mcem 
intricate mixtures algorithms confidently 
sem efficient data exploratory tool locating significant maxima likelihood function 
real data case show sem stationary distribution provides contrasted view loglikelihood emphasizing sensible maxima 

em algorithm dempster laird rubin popular efficient approach maximum likelihood ml estimation locating posterior mode distribution tanner wong green wei tanner incomplete data 
despite appealing features em algorithm documented limitations limiting position strongly depend starting position rate convergence painfully slow provide saddle point likelihood function local maximum 
cases maximization step em intractable 
authors proposed various improvements em algorithm louis silverman jones wilson green 
improvements resulted completely satisfactory version em 
basic motivation stochastic versions em algorithm study overcome mentioned limitations em 
stochastic versions em sem algorithm celeux diebolt celeux diebolt saem algorithm celeux diebolt mcem algorithm wei tanner tanner 
purpose compare characteristics stochastic versions em focus relationships mcem algorithms 
motivations simulation step making pseudorandom draws iteration sem mcem saem variant sem 
hand simulation step sem relies stochastic imputation principle sip generate pseudo completed samples drawing potential unobserved samples conditional density observed data current fit parameter 
hand mcem replaces analytic computation conditional expectation log likelihood complete data observations monte carlo approximation 
despite key words phrases 
stochastic iterative algorithms incomplete data maximum likelihood estimation stochastic imputation principle ergodic markov chain 
typeset stochastic versions em algorithm different motivations sem saem mcem considered random perturbations discrete time dynamic system generated em 
reason successful behavior random perturbations prevent algorithms staying near unstable hyperbolic fixed points em stable fixed points corresponding insignificant local maxima consequence mentioned possible slow convergence situations em avoided 
underlying em dynamics helps find estimates parameter comparatively small number iterations 
statistical considerations directing simulation step algorithms lead proper data driven scaling random perturbations 
section algorithms general setting ideas underly key properties 
section show algorithms apply mixture problem 
measure density finite mixture densities parameterized family ag finite integer weights sum 
mixture problem consists identifying weighting parameters pk parameters ak component densities basis sample observations yn issued 
mixture problem variations extensions relevant areas application em methodology redner walker titterington smith makov 
section main results concerning convergence properties algorithms em sem saem mcem mixture problem setting reviewed 
section detailed monte carlo simulations illustrate compare practical behavior algorithm finite mixture situations 
section em sem compared real data set studied basford mclachlan 

em algorithm stochastic versions em algorithm 
em algorithm dempster iterative procedure designed find ml estimates context parametric models observed data viewed incomplete 
subsection briefly review main features em 
observed data supposed issued density yj respect oe finite measure dy 
objective estimate arg maxl log yj 
basic idea em take advantage usual expressibility closed form ml estimate complete data 
denotes unobserved latent data 
em algorithm replaces maximization unknown xj complete data successive conditional expectation log xj current fit parameter 
formally zjy xj yj denote conditional density respect oe finite measure dz 
log gamma xj delta zjy dz denotes sample space latent data current approximation ml estimate observed data em iteration tn involves steps step computes step determines arg max 
updating process repeated convergence apparent 
em algorithm basic property iteration increases equality iff 
detailed account convergence properties sequence generated em dempster 
wu 
suitable regularity conditions converges stationary point 
celeux diebolt stationary points local maxima minima saddle points necessarily converge significant local maximum 
practical implementations em algorithm observed extremely slow important applications 
noted dempster 
convergence rate em initial position far true value parameter linear governed fraction missing information 
slow convergence generally appears proportion missing information high 
log likelihood surface saddle points sub optimal maxima limiting position em greatly depends initial position 
sem algorithm 
sem algorithm celeux diebolt celeux diebolt designed answer mentioned limitations em 
basic idea underlying sem replace computation maximization simpler computation zjy simulation unobserved update basis pseudo completed sample 
sem incorporates stochastic step step steps 
step directed stochastic imputation principle generate completed sample drawing conditional density zjy observed data current fit parameter 
sem basically tests mutual consistency current guess parameter corresponding samples 
updated estimate ml estimate computed basis analytic expression function derived closed form relevant situations 
worth noting situations step em algorithm analytically tractable step context weibull mixtures censored data see case convolution blind source separation model see moulines 
sem maximizes log pseudo completed data involve difficulties 
random drawings prevent sequence generated sem converging stationary point log encounters 
iteration non zero probability accepting updated estimate lower likelihood value basic reason sem avoid saddle points insignificant local maxima em initiated sem parameter estimate attains maximum value 
sem exploited better fashion 
random sequence generated sem converge pointwise 
turns sequence homogeneous markov chain irreducible zjy positive condition satisfied contexts sem applied 
turns ergodic converges unique stationary probability distribution markov chain 
sem seen stochastic perturbation em algorithm directed em dynamics see 
sem expected detect stable fixed point em comparatively small number iterations 
degree stability fixed point em measured largest eigenvalue matrix gamma gamma obs identity matrice obs complete observed fisher information matrices respectively see dempster redner walker 
shown numerical evidence see celeux diebolt stationary distribution sem sequence expected concentrate stable fixed point em information available knowing missing data gamma obs largest 
accordance approach cutler base estimate number mixture components smallest eigenvalue gamma obs situation similar prevailing bayesian approach viewed posterior probability resulting bayes formula 
bayesian perspective information inference contained probability distribution empirical stochastic versions em algorithm mean sufficiently large number simulations provides point estimate 
simulation step bayesian sampling algorithm see replaced sem deterministic ml step variance matrix expected smaller inverse observed data fisher information matrix 
bayesian perspective tractability complete data likelihood zj viewed posterior density complete data jy zj theta du prior density 
context natural replace step sem step simulation jy 
essence data augmentation algorithm tanner wong considered bayesian version sem 
alternatively sem recovered data augmentation algorithm suitable noninformative prior replacing simulation step jy imputation step updated jy 
see diebolt robert details mixture context 
sip expected provide satisfactory data driven magnitude random perturbations sem 
instance sample observed data small contains little information true value parameter variance random perturbations turns large 
natural case guess updated arising pseudo completed sample generated zjy comparatively far high probability variance stationary distribution sem large 
erratic behavior sem difficult handle small sample sizes 
reason introduced saem algorithm described subsection 
saem algorithm 
saem algorithm celeux diebolt modification sem algorithm convergence distribution replaced convergence possible erratic behavior sem small data sets attenuated sacrificing stochastic nature algorithm 
accomplished making sequence positive real numbers fl decreasing zero fl parallels temperatures simulated annealing see van laarhoven aarts 
precisely current fit parameter saem updated approximation gamma fl em fl sem em resp 
sem updated approximation em resp 
sem 
saem going pure sem pure em 
choice rate convergence fl important 
typically slow rate convergence necessary performance 
practical point view important fl stays near fl iterations algorithm avoid suboptimal stationary values 
theoretical point view see section mixture setting essentially need conditions lim fl fl fl ensure convergence saem local maximizer log starting point 
mcem algorithm 
mcem algorithm wei tanner proposes monte carlo implementation step 
replaces computation empirical version ae drawings zjy 
precisely rth step generate sample zjy update current approximation log celeux diebolt step provides arg max 
mcem reduces sem 
large mcem works approximately em drawbacks em 
maximizing turn nearly difficult maximizing 
wei tanner motivated mcem algorithm alternative replaces analytic computation integral numerical computation monte carlo approximation integral 
contrary sem involve exact approximate computation computation involved step conditional density zjy 
step generally straightforward consists maximizing likelihood completed sample 
situations sem works preferred mcem 
discussion points numerical integration real interest mcem 
comments wei tanner tanner specification turns real interest mcem simulated annealing type version spirit saem algorithm 
wei tanner recommend start small values increase moves closer true maximizer 
precisely select sequence fm integers increases infinity suitable rate perform rth iteration go pure sem pure em 
variance random perturbation term decreases zero resulting mcem version viewed particular type simulated annealing method playing role temperature 
brevity call algorithm simulated annealing mcem algorithm mcem 
note mcem tractable expression derived contrast saem 
instance context weibull mixtures censored data mcem avoids numerical integration involved step em consequently step saem 
stimulating qian titterington introduced stochastic algorithms relying general restoration maximization rm principle 
principle consists restoring missing data iteration 
qian titterington described possible methods restoration 
closely related mcem 
rth step rm algorithm generate sample zjy compute average gamma update current estimation arg max 
rm algorithm reduces sem 
rm coincides mcem zj linear function true general 
important situation linearity occurs mixture problem 
zj linear estimators obtained rm mcem different 
case rm provide ml estimator expected give poor results 
instance estimating coefficients variance matrix multivariate gaussian vector sample values missing random shown rm introduces underestimating bias 
hand rm algorithm avoids potential computational drawbacks mcem 
wei tanner established convergence result mcem simulated annealing version 
section theorem ensures convergence simulated annealing mcem local maximizer suitable sequences fm reasonable assumptions mixture setting 
result shows interest version mcem 
proved derived previous results celeux diebolt convergence saem 

basic example mixture case 
incomplete data structure mixture data 
focus mixture distributions problem 
areas em methodology significant contributions 
authors studied behavior stochastic versions em algorithm em context practical theoretical point view redner walker titterington smith makov celeux diebolt mclachlan basford titterington 
simplicity restrict mixtures densities exponential family see redner walker celeux diebolt 
observed sample assumed drawn mixture density mixing weights sum gamma dimensional vector parameter sufficiently smooth functions normalizing factor standard inner product parameter estimated pk gamma ak lies subset theta gamma sk context complete data written gamma delta vector indicator variables ij defined ij depending ith observation drawn jth component density 
owing independence zjy split product jy probability vector zjy defined zjy iff 
conditionally observations probability drawn jth component log takes form log gamma delta titterington gamma log log delta 
em 
step em algorithm computes posterior probabilities ij step provides updating formulas ij ij ij 
sem saem 
step sem 
step independently draws multinomial distribution parameters ij 
ij celeux diebolt goes step 
threshold satisfying 
role condition avoid absorbing points markov chain generated sem numerical singularities step 
typically chose celeux diebolt 
satisfied new drawn preassigned distribution holds algorithm goes step 
step provides ij ij ij formulas saem algorithm directly derived descriptions em sem 

mcem 
turn description mcem algorithm mixture case 
step subsection 
monte carlo step generates independent samples indicator variables gamma delta conditional distributions gamma ij delta definition mcem updated approximation maximizes log log gamma delta iff ij 
equality written ij gamma log log delta ij fh ij represents frequency assignment jth mixture component rth iteration drawings 
comparing appears mcem just replaces probabilities ij frequencies ij formula resulting step em algorithm 
sem random drawings lead started afresh suitable distribution condition satisfied 
noticed starting increasing infinity iteration index grows produces mcem algorithm quite analogous saem 
mcem sense elegant natural saem dramatically time consuming saem involves random drawings 
illustrated simulations sections 

convergence properties section surveys main results concerning convergence properties algorithms examined particular setup finite mixtures exponential family area application em various versions precise results available 
concerning em redner walker proved local convergence result context mixtures exponential family fisher information matrix evaluated true positive mixture proportions positive probability sufficiently large unique strongly consistent solution likelihood equations defined sequence generated em converges linearly starting point sufficiently near established similar result context mixtures censored data exponential certain non exponential families 
stochastic versions em algorithm concerning sem celeux diebolt proved ergodicity sequence generated sem mixture context 
proof reduces showing sequence fz finite state homogeneous irreducible aperiodic markov chain 
result guarantees weak convergence distribution unique stationary distribution ergodic markov chain generated sem 
index indicates dependence sample 
result guarantee concentrated consistent ml estimator 
celeux diebolt diebolt celeux examined asymptotic behavior start showing sem sequence satisfies recursive relation tn vn tn denotes em operator vn theta measurable function converges distribution uniformly gn compact subset theta gaussian mean positive variance matrix 
particular case mixture mixing proportion unknown parameter established stationary distribution sem asymptotically gaussian mean unique maximizer variance order gamma diebolt celeux 
similar result censored mixture 
results suggest similar behavior hold general mixture context 
celeux diebolt prove result stringent assumption unique fixed point tn concerning saem expressed tn fl vn see celeux diebolt established context finite mixtures exponential family sequence generated saem converges local maximizer starting point 
result holds basic assumption fl decreases lim fl fl fl 
em possibility convergence saddle point 
result ensures saem converge point basic reason saem achieves better results em saem necessarily terminate local maximum encountered 
concerning mcem algorithm expressed tn fz represents vector samples drawn iteration theta measurable function established similar result context assumption exists positive constant ff ff 

simulation procedure purpose compare practical behavior em proposed stochastic versions objective way 
proceeded intensive monte carlo numerical experiments 
examples chosen reflect summarize typical situations want point 
tried situations shown 
general description simulation procedure section necessary catch wide range experiment 

estimation 
sem provide directly pointwise estimate sem schemes derive estimate 
schemes proposed celeux diebolt 
sem mean stage sem procedure warm step consisting iterations sem starting position performed reach stationary regime 
celeux diebolt average estimates iterations 
provides point estimate sem sem em hybrid algorithm sem iterations performed warm 
run em starting position achieved largest value observed likelihood function supposed starting position em 
sem algorithms chose duration warm step fixed proportion th total number iterations algorithms see section 
general procedure compares algorithms em sem mean sem em saem mcem 
saem cooling schedule defined previous experiments celeux diebolt fl cos rff fr cos ff mcem equivalent cooling schedule obtained choosing appropriate 

mixtures consideration 
consider mixtures univariate gaussian distributions parameter component densities delta oe 
selected values chosen give situations hard identify course separated mixtures algorithms perform 
give true parameters pk gamma ak mixtures consideration corresponding densities depicted 
intricate component mixture means close resulting density unimodal skewed right 
parameter component mixture zero means different variances resulting unimodal symmetric density heavy tails 
parameter component mixture separated subpopulations intricate ones 
parameter fig 
true mixtures densities stochastic versions em algorithm 
sample sizes number iterations 
monte carlo experiment trials replications different independent samples selected sample sizes small large selected numbers iterations cases 
warm step needed consisted iterations recording step em step sem em iterations 

initialization 
trial initial position computed initialization schemes defined 
algorithms initial position 
considered initialization schemes true just starts true parameter equal computes sem step starting ij sem step yields initial parameter means draws seeds uniformly observations aggregates observations nearest neighbor procedure resulting partition determines vectors indicators sem step performed yielding initial position 
restarts 
need restart sem algorithms threshold condition equation fulfilled 
goal give equal chance success algorithms define condition similar algorithms need restart procedure em mcem 
natural condition obtained replacing ij probabilities ij em simulated frequencies ij mcem 
give equal chance algorithm defined restarting procedure trial sample follows starting position computed methods algorithm starts position 
algorithm fails fulfill threshold condition iteration restarted initial position path computed 
algorithm fails consecutive times way failure recorded estimate algorithm trial notice em restart identical immediate failure em sequence determined sample starting position 
random drawings trajectory steps monte carlo steps mcem give chance success restart notice procedure consisting dirac ffi restarting distribution favor sem methodology 
major reasons sem algorithm fail poor initial position close boundary 
natural restart strategy diffuse restart distribution order restart possibly better initial position 
point strategy lead non comparable trials case trial general observed results algorithms started different initial positions say em sem mean sem em saem mcem 

switching estimates 
known problem arising mixture setting likelihood function attains largest local maximum different choices see redner walker 
value change component pairs interchanged 
effect label switching usually concern goal just find estimate particular mixture basis real data estimate just obtained label switching 
unfortunately monte carlo experiment replications situation celeux diebolt switching great importance 
situations largest local maximum easy find sequence generated em sem type algorithm converge 
equivalent maxima obtained label switching pairs case computing statistics averages replications lead wrong 
example components intricate mixture equivalent maxima mutually close label switching occur roughly trials 
computing average trials care possible label switching obviously result averaging heterogeneous component estimates 
overcome problem determine label switching occurred sequence produced estimate particular trial 
reasonable idea compare estimate algorithm consideration known true parameter computing distance trying find permutation pairs minimizes 
checked possible expressions euclidean distance distance just coordinate 
retained methods deciding switch occurred trying switch back right permutation 
methods behaved properly simple situations switch back right permutation just looking closely results 
expect intricate situations 
tried methods 
mean distribution means method tries find switch distance gamma smallest 
expected method works mixtures significantly different means 
var mean distribution variances 
method works mixtures means significantly different variances 
class method uses measure accuracy estimate percentage individuals sample assigned algorithm original subpopulation 
measure discussed 
method tries switch back permutation maximizing measure expected behave general situations previous ones 

comparing algorithms 
major difficulties extract comparison algorithms consideration results extensive simulations 
obvious way find algorithm giving accurate estimates 
things simple results mixture depending various sample sizes number iterations initialization schemes 
finding behavior algorithms situations easy 
just outlined significant tendencies revealed experiments 
judgments estimates considerations 
important property mixture setting ability estimation procedure properly separate components means measure euclidean distance true parameter estimate necessarily best choice compute distance meaningful way choosing weights coordinates 
reason computed measure ability find back original partition components denoted tables class 
measure just ratio percentage number individuals sample assigned algorithm proper component 
simulated data true component observation drawn say known 
define assignment algorithm observation arg max kk gamma ik delta stochastic versions em algorithm point estimate algorithm computation estimate depends selected algorithm 
obtain class comparing preliminary experiments separated mixtures showed measure dimensional indicator 
note indicator employed celeux evaluate performance mixture estimation methods 
provide additional information help comparisons 
explained 
mainly concern number failures restarts necessary algorithm find estimate 

reading tables 
table relative experiment 
experiment consists selection factors mixture sample size number iterations initialization method true equal means switching method mean var class computation replications factors 
table column true true values parameters column mle maximum likelihood estimates parameters complete data knowing indicator component density observation 
estimate gives insight available information complete data helps check random number generator 
subsequent columns give estimates information selected algorithms 
information set rows failed number trials replications algorithm failed consecutive restarts 
restarts number restarts averaged successful trials 
number successful trials required restart 
time ms average elapsed time trial sun sparc workstation 
frequency component switching trial 
class measure empirical success rate described averaged successful trials 
subsequent rows give parameter estimates standard deviation computed replications 
point standard deviation measure variance estimates 

computer softwares 
programs computing simulation procedure described section written sun sparc workstation pascal double precision 
floating point software follows ieee standard pseudorandom number generator routine defined ieee math library figures drawn mathematica 
selected results said results section reflect typical behaviors roughly experiments models 
particular give results situation difficult handle 

mixture equal means 
celeux diebolt ran experiments mixture model sample sizes iteration numbers initialization schemes 
appeared sem mean sem em algorithms better em simulated annealing versions saem mcem 
difference significant equal means initializations cases bad guess 
table summarizes typical results obtained situation moderate sample sizes large number iterations 
highlight measure accuracy class particularly significant separates sem algorithms sem mean sem em ones 
estimates significantly better sem mean sem em especially oe oe standard deviations replications smaller sem mean sem em 
see sem mean needed restarts average replication failed trials means needed restarts 
note em failed twice 
table mixture init means switching var items true mle em sem mean sem em saem mcem failed restarts time ms class oe oe oe oe oe oe oe oe oe 
intricate mixture 
mixture harder recover 
situation things better sem experiments em considering experiments general tendency clear 
detailed discussion situation section 
notice situation empirical success rate class separates algorithms sets em algorithm sem algorithms sem mean sem em simulated annealing algorithms saem mcem 
significant tables favor measure comparisons 
mixture expected section label switching detected switching method time 
table 
table measure class clearly favor sem em sem mean algorithms estimates previous situation 
consider ability separate means sem mean performs better 
sem estimates oe poor 
highlight fact sem mean needed restarts average succeed sem em restarts 
consequence sem mean required computing time 
sem mean failed trials ones estimate averages computed basis successful trials 
stochastic versions em algorithm notice experiment sample size small ran comparatively large number iterations 
explain large number restarts needed sem algorithms 
notice em failed trials 
table mixture init means switching class items true mle em sem mean sem em saem mcem failed restarts time ms class oe oe oe oe oe oe oe oe oe table example class just slightly better sem algorithms really informative 
considering estimates see algorithms succeeded separating means 
inversion em sem em mcem 
estimation variance oe especially poor sem algorithms estimation oe poor algorithms 
notice number restarts failures sem smaller previous case due larger sample size 
table mixture init means switching class items true mle em sem mean sem em saem mcem failed restarts time ms class oe oe oe oe oe oe oe oe oe table experiment reflects situation small sample size small number iterations 
class non significant comparing algorithms 
em especially mcem performed better separating means 
estimates provided em general accurate 
smaller number restarts explained smaller number iterations required compute point estimate 
celeux diebolt table mixture init means switching class items true mle em sem mean sem em saem mcem failed restarts time ms class oe oe oe oe oe oe oe oe oe results confirm difficulty algorithms recover intricate mixture 
sem appears slightly better sufficiently large number iterations class comparison 
em preferable small number iterations 
em slow convergence 
usual question em sem possibility huge amount em iterations order overcome inconvenient slow convergence 
illustrate fact iterations em lead non satisfactory estimates tried experiments mixture iterations 
results experiment displayed table 
em algorithm computed huge amount computing time take run experiment algorithms 
notice means poorly separated estimates oe far true values 
results significantly better previous experiments computed reasonable numbers iterations 
table mixture init equal switching class items true mle em failed restarts time ms class oe oe oe oe oe oe oe oe oe 
mixture components 
mixture intricate components labeled components easier recover 
table illustrates typical results obtained 
class clearly favor sem algorithms 
algorithms properly separates means 
distinction stochastic versions em algorithm sem lies estimation components 
sem mean provides accurate estimates sem em slightly accurate 
em simulated annealing saem mcem give poor estimates means 
derived estimates oe oe standard deviations replications estimates oe significantly smaller sem algorithms 
particularly obvious row oe oe 
notice frequency switches greater 
equivalent local maxima previous experiments 
table mixture init means switching class items true mle em sem mean sem em saem mcem failed restarts time ms class oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe 
detailed executions monte carlo experiments shown difficulty algorithms recover intricate mixture 
stationary distribution sem concentrate maxima likelihood function tried plot paths sem mean em trials 
relative trial selection set factors section computation run selected algorithms possible failures restarts 
interested trial need switch 
pictures projection path sem mean em 
smaller dots sem mean em iterations big dots iterations preceding failure sem mean locations sem mean needed restart 
addition particular locations interest shown true value starting position sem mean estimate em estimate em computed 
em iterations easily distinguished sem mean iterations distance consecutive em iterations quite small em path usually looks curved line starting position em estimate 

depicts recovering mixture small sample large number iterations 
celeux diebolt notice sem mean needed huge number restarts finding estimate restarts roughly located lines 
restarts correspond typically situations sem tried fit unique gaussian population giving component weight 
illustrates ability stationary distribution sem concentrate equivalent symmetric maxima likelihood function obtained switching components close locations 
initialization method equal gives halfway equivalent symmetric maxima 
fortunately trial sem estimate computed path close estimate reasonable label switching 
see necessarily case 
em path walked near switched local maximum went restart direction computation iterations lead em failure 
fig 
em sem mean iterations init equal switch em estimate sem estimate true value sem failures starting position 
depicts situation sem mean plotted 
notice initialization method means gives starting position clearly belonging half plane containing switched value 
restarts 
interesting see sem mean concentrates symmetric maxima despite initial position 
estimate computed averaging iterations sem stationary regime fairly poor halfway equivalent true values 
reason clear stationary regime sem visited alternatively symmetric maxima likelihood function stationary distribution presents equivalent modes 
case computing average iterations lead really poor estimates 
stochastic versions em algorithm fig 
sem mean iterations init kmeans switch sem estimate true value sem failures starting position 
em sem real data set data set consideration taken van den 
consists population women carriers described variables log activity log antigen 
data set basford mclachlan illustrate possibility multiple roots likelihood equations analyzing mixture multivariate normal distributions 
case study basford mclachlan considered normal mixture model equal variance matrices model unequal variance matrices model identified likelihood estimates applying em algorithm 
wide choice starting values concluded existence local maxima model denoted displayed table local maxima model denoted displayed table 
section parameter subscripts denote component indicator 
data set illustrate behavior sem em sem mean defined section describe behavior stationary distribution sem situation existence local maxima suspected 
order estimate sem stationary distribution ran sem iterations warm step different starting values chosen random chosen solutions exhibited basford mclachlan 
situations sem em lead parameter estimate provides global maximum loglikelihood function 
global maximizer table 
heteroscedasticity parameter gamma gamma gamma gamma provides loglikelihood gamma 
worth noting survey refine strategy basford mclachlan consisting initiating em number different positions failed locate global maximum 
sem em appears better survey refine strategy parameter space high dimension 

model 
celeux diebolt consider model 
appears fixed point em 
running em initial position sufficiently large number iterations leads slow convergence 
small number iterations fixed point em result basford mclachlan 
pointed short run iterations sem em initial position leads em fixed point detected basford mclachlan new em fixed point gamma gamma gamma gamma loglikelihood gamma 
model sem reveals efficient strategy avoid slow convergence situations met em 
turn analysis sem stationary distribution 
analysis illustrates possibility sem produce label switching apparent displays histogram proportion sem run iterations distribution nearly symmetric respect 
propose heuristic attempt overcome switching difficulty consider sem iterations iterations satisfy condition iterations 
appears restricted sem stationary distribution depend starting position 
table summarizes distribution starting value chosen random equal scheme described section 
noted median coordinate distribution closer corresponding coordinate global maximizer mean 
purpose having graphical view sem restricted stationary distribution twodimensional histogram displayed proportion mean proportion important parameter mixture model looking table able discriminate fixed points em 
shows sem concentrates time global maximizer peak low ridge line joining non desirable local maxima corresponding slow convergence area em 
sem stationary distribution seen providing contrasted view loglikelihood function flattening local maxima emphasizing sensible maxima 
table basford mclachlan solutions heteroscedasticity sigma sigma sigma sigma sigma sigma 
table basford mclachlan solutions sigma sigma sigma 
stochastic versions em algorithm table summary empirical restricted sem stationary distribution quartile third quartile std standard deviation median mean std 
sigma sigma sigma proportion frequency fig 
marginal stationary distribution sem proportion iterations fig 
marginal stationary distribution sem mu iterations mu freq mu freq celeux diebolt 
seen mixture situations mixtures equal means mixtures separated populations sem algorithms general better em simulated annealing versions 
intricate mixtures general preference proposed 
detailed executions show basic reason sem mean leads poor estimates averaging method computing point estimate adapted stationary distribution multiple equivalent modes 
intricate mixtures algorithms confidently sem reveals efficient data exploratory tool 
illustrations figures real data experiment show running sem large number iterations useful locate significant potentially equivalent local maxima likelihood function 
regions interest parameter space theta determined sem results practical considerations prior information coming experimental field 
basford mclachlan 
estimation normal mixture models applied statistics 

sure convergence class stochastic algorithms processes applications 
celeux diebolt 
reconnaissance de de es par un algorithme apprentissage data analysis informatics diday eds 
amsterdam north holland 
celeux diebolt 
sem algorithm probabilistic teacher algorithm derived em algorithm mixture problem computational statistics 
celeux diebolt 
comportement un algorithme apprentissage pour les de lois de probabilit rapport de recherche inria 
celeux diebolt 
probabilistic teacher algorithm iterative maximum estimation classification related methods data analysis bock ed amsterdam north holland 
celeux diebolt 
em sem algorithms mixtures statistical numerical aspects cahiers du 
celeux diebolt 
stochastic approximation type em algorithm mixture problem stochastics stochastics reports 
celeux 
comparison mixture classification maximum likelihood cluster analysis journal statis 
comput 
simul 


extension des algorithmes em sem la reconnaissance de es de distributions de ph thesis universit paris sud orsay france 

algorithmes em sem pour un de distributions de application la revue de statistique appliqu ee 

stochastic em algorithm mixtures censored data statist 

inference appear 
dempster laird rubin 
maximum incomplete data em algorithm discussion journal royal statistical society 
diebolt celeux 
asymptotic properties stochastic em algorithm estimating mixture proportions stochastic models 
diebolt robert 
estimation finite mixture distributions bayesian sampling journal royal statistical society 

classification mixture approach clustering maximum likelihood applied statistics 
green 
em algorithm penalized likelihood estimation journal royal statistical society 
stochastic versions em algorithm van den brock 
stepwise discriminant analysis program density estimation proceedings computational statistics wien physica verlag 
van laarhoven aarts 
simulated annealing theory applications reidel dordrecht 

stochastic algorithm parametric non parametric estimation case incomplete data signal processing appear 
moulines 
stochastic approximation version em algorithm preprint universit paris sud submitted 
louis 
finding observed information matrix em algorithm journal royal statistical society 
mclachlan basford 
mixture models inference applications clustering new york marcel dekker 

fast improvement em algorithm terms journal royal statistical society 

properties adding smoothing step em algorithm statistics probability letters 
qian titterington 
estimation parameters hidden markov models phil 
trans 
soc 
lond 
redner walker 
mixtures densities maximum em algorithm siam review 
silverman jones wilson 
smoothed em approach indirect estimation problems particular emission tomography journal royal statistical society 
celeux diebolt robert 
analyse de pour de application la cin ematique revue de statistique appliqu ee 
tanner 
tools statistical inference lectures notes statistics new york springer verlag 
tanner wong 
calculation posterior distribution data augmentation discussion journal american statistical association 
titterington 
research analysis mixture distribution statistics 
titterington smith makov 
statistical analysis finite mixture distribution new york wiley 
wei tanner 
monte carlo implementation em algorithm poor man data augmentation algorithms journal american statistical association 
cutler 
information ratios validating cluster analyses journal american statistical association 
wu 
convergence properties em algorithm annals statistics 
inria rh alpes de de grenoble dept de statistique france mail address gilles celeux imag fr universit la vall ee analyse de math ematiques appliqu ees rue de la noisy le grand cedex france 
mail address math univ fr cnrs ura eme universit paris vi place jussieu paris cedex france 
mail address ccr jussieu fr 
