bayesian inference hidden markov models reversible jump markov chain monte carlo christian robert tobias en titterington crest paris lund university university glasgow hidden markov models form extension mixture models providing flexible class models exhibiting dependence possibly large degree variability 
show reversible jump markov chain monte carlo techniques estimate parameters number components hidden markov model bayesian framework 
employ mixture zero mean normal distributions main example apply model sets data finance meteorology respectively 
ams classification 
primary 
secondary 
key words 
hidden markov model bayesian inference model selection markov chain monte carlo 
hidden markov models hmms areas convenient representations weakly dependent heterogeneous phenomena examples econometrics hamilton chib finance en biology puterman genetics churchill neurophysiology fredkin rice speech processing rabiner 
refer monograph macdonald 
hidden markov models heterogeneity data represented mixture structure pair kg jz 
assumed independent conditional 
markov nomenclature comes assuming fz distributed finite state markov chain hidden part due fact fz observed 
commonly conditional distributions belong single parametric family normal poisson families corresponds parameter generate inference hmms considered baum petrie treated case takes values finite set 
petrie provided identifiability conditions hmms laboratoire de statistique crest timbre paris cedex france department mathematical statistics lund university box lund sweden department statistics university glasgow glasgow qq scotland baum 
considered maximum likelihood implementing early version em algorithm deriving general proof monotonicity algorithm 
came procedure allows derivation conditional joint distribution see appendix 
proved consistency mle general hmms mild conditions asymptotic normality proved bickel 

apply estimation hmm parameters number values say known 
concerned inference unknown 
approach adopted bayesian inference implement reversible jump markov chain monte carlo techniques proposed green 
non bayesian approaches problem include tests likelihood ratios penalized likelihood methods 
method postulate null hypothesis alternative compute likelihood ratio reject statistic large 
standard theory say true likelihood ratio asymptotically distributed random variable 
result true hmms mixtures essentially true parameters uniquely identified hmms limiting distribution likelihood ratio computed numerically approximated simulation 
mclachlan context mixtures proposed approximate parametric bootstrap en 
applied idea hmms 
approach requires enormous computational efforts likelihood ratio statistic time consuming compute usually bootstrap replications simulated 
en 
able take approach testing vs computational difficulties 
penalized likelihood methods akaike bayesian information criteria aic bic demanding simulated replications involved 
hand produce number quantifies confidence result value 
desired confined parametric bootstrap 
bayesian inference reversible jump markov chain monte carlo methods viable alternative frequentist analysis explores models different values provides figures representing confidence different models 
appealing feature approach bypasses traditional model choice structure requires prior modelling testing different models 
alternative standard hierarchical modelling estimates posterior probabilities different models 
richardson green developed similar methods mixture distribution analysis respects development achievements 
continues robert 
chib robert titterington bayesian estimation hmms fixed number components 
case mixtures hmms unknown number components appealing settings heterogeneous population unknown degree heterogeneity may come scrutiny value matters subsequent analysis secondly density dependent sample may estimated hmm structure provides parsimonious alternative standard nonparametric estimates 
organised follows section details prior modelling section details steps reversible jump mcmc algorithm markovian structure reversibility constraint lead higher level complexity richardson green section examines performance method datasets 
bayesian model choose study mixtures zero mean normals partly want analyse data model previously fitted likelihood techniques see section partly provide variety structure considered richardson green 
wide range mixtures dealt similar lines 
marginal distribution observation oe components stationary vector transition matrix hidden states ij jjz ij set oe denoted oe 
lines similar section richardson green assume joint density variables mentioned far takes form oe ajk particular priori uniform max max specified rows independent dirichlet distribution ffi ffi ffi oe uniform ff sorted ascending order parameters components identifiable 
addition assume ff negative exponential distribution mean max jy reason putting hyperprior ff posterior robust case ff fixed current model expand 
hand chose particular value ffi fact ffi 
complete joint density corresponding richardson green ff oe ff ajk ffi ff mcmc methodology general theory methodology mcmc procedures fairly standard see instance tierney gilks 

particular approach clearly especially indebted formative reversible jump mcmc green richardson green background discussion particularly valuable 
simply recall reversible jump techniques proposed green overcome measure theoretic difficulties implementing standard mcmc algorithms problems varying dimension parameters 
fundamental idea green technique impose transition steps dimension space varies dominating measure chosen positive 
mixture set moves dimension changes restricted called split merge moves component broken components conversely components single component 
inferences sample sample size objective generate realizations conditional joint density ff defined detailed description sweep mcmc procedure follows update transition probability matrix update standard deviations oe update allocations update hyperparameter ff consider splitting component combining consider birth death empty component 
gibbs moves move types gibbs moves follow robert 

th row sampled dirichlet distribution ffi ffi ik ij gamma ifz jg number jumps component component deltag denotes indicator function 
oe gamma sampled truncated gamma distribution density proportional gamma gamma ifu ff ifz ig ifz respectively number observations allocated component corresponding sum squares 
sampling carried simply rejecting gamma random variates condition ff fulfilled sophisticated approaches include slice sampler damien walker 
sampling oe new set oe accepted ascending update rejected 
resampled time conditional probabilities delta delta delta gamma oe iz delta oe density normal random variable zero mean standard deviation oe factor replaced stationary probability factor replaced unity 
ff sampled distribution density proportional gammak gamma ifu max oe sampling density carried method damien walker 
split combine moves moves involved metropolis hastings steps allow increasing decreasing number components 
choose split probability combine probability gamma naturally kmax max gamma 
describe combine move suppose current state mcmc algorithm parameters ij representation components 
randomly select pair adjacent components try combine single new component creating new state components 
done steps 
equal set remaining simply copied 
secondly standard deviation new state oe oe oe remaining oe copied 
stationary probability hidden chain state vector satisfies 
thirdly transition probabilities components involved move set ij ij ij remaining ij obtained equating row sums copying 
note expression conditional probability jumping component current component note new transition probability matrix essentially stationary probabilities readily verify turn implies new hidden markov model oe second moment oe oe oe obviously moments hmms consider identically zero 
split move component randomly chosen split new ones describing assume old representation components new components 
designing split move starting point result 
theorem 

assume new transition probabilities ij ij ij gamma ij gamma taken new stationary probabilities gamma remaining ij obtained copying 
assumed pair chosen ij non negative 
eigenvalue right eigenvector eigenvalue right eigenvector addition gamma gamma gamma theta eigenvalue right eigenvector gamma gammau holds ij ij theorem shows splitting transition probabilities dynamics hidden markov chain essentially preserved difference dynamics new components variable determines stationary probability assigned new component determines dynamics new components 
way splitting component essentially deterministic reversible jump mcmc algorithm 
take guideline designing random split move 
aim split way stationary probabilities hidden chain preserved gamma accomplish follows shape parameters 
set ij ij set gamma gamma ij ij ij gamma ij gamma gamma gamma ij gamma set rows sum unity 
gamma equilibrium equation ij holds expression ensures holds shape parameters taken gamma gamma gamma gamma gamma produces beta distribution mean squared coefficient variation squared coefficient variation distribution mirrored version distribution obtained gamma 
numerical results 
note conditional means conditional means ij ij ij gamma ij respectively 
consistent theorem 
discuss valid range range resulting stochastic max gamma gamma theta ij gamma min gamma gamma theta ij gamma gamma theta gamma gamma may happen case valid split move rejected 
draw symbolically written gamma 
new standard deviations created oe oe oe oe gamma gamma oe oe gamma range oe properly sorted max oe gamma oe gamma oe gamma oe gamma oe oe gamma oe ff gamma oe 
draw 
beta variables independent 
discuss hidden chain split giving different simply copied equal assume gamma sample conditional distribution vector remaining proposed new ij oe 
employ restricted backward algorithm described appendix note split state giving combine components recover split combine pair reversible 
note compute corresponding values combine move 
acceptance probabilities split combine moves min min gamma respectively oe oe theta theta gamma ffi gamma ffi ffi gamma ij gamma gamma ffi ffi gamma ij theta zj theta 
theta ff gamma ife oe ffg ff gamma ffg theta alloc theta gamma gamma gamma gamma theta density alloc probability making particular allocation 
furthermore jacobian determinant transformation 
determinant partly evaluated numerically discussed appendix conditional densities involved simple products oe oe gamma ffi value numerical results expression simplifies oe oe theta theta 
theta zj theta theta ff gamma theta alloc theta gamma gamma gamma gamma theta birth death moves proceed birth death moves 
choose random birth death probabilities respectively 
death move accomplished selecting component random empty ones deleting 
remaining rows unaltered 
birth move start model components want create new empty component draw th row delta new transition probability matrix prior ffi ffi 
furthermore draw set ij gamma ij ij new standard deviation oe drawn uniform prior ff remaining oe copied new component empty 
split combine pair create new component birth move delete death move recover original state 
acceptance probabilities birth death moves min min gamma respectively theta gamma ffi gamma ffi ffi gamma ij gamma gamma ffi ffi gamma ij theta zj theta 
theta ff gamma ife oe ffg ff gamma ffg theta theta ff gamma gamma theta number empty components birth ffi ffi density 
furthermore jacobian determinant transformation ij ij note omit index row sums equal unity 
new row component drawn prior cancellations occur expression ffi simplifies theta theta zj theta theta theta gamma theta gamma gamma expression corresponds factor richardson green formula exponent gamma see richardson green 
death move corresponding deleted row 
sampler behaves desired terms converging realization joint distribution follows arguments similar section richardson green establish irreducibility 
instance irreducibility obtains chain move possible values increasing decreasing value positive probability step allocations positive probability step conditional density positive natural parameter space holds true steps consider consecutive sweeps 
applications description examples investigate behaviour performance reversible jump mcmc algorithm applied different sets data 
set extract stock index 
consists observations daily returns 
series previously analysed en 
subseries called series 
refer readers information data 
second set data series hourly wind measurements athens january 
previously analysed model identical 
difference hidden markov chain jump arbitrarily states base state communicates states jumps disallowed 
third set data consists residuals fit arma model hour planetary activity index measurements 
dataset analysed restricted model refer information 
datasets plotted figures 
wind data discretized steps contain observations equal zero 
likelihood unbounded fixing oe letting remaining oe tend zero see likelihood grows indefinitely 
course unrestricted maximumlikelihood estimation inappropriate substantial effect result bayesian analysis 
data added uniform gamma random variable independent observation 
results set data results sweeps mcmc algorithm burn sweeps 
table shows posterior distribution number components series dead heat 
en 
tested null hypothesis vs bootstrapped likelihood ratio test obtained simulated value 
exact correspondence results indicate similar degree belief 
shows density plots loglikelihood values sampled th sweep 
clear difference curves essentially overlap 
note parameter posterior means oe oe close respectively 

wind data table suggests 
proposed noted somewhat different model 
preprocess data discuss problem unbounded likelihoods 
loglikelihood obtained mle gamma original preprocessed data compared loglikelihood curves 
show clear distinction distinction going 
curve overlaps 
magnetic dataset table suggests components 
proposed mle yielding loglikelihood gamma 
curves plotted sweeps observations en 
dependence structure subseries difficult capture maximum likelihood estimation 
notice hmms consider uncorrelated 
en 
looked covariance function absolute values series quite slowly decaying data 
property extent estimated models 
figures show standard non parametric estimates covariance functions fjy jg corresponding bayesian estimate 
bayesian estimate mean sweep covariance function current model computed functions averaged sweeps 
details computation covariance function hmms see en 

note averaged covariance function covariance function hmm 
shows reasonable agreement estimates lags bayesian estimate decays faster 
similar features emerge sets data 
hand series larger lags estimates order gamma difficult judge true covariance function different zero 
figures show bayesian estimates marginal density kernel density estimates agree 
wind magnetic table posterior distribution number components density plots loglikelihood values sampled th sweep 
upper panel data middle panel wind data lower panel magnetic data 
solid line dashed line dotted line dash dotted line 
data upper panel bayesian estimate kernel estimate marginal density middle panel solid dashed lines bayesian estimate non parametric estimate covariance function lower panel base logarithmic scale solid dashed lines 
wind data upper panel bayesian estimate kernel density estimates different bandwidths marginal density middle panel solid dashed dotted lines bayesian estimate non parametric estimate covariance function lower panel base logarithmic scale solid dashed lines 
magnetic data upper panel bayesian estimate kernel estimate marginal density middle panel solid dashed lines bayesian estimate non parametric estimate covariance function lower panel base logarithmic scale solid dashed lines 
table posterior distribution number components data max jy different choices stability respect prior parameter ff determines range prior oe obviously main design parameter 
ff fixed set larger max jy order restrict range oe 
hand immediate acceptance probability split move inversely proportional ff large ff pushes posterior distribution 
ff affects acceptance ratio posterior distribution directly course undesirable reason making ff hyperparameter 
choice ff negative exponential distribution mean 
table shows data posterior distribution different choices range max jy max jy posteriors agree showing prior structure informative fixed ff 
results obtained max jy performance mcmc algorithm acceptance rates split combine move respectively data wind data magnetic data 
rates bit lower desired split combine moves involve change model parameters updated sweep feel low magnetic data 
results series handled caution 
pointed number invalid split proposals proposals non stochastic small proposals rejected mainly acceptance probabilities keep mind degree precision posterior distribution bounds achievable acceptance rate 
suppose simplicity possible values posterior probability 
long run sweep may change dimension may occur sweeps acceptance rate bounded 
wind magnetic data posteriors quite concentrated course far attaining corresponding bound acceptance rates 
acceptance rates affected parameters beta distributions split move 
run version variants algorithm large number choices parameters ones section best 
birth death move completely redundant sense acceptance rates practically zero theta gamma 
moves just removed algorithm 
figures show plots values burn sets data plotted th sweep settling period empirical estimate posterior distribution values oe sweeps value posterior distribution attains maximum 
upper panel values data plotted th sweep 
middle panel estimated posterior distribution function number sweeps 
lower panel oe oe sweeps 
upper panel values wind data plotted th sweep 
middle panel estimated posterior distribution function number sweeps 
lower panel oe oe oe sweeps 
upper panel values magnetic data plotted th sweep 
middle panel estimated posterior distribution function number sweeps 
lower panel oe oe oe sweeps 
posterior distribution assumptions data solid line wind data dashed line magnetic data dotted line 
comparison analysis interesting compare results obtained assumptions independent 
obviously independent having mixture distribution oe 
implemented reversible jump mcmc algorithm similar richardson green case 
prior uniform maxg prior weights conditional prior structure oe 
split move component weight split gamma standard deviation oe split oe oe gamma gamma oe oe gamma 
ran algorithm sweeps sweep burn 
results quite different obtained modelling hmms 
shows posterior distributions spread table 
wind data mass max magnetic dataset distribution spread datasets mode just hmm case 
comparing posterior means oe find similarities 
data figures hmm analysis respectively wind data magnetic data 
wind data samples analysis may explain larger discrepancy 
acceptance rates birth death move larger algorithm despite sophisticated sense valid range computed 
rates data wind data magnetic data 
scope seen possible construct reversible jump mcmc algorithms bayesian analysis hidden markov models unknown number components 
believe algorithms may strong competitor frequentist approach maximum likelihood computational point view combine parameter estimation model selection way 
obviously desirable find dimension changing moves achieve larger acceptance rates 
may fruitful consider moves preserve marginal distribution lesser extent moves preserve dependence time order autocorrelation 
experimented split move splits standard deviation oe appropriate independent draws rows section 
acceptance rates obtained split combine pair lower reported 
find differences results hidden markov models results assumptions quite striking 
experiences analysis tentative 
adding structure markov dependence model pushes posterior distribution smaller values 
secondly adding structure model difficult design split combine moves generally dimension changing moves yield high acceptance rates 
research cpr relates tmr network spatial computational statistics partially supported crest 
research tr supported swedish natural science research council contract 
aa ma royal swedish academy sciences foundation 
initiated cpr tr visited department statistics university glasgow wish department friendly environment financial support 
density estimates figures computed matlab kernel density estimation toolbox nottingham trent university baum petrie 

statistical inference probabilistic functions finite state markov chains 
ann 
math 
statist 

baum petrie soules weiss 

maximization technique occurring statistical analysis probabilistic functions markov chains 
ann 
math 
statist 

bickel ritov en 

asymptotic normality maximum likelihood estimator general hidden markov models 
ann 
statist 
appear 
robert 

bayesian estimation switching arma models 
econometrics appear 
chib 

calculating posterior distributions modal estimates markov mixture models 
econometrics 
churchill 

accurate restoration dna sequences discussion 
case studies bayesian statistics hodges kass eds vol 
ii 
springer verlag new york 
damien walker 

sampling probability densities uniform random variables gibbs sampler 
preprint 
fredkin rice 

maximum likelihood estimation identification directly single channel recordings 
proc 
royal soc 
lond 



white noise driven hidden markov chains 
time series anal 

gilks richardson spiegelhalter 

markov chain monte carlo practice 
chapman hall london 
green 

reversible jump markov chain monte carlo computation bayesian model determination 
biometrika 
hamilton 

new approach economic analysis nonstationary time series business cycle 
econometrica 


markov switching vector 
lecture notes economics mathematical systems 
springer verlag new york 


maximum likelihood estimation hidden markov models 
stoch 
proc 
appl 

puterman 

maximum penalized likelihood estimation independent markov dependent mixture models 
biometrics 
macdonald 

hidden markov models discrete valued time series 
chapman hall london 
mclachlan 

bootstrapping likelihood ratio test statistic number components normal mixture 
appl 
statist 

petrie 

probabilistic functions finite state markov chains 
ann 
math 
statist 

rabiner 

tutorial hidden markov models selected applications speech recognition 
proc 
ieee 
richardson green 

bayesian analysis mixtures unknown number components discussion 
roy 
statist 
soc 

richardson green 

corrigendum bayesian analysis mixtures unknown number components 
roy 
statist 
soc 

robert celeux diebolt 

bayesian estimation hidden markov chains stochastic implementation 
statist 
prob 
letters 
robert titterington 

resampling schemes hidden markov models application maximum likelihood estimation 
statistics computing appear 
en 

stylized facts daily return series hidden markov model 
appl 
econometrics appear 
tierney 

markov chains exploring posterior distributions discussion 
ann 
statist 

appendix appendix describe restricted backward algorithm update hidden markov chain split move 
time indices gamma proposed oe want sample conditional distribution gamma gamma variables fj easy see inhomogeneous markov chain parameters restricted fj define dimensional vectors oe fj vectors may computed recursively iez gamma gamma ij oe ascending order compute dimensional vector ez gamma oe vector sum draw probability distribution obtained 
normalised vector conditional distribution gamma oe procedure provides sample desired conditional distribution 
replace right hand side unity replace ez gamma right hand side stationary probability appendix appendix evaluate jacobian determinant split move 
look transformation ij ij oe ij ij ij oe oe include diagonal elements tuples omit rows sums equal unity 
obtain table partial derivatives defines jacobian matrix ij ij ij oe oe ij theta ij diag diag gamma theta diag ij ij theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta oe theta theta theta theta denotes identity matrix denotes suitably sized vector matrix zeros theta denotes non zero entries 
jacobian matrix block diagonal structure follows disregard rows columns corresponding ij ij remains upper block diagonal follows consider corresponding ij 
ij ij oe 
oe oe respectively separately 
adding column matrix column change determinant fi fi fi fi fi diag diag gamma diag ij ij fi fi fi fi fi fi fi fi fi fi diag gamma ij fi fi fi fi fi ij second fi fi fi fi fi fi fi fi fi fi fi fi gamma gamma gamma gamma oe gamma gamma gamma gamma oe gamma gamma gamma fi fi fi fi fi fi fi fi fi fi fi fi oe ih gamma gamma gamma gamma ij remains diag diag gamma gamma gammau theta diag gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma non zero entries explicitly 
determinant matrix evaluated numerically 
doing compute partial derivatives looking see main problem calculate recall 
show 
recall diagonal entries considered independent variables 
equilibrium equations gamma gamma differentiating respect writing obtain ffi gamma ffi gamma ffi kronecker ffi 
coefficient matrix linear system equations restriction states matrix gamma prior implies entries non negative irreducible aperiodic 
unity single eigenvalue gamma rank gamma 
restriction gamma states full rank unique solution 
