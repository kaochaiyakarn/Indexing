tutorial learning bayesian networks david heckerman microsoft com march revised july technical report msr tr microsoft research advanced technology division microsoft microsoft way redmond wa examine graphical representation uncertain knowledge called bayesian network 
representation easy construct interpret formal probabilistic semantics making suitable statistical manipulation 
show representation learn new knowledge combining domain knowledge statistical data 
techniques learning rely heavily data 
contrast knowledge encoded expert systems usually comes solely expert 
examine knowledge representation called bayesian network lets best worlds 
representation allows learn new knowledge combining expert domain knowledge statistical data 
bayesian network graphical representation uncertain knowledge people find easy construct interpret 
addition representation formal probabilistic semantics making suitable statistical manipulation howard pearl 
decade bayesian network popular representation encoding uncertain expert knowledge expert systems heckerman 
researchers developed methods learning bayesian networks combination expert knowledge data 
techniques developed new evolving shown remarkably effective domains cooper herskovits aliferis cooper heckerman 
bayesian networks learning process goes follows 
encode existing knowledge expert set experts bayesian network done building probabilistic expert system 
database update knowledge creating new bayesian networks 
result includes refinement original expert knowledge identification new distinctions relationships 
approach robust errors knowledge expert 
expert knowledge unreliable incomplete improve learning process 
learning bayesian networks similar neural networks 
process employing bayesian networks important advantages 
easily encode expert knowledge bayesian network knowledge increase efficiency accuracy learning 
nodes arcs learned bayesian networks correspond recognizable distinctions causal relationships 
consequently easily interpret understand knowledge encoded representation 
brief tutorial bayesian networks methods learning data 
sections discuss bayesian philosophy representation 
sections describe methods learning probabilities structure bayesian network 
sections discuss methods identifying new distinctions world integrating distinctions bayesian network 
restrict discussion bayesian quasi bayesian methods learning 
interesting effective non bayesian approach pearl verma spirtes 

limit discussion problem domains variables take discrete states 
general techniques buntine heckerman 

bayesian philosophy discuss bayesian networks learn data help review bayesian interpretation probability 
primary element language probability bayesian event 
event mean state part world time interval past 
classic example event particular flip coin come heads 
interesting event gold close january 
event prevalent conception probability measure frequency occurs repeat times experiment possible outcomes 
different notion probability represents degree belief held person event occur single experiment 
person assigns probability believes certainty occur 
assigns probability believes certainty happen 
assigns probability degree unsure occur 
interpretation probability frequency series repeat experiments traditionally referred objective frequentist interpretation 
contrast interpretation probability degree belief called subjective bayesian interpretation honor thomas bayes scientist mid helped pioneer theory probabilistic inference bayes hacking 
shall see section frequentist interpretation special case bayesian interpretation 
bayesian interpretation probability belief depend state knowledge person provides probability 
example give coin assign probability event coin show heads toss 
convinced person coin weighted favor heads assign higher probability event 
write probability ej read probability 
symbol represents state knowledge person provides probability 
interpretation person assess probability information assumes true 
example coin assess probability coin show heads eleventh toss assumption coin comes heads tosses 
write je denote probability event event true background knowledge 
researchers written different sets properties satisfied degrees belief cox savage lists properties researchers derived rules rules probability 
basic rules rules may derived sum rule says event complement ej ej product rule says events je denotes probability true 
commonly rules expressed terms variables events 
variable takes values collection mutually exclusive collectively exhaustive states state corresponds event 
variable may discrete having finite countable number states may continuous 
example state binary variable represent possible outcomes coin flip continuous variable represent weight coin 
lower case letters usually near alphabet represent single variables upper case letters represent sets variables 
write denote variable state observe state variable set call set observations state write leave state variable set variables implicit 
probability distribution set variables denoted set probabilities kj states contains continuous variables xj called probability density 
contains discrete continuous variables xj called generalized probability density 
distinctions 
common rule probability bayes theorem jy jx probability distribution know jy probability distribution know distributions called priors posteriors respectively 
cases relative posterior interest 
case bayes theorem written jy jx normalization constant 
rule chain rule jx gamma shall see rule important definition bayesian networks 
generalized sum rule generalized sum includes integrals variables continuous 
expansion rule jy bayesian philosophy extends decision making uncertainty discipline known decision theory 
general decision components decision maker alternatives knows beliefs wants preferences decision theory decision variable represent set mutually exclusive exhaustive alternatives bayesian probabilities represent decision maker beliefs utilities represent decision maker preferences 
decision theory essentially rule maximize expected utility 
rules says set mutually exclusive exhaustive alternatives decision maker assign utility possible outcome possible alternative assign bayesian probabilities possible outcome possible alternative choose alternative maximizes expected utility 
researchers shown rule follows sets compelling axioms von neumann morgenstern savage 
practice decision making uncertainty difficult task 
fact researchers shown people violate rule tversky kahneman 
deviations significant predictable researchers come reject rule kahneman 
researchers argue axioms derive rule compelling reject argue people deviations rule decision theoretic concepts representations important howard 
doubt author view 
bayesian networks problem domain universe just set variables 
bayesian network model usually uncertain relationships variables domain 
precisely domain variables fx joint probability distribution probability distribution states bayesian network represents joint probability distribution representation consists set local conditional probability distributions combined set assertions conditional independence allow construct global joint distribution local distributions 
illustrate representation consider domain troubleshooting car won start 
step constructing bayesian network decide variables states model 
possible choice variables domain battery states bad fuel states empty empty gauge states empty empty turn states start states 
course include variables real world example 
model states variables finer level detail 
example gauge continuous variable states ranging 
second step constructing bayesian network construct directed acyclic graph encodes assertions conditional independence 
call graph structure 
domain fx write joint probability distribution chain rule probability follows jx gamma subset pi fx fx conditionally independent pi jx gamma pi conditional independencies define bayesian network structure 
nodes battery fuel gauge turn start bad empty empty empty empty empty empty bad empty empty bad empty bad empty empty empty empty bayesian network troubleshooting car won start 
arcs drawn cause effect 
local probability distribution associated node shown adjacent node 
structure correspond variables domain 
parents correspond set pi example ordering conditional independencies jb sjf consequently obtain structure shown 
final step constructing bayesian network assess local distributions pi distribution state pi distributions automobile example shown 
combining equations see bayesian network encodes joint probability distribution drawback bayesian networks defined network structure depends variable order 
order chosen resulting network structure may fail reveal conditional independencies domain 
exercise reader construct bayesian network automobile domain ordering 
fortunately practice readily assert causal relationships variables domain assertions construct bayesian network structure variables 
construct bayesian network set variables draw arcs cause variables immediate effects 
cases doing results bayesian network conditional independence implications accurate 
example network constructed assertions gauge direct causal effect battery fuel turn direct causal effect battery start direct causal effect turn fuel 
bayesian network domain determines joint probability distribution domain principle bayesian network compute probability interest 
example suppose want compute probability distribution fuel car doesn start 
rules probability js real world problem variables approach feasible entails summing terms 
fortunately exploit conditional independencies encoded bayesian network computation efficient 
case conditional independencies equation equation js bj bj conditional independence produces decomposition joint probability distribution conjunction distributive law reduce dimensionality computations 
general problem computing probabilities interest possibly implicit joint probability distribution called probabilistic inference 
exact algorithms probabilistic inference bayesian networks exploit conditional independence roughly described different twists 
example howard matheson shachter developed algorithm reverses arcs network structure answer probabilistic query read directly graph 
algorithm arc reversal corresponds application bayes theorem 
pearl developed message passing scheme updates probability distributions node bayesian network response observations variables 
lauritzen spiegelhalter created algorithm transforms bayesian network tree node tree corresponds subset variables domain 
algorithm exploits mathematical properties tree perform probabilistic inference 
ambrosio developed inference algorithm simplifies sums products symbolically transformation equation equation 
exploit assertions conditional independence bayesian network probabilistic inference exact inference arbitrary bayesian network np hard cooper battery fuel start start influence diagram structure decision change battery get fuel 
square node represents alternatives 
diamond node represents utilities possible outcomes 
double line arc start represents assertion know car starts decision 
approximate inference example monte carlo methods np hard dagum luby 
applications networks small simplified sufficiently complexity results fatal 
applications usual inference methods impractical researchers developing techniques custom tailored particular network topologies heckerman suermondt cooper particular inference queries agogino shachter jensen andersen influence diagram extension bayesian network representation decision problems 
bayesian network influence diagram contains nodes representing uncertain variables arcs representing probabilistic dependence 
influence diagram constructs called chance nodes relevance arcs respectively 
addition influence diagrams may contain decision nodes represent decision variables utility node represents decision maker preferences 
influence diagrams may contain information arcs indicate known time decision 
example troubleshooting domain suppose options replace battery get fuel car 
influence diagram decision shown 
square node decision node represents alternatives 
diamond node utility node 
arcs pointing chance oval nodes relevance arcs 
arc start information arc presence asserts time decision know car starts 
general tails heads outcomes thumbtack flip 
probability distribution physical probability heads 
information arcs point decision nodes relevance arcs point chance nodes 
learning probabilities variable case bayesian networks probabilistic interpretation traditional techniques bayesian statistics learn models data 
discuss techniques remainder 
techniques need discussed context learning probability distribution single variable 
section examine case 
consider common thumbtack round flat head supermarkets 
throw thumbtack air land hard flat surface come rest point heads head tails shown 
suppose give thumbtack flips times measures fraction times thumbtack comes heads 
frequentist say long run fraction probability observe flips thumbtack estimate probability 
contrast bayesian perspective recognize possible values fraction variable call true value uncertain 
express uncertainty probability distribution update distribution observe flips thumbtack 
note represent degree belief collections long run fractions satisfy rules probability 
shall refer physical example taken howard 

bayesian network structure depicting conditional independencies associated binomial sample 
probability distinguish degree belief 
shows possible probability distribution 
suppose observe fx xm outcomes flips thumbtack 
refer set observations database 
knew value probability heads flip equal matter outcomes observe 
gamma outcome lth flip thumbtack 
similarly gamma gamma particular outcomes mutually independent 
represent conditional independence assertion bayesian network structure shown 
reality uncertain value 
case expansion rule determine probability toss thumbtack come heads denotes expectation 
probability heads toss just expectation 
furthermore suppose flip thumbtack observe heads 
bayes theorem posterior probability distribution jx heads normalization constant 
obtain posterior distribution multiplying prior distribution function renormalizing 
variable referred frequency objective probability true probability 
heads graphical depiction bayes theorem compute posterior probability distribution physical probability 
procedure depicted graphically 
expected posterior shifted right slightly narrower 
similarly observe single tails obtain jx tails gamma general observe heads tails database jt heads tails gamma assessed prior distribution determine posterior distribution possible database 
note order observe outcomes irrelevant posterior relevant number heads number tails database 
say sufficient statistic database 
simple example outcome variable states heads tails 
imagine discrete outcome variable states 
example variable represent outcome roll loaded die 
thumbtack example define physical probabilities outcome denote theta assume state possible 
addition 
know physical probabilities outcome toss conditionally independent tosses kjx gamma theta database outcomes fx xm satisfies conditions called gamma dimensional multinomial sample physical probabilities theta 
thumbtack example sequence said binomial sample 
concept multinomial sample generalization random sample central remaining discussions 
analogous thumbtack example kj theta theta kj probability case 
note distribution theta technically distribution variables theta symbol denotes set difference 
database outcomes theta jd delta theta number times normalization constant 
note counts sufficient statistic multinomial sample 
multinomial sample user free assess probability distribution theta practice uses dirichlet distribution convenient properties 
variables theta said dirichlet distribution exponents probability distribution theta theta gamma gamma gamma gamma delta gamma function satisfies gamma gamma gamma 
variables theta dirichlet distribution say theta dirichlet 
exponents greater guarantee distribution normalized 
note exponents function user state information 
dirichlet distribution known beta distribution 
probability distribution left hand side beta distribution exponents heads tails 
probability distribution right hand side beta distribution exponents heads tails 
equation see prior distribution theta dirichlet posterior distribution theta database fx xm dirichlet theta jd gamma say dirichlet distribution closed multinomial sampling dirichlet distribution conjugate family distributions multinomial sampling 
theta dirichlet distribution expectation equal probability observation simple expression kj shall see properties dirichlet useful prior learning 
survey methods assessing beta distribution winkler 
methods include direct assessment probability distribution questions regarding relative areas assessment cumulative distribution function assessing posterior means distribution hypothetical evidence assessment form equivalent sample size 
methods generalized varying difficulty non binary case 
equivalent sample size method generalizes particularly 
method equation says assess dirichlet distribution assessing probability distribution xj observation doing may rewrite equation theta delta kj gamma normalization constant 
assessing xj straightforward 
furthermore observations suggest simple method assessing variance distribution theta indication mean theta expected change new observations 
higher variance greater expected change 
said variance measure user confidence mean theta variance dirichlet distribution ar kj gamma kj reflection user confidence 
addition suppose initially completely ignorant domain distribution theta equation exponent 
suppose saw cases sufficient statistics equation prior dirichlet distribution equation 
assess equivalent sample size number observations seen starting complete ignorance order confidence theta 
example obtain probability prior distribution normalized called improper prior 
precise say exponent equal number close zero 
distribution assessed equivalent sample size 
far considered variable discrete outcomes 
general imagine physical probability distribution variable discrete continuous database cases drawn random 
physical probability distribution typically characterized finite set parameters 
outcome variable discrete physical probability distribution parameter corresponding physical probability distribution refer physical probabilities parameters 
outcome variable continuous physical probability distribution may normal distribution 
case parameters mean variance distribution 
database cases drawn physical probability distribution called random sample 
physical probability distribution unknown parameters update beliefs parameters random sample distribution techniques similar discussed 
random samples named distributions including normal gamma uniform distributions exist corresponding conjugate priors offer convenient properties learning probabilities similar properties dirichlet 
priors referred collectively exponential family 
reader interested learning distributions read degroot chapter 
learning probabilities known structure notion random sample generalizes domains containing variable 
domain fx imagine multivariate physical probability distribution contains discrete variables distribution just finite collection discrete physical probabilities 
contains continuous variables distribution multivariate normal distribution characterized mean vector covariance matrix 
random sample physical probability distribution update priors parameters distribution 
updating especially simple conjugate priors parameters available see degroot 
consider 
suppose know multivariate physical probability distribution encoded particular structure may gotten information example causal knowledge domain 
section consider task learning parameters discuss special case variables discrete random sample database fc cm contains missing data case consists observation variables say complete 
section consider difficult problem contains missing data 
buntine heckerman geiger discuss case may contain continuous variables 
database random sample multivariate physical probability distribution encoded simply say random sample example consider domain consisting binary variables xy xy denote parameters physical probabilities joint space physical probability event true false 
note departing standard notation 
saying random sample network structure containing arc assertion parameters joint space satisfy independence constraints xy example xy physical probability associated event true 
difficult show assertion equivalent assertion database decomposed multinomial samples observations multinomial sample parameter observations multinomial sample parameter example suppose assert database variable domain random sample network structure constraints parameters joint space 
furthermore assertion implies database binomial samples observations binomial sample parameter observations cases true binomial sample parameter yjx observations cases false binomial sample parameter yj consequently occurrences conditionally independent case conditionally independent occurrences yjx yj case graphically represent conditional independence assertions associated random samples bayesian network structure shown 
collection random samples shown tempting apply variable techniques learn parameter separately 
unfortunately approach correct parameters dependent shown 
example see occurrences update beliefs beliefs yjx yj change 
suppose parameters independent shown 
provided database complete update parameter separately 
case case case case bayesian network structure binary variable domain fx yg showing conditional independencies associated assertion database random sample structure 
bayesian network structure showing added assumption parameter independence 
remainder section shall assume parameters independent 
call assumption introduced spiegelhalter lauritzen parameter independence 
section discuss methods handling dependent parameters 
complete discussion need notation 
denote assertion hypothesis database random sample bayesian network structure network structure number states variable pi number states pi ijk denote physical probability pi addition theta ij ijk theta theta ij note parameters theta conjunction determine physical probabilities joint space 
assume variable set theta ij dirichlet distribution theta ij jb delta ijk gamma ijk normalization constant 
ijk number cases database pi obtain theta ij jd delta ijk ijk gamma ijk normalization constant 
furthermore applying equation multinomial sample compute probability pi cm case seen database cm jd ijk ijk ij ij ij ijk ij ijk learning structure previous section considered situation uncertain physical probabilities certain network structure encodes probabilities 
suppose uncertain probabilities uncertain structure encodes 
set events express uncertainty assigning prior probability possible hypothesis furthermore update probabilities see cases 
doing learn structure domain 
previous section denote uncertain hypothesis database random sample bayesian network structure bayes theorem jd djb normalization constant 
product rule djb jc gamma evaluate term right hand side equation equation assumption database complete 
obtain posterior probability jd delta delta ij ij delta ij ij ij ij gamma ij ij gamma delta ij ij ij delta ij ij ij ij ij gamma ij ij ij gamma ijr ij gamma ijk delta ijr ij gamma ijk ijr ijr gamma ij ij gamma delta delta gamma ij gamma ij ij delta gamma ijk ijk gamma ijk posterior probabilities equation may compute probability distribution case observed seen database 
expansion rule obtain cm jd cm jd jd important points approach 
happen bayesian network structures represent exactly sets probability distributions 
say structures equivalent verma pearl 
example variable domain fx zg network structures represents distributions conditionally independent consequently network structures equivalent 
example complete network structure missing edges encodes assertions conditional independence 
domain containing variables 
complete network structures network structure possible ordering variables 
complete network structures domain represent joint probability distributions possible distributions equivalent 
general network structures equivalent structure ignoring arc directions structures verma pearl 
ordered tuple arc arc characterization network structure equivalence chickering created efficient algorithm identifying bayesian network structures equivalent network structure 
assertion physical probabilities joint space encoded network structure follows hypotheses associated equivalent network structures identical 
consequently equivalent network structures prior posterior probability 
example variable domain fx yg network structures equivalent probability 
general property called hypothesis equivalence 
light property associate hypothesis equivalence class structures single network structure methods learning network structure interpreted methods learning equivalence classes network structures sake brevity blur distinction 
hypothesis equivalence holds provided interpret bayesian network structures simply representations conditional independence 
stronger definitions bayesian networks exist arcs causal interpretation pearl verma 
heckerman 
argue unreasonable assume hypothesis equivalence working causal bayesian networks second important point approach writing equation assumed hypothesis equivalence classes mutually exclusive 
reality hypotheses mutually exclusive 
example variable domain network structures empty network structure encode parameters satisfying equality yjx hypotheses associated network structures overlap 
approach assume priors parameters network structure bounded densities overlap hypotheses measure zero 
writing equation limited hypotheses corresponding assertions physical probability distribution joint space comes particular network structure 
relax assumption assuming physical probability distribution encoded set network structures 
pursue generalization 
principle approach discussed section essentially learning network structure 
practice user believes alternative network structures possible directly assess priors possible network structures parameters subsequently equations generalizations continuous variables missing data 
example buntine designed software system user specifies priors set possible models bayesian networks manner similar shown 
system compiles specification computer program learns database 
number network structures domain containing variables exponential consequently user exclude network structures issues considered 
particular computational constraints prevent summing hypotheses equation 
approximate cm jd accurately retaining small fraction hypotheses sum 
hypotheses include 
addition efficiently assign prior probabilities network structures parameters 
subsections follow consider issues 
scoring metrics important issue approximate cm jd just small number network structure hypotheses 
question difficult answer theory 
researchers shown experimentally single reasonable adopt weaker assumption likelihood equivalence says observations database help discriminate equivalent network structures 
network structure provides excellent approximation cooper herskovits aliferis cooper heckerman 
example section 
results somewhat surprising largely responsible great deal interest learning bayesian networks 
observation important consideration identify network structures 
approach adopted researchers scoring metric combination search algorithm 
scoring metric takes prior knowledge database set network structures computes goodness fit structures prior knowledge data 
search algorithm identifies network structures scored 
section discuss scoring metrics 
section discuss search algorithms 
obvious scoring metric single network structure equivalence class relative posterior probability structure database 
example compute djb compute bayes factor jd jd network structure empty network structure 
equation compute relative posterior probability scoring metric called bayesian dirichlet bd metric 
network structure highest posterior probability called maximum posteriori map structure 
score set distinct network structures 
note practitioners typically compute logarithms probabilities avoid numerical underflow 
madigan suggest alternative scoring metric uses relative posterior probability conjunction heuristics principle occam razor 
scoring metrics approximate posterior probability metric 
section discuss algorithms find local maximum probability djb theta function theta physical probabilities associated network structure 
local maximum score favor complex network structures place constraints parameters theta local maximum djb theta score penalize structures complexity 
akaike suggests scoring metric log djm theta dim model theta denotes values theta maximize probability dim number logically independent parameters scoring metric called information criterion aic 
bayesian network penalty dim gamma schwarz suggests similar scoring metric penalty term dim log number cases database 
metric called bayesian information criterion bic 
metric approximates posterior probability metric minimum description length mdl rissanen 
mdl network structure sum number bits required encode model increases increasing model complexity number bits required encode database model decreases increasing model complexity relative particular coding scheme 
note limit number cases database approach infinity bd metric uniform priors structures bic mdl give relative scores kass 
unfortunately practice asymptotic equivalence rarely achieved 
score structures decision model domain 
cases score network structure extending structure influence diagram includes decision model domain 
example suppose wish compute score bayesian network medical diagnosis shown complete database disease symptoms 
extend network influence diagram shown 
decision model represented influence diagram includes single treatment decision assertion patient utility depends disease treatment decision 
compute score bayesian network processing cases sequentially 
case database equation predict probability distribution diseases case symptoms case previous cases gamma influence diagram determine optimal treatment 
utilities encoded influence diagram determine utility outcome patient chosen treatment disease recorded database case compute score bayesian network structure summing utilities cases 
metric generalized score multiple network structures 
advantage decision theoretic approach optimizes want optimize expected utility 
disadvantage requires construct influence diagram decision model domain 
priors structures posterior probability decision theoretic metrics require assign priors possible network structures 
section efficient method doing described heckerman 

approach requires user constructs prior network structure domain 
symptom disease treatment symptom symptom 
symptom disease symptom symptom 
bayesian network medical diagnosis 
corresponding influence diagram medical treatment 
method assumes structure user best guess network structure encodes physical probabilities 
prior network structure compute prior probability follows 
variable ffi denote number nodes symmetric difference pi pi pi pi pi pi 
prior network differ ffi ffi arcs 
compute prior probability penalizing constant factor arc set ffi normalization constant ignore 
note approach assigns equal priors equivalent network structures prior network structure empty see heckerman discussion point 
formula simple requires assessment prior network structure single constant user willing provide detailed knowledge assessing different penalties different nodes different parent configurations node buntine 
variant approach allow user categorically assert arcs prior network 
equation set zero priors network structures conform constraints 
priors network parameters posterior probability decision theoretic metrics require assign priors network parameters possible network structures 
information compute aic bic metrics efficiently see section 
authors discussed similar practical approaches assigning priors structures possible cooper herskovits buntine spiegelhalter heckerman 
section describe approach heckerman approach result geiger heckerman 
allowed values physical probabilities possible parameter independence hypothesis equivalence imply physical probabilities complete network structures dirichlet distributions specified equation constraint ijk pi jjb sc user equivalent sample size domain sc hypothesis corresponding complete network structure pi jjb sc user probability pi case seen database 
conditions priors parameters complete network structures may determined constructing prior network case seen probabilities equation may computed assessing equivalent sample size confidence prior network 
section give example prior network 
determine priors parameters incomplete network structures heckerman 
assumption parameter modularity says network structures parents theta ij jb theta ij jb call property parameter modularity says distributions parameters theta ij depend structure network local variable theta ij depends parents 
example consider network structure empty structure variable domain corresponding hypotheses xy structures set parents empty set 
consequently parameter modularity jb jb xy 
assumptions parameter modularity independence simple matter construct priors parameters arbitrary network structure priors complete network structures 
particular parameter independence construct priors parameters node separately 
furthermore node parents pi network structure identify complete network structure parents parameter modularity determine priors node 
geiger heckerman proved result likelihood equivalence 
result special case bd metric called bde metric assigns equal scores equivalent network structures 
section illustrate metric 
search methods section examine search methods identifying network structures high scores 
essentially search methods property scoring metrics call decomposability 
network structure domain say measure structure decomposable written product measures function node parents 
example equation see probability djb bd metric decomposable 
consequently prior probabilities network structures decomposable equation bd metric 
write pi pi function parents 
bayesian non bayesian metrics decomposable 
decomposable metric compare score network structures differ addition deletion arcs pointing computing term pi structures 
consider special case finding network structure highest score structures node parent 
arc including cases null associate weight log jx gamma log 
equation log log log possibly null parent term equation network structures 
network structures node parent ranking network structures sum weights score result 
finding network structure highest weight special case known problem finding maximum branchings described example evans minieka 
problem defined follows 
tree network connected directed acyclic graph edges directed node 
root tree network unique node edges directed 
branching directed forest consists disjoint tree networks 
spanning branching branching includes nodes graph 
maximum branching spanning branching maximizes sum arc weights case 
efficient polynomial algorithm finding maximum branching described edmonds explored karp efficient tarjan gabow 

algorithms find branching highest score regardless metric long associate weight edge 
algorithm appropriate decomposable metric 
metrics assign equal scores equivalent network structures jx jx edges weights equal 
consequently directionality arcs plays role metrics problem reduces finding undirected forest maximum 
search done maximum spanning tree algorithm 
consider case find best network set networks node parents 
unfortunately problem np hard chickering 
appropriate heuristic search algorithms 
commonly discussed search methods learning bayesian networks successive arc changes network employ property decomposability evaluate merit change 
possible changes easy identify 
pair variables arc connecting arc reversed removed 
arc connecting arc added direction 
changes subject constraint resulting network contains directed cycles 
denote set eligible changes graph delta denote change log score network resulting modification decomposable metric arc added deleted pi need evaluated determine delta 
arc reversed pi pi need evaluated 
simple heuristic search algorithm local search johnson 
choose graph 
evaluate delta change delta maximum provided positive 
terminate search positive value delta 
decomposable metrics avoid recomputing terms delta change 
particular parents changed delta remains unchanged changes involving nodes long resulting network acyclic 
candidates initial graph include empty graph random graph graph determined polynomial algorithms described previously section prior network 
potential problem local search method getting stuck local maximum 
methods escaping local maxima include iterated hill climbing simulated annealing 
iterated hill climbing apply local search hit local maximum 
randomly perturb current network structure repeat process manageable number iterations 
variant simulated annealing described metropolis 
initialize system temperature pick eligible change random evaluate expression exp delta 
change change probability repeat selection evaluation process ff times fi changes 
changes ff repetitions searching 
lower temperature multiplying current temperature decay factor fl continue search process 
searching lowered temperature ffi times 
algorithm controlled parameters ff fi fl ffi 
initialize algorithm start empty graph large eligible change creating random graph 
alternatively may start lower temperature initialization methods described local search 
methods escaping local maxima include best search korf gibbs sampling see section 
real world example illustrates application techniques real world domain icu management taken heckerman 

hand constructed bayesian network domain called alarm network beinlich probabilities shown 
database cases sampled alarm network 
hypothetical prior network domain 
heckerman 
constructed network adding deleting reversing arcs alarm network adding noise probabilities alarm network 
shows network structure local search initialized prior network structure bde metric equivalent sample size priors network structures determined equation 
comparing network structures see learned network structure closer case alarm network structure 
prior network encoding user beliefs alarm domain 
case database generated alarm network 
network learned prior network case database generated alarm network 
arcs added deleted reversed respect alarm network indicated respectively 
taken heckerman 
alarm network prior network 
furthermore joint distribution encoded learned network closer alarm network prior network 
particular cross entropy joint distributions prior network respect alarm network cross entropy joint distribution learned network respect alarm network 
learning algorithm effectively database correct prior knowledge user 
missing data real databases observations variables cases typically missing 
section consider extensions previous methods handle missing data 
caution reader methods discuss assume observation missing independent actual states variables 
example methods appropriate medical database data drug response missing patients sick take drug 
methods addressing dependencies omissions explored rubin robins pearl 
fill methods consider simple situation observe single incomplete case domain denote variables observed case 
assumption parameter independence compute posterior distribution theta ij follows theta ij jc jc theta ij jy pi un fx pi jc theta ij jy pi pi jc theta ij jy gamma pi jjc fp theta ij pi jjc fp theta ij jx pi term curly brackets equation dirichlet distribution 
variables pi observed case posterior distribution theta ij way comparison cross entropy empty network probabilities determined marginals alarm network respect alarm network 
linear combination dirichlet distributions 
distributions called dirichlet mixtures probabilities gamma pi jjc pi jjc called mixing coefficients 
observe cases situation complex computation mixing coefficients involves finding means dirichlet mixtures 
general shown cooper herskovits computational complexity exact computation exponential number missing variable entries database 
practice require approximation 
approach approximate correct posterior distribution theta ij single dirichlet distribution continue equation formula mean dirichlet distribution 
approximations described literature 
example titterington describes method called fractional updating theta ij pretend observed fractional number observations corresponding parameter set 
particular suggests approximation theta ij jc pi jjc ijk theta ij drawback method falsely increases equivalent sample sizes dirichlet distributions associated theta ij replaces missing datum fractional sample 
cowell 
suggest approach problem 
approximate theta ij single dirichlet means average variance ar ijk correct dirichlet mixture 
note titterington approach cowell method produces scoring metric assigns equal scores equivalent network structures 
approximations process data database sequentially assumption parameter independence properties dirichlet distribution 
methods including gibbs sampling em algorithm gradient descent process data handle continuous domain variables dependent parameters 
gibbs sampling gibbs sampling described example geman geman special case markov chain monte carlo methods approximate inference hastings 
variables fx joint distribution gibbs sampler approximate expectation function follows 
choose initial state variables 
pick variable current state compute probability distribution assignments gamma variables 
sample state probability distribution compute 
iterate previous steps keeping track average value 
limit number samples approach infinity average equal expectation respect distribution provided conditions met 
gibbs sampler irreducible probability distribution eventually sample possible state possible initial state example contains zero probabilities gibbs sampler irreducible 
second chosen infinitely 
practice algorithm deterministically rotating variables typically 
introductions gibbs sampling including methods initialization discussion convergence york neal 
suppose database fc cm missing data want approximate theta jb 
variant gibbs sampling performing approximation goes follows 
initialize parameters theta 
second case containing missing data fill missing data assigned values theta example suppose variables unobserved case fill sampling distribution jc theta fill sampling distribution jx theta 
step done standard bayesian network inference algorithm 
third reassign parameters theta posterior distribution theta jd completed database 
iterate previous steps sampled values theta approximation theta jb 
buntine discusses approach detail 
gibbs sampling place searching network structures 
modify gibbs sampler described previous paragraph transition network structure 
example sampling pass parameters database sampler evaluate dj structure close current network structure arc addition deletion reversal sample new network structure distribution see madigan 
em algorithm expectation maximization em algorithm approximation algorithm find local maximum probability deltaj theta function parameters theta dempster 
database fc cm missing data approximate djb local maximum djb theta em algorithm 
gibbs sampler em algorithm handle models missing data continuous domain variables dependent parameters 
em algorithm tends provide accurate approximation typically converges quickly gibbs sampler 
em algorithm viewed deterministic version gibbs sampler 
gibbs sampler approximation djb assigning values theta 
sample complete database compute expected sufficient statistics missing entries database 
particular compute ijk theta pi jjc theta variables pi observed case term case requires trivial computation zero 
bayesian network inference algorithm evaluate term 
computation called expectation step em algorithm 
sample new values theta expected sufficient statistics actual sufficient statistics database set new values theta modes posterior distribution theta jd 
example parameters theta dirichlet distribution mode ijk ijk ijk theta gamma ij ij theta gamma mode exists provided ijk ijk theta 
expectation ijk place 
assignment called maximization step em algorithm 
dempster showed certain regularity conditions iteration expectation maximization steps converge local maximum probability djb theta 
general optimization methods gibbs sampling em algorithm practice need compute distributions theta jd efficiently 
computations efficient provided parameters theta dirichlet distribution distribution exponential family 
distributions form general optimization methods maximize djb theta gill press 
approaches example gradient descent conjugate gradient methods exploit derivatives function maximized speed convergence 
situations derivatives computed efficiently closed form 
buntine discusses methods detail 
mention case variables discrete 
situation russell 
shown log djb theta ijk ijk theta ijk ijk theta expected sufficient statistic equation 
noted previous section term may computed standard bayesian network inference algorithm 
learning new variables database missing data particular variable may observed cases may observed 
situation say variable hidden 
methods described previous section learn bayesian networks containing identified hidden variables 
network structure may fixed physical probabilities uncertain network structure parameters may uncertain 
example learning probabilities fixed structure hidden variables autoclass algorithm cheeseman stutz performs unsupervised classification 
model underlying algorithm bayesian network single hidden variable states correspond unknown classes 
number states hidden variable uncertain prior distribution 
hidden variable renders sets observable variables conditionally independent 
algorithm searches variations model including number states hidden variable version em algorithm approximate posterior probability model variation 
addition methods learning missing data identify uncertainty existence new variables 
hypothesize mutually exclusive exhaustive set bayesian network structures containing hidden variables 
assign priors structure parameters update priors data described algorithms handling missing data 
pointers literature tutorials tutorial incomplete 
readers interested learning graphical models methods learning offer additional 
detailed guide literature buntine 
charniak provides easy read bayesian network representation 
spiegelhalter 
heckerman 
give simple discussions methods learning bayesian networks domains containing discrete variables 
buntine heckerman geiger provide detailed discussions 
experimental comparisons different learning approaches cooper herskovits aliferis cooper lauritzen 
cowell 
heckerman 

addition directed models researchers explored graphs containing undirected edges knowledge representation 
representations discussed lauritzen verma pearl 
bayesian methods learning models data described dawid lauritzen buntine 
software systems learning graphical models implemented 
thomas spiegelhalter gilks created system takes learning problem specified bayesian network compiles problem gibbs sampler computer program 

built systems learn directed undirected mixed graphical models variety scoring metrics 
acknowledgments david chickering eric horvitz chris meek padhraic smyth comments earlier versions manuscript 
akaike akaike 

new look statistical model identification 
ieee transactions automatic control 
aliferis cooper aliferis cooper 

evaluation algorithm inductive learning bayesian belief networks simulated data sets 
proceedings tenth conference uncertainty artificial intelligence seattle wa pages 
morgan kaufmann 


model search contingency tables coco 
dodge editors computational statistics pages 
physica verlag heidelberg 
bayes bayes 

essay solving problem doctrine chances 
biometrika 
reprint original 
beinlich beinlich suermondt chavez cooper 

alarm monitoring system case study probabilistic inference techniques belief networks 
proceedings second european conference artificial intelligence medicine london pages 
springer verlag berlin 
buntine buntine 

theory refinement bayesian networks 
proceedings seventh conference uncertainty artificial intelligence los angeles ca pages 
morgan kaufmann 
buntine buntine 

operations learning graphical models 
journal artificial intelligence research 
buntine buntine 

guide literature learning graphical models 
technical report ic nasa ames research center 
charniak charniak 

bayesian networks tears 
ai magazine 
cheeseman stutz cheeseman stutz 

bayesian classification autoclass theory results 
fayyad shapiro smyth uthurusamy editors advances knowledge discovery data mining page 
aaai press menlo park ca 
chickering chickering 

transformational characterization equivalent bayesian network structures 
proceedings eleventh conference uncertainty artificial intelligence montreal qu 
morgan kaufmann 
chickering chickering geiger heckerman 

learning bayesian networks search methods experimental results 
proceedings fifth conference artificial intelligence statistics ft lauderdale fl pages 
society artificial intelligence statistics 
cooper cooper 

computational complexity probabilistic inference bayesian belief networks research note 
artificial intelligence 
cooper herskovits cooper herskovits 

bayesian method induction probabilistic networks data 
machine learning 
cooper herskovits cooper herskovits 
january 
bayesian method induction probabilistic networks data 
technical report smi section medical informatics stanford university 
cowell cowell dawid sebastiani 

comparison sequential learning methods incomplete data 
technical report department statistical science university college london 
cox cox 

probability frequency reasonable expectation 
american journal physics 
dagum luby dagum luby 

approximation probabilistic inference 
technical report may section medical informatics stanford university school medicine stanford ca 
ambrosio ambrosio 

local expression languages probabilistic dependence 
proceedings seventh conference uncertainty artificial intelligence los angeles ca pages 
morgan kaufmann 
de finetti de finetti 

theory probability 
wiley sons new york 
degroot degroot 

optimal statistical decisions 
mcgraw hill new york 
dempster dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society 
edmonds edmonds 

optimum branching 
res 
nbs 
evans minieka evans minieka 

optimization algorithms networks graphs 
marcel dekker new york 


chain graph markov property 
scandinavian journal statistics 
gabow gabow galil spencer 

efficient implementation graph algorithms contraction 
proceedings focs 
geiger heckerman geiger heckerman 
revised february 
characterization dirichlet distribution applicable learning bayesian networks 
technical report msr tr microsoft redmond wa 
geman geman geman geman 

stochastic relaxation gibbs distributions bayesian restoration images 
ieee transactions pattern analysis machine intelligence 
gill gill murray wright 

practical optimization 
academic press new york 


probability weighing evidence 
new york 


kinds probability 
science 


estimation probabilities 
mit press cambridge ma 
hacking hacking 

emergence probability 
cambridge university press new york 
hastings hastings 

monte carlo sampling methods markov chains applications 
biometrika 
heckerman heckerman 

tractable algorithm diagnosing multiple diseases 
proceedings fifth workshop uncertainty artificial intelligence windsor pages 
association uncertainty artificial intelligence mountain view ca 
henrion shachter kanal lemmer editors uncertainty artificial intelligence pages 
north holland new york 
heckerman geiger heckerman geiger 
december 
learning bayesian networks 
technical report msr tr microsoft redmond wa 
heckerman heckerman mamdani wellman 

realworld applications bayesian networks 
communications acm 
heckerman heckerman geiger chickering 

learning bayesian networks combination knowledge statistical data 
machine learning appear 
skj th thiesson 

user guide 
technical report department mathematics computer science aalborg denmark 
howard howard 

decision analysis perspectives inference decision experimentation 
proceedings ieee 
howard howard 

influence relevance knowledge 
oliver smith editors influence diagrams belief nets decision analysis chapter 
wiley sons new york 
howard matheson howard matheson 

influence diagrams 
howard matheson editors readings principles applications decision analysis volume ii pages 
strategic decisions group menlo park ca 
jensen andersen jensen andersen 

approximations bayesian belief universes knowledge systems 
technical report institute electronic systems aalborg university aalborg denmark 
johnson johnson 
fast local search 
focs pages 
kahneman kahneman tversky editors 
judgment uncertainty heuristics biases 
cambridge university press new york 
karp karp 

simple derivation edmond algorithm optimal branchings 
networks 
kass raftery kass raftery 

bayes factors model uncertainty 
technical report department statistics carnegie mellon university pa korf korf 

linear space best search 
artificial intelligence 
lauritzen lauritzen 

lectures contingency tables 
university aalborg press aalborg denmark 
lauritzen spiegelhalter lauritzen spiegelhalter 

local computations probabilities graphical structures application expert systems 
royal statistical society 
lauritzen lauritzen thiesson spiegelhalter 

diagnostic systems created model selection methods case study 
cheeseman editors ai statistics iv volume lecture notes statistics pages 
springer verlag new york 
madigan raftery madigan raftery 

model selection accounting model uncertainty graphical models occam window 
journal american statistical association 
metropolis metropolis rosenbluth rosenbluth teller teller 

equation state calculations fast computing machines 
journal chemical physics 
neal neal 

probabilistic inference markov chain monte carlo methods 
technical report crg tr department computer science university toronto 


representing solving decision problems 
phd thesis department engineering economic systems stanford university 
pearl pearl 

fusion propagation structuring belief networks 
artificial intelligence 
pearl pearl 

probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann san mateo ca 
pearl pearl 

causal diagrams empirical research 
biometrika appear 
pearl verma pearl verma 

theory inferred causation 
allen fikes sandewall editors knowledge representation reasoning proceedings second international conference pages 
morgan kaufmann new york 
press press teukolsky vetterling flannery 

numerical recipes cambridge university press new york 
agogino agogino 

real time expert system fault tolerant supervisory control 
patton editors computers engineering pages 
american society mechanical engineers ca 
rissanen rissanen 

stochastic complexity discussion 
journal royal statistical society series 
robins robins 

new approach causal inference mortality studies sustained exposure results 
mathematical modeling 
rubin rubin 

bayesian inference causal effects role randomization 
annals statistics 
russell russell binder koller 

adaptive probabilistic networks 
technical report csd university california berkeley 
savage savage 

foundations statistics 
dover new york 
schwarz schwarz 

estimating dimension model 
annals statistics 
shachter shachter 

probabilistic inference influence diagrams 
operations research 
shachter shachter andersen 

directed reduction algorithms decomposable graphs 
proceedings sixth conference uncertainty artificial intelligence boston ma pages 
association uncertainty artificial intelligence mountain view ca 
spiegelhalter spiegelhalter dawid lauritzen cowell 

bayesian analysis expert systems 
statistical science 
spiegelhalter lauritzen spiegelhalter lauritzen 

sequential updating conditional probabilities directed graphical structures 
networks 
spirtes spirtes glymour scheines 

causation prediction search 
springer verlag new york 
suermondt cooper suermondt cooper 

combination exact algorithms inference bayesian belief networks 
international journal approximate reasoning 
tarjan tarjan 

finding optimal branchings 
networks 
thomas thomas spiegelhalter gilks 

bugs program perform bayesian inference gibbs sampling 
bernardo berger dawid smith editors bayesian statistics pages 
oxford university press 
titterington titterington 

updating diagnostic system unconfirmed cases 
applied statistics 
tversky kahneman tversky kahneman 

judgment uncertainty heuristics biases 
science 
verma pearl verma pearl 

equivalence synthesis causal models 
proceedings sixth conference uncertainty artificial intelligence boston ma pages 
morgan kaufmann 
von neumann morgenstern von neumann morgenstern 

theory games economic behavior 
princeton university press princeton nj 
winkler winkler 

assessment prior distributions bayesian analysis 
american statistical association journal 
york york 

bayesian methods analysis misclassified incomplete multivariate discrete data 
phd thesis department statistics university washington seattle 
