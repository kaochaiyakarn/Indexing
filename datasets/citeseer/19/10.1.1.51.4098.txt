instance learner entropic distance measure john cleary leonard trigg dept computer science university waikato new zealand 
mail trigg waikato ac nz entropy distance measure benefits 
things provides consistent approach handling symbolic attributes real valued attributes missing values 
approach possible transformation paths discussed 
describe instance learner uses measure results compare favourably machine learning algorithms 
task classifying objects researchers artificial intelligence devoted time effort 
classification problem hard data available may noisy irrelevant attributes may examples learn simply domain inherently difficult 
different approaches tried varying success 
known schemes representations include id uses decision trees quinlan foil uses rules quinlan protos case classifier porter bareiss holte instance learners ib ib aha kibler albert aha 
schemes demonstrated excellent classification accuracy large range domains 
instance algorithms lack integrated theoretical background 
schemes primarily designed handle symbolic attributes difficulty domains features may real valued 
schemes handle real feature values instance learner entropic distance measure extended cope symbolic attributes ad hoc manner 
similar problem apparent classifiers handle missing values 
common approaches treat missing values separate value treat maximally different replace average value simply ignore 
discuss entropy distance measure provides unified approach dealing problems 
instance learner uses measure examine performance range problems 
instance learners instance learners classify instance comparing database pre classified examples 
fundamental assumption similar instances similar classifications 
question lies define similar instance similar classification 
corresponding components instance learner distance function determines similar instances classification function specifies instance similarities yield final classification new instance 
addition components ibl algorithms concept description updater determines new instances added instance database instances database classification 
simple ibl algorithms instance classified moved instance database correct classification 
complex algorithms may filter instances added instance database reduce storage requirements improve tolerance noisy data 
nearest neighbour algorithms cover hart simplest instance learners 
domain specific distance function retrieve single similar instance training set 
classification retrieved instance classification new instance 
edited nearest neighbour algorithms hart gates selective instances stored database classification 
nearest neighbour algorithms slightly complex 
nearest neighbours new instance retrieved whichever class predominant new instance classification 
standard nearest neighbour classification nearest neighbour classifier 
instance learner entropic distance measure aha kibler albert describe instance learners increasing sophistication 
ib implementation nearest neighbour algorithm specific distance function 
real valued attributes normalised common scale attributes equal weighting missing values assumed maximally different value 
ib contains extensions reduce storage requirements misclassified instances saved 
ib extension improve tolerance noisy data instances sufficiently bad classification history forgotten instances classification history classification 
aha described ib ib handle irrelevant novel attributes 
cost salzberg modification stanfill waltz value difference metric conjunction instance weighting scheme system pebls 
scheme designed classifying objects feature values symbolic 
numeric distances symbolic values calculated degree relative frequencies symbols classifications remains 
instances memory weighted past classification accuracy 
desirable properties classification algorithm classification algorithm exhibit characteristics surface defined similarity measure hyperspace predictor attributes smooth 
evaluation functions computer backgammon berliner argues surface defined evaluation function smooth ridges discontinuities surface program utilising evaluation function may decisions detriment 
applies instance learning large jumps distance measure small changes instance database test instance 
natural way adjusting relevances predictor attributes 
intuitive way dealing partial data 
datasets real world domains contain missing values instance learner entropic distance measure ignore instances containing missing values throw away useful information contained values 
algorithm restricted prediction symbolic values able predict real values 
entropy distance measure approach take computing distance instances motivated information theory 
intuition distance instances defined complexity transforming instance 
calculation complexity done steps 
finite set transformations map instances instances defined 
program transform instance finite sequence transformations starting terminating usual development complexity theory programs sequences prefix free appending termination symbol string 
usual definition kolmogorov complexity program length shortest string representing program li vitanyi 
approach kolmogorov distance instances defined length shortest string connecting instances 
approach focuses single transformation shortest possible transformations 
result distance measure sensitive small changes instance space solve smoothness problem 
distance defined tries deal problem summing possible transformations instances 
entirely clear summed doing 
adding lengths different transformations clearly correct thing 
key point note possible associate probability sequence 
complexity length program measured bits appropriate probability particular true defined distance kolmogorov complexity sum probability transformations satisfy kraft inequality 
way interpreting probability program generated random selection transformations 
terms distance instances probability instance learner entropic distance measure instance arrived doing random walk away original instance 
summing paths probability transformed units complexity logarithm 
approach summing possible transforming paths successfully yee allison theory measures distance sequences dna 
empirical evidence mutational transformations strings single shortest path gives robust realistic measure relatedness dna sequences 
specification possibly infinite set instances finite set transformations maps instances instances contains distinguished member symbol completeness maps instances 
set prefix codes terminated members uniquely define transformation probability function defined 
satisfies properties tu tu consequence satisfies probability function defined probability paths instance instance instance learner entropic distance measure easily proven satisfies properties function defined log strictly distance function 
example general non zero function emphasised notation symmetric 
possibly counter intuitive lack properties interfere development algorithm 
properties provable real numbers example theory compute distance real numbers 
done steps 
distance function integers developed modified deal real numbers 
set instances integers positive negative 
transformations set string marker left right respectively add subtract 
probability string transformations determined product probability individual transformations probability symbol set arbitrary value left right 
shown significant effort space depends absolute difference abusing notation slightly write instance learner entropic distance measure log log log log distance proportional absolute difference instances 
note set transformations chosen determined theory 
transformation sets different distance functions envisaged formulation capture idea points equivalent space invariant left right shifts giving results practice 
reformulate real numbers assumption real space discrete space distance discrete instances small 
step take expressions limit approaches transformation strings long 
gives reformulated probability density function probability integer di generated di rescaled terms real value results probability density function reals dx formulation functions scale length example mean expected value distribution 
different applications necessary choose reasonable value instance learner entropic distance measure rough guidelines 
example instances measurement error probably smaller standard deviation errors measurement 
section specifies technique choose values 
symbolic probabilities advantages approach real attributes symbolic attributes dealt framework 
deal symbolic attributes consider set instance values occur probabilities transformations allow instances transmutation instance instance 
probability symbol staying string transformation taken probability probability transformation symbol summing possible transformations gives note probability analogous probability equivalent development real numbers 
reasonable value chosen depending data modelled 
combining attributes compute distance instances attribute straightforward 
set transformations combined attributes taken union transformations individual attributes 
transformation strings modelled sequentially transforming attribute second attribute attributes transformed 
result probability total string product probabilities individual strings distance function sum distances individual attributes 
instance learner entropic distance measure simple additive approach taken results 
worth noting possible way combining attributes 
example instance modelled dimensional space set transformations naturally left right 
summing transformations resulting distance simple additive individual distances ordinates 
missing values issue dealt datasets instances attributes missing 
approaches literature vary widely deal problem 
cases distance missing attribute taken maximum possible entire instance ignored 
values missing instance classified attributes simply ignored predictions just remaining attributes 
interesting case missing values instances stored database 
way chosen deal assume missing values treated drawn random instances database 
easily fitted probability distance setting probability transforming missing value mean probability transforming specified attribute values data base sum specified instances database number instances 
effective distance missing value roughly expected distance random instance attribute 
algorithm implementation instance classifier uses entropic distance measure described need way instance learner entropic distance measure selecting values parameters way values returned distance measure give prediction 
choosing values arbitrary parameters dimension choose values parameters real attributes symbolic attributes 
behaviour distance measure parameters change interesting 
consider probability function symbolic attributes changes 
value close instances symbol different current low transformation probability instances symbol high transformation probability 
distance function exhibit nearest neighbour behaviour 
approaches transformation probability directly reflects probability distribution symbols favouring symbols occur frequently 
behaviour similar default rule learning schemes simply take whichever classification regardless new instance attribute values 
changes behaviour function varies smoothly extremes 
distance measure real valued attributes exhibits properties 
small probability instances drops quickly increasing distance functioning nearest neighbour measure 
hand large instances transformation weighted equally 
cases think number instances included probability distribution varying extreme distribution nearest neighbour extreme instances weighted equally 
nearest neighbour minimum greater 
effective number instances computed function expression instance learner entropic distance measure total number training instances number training instances smallest distance attribute 
algorithm chooses value selecting number inverting expression 
selecting gives nearest neighbour algorithm choosing gives equally weighted instances 
convenience number specified blending parameter varies intermediate values interpolated linearly 
think selected number sphere influence specifying neighbours considered important harsh cut edge sphere gradual decreasing importance 
compute iterative root finder slow results cached instance value reappears precalculated parameters 
symbolic cases simply set directly proportional parameters set dimension independently blend parameter gives equal weighting attribute 
combined attribute distance measure compute size final sphere influence 
usually smaller size specified single attribute level 
category prediction calculate probability instance category summing probabilities instance member probabilities category calculated 
relative probabilities obtained give estimate category distribution point instance space represented techniques return single category result classification 
ease comparison choose category highest probability classification new instance 
alternatives include instance learner entropic distance measure choosing class random relative probabilities returning normalised probability distribution answer 
results get idea algorithm performs practise classification carried datasets commonly machine learning literature 
datasets holte originally taken uci machine learning database repository 
datasets partitioned training test 
different partitions dataset 
schemes run partitions results averaged 
fraction correct classifications table 
best result dataset highlighted bold face 
schemes run default settings 
results obtained pruned trees rules 
default blend setting value datasets 
results obtained blend setting gave best accuracy dataset 
dataset tree rules foil ib best bc ch gl hd ho hy ir la ly se vo table 
classification accuracy datasets 
seen performs wide range schemes 
cases better instance instance learner entropic distance measure learner ib cases gave best results schemes 
allowing blend parameter vary gives better results currently investigating ways set intelligently possibly varying different attributes 
interesting note algorithm classifies single attribute performs datasets indicating varying importance individual attributes improve performance 
instance learning scheme 
real datasets performs range rule instance learning schemes 
underlying technique summing probabilities possible paths solves smoothness problem believe contributes strongly performance 
underlying theory allows clean integration symbolic real valued attributes principled way dealing missing values 
implementation results implementation want explore refinements basic algorithm extensions range applicability 
example possible predict real valued attributes 
exploring applications complex domains similarity images weather maps 
data files simple learning algorithm classification single attribute gets better performance 
lowering blend parameter important attributes raising unimportant ones pay cases 
acknowledgments done part weka project university waikato 
members weka team particular steve garner supplying test datasets doing computer runs 
instance learner entropic distance measure aha kibler albert 
instance learning algorithms 
machine learning pp 

aha 
tolerating noisy irrelevant novel attributes instance learning algorithms 
international journal man machine studies pp 

berliner 
backgammon computer program beats world champion 
artificial intelligence pp 

cost salzberg 
weighted nearest neighbour algorithm learning symbolic features 
machine learning pp 

cover hart 
nearest neighbour pattern classification 
ieee transactions information theory pp 

gates 
reduced nearest neighbour rule 
ieee transactions information theory pp 

hart 
condensed nearest neighbour rule 
ieee transactions information theory pp 

holte 
simple classification rules perform commonly datasets 
machine learning pp 

li vitanyi 
kolmogorov complexity applications springer verlag new york 
porter bareiss holte 
concept learning heuristic classification weak theory domains 
artificial intelligence pp 

quinlan 
induction decision trees 
machine learning pp 

quinlan 
learning logical definitions relations 
machine learning pp 
stanfill waltz 
memory reasoning 
communications acm pp 

yee allison 
reconstruction strings past 
computer applications biosciences pp 

