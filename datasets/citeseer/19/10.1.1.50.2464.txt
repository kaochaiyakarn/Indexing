digiteyes vision hand tracking human computer interaction james rehg takeo kanade dept electrical computer eng 
robotics institute carnegie mellon university carnegie mellon university pittsburgh pa pittsburgh pa cs cmu edu tk cs cmu edu appears proc 
ieee workshop motion non rigid articulated objects austin texas november pages 
computer sensing hand limb motion important problem applications humancomputer interaction hci virtual reality athletic performance measurement 
commercially available sensors invasive require user wear gloves targets 
developed noninvasive vision hand tracking system called digiteyes 
employing kinematic hand model digiteyes system demonstrated tracking performance speeds hz line point features extracted gray scale images unmarked hands 
describe application sensor mouse user interface problem 
human sensor capable tracking person spatial motion techniques computer vision powerful tool human computer interfaces 
sensor located user environment person operate natural conditions lighting dress providing degree convenience flexibility currently unavailable 
purpose visual sensing human hands limbs modeled articulated mechanisms systems rigid bodies connected joints degrees freedom dof 
model applied fine visual scale describe hand motion coarser scale describe motion entire body 
observation formulate human sensing real time visual tracking supported nasa george marshall space flight center ngt 
articulated kinematic chains 
frameworks human motion analysis possible approach main advantages 
tracking hand dof provide user maximum flexibility interface applications 
see examples interfaces requiring hand sensor 
addition general modeling approach kinematics possible track subset hand body states basic algorithm 
benefit full state tracking invariance unused hand motions 
motion particular finger example recognized joint angles regardless pose palm relative camera 
modeling hand kinematics eliminate need application user modeling 
digiteyes system treats hand tracking model sequential estimation problem sequence images hand model estimate hand configuration frame 
possible hand configurations represented vectors state space encodes pose palm rotation translation dof joint angles fingers states finger thumb 
hand configuration generates set image features lines points projection camera model 
feature measurement process extracts hand features grey scale images detecting occluding boundaries finger links tips 
state estimate image computed finding state vector best fits measured features 
basic tracking framework quaternions represent palm rotation resulting model rotational states number dof 
quaternions representation advantage free singularities 
similar 
articulated mechanisms difficult track single rigid objects traditionally addressed computer vision 
major difficulties large size state space nonlinearities state feature mapping called measurement model self occlusions 
finger articulations add additional states rigid body motion palm significantly increasing computational cost estimation 
additional states parameterized joint angles introduce nonlinearities kinematic singularities measurement model 
singularities arise small change state effect image features 
addition problems fingers occlude palm motion making feature measurement difficult 
digiteyes system uses local search deal large state space nonlinear measurement model 
key local gradient approach tracking high image acquisition rate hz limits change hand state image feature location frames 
state space exploit locality linearizing nonlinear state model previous estimate 
techniques robotics provide fast computation necessary kinematic jacobian 
kinematic singularities dealt stabilizing state estimator 
resulting linear estimation problem solved frame producing sequence state corrections integrated time yield estimated state trajectory 
result high image sampling rate change hand features frames small 
image state estimate previous frame predict feature positions 
feature detectors initialized predictions exploit symmetry finger links extract lines points match hand model 
image sequence user places hand known starting configuration initialize tracking 
current system finger link detects features independently limits sensor hand motions occlusions 
extending feature processing approach remove limitation 
described digiteyes system detail gave results tracking dof hand model camera image sequence orthographic projection 
describes extension perspective projection gives detailed example user interface sensor graphical mouse 
difficult problems remain tracking occlusions complicated backgrounds results demonstrate potential approach vision human motion sensing 
previous previous tracking general articulated objects includes 
yamamoto describe system human body tracking kinematic geometric models 
give example tracking single human arm torso optical flow features 
pentland horowitz give example tracking motion human optical flow articulated deformable model 
related approach terzopoulos track articulated motion deformable superquadric models 
describes system interpreting american sign language image sequences single hand 
system uses full set hand dofs employs glove colored markers simplify feature extraction 
earliest systems rourke badler analyzed human body motion constraint propagation 
earlier articulated models demonstrated real time tracking results full state complicated mechanism human hand natural image features 
addition previous articulated object tracking authors applied general vision techniques human motion analysis 
contrast digiteyes analyze subset total hand motion set gestures rigid motion palm 
darrell pentland describe system learning recognizing dynamic hand gestures 
related takes neural network approach hand gesture recognition 
real time unmarked hand images don produce motion estimates difficult apply problems mouse interface 

blake describe real time contour tracking system follow silhouette rigidly moving hand affine motion model 
anchor palm point th finger thumb th finger side view link link link kinematic models illustrated fourth finger thumb 
arrows illustrate joint axes link chain 
hand modeling visual tracking section brief summary kinematic feature models hand 
see detail 
dh representation widely robotics describe hand kinematics 
simplifying assumptions modeling hand illustrated fig 

assume fingers hand planar mechanisms degrees freedom dof 
abduction dof moves plane finger relative palm remaining dof determine finger configuration plane 
finger anchor point position base joint center frame palm assumed rigid 
real fingers deviate modeling assumptions slightly adequate practice 
hand features consist lines points generated projection hand model image plane 
finger link modeled cylinder generates pair lines image corresponding occlusion boundaries 
bisector lines contains projection cylinder central axis link feature 
link feature vector ae gives parameters line equation ax gamma ae 
central axis line link feature eliminates need model cylinder radius slope pair lines relative central axis significant near finger tips 
entire line endpoints difficult measure practice 
fig 
shows link feature lines extracted links finger 
features extracted grey scale hand images simple local operator described 
link link feat link feat tip feat features hand tracking illustrated finger links tip 
infinite line feature projection finger link central axis 
state estimation articulated mechanisms take nonlinear squares approach state estimation 
form residual frame measures difference measured image features projection estimated hand model image 
state correction obtained minimizing squared magnitude residual vector modified gauss newton gn algorithm 
residual vector image gn state update equation gamma gamma jacobian matrix residual evaluated sources nonlinearity model trigonometric terms joint angles inverse depth terms perspective projection 
constant diagonal conditioning matrix stabilize squares solution presence kinematic singularities 
approach rigid body tracking lowe 
remainder section gives derivation link tip residual jacobians orthographic projective cases 
residual derivatives obtained simple image plane projections gradient point positions respect state 
gradients computed quickly standard robotic techniques 
residual measures image plane distance projected hand geometry lines points detected features 
zero hand model perfectly aligned image data 
residual ith tip feature vector image plane defined gamma tip center camera coordinate frame function hand state delta gives projection point image plane measured tip feature 
link residual measures deviation points projected cylinder axis measured feature line 
gamma ae point projection operator written ff ff intrinsic camera model see ideal point projection model 
orthographic case projective case 
calculate tip link jacobians need orthographic case perspective case gammax gammay calculate tip jacobian differentiate respect state vector obtaining ca gradient projection matrix choice determined projection model 
similarly calculate link jacobian differentiate obtaining ca substituting obtain aff bff aff bff gamma aff bff sample graphical environment mouse 
gradient vector link constitutes row total jacobian matrix 
geometrically formed projecting kinematic jacobian points link direction feature edge normal 
kinematic jacobians composed terms form arise frequently robot control obtained directly kinematic model 
see details 
graphical mouse experimental results describe detail experiment graphical mouse user interface digiteyes sensor simplified state hand model 
shows simple graphical environment interface 
consists ground plane cursor drawn pole cursor top spherical object manipulated 
shadows generate additional depth cues 
interface problem provide user control cursor dof 
digiteyes solution partial hand model consisting fourth fingers hand thumb 
palm constrained lie plane table dof interface 
finger articulated dofs fourth finger thumb single dof allowing rotate plan table 
hand model illustrated fig 
corresponding intensity image fig 

single camera oriented approximately degrees table top acquires images hand model mouse application shown estimated state frame motion sequence 
vertical line shows height tip ground plane 
tracking 
palm position table controls base position pole height index finger table controls height cursor 
particular mapping important advantage decoupling controlled dof making possible operate simultaneously 
fourth finger thumb abduction dof plane buttons 
figures give experimental results frame motion sequence 
figures show estimated hand state frame 
frames acquired ms sampling intervals 
pole heights base positions derived estimated hand states depicted fig 

motion sequence phases 
phase frame user finger raised lowered twice producing peaks pole height small variation estimated pole position 
second frame finger raised kept elevated thumb actuated button event 
actuation period frame frame results change pole height negligible change pole position 
third pole height held constant pole position varied 
sequence states varied simultaneously 
figures show input image associated state estimate frame 
sample mouse pole positions sequence illustrated fig 

results demonstrate fairly decoupling desired states useful dynamic range input hand image frame motion sequence 
motion 
largest coupling error occurs frame pole height drops thumb actuated 
simplicity mouse state generated hand state simple scaling coordinate change tends amplify sensor noise 
problems alleviated sophisticated processing sensor output 
example illustrates important advantage hand tracking kinematic models absolute distances finger height table measured single camera image 
ability recover spatial quantities hand motion advantages system gesture recognition 
experiment digiteyes implemented image processing platform called ic 
state estimates transfered ethernet silicon graphics indigo model display user interface demonstrations 
ic board delivers digitized images processor memory video rate computational overhead removing important bottleneck 
digiteyes framework visual hand tracking application mouse user interface problem 
currently extending results ways 
modifying feature extraction process handle occlusions complicated backgrounds 
second analyzing observability requirements articulated object tracking 
authors yuji help ic hardware environment reviewer detailed comments 
mouse pole cursor positions motion sequence 
pole vertical line horizontal shadow thing moving sequence 
samples taken frames chosen illustrate range motion 
blake curwen zisserman 
framework spatiotemporal control tracking visual contours 
int 
computer vision 
darrell pentland 
space time gestures 
looking people workshop chambery france 
dennis schnabel 
numerical methods unconstrained optimization nonlinear equations 
prentice hall englewood cliffs nj 

hand shape identification tracking sign language interpretation 
looking people workshop chambery france 
faugeras 
dimensional computer vision geometric viewpoint 
mit press 
kang ikeuchi 
grasp recognition contact web 
proc 
ieee rsj int 
conf 
int 
robots sys raleigh nc 
lowe 
robust model motion tracking integration search estimation 
int 
computer vision 
terzopoulos 
shape nonrigid motion estimation physics synthesis 
ieee trans 
pattern analysis machine intelligence 
rourke badler 
model image analysis human motion constraint propagation 
ieee trans 
pattern analysis machine intelligence 
pentland horowitz 
recovery nonrigid motion structure 
ieee trans 
pattern analysis machine intelligence 
rehg kanade 
digiteyes vision human hand tracking 
technical report tr carnegie mellon univ school comp 
sci 
rehg kanade 
visual tracking high dof articulated structures application human hand tracking 
eklundh editor proc 
third european conf 
computer vision volume pages stockholm sweden 
springer verlag 

gest learning computer vision system recognizes hand gestures 
michalski tecuci editors machine learning iv 
morgan kaufmann 

robot dynamics control 
john wiley sons 
sturman 
hand input 
phd thesis massachusetts institute technology 
yamamoto 
human motion analysis robot arm model 
ieee conf 
comput 
vis 
pattern rec pages 
frames ms frame joint angle radians finger states palm rotation theta theta theta palm rotation finger joint angles mouse pole hand model 
frames ms frame button states finger thumb joint angles thumb fourth finger mouse pole hand model 
note button event signaled thumb motion frame 
frames ms frame palm translation tx ty tz estimated translation states mouse pole hand model 
axis motion constrained zero due tabletop 
frames ms frame workspace distance mouse pole interface mouse pole height mouse pole mouse pole mouse pole states derived digiteyes sensor motion sequence 
