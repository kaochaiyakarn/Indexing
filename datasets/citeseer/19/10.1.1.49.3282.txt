utility feature weighting nearest neighbor algorithms ron kohavi pat langley yun ronnyk sgi com langley cs stanford edu cs stanford edu silicon graphics shoreline blvd mountain view ca robotics laboratory stanford university stanford ca electrical engineering dept stanford university stanford ca 
nearest neighbor algorithms known depend heavily distance metric 
investigate weighted euclidean metric weight feature comes small set options 
describe diet algorithm directs search space discrete weights cross validation error evaluation function 
large set possible weights reduce learner bias lead increased variance overfitting 
empirical study shows data sets advantage weighting features increasing number possible weights zero little benefit degrades performance 
years instance methods emerged promising approach machine learning researchers reporting excellent results real world induction tasks 
basic approach involves storing training cases associated classes memory test instance finding training cases nearest test instance predict class 
drawback methods combined naive distance metric weights attributes equally suffer curse dimensionality number cases needed maintain error rate grows rapidly number features 
observation led researchers augment nearest neighbor methods techniques determining distinct weights feature 
instance cost salzberg differences probability distributions classes modify distance metric nominal attributes daelemans wettschereck mutual information compute coefficients numeric attributes 
aha reports incremental scheme alters feature weights depending distance predicted class 
researchers report improvement simple version nearest neighbor gives attributes equal weight 
explore different approach determining feature weights nearest neighbor classification 
considering continuous weight space restrict weights small finite set 
reduces representational power increases bias lower variance reduce chances overfitting 
report diet system incorporates wrapper method search weight space cross validation error nearest neighbor algorithm 
experimental studies suggest natural data sets restricting set weights alternatives equivalent feature subset selection gives best results 
decomposition errors bias variance helps explain results 
diet algorithm implement idea adapt wrapper approach originally developed feature selection determine appropriate weights nearestneighbor classification 
way needed number design decisions operation 
example selected best search expands best node expanded explore discrete weight space 
needed evaluation function direct search 
earlier wrappers estimated error induction algorithm case nearest neighbor measured fold cross validation training data 
discrete approach selecting feature weights required specify number nonzero weights considered 
determined set weights 
gamma 
starting point search decided assignment closest middle weight zero weight zero possible weights 
note approach equivalent feature subset selection initial node ignores attributes predicting frequent class 
heuristic search requires operators moving space 
adopted operators replaced current weight feature larger smaller value allowed set maximum minimum reached 
selected halting criterion stopped search encountered consecutive nodes children having scores better parent 
recall node evaluation requires separate cross validation run nearest neighbor appears expensive 
implement nearest neighbor incremental manner get cross validation error removing instance fold classifying repeating folds 
features delta branches step search keep computational costs reasonable 
refer collection design decisions diet algorithm cause features lose weight gain weight studies fixed number neighbors classification goal investigate feature weighting factor 
remain 
described approach feature weighting consider hypotheses behavior experiments designed test 
hypotheses experimental design naturally expect diet outperform methods domains 
data sets contain irrelevant features expect behave comparably simple nearest neighbor slightly worse due increased size hypothesis space 
domains relevant features equal importance expect diet weights outperform diet weights 
test hypothesis designed artificial domains involved continuous features range gamma classes incorporating target concept prototypical instances class 
prototypes 
domains relied prototypes different feature weights 
determine class randomly generated training test instance uniform distribution prototype nearest instance defined domains weights assigned class 
domain assigned weights features making equally relevant domain assigned weight features making irrelevant 
domain assigned weight features features feature 
predicted diet outperform nearest neighbor domains domain running non zero weights outperform variant non zero weight 
ensure relevance wanted study diet behavior natural domains 
determine candidates feature weighting yield improvements inspected learning curves plot error number training cases domains uci repository identified data sets anneal chess segment soybean large vehicle led nearest neighbor reach asymptote early curve 
suggested room improvement data sets 
selected points learning curve nearest neighbor reached asymptote determine number cases training sets 
predicted diet outperform nearest neighbor situations 
selected additional uci domains breast hypothyroid mushroom vote led characteristic 
expect better results nearest neighbor cases 
domain randomly selected training sets associated disjoint test sets diet access test sets computing cross validation estimates 
table shows characteristics various domains 
order test basic hypotheses training test partition ran simple nearest neighbor equal weights feature 
table 
summary domains experimental study 
group consists artificial domains group contains data sets nearest neighbor reached asymptote group contains data sets appeared close asymptotic performance 
group data set train size test size 
features 
classes weights anneal chess segment soybean large vehicle led breast hypothyroid mushroom vote led ran quinlan known decision tree algorithm aha ib instance algorithm incorporates different weighting schemes assigns weights features incrementally 
aim test specific hypotheses simply see diet fares compared known induction algorithms 
experimental results consider results comparative studies artificial domains described followed various uci domains 
effort gain deeper understanding results analyze tradeoff different sets possible weights 
experiments artificial domains results runs artificial domains appear shows mean error combination algorithm domain 
notation diet indicates feature weighting algorithm non zero weights 
predicted nearest neighbor produce slightly better results variants domain assumption features nn ib diet diet diet relevant irrelevant weighted fig 

absolute error rates artificial domains 
equally relevant perfect domain methods search larger space increased variance 
situation quite different domain features irrelevant 
expected versions diet outperform nearest neighbor see increasing number weights increases error 
inspection individual runs revealed told single non zero weight diet correct weights runs versions allowed weights generated run giving poorer performance 
ib weighting heuristic failed find optimal weights domain 
anticipated diet non zero weights outperform non zero weight simple nearest neighbor domain generated different weights 
variant fares better achieve lowest possible error obtained running nearest neighbor correct weights 
examination weights individual runs showed system nonoptimal values 
experiments done show half runs chosen weights higher estimated accuracy estimated accuracy true weights 
cases estimated accuracy chosen weights lower true weights search helped clear search stuck local optimum 
cases estimates higher search overfitting weights data increasing amount search helped true weights selected weights estimated superior 
nn ib diet diet diet chess segment soybean vehicle breast mushroom vote error fig 

absolute error rates domains uci repository 
experiments uci data sets presents experimental results terms absolute errors domains uci repository shows analogous errors measured relative simple nearest neighbor 
obvious result diet non zero weight typically gives lower errors nearest neighbor 
domains chess segment led hypothyroid vote differences significant anneal led mushroom soybean vehicle 
domain breast cancer difference significant favor nearest neighbor 
diet usually performed substantially better ib instance weighting method ran comparison 
cases system outperformed domains fared better nearest neighbor 
note diet better nearest neighbor domains predicted reached asymptote data sets thought hypothyroid vote 
apparently nearest neighbor improving domains large number features gave low learning rate making appear reached asymptotic error level 
reveals increasing number non zero weights rarely reduced classification error 
noted minor improvements going non zero weights anneal breast soybean large vehicle domains weights led large increases error data sets 
effect especially evident weight version diet failed significantly outperform version non zero weights domain 
despite efforts efficient implementation diet reliance wrapper approach generally slow 
running times single training test ib diet diet diet anneal chess segment led breast hypothyroid led algo nearest neighbor ratio fig 

relative error rates data sets 
quantities indicate error algorithm divided nearest neighbor values indicating improved performance method 
partition sgi challenge varied minutes minutes diet non zero weights simple nearest neighbor took seconds 
speeding process important direction research essential understand reasons simple feature selection fared relative diet larger sets weights 
bias variance decomposition way understanding behavior algorithms decompose error rates components 
target concept average error algorithm different data sets size decomposed squared bias component variance component 
squared bias measures closely random instance average prediction learning algorithm matches target value instance 
algorithm uses small set models weights able model target appropriately due lack representational power 
cases predictions biased 
variance term decomposition measures learning algorithm guess changes mean prediction varies training set 
minor changes training data produce changes predictions high variance colloquially overfits training set 
nearest neighbor framework feature weighting reduce bias distances features shrink nearest neighbor better model local effects 
feature weights selected vary nn bias nn var diet bias diet var diet bias diet var diet bias diet var irr chess hypothyroid led segment error fig 

bias variance decomposition error 
changes training set variance reduced space densely populated weights 
classification problems researchers proposed different decompositions bias variance 
decomposition proposed kohavi wolpert desirable property equivalent standard squared error loss leading constant half views label vector zero indicator variables computes squared error loss vector 
shows bias variance decomposition error selected data sets error varied widely different versions diet 
generated results samples data set times original size training set keeping rest instances compute bias variance 
expected shows feature weighting reduces bias especially chess led known irrelevant features 
introducing weights drastically lowers variance nearest neighbor especially non zero weight 
extreme examples effect occur domains chess led 
allow weights variance increases algorithm unstable due large number weight settings fit training set match true weights underlying distribution 
decomposition shows extra power increased set weights non zero weights reduce bias usually increased variance 
setting utility increasing number non zero weights negative 
reason believe different weights help artificial domain data sets larger recommend weights conjunction wrapper approach similar methods 
summary described diet algorithm uses simple wrapper approach heuristically search set weights nearestneighbor classification 
diet typically gave lower errors simple nearest neighbor weights features equally usually better aha ib incorporates incremental weighting method 
interesting finding considering small set weights typically gave better results larger set 
fact real world data sets examined running diet non zero weight assumes feature relevant irrelevant difficult outperform 
detailed analysis suggested restricting set weights reduce algorithm variance lower error rate 
naive assumption weights reduce classification error simply false increasing set possible weights increase variance bias variance tradeoff determine performance improves 
study domains non zero weights significantly reduced classification error 
stress superiority non zero weight feature subset selection hold context wrapper approach feature weighting small training sets 
repeat experiments training sets different sizes determine weights prove useful points learning curve 
run comparative studies diet approaches feature weighting 
example kelly davis report genetic algorithm search space feature weights lowe presents alternative scheme employs conjugate gradient descent weight space 
comparisons help determine relative benefits key ideas diet restricting number weights nearestneighbor classification wrapper method search space weights 
acknowledgments research supported part 
office naval research 
iri national science foundation development mlc software experiments part 
air force office scientific research 

david aha 
tolerating noisy irrelevant novel attributes instance learning algorithms 
international journal man machine studies 

bellman 
adaptive control processes guided tour 
princeton university press nj 

scott cost steven salzberg 
weighted nearest neighbor algorithm learning symbolic features 
machine learning 

daelemans 
acquisition stress approach 
computational linguistics 

dasarathy 
nearest neighbor nn norms nn pattern classification techniques 
ieee computer society press los alamitos california 

stuart geman eli bienenstock rene doursat 
neural networks bias variance dilemma 
neural computation 

george john ron kohavi karl pfleger 
irrelevant features subset selection problem 
machine learning proceedings eleventh international conference pages 
morgan kaufmann july 

kelly davis 
hybrid genetic algorithm classification 
proceedings twelfth international joint conference artificial intelligence pages 
morgan kaufmann 

ron kohavi david wolpert 
bias plus variance decomposition zero loss functions 
saitta editor machine learning proceedings thirteenth international conference 
morgan kaufmann july 
available robotics stanford edu users ronnyk 

lowe 
similarity metric learning variable kernel classifier 
neural computation 

christopher merz patrick murphy 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 

ross quinlan 
programs machine learning 
morgan kaufmann san mateo california 

dietrich wettschereck 
study distance machine learning algorithms 
phd thesis oregon state university 
article processed macro package llncs style 
