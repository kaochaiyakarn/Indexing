reinforcement learning algorithms function optimizers ronald williams jing peng college computer science northeastern university boston ma appears proceedings international joint conference neural networks washington dc vol 
ii pp 

nonassociative reinforcement learning algorithm viewed method performing function optimization possibly noise corrupted sampling function values 
describe results simulations optima deterministic functions studied ackley sought variants reinforce algorithms 
results obtained certain algorithms compare favorably best results ackley 
background thesis ackley explored interesting general approach function optimization differing somewhat common approaches emphasize limiting behaviors terminating conditions 
studied behavior variety general purpose optimization algorithms existing design number optimization problems involving functions defined binary tuples measured performance particular criterion require convergence 
algorithms investigated forms simple hillclimbing algorithm combined random restarts convergence simulated annealing combined random restarts convergence fixed temperature thermally hillclimber essentially simulated annealing annealing variations genetic algorithms combined random restarts convergence combination hillclimbing genetic search random restarts convergence connectionist network algorithm called stochastic iterated genetic hillclimbing contains elements drawn number sources interesting description terms election metaphor 
studied number existing reinforcement learning algorithms appropriate candidates various algorithms stochastic learning automata literature various connectionist reinforcement learning research supported national science foundation iri 
algorithms associative reward penalty algorithm barto colleagues reinforce algorithms williams 
report describes results variants reinforce tasks studied ackley 
problem formulation problem formulation proposed ackley adopted loosely described way imagine generate test scenario particular stopping criterion 
job generator case simply generate trial points tester simply evaluates value function optimized trial point 
stopping criterion process generate test repeated indefinitely 
interesting generator adaptive tends generate better points gains information function optimized 
assume optimization problem involves maximization better point having higher function value 
systems interest ones perform endless repetition simple generate test adapt loop 
view system carrying optimization algorithm add stopping criterion sort way reporting answer step process 
ways issues handled 
way suitable process eventually converges eventually trial point generated imagine testing condition reporting point output algorithm 
way retain memory best point far report point output algorithm terminated 
real value approach attempt isolate possible useful features generate test adapt loop issues involving choice termination criterion 
particular approach require global optimum recognized explicitly process terminates global optimum generated implicitly eventual convergence global optimum 
generator may keep generating inferior points having generated global optimum 
approach stresses ackley called sustained exploration 
believe sustained exploration may turn important feature procedures designed perform sort optimization process particularly large finite search spaces having points involved function optimized varies time expect characteristic realistic optimization problems 
course study various approaches adaptive generation process simulation necessary incorporate adaptive generators studied terminating algorithms 
follow ackley adopt strategy problems optimum point known simply run algorithm point generated maximum computational effort expended 
measure computational effort number function evaluations performed 
measure determine terminate term optimization process meant cover variety possibilities including processes terminate global maximum processes simply continue discover better better points possible terminate 
run declare failure measure performance various adaptive generation techniques studied 
reinforcement learning networks adaptive trial generators connectionist network generator trial points optimization problem long domain function optimized represented set output patterns network 
furthermore reinforcement learning algorithm adjusting weights network cf 
provide means adapting behavior trial point generator regarding function value reinforcement signal delivered network network produces output pattern particular reinforcement learning algorithm represents extreme specialization general formulation reinforcement learning problem reinforcement may stochastic deterministic function network may required perform mapping input output associative reinforcement learning reinforcement signal may depend past input output patterns environment memory 
general formulation encompasses features characteristic realistic learning situations 
reason number sophisticated directions study reinforcement learning systems taken including temporal difference learning methods adaptive prediction reinforcement internal models environment type relevant purposes 
treat problem maximizing deterministic function extremely pared form reinforcement learning task nonassociative involves memoryless environment noise free reinforcement signal 
important recognize connectionist approach optimization different proposed hopfield tank 
approach knowledge function optimized approach taken 
approach requires devising function having global optimum actual function optimized 
new function incorporated network weights network settles local optima function 
contrast technique studied general information concerning function optimized 
information available function arises sampling function values various points 
paradigm studied exploring genetic algorithms function optimization function optimized usually called fitness function accord approach inspired process biological evolution natural selection 
approaches reinforcement learning networks study stochastic units allow sampling variety output patterns 
setting weights network particular distribution output patterns generated 
broad goal construed implying simplest form reinforcement learning task trivial 
studies ackley suggest may learn studying simplest paradigm 
adaptive sampling scheme try warp sampling distribution ways sample better points 
applying reinforcement learning algorithm weights viewed means doing just sample points generated network 
learning boltzmann machine viewed adjusting distribution output patterns generated significant difference approaches objective boltzmann machine learning try output pattern distribution match target distribution objective reinforcement learning networks try output pattern distribution favor high payoff points low payoff points 
network architecture functions optimized domain set binary tuples experiments networks having bernoulli logistic units terminology williams 
output unit determined stochastically bernoulli distribution parameter logistic function usual weighted summation input values unit 
furthermore studies reported trial generating network simple degenerate units output units interconnections 
corresponds team automata literature stochastic learning automata 
call networks teams bernoulli logistic units 
bernoulli logistic unit receiving input unit sources external unit bias weight unit 
team bernoulli logistic units probability ith unit produces simply exp gammaw probability particular output vector appropriate product individual units team draw output values independently 
bias weights adjusted particular learning algorithm details algorithms discussed 
note unit equally pick increasing 
adjusting bias weights team bernoulli logistic units tantamount adjusting probabilities individual components 
algorithms total different algorithms general form tth time step generating output receiving reinforcement increment bias weight picked simple architecture place start turns provides interesting challenges approaches chose study time enlightening discover relatively successful algorithms turned certain problems thought require sophisticated architectures 
fully expect networks nontrivial interconnection topologies lead efficient search comment briefly progress supports expectation 
deltaw gamma ff learning rate ffi weight decay rate parameters algorithm 
call ae reinforcement factor eligibility weight reinforcement factor exponentially weighted average trace prior reinforcement values flr gamma gamma fl computed ae gamma gamma gamma fi fi parameter algorithm 
trace parameter fl set equal experiments reported 
considered different forms eligibility gamma gamma gamma average past values computed exponential weighting scheme fly gamma gamma fl forms eligibility considered varied algorithms studied feature dimensions size decay rate ffi size fi 
particular different settings ffi zero appropriate nonzero value happened experiments 
similarly different settings fi zero appropriate nonzero value happened experiments 
experimental results report combinations variations fi ffi nonzero 
algorithms members reinforce class algorithms member statistically climbs gradient expected reinforcement viewed slight variants 
reinforce algorithms gamma ffi 
follows readily observations 
fi uses standard reinforcement comparison technique advocated sutton essentially tries reinforcement factor zero mean 
version fi gives reinforcement factor negative bias expect destabilize algorithm fails change 
ackley technique component algorithm curious see prevent convergence force sustained exploration 
gamma form eligibility motivated simulation results sutton private communication suggesting faster learning achieved form analytic results suggesting proving 
omit details analysis algorithm results combining standard reinforcement comparison reinforcement factor eligibility factor close relationship linear regression analysis bernoulli logistic units 
weight decay chosen simple heuristic method force sustained exploration discovered algorithms converge 
hard show having decay bias particular member team bernoulli units closely related having nonzero mutation rate particular locus genetic algorithm 
optimization problems optimization problems simulation results report particular maximization problems originally studied ackley 
problems detail contrived problems designed isolate specific features various optimization problems may possess fifth specific combinatorial optimization problem 
description problems consistently notation number coordinates number gamma number tuple tuple problems fifth problem 
max 
function maximized point global maximum false maxima 
max 
function maximized gamma nj global maximum false maximum 
number points space uphill moves lead global maximum somewhat larger number false maximum look attractive hillclimber 
porcupine 
function maximized gamma mod essentially max high frequency component added confound myopic hillclimber 
global maximum point hamming distance local maximum 
plateaus 
function maximized computed follows divide bits equal sized groups 
group compute score bits group 
sum scores 
previous ones function global maximum 
large plateaus function constant 
problem investigated type combinatorial optimization problem known np complete minimum cut graph partitioning problem 
usual form problem graph number nodes assign node groups way groups contain equal numbers nodes number edges connecting nodes lying groups minimized 
nodes graph assignment nodes groups represented giving groups names considering binary tuple represent assignment ith node assigned group order treat optimization problem set binary tuples hard constraint groups contain equal numbers nodes replaced imbalance penalty 
ackley treated graph partitioning problem problem maximizing function gammac gamma gamma number edges cross partition particular assignment ackley studied randomly generated graphs graphs having particular hierarchical structure 
belief problems having type higher order structure hope develop useful general purpose optimization algorithms restricted attention hierarchically structured graphs studied 
graph nodes depicted 
class graphs ackley called multilevel hypercube graphs particular name mlc 
note function optimized problems symmetric bitwise complementation argument graph 
particular graph partitioning problem exactly global maxima number local maxima 
furthermore minimum hamming distance false peak global maximum problem 
results summary results experiments performed table 
parameters fixed problems particular algorithm learning rate ff 
extensive fine tuning performed ff combination algorithm problem 
table shows combination particular value learning rate runs combination rounded mean run length number function evaluations reach global optimum set runs 
problems mean runs algorithm problem combination graph partitioning problem runs performed 
result reported infinite indicates algorithm failed find global maximum runs 
discussed case happened reason algorithm trapped local maximum 
comparison give best average result reported ackley optimization problems 
results reported rounded means runs problems median runs graph partitioning problem 
discussion number observations results just 
experiments convinced algorithms having weight decay eventually converge 
expected algorithms having negative bias reinforcement factor turned true negatively biased reinforcement factor 
convergence generally false maximum reason algorithms usually successful graph partitioning problem 
conclude negative bias mechanism providing sustained exploration believed 
consequence tendency converge problem choice learning rate algorithms weight decay critical disregarding problem typically failed plateaus function 
large learning rate guarantees search converge plateaus 
particularly interesting result algorithms maximum porcupine virtually fast maximum max 
certainly reinforcement learning algorithms able handle noisy reinforcement signals treat porcupine merely noise added underlying function happens max quickly discover climb 
general draw results functions functions property simple bitwise correlation reinforcement function sufficient discover maximum assuming statistics collected 
clearly plateaus function requires waiting longest right statistical pattern emerge bit group sample case reason algorithms take longer 
results table suggest part speedup provided choosing gamma form eligibility reinforce 
situations difference pronounced maximum rapidly 
conjecture due fact learning algorithms form eligibility different estimators underlying quantity gamma better small sample properties 
consistent result noted table combination straight reinforcement comparison gamma form eligibility weight decay gave lowest average time find maximum 
graph partitioning problem obviously provides real challenge algorithms strong tendency statistically avoid failing times getting stuck local maxima 
approach adopted try algorithms problem ackley algorithm converges detect convergence perform random restart 
speculate reason combining weight decay algorithms allows global maximum refusal allow weights grow certain point amounts continuing tendency engage random restarts coupled tendency climb hills statistically 
analysis runs involving weight decay reveal behavior loosely described exploration local maximum followed may amount jump neighborhood local maximum exploration peak eventually jump leads neighborhood global maximum 
discuss additional lines process performing working teams bernoulli logistic units begun study version associative reward penalty algorithm incorporating reinforcement comparison component graph partitioning task appears succeed finding global optimum parameter settings allowing rapidly algorithms detailed succeeded tasks 
greatest interest ways making interconnections units hidden units serve command cells order coordinate choices output units 
learning algorithm allow network retain information gained exploration process control desired coordination trials 
team approach information represented result correlations behaviors individual units reinforcement signal 
problems order correlations carry little information correct direction move higher order correlations helpful 
example graph partitioning problem investigated 
bit vector representing partition reinforcement value componentwise complement 
order correlations individual components outcome zero sample uniformly 
bits strongly committed value discover correlation choice value outcome 
network coordinate choices output units able generate certain combinations bits greater probability individual components selected independently 
particular graph partitioning hope units corresponding highly interconnected group nodes graph operation coordinated point come select high probability assignment subset nodes 
network operates way expect find solution hierarchical graphs type studied quickly coordination 
trick course able represent higher order statistics points learning algorithm allows parameters controlling search distribution adjusted distribution comes capture regularities set points 
interestingly just identified network architecture learning algorithm somewhat sophisticated investigated reason believe behaves desirable way 
results preliminary investigation behavior hierarchical graph partitioning problem studied extremely promising 
example set runs task average steps required discover global optimum 
ackley stochastic iterated genetic hillclimbing ph dissertation dept computer science carnegie mellon university pittsburgh pa 
available connectionist machine genetic hillclimbing 
norwell ma kluwer 
ackley hinton sejnowski learning algorithm boltzmann machines cognitive science vol 
pp 

barto learning statistical cooperation self interested neuron computing elements human neurobiology vol 
pp 

barto anandan pattern recognizing stochastic learning automata ieee trans 
syst man vol 
pp 

barto anderson structural learning connectionist systems proc 
seventh ann 
conf 
cognitive science society irvine ca pp 

barto sutton anderson neuronlike elements solve difficult learning control problems ieee trans 
syst man cybern vol pp 

garey johnson computers intractability guide theory np completeness 
san francisco freeman 
goldberg holland eds special issue genetic algorithms machine learning vol 
nos 

holland adaptation natural artificial systems 
ann arbor university michigan press 
hopfield tank neural computation decisions optimization problems biol 
cybern vol 
pp 

kirkpatrick gelatt vecchi optimization simulated annealing science vol 
pp 

learning algorithms theory applications 
new york springer 
munro dual back propagation scheme scalar reward learning proc 
ninth ann 
conf 
cognitive science society seattle wa pp 

narendra learning automata survey ieee trans 
syst man vol 
pp 

sutton temporal credit assignment reinforcement learning ph dissertation univ massachusetts amherst coins tech 
rep 
sutton learning predict methods temporal differences machine learning vol 
pp 

sutton learning world models connectionist networks proc 
seventh ann 
conf 
cognitive science society irvine ca pp 

williams reinforcement learning connectionist networks mathematical analysis university california san diego inst 
cog 
sci 
tech 
rep 
williams class gradient estimating algorithms reinforcement learning neural networks proc 
ann 
intl 
conf 
neural networks san diego ca vol 
ii pp 

williams theory reinforcement learning connectionist systems northeastern univ boston ma tech 
rep nu ccs 
