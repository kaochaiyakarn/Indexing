learning hybrid bayesian networks data stefano monti intelligent systems program university pittsburgh cl pittsburgh pa isp pitt edu gregory cooper yz center biomedical informatics university pittsburgh forbes tower pittsburgh pa edu technical report april revised aug intelligent systems program university pittsburgh cl pittsburgh pa illustrate different methodologies learning hybrid bayesian networks bayesian networks containing continuous discrete variables data 
methodologies differ way handling continuous data learning bayesian network structure 
methodology uses discretized data learn bayesian network structure original non discretized data parameterization learned structure 
second methodology uses non discretized data learn bayesian network structure parameterization 
direct handling continuous data propose artificial neural networks probability estimators integral part scoring metric defined search space bayesian network structures 
methodologies assume availability complete dataset missing values hidden variables 
report experimental results aimed comparing methodologies 
results provide evidence learning discretized data presents advantages terms efficiency terms accuracy learned models alternative approach non discretized data 
bayesian belief networks bns referred probabilistic networks provide powerful formalism representing reasoning uncertainty 
construction bns domain experts difficult time consuming task 
knowledge acquisition experts difficult experts problems making knowledge explicit 
furthermore time consuming information needs collected manually 
hand databases increasingly abundant areas 
exploiting databases construction time bns may considerably decreased 
approaches learning bn structures data simplifying assumptions circumvent practical problems implementation theory 
common assumption variables discrete variables continuous normally distributed :10.1.1.52.1068:10.1.1.156.9918
interested task learning bns containing continuous discrete variables drawn wide variety probability distributions 
refer bns hybrid bayesian networks 
learning task consists learning bn structure parameterization 
straightforward solution task discretize continuous variables able apply established techniques available learning bns containing discrete variables 
approach appeal simple 
discretization general generate spurious dependencies variables especially local discretization strategies discretization strategies consider interaction variables alternative discretization direct modeling continuous data 
experiments described real synthetic databases investigate discretization data degrades structure learning parameter estimation bayesian network representation 
artificial neural networks anns estimators probability distributions presents solution problem modeling probabilistic relationships involving mixtures continuous discrete data 
particularly attractive allows avoid making strong parametric assumptions nature probability distribution governing relationships participating variables 
offer general semi parametric technique modeling probability mass discrete variables probability density continuous variables 
hand shown experimental evaluation discrete data confirmed evaluation reported main drawback ann estimators computational cost associated training learn bn structure 
continue initiated explore anns probability distribution estimators integral part scoring metric defined search space bn structures 
perform experimental evaluation aimed comparing new learning method simpler alternative learning bn structure discretized data 
results show discretization efficient accurate method model selection dealing mixtures continuous discrete data 
rest organized follows 
section briefly introduce bayesian belief network formalism basics learn bns data 
section describe learning method define ann scoring metric search space bn structures 
section describe artificial neural networks probability distribution estimators 
section experimental results aimed evaluating efficacy proposed learning procedure comparing simple alternative discretization continuous variables 
conclude discussion results suggestions research 
discretization techniques devised classification task mind best take consideration interaction class variable feature variables individually 
global discretization bayesian networks learning discretization consideration interaction dependent variables promising largely unexplored topic research addressed described 
total serum calcium cancer brain tumor coma simple belief network set nodes fx parent sets fx fx fx nodes represent binary variables values domain ftrue falseg 
notation denote false 
probability tables give values gamma 
background bayesian belief network defined triple omega directed acyclic graph set nodes fx xn representing domain variables set arcs representing probabilistic dependencies domain variables omega space possible instantiations domain variables probability distribution instantiations omega gamma node denote set parents give example simple bayesian network structure derived part 
looking network structure giving causal interpretation links displayed see cancer cause brain tumor cause increase total serum calcium 
furthermore brain tumor cause brain tumor increase total serum calcium cause patient lapse coma 
key feature bayesian networks explicit representation conditional independence events domain variables 
particular variable independent non descendants parents 
property usually referred markov property allows parsimonious representation multivariate joint probability distribution terms univariate conditional distributions variable parents set parameters needed fully characterize conditional probability 
example bayesian network variables discrete set parameters represented means lookup table entry table corresponding conditional probability instantiation variable parents probability complete instantiation computed probabilities belief network 
fact shown joint probability particular instantiation variables belief network calculated follows instantiation variables values fx learning bayesian belief networks bayesian framework ideally classification prediction performed weighted average inferences possible bn containing domain variables approach usually computationally infeasible due large number possible bayesian networks attempt select high scoring bayesian network classification 
assume approach remainder 
basic idea bayesian approach maximize probability bs bs network structure bs database cases network structures term purpose model selection suffices calculate bs bs far bayesian metrics studied detail typically rely assumptions bn structure cases drawn independently distribution random sample assumption cases missing values complete database assumption studies relaxed assumption parameters conditional probability distribution variable independent global parameter independence assumption discrete variables parameters associated instantiation parents variable independent local parameter independence assumption 
assumptions restated formally follows 
theta bs complete set parameters bn structure bs set parameters fully characterize conditional probability 
variables discrete iq ij set parameters defining distribution corresponds th possible instantiations parents assumption follows theta bs bs bs assumption follows bs ij bs 
application assumptions allows factorization probability bs bs bs bs bs term measuring contribution parents score network structure bs exact form terms slightly differs bayesian scoring metrics defined far details refer interested reader relevant literature 
date closed form expressions worked cases discrete variables continuous sets variables normally distributed little done applying bn learning methods domains satisfying constraints 
describe metric discrete case defined cooper herskovits experiments 
bayesian network bs domain number states variable xs number possible instantiations ijk denote multinomial parameter corresponding conditional probability index instantiations ijk ijk 
database ijk number cases database ij ijk number cases database irrespective state assumptions described provided variables discrete probability bs bayesian network structure bs bs bs gamma gamma ij gamma ijk gamma gamma function comprehensive guide literature subject see 
see described interesting applications bayesian model averaging approach 
cooper herskovits defined equation factorials generalization gamma functions straightforward 
scoring metric defined search high scoring network structure carried 
search task forms shown np hard 
various heuristics proposed find network structures high score 
heuristic known implements greedy forward stepping search space network structures 
algorithm assumes ordering variables 
simplicity assumes non informative prior parameters structure 
particular prior probability distribution network structures assumed uniform ignored comparing network structures 
previously stated bayesian scoring metrics developed far assume discrete variables continuous variables normally distributed 
section propose generalization allows inclusion discrete continuous variables arbitrary probability distributions 
ann scoring metric section describe detail artificial neural networks probability distribution estimators definition decomposable scoring metric restrictive assumptions functional form class classes probability distributions participating variables need 
assumptions described previous section needed 
ann estimators allows elimination assumption local parameter independence 
fact conditional probabilities corresponding different instantiations parents variable represented ann share network weights training data 
furthermore anns allows seamless representation probability functions containing continuous discrete variables 
denote fc gamma set gamma cases database instantiations th case respectively 
joint probability bs written bs bs bs bs bs bs bs assume uninformative priors decomposable priors network structures form bs probability parents probability bs decomposable 
fact interchange products equation obtain bs bs term square brackets function parents network structure bs prior neglected assume uniform prior network structures 
derivation illustrated equations corresponds application prequential analysis discussed dawid 
usually decomposition carried log terms interpreted predictive score measure success model predicting data fact shown clearly equation form predictive distribution case cases fx gamma seen 
name prequential analysis suggests sequential prediction 
corresponds theoretically sound form cross validation 
bayesian perspective bs terms computed follows bs bs bs cases integral closed form solution map approximation bs bs posterior mode argmax fp bs approximation maximum likelihood ml estimator posterior mode quantities equivalent assume uniform prior probability asymptotically equivalent choice positive prior 
approximation equation corresponds application plug prequential approach discussed dawid 
artificial neural networks designed estimate discrete continuous case 
schemes available training neural network approximate probability distribution density 
section describe softmax model discrete variables mixture density network model introduced bishop modeling conditional probability densities 
notice adopt ml approximation number terms evaluated calculate bs large mn terms number cases records database number variables cases prohibitively 
computation cost reduced introducing approximation 
ml estimator respect dataset estimating distinct group consecutive cases batches cardinality estimate new addition new batch dataset addition new case 
estimated respect dataset compute terms bs gamma gamma bs 
approximation implicitly assumption belief value new cases needed revise belief 
achieve fold reduction computation needed need estimate original fact application approximation computation yields bs gamma tk tk bs regard choice appropriate value example select constant value choose increment function jd second approach preferable 
estimating estimate sensitive addition new cases small increasingly insensitive addition new cases grows 
example adding new case training data set means doubling additional case data set significant difference 
scheme incremental updating summarized equation dle number cases seen cardinality 
example assuming set data set cases updating scheme le require training ann estimators 
ann probability estimators section describe models representation conditional probability distributions neural networks 
softmax model discrete variables mixture density network model introduced bishop modeling conditional probability densities 
softmax model discrete variables discrete variable values set parents set continuous parents set discrete parents 
conditional probability distribution approximated neural network output units input units gamma 
representation discrete input variable values means gamma indicator variables common practice statistical regression 
output units define conditional probabilities follows fk linear output th output unit corresponding network input notice probability interpreted probability class membership probability belongs th classes 
proved neural network configured sum squares cross entropy error function leads network outputs estimate bayesian posteriori probabilities class membership 
mixture density networks continuous variables approximation probability distributions means finite mixture models established technique widely studied statistics literature 
bishop describes class network models combine conventional neural network finite mixture model obtain general tool representation conditional probability distributions 
probability bs approximated finite mixture normals illustrated equation dropped conditioning bs brevity ff oe number mixture components ff ff kernel functions oe normal density form oe exp ae gamma gamma oe oe normalizing constant oe conditional mean variance respectively 
parameters ff oe considered continuous functions estimated properly configured neural network 
neural network outputs kernel functions mixture model total outputs 
set input units corresponds variables shown gaussian mixture model equation adequate choice approximate arbitrary level accuracy probability distribution 
representation equations completely general allows model arbitrary conditional distributions 
details mixture density network model 
notice mixture density network model assumes number kernel components 
case number needs determined 
determination number components mixture model probably difficult step completely general solution strategy available 
strategies proposed 
techniques computationally expensive mixture models minimizing computational cost selection process paramount importance 
set alternative model orders max consider alternative strategies selection best order model selection test set held training hold model selection bayesian information criterion bic 
model selection hold performed splitting dataset training set train test set test mixture density network mk trained training set train model order maximizes probability test mk selected ml estimator respect train model selection bic aims finding model order maximizes mk 
bic provides asymptotic approximation mk mk gamma log number parameters model mk case number weights neural network size dataset ml estimator parameters mk case outputs fff oe kg trained neural network 
certain regularity conditions error bound equation 
repeating model order selection prequential term estimation computationally costly select model order dataset selected order prequential term 
ann training training anns conjugate gradient optimization algorithm described 
algorithm shows faster convergence possibly local maximum backpropagation 
currently regularization technique control fitting 
number hidden units selected function number inputs 
specifically set number hidden units half number input units minimum hidden units 
ann weights randomly initialized real values interval gamma 
true ann corresponding prequential term scoring metric 
anns corresponding subsequent prequential terms weights initialized weights ann previous prequential term 
specifically term need train anns prequential terms bs ann trained database weights network initialization weights ann trained database updating scheme described section 
strategy particularly beneficial large sized addition new case new cases change significantly estimated probability 
experimental evaluation section describe experimental evaluation conducted test viability scoring metric 
describe experimental design 
results discuss 
experimental design experimental evaluation primarily aimed determining new scoring metric equations applicable continuous variables discrete variables offers advantage scoring metric equation applied discretized data 
considered learning algorithms ffl algorithm performs discretization data searches highest scoring network structure scoring metric equation estimates parameters discovered structure ann estimators applied original continuous data 
ffl algorithm searches highest scoring bn structure ann scoring metric equation applied original continuous data estimates parameters discovered structure means ann estimators 
expect structure search faster structure search possibly accurate due information loss resulting discretization 
discretization technique algorithm simple constant density discretization value range continuous variable partitioned number bins approximately equal number contiguous data points assigned bin 
comparison algorithms simpler experiments designed compare performance algorithms discovering set parents response class variable 
evaluation aimed testing predictive accuracy algorithms capability discovering relevant structural patterns data 
regard goal real data fully appropriate allows better testing robustness assumptions 
regard second goal simulated data generated bayesian networks structure parameterization known appropriate generating bn represents gold standard compare model selected learning procedure 
assess predictive accuracy algorithms measured mean square error mse log score ls respect class variable test set distinct training set 
mean square error computed formula mse test gamma test test set cardinality value th case test value predicted learned bn instantiation parents 
specifically expectation respect conditional probability 
similarly log score ls computed formula ls gamma log test gamma test log conditional probability learned bn 
mse ls lower score better model 
evaluation real databases databases data repository uc irvine 
particular databases auto mpg abalone 
databases selected class variable treated continuous variable 
database auto mpg class variable miles gallon 
database abalone class variable integer proportional age treated continuous variable 
database auto mpg total cases variables variables discrete continuous 
database abalone total cases variables variable discrete 
continuous variables normalized 
interested selecting set parents response variable relevant ordering variables needed search algorithm partial ordering response variable successor variables 
statistics reported computed simulations 
simulation cases randomly selected test set learning algorithms remaining cases training 
notice 
general structure synthetic bns experiments denoting parents indirect ancestors simulation test set algorithms 
evaluation simulated databases designed experiments goal assessing capability scoring metrics correctly identify set parents variable 
purpose synthetic bns generated follows 
considered domain containing continuous variables 
variables randomly selected response variable number variables randomly select gamma fyg parents 
denote set variables remaining variables randomly assigned parents variables denote set variables gamma fyg 
designation variables parents aimed testing effectiveness scoring metrics identifying conditional independencies determining conditional independence 
show prototypical structure bns experiments 
bn structure parameterization terms finite mixtures linear models 
conditional probability modeled finite mixture linear models follows ff oe fi fi jk oe denotes normal distribution mean standard deviation oe 
oe real numbers randomly drawn uniform distribution interval 
regression parameters fi jk real numbers randomly drawn uniform distribution interval jfi jk number mixture components 
choice interval justified fact resulting conditional distributions depart significantly singly peaked curve number mixture components increases 
choose increase magnitude regression parameters number mixture components attempt obtain multimodal shape corresponding conditional probability function 
ff real numbers randomly drawn uniform distribution interval normalized sum 
simulations run different combinations parameter settings 
particular number parents generating synthetic network varied ii number mixture components equation varied note number linear models included mixtures generate database iii number bins discretization 
furthermore algorithm strategy order selection mixture density network model hold bic see section 
chose maximum admissible db order gamma selection auto mpg bic hold abalone bic hold db order mse mse ls ls time ratio selection auto mpg bic hold abalone bic hold comparison algorithms real databases 
table reports statistics structural differences models learned algorithms 
particular number parents response variable discovered algorithms respectively 
number arcs added omitted respect shown 
measure tuple min median max shown 
second table reports mean standard deviation mean square error mse log score ls ratio computational time computational time time ratio 
model order referred max section 
best model order selected range 
ran simulations datasets different cardinality 
particular datasets cardinality 
parameter setting algorithms run times run database cases randomly selected test set remaining database cases training 
ran simulations simulation consists steps ffl synthetic bayesian network bs generated described ffl database cases generated bs markov simulation ffl algorithms applied database relevant statistics algorithms performance collected 
collected statistics algorithms compared means standard statistical tests 
particular continuous statistics mse ls simple test welch modified sample test samples unequal variance 
discrete statistics number arcs added omitted median test wilcoxon test 
results summarize results simulations real synthetic databases respectively 
general guideline discrete measure number arcs added omitted report tuple min median max 
continuous measure log score report mean standard deviation 
regard performance algorithms coupled alternative model selection criteria bic model selection hold model selection criteria significantly superior 
report results corresponding bic model selection 
order gs vs gs vs vs cases selection gamma gamma gamma bic hold bic hold bic hold order mse mse ls ls time ratio cases selection bic hold bic hold bic hold comparison algorithms simulated databases 
table comparison term structural differences discovered networks entry reports tuple min median max 
second table comparison terms predictive accuracy entry reports mean standard deviation quantities mse ls 
reports time ratio ratio computational time computational time 
results comparison learning algorithms real databases 
particular report number parents discovered algorithms corresponding mean square error mse log score ls ratio computational time computational time denoted time ratio 
results comparison learning algorithms simulated databases 
particular table reports number arcs added number arcs omitted algorithm respect gold standard bayesian network gs synthetic bn generate database 
reports number arcs added omitted algorithm respect algorithm 
second table reports measures mse ls algorithms time ratio 
notice statistics shown computed different settings stated previous section experimental design 
statistical analysis results reported shows difference prediction accuracy algorithms statistically significant terms mean square error terms log score 
hand algorithms differ significantly compare structure bns discover 
particular regard real databases algorithm tends select significantly larger number parents response variable algorithm 
regard simulated databases algorithm tends add extra arcs algorithm difference number arcs omitted algorithms statistically significant remember number arcs added omitted computed respect gold standard bns generate databases 
unexpected result decreased prediction accuracy algorithms dataset cases respect prediction accuracy algorithms dataset cases 
fact algorithms performance decreases suggests due anomaly sampling 
point verification testing algorithms larger datasets 
discussion results shown figures support hypothesis discretization continuous variables decrease accuracy recovering structure bns data 
show discretized continuous variables construct bn structure algorithm significantly faster factor ranging untransformed continuous variables algorithm 
predictions accurate accurate predictions 
important aspect differentiating learning methods relative variability results algorithm compared results algorithm especially regard structure learned models 
figures number parents class variable discovered algorithm multiple simulations remains basically constant parents database auto mpg parents database abalone 
true algorithm difference minimum maximum number arcs discovered quite high applied database abalone discovers minimum parents maximum parents 
results suggest estimations ann scoring metric stable probably due tendency ann search get stuck local maxima search space 
method learning hybrid bns defined bns containing continuous discrete variables 
method definition scoring metric artificial neural networks probability estimators 
ann scoring metric allows search space bn structures need discretizing continuous variables 
compared method alternative learning bn structure discretized data 
main purpose test discretization degrade accuracy discovered bn structure parameter estimation accuracy 
experimental results suggest discretization variables permits rapid construction relatively high fidelity bayesian networks compared slower method uses continuous variables 
results course rule possibility develop faster accurate continuous variable learning methods investigated 
results lend support discretization viable method addressing problem learning hybrid bns 
acknowledgments chris bishop moises goldszmidt useful comments preliminary version manuscript 
funded iri national science foundation 
binder koller russel kanazawa 
adaptive probabilistic networks hidden variables 
machine learning 
appear 
bishop 
mixture density networks 
technical report ncrg neural computing research group department computer science aston university birmingham february 
bishop 
neural networks pattern recognition 
oxford university press oxford 
bouckaert 
properties learning algorithms bayesian belief networks 
proceedings th conference uncertainty artificial intelligence pages san francisco california 
morgan kaufmann publishers 
bridle 
probabilistic interpretation feedforward classification network outputs relationships statistical pattern recognition 
neuro computing algorithms architectures applications 
springer verlag new york 
buntine 
guide literature learning probabilistic networks data 
ieee transactions knowledge data engineering 
buntine 
theory refinement bayesian networks 
proceedings th conference uncertainty ai pages 
cheeseman stutz 
bayesian classification autoclass theory results 
fayyad piatetsky shapiro smyth editors advances knowledge discovery data mining 
mit press 
chickering geiger heckerman 
learning bayesian networks search methods experimental results 
proceedings th workshop artificial intelligence statistics pages january 
chickering heckerman 
efficient approximation marginal likelihood incomplete data bayesian network 
proceedings th conference uncertainty ai 
cooper 
nestor computer medical diagnostic integrates causal probabilistic knowledge 
technical report hpp dept computer science stanford university palo alto california 
cooper herskovits 
bayesian method constructing bayesian belief networks databases 
proceedings th conference uncertainty artificial intelligence pages los angeles ca 
cooper herskovits 
bayesian method induction probabilistic networks data 
machine learning 
dawid 
position potential developments personal views 
statistical theory 
prequential approach discussion 
journal royal statistical society 
dawid 
prequential analysis stochastic complexity bayesian inference 
bernardo editor bayesian statistics pages 
oxford university press 
druzdzel van der gaag henrion jensen editors 
building probabilistic networks numbers come ijcai workshop montreal qu ebec 
everitt hand 
finite mixture distributions 
chapman hall 
fayyad uthurusamy editors 
proceedings international conference knowledge discovery data mining kdd montreal qu ebec 
aaai press 
friedman goldszmidt 
discretization continuous attributes learning bayesian networks 
saitta editor proceedings th international conference machine learning pages 
geiger heckerman 
learning gaussian networks 
de poole editors th conference uncertainty ai san francisco california 
morgan kaufmann 
geiger heckerman meek 
asymptotic model selection directed networks hidden variables 
technical report msr tr microsoft research may 
gilks richardson spiegelhalter 
markov chain monte carlo practice 
chapman hall 
heckerman geiger chickering 
learning bayesian networks combination knowledge statistical data 
machine learning 
madigan andersson perlman 
bayesian model averaging model selection markov equivalence classes acyclic digraphs 
communications statistics theory methods 
madigan raftery 
bayesian model averaging 
aaai workshop integrating multiple learned models 
merz murphy 
machine learning repository 
university california irvine department information computer science 
www ics uci edu mlearn mlrepository html 
moller 
scaled conjugate gradient algorithm fast supervised learning 
neural networks 
monti cooper 
learning bayesian belief networks neural network estimators 
mozer jordan petsche editors advances neural information processing systems proceedings conference 
pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufman publishers 
raftery 
bayesian model selection social research discussion 
sociological methodology pages 
raftery 
hypothesis testing model selection 
gilks chapter pages 
richard lippman 
neural network classifiers estimate bayesian posteriori probabilities 
neural computation 
robert 
mixtures distributions inference estimation 
gilks chapter pages 
schwarz 
estimating dimension model 
annals statistics 
shachter 
intelligent probabilistic inference 
lemmer editor uncertainty artificial intelligence pages amsterdam north holland 
spiegelhalter dawid lauritzen cowell 
bayesian analysis expert systems 
statistical science 
thiesson 
accelerated quantification bayesian networks incomplete data 
fayyad uthurusamy pages 
titterington smith makov 
statistical analysis finite mixture distributions 
wiley new york 
