appears proceedings fourteenth national conference artificial intelligence july providence ri new supervised learning algorithm word sense disambiguation ted pedersen rebecca bruce department computer science engineering southern methodist university dallas tx seas smu edu naive mix new supervised learning algorithm sequential method selecting probabilistic models 
usual objective model selection find single model adequately characterizes data training sample 
model selection sequence models generated consists best fitting model level model complexity 
naive mix utilizes sequence models define probabilistic model probabilistic classifier perform word sense disambiguation 
models sequence restricted class decomposable log linear models 
class models offers number computational advantages 
experiments disambiguating twelve different words show naive mix formulated forward sequential search akaike information criteria rivals established supervised learning algorithms decision trees rule induction cn nearest neighbor classification pebls 
word sense disambiguation cast problem supervised learning probabilistic classifier induced corpus sense tagged text 
suppose training sample sense tagged sentence represented feature variables fn gamma 
sense ambiguous word represented fn gamma represents selected contextual features sentence 
goal construct classifier predict value untagged sentence represented contextual feature variables 
perform systematic model search probabilistic model selected describes interactions feature variables 
model characterizes training sample determined measuring fit model sample distribution defined model matches distribution observed training sample 
copyright fl american association artificial intelligence www aaai org 
rights reserved 
model form basis probabilistic classifier specifies probability observing combinations values feature variables 
model selected models evaluated discarded 
naive mix combines models best fitting model improve classification accuracy 
suppose training sample sense tagged sentences 
possible combinations values feature variables combination represented feature vector 
frequency probability observing th feature vector respectively 
multinomial distribution parameters 
parameters define joint probability distribution feature variables 
parameters fully saturated model model value variable stochastically dependent values variables 
parameters estimated maximum likelihood methods estimate estimates reliable possible combinations feature values occur training sample 
nlp data sparse highly skewed 
zipf pedersen bruce 
training sample adequately characterized complex model fewer interactions features reliable parameter estimates obtained 
restrict search class decomposable models darroch lauritzen speed reduces model search space simplifies parameter estimation 
short introductions decomposable models model selection 
naive mix discussed followed description sense tagged text experiments 
experimental results summarized compare naive mix range supervised learning approaches 
close discussion related 
decomposable models decomposable models subset class graphical models whittaker turn subset class log linear models bishop fienberg holland 
far fewer decomposable models log linear models set feature variables classes substantially expressive power whittaker 
graphical model variables interdependent conditionally independent 
graphical models graphical representation variable model mapped node graph undirected edge pair nodes corresponding interdependent variables 
sets completely connected nodes cliques correspond sets interdependent variables 
nodes directly connected edge conditionally independent values nodes path connects 
decomposable models graphical models express joint distribution product marginal distributions variables maximal cliques graphical representation scaled marginal distributions variables common maximal sets 
example parameter estimate probability feature vector observed training sample observation represented feature variables specific values suppose graphical representation decomposable model defined cliques marginals 
frequencies marginals sufficient statistics provide information calculate maximum likelihood estimates model parameters 
model parameters simply marginal frequencies normalized sample size joint parameter estimates formulated model parameter estimates follows theta necessary observe marginals estimate parameter 
joint distributions closedform expressions parameters estimated directly training sample need iterative fitting procedure required example estimate parameters maximum entropy models berger della pietra della pietra 
conditionally independent jf js 
model selection model selection integrates search strategy evaluation criterion 
search strategy determines decomposable models set possible decomposable models evaluated selection process 
backward sequential search bss forward sequential search fss 
sequential searches evaluate models increasing fss decreasing bss levels complexity complexity defined number edges graphical representation model 
evaluation criterion judges model characterizes data training sample 
akaike information criteria aic akaike evaluation criterion results extensive comparison search strategies selection criteria model selection reported pedersen bruce wiebe 
search strategy bss begins designating saturated model current model 
saturated model complexity level gamma number feature variables 
stage bss generate set decomposable models complexity level gamma created removing edge current model complexity level member set hypothesized model judged evaluation criterion determine model results degradation fit current model model current model search continues 
stage selection procedure current model best fitting model complexity level search stops hypothesized model results unacceptably high degradation fit current model complexity level zero 
fss begins designating model independence current model 
model independence complexity level zero interactions feature variables 
stage fss generate set decomposable models complexity level created adding edge current model complexity level member set hypothesized model judged evaluation criterion determine model results greatest improvement fit current model model current model search continues 
search stops hypothesized model results unacceptably small increase fit current model saturated 
sparse samples fss natural choice early search models low complexity 
number model parameters small reliably estimated training data 
hand bss begins saturated model parameter estimates known unreliable 
bss fss model selection performs feature selection 
model selected edge connecting feature variable classification variable feature relevant classification performed removed model 
evaluation criteria akaike information criteria aic alternative pre defined significance level judge acceptability model 
aic rewards model fit penalizes models large numbers parameters definition aic gamma theta dof model fit measured log likelihood ratio statistic parameter penalty expressed theta dof dof adjusted degrees freedom model evaluated 
adjusted dof equal number model parameters estimated training sample 
log likelihood ratio statistic defined theta theta log observed expected counts th feature vector respectively 
observed count simply frequency training sample 
expected count count distribution defined model 
smaller value better fit hypothesized model 
bss hypothesized model largest negative aic value selected current model best fitting model complexity level gamma fss hypothesized model largest positive aic value selected current model complexity level 
fit hypothesized models judged unacceptable aic values models greater zero case bss zero case fss 
naive mix naive mix premise best fitting model level complexity sequential search important information exploited word sense disambiguation 
naive mix probabilistic classifier average distributions defined best fitting models complexity level 
sequential model selection results sequence decomposable models gamma initial model final model selected 
model designated current model th stage model selection 
fss model independence feature variables independent edges graphical representation model 
bss saturated model variables completely dependent edges connect node graphical representation model 
naive mix formulated average joint probability distributions defined model sequence gamma generated model selection gamma average gamma gamma represents joint parameter estimates formulated parameters decomposable model averaged joint distribution defined average joint parameters basis probabilistic classifier 
suppose wish classify feature vector having values gamma unknown sense represented variable feature vector fn gamma represents values observed contextual features 
takes sense value highest probability occurring observed contextual features defined parameter estimates argmax gamma average gamma prefer fss bss formulating naive mix 
fss incrementally builds strongest interactions bss incrementally eliminates weakest interactions 
result intermediate models generated bss may contain irrelevant interactions 
experimental data sense tagged text experiments described bruce wiebe pedersen consists sentence acl dci wall street journal corpus contains nouns interest bill concern drug verbs close help agree include adjectives chief public common 
extracted sentences manually tagged senses defined longman dictionary contemporary english ldoce 
number possible senses word number sense tagged training sentences held test sentences word shown 
sentence ambiguous word represented feature set types contextual feature variables morphological feature describing ambiguous word part speech pos features describing surrounding words collocation features 
morphological feature binary nouns indicating noun plural 
verbs indicates tense verb 
feature adjectives 
pos feature variables possible pos tags 
agree bill auction discount treasury chief economist executive officer close cents trading common sense share concern drug fda generic help include interest percent rate month week year public going offering school collocation specific variables tags derived letter tags acl dci wsj corpus 
pos feature variables representing pos words immediately preceding ambiguous word 
binary collocation specific feature variables indicate particular word occurs sentence ambiguous word 
collocations shown 
selected words occurred frequently sentences containing ambiguous word 
words chosen indicative sense ambiguous word test independence 
experimental results success learning algorithm applied particular problem depends appropriate assumptions formulating algorithm data problem 
assumptions implicit formulation learning algorithm result bias preference generalized representation training sample 
experiments different methods disambiguate ambiguous words 
briefly describe algorithm 
majority classifier performance probabilistic classifier worse majority classifier assigns ambiguous word frequently occurring sense training sample 
naive bayes classifier duda hart probabilistic classifier model features fn gamma conditionally independent value classification variable sjf fn gamma gamma js classifier accurate model conditional independence fits data 
pebls cost salzberg nearest neighbor algorithm classification performed assigning test instance majority class closest training examples 
experiments test instance assigned tag single similar training instance features weighted equally 
parameter settings pebls standard nearest neighbor classifier appropriate data features relevant equally important classification 
quinlan decision tree algorithm classification rules formulated recursively partitioning training sample 
nested partition feature value provides greatest increase information gain ratio current partition 
final partitions correspond set classification rules antecedent rule conjunction feature values form corresponding partition 
method biased production simple trees trees fewest partitions classification smallest number feature values 
cn clark niblett rule induction algorithm selects rules cover largest possible subsets training sample measured laplace error estimate 
method biased selection simple rules cover training instances possible 
fss bss aic probabilistic classifier single best fitting model selected fss bss aic evaluation criterion 
procedures biased selection models smallest number interactions 
fss bss aic naive mix probabilistic classifier averaged joint probability distribution sequence models gamma generated fss aic bss aic sequential search 
model generated fss aic formulated potentially extending feature set previous model gamma model generated bss aic formulated potentially decreasing feature set previous model gamma methods biased classification preferences informative features included largest number models sequence 
reports accuracy method applied disambiguation words 
highest accuracy achieved word bold face 
bottom table average accuracy method stated summary comparison performance method fss aic naive mix 
row designated win tie loss states number words accuracy fss aic naive mix greater win equal tie loss method column 
fss aic naive mix naive bayes highest average accuracy 
difference accurate accurate pebls percent 
word word comparison achieves highest ac fss aic bss aic word train majority naive fss naive bss naive senses test classifier bayes pebls cn aic mix aic mix agree bill chief close common concern drug help include interest public average win tie loss disambiguation accuracy methods 
fss aic fares poorly accurate methods 
win tie loss summary shows fss aic naive mix compares favorably pebls fss aic fares bss aic naive mix 
high number losses relative bss aic naive mix interesting contrast lower average accuracy word word performance method 
highlights competitive performance bss aic naive mix data set 
fss aic naive mix fss aic cn perform general specific search adds features representation training sample measure information content increase 
methods perform feature selection bias simpler models 
true bss aic bss aic naive mix perform specific general search simplest model 
methods suffer fragmentation sparse data 
fragmentation occurs rules model complex incorporating large number feature values describe small number training instances 
occurs inadequate support training data inference specified model rule 
fss aic naive mix designed reduce effects fragmentation general specific search averaging distributions high complexity models low complexity models include relevant features 
nearest neighbor approaches pebls suited making classifications require full feature set long features independent relevant 
naive bayes classifier pebls perform search create representation training sample 
naive bayes specifies form model features classification pebls interdependencies considered 
weights assigned features parameter estimates training sample 
weights allow discounting relevant features 
implemented pebls stores instances training sample treats feature independently equally making susceptible misclassification due irrelevant features 
shown bruce wiebe pedersen features experiments indicators classification variable equally 
lower accuracy pebls relative naive bayes indicates weighting appropriate 
related sequential model selection decomposable models applied word sense disambiguation bruce wiebe 
naive mix extends considering entire sequence models just best fitting model 
comparative studies machine learning algorithms applied word sense disambiguation relatively rare 
leacock towell voorhees compares neural network naive bayes classifier content vector disambiguating senses line 
report methods equally accurate 
mooney utilizes data applies wider range approaches comparing naive bayes classifier perceptron decision tree nearest neighbor classifier logic disjunctive normal form learner logic conjunctive normal form learner decision list learner 
finds naive bayes classifier perceptron accurate approaches 
feature set studies line data different 
binary features represent occurrence words approximately word window ambiguous word resulting nearly binary features 
surprising simple model naive bayes provide manageable representation large feature set 
pebls applied word sense disambiguation ng lee 
sense tagged text interest draw comparisons pebls probabilistic classifier best fitting single model model search bruce wiebe 
find combination pebls broader set features leads significant improvements accuracy 
recognition uncertainty model selection trend model selection research away selection single model madigan raftery naive mix reflects trend 
similar trend exists machine learning supposition learning algorithm superior tasks 
supposition lead hybrid approaches combine various methods domingos approaches select appropriate learning algorithm characteristics training data brodley 
naive mix extends existing statistical model selection advantage intermediate models discovered selection process 
features selected systematic model search appropriately weighted averaged parameter estimates 
experimental evidence suggests naive mix results probabilistic model usually accurate classifier single model selected sequential search 
proves competitive diverse set supervised learning algorithms decision trees rule induction nearest neighbor classification 
acknowledgments research supported office naval research number 
akaike 
new look statistical model identification 
ieee transactions automatic control ac 
berger della pietra della pietra 
maximum entropy approach natural language processing 
computational linguistics 
bishop fienberg holland 
discrete multivariate analysis 
cambridge ma mit press 
brodley 
recursive automatic bias selection classifier construction 
machine learning 
bruce wiebe 
word sense disambiguation decomposable models 
proceedings nd annual meeting association computational linguistics 
bruce wiebe pedersen 
measure model 
proceedings conference empirical methods natural language processing 
clark niblett 
cn induction algorithm 
machine learning 
cost salzberg 
weighted nearest neighbor algorithm learning symbolic features 
machine learning 
darroch lauritzen speed 
markov fields log linear interaction models contingency tables 
annals statistics 
domingos 
unifying instance rule induction 
machine learning 
duda hart 
pattern classification scene analysis 
new york ny wiley 
leacock towell voorhees 
corpus statistical sense resolution 
proceedings arpa workshop human language technology 
madigan raftery 
model selection accounting model uncertainty graphical models occam window 
journal american statistical association 
mooney 
comparative experiments senses illustration role bias machine learning 
proceedings conference empirical methods natural language processing 
ng lee 
integrating multiple knowledge sources disambiguate word sense exemplar approach 
proceedings th annual meeting society computational linguistics 
pedersen bruce wiebe 
sequential model selection word sense disambiguation 
proceedings fifth conference applied natural language processing 
pedersen bruce 
significant lexical relationships 
proceedings thirteenth national conference artificial intelligence 
quinlan 
programs machine learning 
san mateo ca morgan kaufmann 
whittaker 
graphical models applied multivariate statistics 
new york john wiley 
zipf 
psycho biology language 
boston ma houghton mifflin 
