nexus task parallel runtime system foster kesselman tuecke tr november center research parallel computation rice university south main street ms houston tx nexus task parallel runtime system ian foster carl kesselman steven tuecke math computer science beckman institute math computer science argonne national laboratory caltech argonne national laboratory argonne il pasadena ca argonne il foster mcs anl gov carl caltech edu tuecke mcs anl gov runtime system provides parallel language compiler interface low level facilities required support interaction concurrently executing program components 
nexus portable runtime system task parallel programming languages 
distinguishing features nexus include support multiple threads control dynamic processor acquisition dynamic address space creation global memory model interprocessor asynchronous events 
addition supports heterogeneity multiple levels allowing single computation utilize different programming languages executables processors network protocols 
nexus currently compiler target task parallel languages fortran compositional nexus design outline techniques implement nexus parallel computers show compilers compare performance runtime system 
compilers parallel languages rely existence runtime system 
runtime system defines compiler view parallel computer computational resources allocated controlled parallel components program interact communicate synchronize 
existing runtime systems support multiple data spmd programming model supported office scientific computing department energy contract eng national science foundation center research parallel computation contract ccr 
implement data parallel languages 
model processor parallel computer executes copy program 
processors exchange data synchronize calls runtime library typically designed optimize collective operations processors communicate time structured fashion 
major research goal area identify common runtime systems shared variety spmd systems 
task parallel computations extend spmd programming paradigm allowing unrelated activities take place concurrently 
need task parallelism arises time dependent problems discrete event simulation irregular problems sparse matrix problems multidisciplinary simulations coupling multiple possibly data parallel computations 
task parallel programs may dynamically create multiple potentially unrelated threads control 
communication synchronization threads processors occur asynchronously subset threads point time 
compiler little global information task parallel computation opportunities exploiting optimized collective operations 
design nexus shaped requirements task parallel computations desire support heterogeneous environments heterogeneous collections computers may connected heterogeneous networks 
design goals include efficiency portability diverse systems support interoperability different compilers 
clear extent various goals satisfied single runtime system particular need efficiency may conflict need portability heterogeneity 
preliminary performance results address question 
describe nexus compiler target task parallel languages fortran fm compositional cc 
initial experiences resulting compilers considerably simpler earlier prototypes nexus services 
space permit detailed discussion related 
note system similar design goals adopts different solutions 
nexus design implementation describing nexus interface implementation review requirements assumptions motivated nexus design 
nexus intended general purpose runtime system task parallel languages 
currently contains specialized support data parallelism data parallel languages pc hpf principle runtime layer 
nexus designed specifically compiler target library application programmers 
consequently design favors efficiency ease 
believe parallel computing lies heterogeneous environments diverse networks communications protocols interconnect pcs workstations small shared memory machines large scale parallel computers 
expect heterogeneous applications combining different programming languages programming paradigms algorithms widespread 
nexus abstractions need close hardware order provide efficiency machines provide appropriate low level support 
operations occur frequently task parallel computations thread creation thread scheduling communication need particularly fast 
time nexus abstractions easily layered top existing runtime mechanisms provide portability machines support nexus abstractions directly 
communication mechanisms considered designing nexus include message passing shared memory distributed shared memory message driven computation 
nexus intended lingua franca compilers promoting reuse code compiler implementation interoperability code generated different compilers 
important issues purposefully addressed initial design include reliability fault tolerance real time issues global resource allocation replication data code migration scheduling policies 
expect examine issues research 
core abstractions nexus interface organized basic abstractions nodes contexts threads global pointers remote service requests 
associated services provide direct support light weight threading address space management communication synchronization 
computation consists set threads executing address space called context 
individual thread executes sequential program may read write data shared threads executing context 
generate asynchronous remote service requests invoke procedures contexts 
nodes 
basic abstraction nexus node 
node represents physical processing resource 
consequently set nodes allocated program determines total processing power available computation 
program nexus starts initial set nodes created nodes added released dynamically 
programs execute directly node 
discuss computation takes place context context mapped node 
nexus provides set routines create nodes named computational resources symmetric shared memory multiprocessor processor distributed memory computer 
node specifies computational resource imply specific communication medium protocol 
naming strategy implementation dependent node manipulated manner created 
contexts 
computation takes place object called context 
context relates executable code data segments node 
contexts mapped single node 
contexts migrated nodes created 
contexts created destroyed dynamically 
anticipate context creation occurring frequently instructions 
consequently context creation inexpensive certainly expensive process creation unix 
feasible unix processes contexts guarantee protection 
note behavior concurrent operations contexts currently undefined 
compiler defined initialization code executed automatically nexus context created 
initialization complete context inactive thread created explicit remote service request context 
creation operation synchronized ensure context completely initialized 
separation context creation code execution unique nexus direct consequence requirements task parallelism 
threads control context equivalent computation created asynchronously 
threads 
computation takes place threads control 
thread control created context 
nexus distinguishes types thread creation context currently executing thread different context currently executing thread 
discuss thread creation contexts 
nexus provides routine creating threads context currently executing thread 
number threads created context limited resources available 
thread routines nexus modeled subset posix thread specification 
operations supported include thread creation termination yielding current thread 
mutexes condition variables provided synchronization threads context 
basing nexus posix threads pragmatic choice vendors support posix threads similar allows nexus implemented vendor supplied thread libraries 
drawback approach posix designed application program interface features real time scheduling support may add overhead parallel systems 
lower level interface designed specifically compiler target result better performance investigated research 
summarize mapping computation physical processors determined mapping threads contexts mapping contexts nodes 
relationship nodes contexts threads illustrated fig 

global pointers 
nexus provides compiler global namespace allowing global name created address context 
context context context nodes contexts threads name called global pointer 
global pointer moved contexts providing movable 
global pointers conjunction remote service requests cause actions take place different context 
global pointers motivated considerations 
ffl data parallel programming model naturally associates communication section code generates consumes data programs need associate communication specific data structure specific piece code 
global namespace facilitates 
ffl dynamic behaviors rule task parallel computation 
data structures need passed contexts 
ffl data structures arrays need supported 
general global pointer mechanism facilitates construction complex distributed data structures 
ffl distributed memory computers provide direct hardware support global shared namespace 
wanted reflect trend nexus 
global pointers implement data structures pointers 
example fm compiler uses implement channels 
remote service requests 
thread request action performed remote context issuing remote service request 
remote service request results execution special function called handler context pointed global pointer 
handler invoked asynchronously context action executing receive needs take place context order handler execute 
remote service request remote procedure call return value call thread initiated request block 
remote service requests similar respects active messages 
differ significant ways 
active message handlers designed execute interrupt handler restrictions ways modify environment node 
example call memory allocation routines 
restrictions hinder active messages data transfer limit utility mechanism creating general threads control 
contrast remote service requests expensive restrictive 
particular create threads control handlers execute concurrently 
remote service request data transferred contexts buffer 
data inserted buffer removed buffer packing unpacking functions similar pvm mpi 
invoking remote service request step process 
remote service request initialized providing global pointer address destination context identifier handler remote context 
buffer returned initialization operation 

data passed remote handler placed buffer 
buffer uses global pointer provided initialization determine data conversion encoding required 

remote service request performed 
performing request nexus uses global pointer provided initialization determine communication protocols communicate node context resides 
handler invoked destination context local address component global pointer message buffer arguments 
general form remote service request handler runs new thread 
compiler specify handler execute preallocated thread knows handler terminate suspending 
avoids need allocate new thread addition parallel computer system allows handlers read directly message interface avoids copying intermediate buffer necessary thread safe execution 
example handler implements get put operations split take advantage optimization 
implementation order support heterogeneity nexus implementation encapsulates thread communication functions thread protocol modules respectively implement standard interface lowlevel mechanisms fig 

current thread modules include posix threads dce threads threads solaris threads 
current protocol modules include local communication tcp socket intel nx message passing 
protocol modules mpi pvm svr shared memory fiber channel ibm message passing library aal atm adaptation layer asynchronous transfer mode atm cray get put operations planned development 
communication mechanism single program 
example context communicate contexts different communication mechanisms located different nodes 
functionality supported follows 
protocol module initialized creates table containing functions implement low level interface small descriptor specifies protocol 
protocol descriptors small objects typically words depending protocol 
global pointer created context list descriptors protocols supported context attached global pointer 
protocol descriptor list part global pointer passed global pointer transferred contexts 
recipient global pointer compare protocol list local protocols determine best protocol communicating global pointer 
existing message passing systems support limited network heterogeneity generality 
example pvm allows processors parallel computer communicate external computers sending messages daemon process acts message forwarder 
approach optimal machines ibm sp intel paragon nodes able support tcp directly limits pvm programs just protocol addition tcp 
special implementations version paragon network protocol network protocol thread library system services protocol module protocol module nexus thread module nexus services nexus protocol module interface structure nexus implementation allows nodes nx tcp 
allow arbitrary mixing protocols 
performance studies section results preliminary nexus performance studies 
note thrust development effort date provide correct implementation nexus 
tuning optimization done 
addition operating system features implement nexus completely generic exploited simplest operating system features nonblocking consequently results reported viewed suggestive nexus performance way conclusive 
experiments describe designed show cost nexus communication abstraction compared traditional send receive 
nexus style communication supported current machines nexus implemented send receive 
nexus operations overhead compared send receive 
objective quantify overhead 
note support nexus build directly system software machine case nexus performance meet exceed performance traditional process oriented send receive system 
started development effort ibm watson research center explore possibility 
experiments reported compare performance cc program compiled nexus similar program pvm communication 
cc program uses function call cc global pointer transfer array double precision floating point numbers processor objects nexus contexts 
measure cost remote thread creation preallocated thread execute remote service request 
pvm program uses send receive transfer array 
systems compiled sun compilers performs data conversion 
cases data starts finishes user defined array 
array circulated endpoints repeatedly accumulated execution time sufficient measure accurately 
execution time measured range array sizes 
results experiments summarized fig 

see despite lack optimization nexus competitive pvm 
execution times consistently lower cent remote service requests executed preallocated thread indicates latency word transfer costs lower 
surprisingly execution times higher thread created dynamically cent small messages cent larger messages 
summary nexus runtime system compilers programming languages 
provides integrated treatment multithreading address space management communication synchronization supports heterogeneity architectures communication protocols 
nexus operational networks unix workstations communicating tcp ip networks ibm sp intel paragon nx ported platforms communication protocols 
nexus implement different task parallel programming languages cc fortran cases experience basic abstractions positive transfer time message length double floats nexus remote thread nexus remote thread pvm round trip time function message size sun workstations solaris unloaded ethernet 
complexity compilers reduced considerably compared earlier prototypes nexus facilities 
addition able reuse code laid foundations interoperability compilers 
preliminary performance studies reported suggest nexus facilities competitive runtime systems 
peter buhr 
system providing light weight concurrency sharedmemory multiprocessor systems running unix 
software practice experience pages september 
butler lusk 
monitors message clusters parallel programming system 
parallel computing appear 
mani chandy carl kesselman 
cc declarative concurrent object oriented programming notation 
research directions object oriented programming 
mit press 
david culler parallel programming 
proc 
supercomputing 
acm 
dongarra geist manchek sunderam 
integrated pvm framework supports heterogeneous network computing 
computers physics april 
message passing interface forum 
document standard passing interface march 
available netlib 
ian foster mani chandy 
fortran language modular parallel programming 
parallel distributed computing 
appear 
ian foster carl kesselman robert olson steve tuecke 
nexus interoperability toolkit parallel distributed computer systems 
technical report anl mcs tm argonne national laboratory 
haines mehrotra 
design talking threads package 
technical report icase 
ieee 
threads extension portable operating systems draft february 
thorsten von eicken david culler seth copen goldstein klaus erik schauser 
active messages mechanism integrated communication computation 
proc 
th int symposium computer architecture may 
