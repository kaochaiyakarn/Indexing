heuristic variable grid solution method pomdps ronen brafman department computer science university british columbia vancouver canada brafman cs ubc ca www cs ubc ca spider brafman partially observable markov decision processes pomdps appealing tool modeling planning problems uncertainty 
incorporate stochastic action sensor descriptions easily capture goal oriented process oriented tasks 
unfortunately pomdps difficult solve 
exact methods handle problems states approximate methods 
describe simple variable grid solution method yields results relatively large problems modest computational effort 
markov decision processes mdps bellman provide mathematically elegant model planning problems actions stochastic effects tasks process oriented complex graded notion goal state 
partially observable mdps pomdps enhance model allowing noisy imperfect sensing 
unfortunately solving pomdps obtaining optimal prescription action choice information state computationally daunting task feasible extremely small domains 
example witness algorithm kaelbling littman cassandra best exact solution algorithms handle problems states realistic alternative larger domains littman cassandra kaelbling 
consequently attempts come methods obtaining approximately optimal policies lovejoy littman cassandra kaelbling parr russell 
hoped combination approximation algorithms structured problems clever encodings lead acceptable solutions realistic pomdps 
describe variable grid algorithm obtaining approximate solutions pomdps infinite horizon case shows promise 
known sondik optimal policy pomdp states obtained solving belief space mdp state space consists probability distributions state space pomdp dimensional simplex 
grid approximations attempt copyright fl american association artificial intelligence www aaai org 
rights reserved 
solve belief space mdp directly placing discrete finite grid dimensional space restricting calculations points grid 
computational effort required approximating value function point grid roughly equivalent solving state mdp 
fixed grid approximations lovejoy construct grid size state space 
problem independent 
grid construction simple fast regularity grid facilitates simple value interpolation algorithms 
variable grid methods problem dependent grid may change solution process 
allow concentrate effort needed obtain approximations smaller grid 
interpolation harder perform variable grid construction methods cheng quite complex requiring considerable computation time space 
describe simple variable grid method performs number test problems littman cassandra kaelbling 
experience indicates method requires modest computational effort believe realistic alternative solving large pomdps 
approach motivated hypothesis variable grids lead considerable efficiency allowing gain information needed minimizing grid size 
rough solutions provide heuristic information construct better grids suggesting iterative approach 
addition observe computationally feasible variable grid methods call simple grid construction method 
roughly algorithm operates follows step underlying mdp generate initial grid initial estimate value function 
step current estimate generate new grid points 
step new grid estimate value function 
step desirable feasible go back step 
final estimate value function generate policy 
experiments state pomdp described littman cassandra kaelbling able obtain results point grid 
contrast coarsest grid required fixed grid method lovejoy contains points 
organized follows section discusses related grid methods section describes algo rithm section presents experimental results section concludes 
assume familiarity mdps pomdps 
introductions consult kaelbling littman cassandra lovejoy 
grid methods state pomdp corresponding belief space mdp uncountably large state space consisting dimensional simplex 
solved standard mdp solution algorithms 
grid solution methods attempt approximate value function entire state space estimating finite number belief states contained chosen grid 
main dimensions grid methods differ grid points generated value function gradient estimated grid value estimates grid points generalized space 
follows discuss approximation methods closely related approach noting addresses questions 
complete description approximation methods appears full see lovejoy white 
qmdp method littman cassandra kaelbling viewed grid method grid consists states pomdp 
underlying mdp solved value functions estimates grid points correspond states underlying mdp 
non grid belief state action estimates inner vectors vector values states underlying mdp grid points action method performs relatively number navigation domains introduced 
causes agent act state perfect information belief state corresponding underlying mdp states 
consequently agent executes action sole effect obtain information 
method relax assumption allowing agent act belief states part grid 
known optimal value function mdp fixed point dynamic programming update operator defined hv max delta value function set actions set states reward function transition function 
repeated applications random initial value function approach cheng iterative discretization procedure idp cheng solves pomdps approximate version linear support algorithm exact algorithm performing dynamic programming updates 
linear support algorithm creates set polygonal regions exists single action value maximal belief states region 
idp works follows value function vn apply operator obtained terminate linear support algorithm prior completion obtaining new estimate calculation grid repeating number steps option practical 
cheng applies discrete approximation step value updated set points space 
grid points suggests set vertices regions created previous step applied variable grid 
set gradients value function generated grid points value function belief state defined max delta cheng algorithm alternates step discrete approximation step 
error estimate obtained requires computational effort 
lovejoy develops fixed grid method calculates upper lower bounds optimal value function 
bounds get approximately optimal policy error estimate 
method geared finite horizon problems extension infinite horizon case considered 
lovejoy insist particular grid generating lower bound 
grid standard method generates gradients value function grid points 
hinted set gradients value function set points approximate value function 
estimate grid points lower bound value function obtained lower bound entire space 
upper bound lovejoy uses triangulation construct particular grid partitions belief space simplex order jsj sub simplices 
dynamic programming updates value function estimated grid points 
lovejoy choice grid allows elegant efficient interpolation method estimates value arbitrary belief states value grid points smallest sub simplex containing state 
value function continuous convex obtain upper bound optimal value function method assuming estimates grid points upper bounds 
lovejoy reports results pomdps approximately states 
method iterative uses discrete approximation steps cheng idp simpler ones 
updates value function grid points dynamic programming updates lovejoy method obtaining upper bounds 
fact method yields upper bound optimal value function 
variable grid simpler probably weaker interpolation method generalize estimates grid 
generating approximating mdps method works generating sequence grid approximate value function belief space mdp 
function generated assigning values number belief states grid points interpolated obtain assignment points 
naturally interpolated values stored explicitly calculated needed 
function obtained choosing states underlying mdp grid points 
function generated adding new points grid recalculating values interpolating states 
decision add particular belief state grid online importance obtaining better estimate value assessed current value function induced policy likelihood reaching state 
states added informed gradual manner 
hoped approach shall obtain progressively better estimates adding states 
basic process follow similar cheng idp generate grid 
grid implicitly generate value function 
depending resource constraints time memory needs current value function generate new finer grid repeating process 
implicitly defined value function generate policy needed 
initial grid generated contains perfect information belief states correspond states underlying mdp 
value estimated value corresponding states underlying mdp 
estimate expanded space interpolation method stage generates value qmdp generates 
initial estimate identical obtained qmdp order complete method description explain method add new grid points interpolation method 
start question 
current value function wish generate new finer grid 
add new belief states current grid examining combinations existing belief states usually pairs considering convex hull useful 
considerations different policy region opposed boundary region reach states region 
illustrate point consider arbitrary points belief space suppose moment considering step horizon 
states optimal action optimal action points segment connecting best action best action jq gamma jq gamma small close optimal cases little motivation explore region farther 
hand differences negligible attention paid region 
argument generally valid consider horizon compare stage decision trees single actions consider infinite horizon problems compare infinite stage decision trees 
provides useful heuristics 
second consideration employ reachability 
value differences negligible may case reach belief state full fledged reachability analysis computationally costly may generate vast number states see section results line 
simpler local method conclude reach state messages receive states different 
current implementation uses ideas follows 
pair grid states value differences discussed calculated current value function 
differences cross threshold value consider adding mixture state delta delta assess likelihood reaching examining probability obtaining message likelihood greater threshold add set grid points 
naturally examining pairs states examine triplets identical considerations 
experiments reported done 
having generated new grid generate new estimate value function 
iterative process similar standard value iteration algorithm bellman 
value function updated grid points standard dynamic programming update mdp solution method 
update process repeated value function grid points stabilizes difference value functions obtained consecutive iterations smaller fixed parameter 
typically update operation require estimating value function non grid points 
addition value grid points set able generate value function belief space wish able assign actions arbitrary belief states 
considered methods assigning values non grid points 
method value nearest neighbor th norm distance belief space 
method poorly detailed results performance see section limited application nearest neighbor method 
second method interpolation method belief state represented linear combination grid states positive coefficients 
value function grid points upper bound optimal value function case approach method generates upper bound optimal value function states 
ideally grid points closest perform interpolation yield best approximation 
simple observation follows fact estimates upper bounds optimal value function convex 
grid fixed lovejoy just 
come reasonable alternative linear combinations containing grid states agent information preferred informative states 
states closer typically consider action performed discussion extends naturally case 
fact experiments reported added message 
target state obtain preference grid points ordered amount information encode 
states complete information come followed states information defined entropy 
belief state list grid points searched backwards belief state grid assigns positive probability states assigns positive probability 
maximum coefficient gamma delta calculated process continues gamma delta method perfect computationally simple ensures states close interpolation 
nearest neighbor approach value obtained takes account positive probability states 
notice prefer states information search grid points forward approximation methods qmdp final value function policy belief state generated follows value action estimated interpolation method values grid states 
action value best chosen 
variants methods discussed 
experimental results assessing suitability techniques generating policies pomdps simple task 
analytical computational methods generating optimal solutions pomdps states compare generated policies optimal policy 
option supply error estimate possible emulating lovejoy method generating lower bound 
want assess quality current heuristic method 
suspect problems examine complex useful solution methods nature error bounds lax require considerable computational effort cheng idp seeking uniform bounds lovejoy method 
shall resort simulations means assessing quality proposed solution method 
approach runs difficulties little experience large state spaces results compare 
attempts experiment large pomdps aware littman cassandra kaelbling cassandra kaelbling kurien examine performance method domains 
describes largest test problem examined littman cassandra kaelbling fact largest test problem seen 
problem robot reach goal position starting random initial state 
position robot may possible orientations take actions stay inplace move forward move right move left turn 
effects actions stochastic 
action robot receives information configuration walls 
sensors noisy state maze may erroneously indicate wall exist vice versa 
precise description set including transition probabilities message littman cassandra kaelbling 
initial belief state robot uniform possible robot configurations position orientation pairs outside goal 
reaching goal robot receives reward 
discount factor 
littman cassandra kaelbling best results domain obtained qmdp method stay place action suppressed 
equally results obtained learning methods consider 
conducted experiments robot maximum steps reach goal state 
recorded percentage experiments goal reached median length experiment 
interestingly domain little training human performance better 
cassandra kaelbling kurien number additional methods solving navigation pomdps proposed 
particular authors recommend method agent performs action assigned underlying mdp state mls 
note method instance nearest neighbor method discuss 
results obtained method grid sizes interpolation method estimate values actions belief state 
grid generated underlying mdp 
case method equivalent qmdp method 
second grid contains states obtained adding belief states described earlier 
added pairs states message assigned different actions underlying mdp values differed pair preferred actions see earlier discussion 
grid contained states obtained removing restriction values allowing combinations states 
results described table 
standard refers interpolation method discussed earlier standard stay refers standard method consider stay place action 
methods see significant improvement state space increases 
particular modest increases obtain results substantially outperform qmdp method better mls method discussed 
size estimation method success median human qmdp standard standard standard qmdp stay standard stay standard stay table results state maze interpolation method state maze negative rewards standard method stay place may 
fact viewed crude implementation general heuristic overcomes inherent problem interpolation method loops 
interpolation method uses values different grid points estimate values particular belief state 
mdp solution algorithms relative size values important 
approximation algorithms actual value interpolation 
approximates values need interpolation value mutated policy obtained current policy assigned notice op belief state stay place certain belief states value close 
close value original policy values somewhat misleading 
particular case different motion actions states bad 
stay place harmless judging value 
sufficient uncertainty value highest 
states really op loops back belief state 
implement loop detection overcome problem 
domain examined littman cassandra kaelbling shown 
smaller problem set bring need actively seeking information 
goal state marked possible initial states marked agent may start initial state facing upwards initial belief state assigns equal probability possible initial configurations squares marked represent states op stuck obtaining reward 
actual value depends chance escaping case stuck forever 
loop avoidance idea process oriented domains op causes harm 
positions theta headings negative reward gamma obtained 
goal reached reward agent randomly moves initial states 
initial configurations moving forward best action perfect information 
moving forward idea initial belief state 
moving forward agent reaches upper positions adjacent goal 
differentiate positions generate sensor readings 
know 
depending move direction take goal state negative reward generated 
right strategy sense performing stay place action 
tell agent know moves forward 
results described table 
underlying mdp states 
point grid contains states corresponding belief states various pairs underlying states equally 
state mdp contains additional combinations containing possible states 
reaching goal quickly objective average discounted reward adr obtained method experiments lasting steps 
std standard deviation 
see improved performance larger approximating mdps 
interpolation method offer results 
combined lookahead step gives excellent results close optimal 
lookahead step employed follows current belief state action determine set possible belief states probability 
values belief states estimated described previously expected value calculated action 
action leading state maximal value chosen 
contrasted regular method estimates values current belief states values grid points 
success interpolation method combined lookahead surprising 
lookahead step need making initial observation revealed able act accordingly 
need foreseen standard interpolation method choice states 
recall grid points assigned action current estimate add mixture grid 
case initial states assigned move forward action optimal policy underlying mdp add larger grids 
important notice step useful richer state space 
seen qmdp method combined lookahead performs poorly 
notice move point grid point grid enhance solution quality 
reason phenomena domain reachable belief states underlying states highly 
result particular initial state nature actions observations domain 
point grid size estimation method adr std qmdp standard standard standard qmdp stay standard stay standard stay standard lookahead standard lookahead standard lookahead table state maze interpolation method contains belief states probable pomdp states algorithm consider point grid contains additional mixtures underlying states interpolation method little influence generated estimates 
nearest neighbor method see table states come play see modest improvement moving larger grid 
alternative approach grid point values generate policy nearest neighbor technique 
belief state search nearest grid point imitate behavior 
various metrics variants nearest neighbor 
surprisingly nearest neighbor method works quite larger domain underlying mdp case equivalent mls method mentioned earlier 
fails reach goal median number steps 
requires little computation 
clear method limited scope domains errors recoverable carry large penalty 
assumptions false smaller domain contains negative rewards performs poorly 
average reward compared better methods 
large pomdp average reward half optimal reward lower discount factor relative performance worse 
experimented nearest neighbor technique different metrics general inferior interpolation method 
figures contain results obtained norm distance belief space 
longer version contains additional results method explanation observed decline solution quality point grids 
discussion simple heuristic variable grid method generating policies stochastic domains partial infor nearest neighbor general technique extending value function grid points space 
remarked earlier method initial stages grid construction leads poor performance 
size estimation method success median mls nearest neighbor nearest neighbor nearest neighbor table state nearest neighbor size estimation method adr std mls nearest neighbor nearest neighbor nearest neighbor table state nearest neighbor mation 
existing domains methods performs reasonable effort 
believe results constitute proof concept indicate solution approach worth studying 
current form method investigating larger domains 
example obtaining solution point grid state pomdp required roughly second cpu time sun ultrasparc unoptimized algorithm 
anticipate additional refinements grid construction method lead methods grid construction informed information initial state better current information value interpolation efficient choosing better set points basis value function estimation addition general heuristics loop elimination improve policy generated 
enhancements code optimization lead powerful efficient methods 
initial experiments techniques generating grids quite positive 
particular examined reachability analysis lookahead 
reachability analysis grid constructed set belief states reachable initial belief state 
set states small ideal candidate set grid points 
essentially method sondik solving small pomdps 
unfortunately pomdp large sufficient uncertainty actions effects set reachable states large 
case methods developed allow integrate partial form reachability analysis grid generation process 
state grid discussed able obtain results ad hoc method considerably limited number reachable states 
considered belief states probability assigned set underlying states probability assigned states 
grid points able obtain success rate median steps goal interpolation method stay place action 
better size estimation method success median standard standard standard standard stay standard stay standard stay table lookahead choose new grid points results described earlier indicates state maze despite larger test domains challenging 
larger domains set reachable states form large directly sophisticated techniques reachability analysis needed 
addition noted reachability analysis quite costly 
require greater time space resources solution phase 
larger easily generated grid better alternative 
method insights gained earlier experiments uses technique determine add belief state grid estimate value best action candidate state current value function compare estimate obtained state added current grid 
estimates significantly different add state grid 
essentially method compares current estimate candidate state lookahead estimate 
method works poorly current estimate poor 
reasonable value function making choices 
table shows improvement performance new states added method start estimate obtained earlier point grid 
problem addressed approach handling information 
domains experimented sensing performed instantaneously free action 
reasonable model certain robots general 
information obtain free larger grid generate 
hope problem handled iterative process 
grid built assumption information instantaneous free 
assumption gradually weakened generate subsequent grids 
unexplored territory 
caution understanding heuristic methods requires examining diverse large pomdps 
clearly robot navigation environments special properties may give skewed picture algorithm performance 
particular domains exhibit symmetry goal oriented actions reversible little flexibility manner goals achieved 
mls general method performs domains 
believe method applicable diverse domains study prove 
hope growing interest pomdps lead generation new diverse test problems better assessments various solution techniques 
acknowledgment wish tony cassandra michael littman providing test data advice 
am grateful craig boutilier david poole useful comments suggestions 
supported iris project ic nserc 
bellman 
dynamic programming 
princeton university press 
cassandra kaelbling kurien 
acting uncertainty discrete bayesian models mobile robot navigation 
proc 
int 
conf 
intelligent robots systems 
cheng 
algorithms partially observable markov decision processes 
ph dissertation university columbia 
school commerce 

course triangulations solving differential equations deformations 
berlin springer verlag 
kaelbling littman cassandra 
planning acting partially observable stochastic domains 
submitted artificial intelligence 
littman cassandra kaelbling 
learning policies partially observable environments scaling 
proc 
th 
machine learning 
littman cassandra kaelbling 
learning policies partially observable environments scaling 
technical report cs dept cs brown university 
lovejoy 
computationally feasible bounds pomdps 
operations research 
lovejoy 
survey algorithmic techniques pomdps 
annals 
parr russell 
approximating optimal policies partially observable stochastic domains 
proc 
fourteenth international joint conference ai 
sondik 
information seeking markov decision processes 
technical report southwest fisheries center national marine fisheries service noaa honolulu hi 
sondik 
optimal control partially observable markov processes infinite horizon discounted costs 
operations research 
white 
survey solution techniques pomdps 
annals 
