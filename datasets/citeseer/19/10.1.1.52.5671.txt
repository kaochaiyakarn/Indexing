scalable computing mccoll programming research group oxford university computing laboratory wolfson building parks road oxford ox qd england 
scalable computing years normal form computing 
unified framework bsp model aims serve foundation evolutionary development 
number important techniques tools methodologies design sequential algorithms programs developed past decades 
transition sequential scalable computing find new requirements universality predictable performance necessitate significant changes emphasis areas 
programs scalable computing addition fully portable efficiently universal offering high performance predictable way general purpose parallel architecture 
bsp model provides discipline design scalable programs kind 
outline approach discuss issues involved 
years sequential computing normal form computing 
early proposal von neumann day sequential computing grown embracing new technologies emerged discarding better came 
today huge global industry 
parts industry hardware software quite different 
sequential hardware life span years 
new technologies continually offer new ways realising basic design form cheaper powerful 
contrast sequential software systems take years develop expected long time preferably 
key reason spectacular success sequential computing widespread universal adoption basic model proposed von neumann 
see happened necessary go back turing 
theoretical studies turing demonstrated single general purpose sequential machine designed capable efficiently performing computation performed special purpose sequential machine 
stated concisely demonstrated appear lncs volume 
van leeuwen editor 
springer verlag 
supported part esprit basic research project foundations general purpose parallel computing 
efficient universality achievable sequential computations 
von neumann produced proposal general purpose sequential computer captured principles turing practical design 
design come known von neumann computer served basic model sequential computers produced late time 
stability universality provided von neumann model permitted encouraged development high level languages compilers 
turn created large diverse software industry producing portable applications software run high performance sequential computer 
sequential computing von neumann model parts computing industry require succeed 
hardware industry provided focus technological innovation confidence model stable software companies find time effort involved developing software justified 
software industry sees stability model providing basis high level cost effective development applications software providing guarantee software produced obsolete technological change 
earliest days sequential computing clear sooner sequential computing superseded parallel computing 
happened despite availability numerous parallel machines demand increased computing power 
argue evolution parallel computing take place years 
show essential requirements transition satisfied 
particular bsp model argue play unifying role analogous von neumann model played development sequential computing 
architectures parallel computing new 
years academic research groups companies producing parallel computers trying persuade people write parallel software 
early parallel systems global communications networks poor scalable performance achieved designing algorithms programs carefully exploited particular architectural features machine 
tedious time consuming cases approach typically produced software easily adapted run machines 
years situation improved 
variety technological economic reasons various classes parallel computer distributed memory machines shared memory multiprocessors clusters workstations alike 
economic advantages standard commodity components major factor convergence 
influential factors need efficiently support global ad dress space distributed memory machines ease programming need replace buses networks achieve scalability shared memory multiprocessors 
various pressures acted produce rapid significant evolutionary restructuring parallel computing industry 
growing consensus combination technological commercial software reasons see steady evolution years standard architectural model scalable parallel computing 
consist collection workstation processor memory pairs connected communications network efficiently support global address space 
message passing shared memory programming styles efficiently supported architectures 
successful models plenty scope different designs technologies realise systems different forms depending cost performance requirements sought 
simplest cheapest probably common architectures clusters personal computers 
intel microprocessor currently referred high volume commodity microprocessor personal computer market hardware support multiprocessing 
couple years low cost multiprocessor personal computers available low cost networking technologies software allowing assembled clusters high performance computing 
spectrum continue small group companies producing large powerful expensive parallel systems require computing resources 
example operating market cray research 
cray powerful distributed memory architecture dec alpha microprocessor 
addition offering extremely high bandwidth global communications specialised hardware mechanisms enable efficiently support parallel programs execute global address space 
mechanisms include hardware barrier synchronisation direct remote memory access 
permits processor get value directly remote memory location machine put value directly remote memory location 
done way avoids performance penalties normally incurred executing operations distributed memory architecture due processor synchronisation unnecessary activities low level systems software 
companies compete top market today focus attention highly optimised architectural design new expensive technologies offer increased performance 
example optical technologies achieve efficient global communications achieved vlsi systems 
implementation global address space distributed memory architecture requires efficient mechanism distributed routing put get requests replies get requests network processors 
number efficient routing memory management techniques developed problem see 
consider problem packet routing processor network 
relation denote routing problem processor packets send various processors network processor due receive packets processors 
packet word information real number integer 
phase randomised routing example show log relation realised processor hypercube log steps 
simple practical randomised method routing relations optical communications system described 
optical system physically realistic method requires log log log steps 
simple efficient protocol routing relations total exchange primitive described 
process architectural convergence described brings hope years establish scalable parallel computing normal form computing see growth large diverse global software industry scalable computing similar currently exists sequential computing 
main goal industry produce scalable software run unchanged high performance architecture cheap multiprocessor pc large parallel supercomputer 
major step goal develop foundation architecture independent programming emerging range scalable parallel systems 
bsp approach described provides just foundation 
offers prospect achieving scalable parallel performance architecture independent parallel software provides framework permits performance parallel distributed systems analysed predicted precise way 
bsp model architectural terms bsp model essentially standard model described 
bulk synchronous parallel bsp computer consists set processor memory pairs global communications network mechanism efficient barrier synchronisation processors 
specialised broadcasting combining facilities efficiently realised software required 
model deal directly issues input output vector units easily extended 
define time step time required single local operation basic operation addition multiplication locally held data values performance bsp computer characterised parameters number processors number time steps barrier synchronisation total number local operations performed processors second total number words delivered communications network second 
course fourth parameter number time steps second 
parameters normalised respect ignored design algorithms programs 
parameter corresponds network latency 
parameter corresponds frequency remote memory accesses affecting total time computation machine higher value remote memory accesses frequently 
formally related time required realise relations situation continuous message traffic value relation performed delta time steps 
scalable parallel system regarded bsp computer benchmarked accordingly determine bsp parameters bsp model prescriptive terms physical architectures applies 
scalable architecture viewed algorithm designer programmer simply point space bsp machines 
parameters characterise communications performance bsp computer contrasts sharply way communications performance described distributed memory architectures market today 
major feature bsp model lifts considerations network performance local level global level 
longer particularly interested network array butterfly hypercube implemented vlsi optical technology 
interest global parameters network describe ability support remote memory accesses uniformly efficient manner 
design implementation bsp computer values achieved depend capabilities available technology amount money willing spend communications network 
computational performance machines continues grow find keep low necessary continually increase investment communications hardware percentage total cost machine 
asymptotic terms values expect various processor networks ring array butterfly log log hypercube log 
asymptotic estimates entirely degree diameter properties corresponding graph 
practical setting channel capacities routing methods vlsi implementation significant impact actual values achieved machine 
new optical technologies may offer prospect reductions values achieved providing efficient means non local communication possible vlsi 
current vlsi networks values similar 
interested problem designing improved networks routing methods reduce obvious approach concentrate alternative problem reducing strategy suggested simple reasoning messages network shorter period time network capacity fixed possible insert messages network frequently 
section see cases performance bsp computation limited suggests designing networks routing methods may advantageous accept significant increase order secure modest decrease raises number interesting architectural questions explored 
contains interesting initial ideas direction 
interesting note characteristics optical communication systems slow switching high bandwidth compatible alternative approach 
bsp computer operates way 
computation consists sequence parallel supersteps superstep consists sequence steps followed barrier synchronisation point remote memory accesses take effect 
superstep processor perform number computation steps values held locally start superstep send receive number messages corresponding remote get put requests 
time superstep determined follows 
maximum number local computation steps executed processor maximum number messages sent processor maximum number messages received processor 
time delta steps 
total time required bsp computation easily obtained adding times supersteps 
produce expression form delta delta typically functions designing efficient bsp algorithm program problem solved sequentially time goal general produce algorithm requiring total time delta delta small possible range values large possible 
cases require carefully arrange data distribution minimise frequency remote memory 
bsp algorithm design illustrate number important issues bsp algorithm design consider computational problems involving matrices 
dense matrix vector multiplication section consider problem multiplying theta matrix element vector processors dense 
bsp cost problem sparse dense matrices theoretically experimentally analysed 
bsp algorithm describe shown 
complexity delta omit various small constant factors formulae 
dense standard sequential algorithm adapted run processor bsp machine follows 
matrix elements initially distributed uniformly processors processor holding theta submatrix referred block block distribution 
vectors uniformly distributed machine elements elements allocated processor 
superstep processor gets set vector elements holds cost step delta second superstep processor computes aj delta holds sends partial sum processor holds time required step delta third final superstep processor computes value vector elements adding values receives 
time required final step input output complexity argument similar show bsp algorithm computes deltav noting sequential computation cost optimal see method strong sense best possible bsp algorithm problem 
simultaneously achieves optimal computation cost optimal communication cost delta delta optimal synchronisation cost delta matrix distributions block grid distribution give bounds 
block grid distribution defined follows 
proc denote processors 
matrix elements uniformly distributed processors allocated proc div mod 
matrix multiplication consider problem multiplying theta dense matrices processors 
standard sequential algorithm adapted run processor bsp machine follows 
processor computes theta submatrix delta require elements number distributed uniformly processors processor holding elements matrix total time required algorithm delta describe efficient bsp implementation standard algorithm due author valiant 
bsp complexity delta previous algorithm distributed uniformly arbitrarily processors 
computation elements distributed uniformly processors 
denote theta submatrix consisting elements div div define similarly 
delta 
proc denote processors 
superstep processor proc gets set elements 
cost step delta second superstep proc computes delta sends resulting values unique processor responsible computing corresponding value cost step delta final superstep processor computes elements adding values received element 
cost step input output complexity argument show bsp implementation standard sequential algorithm algorithm best possible bsp implementation standard method sense simultaneously achieves optimal values 
lu decomposition static computations conveniently modelled directed acyclic graphs node corresponds simple operation arcs correspond inputs outputs 
cn denote directed acyclic graph nodes arcs nodes exist 
shown lu decomposition theta non singular matrix computed pivoting set definitions gamma gamma gamma gamma gamma gamma gamma delta gamma definitions directly translated directed acyclic graph subgraph cn produce bsp algorithm lu decomposition sufficient schedule cn processor bsp computer 
partitioning cn subgraphs isomorphic denote subset nodes cn div div div simple schedule cn requires gamma supersteps superstep computed processors computed processor 
structure cn clear superstep processor receive values send values perform computation steps 
total time required bsp implementation computation modelled cn lu decomposition delta delta solution triangular linear system theta non singular lower triangular matrix element vector 
linear system delta solved back substitution recurrence gamma delta case lu decomposition produce efficient bsp implementation back substitution realising computation directed acyclic graph scheduling graph 
dn denote directed acyclic graph nodes arcs nodes exist 
back substitution recurrence reformulated set definitions gamma gamma delta gamma gamma gamma 
definitions directly translated directed acyclic graph subgraph dn graph dn scheduled processor bsp computer similar manner cn case partition dn subgraphs isomorphic denote set nodes dn div div 
simple schedule dn requires gamma supersteps superstep computed processors computed processor 
structure dn clear superstep processor receive values send values perform computation steps 
total time required bsp implementation computation modelled dn back substitution delta delta situations processors bsp architecture increase runtime 
example consider bsp architecture ring runtime bsp algorithm solution theta triangular linear system np runtime minimised increasing number processors value increase runtime 
sparse matrix vector multiplication return problem computing delta interest case matrix sparse 
sparse matrix vector multiplication problem fundamental importance scientific computing 
heart supercomputing applications iterative methods solve large linear systems 
enable multiplication performed repeatedly additional data redistribution require result vector delta distributed parallel machine way input vector processor holding start computation hold computation 
denote adjacency matrix directed ary dimensional hypercube graph 
nodes graph form dimensional grid points numbered lexicographically 
node directed arcs immediate neighbours dimension 
purposes discussion consider just theta sparse matrices 
instances 
mesh mesh hypercube log 
matrices kind model finite difference operators solution partial differential equations 
fourth matrix random structure row column contains nonzeros 
refer expander matrix 
note 
value particularly significant 
chosen small integer value greater 
theoretical experimental analysis shows compared random data distributions matrix distributions block grid offer reductions bsp communication cost sparse matrix vector multiplication problems 
shows cases significant reductions achieved data distribution efficient decomposition underlying graph 
example nodes mesh graph partitioned regions corresponds mesh nodes 
corresponding data distribution matrix elements gives bsp algorithm delta mesh matrix total cost delta approach applied mesh matrix gives bsp algorithm total cost delta applied hypercube matrix gives bsp algorithm total cost log log delta case minimise communication cost algorithm partitioning nodes graph equal sized subsets way minimises number arcs different subsets 
lower bounds efficiency partitions derived known isoperimetric inequalities graph theory see 
expander matrix best upper bound trivial delta obtained randomly distributing matrix elements 
techniques similar show expander matrix partition nodes equal sized subsets gives value trivial expander matrix delta inherently non local problem 
bsp programming described bsp computer architectural model view bulk synchrony programming discipline 
essence bsp approach parallel programming notion superstep communication synchronisation completely decoupled 
bsp program simply proceeds phases necessary global communications place phases 
approach parallel programming applicable kinds parallel architecture distributed memory architectures shared memory multiprocessors networks workstations 
provides consistent general framework develop portable software scalable computing 
early message passing synchronised point point communication dominant programming approach area parallel computing 
advantages bsp programming message passing 
heterogeneous parallel architectures individual nodes quite varied answer probably lot gained bsp approach 
homogeneous distributed memory architectures low capacity global communications approaches broadly similar terms efficiency achieved 
shared memory architectures modern distributed memory architectures powerful global communications synchronised message passing efficient bsp programming communication synchronisation decoupled 
especially true modern distributed memory architectures hardware support non blocking direct remote memory access sided communications 
message passing systems pairwise barrier synchronisation suffer having simple analytic cost model performance prediction simple means examining global state computation debugging 
comparing message passing bsp approach offers higher level abstraction programmer cost model performance analysis prediction simpler compositional efficient implementations machines 
message passing systems provide primitives various specialised communication patterns arise frequently message passing programs 
include broadcast scatter gather complete exchange reduction scan standard communication patterns arise frequently design bsp algorithms 
important structured patterns conveniently expressed efficiently implemented bsp programming language addition primitive operations put get generate arbitrary unstructured communication patterns 
efficient implementation broadcasting combining bsp architecture discussed 
data parallelism important niche field scalable computing 
number interesting programming languages elegant theories developed support data parallel style programming see 
bsp approach outlined aims offer flexible general style programming provided data parallelism 
approaches incompatible fundamental way 
applications increased flexibility provided bsp approach may required limited data parallel style may offer attractive productive setting parallel software development frees programmer having provide explicit specification various scheduling memory management aspects parallel computation 
situation bsp cost model play extremely important role terms providing analytic framework performance prediction data parallel program 
give pseudocode version bsp algorithm dense matrix vector multiplication data distribution defined 
simplicity assume multiple perfect square 
assumption ideal match problem size machine size course unrealistic practice 
permit give clear concise description main points bsp algorithm implementation having give technical details required general case 
various non standard constructs pseudocode informally described 
integers 
const const const const side 
var array side side real blocksize view mod div var array side 
real blocksize view view par 

const 
var real get seq bi 
seq bj bi bj bj put bi par 
seq 
seq 
fig 

bsp pseudocode theta matrix vector product processors 
var array 

real blocksize declares dimensional array size delta theta delta elements partitioned delta contiguous blocks size theta storage complete block allocated single processor memory pair block split memory modules 
arrangement various complete blocks uniformly distribute set memory modules machine 
get 
get local copy variables operations superstep 
variable held locally get operation effect 
implicit barrier synchronisation sequence get statements 
superstep attempt access value held locally result run time error 
put put value local variable remote variable remote write completed current superstep 
execute parallel thread processor memory pair variable allocated 
inclusion declaration thread optional 
fig 

notations bsp pseudocode 
bsp programming languages noted bsp model prescriptive terms physical architectures applies 
prescriptive terms programming languages programming styles applies 
section briefly describe number programming languages libraries currently available support bsp programming currently development 
pvm message passing library widely implemented widely 
mpi message passing interface elaborate 
supports blocking non blocking point point communication number collective communications broadcast scatter gather reduction 
libraries directly aimed supporting bsp programming purpose 
oxford bsp library consists set subroutines called standard sequential languages fortran core library consists just routines bsp start bsp finish bsp sstep bsp sstep bsp fetch bsp store 
process management barrier synchronisation communication 
higher level operations broadcast reduction available 
library supports static spmd style bsp programming 
implemented large number machines growing community applications developers produce source codes run unchanged efficiently wide variety parallel distributed systems 
generic versions library freely available ftp run homogeneous parallel unix machine pvm tcp ip system shared memory primitives 
highly optimised native implementations produced ibm sp sp sgi power challenge cray machines 
preliminary benchmarking studies carried systems estimate values programmer native implementation expect 
gpl new programming language scalable computing 
developed oxford author miller part esprit project foundations general purpose parallel computing 
prototype compiler currently operational 
preliminary description ideas 
language designed permit efficient high level programming static dynamic bsp computations permit performance programs accurately analysed predicted 
notations pseudocode example similar style gpl 
language procedural explicitly parallel strongly typed 
allows programmer explicitly control scheduling synchronisation memory management combining properties program may crucial achieving efficient scalable predictable performance retaining portability 
example run bsp architecture program access constants corresponding bsp parameters machine 
program optimise computation 
multiuser system constants supplied operating system run time 
point bsp computing may customary applications programs assist optimising resource allocation multiuser multiprocessors indicating operating system resources efficiently utilise cope 
example dense matrix vector multiplication code previous section contain declaration indicating pmax gmax max way 
language high performance programming key requirement gpl language implementations accurate cost model applicable implementation language 
reliable cost model kind programmer able appropriate design decisions achieve highest possible performance 
split parallel extension supports efficient access global address space current distributed memory architectures 
gpl aims support careful engineering optimisation portable parallel pro grams providing cost model performance prediction 
language extension small set global access primitives simple memory management declarations support bulk synchronous message driven styles parallel programming 
bsp experimental bsp programming language development harvard 
language explore effectiveness various constructs added conventional languages fortran support bsp programming 
important objective reach better understanding issues involved designing optimising compilers efficient run time systems programming languages bsp model 
challenges study bsp program design begun 
contrast sequential computing developed techniques tools methodologies dealing specification refinement transformation verification modularity reusability fault tolerance important aspects bsp programs 
blueprint development high performance operating system multiuser bsp computers 
problems remain challenges scalable computing 

aggarwal chandra snir 
communication complexity prams 
theoretical computer science 

arpaci culler krishnamurthy steinberg yelick 
empirical evaluation cray compiler perspective 
proc 
nd annual international symposium computer architecture june 

mccoll 
scientific computing bulk synchronous parallel architectures 
technical report department mathematics university utrecht december 
short version appears proc 
th ifip world computer congress 
volume simon eds elsevier pp 


bollob 
random graphs 
academic press 

burks von neumann 
preliminary discussion logical design electronic computing instrument 
part volume 
institute advanced study princeton 
report army department 
edition june 
second edition september 
appears papers john von neumann computing computer theory burks editors 
volume charles institute reprint series history computing mit press 

fahmy stefanescu valiant 
bulk synchronous parallel computing paradigm transportable software 
proc 
th hawaii international conference system science january 

culler dusseau goldstein krishnamurthy von eicken yelick 
parallel programming split 
proc 
supercomputing pages november 

geist beguelin dongarra jiang manchek sunderam 
pvm parallel virtual machine users guide tutorial networked parallel computing 
mit press cambridge ma 


efficient optical communication parallel computers 
proc 
th annual acm symposium parallel algorithms architectures pages 

gibbons spirakis editors 
lectures parallel computation volume cambridge international series parallel computation 
cambridge university press cambridge uk 

hong kung 
complexity red blue pebble game 
proc 
th annual acm symposium theory computing pages 

manzini 
sparse matrix vector multiplication distributed architectures lower bounds average complexity results 
information processing letters june 

mccoll 
general purpose parallel computing 
gibbons spirakis pages 

mccoll 
special purpose parallel computing 
gibbons spirakis pages 

mccoll 
bsp programming 
blelloch chandy jagannathan editors specification parallel algorithms 
proc 
dimacs workshop princeton may volume dimacs series discrete mathematics theoretical computer science pages 
american mathematical society 

message passing interface forum 
mpi message passing interface standard 
technical report may 

miller 
library bulk synchronous parallel programming 
proc 
british computer society parallel processing specialist group workshop general purpose parallel computing december 
revised extended version available anonymous ftp ftp comlab ox ac uk directory pub packages bsp oxford bsp library software distribution 

rao suel 
efficient communication 
proc 
th international parallel processing symposium 

skillicorn 
foundations parallel programming volume cambridge international series parallel computation 
cambridge university press cambridge uk 

turing 
computable numbers application entscheidungsproblem 
proceedings london mathematical society 
series 
corrections ibid 

valiant 
bridging model parallel computation 
communications acm 

valiant 
general purpose parallel architectures 
van leeuwen editor handbook theoretical computer science volume algorithms complexity pages 
north holland 

valiant 
combining mechanism parallel computers 
meyer auf der heide monien rosenberg editors parallel architectures efficient 
proceedings heinz symposium paderborn november 
lncs vol 
pages 
springer verlag 
