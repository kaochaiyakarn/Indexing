advances neural information processing systems pp 
mit press 
generalization reinforcement learning successful examples sparse coarse coding richard sutton university massachusetts amherst ma usa rich cs umass edu large problems reinforcement learning systems parameterized function approximators neural networks order generalize similar situations actions 
cases strong theoretical results accuracy convergence computational results mixed 
particular boyan moore reported year meeting series negative results attempting apply dynamic programming function approximation simple control problems continuous state spaces 
positive results control tasks attempted significantly larger 
important differences sparse coarse coded function approximators global function approximators learned online learned offline 
boyan moore suggested problems encountered solved actual outcomes classical monte carlo methods td algorithm 
experiments resulted substantially poorer performance 
conclude reinforcement learning robustly conjunction function approximators little justification avoiding case general reinforcement learning function approximation reinforcement learning broad class optimal control methods estimating value functions experience simulation search barto bradtke singh sutton watkins :10.1.1.132.7760
methods dynamic programming temporal difference learning build estimates part basis estimates 
may practice estimates exact large problems parameterized function approximators neural networks 
estimates imperfect turn targets estimates possible ultimate result poor estimates divergence 
methods shown unstable theory baird gordon tsitsiklis van roy practice boyan moore 
hand methods proven stable theory sutton dayan effective practice lin tesauro zhang dietterich crites barto :10.1.1.132.7760
key requirements method task order obtain performance 
experiments part narrowing answer question 
reinforcement learning methods variations sarsa algorithm rummery niranjan singh sutton 
method td algorithm sutton applied state action pairs states predictions basis selecting actions :10.1.1.132.7760
learning agent estimates action values defined expected reward starting state action policy 
estimated states actions policy currently followed agent 
policy chosen dependent current estimates way jointly improve ideally approaching optimal policy optimal action values 
experiments actions selected call ffl greedy policy time action selected state action estimate largest ties broken randomly 
small fraction ffl time action selected randomly uniformly action set discrete finite 
variations sarsa algorithm conventional accumulate traces replace traces singh sutton 
details algorithm 
apply sarsa algorithm tasks continuous state space combined sparse coarse coded function approximator known cmac albus miller gordon kraft watkins lin kim dean tham 
cmac uses multiple overlapping tilings state space produce feature representation final linear mapping learning takes place 
see 
effect network fixed radial basis functions particularly efficient computationally respects expect rbf networks similar methods see sutton whitehead just 
important note tilings need simple grids 
example avoid curse dimensionality common trick ignore dimensions tilings slices boxes 
second major trick hashing consistent random collapsing large set tiles smaller set 
hashing memory requirements reduced large factors little loss performance 
possible high resolution needed small fraction state space 
hashing frees curse dimensionality sense memory requirements need exponential number dimensions need merely match real demands task 
convergence control problems applied sarsa cmac combination continuous state control problems studied boyan moore gridworld world mountain car model task dynamics applied dynamic programming backups offline fixed set states learned online model backed states encountered complete trials 
boyan 
initially qo actions cmac tiles 

start trial random state features ffl greedy policy 

eligibility traces 
accumulate algorithm 
replace algorithm 
environment step take action observe resultant reward state 
choose action features terminal state ffl greedy policy 

learn ff gamma 
loop terminal state go go 
sarsa algorithm finite horizon trial tasks 
function ffl greedy policy returns probability ffl random action probability gamma ffl computes action returns action sum largest resolving ties randomly 
function features returns set cmac tiles corresponding state number tiles returned constant ff scalar parameters 
dimension dimension tiling tiling involve multiple overlapping tilings state space 
show theta regular tilings offset overlaid continuous dimensional state space 
state shown dot exactly tile tiling 
state tiles represent sarsa algorithm described 
tilings need regular grids shown 
particular slices number grows sub exponentially dimensionality space 
widely conjunction reinforcement learning systems watkins lin kim dean shewchuk tham 
moore robust performance tasks 
report results world mountain car difficult tasks considered 
training consisted series trials starting randomly selected state continuing goal region reached 
step penalty negative reward gamma incurred 
world task additional penalty incurred state regions 
details appendix 
plots show estimated cost goal state max 
world task consisted tilings theta 
mountain car task tilings theta 
task cost goal function learned run 
mountain car task cost goal function learned run 
engine weak accelerate directly slope reach goal car move away 
plot shows value function learned goal reached 
experimented larger difficult task attempted boyan moore 
acrobot link actuated robot roughly analogous swinging dejong 
joint corresponding hands bar exert torque second joint corresponding bending waist 
object swing endpoint feet bar amount equal links 
mountain car task actions positive torque negative torque torque reward gamma steps 
see appendix 
steps trial log scale trials typical single run median runs smoothed average runs acrobot learning curves goal raise tip line acrobot torque applied tip acrobot learning curves 
effect key question reinforcement learning better learn basis actual outcomes monte carlo methods td learn basis interim estimates td 
theoretically asymptotic advantages function approximators dayan bertsekas empirically thought achieve better learning rates sutton :10.1.1.132.7760
hitherto question put empirical test function approximators 
figures shows results test 
steps trial averaged trials runs mountain car accumulate replace cost trial averaged trials runs replace world effects ff mountain car world tasks 
summarizes data systematic studies different tasks picture effect cases performance inverted shaped function performance degrades rapidly approaches worst performance obtained 
fact performance improves increased argues eligibility traces step methods td step learning 
fact performance improves rapidly reduced argues monte carlo methods 
despite theoretical asymptotic advantages methods appear inferior practice 
acknowledgments author gratefully acknowledges assistance justin boyan andrew moore satinder singh peter dayan evaluating results 
accumulate random walk failures steps cart pole steps trial mountain car replace cost trial world root mean squared error accumulate replace accumulate replace performance versus best ff different tasks 
left panels summarize data 
upper right panel concerns state markov chain objective predict state probability terminating terminal state opposed singh sutton 
lower left panel concerns pole balancing task studied barto sutton anderson 
previously unpublished data earlier study sutton 
albus 
brain behavior robotics chapter pages 
byte books 
baird 
residual algorithms reinforcement learning function approximation 
proc 
ml 
morgan kaufman san francisco ca 
barto bradtke singh 
real time learning control asynchronous dynamic programming 
artificial intelligence 
barto sutton anderson 
neuronlike elements solve difficult learning control problems 
trans 
ieee smc 
bertsekas 
counterexample temporal differences learning 
neural computation 
boyan moore 
generalization reinforcement learning safely approximating value function 
nips 
san mateo ca morgan kaufmann 
crites barto 
improving elevator performance reinforcement learning 
nips 
cambridge ma mit press 
dayan 
convergence td general machine learning 
dean shewchuk 
reinforcement learning planning control 
minton machine learning methods planning scheduling 
morgan kaufmann 
dejong 
swinging acrobot example intelligent control 
proceedings american control conference pages 
gordon 
stable function approximation dynamic programming 
proc 
ml 
lin 
self improving reactive agents reinforcement learning planning teaching 
machine learning 
lin 
kim 
cmac adaptive critic self learning control 
ieee trans 
neural networks 
miller kraft 
cmac associative neural network alternative backpropagation 
proc 
ieee 
rummery niranjan 
line learning connectionist systems 
technical report cued infeng tr cambridge university engineering dept singh sutton 
reinforcement learning replacing eligibility traces 
machine learning 

robot dynamics control 
new york wiley 
sutton 
temporal credit assignment reinforcement learning 
phd thesis university massachusetts amherst ma 
sutton 
learning predict methods temporal differences 
machine learning 
sutton whitehead 
online learning random representations 
proc 
ml pages 
morgan kaufmann 
tham 
modular line function approximation scaling reinforcement learning 
phd thesis cambridge univ cambridge england 
tesauro 
practical issues temporal difference learning 
machine learning 
tsitsiklis van roy 
feature methods large scale dynamic programming 
report lids mit cambridge ma 
watkins 
learning delayed rewards 
phd thesis cambridge univ zhang dietterich reinforcement learning approach job shop scheduling 
proc 
ijcai 
appendix details experiments world actions right left moved approximately directions movement cause agent leave limits space 
random gaussian noise standard deviation added motion dimensions 
costs negative rewards task gamma time step plus additional penalties oval entered 
penalties times distance distance nearest edge 
radius located center points 
initial state trial selected randomly uniformly non goal states 
run ff ffl 
gamma 
details mountain car task singh sutton 
run ff ffl 
gamma 
acrobot task tilings 
dimensions divided intervals 
tilings depended usual way dimensions 
tilings depended dimensions tilings sets dimensions 
depended dimensions tilings sets dimensions 
tilings depended dimension tilings dimension 
resulted total delta delta delta delta tiles 
equations motion gammad gamma oe gamma gamma oe gamma oe cos cos oe gammam sin gamma sin cos gamma oe oe cos gamma gamma torque applied second joint delta time increment 
actions chosen state updates equations corresponding hz 
angular velocities bounded gamma gamma 
remaining constants masses links lengths links lengths center mass links moments inertia links gravity 
parameters ff ffl 
starting state trial 
