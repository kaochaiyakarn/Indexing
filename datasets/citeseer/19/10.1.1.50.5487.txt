international computer science institute center st ffl suite ffl berkeley california ffl ffl fax survey fuzzy clustering algorithms pattern recognition tr october clustering algorithms aim modelling fuzzy ambiguous unlabeled patterns efficiently 
goal propose theoretical framework clustering systems compared basis learning strategies 
part issues reviewed relative probabilistic absolute possibilistic fuzzy membership functions relationships bayes rule batch line learning growing pruning networks modular network architectures topologically perfect mapping ecological nets neuro fuzziness 
discussion equivalence concepts fuzzy clustering soft competitive learning clustering algorithms proposed unifying framework comparison clustering systems 
set functional attributes selected dictionary entries comparison 
second part clustering algorithms taken literature reviewed compared basis selected properties interest 
networks clustering models selforganizing map som ii fuzzy learning vector quantization flvq iii fuzzy adaptive resonance theory fuzzy art iv growing neural gas gng fully self organizing simplified adaptive resonance theory fosart 
theoretical comparison fairly simple yields observations may appear international computer science institute center street suite berkeley ca ph fx icsi berkeley edu cnr bari italy ph fx ba cnr 
firstly flvq fuzzy art fosart exploit concepts derived fuzzy set theory relative absolute fuzzy membership functions 
secondly som flvq gng fosart employ soft competitive learning mechanisms affected asymptotic case flvq som gng fosart considered effective fuzzy clustering algorithms 
key words possibilistic fuzzy membership line batch learning modular architectures topologically correct mapping ecological nets fuzzy clustering 
ii years synthesis clustering algorithms fuzzy set theory led development called fuzzy clustering algorithms aim model fuzzy ambiguous unsupervised unlabeled patterns efficiently 
goal review compare self organization strategies clustering algorithms called fuzzy considered fuzzy definitions existing literature 
best knowledge comparative studies attempted 
may due objective difficulty comparing great variety clustering approaches meaningful set common functional features 
select set interesting functional features part concepts relative absolute fuzzy membership functions batch line learning growing pruning networks modular network architectures topologically correct mapping ecological net neuro fuzziness reviewed 
selected functional attributes employed dictionary entries comparison clustering algorithms 
derive interpretation fuzzy clustering inductive learning intended synonym soft competitive parameter adaptation clustering systems existing literature 
conceptual equivalence employed unifying framework comparison clustering algorithms 
approach considered interesting quite reasonable researchers 
authors believe fuzziness incorporated various levels generate fuzzy neural network input output learning neural levels see input feature expressed terms fuzzy membership values indicating degree belonging linguistic properties low medium high claim calling certain networks fuzzy improper acceptable 
cases comments pertaining proposed terminology fuzzy clustering algorithm affect core centered comparison learning mechanisms adopted clustering algorithms model fuzzy ambiguous patterns 
second part basic functional features clustering algorithms investigated see algorithms employ degree fuzzy set theoretic concepts absolute relative membership functions fuzzy set operations belong conceptual framework fuzzy clustering systems 
clustering models investigated line learning static sizing static linking self organizing map som ii line learning static sizing linking fuzzy learning vector quantization flvq called fuzzy kohonen clustering network iii line learning dynamic sizing linking fuzzy adaptive resonance theory fuzzy art iv line learning dynamic sizing dynamic linking growing neural gas gng line learning dynamic sizing dynamic linking fully self organizing simplified adaptive resonance theory fosart fuzzy simplified adaptive resonance theory fuzzy sart model 
fuzzy membership probability density functions section proposes brief review probabilistic possibilistic fuzzy membership concepts compared bayes view posterior probability likelihood 
absolute relative fuzzy memberships instance input manifold total number input instances 
assume may belong generic state termed category component total number possible states 
extent compatible vague fuzzy concept associated generic state interpreted terms possibility compatibility distribution terms probability distribution 
possibility distributions called fuzzy membership functions believe useful find difficult justify basis objective probabilities 
depending conditions required state fuzzy states fuzzy partition input data set membership functions divided categories 
relative probabilistic constrained fuzzy membership values 
absolute possibilistic fuzzy membership typicality values index ranges patterns index concepts 
absolute relative membership types related equation relative typicality values satisfy conditions ii iii constraint ii inherently probabilistic constraint relating values posterior probability estimates bayesian framework 
condition ii values relative numbers dependent absolute membership pattern classes indirectly total number classes 
means processing elements pes exploiting relative membership function activation function contextsensitive provides tool modeling network wide internode communication assuming pes coupled feed sideways lateral connections 
possibilistic membership functions may satisfy conditions iii listed case upper bound membership function fuzzy set termed normal differ probabilistic memberships condition ii relaxed follows iv max fa owing condition iv sum absolute memberships noise point categories need equal 
term absolute similarity value depending fuzzy state exclusively input pattern words context insensitive affected state 
pes exploiting absolute membership activation function independent feature lateral connection 
probabilistic possibilistic fuzzy clustering affected known drawbacks 
hand probabilistic fuzzy clustering owing condition ii noise points outliers featuring low possibilistic respect templates codebooks may significantly high probabilistic membership values may severely affect prototype parameter estimate refer 
hand possibilistic fuzzy clustering learning rates computed absolute tend produce coincident clusters 
poor behavior explained fact cluster prototypes uncoupled possibilistic clustering possibilistic clustering algorithms try minimize objective function operating cluster independently 
leads increase number local minima 
different expressions existing literature consistent definition provided useful 
include gaussian gamma oe gaussian mixtures assumed euclidean distance input pattern prototype receptive field center th category 
variables oe resolution parameters belonging range see 
fuzzy memberships mixture probabilities investigate relationship objective probability density functions useful fuzzy membership functions note absolute membership function relates probabilistic membership gaussian mixture models widely employed framework optimization problems featuring firm statistical foundation 
mixture probability model consisting mixture components priori probability pattern belongs mixture component jc conditional likelihood pattern pattern state statistics known posteriori conditional probability jx estimated bayes rule jx jc delta jc delta cg states assumed equally equation jx jc jc relationships hold true jx jc belong range ii jx mixture components provide complete partition input space iii 
comparison equation equation properties iv section properties iii write priors considered ignored fp jc ae fa fp jx ae fr words priors ignored objective likelihood posterior probabilities subset useful absolute relative fuzzy membership functions respectively 
summarize combination equation constraints iv section allows human designer choose absolute membership function addition satisfying mild condition iv section considered useful application hand choice may difficult justify basis objective probabilities see section 
chosen absolute membership function satisfies condition iv section severe constraint absolute membership values equivalent likelihood values consequence relative membership values computed equation considered posterior probability estimates case priors ignored 
line versus line model adaptation inductive learning systems learning systems progressing special cases training data general models concepts information extracted finite set observed examples termed training data set answer questions unobserved samples belonging called test set unknown properties hidden training data ii goal learning process minimize risk functional theoretically computed infinite data set adapting system parameters basis finite training set learning problem turned optimization problem 
system parameters learned training data classes learning situations depending data learner batch setting data available block line setting data arrive sequentially 
practical problems sequential data stream stored analysis block block data analyzed sequentially user free take batch line point view 
goal line learning methods avoid storage complete data set discarding data point 
line learning methods required necessary respond real time input data set may huge batch methods impractical numerical properties see computation time memory requirement input data comes continuous stream unlimited length totally impossible apply batch methods 
line learning typically results systems order dependent training line biological complex systems 
line systems parameter adaptation example driven sensitive presence noise average noise data tend provide highly oscillatory non smooth output functions perform poorly terms generalization ability 
example clustering systems fuzzy art single poorly mapped pattern suffices initiate creation new unit may affected overfitting system may fit noise just data 
batch methods preferred interest final answer best answer obtain finite training data set exact closed form solution minimization problem 
batch learning problems simple case linear model regression see exact closed form solutions lead numerical difficulties large data sets 
cases computationally feasible alternative provided iterative batch methods gradient descent cost function sweep repeatedly data set 
summarize batch learning methods subdivided exact closed form solutions cost function minimization problem solutions numerically computationally inapplicable large data sets ii iterative batch learning algorithms gradient descent cost function 
iterative batch learning algorithms learning rate parameter small choice fairly critical learning rate small reduction error small large instabilities divergent oscillations may result 
analytically convergence iterative batch algorithms exact closed form solutions analyzed useful hints constraint learning rate value gathered 
iterative batch learning algorithms developed process large data sets spend enormous amount computation time processing entire training set order compute gradient may small learning step parameter space processing epoch 
case gradient descent algorithms functional feature clearly incompatible original motivation justifies study iterative batch learning schemes 
situations alternative solution called mini batch average parameter update values subsets entire training data set 
intermediate steps parameter space iterative mini batch algorithms may converge faster iterative full batch counterparts 
stretching idea alternative solution iterative full batch algorithms develop line stochastic approximations 
line learning algorithms simple intuitive heuristic sum training samples iterative batch solution cost function minimization task dropped separating contribution data point provide sequential update formula allow parameter update new data point presentation 
heuristic reasonable analytical proof line proce stochastic refers assumption single data point learning system chosen basis stochastic process 
dure converges solution 
known difference exact batch mode heuristic line updating arbitrarily reduced adoption small learning rates 
view line procedures approximations iterative batch algorithms learning rate constraints capable guaranteeing convergence iterative batch mode may applied line problem appropriate definition convergence linear regression case see 
conditions hold observed line learning systems requiring significantly computation time parameter update significantly faster converging iterative batch algorithms 
general learning rate ff line update rule satisfy conditions applied coefficients robbins monro algorithm finding roots function iteratively see pp 
lim ff ii ff iii ff 
example line update procedure learning rate remains fixed algorithm converges stochastic sense model parameters drift initial positions quasi stationary positions start wander dynamic equilibrium 
learning rate decreases monotonically robbins monro conditions ff see line learning algorithms shown converge point parameter space 
brief review batch update line update techniques refer 
error gradient descent advanced techniques learning data years neural network community considerable effort search learning techniques effective dealing local minima generalization new examples traditional approaches simple gradient descent 
alternative approaches deal effectively curse dimensionality qualitative principle known occam razor 
example curse dimensionality consider function estimator increases number adjustable parameters dimensionality input space 
consequence size training data required compute reliable estimate adaptive parameters may huge practical problems 
complexity learning system increases number independent adjustable parameters termed degrees freedom adapted learning process 
qualitative principle occam razor sound basis generalizing set examples prefer simplest hypothesis fits observed data 
principle states effective cost function minimized inductive learning system provide trade model fits training data model complexity 
means model complexity controlled priori background knowledge subjective knowledge available evidence empirical risk provided training data observed 
different principles provide cost functions considered different quantitative formulations occam qualitative principle 
rough taxonomy advanced techniques optimal learning originally proposed 
global optimization local algorithms gradient descent may explore techniques guarantee global optimization effectively facing curse dimensionality 
significant developments area support vector machines svms vapnik chervonenkis vc statistical learning theory capable detecting global minimum cost function classification problems increasingly popular finding solutions classification function regression tasks 
growing networks pruning human designers typically opportunity embed task specific prior knowledge inductive learning algorithm setting topology complexity multilayer perceptron backpropagation weight adaptation applied 
pruning algorithms training network expected large respect problem hand continue pruning nodes affect learning process significantly 
vice versa growing networks start small networks grow gradually convergence reached 
result complexity network expected tuned problem hand generalization capability expected increase 
modular architectures prior structures experience self organizing neurological systems consist highly structured hierarchical architectures provided feed back mechanisms 
systems combination initial architecture produced evolution experience additional fine tuning prepares system function entire domain generalizing learned behaviour instances previously encountered 
line biological learning systems classical engineering paradigm consists partitioning solution problem modules specialized learning single task modular architectures natural solution significant practical problems 
applied mathematics principle tackling problem dividing simpler subproblems solutions combined yield solution complex problem termed divide conquer 
supervised learning interesting modular proposal addresses major problem providing effective integration system modules 
analytically importance developing modular architectures stressed sufficient necessary conditions capable guaranteeing local minima free cost functions detected simple gradient descent algorithm reach absolute minimum error surface 
contiguous problem fine tuning modular learning systems basis training experiences problem prior structures problem learning tabula rasa 
minsky claims significant learning significant rate presupposes significant prior structure 
words important properties model hard wired built tuned experience learned statistical meaningful way 
intuitively starting prior learning structure experience inductive fine tuning network parameters allow structured organization distributed system emerge naturally elementary interactions pes 
tantamount saying competitive adaptation distributed localized neuron synapse parameters expected enhance development structured nets consisting specialized subsystems modules line biological neural systems 
regard exploitation distributed parameters observed hand line clustering algorithms existing literature som exploit global network time counter time variable specialized localized basis distributed localized neuron time variables 
consequence processing time plasticity potential ability moving template vector input pattern pe networks 
hand years line kohonen network models exploiting distributed time variables 
example gng fosart connection time variable equal number times synapse selected adaptation steps 
fuzzy sart fosart neuron time variable equal number epochs pe survived 
summarize line clustering algorithms literature depend different degrees distributed parameters competitive adaptation consistent development structured systems specialized subsystems 
regard batch iterative algorithms fuzzy fcm flvq global time distributed time variables employed processing time plasticity pe net 
justified considering fuzzy clustering mechanisms employed pes acquire input pattern simultaneously 
may consider fuzzy membership functions allow pattern belong multiple categories different degrees 
words batch algorithms plasticity pe considered learning histories pes may differ significantly 
obvious example pe define distributed neuron time counter equivalent inverse plasticity stability variable sum neuron learning rates adaptation steps learning rate pe decreases monotonically local time 
summarize batch clustering algorithms literature exploitation distributed global parameters may deserve investigation framework developing self organizing networks consisting specialized subsystems 
topologically correct mapping presentation competitive hebbian rule chr introducing competition synaptic links represented fundamental breakthrough evolution fully self organizing artificial neural network models 
synaptic link defined lateral connection pes belonging neural layer models defined distributed systems capable dynamically generating removing pes ii dynamically generating removing synaptic links 
chr generates synaptic links follows 
pattern ng total number input patterns consider winner unit pew featuring shortest inter pattern distance tw total number pes tw template pattern pew euclidean inter pattern distance second best unit pew featuring activation value tw 
exploitation euclidean inter pattern distance competitive learning shapes neuron receptive fields voronoi polyhedra 
chr connection pew pew exist generated 
proved chr forms topology preserving maps 
define tpm consider input manifold dimensionality input space graph network consisting vertices neural units pe pointer termed category template prototype belonging pointer set fsg ft related attached vertex pe mapping oe input space vertices defined oe feature vector input pattern ng vertex pe termed winner unit determined delta euclidean inter vector distance 
equations feature vector mapped vertex pe pointer closest equivalent stating mapped vertex pe voronoi polyhedron encloses voronoi polyhedron neuron pe receptive field pe centered identified masked voronoi polyhedron neuron defined 
inverse mapping oe gamma defined oe gamma pointers termed adjacent feature manifold masked voronoi polyhedra adjacent 
vertices pe pe termed adjacent connected synaptic link 
mapping oe defined neighborhood adjacency preserving pair adjacent pointers assigned vertices pe pe adjacent mapping oe gamma defined neighborhood preserving pair vertices pe pe adjacent assigned adjacent locations tpm defined mapping oe oe inverse mapping oe gamma neighborhood adjacency preserving 
note exploitation chr allows generation networks consisting mutually disjointed specialized independent maps 
modular organization enables learning system perform cooperative learning adapting processing units connected graph units belong map graph competitive learning disjointed maps enhance specialization system modules 
artificial cognitive systems ecological nets perform cognitive tasks biological neural systems exploit nets types pes combined ii structured architectures consisting hierarchies subnets iii feed back mechanisms feed back information provided external environment natural system response system actions 
presence feed back interaction environment characterizes natural systems featuring cognitive capabilities 
artificial neural systems feature biological properties listed 
example som hopfield network homogeneous systems feature structured architecture supervision reinforcement feedback external environment termed supervisor 
reinforcement learning neural system allowed react training case 
told reaction effective 
increase biological plausibility artificial neural models employ differentiated structures provided layers specialized subnets hierarchies maps parallel study artificial neural nets stand systems evolve science ecological nets neural systems external environments modeled 
example kohonen networks art system employs structured architecture self adjust network dimension problem specific conditions 
particular art orienting subsystem models responses external environment learning activities attentional subsystem 
art system belongs class ecological nets 
fuzzy clustering algorithms clustering algorithm performs unsupervised detection statistical regularities random sequence input patterns 
attention focused clustering learning schemes 
definition stated artificial neural network nn model performs fuzzy clustering allows pattern belong multiple categories different degrees depending neurons ability recognize input pattern 
approach known traditional field coding techniques data compression 
traditional means clustering algorithms feature cost function discretization error characterized local minima data compression techniques modify means algorithms replacing winner takes strategy wta termed crisp hard competitive soft max adaptation rule referred soft competitive learning 
note terms soft max soft competitive learning rule adopted confused wta parameter adaptation strategy purely competitive allows cooperative soft competitive learning 
sensitive initialization templates different initializations may lead different minimization results 
fact wta adaptations may able get system poor local minimum lies proximity status system started 
wta learning soft competitive learning scheme defined learning strategy adjusts winning cluster affects cluster centers depending proximity input pattern 
general soft competitive learning decreases dependency initialization reduces presence dead units 
observe 
fuzzy clustering definition provided equivalent definition soft competitive adaptation rule traditionally employed field data compression clustering algorithm termed fuzzy clustering algorithm iff employs soft competitive non crisp parameter adaptation strategy 

corollary point 
fuzzy clustering algorithm necessarily exploit concepts derived fuzzy set theory fuzzy set membership functions fuzzy set operations 
example som pursues soft competitive learning means biologically plausible update rules employ fuzzy set theoretic concept 
interesting example provided expectation maximization em algorithm applied optimize parameters gaussian mixture 
features firm statistic foundation employs fuzzy set theoretic concept em applied gaussian mixtures termed fuzzy definition 

noted relative probabilistic fuzzy membership functions traditionally applied clustering algorithms intuitive belief vector quantizers winner non winner information relationship input pattern prototypes better representatives structure input data local information 
clustering algorithms fuzzy fcm fuzzy learning vector quantization flvq algorithms combine local global information computation relative fuzzy membership function 
functional standpoint connectionist models useful relative fuzzy membership function objective posterior probability estimate computed flvq em applied gaussian mixtures respectively equivalent distributed systems contextual competitive cooperative effect mechanism employing feed sideways intra layer connections employed see section 
note clustering networks employ intra layer connections explicitly means specific data structures parameter adaptation strategies 
called soft max function normalized exponential employed mixture models experts 
dictionary entries comparison clustering algorithms general discussion developed sections derive set basis features dictionary entries theoretical comparison data clustering algorithms ffl fuzzy set theoretical concepts absolute relative membership functions see section 
ffl line batch mini batch learning modes apply global network local neuron connection parameters distributed learning systems see section 
ffl prior structure modular architectures growing pruning distributed systems see section 
ffl topologically correct mapping concept related modular architectures consisting specialized disjointed mutually independent maps belong output lattice processing units see section 
ffl ecological nets see section 
ffl interpretation fuzzy clustering algorithm intended clustering system exploiting soft competitive cooperative competitive versus hard crisp purely competitive adaptation system parameters see section 
som starting kohonen line vector quantization model vq acronym line employs wta learning strategy line som develops soft competitive learning technique constraints derived neurophysiological studies providing annealing schedule 
consist empirical functions time user defined obey heuristic rules requires learning rates decrease monotonically time cooling scheme number input pattern presentations increases learning rates winner non winner decrease zero line theorem see section 
important properties cooling schedule analyzed 
second heuristic rule applies output lattice processing units requires size update resonance neighborhood centered winner node decrease monotonically time soft competitive learning strategy changes hard competitive wta 
model transition equivalent stating initial overlap oversampling nodes receptive fields decrease monotonically time reduced zero hard competitive learning renders receptive fields equivalent voronoi polyhedra 
interpretations second heuristic rule relationships som optimization techniques maximum entropy clustering deterministic annealing expectation maximization em optimization algorithm discussed 
general perspective remembered compared hard competitive learning soft competitive learning decreases dependency initialization reduces presence dead units see section 
important point batch iterative som version 
input parameters simplify discussion approach deal finite training data set fxg consisting input patterns dimensionality input space 
input data set repeatedly network termination criterion satisfied 
ffl som employs output array pes size fixed priori basis human designer design choice prior knowledge particular learning task 
ffl dimensionality output lattice set human designer 
choice implicitly fixes number adjacent vertices equivalent fixing number synaptic links emanating pe output lattice see section 
example pe belonging lattice features adjacent vertices pes belonging lattice feature adjacent vertices property som termed static linking 
peculiar feature som deals topological relationship node adjacency graph dealing synaptic links explicitly shown equation see section 
ffl assumes initial position clusters known advance 
initial positions randomly assigned may driven sample vectors extracted input data set 
solution reduce formation dead units 
ffl employs user defined monotone decreasing functions time compute step size ffl cooling schedule see section ii size oe resonance domain centered winner pe output lattice 
decreasing functions combined multiplied compute lateral excitatory signal generated winner featuring shape bell curve see equation 
time number input pattern presentations 
ffl requires termination threshold maximum number epochs max say number times finite training data set repeatedly network 
description algorithm ffl geometrical distance relationships input measurement space employed enforce competitive learning mechanisms input pattern ng total number patterns number input pattern presentations winner unit pe detected processing unit featuring shortest inter pattern distance template pattern winner unit pe euclidean hamming distance 
exploitation euclidean inter pattern distance competitive learning shapes neuron receptive fields voronoi polyhedra see section 
ffl topological relationships pes belonging output lattice graph see section employed detect neighboring cooperative soft competitive effects 
greater detail pes belonging update neighborhood centered winner unit pe affected lateral excitatory signal generated winner shaped bell curve bubble strategy ff ffl delta gamma kr gammar oe ff learning rate processing unit pe time denote spatial coordinates output unit pe winner unit pe external lattice ffl oe user defined monotone decreasing functions time describing amplitude standard deviation gaussian function respectively 
note equation distance kr gamma equivalent neighborhood ranking processing units external lattice som differs neural gas ng algorithm employs neighborhood ranking vectors input space clustering algorithms maximum entropy clustering flvq fosart see sections adaptation step function absolute distance vectors current input pattern input space 
analysis equation reveals oe nodes som purely competitive som equivalent hard crisp means clustering procedure ii ffl som reaches termination template vector attracted input pattern 
ffl som employs kohonen weight adaptation rule position optimally cluster prototypes belong resonance domain update neighborhood centered winner pe ff delta gamma ff computed means equation ff ffl 
important observe equation related line mcqueen means batch lloyd forgy version special case em optimization gaussian mixture 
long equation features oe long som soft learning strategy equivalent wta strategy specify cost function minimized equation exists cost function yielding equation gradient 
som features set potential functions node independently minimized stochastic line gradient descent 
cost function leads update strategy similar precisely equations discussed 
cost function introduced context design optimal vector quantizer codebook encoding data transmission noisy channel 
limitations despite successes practical applications som contains major deficiencies acknowledged 
ffl som minimize known objective function termination optimizing model process data 
ffl line som order dependent due line learning final weight vectors affected order input sequence 
ffl prototype parameter estimates may severely affected noise points outliers 
due fact learning rates som computed function number input presentations node positions grid independent actual distance separating input pattern cluster template 
ffl size output lattice step size size resonance neighborhood varied empirically data set achieve useful results 
ffl probability density function pdf estimation achieved 
attempts interpret density codebook vectors model input data distribution limited success 
ffl employed topology preserving mapping dimension input space larger 
fact som tries form neighborhood preserving inverse mapping oe gamma lattice input manifold necessarily neighborhood preserving mapping oe 
obtain topologically correct map running som algorithm topological adjacency structure preset graph match topological structure unknown manifold 
advantages ffl owing soft competitive implementation som expected trapped local minima generate dead units hard competitive alternatives 
ffl batch som order independent final weight vectors affected order input sequence 
ffl som employed vector system 
authors som intended pattern classification 
som attempts find topological structures input data display dimensions som employed data visualization tasks som simply attempts achieve consistent spatial mapping training vectors usually dimensions 
precisely stated som employed topology preserving mapping iff dimension input space larger dimension output lattice 
architectural features main features som summarized table 
flvq flvq called quickly gained popularity fairly successful batch clustering algorithm 
flvq design aims improve performance usability kohonen line vq som algorithms combining line kohonen weight adaptation rule fuzzy set membership function proposed batch fcm algorithm 
allows flvq compute learning rate size update neighborhood directly data 
flvq employs smaller set user defined parameters som 
som flvq batch clustering method employs metrical neighbors input space topological neighbors belonging output lattice 
particular compute adaptation step flvq takes account absolute distances vectors respect current input pattern som employs neighborhood ranking processing units external lattice 
advances field broad family batch flvq algorithms formally defined class cost function minimization schemes 
class batch vector quantizers referred extended flvq family 
flvq updating seen special case learning schemes restricted range weighting exponent 
flvq related line fuzzy clustering algorithms sequential generalized lvq glvq glvq family algorithms glvq class line fuzzy algorithms learning vector quantization proposed instances termed 
table summarizes functional comparisons learning vector quantization algorithms 
detailed analysis models refer 
table batch sequential updates updates hard crisp purely competitive learning vq relative membership function weighting exponent constant fcm glvq relative membership function weighting exponent function training epochs flvq membership functions soft weighting exponent glvq competitive learning width learning rate glvq glvq distrib 
constant fcm width learning rate distrib 
decreases time flvq cooling schedule vq learning rate decreases time glvq glvq cooling schedule fcm flvq see 
see 
extended entire net 
see 
see 
input parameters ffl flvq requires user define number natural groups detected 
ffl requires initial final weighting exponent controlling amount fuzziness algorithm 
heuristic constraint recommended 
ffl parameter max defined maximum number epochs 
ffl convergence error ffl employed termination strategy 
description algorithm ffl descending flvq employs weighting exponent monotonically decreasing processing time number processing epochs 
decreasing expression gamma delta gamma max ffl flvq employs metrical neighbors input space analogously glvq glvq 
particular flvq computes membership function providing degree compatibility input pattern vague concept represented cluster center 
compute inter pattern similarity measure flvq combines winner non winner information follows gamma gamma total number categories total number input patterns input pattern cluster template epoch time euclidean inter pattern distance 
equation relative probabilistic membership function satisfies conditions required state fuzzy subsets fuzzy partition input space see section 
ffl flvq learning rule delta gamma ff gamma ff delta ff delta observed causes relative membership problem fcm 
means equation provides membership values relative numbers noise points outliers may significantly high membership values may severely affect prototype parameter estimate 
note input patterns outliers set template vectors ff equation templates collapse center gravity input data set 
observed equation belongs class weight adaptation rules employed batch form som algorithm batch statistically firm em optimization gaussian centers mixture model 
equation shows fixed flvq equivalent fcm 
proved decreasing weighting exponent descending flvq tends reduce width learning rate distribution means model transitions 
case descending flvq satisfies kohonen second learning constraint requiring size update neighborhood decrease monotonically see section 
unfortunately due improper initialization parameter initial learning phase flvq may lead algorithm perform trivial non adaptive vector quantization 
demonstrated analysis flvq asymptotic behaviors 
asymptotic case causes trivial vector quantization 
easy verify lim gamma equation shows input patterns weighted equally category may 
flvq cluster centers collapse point input patterns mapped prototype center gravity input data set 
flvq shares fcm asymptotic behavior leading trivial vector quantization 
unfortunately equations applied iteratively unable identical centroids move away centroids converge center gravity grand mean input data set separate processing units longer adapted different degrees 
asymptotic behavior acknowledged recommended keep away infinity prevent numerical instability flvq 
analysis suggests numerical instability limiting condition causes prototype collapse flvq fcm 
asymptotic case produces hard competitive learning plus trivial dead unit relocation 
demonstrated lim neuron winner unit pattern tends zero 
consequence lim ff neuron winner unit total number times neuron best matching unit current epoch neuron winner unit unit winner pattern ng neuron winner unit input pattern neuron dead unit lim fx set fx consists input patterns featuring best matching template epoch number equation shows fcm flvq similar equivalent batch forgy means algorithm 
forgy means features singularity condition hard competitive prototype updating fx deals dead unit 
case prototype update calculated 
contrary equation encounters dead units cluster centers moved center gravity input data set 
case descending flvq implementation non recoverable collapse dead units described equation eventually occurs iteration process loss resources considered asymptotic deficiency algorithm 
model case 
descending weighting exponent kept away infinity asymptotic case value asymptotic case update neighborhood includes nodes characterized unequal excitation 
summarize asymptotic case avoided descending flvq hand width learning rate distribution decreases time model case approaches asymptotic case flvq behaves consistently second kohonen constraint 
theoretical transitions flvq learning strategy function decreasing increasing epoch time consistent regarding fcm glvq systems employing equation relative membership function see table 
analysis consistent heuristic choice recommended 
hand learning rate values necessarily decrease time flvq behave consistently kohonen constraint update mechanism roughly opposite clustering algorithms som 
example som learning rates winner non winners decrease increases vice versa flvq asymptotic equations show learning rate ff may increase winner neuron increases starting value may close see asymptotic case gamma rates tend zero see asymptotic case 
intuitively learning policy desirable provide prototypes large initial plasticity values required pursue fast rough initial learning 
ffl clarified flvq som optimize known objective function expected reach termination fcm objective function approximately minimized 
learning schemes formally derived minimize functional constant 
shown flvq updating seen special case learning schemes restricted range weighting exponent 
mean flvq minimizes functional hypothesis constant hold true flvq 
conclude despite advances field objective function minimized flvq unknown just minimized som unknown 
limitations ffl flvq minimize known objective function termination optimizing model process data 
ffl flvq affected relative membership problem high sensitivity noise low robustness 
ffl provide prototypes large initial plasticity values required pursue fast rough initial learning 
ffl flvq features instability traditional termination criterion employed gamma ffl flvq terminated 
experimental tests reveal clustering results improve convergence reached values close termination parameter ffl ignored see section 
ffl provide pdf estimation 
ffl employed topology preserving mapping 
advantages ffl owing soft competitive implementation flvq expected trapped local minima generate dead units hard competitive alternatives fcm 
ffl flvq due batch learning final weight vectors affected order input sequence traditional termination criterion removed 
ffl respect som flvq requires smaller set input parameters learning rate size update neighborhood computed directly data 
ffl flvq employed vector system 
architectural features main features flvq summarized table 
fuzzy art years art models 
art categorizes binary patterns features sensitivity order presentation random sequence 
finding led development improved art system dependent art order presentation input sequence 
adaptive hamming net ahn functionally equivalent art optimizes art terms computation time storage requirement 
art designed detect regularities analog random sequences employs computationally expensive architecture presents difficulties parameter selection 
overcome difficulties fuzzy art system developed generalization art 
means art structural problems may affect fuzzy art 
structured organization art systems subsystems termed attentional orienting subsystem 
original form art attentional subsystem employs bottom feed forward top feed backward connections easy prove module mathematically equivalent attentional subsystem feed forward connections adopted exclusively 
example ahn feed forward network functionally equivalent art 
simplification yields major consequence change meaning term resonance traditionally applied art systems 
term longer indicate basic feature art systems notably pattern matching bottom input top learned prototype vectors just term resonance applied pattern matching activities performed feed forward network som 
view term resonance employed art means art algorithms share modular architecture consisting completely generic unsupervised flat hidden layers feed forward bottomup pattern recognition network kohonen networks vq som termed attentional subsystem ii supervised unsupervised knowledge interface unit termed orienting subsystem quality unsupervised bottom pattern recognition compared top requirements expectations prior knowledge provided external environment supervisor orienting subsystem unsupervised knowledge matches external expectations resonance occurs 
means unsupervised pattern recognition activity attentional module reinforced reinforcement learning mechanism prototype adaptation takes place 
resonance occur orienting subsystem allows attentional module increase resources processing elements meet external requirements 
fuzzy art requires preprocessing stage input pattern normalization complement coding prevent category proliferation 
input data normalization loses vector length information 
complement coding normalizes input vectors preserving amplitude information doubles number network connections 
proved art systems affected design inconsistency employ inherently asymmetrical architecture perform inherently symmetrical task assessment degree match 
input parameters simplify discussion approach deal finite training data set fxg consisting input patterns dimensionality input space 
input data set repeatedly network termination criterion satisfied 
ffl fuzzy art employs parameter ff ff break ties bias function favor longer template vectors see equation 
typical ff value 
ffl requires vigilance threshold ae relative number representing external expectations 
detail ae controls neuron proliferation see equation coarser grouping input patterns obtained vigilance parameter lowered 
parameters ae ff interrelated illustrated ae decreases ff decrease 
ffl learning rate fi independent time set range see equation 
ffl requires termination threshold maximum number epochs max number times finite training data set repeatedly network 
description algorithm ffl fuzzy art attentional subsystem computes activation values set relative numbers 
activation value represents unidirectional asymmetrical degree input pattern matches template vector 
words fuzzy art activation function asymmetric respect input template vector pair provides assessment degree template vector matches input pattern 
best matching unit selected featuring largest activation value 
fuzzy art orienting subsystem overlooks pattern recognition activity performed attentional computing value unidirectional asymmetrical match function 
match value relative number representing degree winner template matches input pattern 
vigilance testing performed match value compared user defined vigilance threshold ae 
match value vigilance threshold winner template updated mismatch reset condition search process run sequence 
winner template detected activation function necessarily best matching template match function orienting subsystem inhibits winner neuron submits processing element featuring second largest activation value vigilance test 
sequence operations consisting mismatch reset condition search process repeated vigilance test passed nodes available testing 
ffl attentional subsystem pattern ng finite size training set number input pattern presentations winner unit pe detected neuron satisfies condition maxf cg activation value computed af minft ff ng af delta activation function total number neurons dimensionality input space th component vector ff user defined parameter see section 
equation inter pattern similarity measure satisfies requirement iv section considered absolute fuzzy membership function 
equation measures normalized degree pattern matches template assess reverse situation degree matches 
words activation function symmetrical respect af af 
ffl orienting subsystem fuzzy art employs vigilance test defined follows mf minft ae mf delta match function measures normalized degree template matches pattern assess reverse situation mf mf 
line af delta mf delta inter pattern similarity measure satisfies requirement iv section considered absolute fuzzy membership function 
vigilance test satisfied attentional subsystem activated sequentially adjust winner template equation 
mismatch reset condition search process activated see 
ffl fuzzy art employs wta strategy 
weight transformation law applied winner template gamma fi delta fi delta gamma note kohonen clustering networks see section fuzzy art employs cooling schedule learning rate constant time 
ffl existing neuron satisfies vigilance test new processing unit pe allocated orienting subsystem new template vector initialized 
ffl regard equations fuzzy art substitutes operators employed art activation function match function fuzzy operations intersection cardinality 
observed simpson correctly interpreted fuzzy operations operations applied fuzzy set membership values parameters pattern template vectors absolute fuzzy set membership functions 
summarize fuzzy art employs absolute fuzzy membership functions employs fuzzy set theoretic concepts 
applies soft competitive learning strategy fuzzy art termed fuzzy defined section 
limitations ffl fuzzy art minimize known objective function termination optimizing model process data 
ffl owing hard competitive implementation fuzzy art trapped local minima generate dead units soft competitive alternatives 
ffl fuzzy art order dependent due line learning example driven neuron generation 
experimental evidence theoretical analysis reveal fuzzy art sensitivity order presentation input sequence due inconsistencies detected system implementation 
ffl fuzzy art may severely affected noise points outliers may fit noise just data 
words fuzzy art may affected overfitting single poorly mapped pattern suffices initiate creation new unit noise category removal mechanism employed system 
ffl learning rate independent time fuzzy art lacks stability excessive plasticity processing new data set may affect move input space templates detected system previous learning phase 
length template decrease learning templates cycle fuzzy art sort stability 
ffl fuzzy art time consuming respect functionally equivalent implementations 
traditional implementation consisting nodes see input pattern fuzzy art search process requires minimum iterations detect winner unit features largest activation value units capable satisfying vigilance test 
art models functionally equivalent fuzzy art ahn functionally equivalent fuzzy art fuzzy sart model reduce number searches input pattern may 
curious observe criticism applied fuzzy min max clustering algorithm simpson ffl requires input data preprocessing normalization complement coding case data vector length information lost second case number network connections doubles prevent category proliferation 
ffl provide pdf estimation 
ffl employed topology preserving mapping 
advantages ffl feed back interaction attentional orienting subsystems allows fuzzy art self adjust size depending complexity clustering task 
ffl distinct sample vectors employed initialize vectors 
choice reduces risk dead unit formation may reduce computation time respect random initialization 
ffl fuzzy art employed vector system 
architectural features main features fuzzy art summarized table 
gng gng combines growth mechanism inherited earlier proposed growing cell structures synapse rule chr 
gng capable generating removing synaptic links pes gng belongs class models see section 
particular starting units generally new unit inserted adaptation steps near unit featuring largest local error measurement 
words gng employs mini batch approach see section decide locate new pes accumulating error information number pattern presentations averaging noise data gng allow single poorly mapped pattern initiate creation new unit 
anticipated development gng employ neuron insertion criterion described rule neuron removal adaptation steps unit featuring lowest utility error reduction removed 
utility measure defined increase error occuring unit interest removed set templates 
utility template vector cg total number neurons number input pattern presentations defined follows fm gamma euclidean inter pattern distance vectors presentation time second best matching template vector fm subset input pattern presentation sequence featuring template unit pe closest vector fm fx unit pe satisfies condition additional mini batch learning policy employed remove synaptic links utility progressively faded away presentation input sequence goes 
summarize gng exploits mini batch learning techniques inserting processing units deleting processing units deleting synapses example driven chr generate synaptic links strategy sensitive presence noise mini batch learning 
input parameters simplify discussion approach deal finite training data set fxg consisting input patterns dimensionality input space 
input data set repeatedly network termination criterion satisfied 
ffl gng employs user defined learning rates ffl applied winner neuron ffl ffl ffl applied neurons belonging update neighborhood 
typical values ffl ffl 
ffl parameter controls neuron generation adaptation steps see 
ffl parameters ff fi decrease error variables signals weighted heavily previous ones 
typical values ff fi 
ffl parameter max maximum age synaptic link 
typical value max 
ffl gng employs termination parameter maximum number neurons max 
description algorithm ffl line som gng employs geometrical distance relationships input measurement space enforce competitive learning mechanisms 
consider winner unit pe featuring shortest euclidean inter pattern distance number input pattern presentations second best unit pe featuring activation value 
exploitation euclidean inter pattern distance competitive learning shapes neuron receptive fields voronoi polyhedra see section 
ffl soon neuron detected local error variable winner neuron updated example tasks increase accumulated error defined gamma probability density function estimation gamma ffl line som topological relationships pes belonging output lattice employed detect soft competitive competitive cooperative effects 
particular gng applies update equation winner unit pew topologically adjacent neighbors resonance neighborhood consists pes directly connected winner synaptic link identified cg exists time 
due dynamic generation removal pes synaptic links resonance domain changes time include pes topologically adjacent winner 
behavior strictly satisfy kohonen constraint requiring size update neighborhood decrease monotonically time 
ffl templates belonging update neighborhood adapted line kohonen weight adaptation rule see equation 
update equation ffl delta gamma ffl ffl fixed user see section ffl ffl pe adjacent winner time see 
ffl regard generation pes gng employs strategy new units inserted adaptation steps near unit featuring largest local error measure identified unit particular receptive field center new unit located half way templates units identifies neuron featuring maximum accumulated error neighbors synaptic link exist 
local error new unit set decreased fraction ff 
ffl regard generation synaptic links gng employs strategy 
soon units pe pe detected chr applied pe pe connection exist generated see section 
case age connection pe pe reset zero connection refreshed 
ffl regard removal superfluous synaptic links neurons gng employs strategy 
chr applied unit pair pe pe connection refreshed age edges emanating pe increased 
connections edges age larger max removed 
results units having emanating edge units removed 
ffl input pattern presentation error variables units decreased fraction fi ii termination test checked possibly involving parameter max mean accumulated error 
limitations ffl gng minimize known objective function termination optimizing model process data 
ffl neurons feature cooling schedule learning rate satisfy kohonen learning constraint gng lacks stability excessive plasticity processing new data set radically alter maps detected system previous processing stage try simulations web address reported 
ffl easy requires user defined parameters meaning straightforward 
ffl sample vectors employed initialize vectors 
particular new neurons initialized outside input manifold increases computation time termination reached 
ffl combines mini batch learning techniques neuron generation removal synapse removal example driven generation synapses 
advantages ffl owing soft competitive implementation gng trapped local minima generate dead units hard competitive alternatives 
note gng employs soft competitive strategy update neighborhood size decrease time required kohonen constraints 
ffl due insertion strategy accumulated errors robust noise avoids overfitting 
ffl tests provided involving spiral data set patterns classes stage hybrid classifier supervised unsupervised exploiting gng layer performs better classification distributed systems literature 
excellent adaptivity gng directly tested simulations web 
ffl gng computationally efficient computation time increases linearly 
links 
ffl gng employed vector quantization density function estimation structure detection input data mapped topologically correct way output lattice pursuing dimensionality reduction 
architectural features main features gng summarized table 
fosart created overcome gng deficiencies listed section fosart line learning algorithm combines properties som gng fcm fuzzy sart algorithms 
fosart employs relative fuzzy membership function derived employed fcm fuzzy sart compute activation values gaussian basis function gbf compute absolute possibilistic fuzzy membership values see section 
generalizing soft competitive learning strategy adopted gng fosart considers output map best matching unit belongs resonance neighborhood best matching unit pe line fuzzy art fosart applies art example driven vigilance test control neuron generation 
similarly gng fosart employs new version chr termed constrained competitive hebbian rule generate synaptic links example driven basis 
line gng fosart removes synaptic links neurons mini batch parameter adaptation scheme 
summarize fosart gng belongs class models see section 
input parameters simplify discussion approach deal finite training data set fxg consisting input patterns dimensionality input space 
input data set repeatedly network termination criterion satisfied 
presentation sequence termed training epoch 
ffl fosart requires user define art vigilance threshold relative number ae 
coarser grouping input patterns obtained vigilance parameter lowered biological terms vigilance threshold ae interpreted quantity provided external environment supervisor learning system neuron proliferation 
ffl employs synapse max min length ratio threshold lr 
ffl reach termination fosart requires lower limit number training epochs node survive min parameter affecting number training epochs required algorithm reach termination consider fosart units generated removed dynamically number input pattern presentations increases 
description algorithm fosart extensively literature subjected continuous refinements current version provided 
step 
initialization 
pattern counter set 
input patterns randomly chosen input set processing elements pes pe pe generated 
pe counter set 
template vectors representing centers receptive fields equivalent voronoi polyhedra input space set equal pattern respectively 
pe local epoch counters initialized 
fosart employs pe time counters compute pe learning rates 
fosart age local time processing unit integer value equal number times finite input data set iteratively system processing unit exists 
pe local best matching counters bm bm initialized 
step 
input pattern presentation 
pattern counter increased new pattern ng network 
step 
detection processing units eligible resonant 
determine best matching unit pe second best unit pe cg pair pes featuring respectively largest second largest gaussian radial basis function value arg max arg min fd arg max arg min fd gaussian gamma oe euclidean distance input pattern prototype vector receptive field center processing unit pe presentation time equation spread resolution parameter oe computed oe ae equation considers oe inversely related user defined parameter ae intuitively relationship means number pes employed map input manifold output lattice network increases number proportional art severity threshold input space size receptive fields neural units decreases 
note gaussian feature spread parameter oe enforcing competitive learning equivalent considering neuron receptive fields voronoi polyhedra 
step 
resonance domain detection 
processing units attentional subsystem resonant orienting subsystem selects match external requirements 
units said belong resonance domain 
select units orienting subsystem employs art vigilance constraint 
vigilance test applied pattern matching activity attentional module gaussian ae ae user defined vigilance parameter ae provides model top external requirements expectations prior knowledge provided external environment supervisor 
orienting subsystem unsupervised knowledge matches external expectations resonance occurs 
means unsupervised pattern recognition activity attentional module reinforced reinforcement learning mechanism prototype adaptation takes place 
resonance occur orienting subsystem allows attentional module increase resources processing elements match external requirements 
step 
resonance condition reinforcement learning 
vigilance test satisfied resonance occurs attentional subsystem allowed reinforce pattern matching activity adjusting template vectors units belonging resonance domain 
sequence operations performed 

best matching counter unit pe increased bm bm gamma 
constrained competitive hebbian rule introduces competitive mechanism synaptic links applied 
requires ratio length computed input space see longest shortest connection emanating processing unit time cases lr lr user defined threshold see section 
processing step best second best matching unit pe pe respectively locates synaptic link identified connection exist ii ratio length computed input space euclidean distance template vectors shortest synapse emanating pe pe user defined threshold lr 
connection generated iii synapses emanating pe pe satisfy removed iv synapse local selection counter sbm initialized 

synapse exists local counter increased sbm sbm gamma 
activation values pes belonging map winner unit pe pes topologically connected winner unit computed 
activation values relative fuzzy membership values computed pc absolute membership value number pes belonging map winner unit pe presentation time equation shows computed basis absolute memberships pes topologically connected pe means pes belonging map coupled computation relative membership values pes belonging disjointed maps computationally independent 
absolute membership computed see equation section gamma gaussian 
fosart applies soft competitive learning mechanism neighborhood gamma ranking neural units belonging map best matching unit pe best ranking assigned best matching unit pe processing unit pe directly linked pe processing unit pe indirectly linked pe directly linked processing unit featuring neighborhood ranking equal 
stochastic line sequential weight adaptation law derived stochastic optimization parameters gaussian mixture model applied processing units belonging resonance domain 
unit pe topologically connected best matching unit pe adaptation receptive field center computed follows fi delta gamma ff delta ff ng fi ff ff ff ffl delta learning coefficients fi ff ffl belong range ff line kohonen second learning constraints see section adaptation step ffl expected decrease monotonically time ii neighborhood function expected monotonically decrease lattice distance units pe pe monotonically decrease spread parameter time 
inspired neural gas algorithm fosart computes ffl ffl ffl ffl ini ffl fin ffl ini min pe epoch counter presentation time min userdefined decay parameter described section variables ffl ini ffl fin computed ffl ini fflg ffl fin minfr fflg parameter ffl maximum lower limit learning rate ffl fixed human designer 
coefficient ffl monotonically non increasing pe local epoch counter monotonically non decreasing 
owing exploitation equation ffl depends set distances input pattern prototypes belonging pes connected winner 
dependency learning step size set distances input pattern template vectors relates fosart clustering algorithm 
equation term reduces overlap receptive fields expression inspired algorithm fl gammar fl neighborhood ranking node pe fl spread value computed monotonically decreasing function time fl fl ini fl fin fl ini min fl ini fl fin widely employed settings parameters fl ini fl fin 
learning coefficient monotonically decreasing neighborhood ranking pe epoch counter 
step 
non resonance condition new processing element allocation 
resonance condition satisfied resonance occur new processing unit dynamically allocated match external expectations 
pe counter increased new node pe allocated match input pattern 
consequence fosart requires randomization initial templates initial values data driven 
pe local epoch counter initialized 
pe best matching counter bm initialized 
step 
controls epoch termination 
entire input data set system operator computes remainder divided operations occur pe time epoch counters incremented superfluous cells removed pe features local counter bm pe best matching unit pattern assignment latest processing epoch removed pe counter decreased gamma superfluous synaptic links removed cg cg synapse exists features sbm synapse selected pattern assignment latest processing epoch removed pe local counters bm reset zero synapse local counters sbm cg cg reset zero 
step 
pe local counter min 
goto step 
limitations ffl fosart minimize known objective function termination optimizing model process data 
ffl fosart order dependent due line learning example driven neuron generation 
ffl combines mini batch learning techniques neuron generation removal synapse removal example driven generation synapses 
ffl fosart employs parameters defined human designer user 
parameters considered significant prior structures see section important property model hard wired built tuned experience learned statistically meaningful way 
ffl tests regarding satellite image clustering progress fosart features mean square error values larger obtained som 
advantages ffl owing soft competitive implementation fosart expected prone trapped local minima generate dead units hard competitive alternatives 
tests provided system stable respect small changes input parameters order presentation sequence 
ffl due neuron removal strategy robust noise avoids overfitting 
ffl feed back interaction attentional orienting subsystems allows fosart self adjust size depending complexity clustering task 
ffl distinct sample vectors employed initialize vectors 
choice reduces risk dead unit formation may reduce computation time respect random initialization 
ffl tests provided involving simpson data set patterns data set patterns classes digitized human face patterns iris data set patterns classes fosart performances competitive better clustering models literature vq fuzzy min max fcm flvq glvq ing 
example typical error rates unsupervised categorization iris data set mistakes 
case fosart scoring mismatches performs better fuzzy min max neural network model smallest number misclassified patterns number clusters ii line fuzzy means algorithm affected misclassifications iii line kohonen vq algorithm affected misclassifications iv class line algorithms affected misclassifications line glvq algorithms affected misclassifications 
ffl fosart computationally efficient computation time increases linearly 
links 
ffl fosart employed vector quantization density function estimation structure detection input data mapped topologically correct way output lattice pursuing dimensionality reduction 
architectural features main features fosart summarized table 
comparison clustering models synthesized table shows som gng fosart develop soft competitive adaptation strategy models satisfy definition fuzzy clustering network adopted 
paradoxically architectures fosart exploits fuzzy set theoretic concepts absolute relative fuzzy membership functions 
fuzzy art employs hard competitive adaptation mechanism 
flvq affected asymptotic 
gng fosart inherit features som importance som outstanding 
stem critical review 
fuzziness som shows network exploiting local rules derived neurophysiological evidence verify definition fuzzy clustering network proposed fuzzy set theoretic concept explicitly adopted 
effective soft competitive strategy pes mutually coupled terms competitive cooperative contextual effects architectural terms pes mutually coupled basis context feature horizontal intra layer connections 
intra layer connections biological cognitive system explicit adaptation introduced artificial neural networks 
framework multi disciplinary science artificial complex systems fuzzy set theoretic concepts may adopted model soft competitive learning mechanisms considered useful human developer may difficult justify basis objective probabilities 
useful mechanisms may identified advance neurophysiological studies biological neural systems see point 
table functional clustering algorithms features som flvq fuzzy art gng fosart network time time time rate rfs global variables rate neuron rfc rfc rfc rfc rfc variables rate rate accum error accum error age rate connection age age variables coop competitive sc sc hc sc sc rfc adaptation line batch line batch line line line rfc adaptation neuron generation example mini example strategy driven batch driven neuron removal mini mini strategy batch batch link generation example example strategy driven driven link removal mini mini strategy batch batch neuron receptive rfc rfc rfc rfc rfc field information rfs inter pattern distance eucl dist inverse peculiar eucl dist gaussian similarity measure eucl dist basis funct 
size neuron nb nb ub nb ub receptive field learning rate mtd peculiar tco tco mtd size update mtd td neighborhood objective function minimization fuzzy set theoretic concepts prior modular structure self organizing modular structure vector quantization density estimation tpm sc soft competitive cooperative competitive hc hard competitive purely competitive nb bounded ub upper bounded rfc receptive field center rfs receptive field spread absolute membership function relative membership function mtd monotonically time decreasing tco constant time changes time tpm topology preserving mapping flvq learning strategy may affected asymptotic 
fuzzy art employs similarity measurement symmetrical 
descending flvq increasing training epochs learning rate winner unit tends increase see section 
resonance neighborhood consists neurons directly connected winner 
resonance neighborhood consists neurons belonging map winner 
generation disjointed maps 
som maps different input patterns spatially segregated areas output lattice segregated areas topologically disjointed 
som performs tpm iff dimension input space larger dimension output lattice 
fuzzy clustering critical analysis contextual mechanisms employed neural network models spie proc 
applications fuzzy logic technology iii bezdek eds spie vol 
pp 

anonymous referee ieee trans 
neural networks 
mitra pal self organizing neural network fuzzy classifier ieee trans 
systems man cybernetics vol 
pp 

kohonen self organizing map proceedings ieee vol 
pp 

kohonen self organizing maps 
nd edition springer verlag berlin 
bezdek pal soft relative learning vector quantization neural networks vol 
pp 

tsao bezdek pal fuzzy kohonen clustering network pattern recognition vol 
pp 

simpson fuzzy min max neural network part clustering ieee trans 
fuzzy systems vol 
pp 

martinetz schulten neural gas network vector quantization application time series prediction ieee trans 
neural networks vol 
pp 

ritter martinetz schulten neural computation self organizing maps 
reading ma addison wesley 
generalized gabor scheme image representation biological machine vision ieee trans 
pattern anal 
machine intell vol 
pp 
july 
rose fox statistical mechanics phase transitions clustering physical rev letters vol 
pp 

rose fox deterministic approach clustering pattern recognition letters vol 
pp 

luttrell derivation class training algorithms ieee trans 
neural networks vol 
pp 

luttrell bayesian analysis self organizing maps neural computation vol 
pp 

jain dubes algorithms clustering data 
englewood cliffs new jersey prentice hall 
construction vector noisy channels elect 
eng 
jpn vol 
pp 

fontana ferrari image reconstruction improved neural gas proc 
italian workshop neural networks eds 
singapore world scientific pp 

carpenter grossberg rosen fuzzy art fast stable learning categorization analog patterns adaptive resonance system neural networks vol 
pp 

carpenter grossberg reynolds rosen fuzzy artmap neural network architecture incremental supervised learning analog multidimensional maps ieee trans 
neural networks vol 
pp 

carpenter grossberg massively parallel architecture selforganizing neural pattern recognition machine computer vision graphics image processing vol 
pp 

carpenter grossberg art self organization stable category recognition codes analog input patterns applied optics vol 
pp 

hung lin adaptive hamming net fast learning art model searching neural networks vol 
pp 

alpaydin simplified art new class art algorithms international computer science institute berkeley ca tr 
fuzzy combination kohonen art neural network models detect statistical regularities random sequence multi valued input patterns proc 
int 
conf 
neural networks houston tx vol 
pp 

fritzke growing neural gas network learns topologies advances neural information processing systems tesauro touretzky leen eds 
cambridge ma mit press pp 

fritzke competitive learning methods draft document www neuroinformatik ruhr de ini dm research 
fuzzy neural network model capable generating removing neurons synaptic links dynamically proc 
ii italian workshop fuzzy logic bari italy march eds 
singapore world scientific 
novel neural network model combining radial basis function competitive hebbian learning rule fuzzy simplified adaptive resonance theory proc 
spie optical science engineering instrumentation applications fuzzy logic technology iv san diego ca vol 
pp 

pao adaptive pattern recognition neural networks 
reading ma addisonwesley 
dempster laird rubin maximum likelihood incomplete data em algorithm royal statist 
soc 
ser 
vol 
pp 

alpaydin soft vector quantization em algorithm neural networks press 
bishop neural networks pattern recognition 
oxford uk clarendon press 
bishop williams gtm principled alternative self organizing map proc 
int 
conf 
artificial neural networks icann 
springer verlag pp 

cherkassky learning data concepts theory methods 
new york wiley 
vapnik nature statistical learning theory 
new york springer verlag 
burges tutorial support vector machines pattern recognition submitted data mining knowledge discovery 
jordan jacobs hierarchical mixture experts em algorithm neural computation vol 
pp 

jordan bishop graphical models machine learning draft document 
gori optimal learning artificial neural networks review theoretical results neurocomputing vol 
pp 

frasconi gori learning local minima radial basis function networks ieee trans 
neural networks vo 
pp 

mitchell machine learning 
new york mcgraw hill 
buhmann learning data clustering handbook brain theory neural networks arbib ed 
bradford books mit press 
serra complex systems cognitive processes 
berlin germany springer verlag 
churchland mind continuum 
cambridge ma mit press 
design evolution modular neural network architecture neural networks vol 
pp 

minsky papert perceptrons expanded edition 
cambridge ma mit press 
rumelhart hinton williams learning internal representations error propagation parallel distributed processing rumelhart mcclelland eds 
cambridge ma mit press 
martinetz schulten topology representing networks neural networks vol 
pp 

parisi la tra vita scienze dell dal eds 
bologna italy patron pp 

zheng effect concave convex weight adjustments self organizing maps ieee trans 
neural networks vol 
pp 

hopfield neural networks physical systems emergent collective computational abilities proc 
national academy sciences vol 
pp 

masters signal image processing neural networks 

new york wiley 
pai fuzzy algorithms learning vector quantization ieee trans 
neural networks vol 
pp 

integrated approach fuzzy learning vector quantization fuzzy means clustering intelligent engineering systems artificial neural networks dagli eds vol 
new ny asme press pp 
bezdek integrated approach fuzzy learning vector quantization fuzzy means clustering ieee trans 
fuzzy systems vol 
pp 

learning vector quantization review int 
journal smart engineering system design vol 
pp 

bezdek pal generalized clustering networks kohonen selforganizing scheme ieee trans 
neural networks vol 
pp 

bezdek pal pai repair glvq new family competitive learning schemes ieee trans 
neural networks vol 
pp 

model transitions descending flvq ieee trans 
neural networks vol 

krishnapuram keller possibilistic approach clustering ieee trans 
fuzzy systems vol 
pp 

dav krishnapuram robust clustering method unified view ieee transactions fuzzy systems vol 
pp 

comments possibilistic approach clustering ieee trans 
fuzzy systems vol 
pp 

ancona importance sorting neural gas training vector quantizers proc 
international conference neural networks houston tx june vol 
pp 

erwin obermayer schulten self organizing maps ordering convergence properties energy functions biol 
cybernetics vol 
pp 

williamson gaussian artmap neural network fast incremental learning noisy multidimensional maps neural networks vol 
pp 

williamson constructive incremental learning network mixture model ing classification neural computation vol 
pp 

ftp ftp sas com pub neural faq 
shih moh chang new art neural architecture pattern classification image enhancement prior knowledge pattern recognition vol 
pp 

huang fuzzy art properties neural networks vol 
pp 

geman bienenstock neural networks bias variance dilemma neural computation vol 
pp 

neuro fuzzy analysis remote sensed antarctic data proc 
ii italian workshop fuzzy logic bari italy march eds 
singapore world scientific 
fritzke growing cell structures self organizing network unsupervised supervised learning neural networks vol 
pp 

kim mitra integrated adaptive fuzzy clustering algorithm proc 
second ieee international conference fuzzy systems vol 
pp 

fritzke 
personal communication 
fritzke lbg method vector quantization improvement lbg inspired neural networks neural processing letters vol 

