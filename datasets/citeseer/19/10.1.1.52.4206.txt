maximum entropy language model integrating grams topic dependencies conversational speech recognition sanjeev khudanpur jun wu center language speech processing johns hopkins university baltimore md mail jhu edu compact language model incorporates local dependencies form grams long distance dependencies dynamic topic conditional constraints 
constraints integrated maximum entropy principle 
issues assigning topic test utterance investigated 
recognition results switchboard corpus showing small increase number model parameters reduction word error rate language model perplexity achieved trigram models 
analysis follows demonstrating gains larger content bearing words 
results compared obtained interpolating topic specific gram models 
framework extends easily incorporate forms statistical dependencies syntactic word pair relationships hierarchical topic constraints 

language modeling crucial component systems convert various language modalities including speech handwriting text 
current algorithms language modeling tend suffer acute myopia basing predictions word immediately preceding words 
humans faced comparable task easily outperform models richer linguistic information available complete context 
method exploiting long distance dependencies dynamic models topic conversation 
compact model integrates dependencies grams statistically sound manner maximum entropy framework 
models combine topic related information gram models studied 
essential idea comes information retrieval ir literature extensive weighted term frequencies discern topic genre document 
schemes exploit differences language modeling constructing separate gram models individual genre topic capture differences 
construction results fragmentation training text usual remedy interpolate topic specific gram model topic independent model constructed available data 
alternative starts similar ad hoc changes exponential model limited objective fast rescoring 
read speech similar supported national science foundation iri dynamics modeled cache notions semantic notion topic 
approach latent semantic analysis proposed refreshing departure methods presents perplexities newspaper text 
method term frequencies treated topic dependent salient features corpus just ngram frequencies topic independent salient features 
admissible model required satisfy constraints reflect sets features 
principle select statistical model meets constraints 
method advantage constraints term frequencies vary significantly topics topic dependent topic independent 
result model topic trained training data making possible obtain better estimates topic independent components model 
model small number free parameters follows consequence 
experimental results features easily include higher order gram frequencies documents particular topic syntactic word pair dependencies frequencies word classes pos constraints easy integrate unified manner framework simply features corpus model constrained match 
section contains formulation model 
issues assigning topic test utterance discussed section 
section describes experiments switchboard corpus spontaneous american english telephone conversations provides analysis results 
section comparisons method method combining topic specific ngram models linear interpolation 

combining gram topic dependencies denote vocabulary speech recognizer 
language model may viewed family wk jw wk gamma conditional probability mass functions words wk may appear th position equivalence classification hk history wk gamma trigram model wk jw wk gamma wk gamma wk gamma hk wk gamma wk gamma 

framework long range history wk gamma assign topic wk gamma conversation 
sufficient statistic history triple hk wk gamma wk gamma wk jw wk gamma pt wk gamma wk gamma intuition suggests word vocabulary strong dependence topic conversation 
estimating separate conditional pmf wk gamma wk gamma fragments training data may result poor estimates words 
additionally topic related terms may seen word context wk gamma wk gamma 
seek model addition topic independent gram constraints meets marginal constraints pt wk gamma wk gamma wk wk gamma wk gamma wk gamma wk gamma gamma gamma pt wk wk gamma wk gamma jt wk tk note marginal probabilities reliably estimated conditional ones 
unreliable marginal probabilities observations may completely left model requirements replaced turing estimates 
linear constraints form described define family choose model family highest entropy corresponding qualitatively additional assumptions maximal smoothness model 
known model exponential form parameter corresponding linear constraint placed model wk gamma wk gamma delta gamma delta gamma gamma delta wk gamma wk gamma suitable normalization constant 
numerator terms correspond standard gram constraints fourth topic unigram parameter determined particular topic 

computational issues model estimation generalized iterative scaling gis algorithm compute model parameters challenges predominantly associated computational storage needs parameter estimation procedure overcome order successfully implement language model lm incorporates topic dependencies gram constraints unified manner 
overcome increased complexity addition topic dependent unigram constraints partition training corpus topics conversation perform iteration gis algorithm updating separately part correctly combine updates 
topic division corpus reduces computational complexity order magnitude 

topic assignment test utterances issues arise topic dependent lm speech recognition 
actual spoken words available topic assignment topic assignment recognizer hypotheses 
investigate impact recognition errors process 
known topic conversation may change conversation progresses 
examine topic assigned entire test conversation utterance parts utterance 
study topic assignment utterance utterance include preceding utterances include preceding succeeding utterances 
results section 
experimental results switchboard training set models described consists nearly conversations containing total words 
conversation annotated topics ranging affirmative action topic recommended callers data collection call adheres assigned topic 
vocabulary task words includes words training test set 
performance various lms evaluated test set conversations conversation sides comprising words utterances 
test utterance list best hypotheses generated htk recognizer state clustered crossword triphone hmms gaussian mixture output densities back bigram lm 
recognition word error rate wer rescoring hypotheses average perplexity transcriptions test set reported 

baseline experiments table shows performance standard back trigram models model gram constraints 
minimum count bigram included model indicated trigram leave infrequent bigrams model gram cutoffs perplexity wer back cutoffs back table perplexity back trigram models maximum entropy models trigram constraints 
trigrams model rapid training models grams 
smaller back gram model constructed calibrate corresponding model ngram constraints 
seen gram constraints model essentially replicates performance corresponding back gram model 
improvements degradations adding topic dependent constraints may yield attributable features method 

estimation topic conditional models conversation side training corpus processed obtain representative vector weighted frequencies vocabulary terms excluding words word list words low semantic content ignored topic classifier 
vectors clustered means procedure initial cluster assignments derived manually assigned topics conversations 
resulting cluster assignment fixed conversation side remainder training process 
words unigram frequency cluster differs significantly frequency corpus designated topic related words 
choose words log word related topic roughly words topic cluster words vocabulary constitute training tokens 
models trained constraints kind words addition gram constraints 

topic assignment testing topic dependent model rescoring topic assigned test utterances 
investigate options assignment manual assignment topics conversation automatic topic assignment ii transcriptions iii best hypotheses generated recognition pass iv assignment oracle minimize perplexity wer 
results table clearly indicate source text perplexity wer topic classification baseline manual assignment ref 
transcriptions best hypotheses oracle optimal table topic assignment erroneous recognizer hypotheses causes little degradation performance 
wer small loss perplexity negligible loss wer topic assignment recognizer hypotheses correct transcriptions 
comparisons oracle indicate little room improvement 
investigated topic assignment granularities best recognition performance achieved assigning topic utterance best hypotheses current preceding utterances 
results table 
note utterance level topic text perplexity wer topic classification baseline ref 
transcriptions best hypotheses oracle optimal table dynamic topic assignment individual utterances current preceding utterances 
table effective conversation level assignment table 
adding topic dependent constraints reduces absolute wer relative perplexity 
gain insight improved performance topic assignment examine agreement topics assigned levels 
seen table utterances prefer topic independent model filler utterances probably serving vital discourse functions acknowledgments hard decision assigning closest matching topic results formalism extends easily soft topic decisions 
employ standard cosine similarity measure commonly ir community assign topic test sentences 
null topic defaults topic independent baseline model available choices topic classifier 
source agreement utt 
level topic text topic conv 
utt 
disagreeing conv 
classification level topics topic topic ref 
trans 
best hyps 
table topic dynamics viewed dis agreement utterance conversation level topic assignment 
back channel responses 
remaining utterances majority closest topic assigned 
large fraction closer topic preferred conversation level equally remarkable result cases topic assigned conversation level close second topics similar 

analysis recognition performance see improve model aim improve vocabulary divided sets words topic conditional unigram constraints topics 
word token transcription marked belonging sets perplexity calculated separately 
words recognizer output similarly marked recognition error assigned sets separating wer sets words 
tokens test set topic dependent constraints 
table shows breakdown results set language model topic words words gram cutoffs ppl wer ppl wer topic table analysis performance gains topic dep 
lm 
topic dependent independent words models topic dependent constraints 
divide vocabulary simply content bearing words words defined earlier 
partition nearly tokens test set content bearing remainder words 
table presents performance gains analyzed partition 
language model content words words gram cutoffs ppl wer ppl wer topic table performance improvement content bearing words 
clear tables gain perplexity comes predominantly content bearing words improvement wer content bearing words greater wer improvement important consideration users 

interpolated topic grams compared back trigram model parameters topic conditional models introduce additional parameters modify probabilities words context topic 
alternative modeling approach partition training data build separate gram models topic topic gram trained smaller dataset interpolate topic specific model topic independent model trained data obtain smooth topic dependent model 
comparable approach described 
construct back unigram bigram trigram models specific topic partitioning word corpus models described section 
interpolate topic specific gram topic independent trigram model obtain smooth topic dependent gram models 
usually tune interpolation coefficient held set 
case cheat choose interpolation weight minimize perplexity test set interpolated model 
table shows recognition performance interpolated models 
topic test utterance interpolated model topic model 
model gram cutoff params perplexity wer back back topic gram theta back topic gram theta back topic gram theta topic table comparison interpolated topic gram models 
may argued approach permits combine unigram constraints effective information get interpolating topic specific trigram models 
argue due systematic integration topic dependent topic independent constraints model 

topic dependent models including grams lower counts rapid turnaround experiments described preceding section compare topic models impose constraints low count bigrams trigrams 
implement topic dependent models constraints frequent grams compare best back models 
performance models shown table 
clear topic conditioning reduces absolute model gram cutoffs perplexity wer back cutoffs back topic back topic table final comparison gram topic dependent lms 
wer amount case corresponding back model topic constraints topic dependent models reduce absolute wer relative perplexity best trigram model 
improvements words twice section 

concluding remarks described language model combines topic dependencies gram constraints unified fashion provides small significant performance gains cost additional parameters 
performance improvement words significant 
small number unigrams able provide improvement information provide complementary integrated modeling ability grams 
framework extends easily combining dependencies current efforts direction exploiting syntactic structure obtained left right partial parse utterance described 
syntactic constraints provide information complements grams topic dependencies 
additional constraints word class frequencies parts speech hierarchical topic dependencies consideration order extend model derive benefits flexibility offered framework 

acknowledgments substantial assistance topic classification provided radu florian david yarowsky 
particular radu florian provided list words algorithm cosine similarity automatically assigning topics test utterances 

bellegarda exploiting local global constraints statistical language modeling proc 
icassp vol 
pp 
may 
chelba jelinek exploiting syntactic structure language modeling proc 
coling acl vol 
pp 
aug 
chen topic adaptation language modeling unnormalized exponential models proc 
icassp vol 
pp 
may 
clarkson robinson language model adaptation mixtures exponentially decaying cache proc 
icassp vol 
pp 
apr 
csiszar squares maximum entropy 
axiomatic approach inference linear inverse problems annals statistics vol 
dec 
darroch ratcliff generalized iterative scaling log linear models annals math 
stats vol 

florian exploiting nonlocal syntactic word relationships language models ph qualifying project report cs dept johns hopkins university aug 
iyer ostendorf modeling long range dependencies language proc 
icslp vol 
pp 
oct 
kneser language model adaptation dynamic marginals proc 
eurospeech vol 
pp 
sept 
martin adaptive topic dep 
language modeling word proc 
eurospeech vol 
pp 
sept 
young jansen odell woodland htk book version cambridge 
