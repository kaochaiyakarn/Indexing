predictive paradigm compression model bias human cognition mark davis fall natural social phenomena redundant underlying generator processes redundant necessity redundancy language social interactions effective communication 
systematic nature universe suggests human cognition reflect order prediction control environment take place 
compelling paradigm understanding nature human cognition memory formulated basic supposition memory learning act coordinated manner fashion models external phenomena produce accurate predictive basis understanding sensory world 
compression introduced account extraction significant invariant features senses ordered incorporation new information existing models 
paradigm described brings significant new advances statistical connectionist learning theory ecological perception paradigm existing cognitive accounts suggest complex human thinking explained parsimonious selection models internal environmental phenomena 
contents systematic universe 
models compression bias 
implicit latent data structure convergent application predictive paradigm 
psychological implications 

appendix 
appendix 
list tables minimum description length calculations polynomial generator process model 
matrix term cooccurrence frequencies 
error cross entropy bit length parameter count summed length svd sample sentences 
list figures experimental polynomial generated data th order fitted polynomial rms error scores fitted held data 
note rms error monotonically decreasing fitted data leading overfitting 
orders fitted model generalizes increasingly poorly th order fitted polynomial 
distributions errors orders 
feature matrix component magnitudes reconstructed matrixes connectivity graphs svd reconstructed feature matrix singular value 
connectivity graphs svd reconstructed feature matrix singular 
connectivity graphs svd reconstructed feature matrix fifteen singular values 
interface preliminary human tests prediction capabilities cartesian error multiple trials 
systematic universe universe simply random life possible 
thermodynamic processes concern transformation energy chemical means require ordered differentials temperature positive flow energy system self organization take place 
gibbs boltzmann formulation thermodynamics pure randomness equated maximum entropy level organization particles system 
systematicity arises thermodynamic inevitability organism ability predict environment tantamount ability survive directing organism successful foraging reproductive strategies 
prediction requires accurately modeling environment 
accuracy influenced numerous factors 
organism limited exposure environmental signal may bias organism model way generalization real range environmental expression inherently flawed 
overwhelming noise environmental signal may diminish distinguishability systematic aspects signal essentially irrelevant 
biases may arise due overfitting set data available organism resulting reduced flexibility poor generalization capacity 
expect biases minimized successful species phylogenetic history value better general predictions represent significant adaptive advantage 
details phenotypic solutions species may radically different error surface articulated accurate prediction control completely general interchangeably concept fitness numerous authors 
note specific requirement prediction necessarily computational neural process 
may equal facility associated metabolic limitations capabilities correspond accurate homeostatic regulation aspect organism physical state 
prediction control tightly coupled autonomic feedback loop critical component accurate prediction current state sensory information similarly accurate tracking prediction relative differences desired current states 
generality notion prediction central aspect ecology living things extend previous psychological theories account certain observed complexities human learning behavior 
paradigm bear resemblance direct perception theories emphasized role environment shaping arc perception action cognition differ providing detailed framework explaining precisely systematic aspects universe impress mind 
direct perception framework mind considered simple transducer environmental signals activity typically left empty center 
somewhat disregard cognitive facilities memory planning manner equally disregard role environment shaping cognitive capacities constructivist cognitive science psy done 
tall task remains providing useful conceptual framework describing shape research agenda new paradigm making predictions easily afforded psychological paradigms integrating past results domains psychological inquiry 
salient aspects new psychological paradigm treat cognition perception primarily associated environmental prediction 
paradigm immediately predictions functional biases observed human memory learning task performance 
data somewhat diffuse time appear growing body evidence predictive paradigm correct offers insights explaining range experimental results easily accommodated existing models 
initial section focuses properties statistical prediction identify biases naturally introduced predictive model construction 
second section introduces experimental evidence preliminary simulation results show direct application predictive paradigm intelligent systems 
final sections elaborate theme sketching current psychological theory new paradigm meet differ 
models compression bias alluded accurate model sensory data may biased different reasons 
model mean effective procedure predicting outcome events explaining existence patterns sensory data 
mathematically may consider sensed processes governed underlying rules laws including laws physics common patterns human interactions broad definition 
sensory data may represented set discrete observations ft xng vector observed data values cue vector xn model characterized representation relationships observations 
model derived prediction attempt correctly compute xn known values deductive inference unknown values inductive inference 
form computation generally regarded functional mapping xn observations form random variables parameterized vector parameters theta bias introduced reasons 
model may simply predicated wrong assumptions form model 
example assume reasonable parameterization model set observed fair dice rolls gamma distribution parameterization functional form model easily capable representing true state affairs uniform distributions 
second source bias model may due inadequate training data 
example handful observations dice rolls hard detect loaded dice subtle distortions loading exceed threshold noise 
observed distributions really approach underlying process sheer luck process simple limit number observations goes infinity 
small sample sizes subject study statistics 
third source bias model incorrect parameterization overfitting 
correct form model may choice parameterizations fitting observations generalize new data 
approaches problem overfitting training data primarily pattern recognition neural network literature mathematical statistics literature 
shall consider primary formulation problem rissanen minimum description length formulation 
rissanen formulation statistical articulation occam razor new formulation qualitative suggestion william occam level quantifiable generality 
way explanation consider scientist sampling phenomena measuring device generates single number reading readings taken regular time space intervals 
assume know scientist observations created fourth order polynomial gamma gamma oe term represents noise term defined gaussian random variable mean variance 
graph scientist samples process look 
bottom graph root rms error fitted polynomial model original data plotted models order 
performance model additional data points training set 
note rms error training data decreases model order increases true model evaluated additional points 
fact minimum error training data new data precisely order model generator process identical 
monotonically decreasing rms error increasing model order example overfitting training data 
model extremely fitted training data expense generalization 
model parameterization biased incorrectly choosing parameterization fitted polynomial model 
rissanen identified way capturing relationship principled manner 
approach consider problem choosing parameterization essentially identical problem communicating form collection data 
known result shannon formalizes statistical theory coding procedures communications 
put simply theory shows messages compressed encoding symbols message stream accordance relative frequencies messages sent 
symbol common want assign short code polynomial simulations conducted matrix analysis package 
detailed listings relevant code appendix experimental polynomial generated data th order fitted polynomial rms error scores fitted held data 
note rms error monotonically decreasing fitted data leading overfitting 
orders fitted model generalizes increasingly poorly th order fitted polynomial 
original fitted curves training evaluation data sets rms error fitted polynomial curves training test data cause sending code time 
morse code sent single dot requires elaborate dots dashes 
morse code efficient encoding english generally latin alphabet 
deeper english language production 
followed example 
higher order correlations simple example conditional symbol probabilities defining characteristic natural language 
dropping occurrence english probably significantly impact readability precisely common redundant information value low 
rissanen applied shannon description information problem trying communicate stream symbols generated statistical process 
method bears close similarity earlier akaike 
assumed sender receiver agreed basic form model generate received data stream 
remaining task sender encode actual data diverges predicted chosen model form transmit parameterization model 
writing equations calculating length required transmission bits error arises naturally sender model identical actual generative process error deviation component model minimized 
relationship succinctly expressed gibbs theorem log log relates process generates symbols probabilities entropy process model generates symbols probabilities 
inequality simply says model compressed optimally coded shortest possible length model probabilities exactly source probabilities 
probabilities identical sender model assign short long code particular symbols chosen code lengths won accurately scale complete encoding minimize length sent message 
put way code choices optimally compress description process 
case polynomial model described error term function distribution errors model process 
maximum likelihood formulation estimate probability error multiplying individual error probabilities ej theta probability error occurring 
assume errors gaussian distributed maximum likelihood formulation jointly distributed gaussian variance oe ej theta oe exp ae gamma oe gamma oe calculate length bits component description log function table minimum description length calculations polynomial generator process model model order log ej theta log theta theta delta log ej theta log theta theta delta log ej theta oe gamma having dealt error deviation term tell receiver parameterization model 
form similar argument error term statistics parameterization define length necessary encoding 
rarity surprise associated unusual distributions relevant random variables describe parameterization 
polynomial model consider distribution parameters case vector polynomial coefficients theta governed gaussian zero mean variance oe theta oe exp ae gamma oe thetak oe pressed ahead maximum likelihood formulation comment 
parameter component rissanen mdl measure log theta oe thetak combined expression calculate description length different possible models polynomial fitting problem 
calculation take place error statistics fitted data set hope show cross validation experiment conducted earlier held data set repeated mdl approach training set 
table shows values error term parameter length term sum 
seen error term decreases model order increases minimum description length achieved model order precisely order generating function 
result calculated exclusively training set data 
careful reader note st order result table 
reason order error distribution resemble gaussian skewed range high values 
gaussian error parameter models appropriate consideration order model line omitted simplicity 
comparison actual distributions st th order model error shown visual inspection led decision gaussian models orders 
analytical verification accuracy gaussian performed test easily performed example kolmogorov test 
similar arguments stand gaussian formulation parameter distribution 
implicit latent data structure convergent application rissanen formulation suggests compelling ground discuss veracity model 
minimum length description model parameters maximally compressed representation leads accurate models minimize assumptions invariances observed data set 
discussing psychological implications principle worth examining divergent domain research indirectly arrived 
domain interest latent semantic analysis 
latent semantic analysis lsa information retrieval counterpart latent semantic indexing automatic process attempts discover regularities feature cooccurrence patterns 
approach bears close resemblance principal component analysis factor analysis 
terms lsa uses reduced dimensionality spaces model distribution features 
choosing number dimensions subspace carefully significant features amplified relevant features smoothed 
general approach singular value decomposition svd decompose feature matrix matrixes matrixes diagonal matrix singular values correspond eigenvalues principal component analysis reconstruct feature matrix reduced singular value matrix 
singular values lay diagonals singular value matrix represent dimensions rotated subspace feature matrix magnitudes reflect relative contributions structure original feature matrix vector dimension axis 
choice number singular values reconstructing feature matrix traditionally empirical investigations relative performance reconstructed matrix specific tasks 
astute reader note conceptual similarity mdl approach elucidated earlier number singular values treated parameters formulated description length metric 
typical lsa applications surface feature matrix normalized counts term cooccurrence patterns text mutual distributions errors orders 
count count count count count count count distribution errors order error magnitude error magnitude error magnitude error magnitude error magnitude error magnitude distribution errors order distribution errors order distribution errors order distribution errors order distribution errors order distribution errors order information figures cooccurring terms 
reconstructed matrix interpreted better reflecting underlying generative process text surface counts 
lsa interpreted smoothing operation eliminates noise grossly sampling word cooccurrences independent statistical events enhances marginal distributions text generation process 
counts lsa approach attempting eliminate predictive bias text model specific prediction applications typically complex simply generalizing sample data sets 
investigate properties lsa consider sentences giraffe lives zoo 
man lives house 
gorilla lives zoo 
woman lives house 
giraffe animal 
woman human 
resort knowledge animals suspect giraffe lives zoo gorilla lives zoo man woman live house gorilla giraffe animal 
intuitions disturbed logical implication samples text suggests gorilla closely related giraffe animal specific statement concerning living zoo 
type connection just smoothing operations lsa attempt discover 
build term cooccurrence matrix sentences creating matrix normalized counts term terms elements matrix 
resulting matrix shown table 
discussion follows matrix referred feature matrix singular value decomposition sigmav composed right left singular vectors sigma diagonal matrix containing min singular values dimensions proof induction orthonormal basis sets detail 
importance singular values define orthonormal projections rotated space similar eigenvalues reconstructions pca svd tractable rank deficient data sets 
relationship factor analysis evident relative magnitudes singular values indicate contributions variance original data rotated space 
formulating mdl argument faced slightly different situation polynomial predictor earlier 
particular assuming svd reconstruction model model discrete probabilities term term cooccurrences generating function polynomial example 
parameters random table matrix term cooccurrence frequencies giraffe lives zoo man house gorilla woman animal human giraffe lives zoo man house gorilla woman animal human variables 
scalar quantities regardless choose reconstruction 
conceptually rissanen argument claim sender needs transmit error incurred number singular values number singular values receiver 
receiver polynomial case assumed access input data know basic form model case svd 
svd components invariant parameterizations svd additional length considerations errors parameterization number singular values 
error component mdl formulation need measure difference pool discrete probabilities original feature matrix probabilities predicted reconstructed svd matrix 
need determine length difference 
convenient measure case cross entropy feature matrix reconstructed matrix 
discrete cross entropy measure similarity discrete distributions equated average length bits code needed transmit symbol set 
case cross entropy performed matrixes gamma nm nm thetam nm bn thetam reconstructed matrix 
additional information needed specify number singular values simply length number bits theta log number ordered singular values 
example sentences easily perform calculation singular values 
results table 
note reconstruction uses singular values non zero entropy non zero error length component despite intuition perfect reconstruction original feature matrix incur cost error length specification 
fact cross entropy introduces additional cost cost specifying feature matrix addition specifying error 
cost parameterizations affect comparison relative lengths different parameterizations 
singular values suggested mdl calculations feature matrix 
visualize smoothing occurs different numbers singular values reconstruction examining graphs feature matrix reconstructed matrix 
histograms show magnitudes feature matrix components independent axis scaled theta components matrix 
graph original feature matrix followed graphs reconstructed feature matrixes fifth singular values 
graphs apparent degree smoothing singular values introduces feature matrix 
singular value reconstruction smoothing increasingly subtle 
functions lsa lsi appendix feature matrix component magnitudes reconstructed matrixes table error cross entropy bit length parameter count summed length svd sample sentences order error length parameter length summed length intuition animals due shared cooccurrence pattern described earlier investigated reconstructed feature matrixes 
figures show energy minimization graphs connectivity reconstructed feature matrixes articles low content words shown clarity 
relative positions square nodes indicate magnitude interactions features 
energy minimization procedure treats collected features connected springs tensions proportional values matrix elements 
dynamic simulation performed allowing elements relax 
note regularity interconnections 
hand evident convergence animal gorilla giraffe zoo right graph human house man woman share left portion graph 
lives positioned sides shared context animal human groupings 
original feature matrix comparison 
footnote section worth noting artificial neural network research converged concerning description length bias overfitting 
overfitting continuous theme ann research 
typical ann training consists training cross validation phases establish generalizability network 
ann research studies overfitting variety mechanisms enhancing generalization including weight decay regularization occam factors early stopping soft weight sharing pruning algorithms graphs prepared dot available www att com connectivity graphs svd reconstructed feature matrix singular value 
giraffe lives zoo man house gorilla woman animal human connectivity graphs svd reconstructed feature matrix singular 
giraffe lives zoo gorilla woman animal human man house connectivity graphs svd reconstructed feature matrix fifteen singular values 
giraffe lives zoo animal man house gorilla woman human specific examination overfitting generalization vapnik chervonenkis analysis 
hinton implemented algorithm directly implements mdl called wake sleep algorithm 
predictive paradigm point purely examination principles compression ties ideas overfitting generalization 
seen numerous pattern recognition theories theories text retrieval learning point utility generality principle 
things concrete sketch predictive paradigm look basic assumptions 
want careful state dimensions avoid suggesting applicable human mental facilities text retrieval systems neural network classifiers 
shall consider models predictive systems pss purposes understanding doing inherently neglecting profound differences depth complexity intelligent behavior systems 
assumption ps operates maximize accuracy predicting real system 
abstractly refer ps system takes input establishes model produces possibly null output triggered model 
input considered set symbols ft associated cues fx necessary implications type range symbols discrete continuous stationary non stationary countably finite infinite 
model mapping cues simulations symbols output set symbols fo transformation input symbols cues model output symbols take imaginable form represented relation maps model states output symbols ff special restrictions placed form relation 
predictive paradigm asserts ps minimizes mismatch input symbols model symbols special care model maximally compressed avoid biases predictions 
consider length operator measures length logarithm function statistics model arrive formulation ps 
predictive system ps function xn 
ow maps sets input cues input symbols combined internal parameters predictions symbols output symbols 
ps constrained min err xn err maps differences model predictions input symbols real numbers 
accuracy heuristic imposed constraint condition fundamental importance real pss condition represents optimum may achieved practice 
anticipate varying degrees constraint observed influencing real pss systems constraint accurately predict environment competed systems phylogenetic history 
paradigm teeth examine properties real pss search experimental evidence support compression accurate predictions play role cognition 
psychological implications formulation useful consider implications model psychological theories 
primarily differences associated structure research agendas predictive paradigm versus contending models 
focus new paradigm fundamentally derived perspective treats cognition mechanism modeling environment 
modern theories perception broadly classified direct constructivist assumptions relative contributions theory ascribes mental operations interpreting sensory data 
direct ecological theories perception tend dismiss claim understanding perceptual artifacts highly mutable plastic subject full range variability 
theories focused invariant nature certain aspects environmental stimuli due natural constraints physical observation faced 
mentioned unfortunate side effect certain types explanation throwing mental baby came tendency postulate extremes learnability 
requirement situ experimentation establish database real world observations probably diminished force direct perception movement essential lesson remains important component predictive paradigm 
specifically expect predictive system models approximate order form process observed 
certainly necessity distinguish important unimportant stimuli observing chaotic dynamics waterfall doesn mean observer mind abstracts complex dynamical simulation waterfall 
level detail necessary maintain model outweighs potential benefits knowing intricate interactions involved large 
stimuli specific interest creating detailed models 
human social interaction communication candidates 
surprising find accuracy human predictions human behaviors fact better predictions physical phenomena 
lines kahneman considered somewhat informative 
base rate errors observed kahneman tversky surface predictions poor payoffs 
distortions predictions due called representativeness heuristic necessarily poor predictions general typically reflect preoccupation prototypical features people 
predictions simply tuned social sphere incorrect 
amounts vitro versus situ criticism results kahneman tversky doesn directly suggest specific value predictive paradigm 
potential experimental scenario carefully elicit predictive capabilities human minds involve complex predictive task isn polluted specific real situations people 
subjects asked predict position ball having seen snapshots earlier trajectory 
ball position fact governed noisy polynomial relationship earlier easy verify series additional predictions better described relationship order 
preliminary version experiment carried single year old female subject 
user interface created tcl tk 
snapshot interface shown 
position ball experiment governed third order polynomial gamma gamma results subject shown 
regions indicated bottom error graph individual trials subject 
note error grows longer ball hidden 
experiments extremely preliminary simply suggest potential methodology examination predictive error 
subject experiment source code available request 
interface preliminary human tests prediction capabilities report difficulty moving pointer notebook trackball may impact results 
expectation subjects expanded version experiment get better experienced generating function despite fact masked region available inspection crossvalidation possible 
variable aspects experiment noise magnitude projectile speed 
structure episodic semantic memory candidate examination 
anderson schooler suggested textual structure degree patterns observed retention recall words texts 
anderson constructed needs function closely correlated log frequency word occurring text previous pattern occurrence 
function shared remarkably similar properties observed retention rates human subjects letter triplets 
specifically needs function retention functions power law relationships 
hope see similarity statistics environmental stimuli significance attached stimuli memory storage 
see distinct advantages storage mechanisms enhance cartesian error multiple trials relationships semantic primitives relationship significant minimization criteria 
appears evidence suggests memory encoding highly compressed 
example ratcliff mckoon derived theory minimal encoding experimental evidence speed response reading experiments 
semantic primitive encoded way minimal inferential associations directly tied 
death encoded bad 
notion minimal encoding lends strong support predictive paradigm view memory operate death example broad range contexts metaphorical instances analogical linguistic constructs 
real constraints surface level syntactic constraints combined broad associated terms imagery 
simple expression bad 
minimal encoding seen evidence predictive paradigm human memory groundwork understanding relatively plastic nature language meaning extraction 
predictive paradigm offers broad insights prediction control intelligent organisms 
additional areas potential application discussed report due lack space 
example child language acquisition including ability children segment vocal stream individual lexemes requires selection construction numerous linked models language 
rates language acquisition appear radically range example utterances instances corrective feedback child receives 
inferences necessary acquire com plex linguistic abilities possible parsimony condition similar minimization restriction 
de marcken shown segmentation vocal stream accomplished high rate accuracy minimum description length framework 
continued development safely expect interesting effective insights arise predictive paradigm 
appendix polynomial simulations conducted freeware matlab matrix manipulation package available numerous ftp sites 
primary author ian searle eskimo com 
described complete set curve fit mdl functions completeness 
function func start step local sum zeros rand normal start eval func rand step return function local total total zeros nr nr total total nr total total return total function order local jp zeros order zeros order order jp order calculate matrix nr jp jp jp calc 
matrix order nr jp jp jp return solve function local sum hist incr zeros nr nr int error magnitudes count log distribution error log rms function local sum sum nr sum sum sum sum nr return sqrt sum function func start step num num order local func start step num order return function func start step num num order local func start step num order generate additional num points function func start step num step num ym max diff ym min rms rms rms rms subplot axis axis st line st line st st original st fitted st sprintf rmss rms rms sprintf os order original fitted curves func nn order os rms rmss plot start step num step num ym diff subplot func subplot func return rms rms function func local func st fitted data st evalu 
data st mean error st order model log error log error fitted polynomial curves training test data plot return function func local zeros func nr nr error magnitudes count sprintf rm mean std sprintf rd sprintf sprintf distribution errors orders mean rm min max min variance rd min max min return function func local zeros func nr nr subplot sprintf error magnitudes order count distribution error order return mdl function order local stddev var err total fx val sum local pvar pm second sum nr err sum sum err stddev std err mean err var stddev write stdout order write stdout stddev write stdout var total var sum write stdout total nr parameter distribution handling std pm mean pvar write stdout write stdout pm write stdout pvar assume gaussian structure parameter vectors sum nr sum sum pvar sum return total function func local func mdl log plot return function return appendix lsa code provided completeness 
lsi function local indexes values svd indexes sort sigma ind values sort sigma val zeros sigma nc sigma nc sigma nc indexes indexes values return vt function local indexes values svd indexes sort sigma ind values sort sigma val zeros nr nr zeros sigma nc sigma nc sigma nc values indexes indexes values vt log error length entropy averages st error entropy average error length st parameter length st combined length number singular components entropy st plot return function return function file structure term occurrence files mxm matrix terms terms normalized counts elements local tmp tmp file return tmp function local en en nr nc en en log return en function local vec nr nc vec vec st point st plot vec function local return function start local ind start subplot original values feature matrix vectorized matrix index value ind start subplot ind vectorized matrix index value reconstructed feature matrix num str singular values lsi ind ind bibliography akaike 
entropy minimization principle 
applications statistics 
north holland 
anderson schooler 
reflections environment memory 
psychological science 
atmar 
rules nature simulated evolutionary programming 
proceedings annual conference evolutionary programming 
baum haussler 
size net gives valid generalization 
neural computation 
best 
cognitive psychology 
west publishing 
bishop 
neural networks pattern recognition 
oxford university press 
de marcken 
unsupervised acquisition lexicon continuous speech 
technical report mit 
deco 
information theoretic approach neural computing 
springer verlag 
deerwester dumais furnas landauer harshman 
indexing latent semantic analysis 
journal american society information science 
golub van loan 
computations 
johns hopkins university press 

factor analysis 
saunders 
hinton dayan frey neal 
wake sleep algorithm unsupervised neural networks 
science 

principal component analysis 
springer verlag 
kahneman tversky 
psychology prediction 
psychological review 
papoulis 
probability random variables stochastic processes 
mcgraw hill 
ratcliff mckoon 
memory models text processing retrieval 
editors varieties memory consciousness essays honor tulving 
erlbaum nj 
rissanen 
modeling shortest data description length 
automatica 
