local induction decision trees interactive data mining fulton cs jhu edu simon kasif kasif cs jhu edu steven salzberg salzberg cs jhu edu david waltz waltz research nj nec com decision trees important data mining tool applications 
classification techniques decision trees process entire data base order produce generalization data subsequently classification 
large complex data bases amenable global approach generalization 
explores methods extracting data local query point local data build generalizations 
adaptively constructed neighborhoods provide additional information query point 
new algorithms experiments algorithms described 
keywords local learning decision trees data mining 
computer science dept johns hopkins baltimore md nec research institute princeton nj large complex body data need compute summaries extract generalizations characterize data 
data mining research focuses processing large databases computing summaries detecting patterns performing automatic classification new data 
example modern medical databases contain enormous quantities patient data provide great value treatment patients 
order gain maximum value databases data mining tools essential 
popular successful data mining technique decision tree classifier qui classify new examples providing relatively concise description database 
describe notion interactive data mining wait user provide query specifies neighborhood mined 
query form specific example space instances database 
produce information may contain classification point confidence classification summary visual display local neighborhood point comparison neighborhood additional local information 
approach intended address statistical problem known decision tree research community data fragmentation 
occurs large databases tree induction algorithm recursively splits data smaller smaller subsets 
leaves decision tree little data available classification decisions 
specific query point retrieve locally relevant instances database able build better classifier 
describes local approach building decision trees collecting data vicinity query example building tree fly 
intuition simply new example classification retrieve set relevant examples database build decision tree examples 
approach potential circumvent data fragmentation problem decision sufficient relevant local information 
idea building local decision trees similar spirit standard nearest neighbor algorithm 
ideas face important implementation question decide appropriate local neighborhood 
problem addressed algorithms described approaches differently 
explores sophisticated methods choosing neighborhood 
databases domains importance certain features varies part space 
example feature blood pressure great importance part medical database genetic factors important 
capture notion formally devised algorithm defines adaptive neighborhood local characteristics data 
extends usual notion distance domain dependent query dependent 
notation section give definitions necessary algorithm descriptions 
length consideration omit formal notation 
set instances called instance space 
simplify presentation assume set points multi dimensional unit square large set instances subset database objects 
machine learning referred training set 
instance associate class label 
typical statistical classification problem classify new instances contained decision trees established useful tool classification data mining summarize database produce classification new examples 
geometrically decision tree corresponds recursive partitioning instance space mutually disjoint regions 
region represented leaf node tree associated particular value giving class label instances contained region 
important note geometrically leaf label corresponds hyperrectangle simplicity refer hyperrectangles rectangles 
define monochromatic rectangle rectangle contains points labelled class label 
local induction algorithms memory reasoning local induction algorithms simple idea 
building complex statistical model describes entire space construct simpler model describes space particular neighborhood 
local learning special case memory reasoning mbr 
applications mbr include classification news articles census data software agents mk computational biology yl cs robotics ams atk mas computer vision ede pattern recognition machine learning applications 
statistics addresses issue adaptive neighborhood query improve nearest neighbour algorithms ht fri 
see relevant theoretical framework local learning described bv vb 
key steps local learning mbr algorithms instance retrieve set instances training set relevant query build model classifier function approximator retrieved points model process query point classify approximate function value 
interactive data mining applications notion local learning extended provide visual statistical query dependent information 
particular local neighborhood features may sufficient produce classification entire domain may need complex classifier 
cases user may care query specific accuracy estimate accuracy global classifier see 
choosing appropriate local neighborhood query point difficult task especially high dimensional spaces 
report new approaches primarily designed complement standard decision tree algorithms interactive data mining applications 
methods useful database large user interested exploring small neighborhood query point 
choosing local neighborhood nearest neighbors simplest approach local learning decision trees simply extracting nearest neighbors constructing decision tree instances 
neighborhood size decision tree algorithm equivalent nearest neighbor nn algorithm 
neighborhood size full size training set algorithm equivalent conventional full induction 
nuances training set greatly affect histogram accuracy different values standard nearest neighbor algorithms optimal value determined specially reserved training set cross validation techniques 
clear best neighborhood size may vary query point 
algorithm robust voting scheme follows 
query point sequence trees induced nearest points 
trees vote class query point 
call local induction voting liv algorithm 
practice shown experiments 
voting method implemented different ways experiments ad hoc method 
weigh vote tree number class examples leaf classify example 
choosing local neighborhood layers composite rectangles sake gaining intuition algorithm described section assume target partitioning space decision tree query point contained leaf node monochromatic rectangle implies point forms monochromatic rectangle corners 
words points database contained rectangle defined class label 
remove points database form monochromatic rectangles points regions adjacent important classification refer points database form monochromatic rectangle query point layer 
points superset points remove points layer refer remaining points form monochromatic rectangles query point layer forth 
algorithm defines adaptive neighborhood query point points layer layer 
algorithm 
dimensional example illustrates idea geometrically 
algorithm described running time size database 
turns utilize computational geometry techniques reduce running time log gamma obtaining practical improvement dimensionality small 
provide rough sketch algorithm dimensions 
see complete description 
point compute set monochromatic rectangles defined point database 
recall points include local neighborhood sort points quadrants query point defining origin 
compute set monochromatic rectangles separately quadrant 
note number non empty quadrants bounded irrespective dimensionality 
determine point database dominates point different class excluded consideration 
point dominates point jx jy 
call type dominance monochromatic dominance extends standard notion dominance ps 
randomly choose dimension project points dimension 
compute median projected points median defines separator vertical line recursively solve dominance separately points larger median local decision tree local neighborhood defined layers homogeneous rectangles 
points layer layer rectangles shown local tree superimposed 
query point point database check form rectangle include layer 
remove points layer 
repeat procedure construct second layer 
set points layers 
construct decision tree points 
multi layer local decision tree algorithm query point initial neighborhood query point 
find local neighborhood neighborhood search 
induce decision tree local 
classify query point neighborhood search expand neighborhood unblocked dimensions obstacle point reached 
dimensional branch caused obstacle point recursively call neighborhood search 
return highest scoring neighborhood branches 
adaptive boundary local induction algorithm smaller median 
project points line defined median 
climbing line check monochromatic dominance points left median points right median 
algorithm running time log 
choosing local neighborhood adaptive boundary third algorithm inspired method hastie tibshirani ht defined technique creating ellipsoidal neighborhood query 
implemented iterative search rectangular neighborhood query 
obvious greedy algorithm start hyperrectangle query point expand outwards enclosed region violated constraint 
reasonable constraint place neighborhood remain linearly separable examples contained rectangle classified single hyperplane 
expansion reaches obstacle point inclusion violate constraint limit growth dimension 
exploring possibilities obstacle point create branch search tree 
algorithm sketched 
size search tree exponential number dimensions feature space experimented greedy approximation algorithms see 
experiments section focus performance local induction voting algorithm scientific domains breast cancer diagnosis star galaxy classification identification coding regions dna 
performed experiments algorithms artificial datasets 
dim astronomy dataset contains examples dimensions classes stars galaxies occur approximately equal frequency 
human dna dataset contains approximately examples dimensions classes coding noncoding occur unequal frequency 
example represents piece human dna bases long comes exon coding region part num num prune full induction liv data set train test accuracy accuracy human dna human dna star galaxy star galaxy breast cancer table comparison full induction local induction voting 
dna genes produce proteins intron noncoding region 
noncoding dna times common coding dna data set common human genome sake experiments different training sets size constructed containing equal numbers coding noncoding regions 
results liv algorithm datasets appear table 
accuracy reported dna data set unweighted average class accuracies higher frequency noncoding dna want swamp estimate accuracy coding dna average 
experiments decision tree induction algorithm implemented standard axis parallel splits information gain goodness criterion 
algorithm similar quinlan qui 
decision trees pruned standard pruning portion training set set aside pruning set 
purpose experiments determine local induction voting improvement full induction 
graphs section show accuracy function parameter varies 
size training points largest local window query point 
point abscissa window sizes contribute vote classification query point 
general accuracy increases plateaus increased 
accuracy decision tree induced full training set shown horizontal rule graph 
accuracy local induction voting method typically surpasses accuracy full induction relatively small value databases experimented sizes training set 
intent differently sized training sets determine local induction better suited sparse training sets larger training sets 
artificial data sets perform detailed experiments multi layer algorithm adaptive neighborhood algorithm 
datasets generated creating random decision tree approximately nodes dimensions 
randomly generated instances 
instance classified true decision tree 
standard decision tree algorithm nn new methods classify data 
multi layer algorithm dimensions exhibited similar performance standard decision tree algorithm outperformed nn 
dimensions points obtained better results multi layer algorithm 
points results similar methods 
adaptive neighborhood algorithm computationally expensive needs tuning effectively large datasets 
average class accuracy maximum value voting full induction achieves accuracy star galaxy dataset training points average class accuracy maximum value voting full induction achieves accuracy star galaxy dataset training points unweighted average class accuracy maximum value voting full induction achieves accuracy hum dataset training points unweighted average class accuracy maximum value voting full induction achieves accuracy hum dataset training points accuracy local induction voting algorithm compared full induction hum star galaxy datasets 
graphs left show performance large training sets right small training sets 
number query points distance boundary correct classifications number query points distance boundary incorrect classifications frequency distances query points closest decision tree boundary star galaxy dataset 
local error analysis useful data mining tool provided decision trees local global error analysis 
histogram numbers query points close decision tree boundaries standard algorithm star galaxy data set 
query compute bin histogram report confidence prediction query distance boundary 
note reveals difference distances correct incorrect points boundary incorrectly classified points closer average boundary 
described new algorithms local induction decision trees reported initial experimental data techniques 
algorithm local induction voting simplest far best experiments 
multi layer composite neighborhood algorithm complex especially efficient log gamma version 
main design goal battle data fragmentation provide intuitive feel distribution points local region query point 
query point enumerate monochromatic rectangles include 
distribution rectangles helpful constructing new confidence measures classifiers 
third method created axis parallel adaptive neighborhoods far shown inferior experimental performance reported needs refinement 
interesting potential applications concept boundaries locally 
main drawback algorithms reported time perform single classification 
running time local induction voting query similar practice standard nn algorithms dominated size training set 
situations medical diagnosis accuracy utmost importance worth extra computation identify accurate method 
experiments performed indicate local induction voting outperform full induction terms accuracy 
similarity graphs accuracy vs experiments different databases indicates local induction voting behave similarly databases 
local induction voting potentially candidate improving classification tools general 
ams christopher atkeson andrew moore stefan schaal 
locally weighted learning 
submitted publication april 
atk atkeson 
local models control movement 
neural information processing systems conf 
breiman friedman olshen stone 
classification regression trees 
wadsworth 
group 
bv bottou vapnik 
local learning algorithms 
neural computation 
masand smith waltz 
trading mips memory knowledge engineering 
communications acm 
cs cost salzberg 
nearest neighbor algorithm learning symbolic features 
machine learning 
ede edelman 
representation similarity chorus prototypes 
minds machines 
fulton kasif salzberg waltz 
local induction decision trees interactive data mining 
technical report johns hopkins university baltimore md 
fri friedman 
flexible metric nearest neighbor classification 
technical report stanford university statistics dept november 
ht hastie tibshirani 
discriminant adaptive nearest neighbor classification 
technical report stanford university statistics dept december 
mas moore atkeson schaal 
memory learning control 
artificial intelligence review appear 
mk maes kozierok 
learning interface agents 
proc 
eleventh national conf 
artificial intelligence pages washington 
mit press 
sreerama murthy simon kasif steven salzberg 
system induction oblique decision trees 
journal artificial intelligence research august 
masand waltz 
classifying news stories memory reasoning 
proceedings sigir pages 
ps preparata shamos 
computational geometry 
springerverlag new york ny 
qui quinlan 
programs machine learning 
morgan kaufmann publishers san mateo ca 
vb vapnik bottou 
local learning algorithms pattern recognition dependency estimation 
neural computation 
yl 
yi lander 
protein secondary structure prediction nearest neighbor methods 
journal molecular biology 
