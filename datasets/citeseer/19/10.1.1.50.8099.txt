media lab perceptual computing learning common sense technical report nov revised jun coupled hidden markov models modeling interacting processes matthew brand brand media mit edu media lab mit www media mit edu brand ames street cambridge ma usa methods coupling hidden markov models hmms model systems multiple interacting processes 
resulting models multiple state variables temporally coupled matrices conditional probabilities 
introduce deterministic cn approximation maximum posterior map state estimation enables fast classification parameter estimation expectation maximization 
heads dynamic programming algorithm samples highest probability paths compact state trellis minimizing upper bound cross entropy full combinatoric dynamic programming problem 
complexity cn chains states apiece observing data points compared tn naive cartesian product exact state clustering stochastic monte carlo methods applied inference problem 
experiments examining training time model likelihoods classification accuracy robustness initial conditions coupled hmms compared favorably conventional hmms energy approaches coupled inference chains 
demonstrate compare algorithms synthetic real data including interpretation video 

hidden markov models hmms popular probabilistic framework modeling processes structure time 
clear bayesian semantics efficient algorithms state parameter estimation automatically perform dynamic time warping signals locally squashed stretched 
hmm essentially quantization system configuration space small number discrete states probabilities transitions states 
single finite discrete variable indexes current state system 
information history process needed inferences reflected current value state variable 
interesting systems composed multiple interacting processes merit compositional representation variables 
typically case systems structure draft review 
brand time space 
single state variable markov models ill suited problems 
methods coupling hidden markov models capture interactions 
chains coupled matrices conditional probabilities modeling causal temporal influences hidden state variables 
coupled hmms densely connected inference graphs efficient exact inference algorithms known principally implicit state space exponential number state variables introduce deterministic approximation maximum posterior map state estimation enables fast classification parameter estimation expectation maximization 
obtain upper bound cross entropy full combinatoric posterior minimized subspace linear number state variables 
heads dynamic programming algorithm samples highest probability paths compacted state trellis complexity cn chains states apiece observing data points 
interesting cases limited couplings complexity falls 
naive cartesian product exact state clustering stochastic monte carlo approaches coupled hmm inference problems involve combinatoric number states typically requiring tn computations 
hmm modeling practice fact optimization naive strategy definition fails model interprocess interactions usually consists state counts overfit data 
maximum entropy projection coupled hmms cartesian product hmms supports estimation classification 
heads projected hmms compare favorably conventional hmms reduced complexity energy coupling methods mean field approximations 
demonstrate compare algorithms synthetic real data including interpretation video 
experiments real simulated data chmms trained fastest modeled best classified accurately performed robustly sensitivity analyses 
heads algorithm readily lends variations chains different state structures time scales degrees influence superimposed outputs 

hmms markov representation short review hidden markov models 
readers familiar model may wish skip section page find key useful 
extended tutorial available rabiner 
hmm defined set discrete states fa time indexed discrete variable state state transition probabilities ijj js gamma prior probabilities state output probabilities state 
output probabilities usually drawn continuous exponential distribution gaussian sigma gammad gamma sigma gamma gamma sigma coupled hidden markov models state variable conditional probability table exponential state label probabilistic inference graph time slices hmm output variable path state trellis state hmm time slices joint probability coupling hmms key interpreting graphical notation article 
probabilistic inference graphs convenient way representing independence structure distribution random variables smyth article 
graphically hidden markov models depicted rolled time state variable new value time slice 
tree structured graphs node independent ancestors sole parent 
hmm states independent past states state outputs independent states 
th assertion state context needs continued inference essence markov property 
markov property mandate single state variable single parent classical markov models formulated way traditional boundary practicable impracticable inference graphs 
hmms classification ways sequence observations fo know model collection hmms sequence states model 
bayes rule argmax argmax ojs numerator prior probability state sequence gamma js js js gamma gamma js js js gamma markov property js gamma brand viterbi state transition searches state trellis state hmm path probability state ff fi probability transition 
times joint density observations sequence ojs js markov property denominator usually unknown presumed constant sequences ignored 
seek state sequence maximizes numerator 
substituting forms equation obtain tractable statement state estimation problem argmax js gamma exploring state trellis shown 
column corresponds possible states time slice 
path trellis multiplies probabilities associated traversed state transition 
path brute force search possible paths dynamic programming provides recursive tn procedure incomplete path gamma jo leading state time gamma path leading state time jo argmax delta ijj delta gamma jo state elects continue best paths previous time slice 
full state sequence argmax jo 
viterbi algorithm forney 
replace argmax consider sum likelihood possible paths obtain jo probability model observation jo summing obtains forward variable ff joint density observations time similarly backward coupled hidden markov models variable fi joint density observations time values estimate probability transition observation gamma ff gamma ijj fi re estimate conditional transition probabilities ijj gamma ff gamma fi gamma baum welch re estimation baum 
forward backward analysis provides expected state values observations model parameters reestimation adjusts parameters maximize model likelihood state probabilities observation 
form expectation maximization procedure repeated convergence finds hmm parameters corresponding local maximum model likelihood dempster 
efficiency training procedure hmms attractive applications time series modeling classification 

multiple channels versus multiple processes markov property holds current state context needs modeling states independent past states 
markov models realized representation context limited single state variable 
accommodate multiple channels data hmms usually formulated multivariate output variables 
multiple processes generating channels hope processes evolve lockstep variation modeled noise 
variation carries information processes interact hmms may prove inappropriate models context limited single variable distinct state reserved possible combination signals channels 
cartesian product solution rapidly untenable 
variations hmm structure aimed multiple channel modeling include neural networks outputs baldi chauvin inputoutput models bengio frasconi state mixture models 
traditional markov formulation single process dynamic 
signals systems multiple processes common interesting modeling problems 
nearly signal produced human behavior usefully decomposed group interacting processes interacting articulatory centers speech production multiple levels control motor behaviors gesture action 
modeling system single lumped process brand may yield recognition rates performance ceiling broken considering system detail apart process structure multiple state variables 
naive solution mentioned modeling large hmm having cartesian product possible process states rarely satisfactory 
computational cost prohibitive parameters leads overfitting insufficient data large number states usually resulting undersampling numerical underflow errors 
retreating smaller state spaces problem somewhat introduces new problems underfitting states distinct fused 
case interaction crudely tabulated modeled 
correct number states vast amounts data large hmms generally train poorly data partitioned states early incorrectly training markov independence structure ensures data shared states reinforcing mistakes initial partitioning 
systems multiple processes states share properties emit similar signals markovian framework works systematicity 
hmm model system principle practice simple independence structure liability large systems systems compositional state 
models compositional state representations offer conceptual advantages parsimony clarity consequent computational benefits efficiency accuracy 
graphical notation construct various architectures multi hmm couplings offering compositional state various assumptions independence 
markov property conserved basic assumption markov models violated require new inference estimation procedures 
algorithms follow directly regularities inference graph consider varieties coupling selecting algorithmic development 

varieties couplings basic kinds couplings obtainable extending hmm framework 
weakest independent processes nominally coupled output superimposing outputs single signal 
generally known source separation problem signals zero mutual information overlaid single channel unrelated voices cocktail party 
true couplings processes dependent interact influencing states 
generally known sensor fusion problem multiple channels carry complementary information different components system acoustical signals speech visual features lip tracking stork 
source separation done dependent processes risk splitting hairs probably appropriate interesting cocktail parties 
coupled hidden markov models independent dependent inference graphs couplings hmms chains rolled time slices 
case nominal couplings possible simultaneously factor signal component processes model process independently 
come known factorial hidden markov models fhmms 
clear representational advantage hmms model processes states require hmm joint states typically intractable space time 
fhmms tractable space nc states inference problem equivalent combinatoric hmm exact solutions intractable time 
tractable approximations available mean field theory statistical physics williams hinton parisi 
ghahramani jordan ghahramani jordan developed elegant structured mean field approximation inference graph partitioned subgraphs tractably estimated exact methods forward backward analysis independent hmms 
free energy function defined graph 
severed links replaced coupling terms energy function 
mean field approximation alternately minimizes energy function re estimates subgraphs 
principle mean field approximation train virtually elaboration hmm structure time simply severing appropriate links 
strong varied interactions severed links approximation quite poor 
conventional hmms excel processes evolve lockstep fhmms meant processes evolve independently 
problems tend lie extremes 
processes may interact wholly determining voices polyphonic music players tennis game arm motions gesture 
process internal dynamic influenced possibly causally 
process may influential depending state tennis playing net substantially constrains range returns player brand chmm varieties couplings dependent processes 
output nodes omitted 
playing constraining 
variety inference architectures proposed model phenomena 
classes graphs license special algorithms appropriate certain kinds problems shall discuss briefly 
shows graph joint probabilities synchronous states encoding state pairs occur frequently 
may call inference graph linked hidden markov models equivalent cartesian product hmms bias probability joint state 
independence structure suitable expressing symmetrical synchronous constraints fact rare see tennis opponents playing net time 
saul jordan tn exact algorithm training equivalent chain boltzmann machine decimation method statistical mechanics marginal distributions singly doubly connected nodes integrated 
limited class graphs recursively decimated obtaining correlations connected pair nodes 
includes chain ladder graph shown 
shows cascade synchronous conditional probabilities ordered hierarchy hmms meant model constraints imposed master processes slave processes 
hidden markov decision trees jordan structured mean field approximations break graph hmm decision tree subgraphs 
independence structure suitable representing hierarchical structure signal example baseline song constrains melody constrain harmony 
general inference architectures synchronous links assume lockstep processes causal temporal influences 
capture interprocess influences time coupling bridge time slices conditional probabilities 
arguably offers strongest model interprocess influences 
intuitively empirically appropriate processes influence asymmetrically possibly causally tennis going net drive opponent back weak serves pull coupled hidden markov models forward shall call architecture generalizations coupled hidden markov models chmms 
inference chmms time complexity cartesian product hmms exact algorithms attractive 
graphical analysis independence structure chain chmm reveals exact maximum posterior map inference tn computation jensen kjaerulff smyth 
years researchers adopted monte carlo sampling methods estimation problems class chmm inference graphs called dynamic probabilistic networks dean kanazawa 
algorithms improve random sampling discarding weighting varying sample state sequences posteriors henrion fung chang kanazawa respectively 
space time complexities typically exponential number simultaneous state variables binder 
algorithms consistent converge asymptotically true distribution known efficient produce best estimates order computation 
turn developing deterministic tn algorithm approximations state parameter values chmms 

chmm posteriors posterior state sequence fully coupled chain hmm fcg jo prior st output outputs js gamma transition couplings indexes chain probability output state chain js gamma probability state chain previous state chain article concentrate couplings dramatic gain modeling power expected 
write posterior js gamma js gamma js gamma js gamma 
coupled hidden markov model mild misnomer key restriction associated markov models 
markov property preserved name helps proliferation unrelated names related models 
brand denote states observations opposing hmms 
important feature kind distribution graph licenses efficient algorithm constructing sampling reduced complexity state trellis develop section 

heads dynamic programming dynamic programming engine heart expectation maximization algorithms including classic hmm procedures calculating model likelihoods forward analysis estimating states viterbi analysis 
dynamic programming allows collect statistics exponential number possible paths hmm state trellis polynomial time evidence paths share state time may combined loss information 
paths explored tracking path heads 
collect statistics estimation necessary pass state transition time slice dynamic programming trellis length width takes tn time 
coupled hmm chains joint state trellis principle states wide associated dynamic programming problem tn 
show possible relax assumption transition visited obtaining cn algorithm closely approximates full combinatoric result 
wish formulate policy sampling small subset possible state sequences obtaining information possible 
intuition random sampling state sequences give representative picture model statistics fact desirable 
posterior probability mass hmm distributed evenly possible state sequences definition concentrated state sequences close true map state sequence 
low probability sequences carry relatively little information estimation problems 
show formally 
training data posterior model sum posteriors possible paths fg model denote expectation value time slice hp jx fg assume subset paths fqg explored policy path selection formulate information lost 
may cast problem greedily minimizing cross entropy paths posterior fg ks fqg hp jx fg log hp jx fg hp jx fqg hp jx fg log hp jx fg gamma hp jx fg log hp jx fqg coupled hidden markov models gamma hp jx fg log hp jx fqg gamma hp jx fg hlog jx fqg step jensen inequality push log expectation obtain upper bound cross entropy 
term monotonic function entropy constant concentrate second term 
carrying standard assumption training sequences equiprobable may treat expectation equation constant pull sum fg jjs fqg gamma hlog jx fqg minimizing upper bound strictly matter maximizing expectation second term choosing set paths fqg greatest probability mass shall expand expectation posterior chain chmm argmax fg hlog jx argmax fg log js gamma js gamma js gamma js gamma argmax fg log log log log log js gamma log js gamma log js gamma log js gamma log log practical procedure finds fqg paths tn time satisfy requirements 
time space path heads tracked 
visiting component state visited statistics may collected re estimating transition output probabilities 
visualize policy graphically represent problem coupled state component hmms chain chmm 
state sequence trellis double tracked having head hmm associated opposite hmm 
call sequence fhead pairs path subsequence leading particular fhead pair antecedent path 
satisfy visiting criterion guarantee component state time head path conventional trellis algorithms 
coupling hmms states takes heads coupling hmms states takes heads 
component state head equation brand single path standard state conventional hmm trellis reduced trellis chmm theta joint states 
solid lines represent transition probabilities dashed lines coupling probabilities 
simplified set paths pass heads chain left hand column parenthesized sums equation constant log ijs gamma 
consequently paths need maximize right hand columns amounts choosing map argmax fg log log log js gamma log js gamma log converse applies heads opposite chain choose maximize left hand columns 
choices heads antecedents mutually contingent fhead tuples chain chmm find map tuples time associating map antecedent paths associating antecedent paths new head 
directly leads approximate forward viterbi analyses developed 
forward backward analysis step forward analysis seek map mass fhead pairs antecedent paths 
head chain sums set paths shares 
simply maximum marginal posterior antecedent paths 
fortunately directly marginalizing chain choose map state antecedent paths 
heads chains 
head calculate new path posterior antecedent paths 
calculate forward variable ff associated head marginalize 
coupled hidden markov models backward variable fi similarly calculated tracing back paths selected forward analysis 
procedures detailed appendix viterbi analysis step viterbi analysis seek map density fhead pairs antecedent paths 
ordinary viterbi algorithms head time choose antecedent path gamma reduced trellis algorithm choose contrast forward backward analysis head different 
mutually contingent choices done steps antecedent path gamma select map head select antecedent path associated maximizes new head posterior 
prior knowledge dynamics dominate dynamics may heuristic method jordan perform conventional viterbi analysis chain holding state assignments constant cycle chains convergence local maximum 
re estimation collecting statistics heads method transition matrices chains re estimated conventional hmm formulae equation formula coupling matrices hmms similar gamma ff gamma jj fi jj gamma ff gamma fi gamma alternately statistics obtained heads algorithm gradient descent methods baldi chauvin binder yield line training procedure 
complexity multiple couplings procedures generalize directly graphs containing multiple hmms associate antecedent paths associate antecedent paths new heads 
cn paths complexity fully coupled set hmms cn chains having states system varied couplings largest coupling models chain constant number couplings rings see 
full couplings large numbers hmms result densely connected graphs 
exact estimation graphs np hard cooper major classes dense graphs approximation methods np hard dagum luby 
brand coupling hmm chains equivalent inference graph right 
full partial ring sheet couplings hmm chains 
heads method exploits regularity structure inference graph avoid problems 
approximation course weaken hmms coupled exponentially small fraction paths sampled 
practice rarely necessary employ chains couplings intuitions system suggest 
signal channels suspected pairwise interactions necessarily require fully coupled chain model interactions processes channels may similar amenable modeling just couplings allowing chain observe channels 
section illustrate application eighteen channels processes successfully modeled single coupling 
nature appears prefer decompose pairwise interactions rare higher order interaction profitably modeled currying effect set pairwise interactions 

conversion cartesian product hmms transition probabilities coupled hmm converted cartesian product hmm 
useful comparison methods formed basis earlier heuristic training procedures 
briefly sketch mathematical rationale chain coupling 
cartesian product hmm transition matrix coupled hidden markov models normalized product tensor products sjs omega js delta js omega sjs row permute operator swaps fast slow indices 
element wise product probabilities ss ss jss sjs js sjs js generically state component hmm 
transition matrix normalized stochastic fact ik jss 
factor matrix back transition coupling matrices seek projection parameter space cartesian product hmm parameter space coupled hmms 
appendix show projection minimizes cross entropy coupled cartesian posteriors ik ijj ik prior heads dynamic programming formed basis algorithm training coupled hmms cartesian product hmm factored re estimation 
compared heads method algorithm inferior complexity accuracy compares favorably algorithms interesting property yields smallest train test divergence evaluated algorithms cross validation trials 
earlier accurate factor couple algorithm arithmetic factoring performed enable applications discussed section 
variant couple sum tensor products factor square root normalization 
corresponding distribution fact properly formed independence network corresponds sum proper posteriors product hmm posteriors product coupling matrices 

experiments tested coupled hmms battery experiments synthetic data gauge performance real data computer vision verify advantages existing hmm algorithms 
brand benchmarks synthetic data test coupled hmm variations cartesian product hmms generated random intra hmm transition probabilities coupling matrices structured state hmm favored state 
cartesian product matrices perturbed uniform noise renormalized generate data random walks 
ensured dynamics arose interactions processes generating system conformant chmm distributional form 
dimensional output means processes drawn randomly gaussian distribution oe 
covariances set 
sequences observations taken half training half testing 
data generators discarded variety higher order hmm algorithms model data tn heads chmm tn factor couple chmm tn cartesian product hmm tn factorial hmm structured mean field approximation modified include coupling terms energy function algorithms take observation vectors time slice take received concatenation vectors appropriate terms initial means covariances set accordingly 
model trained slope log likelihood curve fell stopped epochs passed 
result tested training sequences cross validated test sequences 
entire procedure repeated times 
joint state coupled hmms total trials training models sequences 
results shows graph log likelihoods training data test data 
disparity indicates overfitting shown 
heads chmm yielded accurate general models followed closely factored chmm 
cartesian product hmm failed model interaction managed remote third polynomial order magnitude free parameters fit data 
mean field fhmm having abundant free parameters strong prior coupling poorly 
graph train test divergence clear cartesian product hmm grossly overfitted 
mean field approximations chmm inference graphs tn followed suggestion specifying higher order couplings probability transition matrices introduce second order interaction terms energy function 
ghahramani jordan independently compared mean field algorithm coupling terms energy function terms reduce divergence cross validation increasing test data likelihoods worsening training data likelihoods somewhat larger amount 
suggests coupling terms keep component hmms overfitting data 
coupled hidden markov models joint states epochs joint states mean field hmm cartesian chmm factored chmm heads training log likelihoods test log likelihoods train test divergence training epochs total flops flops epoch comparison chmm hmm fhmm algorithms 
training data generalized poorly test data system addition factoring generalized best 
heads chmm placed close second ahead mean field 
bottom row graphs compares time performance algorithms epochs total flops flops epoch 
readily apparent heads chmm mean field fhmm tn algorithms cartesian factored algorithms tn 
surprising heads mean field algorithms best worst respectively terms computation 
largely mean field took large number iterations converge terminate heads converged fairly rapidly 
brand epoch training hmms independent coupling generator reconstructed coupled hmm trained sequences length dimension log likelihood curve hmms trained coupled trained 
transition probabilities reconstructed data training chmm states component hmm 
line depicts probabilities transitions state 
qualitative structure transition matrices largely recovered 
demonstrate additional information captured coupling trained hmms independently coupled continued training 
shows log likelihood model epochs independent training conventional hmm epochs trained chmm 
gain coupling concretely demonstrated applications real data 
interestingly stage training improve way training couplings start 
small couplings data contains information reasonable reconstruction original transition probabilities shown 
test classification power compared models task distinguish test sequences sequences reversed having stationary densities opposite dynamics 
estimate probability classification error computed log likelihood ratio test sequence reversal 
large log ratios desirable nonpositive log ratio implies misclassified sequences 
train test trials compared heads chmm log ratio algorithms results table 
difference chmm algorithms statistically significant chmms advantages conventional hmms factorial hmms substantial significant 
coupled hidden markov models log discrimination ratios head chmm versus factored chmm conventional hmm factorial hmm mean value gamma table relative log discrimination ratio chmm models estimated likelihood ratios sequence reversed sequence 
means rounded measure successfully chmms separated classes 
state occupancies mean median min visits histogram state visits generating training data trials 
joint states approximately equal measure 
may objected efforts optimize reduce number states conventional hmms principle joint states may negligible measure representation 
fact histogram state visit counts data generation shows case probability mass spread states 
visited state occupied half mean 
reduced state hmm data 
efforts optimize architectures simulations seek optimal models real data task 
brand single linked coupled head coupled hmms hmms hmms hmms accuracy parameters table accuracies free parameter counts classifying handed ai chi gestures 
parameter counts transitions means covariances 
tests real time computer vision data ai chi ch uan chinese martial art exercise consisting stylized full body upper body gestures emphasize balance fluidity 
limbs may principle move independently maintaining balance induces coupling 
stereo vision blob tracker obtained hand tracking data hands ai chi gestures elaborate arm motions single cobra brush knee 
collected sequences channels hz lasting time steps 
hundreds trials varying state counts data best modeled state chmms state state hmms 
architectures correspond physical hypotheses chmm spatial coupling time dynamic balance spatial coupling instantaneous balance hmm data best modeled just arbitrary spacetime curve balance informative 
examples gesture train model 
models classify sequences mixed novel training data 
results summarized table showing chmm yielded best classifier expected result ai chi fact emphasize dynamic balance 
proved inappropriate classified third claimed sequences 
hundreds different conventional hmms tried yielded accuracies 
fact considerable variation classification power state count training data 
chmm proved far sensitive initial conditions produced better consistent models regardless initial random parameter values 
ran cross validation trials algorithm collected statistics model posteriors data 
results plotted fitted normal curves chmm distinguished sharp high likelihood peak contrasting broad spread low likelihood hmm results 
details brand oliver 
second video interpretation task chmms trained recognize continuous sequences manipulation picking putting pushing tool coupled hidden markov models log likelihood single test likelihood distribution coupled hmms linked hmms single hmm distribution models trained cross validated gesture data 
vision front tracked surfaces image plane corresponding hand tool affected object 
ellipse parameters surface plus mutual spatial relations collected observation vectors chmm 
half vector observed component chain 
interactions eighteen channels distinctive various actions channels observed chains training chmm discovered interactions channels useful model 
manner possible layer spatial agent patient instrument coupling models trained chained finite state machine accordance rules governing event sequences put twice picked 
novel action sequences processed map state sequence composite chmm examined obtain sequence recognized actions 
hit false alarms precision recall statistics compiled variants system systems shown table 
details brand 

variations extensions number interesting chmm variations possible cn complexity heads algorithm 
briefly mention extensions motivated problems acoustical signal processing relating inference graphs 

representational trick works body systems gives combinatorics begins explode pairwise spatial relations myriad tertiary interactions consider 
notably participant action verbs natural language 
brand model hits misses false alarm recall precision hmm hmm rules chmm chmm rules table performance statistics recognizers tested continous problem 
chmm inference graph source separation 
fhmm inference graph source separation 
chmm inference graphs signals multiple time scales 
source separation superimposed signals systems interest signals component processes superimposed single small number channels 
example acoustical waveform string quartet 
derived extension algorithm performs source separation hand hand state estimation brand 
briefly posterior expressed terms choice choice component signals sum actual observed signal 
constrained maximization posterior yields fixed point equation component signal 
choice calculation component signals alternates cn expectation maximization loop 
degenerate case chains wholly uncoupled equations yield algorithm source separation independent sources unrelated conversations cocktail party 
coupled hidden markov models processes different time scales application areas coupled processes may differ complexity time scale 
case speech reading modulates acoustic signal faster lip articulations modulate visual signal 
models problem proposed saul jordan bengio 
inference graphs time scales shown 
architectures evaluated dn variants heads algorithm brand number differently conditioned hidden nodes inference graph 
procedure generalizes multiple chains doubling time scales 

coupled hidden markov models chmms complex regularly structured inference graphs suited modeling dynamic interactions processes combining evidence disparate sensors modalities 
graphs hidden markov models subgraphs add coupling probabilities chains time 
strong model causal potentially asymmetric interactions processes 
chmms model signals variance classic markov formulations new estimation procedures needed 
general heads dynamic programming algorithm cn possible ct state paths explored resulting cn algorithm inference fully coupled chains number couplings constant 
contrasts tn naive cartesian exact clustered solutions 
heads method minimizes cross entropy full combinatoric posterior probability 
readily accommodates component hmms different couplings state structures generalized allow chains operating different time scales superimposed outputs 
experiments range algorithms trained real synthetic data heads method yielded highest quality models time high posteriors robustness initial conditions discrimination properties generalization novel examples 
chmms appropriate distributional form systems structure time space proven especially useful realm video licensed novel applications interpretation human gesture action 

mike jordan source illuminating conversations papers 
zoubin ghahramani provided software fhmms 
andy wilson provided basic hmm software 
oliver acquired ai chi data ran experiments summarized table 
sandy pentland dave becker originally proposed ai chi task domain brand hmm methods 
improvements presentation due comment students colleagues reviewers 
appendix forward backward analysis denote heads indices time slice ff probability mass associated head partial posterior probability absence state ff gamma output maximizing policy selects maximizes jff gamma 
calculate partial posteriors gamma ijk gamma ff gamma 
choose best chain argmax 
choice heads fixed 
calculate full posteriors path ff gamma ijk gamma jh gamma jk gamma ff gamma 
marginalize head possible obtain forward variables chain ff gamma ijk gamma jh gamma jk gamma ff gamma gamma ijk gamma maximum sum yields heads viterbi procedure steps 
note contrast conventional forward backward analysis different kinds forward variables ff variables propagating probabilities marginalized ff variables re estimating parameters 
backward variables fi ji ji jk jk fi computed forward analysis similarly marginalized 
coupled hidden markov models increasing complexity improve slightly greedy approximation conditioning choice ff gamma chosen 
practice causes forward calculation occasionally backtrack time slice 
similarly may recalculate ff gamma fi recalculate forward backward variables 
schemes obtain slightly higher posteriors expanding temporal scale greedy method substantial impact parameter estimation suggesting basic algorithm near optimum 
scaling joint probabilities quickly vanishingly small scaling procedure prevent numerical underflow 
typically scaling variable ff normalize forward variables ff ff iteration 
backward variables rescaled values fi fi 
scaling preserve invariant posterior probabilities states chain sum time slice fl ff fi 
conventional hmm dynamic programming exhaustively samples possible state sequences invariant obtains automatically 
heads algorithm small fraction state sequences sampled fl 
noting ff ff gamma ijj obtain simple procedure rescaling backward variables implicitly restores invariant fl ff fi ff fi fi fl ff fl ff gamma ijj appendix factoring cartesian product transition matrices maximum entropy factoring transition matrix transition matrices coupling matrices minimizes cross entropy distributional priors js gamma gamma js gamma js gamma js gamma js gamma partial derivative respect coupled transition probability js gamma js gamma js gamma gamma log js gamma gamma js gamma js gamma js gamma js gamma note drawn distribution signals transition probabilities modeling js gamma gamma gamma brand js gamma js gamma gamma gamma js gamma gamma log js gamma gamma gamma log js gamma gamma log js gamma gamma log js gamma gamma log js gamma gamma js gamma gamma js gamma obtain js gamma add lagrange multipliers guaranteeing js 
js gamma js gamma gamma js gamma combining equation gamma js gamma gamma js gamma js gamma js gamma sp gamma js gamma gamma probabilities write js gamma js gamma qp gamma js gamma gamma sjs gamma baldi chauvin baldi chauvin 

smooth line learning algorithms hidden markov models 
neural computation 
baldi chauvin baldi chauvin 

hybrid modeling hmm nn architectures protein applications 
neural computation 
baum baum 

inequality associated maximization technique statistical estimation probabilistic functions markov processes 
inequalities 
bengio frasconi bengio frasconi 

input output hmms sequence processing 
ieee transactions neural networks 
binder binder koller russell kanazawa 

adaptive probabilistic networks hidden variables 
machine learning 
press 
coupled hidden markov models binder binder murphy russell 

space efficient inference dynamic probabilistic networks 
proceedings international joint conference artificial intelligence 
brand brand 

coupled hidden markov models multiple time scales 
vision modeling tr mit media lab 
brand brand 

inverse hollywood problem video scripts storyboards causal analysis 
proceedings conference artificial intelligence aaai 
mit media lab vision modeling tr 
brand brand 

source separation coupled hidden markov models 
vision modeling tr mit media lab 
brand oliver brand oliver 

coupled hidden markov models complex action recognition 
proceedings ieee conference computer vision pattern recognition pages 
mit media lab vision modeling tr 
cooper cooper 

computational complexity probablistic inference bayesian belief networks 
artificial intelligence 
dagum luby dagum luby 

approximating probabilistic inference bayesian belief networks np hard 
artificial intelligence 
dean kanazawa dean kanazawa 

probabilistic temporal reasoning 
computational intelligence 
dempster dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society 
forney forney 

viterbi algorithm 
proceedings ieee 
fung chang fung chang 

weighting integrating evidence stochastic simulation bayesian networks 
proceedings conference uncertainty artificial intelligence volume ontario canada 
morgan kaufmann 
ghahramani jordan ghahramani jordan 

factorial hidden markov models 
touretzky 
henrion henrion 

propagation uncertainty bayesian networks probabilistic logic sampling 
lemmer kanal editors uncertainty artificial intelligence volume 
elsevier north holland amsterdam 
bengio bengio 

hierarchical recurrent networks long term dependencies 
touretzky 
brand jensen jensen lauritzen olesen 

bayesian updating recursive graphical models local computations 
computational statistical quarterly 
jordan jordan ghahramani saul 

hidden markov decision trees 
touretzky 
kanazawa kanazawa koller russell 

stochastic simulation algorithms dynamic probabilistic networks 
proceedings conference uncertainty artificial intelligence volume montreal canada 
morgan kaufmann 
kjaerulff kjaerulff 

computational scheme reasoning dynamic probabilistic networks 
proceedings conference uncertainty artificial intelligence volume pages 
morgan kaufmann 
parisi parisi 

statistical field theory 
addison wesley redwood city ca 
rabiner rabiner 

tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
saul jordan saul jordan 

boltzmann chains hidden markov models 
tesauro 
saul jordan saul jordan 

exploiting tractable substructures intractable networks 
touretzky 
smyth smyth heckerman jordan 

probabilistic independence networks hidden markov probability models 
neural computation 
stork stork editors 
humans machines volume nato asi series computer systems sciences 
springer verlag berlin 
tesauro tesauro touretzky leen editors 
advances neural information processing systems volume cambridge ma 
mit press 
touretzky touretzky mozer hasselmo editors 
advances neural information processing systems volume cambridge ma 
mit press 
williams hinton williams hinton 

mean field networks learn discriminate temporally distorted strings 
proceedings connectionist models summer school pages san mateo ca 
morgan kaufmann 
