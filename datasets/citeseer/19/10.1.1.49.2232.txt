appear neural information processing systems mit press bayesian methods mixtures experts steve waterhouse cambridge university engineering department cambridge cb pz england tel srw eng cam ac uk david mackay cavendish laboratory rd cambridge cb england tel mackay cam ac uk tony robinson cambridge university engineering department cambridge cb pz england 
tel eng cam ac uk bayesian framework inferring parameters mixture experts model ensemble learning variational free energy minimisation 
bayesian approach avoids fitting noise level estimation problems traditional maximum likelihood inference 
demonstrate methods artificial problems sunspot time series prediction 
task estimating parameters adaptive models artificial neural networks maximum likelihood ml documented 
geman bienenstock doursat 
ml estimates typically lead models high variance process known fitting 
ml yields confident predictions regression problems example ml underestimates noise level 
problem particularly dominant models ratio number data points training set number parameters model low 
consider inference parameters hierarchical mixture experts hme architecture jordan jacobs 
model consists series experts modelling different processes assumed underlying causes data 
expert may focus different subset data may arbitrarily small possibility fitting process increased 
bayesian methods mackay avoid fitting specifying prior belief various aspects model parameter uncertainty 
regularisation weight decay corresponds prior assumption model smooth outputs 
equivalent prior ja parameters model hyperparameters prior 
set priors may specify posterior distribution parameters data jd variable encompasses assumptions model architecture type regularisation assumed noise model 
maximising posterior gives probable parameters mp may set hyperparameters cross validation finding maximum posterior distribution hyperparameters ajd known evidence gull 
describe method motivated expectation maximisation em algorithm dempster laird rubin principle ensemble learning variational free energy minimisation hinton van camp neal hinton achieves simultaneous optimisation parameters hyperparameters hme 
demonstrate algorithm simulated examples time series prediction task 
task bayesian methods prevents fitting data gives better prediction performance 
describe algorithm specify model associated priors 
mixtures experts mixture experts architecture jordan jacobs consists set experts perform local function approximation 
expert outputs combined gate form output 
hierarchical case experts mixtures experts extending network tree structured fashion 
model generative assume data generated domain series independent processes selected stochastic manner 
specify set indicator variables fz 

ng output generated expert zero 
consider case regression data set fx 
ng 
specify conditional probability scalar output input vector exemplar jx jx jx fx set gate parameters set expert parameters 
case jx gaussian jx gamma exp gamma gamma variance expert output expert giving probabilistic mixture model 
restrict expert output linear function input model action selecting process gate outputs softmax function inner products input vector gate parameter vectors 
conditional probability selecting expert input jx exp exp straightforward extension model gives conditional probability expert having selected input output jy 
parameter expert common mackay consider hyperparameter gaussian noise prior 
notation assume input vector augmented constant term avoids need specify bias term parameter vectors 
priors assume separable prior parameters model ja ja fa hyperparameters parameter vectors experts gate respectively 
assume gaussian priors parameters experts fw gate fx example ja exp gamma simplicity notation shall refer set smoothness hyperparameters set noise level hyperparameters fb assume gamma priors hyperparameters priors example log jr exp gammab hyper hyperparameters specify range expect noise levels lie 
inferring parameters ensemble learning em algorithm jordan jacobs train hme maximum likelihood framework 
em algorithm specify complete data set fd zg includes observed data set indicator variables gamma step em algorithm computes distribution zjd gamma step maximises expected value complete data likelihood distribution 
case hme indicator variables specify expert responsible generating data time 
outline algorithm simultaneous optimisation parameters hyperparameters framework ensemble learning variational free energy minimisation hinton van camp 
optimising point estimate optimise distribution parameters 
builds neal hinton description em algorithm terms variational free energy minimisation 
specify approximating ensemble optimise approximates posterior distribution zjd 
objective function chosen measure quality approximation variational free energy dw dx da db dz log djh joint probability parameters fw xg hyperparameters fa bg missing data observed data djh ja jr jx jx 
free energy viewed sum negative log evidence gamma log djh kullback leibler divergence zjd 
bounded gamma log djh equality zjd 
constrain approximating ensemble separable form 
find optimal separable distribution considering separately optimisation separate ensemble component components fixed 
optimising qw 
functional dw gamma log const variable denotes da noting dependent terms log posterior distribution divergence log minimised setting write distribution minimises expression 
data optimising distribution opt opt opt exp gamma gamma gamma const set gaussian distributions means exactly quadratic optimisation 
denote variance covariance matrices opt fs analogous expression gates opt obtained similar fashion opt opt exp gamma log const 
approximate opt gaussian distribution fitted maximum variance covariance matrix optimising similar procedure optimal distribution opt opt exp exp gamma gamma mp gamma value computed 
standard step gives distribution fixed value parameters data shown equation 
case finding optimal obtain alternative expression dependencies uncertainty experts predictions 
ideally assumption separable distribution expected contain additional effect uncertainty gate parameters 
introduce method mackay classifiers case binary gates 
optimising hyperparameter distributions optimal values ensemble functions give values gamma sw 
analogous procedure set hyperparameters gate 
making predictions order predictions model marginalise parameters hyperparameters get predictive distribution 
optimal distributions opt approximate posterior distribution 
experts marginalised outputs mp variance yja may marginalise gate parameters mackay give marginalised outputs gates 
predictive distribution mixture gaussians mean variance second moments yja yja gamma 
simulations artificial data order test performance bayesian method constructed artificial data sets 
data sets consist known function corrupted additive zero mean gaussian noise 
data set shown consists points piecewise linear function leftmost portion corrupted noise variance times greater rightmost portion 
second data set shown consists points function 
gammat gamma gamma gamma corrupted gaussian noise constant variance 
trained number models data sets provide typical set results maximum likelihood bayesian methods error bars bayesian solutions 
model architecture deep binary hierarchy linear experts 
cases ml solutions tend overfit noise data set 
bayesian solutions hand smooth functions better approximations underlying functions 
time series prediction bayesian method evaluated time series prediction problem 
consists yearly readings sunspot activity original noise ml solution original function bayesian solution 
error bars effect regularisation fitting known functions corrupted noise 
considered connectionist community weigend huberman rumelhart mlp hidden tanh units predict coming year activity activities previous years 
data set chosen consists relatively small number examples probability fitting sizeable models large 
previous considered mixture experts problem 
due problems fitting inherent ml constrained cross validation training early 
constrained selection model order branches deep networks tend ml training resulting local minima training 
bayesian method avoids fitting gates allows large models 
table single step prediction sunspots data set lag vector years 
nmse mean squared prediction error normalised variance entire record 
models weigend mlp result hme cv mixture experts trained maximum likelihood cross validation scheme hme ml hme bayes deep binary hme trained maximum likelihood ml bayesian method bayes 
model train nmse test nmse hme cv hme ml hme bayes table shows results obtained variety methods sunspots task 
bayesian method performs significantly better test sets maximum likelihood method hme ml competitive mlp weigend 
noted number parameters deep binary hme larger number training examples bayesian method avoids fitting data 
allows specify large models avoids need prior architecture selection cases selection may advantageous example number processes inherent data known priori 
experience linear experts smoothness prior output function expert important effect prior gates bayesian inference noise level important factors 
expect smoothness prior important experts complex basis functions 
discussion em algorithm special case ensemble learning algorithm em algorithm obtained constrain qq delta functions fix 
bayesian ensemble works better includes regularization uncertainty parameters taken account predictions 
interest investigate models trained em benefit ensemble learning approach hidden markov models 
bayesian method avoiding fitting shown lend naturally mixture experts architecture 
bayesian approach implemented practically small computational overhead gives significantly better performance ml model 
dempster laird rubin 
maximum likelihood incomplete data em algorithm journal royal statistical society series 
geman bienenstock doursat 
neural networks bias variance dilemma neural computation 
gull 
developments maximum entropy data analysis skilling ed maximum entropy bayesian methods cambridge kluwer dordrecht pp 

hinton van camp 
keeping neural networks simple minimizing description length weights proceedings colt 
jordan jacobs 
hierarchical mixtures experts em algorithm neural computation 
mackay 
bayesian interpolation neural computation 
mackay 
evidence framework applied classification networks neural computation 
neal hinton 
new view em algorithm justifies incremental variants biometrika 
submitted 
weigend huberman rumelhart 
predicting connectionist approach international journal neural systems 
