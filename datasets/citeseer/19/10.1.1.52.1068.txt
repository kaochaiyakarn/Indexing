theory refinement bayesian networks wray buntine riacs ai research branch nasa ames research center mail field ca usa phone wray ptolemy arc nasa gov theory refinement task updating domain theory light new cases done automatically expert assistance 
problem theory refinement uncertainty reviewed context bayesian statistics theory belief revision 
problem reduced incremental learning task follows learning system initially primed partial theory supplied domain expert maintains internal representation alternative theories able domain expert able incrementally refined data 
algorithms refinement bayesian networks illustrate meant partial theory alternative theory representation algorithms incremental variant batch learning algorithms literature batch incremental mode 
theory refinement task updating domain theory light new cases 
key idea expert prior domain knowledge prime learning system knowledge acquisition process 
subsequent refinement theory proceeds having learning system accept examples ask key questions expert 
shapiro shapiro instance developed comprehensive theory suite algorithms task refining horn clause theories logic programs 
ginsberg applied heuristic approach refinement rule base context medical diagnosis ginsberg 
research area ourston mooney towell grew need uncertainty artificial intelligence proceedings pp 
inductive learning algorithms available knowledge intensive mimic perceived benefits analytic learning methods explanation learning 
research faces problems imperfect uncertain domain theories noisy training cases handled analytic methods 
example hybrid learning approach follows towell rule base knowledge domain transcribed neural network initialize network new training cases run back propagation algorithm refine network 
approach addresses research question build learning algorithm covers full spectrum theory refinement standard batch learning starting non informative theory assuming learning occurs just batch cases incremental learning assuming new cases come smaller batches theory gradually refined 
second example theory refinement bayesian networks medical expert systems lauritzen spiegelhalter 
experts set appropriate graphical structure estimate needed probabilities new examples may arrive daily basis expert system needs refined 
spiegelhalter argue expert experience confidence setting initial model needs quantified spiegelhalter lauritzen instance examples order refinement carefully 
expert initial model cases reliable new noisy cases obtained happen unusual wrongly suggest expert initial model requires major refinement 
spiegelhalter approach addresses second research question new possibly anomalous cases start refining drastically refine disregard anomalous cases noise 
spiegelhalter address issue refining structure bayesian network continuous parameters probability distributions 
considers broad research questions 
approach theory refinement suggested follows learning system primed partial theory supplied domain expert maintains alternative theory representation able domain expert able incrementally refined data 
furthermore partial theory initially null incorporates quantification expert experience right amount refinement done new cases 
approach learning networks incorporates partial theory srinivas 
general approach developed bayesian principles belief updating form basis learning algorithms buntine cooper herskovits 
principles specify precisely normative approach theory refinement approach suggested approximates 
normative property claim principles set standard theory refinement learning algorithms approximate fail return poorer refined learned theories average 
popular learning framework computing area uniform convergence pac model instance haussler 
approach approximates normative bayesian approach sample sizes large 
researchers reported unsurprisingly bayesian approach superior smaller size training samples buntine opper haussler range batch learning problems 
previous methods learning bayesian networks geiger spirtes glymour verma pearl srinivas closer large sample uniform convergence framework assume independence information unambiguously determined 
algorithms assumption geiger spirtes glymour verma pearl unknown probability distribution dag isomorph pearl 
means independencies problem perfectly captured bayesian network may case particular problem instance non chordal markov networks dag isomorphic 
algorithms seemingly discover causality data existence causality immediate assumption dag isomorphism 
restrictive assumption practice sensitive algorithms failure 
approach contrast requires ordering possibly causal variables supplied system 
assumes underlying probability distribution bayesian network ordering 
algorithms assume example training sample variable values fully specified 
assumption relaxed involve considerable computational cost done properly 
approach initial partial theory obtained domain expert interpreted prior information space possible theories alternative theory representation interpreted subspace alternative theories reasonable posterior represented compact form 
simple learning approaches approximate space alternative theories single high posterior structure cooper herskovits buntine experiments show averaging larger sized space yields considerable improvement buntine improved performance corresponds improved accuracy gained system system approximates posteriors alternative disease sets single disease set henrion 
space alternative theories difficult domain expert readily summarized ways expert interrogation theory refinement approaches described 
theory refinement algorithm course applies bayes theorem space alternatives 
generate space reasonable alternatives search space high posteriors similar style motivation system bayesian averaging method trees buntine 
theory refinement approach developed bayesian networks 
networks introduced representation partial theories transformation prior described 
representation alternative theories described theory refinement interrogation algorithms 
major sections describe approach assume conditional probability distributions node bayesian network represented full conditional joint distribution values variables supplied training case 
course larger practical systems assumptions rarely apply 
final section describes noisy gates conditional distributions parameters learnt theory refinement framework 
bayesian networks bayesian networks specify dependence properties variables directed acyclic graph 
describe probabilistic models useful nondirected classification 
predict similar results reported spirtes justification different 
compute likelihoods subset variables 
contrast class probability trees quinlan buntine allow directed classification yield predictions special target variable usually referred class 
shows simple bayesian network 
set upsilon sigma xi pi poor diet upsilon sigma xi pi family history upsilon sigma xi pi smoking upsilon sigma xi pi upsilon sigma xi pi unfit hj upsilon sigma xi pi heart disease phi phi phi phi phi upsilon sigma xi pi phi phi phi phi phi upsilon sigma xi pi chest pains bayesian network simple system variables outgoing arcs variable called parents variable variable associated conditional probability table gives probabilities different values variable conditioned values parent variables 
instance graph need values dja parent parent structure specifying network conditional probability tables methods exist computing arbitrary conditional marginal likelihoods variables lauritzen spiegelhalter 
notation 
bayesian network consists set discrete variables variable set parent variables pi full parent structure denoted pi 
instance graph pi fag pi fa bg set possible values variable cartesian product variables pi pi 
instance boolean ftrue falseg pi true true true false false true false false denotes cardinality 
assignment variables denote corresponding assignments jx pi ae pi instance fu wg pi fv wg boolean true false true ju true pi false true 
denotes matrix conditional probabilities parent variables pi conditioned values 
pi pi ijj able determine probability full set variables standard expansion pi jx ji pi gives likelihood single example pi product forms gives likelihood independently identically distributed training sample calculating various posteriors 
partial bayesian networks initial partial theory expert transformed prior probability space theories 
bayesian network fully specified parent structure pi conditional probabilities initial partial theory specifies prior distribution pi 
section describes information obtained expert converted prior bayesian networks 
experience shows experts able suggest roughly variables influence 
experts usually better expressing qualitative knowledge quantitative weak domain theories indicate influence exact equational form 
variables ordered time occurrence instance family history heart disease pre dates heart disease potential influences time impossible 
partial theory obtained expert ordering variables bayesian network specified pictorially shades grey 
black arcs indicate definite parents prior 
missing arcs indicate definite non parents prior 
grey arcs indicate parents status uncertain prior belief proportional grey level allow greater range log prior mapped grey level 
tells theory refinement algorithm eager modify potential parent status light new evidence 
ask expert provide total ordering oe variables variable parents subset variables pi oe 
ask expert indicate strongly believes potential parent parent measured units subjective probability 
denote information variables oe prior probability parent denoted oe 
assuming independence full prior parent structure conditioned total ordering variables pi oe pi oe assuming pi consistent oe pr pi oe pi oe delta pi gamma oe extend simple model partial theory introduce correlations potential parents 
partial bayesian network specified total ordering variables oe prior probability potential parent allows evaluate oe 
complete prior need specify pi oe 
assume independent oe pi develop prior pi 
choose prior conjugate prior yields posterior functional form mathematics simple berger assumes amount information known conditional probability tables 
product standard non informative priors multinomial distributions conditional probability distribution multinomial symmetric dirichlet prior buntine berger assumes prior independence cells conditional probability table pi pix ff gamma ijj ff ff dimensional beta function nc gamma gamma gamma gamma function gamma ff parameter prior variable particular bayesian network equivalent set bayesian networks arc directions changed verma pearl 
ff ff jv pi prior gives equivalent networks equivalent priors means marginal priors individual variables non informative 
proof involved space 
representing alternative bayesian networks total ordering variables theory refinement algorithm section considers reasonable alternative parent sets variable determined criteria reasonableness 
variable alternative parent sets pi collection subsets oe combining gives space alternative parent structures represented cartesian product sets reasonable parent sets 
possible parent structure pi know posterior probability sufficient information update new examples 
space parent structures additional information thought similar version space mitchell 
inherent uncertainty theories considered version space updated considering consistency training sample specific generalizations bayes theorem indicates normative way updating version space alternative parent structures posterior belief 
unfortunately full space parent structures super exponential store update details 
overcome store posterior quite high relative terms structures significant 
section outlines calculate posterior parent structure reasonable set alternative parent structures stored 
refer representation set reasonable parent structures conditional probability tables associated statistics combined bayesian network 
reasonable context precise meaning section shown maintain update combined bayesian networks 
denote set containing sets reasonable parent variables variable fair belief true pi space reasonable parent structures pi total ordering oe cartesian product number different reasonable parents jp denoted note useful determining operation count algorithms 
reasonable parent structure pi associated subjective posterior probability indicating strongly currently believe true structure 
having seen sample sample obtained information oe expert pi sample oe 
standard rules probability calculated pi sample oe pi joe delta pi sample pi pi sample oe pi sample oe pi joe pix jj ff mx jj ff ff ff ijj number examples training sample sample pi assuming example sample variable values fully specified 
solution integral follows standard properties dirichlet integral buntine 
counts ijj parameters posterior affected training sample referred sufficient statistics berger need maintained incremental learning 
reasonable parent structure estimates parameters specifying conditional probability tables 
estimated table variable pi gamma ijj delta standard rules probability calculated pi gamma ijj delta ijj sample pi pi sample pi pi ijj ff jj ff jj mx ijj integrations done standard properties dirichlet integral simplified recursive properties gamma function gamma gamma 
basic information ready describe representation combined bayesian network 
order reconstruct necessary conditional probability tables compute posteriors set parent variables pi sufficient corresponding counts ijj kept 
save computation posterior pi sample oe totals jj kept 
access alternative parent sets pi efficiently stored lattice structure subset superset parent sets linked web denoted parent lattice full set lattices size jx potentially exponential jx parent sets significant posterior probabilities stored linked 
instance store parent sets posterior factor maximum posterior parent set far instance restrain jx 
structure updating algorithm 
increasing factor close guaranteed full set lattices manageable size expense losing accuracy theory refinement 
posterior probabilities usually vary exponentially learning set reasonable parent sets manageable 
root node parent lattice empty set leaves sets pi supersets contained refer entire representation combined bayesian network 
notice easily fill lattice fag fa bg fa cg fa dgg adding fa cg fa dg fa dg reduce number leaves new leaves may insignificant posterior probabilities 
assist search update lattice theory refinement nodes parent sets associated statistics labeled alive dead asleep 
alive nodes represent set reasonable alternatives having high posteriors correspond parent sets dead nodes exist lattice dead markers search space explored forever determined unreasonable alternatives expanded 
asleep nodes similar considered unreasonable may alive 
furthermore nodes open closed depending require expansion search 
theory refinement section proposes algorithms modification interrogation combined bayesian network 
algorithms linear time jv pi jx may jx relevant variables 
structure update algorithm adjustable search algorithm time vary fast greedy search slower beam search 
parameter updates training sample sample extended require rapid incremental update combined bayesian network simple parameter update done altering structure parent lattices 
means variable reasonable parent set pi increment corresponding cell counts update posteriors 
normally process effect alive nodes parent lattice 
instance suppose sample extended sample new example having pi increment ijj pi sample oe pi sample oe ijj ff jj gamma ff jj ff ijj gamma ff follows recursive properties gamma function 
full update process take operations 
increase sample adding extra examples batch repeat process times 
process sped initially updating leaf nodes parent lattices change example counts filtered upwards examples 
structure updates additional time time search begun extend modify reasonable parent structures corresponding parent lattices ensure high posterior parent sets represented 
algorithm time batch algorithm starting empty lattice differentiated produce incremental version 
algorithm simple beam search algorithm parameters vary search explained 
alternatively branch bound algorithm developed upper bounds posterior probabilities corresponding decision theoretic search algorithm 
batch beam search algorithm finds parent sets posteriors factor best 
beams searched parents sets factor best 
algorithm pseudo code 
search input variable prior parent sets pr pix training sample 
output parent lattice corresponding sample 
algorithm 
set best posterior posterior pi 

set open list maintains list parent sets factor best posterior expanded search 

set alive list maintains list parent sets factor best posterior considered alive parent lattice 

take top parent set pi open list 

posterior delta best posterior mark parent set dead restart step 
posterior delta best posterior ignore parent set restart step 
generate children calculate posterior probabilities conditioned training sample 

greatest posterior best posterior update best posterior modify alive list reflect new maximum 

mark children posterior delta best posterior sample size jx jjv pi dead 

add children posterior delta best posterior open list 

add children posterior delta best posterior alive list mark alive 

mark remaining unmarked children asleep 

repeat step open list empty 
batch learning algorithm easier fact posterior probabilities alternative structures tend vary exponentially structures change high posterior structures tend clump 
beam search efficient 
parent sets marked dead time posterior factor best fairly stable probability estimates 
parent sets marked dead posteriors poor parent structures decrease exponentially increasing sample size 
dead nodes expanded reduces search 
notice set close algorithm greedy search high posterior parent set 
process reproducing result algorithm run incrementally 
needed additional batch examples received 
asleep nodes updated previous additional samples parameter update process previous section asleep nodes parameters updated best recalculated 
processing interruptible achieve time feature search 
adjust alive list open list reflect new best 
expand nodes open list continue search 
nodes may oscillate alive list open list posterior ordering parent sets oscillate training samples increases posteriors modified 
problem repeated restructuring reported crawford occur incremental learning algorithms crawford 
prevented making differential placing node node 
structure posteriors useful form feedback expert return information exactly format initially obtained expert partial bayesian network 
means calculating posterior probability conditioned training sample variable parent sample oe pi px pi pi sample oe full calculation variables take jx delta operations 
information pictorially represented graph arcs done shades grey indicate strength belief 
standard asymptotic properties bayesian methods assure sample size gets arbitrarily large posterior probabilities converge 
alternative bayesian networks useful form feedback expert return bayesian networks stored combined bayesian network 
selecting set parents pi associated conditional probability distribution 
ensure truly representative networks return collection networks compressed format corresponding single bayesian network denoted smoothed bayesian network 
similar operation class probability trees buntine 
variable choose leaf parent lattice probabilistic method described 
provides potential parent set may high posterior parents sets subsets shall average corresponding conditional probability tables obtain single representative conditional probability table 
denote set parent sets subsets pi pi pi merge parent sets average conditional probability tables obtain single representation 
done formulae posterior probability true set parents sample oe pi sx pi sample oe posterior expected conditional probability table conditioned assuming true set parents pi sample oe pi pi sx pi pi pi sample delta pi sample oe sample oe note st probability right hand side equation calculated equation posterior expected probability parent assuming true set parents sample oe pi sx pi pi sample oe sample oe formulae follows choose leaf randomly proportion sample oe 
full set variables takes operations 
process relies selection leaves parent lattice may advantageous reduce number leaves discussed structure update algorithm 
variable display set parents pictorially grey scales discussed previously sample oe probability parent full set variables takes jx delta operations 
generate conditional probability tables value computing pi sample oe pi 
represents average various conditional probability tables corresponding parent sets empirically effect smoothing conditional probability tables computed equation 
takes jv jjs operations 
small training sample technique produce different smoothed bayesian networks corresponding different alive leaves parent lattices 
perusal give expert idea current variability choice bayesian network 
training sample size increases asymptotic properties bayesian methods assure sets high posterior parents conditional probability tables roughly equivalent different smoothed bayesian networks produced differ eventually converge 
extensions section briefly considers relaxing assumptions previous section full conditional joint distributions exist node 
extensions handling missing values examples variable values missing handling expert designated hidden variables structure 
problems handled em algorithm dempster 
full conditional joint distributions general model specification requires exponential number parameters 
estimating parameter values data severe problem trying elicit probabilities expert 
way introduce approximate distributions lower dimension 
issues consider learn parameters specific conditional distribution 
patch distribution learnt broad framework previously 
ways representing restricted conditional probability distributions trees buntine logistic regression qualitative models popular economic statistics noisy gate popular ai pearl 
noisy gate described follows 
suppose boolean variable conditioned boolean variables xn noisy parameters xn false gamma true indicator function true zero 
similar conditional probability distribution multivariate logistic regression function slightly modified form applies boolean variable conditioned boolean variables xn xn false true 
usually parameters forms approximate product small 
generally logistic function symmetrized version noisy 
versions function exist variables valued discrete variables introduce higher order correlations variables 
logistic function functional form simple idiot bayes classifier obtained conditional distribution quadratic exponential distribution discrete variables xn incorporate methods framework just need able calculate posterior expected parameter values relative posterior probability noisy function logistic regression function true independently parameter values 
conditional distribution associated particular set parent variables parameter values posterior placed parent lattice combined bayesian network 
posterior instance search space logistic regressions different parent sets algorithm determining structure posteriors 
posterior expected parameter values relative posterior probability noisy logistic regression models readily estimated standard maximum likelihood bayesian methods 
simple show sample likelihood functions noisy logistic regression function convex 
dominant likelihood term posterior parameters unimodal maximum posterior parameters search methods scoring newton raphson conjugate gradient 
multivariate normal approximation posterior point berger marginalize parameters approximate posterior probability noisy function logistic regression function true 
notice numeric search algorithms iterative readily placed incremental framework 
new training cases start iterative search previous maximum posterior point convergence rapid new point posterior unimodal catastrophic changes maximum posterior point 
representation theory refinement algorithms learning bayesian networks 
important properties ffl representation initiated partial bayesian network quantifies expert experience confidence 
similar approach suggested srinivas 
representation maintains reasonable hypotheses form version space 
ffl algorithms approximate normative bayesian solution corresponding batch learning problem 
analogous approximation class probability trees significantly outperformed standard statistical ai methods buntine large range problems 
weaker approximation batch learning finds single high posterior network reported empirically cooper herskovits parameter updating component algorithm corresponds previous spiegelhalter lauritzen 
ffl incremental algorithm allows time return varied processing times receipt new examples 
development algorithm illustrates batch learning algorithm converted incremental learning algorithm 
ffl algorithms allow user interrogate current hypotheses bayesian networks get idea variability 
ffl algorithms parameters allow fuller approximation normative solution 
parameters allow trade space time complexity average case quality learned theories compare buntine henrion 
ffl extensions suggested show handle different conditional probability models noisy gates logistic functions 
experience similar approach learning trees suggests algorithms additional hacking 
ideas suggested independently bob fung benefited discussion 
writing motivated comments pat langley 


advanced econometrics 
harvard university press cambridge ma 
berger 

statistical decision theory bayesian analysis 
springer verlag new york 
buntine 

learning classification trees 
technical report fia riacs nasa ames research center moffett field ca 
third international workshop artificial intelligence statistics 
buntine 

theory learning classification rules 
phd thesis university technology sydney 
forthcoming 
cooper herskovits 

bayesian method induction probabilistic networks data 
technical report ksl knowledge systems laboratory medical computer science stanford university 
crawford 

extensions cart algorithm 
international journal man machine studies 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
roy 
statist 
soc 

geiger paz pearl 

learning causal trees dependence information 
eighth national conference artificial intelligence pages boston massachusetts 
ginsberg weiss 

automatic knowledge base refinement classification systems 
artificial intelligence 
haussler 

decision theoretic generalization pac learning model application feed forward neural networks 
information control 
appear 
henrion 

efficient inference multiply connected belief networks 
oliver smith editors influence diagrams belief nets decision analysis pages 
wiley 
lauritzen spiegelhalter 

local computations probabilities graphical structures application expert systems 
roy 
statist 
soc 

mitchell 

generalization search 
artificial intelligence 
opper haussler 

generalised performance bayes optimal classification algorithm learning perceptron 
colt workshop computational learning theory 
morgan kaufmann 
manuscript 
ourston mooney 

changing rules comprehensive approach theory refinement 
eighth national conference artificial intelligence pages boston massachusetts 
pearl 

probabilistic reasoning intelligent systems 
morgan kauffman 
quinlan 

induction decision trees 
machine learning 
shapiro 

algorithmic program debugging 
mit press 
spiegelhalter lauritzen 

sequential updating conditional probabilities directed graphical structures 
research report institute electronic systems aalborg university aalborg denmark 
spirtes glymour 

algorithm fast recovery sparse causal graphs 
report laboratory computational linguistics carnegie mellon university 
spirtes scheines glymour 

simulation studies reliability computer aided model specification tetrad ii 
eqs programs 
methods research 
srinivas russell agogino 

automated construction sparse bayesian networks 
henrion kanal lemmer editors uncertainty artificial intelligence pages 
elsevier science publishers amsterdam 
towell shavlik noordewier 

refinement approximate domain theories knowledge neural networks 
eighth national conference artificial intelligence pages boston massachusetts 
verma pearl 

equivalence synthesis causal models 
sixth workshop uncertainty artificial intelligence cambridge ma 
