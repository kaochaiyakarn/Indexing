local learning algorithms eon bottou vladimir vapnik bell laboratories holmdel nj usa september rarely training data evenly distributed input space 
local learning algorithms attempt locally adjust capacity training system properties training set area input space 
family local learning algorithms contains known methods nearest neighbors method knn radial basis function networks rbf new algorithms 
single analysis models aspects algorithms 
particular suggests knn rbf non local classifiers achieve best compromise locality capacity 
careful control parameters simple local learning algorithm provided performance breakthrough optical character recognition problem 
error rate rejection performance significantly improved 

simple local algorithm testing pattern select training examples located vicinity testing pattern train neural network examples apply resulting network testing pattern 
algorithm looks slow stupid 
small part available training examples train network 
empirical evidence defeats analysis 
proper settings simple algorithm improves significantly performance best optical character recognition networks 
years ago vapnik devised theoretical analysis local algorithms briefly discussed vapnik 
analysis introduces new component named locality known tradeoff capacity learning system number available examples 
attempts explain demonstrates algorithm efficient certain tasks underlying ideas profit 
voluminous equations theoretical analysis discussed 
complexity related imperfection current generalization theories introduce unnecessary noise discussion 
section show handling rejections pattern recognition problems requires different properties learning device different areas pattern space 
section idea local learning algorithm discuss related approaches discuss impact locality parameter generalization performance 
section demonstrate effectiveness local learning algorithm real size optical character recognition task 
rejection ambiguity 
class pattern determined confidence 
rejection lack data 
examples find decision boundary area confidence 
piece imaginary pattern space 
gray black circles examples classes 
thin lines actual bayesian decision boundary classes 
crosses represent rejected patterns 
rejection 
ideal isolated character recognition system assigns right symbolic class character image 
real recognizer commit error perform rejection 
errors expensive correct 
zipcode recognition system instance erroneously send parcel world 
system reject pattern classification achieved confidence 
having pattern processed human usually expensive fixing error 
selecting proper confidence threshold reduces cost handling rejected patterns cost correcting remaining errors 
quality pattern recognition system measured rejection curve cf 

curve displays possible compromises number rejected patterns numbers remaining errors 
different situations reduce classification confidence cause rejection cf 

ffl patterns ambiguous 
instance certain people write people write 
cause rejection inherent problem 
ambiguities arise important information contextual knowledge writing style provided input system 
knowing exactly probability distribution patterns classes eliminate rejections 
pattern rejected probable class win sufficient margin 
ffl patterns unrelated training data defining classifier 
instance atypical writing styles represented training database 
low probability areas pattern space poorly represented training set 
decision boundary classifier areas mere side effects training algorithm 
boundaries just irrelevant 
second cause rejection direct consequence finite nature training set 
knowing exactly probability distribution pattern classes reveal exact bayesian decision boundaries 
cause rejection rarely studied literature mere definition involves non asymptotical statistics closely related generalization phenomenon 
ffl high capacity learning system able model accurately high confidence parts decision boundary described training examples 
areas rejections misclassifications rare 
system produce unreliable high confidence decision boundaries poorly sampled areas pattern space rejection rare misclassifications frequent 
ffl alternatively low capacity learning system builds low confidence boundaries poorly sampled areas pattern space 
system rejects atypical patterns reduces number misclassifications 
unfortunately device performs poorly sampled areas 
unable take profit abundant data builds poor decision boundaries rejects looks ambiguous 
fact different properties learning algorithm required different areas input space 
words local capacity learning device match density training examples 
local learning algorithms 
generally admitted generalization performance affected global trade number training examples capacity learning system 
various parameters monotonically control capacity learning system guyon including architectural parameters number hidden units preprocessing parameters amount smoothing regularization parameters weight decay 
best generalization achieved optimal values capacity control parameters depend size training set 
fact holds rejection raw performance case pattern recognition regression density estimation tasks 
distribution patterns input space uneven proper local adjustment capacity significantly improve performance 
local adjustment requires capacity control parameters impact limited individual regions pattern space 
ways introduce parameters ffl experiment cf 
section illustrates solution 
testing pattern train learning system training examples located small neighborhood testing pattern 
apply trained system testing pattern 
parameters locally trained system de facto affect capacity global system small neighborhood defined testing pattern 
shall show nearest neighbor knn algorithm just particular case approach 
knn systems slow 
recognition speed penalized bayesian approach suggested denker le cun estimating error bars outputs learning system 
useful information affects interpretation outputs improve rejection performance suggested reviewer 
method improve rejection local algorithms 
square kernel selects examples located specific neighborhood 
smooth kernel gives different weight examples position respect kernel 
locality parameter measures size neighborhood 
selection closest training patterns execution training algorithm local learning system 
ffl second solution structure learning device ensures parameter affects capacity system small neighborhood 
example separate weight decay radial basis function rbf unit rbf network 
weight decay parameter affects capacity network locally 
similarly architectural choices modular network jacobs local impact capacity global system 
system trained recognition time affected local nature learning procedure 
subsections shall describe general statement local learning algorithms discuss related approaches explain locality parameter affects generalization performance 
weighted cost function 
general statement local learning algorithms 
define fw loss incurred network gives answer fw input vector actual answer capacity control parameters fl directly indirectly define subset fl weight space guyon 
non local algorithm searches subset weight vector fl minimizes empirical average loss training set delta delta delta 
fl arg min fl fw neighborhood defined point local algorithm searches weight vector fl minimizes weighted empirical average loss training set 
fl arg min fl gamma fw weighting coefficients defined kernel gamma width centered point various kernels including square kernels smooth kernels cf 

separate minimization performed neighborhood capacity control parameters fl kernel width adjusted separately neighborhood 
related approaches 
formulation allows variations concerning class function fw number neighborhoods shape kernels gamma scaling laws kernel width parameters fl 
selecting class constant functions respect input vectors quadratic loss gamma leads popular algorithms knn method rbf networks 
specific neighborhood algorithms try find constant approximation desired output arg min gamma gamma instance consider pattern recognition problem 
pattern belongs th class th coefficient corresponding desired output equal coefficients equal 
ffl testing pattern consider square kernel width adjusted contain exactly examples 
optimum mean desired outputs closest patterns 
highest coefficient corresponds represented class closest patterns nearest neighbors algorithm knn 
smooth kernel square kernel minimizing testing pattern computes estimates posterior probability classes 
parzen windows algorithm 
ffl consider fixed neighborhoods defined centers standard deviation gaussian kernels 
minimizing neighborhood computes weighted average desired values training examples 
evaluate output global complete system input pattern merge weighted averages values kernels global gamma radial basis functions rbf network broomhead lowe moody darken 
locality capacity 
theoretical methods developed non local algorithms apply local optimization 
particular best value capacity control parameters guyon depend number training examples 
context local algorithm effective number training examples modulated width kernels 
instance square kernel selects training set subset cardinality depends local density training set kernel width classical tradeoff capacity number examples reinterpreted tradeoff capacity locality 
increase locality reducing implicitly reduce effective number training examples available training local system 
knn rbf networks small kernels class constant functions 
reason believe best results obtained low capacity device 
conversely big multi layer networks non local high capacity 
modular networks jacobs sit somewhat extremes 
kernel functions embodied gating network selects combines outputs modules input data outputs modules 
phenomenon situation slightly complex capacity control parameters adjusted separately neighborhood 
local adjustment accurate kernel width small 
hand little adjust low capacity device 
experiments 
section discusses experiments simple local learning algorithm real size pattern recognition problem 
comparisons carried back propagation network knn parzen windows algorithms 
ffl back propagation network non local algorithm comparatively high capacity 
comparison shows introducing locality reducing capacity improves resulting performance network 
ffl knn classifier extremely local algorithm low capacity 
comparison shows reducing locality increasing capacity improves resulting performance 
simple local learning algorithm 
implemented simple local algorithm testing pattern linear classifier trained closest training examples euclidian distance 
trained network applied testing pattern effective number examples smaller number weights linear classifier 
strong weight decay fl required reduce capacity linear classifier 
weight decay pulls weights arbitrary origin 
isotropy reasons origin input space translated testing pattern subtracting selected training patterns 
advantage reducing eigenvalue spread hessian matrix 
training procedure computes explicit minimum quadratic cost incorporating weight decay term 
positive weight decay fl ensures required matrix inversion possible 
layer iii cells layer iv features input image field layer classification units layer ii cells layer cells lenet layers ii iii iv compute features 
layer performs classification 
replace layer local algorithm 
fl arg min gamma fw gamma new coordinate system centered testing pattern output network testing pattern equal bias vector 
highest output determines class selected pattern difference highest output second highest output certain threshold pattern rejected 
simple heuristic rule controls capacity versus locality tradeoff neighborhood 
adjusts locality leaves capacity constant 
words value weight decay parameter fl neighborhoods 
locality usually controlled kernel size increase density training example decreases 
fact selecting closest training examples equivalent having square kernel size somewhat adjusted local density training examples 
just value neighborhoods 
system extremely inefficient implements wide range compromises locality capacity values parameters locality parameter regularization parameter fl 
quality attractive experiment 
results comparisons 
trained system training set composed normalized images handwritten digits printed digits 
performance measured test set handwritten digits 
database le cun 
table gives raw error rejection error various systems 
raw error percentage misclassifications rejection performed 
rejection error percentage rejected pattern rejection threshold adjusted allow misclassification remaining patterns 
human performance segmented preprocessed digits provides point sackinger bromley personal communication 
nickname lenet designates network described le cun 
layer network performs feature extraction classification images single handwritten digits 
successive convolutional layers extract translation invariant features raw error rejection error human segmented digits lenet segmented digits knn lenet features parzen lenet features local lenet features table results optical character recognition task 
layer fully connected units performs classification cf 

network achieved error rejection error features computed lenet inputs pattern recognition systems 
systems viewed replacements layer lenet 
results directly compared 
best knn performance raw error obtained closest neighbors 
neighbors allow meaningful rejection strategy 
parzen system similar knn 
just replace square kernel gaussian kernel standard deviation half distance th closest pattern 
variations tested report best results raw error rejection error 
tested simple local learning algorithm described closest patterns weight decay 
weight decay train weights patterns 
time raw error rejection rate error best performances reached data set 
derivation reported appendix shows performance improvement statistically significant compares rejection curve local system rejection curve lenet 
local system performs better values threshold 
rejection single remaining error mislabeled pattern 
proper adjustment locality capacity parameters simple algorithm outperforms non local algorithm layer lenet extremely local algorithms knn parzen windows 
shows raw error local system changes best values weight decay fl 
significant performance changes obtained smooth kernels fancy heuristic controlling kernel width weight decay 
recognition speed 
simple system spends seconds recognizing single digit 
training network testing pattern certainly practical approach optical character recognition problem 
section solutions building local learning algorithms 
deliberately chosen implement simpler leads slow recognizers 
design systems second solution network structure allows local control capacity system 
system slightly slightly worse raw error rejection error reported le cun 
due slightly different definition rejection performance error remaining patterns vs error total ii robust preprocessing code 
error rejection curve 
curve shows error rates various rejection rates plain lenet dashed curve simple local algorithm operating features computed lenet plain curve 
times decay raw error evolutions raw error local system best values weight decay fl 
fact decay axis displays product flk 
complex handle smaller recognition time 

particular architectural change responsible performance breakthrough 
fact system linear replaces linear decision layer lenet 
change concerns training procedure 
performance improvement simply results better control basic tradeoffs involved learning process 
remains understood learning practical theoretical knowledge basic tradeoffs 
understanding tradeoffs affect specific learning problem allows take profit properties practical applications 
local learning algorithms just successful example strategy 
appendix confidence intervals 
section presents derivations establishing significance results 
derive non asymptotical formula computing confidence comparing classifiers test set independent examples 
classifier defines certain decision boundaries pattern space 
compare classifiers cases classifier right wrong 
call conditional probabilities error classifier classifier gives wrong answer 
similarly define numbers errors classifier classifier classifies correctly number common errors 
large number inequality hoeffding gamma gamma gamma ffl gamma gamma ffl furthermore name measured error rates test set gamma gamma gamma solving ffl right hand side inequality gamma compute minimum difference gamma ensures gamma larger probability gamma comparing decide classifier better result valid gamma gamma ln classifier better classifier probability gamma case systems achieve error test set size 
quantity smaller probably large margin 
choose get minimal significative error difference 
measuring actual value reduce margin 
significance results established refinement 

wish larry jackel group bell labs continuous support useful comments 
especially grateful yann le cun providing networks databases sackinger bromley providing human performance results 
broomhead lowe 

multivariate functional interpolation adaptive networks 
complex systems 
denker le cun 

transforming neural net output levels probability distributions 
lippmann moody touretzky editors advances neural information processing systems nips denver morgan kaufman 
guyon vapnik boser bottou solla 

structural risk minimization character recognition 
advances neural information processing systems volume denver 
morgan kaufman 
hoeffding 

probability inequalities sums bounded random variables 
journal american statist 
ass 
jacobs jordan nowlan hinton 

adaptive mixture local experts 
neural computation 
le cun boser denker henderson howard hubbard jackel 

handwritten digit recognition back propagation network 
touretzky editor advances neural information processing systems volume denver 
morgan kaufman 
moody darken 

fast learning networks locally tuned processing units 
neural computation 
vapnik 

principles risk minimization learning theory 
advances neural information processing systems volume denver 
morgan kaufmann 
appear 
