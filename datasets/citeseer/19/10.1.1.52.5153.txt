feature selection concave minimization support vector machines bradley computer sciences department university wisconsin madison wi cs wisc edu mangasarian computer sciences department university wisconsin madison wi cs wisc edu computational comparison feature selection approaches finding separating plane discriminates point sets dimensional feature space utilizes features dimensions possible 
concave minimization approach separating plane generated minimizing weighted sum distances misclassified points parallel planes bound sets determine separating plane midway 
furthermore number dimensions space determine plane minimized 
support vector machine approach addition minimizing weighted sum distances misclassified points bounding planes maximize distance bounding planes generate separating plane :10.1.1.47.5914
computational results show feature suppression indirect consequence support vector machine approach appropriate norm 
numerical tests public data sets show classifiers trained concave minimization approach trained support vector machine comparable fold cross validation correctness 
data sets tested classifiers obtained concave minimization approach selected fewer problem features trained support vector machine 
feature selection problem addressed discriminating finite point sets dimensional feature space separating plane utilizes features possible 
classification performance determined inherent class information available features provided 
logical conclude large number features provide discriminating ability 
finite training sample high dimensional feature space empty separators may perform training data may generalize 
importance feature selection problem classification 
optimization formulations section exploit realization occam razor bias compute separating plane small number predictive features discarding irrelevant redundant features 
formulations considered wrapper models defined 
approach described section involves minimization concave function polyhedral set 
plane constructed weighted sum distances misclassified points plane minimized dimensions original feature space 
achieved constructing parallel bounding planes small dimensional space possible bound sets extent possible placing sets opposite halfspaces determined planes 
planes determined sum weighted distances points wrong halfspace bounding plane minimized 
leads minimization concave function polyhedral set problems stationary point obtained successive lin algorithm algorithm 
final separating plane taken midway bounding parallel planes 
second approach support vector machine described section constructs parallel bounding planes dimensional space approach outlined addition attempts push planes far apart possible :10.1.1.47.5914
justification apart reducing vc dimension turn improves generalization linearly separable case apart planes smaller halfspace assigned sets reducing possibility new unseen points wrong set lie halfspace 
improved generalization primary purpose support vector machine formulation turns linear program resulting employing norm measure distance bounding planes leads feature selection method linear program resulting norm quadratic program resulting norm lead feature selection methods 
section describe computational experiments publicly available data sets approaches described sections 
goal evaluate generalization ability classifiers trained solving concave optimization problem versions support vector machine problem different norms robust linear program rlp 
rlp underlies proposed feature selection methods feature suppression capability built 
measure generalization ability fold cross validation 
numerical tests public data sets show classifiers trained concave minimization approach trained support vector machine comparable fold cross validation correctness 
data sets tested classifiers obtained concave minimization approach selected fewer problem features trained support vector machine 
computational time normally quadratic programming approach svms orders magnitude larger proposed linear programming approaches 
describe notation give background material 
vectors column vectors transposed row vector superscript vector jxj denote vector absolute values components vector denotes vector components maxf vector denotes vector components result applying step function component wise 
base natural logarithm denoted vector gammay denote vector components gammay kxk jx kxk max jn jx general norm delta dual norm delta defined kxk max kyk norm norm dual norms norm norm 
notation thetan signify real theta matrix 
matrix denote transpose denote th row vector ones real space arbitrary dimension denoted vector zeros real space arbitrary dimension denoted 
notation arg min denote set minimizers set separating plane respect point sets plane attempts separate halfspaces open halfspace contains points fsv feature selection concave minimization part describe feature selection procedure effective medical applications 
point sets represented matrices thetan thetan respectively wish discriminate separating plane fx flg normal norm distance origin jflj kwk 
shall attempt determine fl separating plane defines open halfspaces fx flg containing points fx flg containing points normalization wish satisfy aw efl bw efl gamma extent possible 
conditions satisfied convex hulls disjoint 
case real world applications 
attempt satisfy best sense minimizing norm average violations min fl fl min fl efl bw gamma efl recall vector denotes vector components maxf principal reasons choosing norm problem reducible linear program important theoretical properties making effective computational tool norm sensitive outliers occurring underlying data distributions pronounced tails similar effect robust regression pp 
formulation equivalent robust linear programming formulation rlp proposed effectively solve problems realworld domains minimize fl subject efl bw gamma efl linear program equivalently formulation define separating plane approximately satisfies conditions sense 
positive value determines distance kwk theorem point lying wrong side bounding plane fl lying open halfspace fx fi fi fl bounding plane fl 
similarly fl gamma 
objective function linear program minimizes average sum distances weighted kwk misclassified points bounding planes 
separating plane midway bounding planes parallel 
feature selection imposed attempting suppress components normal vector separating plane consistent obtaining acceptable separation sets achieve introducing extra term parameter objective weighting original objective gamma follows minimize fl gamma jwj subject efl bw gamma efl note vector jwj components equal corresponding components nonzero components equal zero corresponding components zero 
recall vector ones jwj simply count nonzero elements vector problem balances error separating sets number nonzero elements jwj 
element zero corresponding feature removed problem 
introducing variable able eliminate absolute value problem leads equivalent parametric program minimize fl gamma subject efl bw gamma efl gammav appears positively weighted objective constrained gammav effectively models vector jwj 
feature selection problem solved value resulting classification obtained separating plane midway bounding planes fl sigma generalizes best estimated cross validation tuning procedure 
typically achieved feature space reduced dimensionality number features 
discontinuity step function term approximate concave exponential nonnegative real line 
approximation step vector concave exponential ff gamma ff leads smooth problem fsv feature selection concave minimize fl gamma gamma subject efl bw gamma efl gammav shown theorem finite value ff appearing concave exponential smooth problem generates exact solution nonsmooth problem 
note problem minimization concave objective function polyhedral set 
difficult find global solution problem fast successive linear approximation sla algorithm algorithm terminates finitely usually steps stationary point satisfies minimum principle necessary optimality condition problem theorem leads sparse generalization properties 
convenience state sla algorithm 
algorithm successive linearization algorithm sla fsv 
choose 
start random fl 
having fl determine fl solving linear program minimize fl gamma ff gamma subject efl bw gamma efl gammav gamma gamma gamma ff gamma comment parameter ff set 
parameter chosen maximize generalization performance 
useful solutions fixed value ff 
approach involving computation solve increasing sequence ff values 
svm feature selection support vector machines support vector machine idea originally intended feature selection tool fact indirectly suppress components normal vector separating plane appropriate norm measuring distance parallel bounding planes sets separated :10.1.1.47.5914
svm approach consists adding term kwk objective function rlp similar manner appended term jwj problem 
delta dual norm measure distance bounding planes 
justification term follows 
separating plane generated rlp linear program lies midway parallel planes fl fl gamma 
distance measured norm delta planes precisely kwk theorem 
appended term objective function rlp kwk reciprocal distance driving distance planes obtain better separation 
results mathematical programming formulation svm formulation minimize fl gamma kwk subject efl bw gamma efl points appearing active constraints linear program positive dual variables constitute support vectors problem 
points data points relevant determining optimal separating plane 
number usually small proportional generalization error classifier 
norm measure distance planes dual norm norm accordingly kwk kwk leads linear programming formulation minimize fl gamma subject efl bw gamma efl gammae similarly norm measure distance planes dual norm norm accordingly kwk kwk leads linear programming formulation minimize fl gamma subject efl bw gamma efl gammas note method pattern separation proposed implemented just support vector machine approach forcing parallel planes bound sets separated far apart possible 
usually support vector machine problem formulated norm objective :10.1.1.47.5914
norm dual follows distance parallel planes defining separating surface measured norm formulation 
case kwk kwk usually appends term kwk objective resulting quadratic program minimize fl gamma subject efl bw gamma efl nonlinear separating surfaces linear parameters easily handled formulations 
data mapped nonlinearly phi nonlinear separating surface easily computed linear separator practice usually solves way dual 
formulation data enter inner products computed transformed space kernel function phi delta phi 
note separation errors weighted equally conforming svm formulations 
contrast formulations measure average separation error 
minimizing average separation error ensures solution occurs iff case unique theorem 
turn attention computational testing comparison 
computational results data sets wisconsin prognostic breast cancer database consists instances features representing follow data breast cancer case 
variants data set 
data set created elements set nuclear features plus diameter excised tumor number positive lymph nodes instances corresponding patients cancer months points 
set consisted features patients cancer months points 
second variant data set consisted features splits data differently 
elements corresponds patients cancer recurrence months points corresponds patients cancer months points 
johns hopkins university ionosphere data set consists continuous features instances 
instance represents radar return ionosphere 
set consists radar returns termed showing type structure ionosphere 
set consists radar returns termed bad signals pass ionosphere 
cleveland heart disease data set consists instance features see documentation 
set consist instance 
set consists instances 
correctness vs tune test tuning testing sets correctness support vector machine versus sparsity inducing parameter months data set 
dashed tuning correctness solid test correctness 
pima indians diabetes data set consists instances features plus class label see documentation 
instances class label place instances class label placed bupa liver disorders data set consists instances features plus selector field split data sets see documentation 
set consists instances set consists instances 
experimental methodology goal evaluate generalization ability classifiers obtained solving concave minimization problem fsv svm norm problem svm norm problem svm norm problem robust linear program rlp 
estimate generalization ability classifier fold cross validation 
note objective function parameter induce sparsity chosen carefully maximize generalization ability resulting classifier 
choosing maximize training correctness resulting classifier classifier performs poorly data training set 
employ tuning set procedure choosing fold fold crossvalidation candidate set perform set aside training data tuning set ii obtain classifier value iii determine correctness tuning set iv repeat steps iii times time setting aside different portion training data 
score value average correctness values determined iii 
fix value best score determined tuning procedure ties broken choosing smallest value 
value fold fold cross validation 
set set candidate values experiments set 
curves indicate value maximizes tuning score dashed curve estimate value maximizes test set correctness solid curve 
experimental results table summarizes average number original problem features selected classifiers trained methods 
table summarizes results fold crossvalidation experiments real world data sets 
train test numbers average folds 
value indicator significance difference test correctness classifiers obtained solving fsv classifiers obtained solving svm norm problem recall high value indicates difference significant 
note values calculated pairwise comparisons solutions obtained solving svm norm svm norm rlp suppress problem features see table 
discussion fsv svm norm problems ones exhibiting feature selection table 
data sets tested svm norm classifiers performed slightly better data sets fsv classifiers performed slightly better data sets 
minimum value indicates classifiers obtained fsv svm norm methods similar generalization properties 
applying paired test fold cross validation results may indicate difference average test specifically value tailed paired test testing hypothesis difference test fsv svm norm classifiers zero set correctness 
results experiments may similar indicated values 
note classifiers obtained solving svm norm suppressed original problem features largest values near general little accompanied poor set separation 
similar behavior observed solving svm norm problem 
note norm sensitive outliers norm squared 
classifiers obtained solving fsv problem selected fewer problem features svm formulations rlp fsv classifiers reduced number features svm norm month maintaining comparable generalization performance 
month dataset fsv classifiers svm norm classifiers selected nuclear area feature number lymph nodes removed patient 
features deemed relevant prognosis problem 
linear programs formulations solved cplex package called matlab 
quadratic programming problem solved matlab quadratic optimization solver encountered difficulty conditioning qp constraint matrix may affect interpretation results approach 
see table average solve times 
summary computational comparisons classifiers obtained solving mathematical optimization problems 
optimization formulations linear quadratic solved finite sequence linear programs solving algorithm 
classifiers obtained solving fsv problem svm norm problem exhibit feature suppression comparable generalization performance publicly available real world data sets tested 
classifiers obtained solving fsv problem suppressed features corresponding svm norm classifiers 
quadratic svm took orders magnitude time svms 
distance parallel planes defining separating surface svm problem chosen norm resulting svm optimization problem norm dual norm norm appearing objective 
classifiers obtained solving problem svm norm exhibit feature selection 
similar behavior observed classifiers obtained solving svm norm problem 
generalization ability classifiers comparison needs investigated 
includes analysis benefits measuring distance bounding parallel planes defining separating plane resulting optimization problem utilizing dual norm 
characterization classes data sets lend better separation choice norm allow practitioners choose priori optimization formulation believed best suited separation problem hand 
supported national science foundation ccr ccr air force office scientific research mathematical programming technical report february 
bennett blue 
support vector machine approach decision trees 
department mathematical sciences math report rensselaer polytechnic institute troy ny 
www math rpi edu 
bennett mangasarian 
robust linear programming discrimination linearly inseparable sets 
optimization methods software 
blumer ehrenfeucht haussler warmuth 
occam razor 
information processing letters 
bradley mangasarian rosen 
parsimonious norm approximation 
technical report computer sciences department university wisconsin madison wisconsin march table average number features selected classifiers 
asterisk indicates full experiment carried excessive time see table results averaged folds completed 
data set fsv svm delta svm delta svm delta rlp mo mo ionosphere cleveland pima indians bupa liver table fold cross validation correctness results publicly available data sets 
asterisk indicates full experiment carried excessive time see table results averaged folds completed 
data set fsv svm delta value svm delta svm delta rlp train train train train train test test test test test mo mo ionosphere cleveland pima indians bupa liver 
computational optimization applications appear 
ftp ftp cs wisc edu tech reports ps bradley mangasarian street 
feature selection mathematical programming 
informs journal computing 
appear 
ftp ftp cs wisc edu tech reports ps burges 
tutorial support vector machines 
data mining knowledge discovery 
appear 
cortes vapnik 
support vector networks 
machine learning 
cplex optimization incline village nevada 
cplex tm linear optimizer cplex tm mixed integer optimizer version 
dietterich 
approximate statistical tests comparing supervised classification learning algorithms 
neural computation 
appear 
www cs orst edu tgd cv pubs html 
girosi 
equivalence sparse approximation support vector machines 
memo artificial intelligence laboratory mit cambridge massachusetts 
www ai mit edu people girosi homepage svm html 

fundamentals artificial neural networks 
mit press cambridge ma 
table average running ionosphere data set 
method time seconds algorithm delta delta delta rlp hertz krogh palmer 
theory neural computation 
addison wesley redwood city california 
huber 
robust statistics 
john wiley new york 
john kohavi pfleger 
irrelevant features subset selection problem 
proceedings th international conference machine learning san mateo ca 
morgan kaufmann 
koller sahami 
optimal feature selection 
saitta editor machine learning proceedings thirteenth international conference icml bari italy july pages san francisco ca 
morgan kaufmann 
mangasarian 
linear nonlinear separation patterns linear programming 
operations research 
mangasarian 
multi surface method pattern separation 
ieee transactions information theory 
mangasarian 
nonlinear programming 
mcgraw hill new york 
reprint siam classic applied mathematics philadelphia 
mangasarian 
machine learning polyhedral concave minimization 
fischer editors applied mathematics parallel computing festschrift klaus ritter pages 
physica verlag springer verlag heidelberg 
ftp ftp cs wisc edu tech reports ps mangasarian 
arbitrary norm separating plane 
technical report computer sciences department university wisconsin madison wisconsin may 
operations research letters submitted 
ftp ftp cs wisc edu tech reports ps mangasarian street wolberg 
breast cancer diagnosis prognosis linear programming 
operations research july august 
matlab 
user guide 
mathworks 
murphy aha 
uci repository machine learning databases 
technical report department information computer science university california irvine 
www ics uci edu mlearn mlrepository html 
osuna freund girosi 
training support vector machines application face detection 
ieee computer vision pattern recognition puerto rico june 
www ai mit edu people girosi homepage svm html 
shavlik dietterich editors 
readings machine learning 
morgan kaufman san mateo california 
stone 
cross choice assessment statistical predictions 
journal royal statistical society 
vapnik 
nature statistical learning theory 
springer new york 
wahba 
support vector machines reproducing kernel hilbert spaces randomized 
technical report department statistics university wisconsin madison wi 
ftp ftp stat wisc edu pub wahba index html 
