convergence iterative decoding graphs single cycle srinivas aji gavin horn robert mceliece department electrical engineering california institute technology pasadena ca usa mail mas systems caltech edu understood turbo decoding algorithm instance probability propagation algorithm ppa graph cycles :10.1.1.73.8252:10.1.1.115.4360
ppa type algorithms known give exact results underlying graph cycle free 
important study approximate correctness ppa graphs cycles 
step discussing behavior ppa graphs single cycle 
directly relevant study iterative decoding tail biting codes underlying graph just cycle 
shall show strictly positive local kernels iterations ppa converge fixed point regardless scheduling order 
length cycle play role convergence 
secondly shall generalize result mceliece showing hidden variables cycle binary valued decision ppa brute force map calculation 
hidden variables assume values ppa may converge erroneous non map decision rare 
shall results experiments give theorem nonbinary case serve characterize various behaviors ppa graph single cycle 

consider behavior ppa type algorithms graphs single cycle 
kinds graphical message passing algorithms decoding 
algorithms include gallager tanner wiberg decoding algorithm forward backward algorithm pearl belief propagation algorithm shafer shenoy probability propagation algorithm turbo decoding algorithm 
shall simply call algorithms probability propagation algorithms ppa 
outline 
section introduce notation aji mceliece describe single cycle junction graph message passing state computation ppa graph 
section message passing schedule section prove ppa converges fixed point observed anderson weiss 
show message passing single cycle junction graph viewed matrix operation 
section compare final state node objective function node section partially supported nsf 
ncr afosr qualcomm partially supported nserc scholarship 
intuitive explanation matrix associated node 
section generalizing result mceliece show ppa converges correct decision binary hidden variable 
observed independently weiss 
section experiments theorem illustrate behavior ppa non binary case 

background section shall introduce notation aji mceliece describe message passing states nodes ppa junction graph 
general case single cycle junction graph 
xn variables values finite sets respectively 
fi subset ng xs represents variable list fx variable list fx xng denoted define fs sm subsets ng shall call local domains 
local domain shall define function called local kernel ff set non negative real numbers 
global kernel defined product local kernels follows fi xn ff xs compute marginalization global kernel subset variables shall call th objective function fi xs defined fi xs fi may want compute objective function local domain just subset 
define set vertices fv vm corresponds local domain connect vertices form junction graph set edges graph 
junction graph graph induced subgraph formed vertices xs connected 
junction graph unique 
consider junction graphs contain exactly cycle 
graph pass messages edge adjacent vertices 
select vertices message passed table containing values function define state vertex table containing values function oe table contains jas ja theta delta delta delta theta ja entries 
define rules update oe xs fl ns ff xs xs oe xs fl ff xs xs fl case normalization constant sum elements table constant 
junction graph tree message passing compute fi desired vertex appropriate scheduling 
objective function fi state vertex message passing terminates 
junction graph tree message passing general compute objective function fi message state computation rules 
sections follow investigate relationship objective function fi vertex state oe perform message passing junction graph contains single cycle 

message passing schedule consider junction graph local kernel ff xs 
contain exactly cycle length relabel vertices cycle fv gamma relabel rest vertices arbitrarily fv vm define gamma possible scheduling message passing begins directing edge cycle vertex cycle 
messages sent direction edges 
vertex cycle send message neighbor time received messages neighbors 
scheduling message passing begins leaves proceeds vertex cycle received messages neighbors cycle 
define gamma variables shared vertex cycle 
define new local kernel ff follows ff ns ff xs gamma xs define gamma gamma vertices adjacent cycle 
perform message passing induced subgraph new local kernel ff 
single cycle junction graph 
example consider junction graph shown vertices cycle relabeled 
stage message passing results shown 
new junction graph just single cycle vertices having new local kernels ff ff ff respectively 
note junction graph expressed junction tree maintain junction property 
fx fx fx fx fx fx fx fx fx original junction graph junction graph stage message passing completed 
second stage pass messages serially directions vertex starting vertex wish compute objective function 
message passed neighbor time message received neighbor 
section show message passing simply matrix multiplication operation messages converge strictly positive local kernels 

message passing convergence consider vertices gamma gamma shown 
rewrite tables gamma vectors length ja gamma ja respectively 
rewrite local kernel ff ja theta ja gamma matrix label columns elements gamma ordering vector gamma label rows elements ordering vector entries matrix consist value ff row column labeling consistent assignment values 
denote resultant vectors gamma matrix phi update rule clear phi gamma pass messages vertices opposite direction see gamma phi construct matrix phi elements phi function original local kernel ff xs incoming messages received vertices adjacent outside cycle 
elements matrix phi value depending define local kernel incoming messages 
consider messages passed directions edge message passed direction propagate vertices cycle due message passing schedule 
message multiplied matrix vertex cycle rewrite updated message terms old message follows new old gamma gamma gamma vertices gamma form part cycle phi phi phi gamma delta delta delta phi ordered product matrices associated vertex visited cycle 
similarly new old strictly positive eigenvalue largest modulus perron theorem exists unique largest positive eigenvalue corresponding non negative eigenvector 
normalized converge non negative left right eigenvectors largest eigenvalue largest eigenvalue strictly positive matrix unique convergence fixed point guaranteed initial vectors 
product matrices chosen arbitrarily produce matrix elements cycle arbitrary length 
state vertex product incoming messages local kernel 
means state converges fixed point 
matrix determines final state arrive final state length cycle 
performance ppa respect making optimal decisions independent cycle length dependent local kernels incoming messages cycle 
ppa converges single cycle graph scheduling 
shown ppa converge fixed point general message passing schedules example parallel scheduling flooding described :10.1.1.73.8252
question remains converges 
junction tree state vertex objective function vertex message passing terminated 
true general junction graph cycles 
section compare ppa computed state vertex objective function vertex 

objective function vs state compute objective function set variables xs ae fx xng edges cycle 
ways 
vertex xs xs local kernel ff xs constant compute objective function vertex exist create vertex constant kernel defined variable set xs insert vertex edge xs xs xs clear addition affect messages kernel constant messages normalized step 
define new fug new fe state vertex product incoming outgoing messages edges 
see state computed vertex compares objective function vertex look example example consider junction graph shown 
compute objective function fi xs 
ff xs need add new vertex 
add vertex edge assume ff message passing converged oe ff ff messages eigenvectors matrix corresponding vertex message originates 
objective function fi fi fi ff ff second equality due definition fi function junction graph 
clear general state equal objective function oe fi general 
matrix associated phi phi phi identity matrix 
look diagonal entries find values function fi multiplied constant 
re normalize affecting messages passed 
general final state vertex componentwise product left right eigenvectors associated largest eigenvalue matrix values objective function diagonal 
equivalently say message passing estimates matrix matrix consisting outer product left right eigenvectors largest eigenvalue 
know best rank estimate matrix consisting outer product left right singular vectors associated largest singular value really interested approximating diagonal case singular value estimate optimal general 
section examine matrix detail intuitive explanation meaning diagonal elements 

interpretation elements consider junction graph shown 
compute objective function variable set xs fx associated vertex additional constraints xs xs xs xs xs ff xs constant 
final state simply component wise product left right eigenvectors associated largest eigenvalue matrix copy vertex identical vertices break cycle get graph shown 
vertices variables common original junction graph single cycle graph results vertex split cycle broken 
resulting graph may satisfy junction property 
order satisfy junction property relabel variables associated xs fx relabel variables associated vertices accordingly junction property holds 
pass messages serially write final state local kernels probabilities interpret probability transition matrix markov chain 
choice original kernels arbitrary necessarily stochastic matrix row may sum different value 
th element matrix interpreted proportional probability paths chain starting state diagonal matrix contains probabilities path starts ends state 
initially variable set normalize dividing trace case probabilities elements diagonal matrix values computed objective function fi diagonal copies argument fi equal diagonal 
diagonal elements meaning non cyclic graph variable assigned single value time 
values diagonal elements determined structure local kernels vertex cycle affect performance ppa cycle 
generally coding concerned actual values state computed ppa values objective function 
want map decision values care objective function ppa state result decision 
sections follow show ja hidden variable associated binary valued ppa map decision computed objective function 
hidden variable set take values diagonal elements tend act noise cause ppa decision different objective function decision 

binary valued hidden variables consider vertex ja contains single binary valued variable say matrix defined normalized 
normalized trace diagonal elements longer constrained 
values objective function computed fi fi respectively 
largest eigenvalue gamma right left eigenvectors gamma gamma respectively 
loss generality decide hard decision objective function componentwise product eigenvectors decision satisfy gamma easily verified 
define delta gamma gamma difference value computed objective function ppa state 
observations delta 
ffl delta iff singular 
rank equal outer product left right eigenvectors associated ffl difference dependent product diagonal elements individual values 
ffl lim delta diagonal elements large respect diagonal tends decision ppa certain 
ffl lim delta diagonal elements small respect diagonal ppa tends hard decision 
binary valued hidden variable ppa correct decision certainty decision dependent product diagonal elements 
state vertex objective function matrix singular 
junction graph fx zg fxg fx zg belief network ye ze simple belief network consisting nodes loop corresponding junction graph shown stage message passing completed 

ary valued hidden variables ja harder characterize performance ppa terms matrix section results experiments theorem highlight characteristics ppa ary valued random variables 
experiments state objective functions deal probabilities refer values computed state function ppa probabilities computed objective function optimal exact probabilities 
experiment experiment demonstrates simple junction graph ppa estimates values objective function typically erroneous decision decision exact probabilities close call 
consider belief network junction graph stage message passing shown figures respectively 
hidden variables ternary valued 
dependencies belief network defined follows uniform prior symmetric channel crossover probability mod sum ze ye noisy versions respectively noise gaussian mean variance oe fix crossover probability variance noise oe compute objective function node decision hidden variable observation variables ze ye making decision ppa form matrix particular evidence set find component wise product left right eigenvectors largest eigenvalue 
decision value corresponding largest value component wise product 
equivalent performing ppa algorithm network large number iterations decision converge final value 
shows ppa decision regions various observed values ze ye shows decision regions determined computing objective function shows regions decisions ppa objective function differ 
ppa optimal error vs decision regions hidden variable ppa exact probabilities objective function 
black grey white regions correspond deciding respectively 
plots show regions ppa objective function different decisions vs channel crossover probability respectively 
experiment ppa incorrect decision objective function decision near decision region boundary exact probabilities values near objective function decision close call 
fixed calculate probability ppa making incorrect decision respect objective function decision integrating joint distribution ze ye error region 
results shown various 
alternatively look ppa estimates objective function computed shows ppa probability vs optimal probability computed trials 
see experiment particular network ppa estimates objective function quite 
experiment second experiment demonstrates effect diagonal elements convergence ppa correct decision 
consider system 
define signal matrix consisting non negative diagonal matrix trace equal 
define noise matrix consisting non negative matrix zero diagonal diagonal elements 
define real non negative constants get matrix bn matrix associated vertex hidden variable junction graph single cycle 
optimal probability plot state vs objective function trials network 
shown create junction graph produce state product incoming messages values diagonal signal matrix calculated objective function vertex diagonal noise matrix values due single cycle nature junction graph 
observations ffl lim diagonal elements ppa hard decision correctly decide largest element diagonal 
ffl lim diagonal elements ppa decision entirely left right eigenvectors associated largest eigenvalue ppa decision independent actual objective function correct incorrect depending nature noise 
ffl intermediate values ppa may may correct decision depending mean diagonal diagonal ratio dor elements generally large dor ppa correct decision 
likelihood correct decision decreases dor similar manner way codes perform poorly signal noise ratio decreased 
theta matrices shows plot state probabilities calculated ppa vary simplex varied 
case state probability estimate starts corner corresponding largest element diagonal moves estimate performing ppa just noise matrix theta matrices chosen ppa acting noise matrix result incorrect decision 
indicates position diagonal simplex see large small mean dor ppa incorrect decision 
noise matrix results ppa making correct decision conjecture ppa optimal probability plots state probability vector varies varied ppa probability vs optimal probability computed trials picked diagonal elements uniformly 
correct decision 
proof run numerous experiments find counter example 
plot shows ppa probability vs optimal probability computed trials picked diagonal elements uniformly 
see elements chosen random actual junction graph ppa perform 
plot shows probability making correct decision vs mean dor different distributions diagonal elements 
ran experiments selected diagonal elements absolute value gaussian random variable exponential uniform distribution similar results case 
mean dor cases important determining probability correct decision ppa actual distribution diagonal elements 
results occurred experiments theta theta matrices 
theorem theorem serves illustrate effect noise matrix diagonal elements 
case mean diagonal diagonal ratio pr gaussian exponential uniform plot ppa vs mean dor different distributions diagonal elements various distributions diagonal elements 
noise matrix written gamma ones matrix identity matrix 
shall show case ppa converges correct decision regardless mean dor 
theorem consider matrix gammai largest eigenvalue associated eigenvector un 
ii proof consider matrix ii write mu am bu bun bu am bun 
bu bu 
un am bu bun am ii bu bun gamma bu subtracting get gamma ii gamma define hermitian largest eigenvalue am pi goes certainty decision decreases decision remains correct 
ppa leaves correct decision region regardless value 
message passing algorithms reduce complexity decoding tail biting codes 
single cycle graph message passing converges strictly positive local kernels interpreted matrix multiplication node 
binary hidden variables ppa correct decision actual probability correct side 
non binary case performance message passing respect optimal decoder dependent diagonal elements matrix diagonal elements function structure code noise channel 
ppa performance code design code minimize diagonal elements nearly 
topic subject research 
aji mceliece generalized distributive law preliminary versions isit 
current version available www systems caltech edu ee faculty anderson map decoders ieee selected areas communication vol 
pp 
february 
calderbank forney jr vardy minimal tail biting code submitted ieee trans 
inform 
theory august 
forney jr forward backward algorithm proc 
th allerton conference communication control computing october pp 

horn johnson matrix analysis 
new york cambridge university press 
jensen bayesian networks 
new york springer verlag 
kschischang frey iterative decoding compound codes probability propagation graphical models ieee selected areas communication vol 
pp 
february 
mceliece mackay 
cheng turbo decoding instance pearl belief propagation algorithm ieee selected areas communication vol 
pp 
february 
mceliece 
cheng turbo decision algorithm proc 
rd allerton conference communication control computing october pp 

pearl probabilistic reasoning intelligent systems networks plausible inference 
san mateo ca morgan kaufmann publishers 
shafer shenoy probability propagation annals math 
art 
intel vol 
pp 

solomon connection block convolutional codes siam appl 
math vol 
pp 
october 
weiss belief propagation revision networks loops memo november 
