language dynamical system jeffrey elman university california san diego despite considerable diversity theories humans process language number fundamental assumptions shared theories 
consensus extends basic question counts cognitive process 
cognitive scientists fond referring brain mental organ chomsky implying similarity organs liver kidneys assumed brain organ special properties set apart 
brains carry computation argued entertain propositions support representations 
brains may organs different organs body 
obviously substantial differences brains kidneys just kidneys hearts skin 
silly differences 
hand cautionary note order 
domains various organs operate quite different common biological substrate quite similar 
brain quite remarkable things similar human symbol processors profound differences brain digital symbol processors attempts ignore grounds simplification abstraction run risk fundamentally misunderstanding nature neural computation churchland sejnowski 
larger sense raise general elman page warning ed hutchins suggested cognition may think 
things suggest chapter language cognition general may usefully understood behavior dynamical system 
believe view acknowledges similarity brain bodily organs respects evolutionary history nervous system acknowledging remarkable properties possessed brain 
view outline representations symbols regions state space 
rules operations symbols embedded dynamics system dynamics permits movement certain regions making transitions difficult 
emphasize am arguing language behavior rule governed 
suggest nature rules may different conceived 
remainder chapter organized follows 
order clear dynamical approach instantiated concretely connectionist network differs standard approach summarizing central characteristics traditional approach language processing 
shall describe connectionist model embodies different operating principles classical approach symbolic computation 
results simulations architecture discussed 
discuss results may yielded perspective 
grammar lexicon traditional approach language processing traditionally assumed involve lexicon repository facts concerning individual words set rules constrain ways words combined form sentences 
point view elman page listener attempting process spoken language initial problem involves acoustic input retrieving relevant word lexicon 
process supposed involve separate stages lexical access contact candidate words partial information lexical recognition retrieval selection choice specific word finer grained distinctions may useful tyler 
subsequent recognition retrieved word inserted data structure eventually correspond sentence procedure assumed involve application rules 
described scenario may simple straightforward controversial 
fact considerable debate number important details 
instance lexicon passive active 
models lexicon passive data structure forster 
models lexical items active mcclelland elman morton style selfridge demons selfridge 
lexicon organized entry points 
active models internal organization lexicon issue lexicon usually content addressable direct simultaneous contact unknown input relevant lexical representations 
passive models additional look process required organization lexicon important efficient rapid search 
lexicon may organized dimensions reflect phonological orthographic syntactic syntactic properties may organized usage parameters frequency forster 
problems include catalog morphologically related elements telephone separate entries 
girl girls 
ox represent words multiple meanings various meanings bank may different elman page warrant distinct entries various meanings run subtly different distant clearly related meanings lexicon includes information argument structure 
recognition graded 
theories recognition occurs point spoken word uniquely distinguished competitors marslen wilson 
models may consistent point recognition occurs recognition graded process subject interactions may hasten slow retrieval word context 
recognition point strategically controlled threshold mcclelland elman 
lexical competitors interact 
lexicon active potential interactions lexical competitors 
models build inhibitory interactions words mcclelland elman suggested empirical evidence rules word word 
sentence structures constructed words 
single question rise vast complex literature 
nature sentence structures debated reflecting diversity current syntactic theories 
addition considerable controversy sort information may play role construction process degree pass parse restricted purely syntactic information available frazier rayner trueswell tanenhaus kello 
considerable number questions remain open 
believe accurate say considerable consensus regarding certain fundamental principles 
take consensus include 
commitment discrete context free symbols 
elman page readily obvious case classical approaches connectionist models utilize localist representations entities discrete atomic graded activations may reflect uncertain hypotheses 
central feature forms representation localist connectionist symbolic intrinsically context free 
symbol word example regardless usage 
gives systems great combinatorial power limits ability reflect idiosyncratic contextually specific behaviors 
assumption leads distinction types tokens motivates need variable binding 
types canonical context free versions symbols tokens versions associated specific contexts binding operation enforces association means indices subscripts diacritics 
view rules operators lexicon operands 
words models conceived objects processing 
models lexical entries may active word recognized subject grammatical rules build higher level structures 
static nature representations 
processing language clearly unfolds time representations produced traditional models typically curiously static quality 
revealed ways 
instance assumed lexicon pre exists data structure way dictionary exists independently 
similarly higher level structures created sentence comprehension built process successful product comprehension mental structure constituent parts words categories relational information simultaneously 
presumably inputs subsequent interpretive process constructs discourse structures 
processing models performance models elman page take seriously temporal dynamics involved computing target structures target structures inherited theories ignore temporal considerations competence models 
building metaphor 
traditional view act constructing mental representations similar act constructing physical 
precisely claimed physical symbol system hypothesis simon 
view words constituents bricks building rules mortar binds 
processing proceeds representation grows building construction 
successful processing results mental complete consistent structure building 
take assumptions widely shared researchers field language processing rarely stated explicitly 
furthermore assumptions formed basis large body empirical literature played role framing questions posed interpreting experimental results 
certainly incumbent theory offered replacement provide framework describing empirical phenomena improving understanding data 
interested theory 
reason view mental life just described view relies discrete static passive context free representations appears sharply variance known computational properties brain churchland sejnowski 
acknowledged theories language subscribe assumptions listed provide great deal coverage data coverage flawed internally inconsistent ad hoc highly controversial 
unreasonable raise question shortcomings theories arise assumptions basically flawed 
better ways understanding nature elman page mental processes representations underlie language 
section suggest alternative view computation language processing seen place dynamical system 
lexicon viewed consisting regions state space system grammar consists dynamics attractors constrain movement space 
see approach entails representations highly context sensitive continuously varied probabilistic course probabilities objects mental representation better thought trajectories mental space things constructed 
entry point describing approach question deals time problem serial processing 
language behaviors unfolds processed time 
simple fact simple trivial turns problematic explored detail 
turn question time 
describe connectionist approach temporal processing show applied linguistic phenomena 
final section turn pay attempt show approach leads useful new views lexicon grammar 
problem time time medium behaviors unfold context understand world 
recognize causality causes precede effects learn coherent motion time points retinal array indicator difficult think phenomena language goal directed behavior planning way representing time 
time arrow central feature world easy think having acknowledged pervasive presence little needs elman page said 
time stumbling block theories 
important issue models motor activity example nature motor intention 
action plan consist literal specification output sequences probably represent serial order manner probably fowler jordan rosenbaum kelso 
realm natural language processing considerable controversy information accumulates time information available altmann steedman ferreira henderson trueswell tanenhaus kello press 
time challenge connectionist models 
early models reflecting initial emphasis parallel aspects models typically adopted spatial representation time mcclelland rumelhart 
basic approach illustrated 
temporal order input events represented spatial order left right input vector 
number problems approach see elman discussion :10.1.1.117.1928
serious left right spatial ordering intrinsic significance level computation meaningful network 
input dimensions orthogonal input vector space 
human eye tends see patterns having undergone spatial temporal understand representing ordered sequence translation notation suggests special relationship may exist adjacent bits 
relationship result considerable processing human visual system intrinsic vectors 
element vector closer useful sense second element element 
important available simple networks form shown 
particularly unfortunate elman page consequence basis architectures generalizing learned spatial temporal stimuli novel patterns 
insert models explored intuitively appropriate idea time represented effects processing 
network connections include feedback loops goal achieved naturally 
state network function current inputs plus network prior state 
various algorithms architectures developed exploit insight elman jordan mozer pearlmutter rumelhart hinton williams :10.1.1.117.1928
shows architecture simple recurrent network studies reported 
insert srn architecture time hidden units receive external input collateral input time context units simply implement delay 
activation function hidden unit familiar logistic net input unit time net input tick clock includes weighted sum inputs node bias weighted sum hidden unit vector prior time step 
henceforth referring state space system shall referring specifically dimensional space defined hidden units 
typical feedforward network hidden units develop representations enable network perform task hand rumelhart hinton net net net ij ik elman page williams 
representations may highly 
similarity structure internal representations reflects demands task learned similarity inputs form 
recurrence added hidden units assume additional function 
provide network memory 
true feedforward network encoding temporal history task relevant may highly rarely case encoding resembles verbatim tape recording 
task srn proven useful prediction 
reasons attractive train network predict 
arises supervised learning algorithms backpropagation error question teaching information comes 
cases plausible rationales justify teacher 
teacher reflects important theoretical biases avoid example interested network generate alternative theories 
teacher prediction task simply time lagged input represents information directly observable environment relatively theory neutral 
furthermore reason believe anticipating plays important role learning world 
prediction powerful tool learning temporal structure 
insofar order events may reflect past complex non obvious ways network required develop relatively encodings dependencies order generate successful predictions 
srn architecture forms recurrent networks variety applications yielded promising results 
srn ability handle temporal sequences particularly relevant architecture modeling language behaviors 
deeper question arises solutions recurrent network architectures differ substantial ways traditional models 
solutions elman page different differences positive negative 
rules representations dynamical perspective observation networks dynamical systems 
means state point time function reflects prior state see norton volume detailed review definition characteristics dynamical systems 
computational properties networks fully known clear considerable siegelmann sontag 
reasonable conceptual notions associated discrete automata theory symbolic computation may offer insight functioning concepts dynamical systems theory pollack 
networks applied problems relevant language processing suggest different view underlying mechanisms language 
way approach consider problem elements language may ordered 
language domain ordering elements particularly complex 
word order instance reflects interaction multiple factors 
include syntactic constraints semantic pragmatic goals discourse considerations processing constraints verb particle constructions run may split direct object noun phrase long disrupt processing discontinuous verb unit 
subscribes view knowledge sources exert effects autonomously interactively question final output word stream reflects joint interplay 
know linear order linguistic elements provides poor basis characterizing regularities exist sentence 
noun may elman page agree number verb immediately follows separated arbitrarily great distance 
children pl pl ice cream 
girl sg baby sits wednesday parents go likes sg ice cream 
considerations led miller chomsky argue algorithms infeasible language learning number sentences listener need hear order know precisely words precede likes determines correct number likes vastly outnumber data available fact conservative estimates suggest time needed available entire individual lifetime 
hand recognition dependencies respect underlying hierarchical structure vastly simplifies problem subject nouns english agree number verbs embedded clauses may intervene participate agreement process 
way challenge simple recurrent network problem relevance language attempt train predict successive words sentences 
know hard problem solved general way simple recourse linear order 
know task psychological validity 
human listeners able predict word endings beginnings listeners predict grammaticality partial sentence input sequences words violate expectations unpredictable result distinctive electrical activity brain 
interesting question network trained predict successive words 
simulations shall see course solving task network develops novel representations lexicon elman page grammatical rules 
lexicon structured state space words may categorized respect factors 
include traditional notions noun verb argument structures associated semantic features 
characteristics predictive word syntagmatic properties 
reverse true 
distributional facts infer word semantic categorial features 
goal simulation see network backwards just sense 
small lexicon nouns verbs form simple sentences see elman details :10.1.1.117.1928
word represented localist vector single randomly assigned bit turned 
input representation ensured form word correlated properties classifications discovered network solely distributional behavior 
network similar shown trained set sentences word sequence network sentence concatenated preceding sentence 
task network predict successive word 
word input output prediction input compared actual word weights adjusted backpropagation error learning algorithm 
training network tested comparing predictions corpus 
corpus non deterministic reasonable expect network short memorizing sequence able exact predictions 
network predicted cohort potential word successors context 
activation cohort turned elman page highly correlated conditional probability word context mean cosine output vector empirically derived probability distribution 
behavior suggests order maximize performance prediction network identifies inputs belonging classes words distributional properties occurrence information 
classes represented overt form word orthogonal 
network free learn internal representations hidden unit layer capture implicit information 
test possibility corpus sentences run network final time 
word input hidden unit activation pattern produced word plus context layer saved 
words mean vector computed averaging instances word contexts 
mean vectors taken prototypes subjected hierarchical clustering 
point see inter vector distances revealed similarity structure hidden unit representation space euclidean distance taken measure similarity 
tree constructed hierarchical clustering 
insert similarity structure revealed tree indicates network discovered major categories words 
largest categories correspond input vectors verbs nouns 
verb category subdivided verbs require direct object intransitive corpus direct object optional 
noun category broken animates 
animates contain classes human nonhuman subdivided large animals small animals 
divided elman page miscellaneous 
said network obviously knows real semantic content categories 
simply inferred category structure exists 
structure inferred provides best basis accounting distributional properties 
obviously full account language require explanation structure content grounded body world 
interesting evidence structure inferred easily basis form internal evidence result may encourage caution just information implicit data difficult may information construct framework conceptual representation 
main point suggest primary way grammatical categories acquired children believe cooccurrence information may play role learning 
primary thing focus simulation suggests nature representation systems sort 
consider representational properties networks apart specific conditions give rise representations 
lexicon network 
recall earlier assumptions lexicon typically conceived passive data structure 
words objects processing 
subject acoustic phonetic analysis internal representations accessed recognized retrieved permanent storage 
internal representations inserted grammatical structure 
status words system sort described different words objects processing inputs drive processor direct manner 
wiles suggest useful understand inputs networks sort operators elman page operands 
inputs operate network internal state move position state space 
network learns time response different words context account 
words reliable systematic effects behavior surprising instances word result states tightly clustered grammatically semantically related words produce similar effects network 
choose think internal state network processes word representing word context accurate think state result processing word representation word 
note implicitly hierarchical organization regions state space associated different words 
organization achieved spatial structure 
conceptual similarity realized position state space 
words conceptually distant produce hidden unit activation patterns spatially far apart 
higher level categories correspond large regions space lower level categories correspond restricted subregions 
example dragon noun causes network move noun region state space 
animate reflected subregion noun space results 
large animals typically described different terms different things small animals general region space corresponding dragon monster lion distinct occupied mouse cat dog 
boundaries regions may thought hard cases nouns far verbs soft sandwich cookie bread far car book rock 
imagine cases certain contexts tokens word overlap tokens 
cases say system generated elman page highly similar different words 
rules attractors lexicon represented regions state space rules 
seen aspects grammar captured tokenization words fairly limited sense grammar 
formedness sentences depends relationships readily stated terms simple linear order 
proper generalization main verb plural main subject plural word words prior plural noun 
ability express generalizations require mechanism explicitly representing grammatical structure including constituent relationships notion elements part 
notations phrase structure trees provide precisely capability 
obvious complex grammatical relations expressed distributed representations 
argued distributed representations sort exemplified hidden unit activation patterns previous simulation constituent structure systematic fashion fodor pylyshyn 
backup fodor pylyshyn suggest distributed representations systematic constituent structure merely implementations call classical theory case language thought fodor 
fact grammar simulation extremely simple difficult explore issues 
sentences declarative 
simulation sheds little light grammatical potential networks 
better test train network predict words complex sentences contain long distance dependencies 
done elman strategy similar outlined prior elman page simulation sentences characteristics nouns verbs agreed number 
singular nouns required singular verbs plural nouns selected plural verbs 
verbs differed regard verb argument structure 
verbs transitive intransitive optionally transitive 
nouns modified relative clauses 
relative clauses object relatives head object role clause head subject clause subject object nouns relativized 
previous simulation words represented localist fashion information grammatical category noun verb number singular plural contained form word 
network saw positive instances grammatical sentences 
properties interact ways designed prediction task difficult 
prediction number easy sentence harder 

boys pl chase pl dogs 
boys pl dog sg chases sg run pl away 
case verb follows immediately 
second case noun agrees second verb run plural verb closest chase singular agrees intervening word dog 
relative clauses cause similar complications verb argument structure 
difficult network learn chase requires direct object elman page see permits require lives intransitive 

cats chase dog 
girls see 
girls see car 
patient lives 
hand consider 
dog cats chase run away 
direct object verb chase relative clause dog 
dog head clause subject main clause 
chase grammar transitive network learn occurs structures object position left empty gapped direct object mentioned filled clause head 
data illustrate sorts phenomena linguists argue representations constituent structure chomsky motivate claim language processing requires form pushdown store stack mechanism 
impose difficult set demands recurrent network 
training network stimuli elman appeared network able correct predictions mean cosine outputs empirically derived conditional probability distributions perfect performance 
predictions honored grammatical constraints training data 
network able correctly predict number main sentence verb presence intervening clauses conflicting number agreement nouns verbs 
network learned verb argument structure differences correctly remembered head appeared predict noun embedded transitive verb 
shows predictions network elman page testing novel sentence 
insert behavior achieved 
nature underlying knowledge possessed network allows perform way conforms grammar 
network simply memorized training data network able generalize performance novel sentences structures seen 
just general solution just systematic 
previous simulation hierarchical clustering measure similarity structure internal representations words 
gives indirect means determining spatial structure representation space 
determine structure able visualize internal state space directly 
important allow study ways network internal state changes time processes sentence 
trajectories tell grammar encoded 
difficulty arises trying visualize movement hidden 
example imagine tree major branches sub branches 
certain items major branches occupy different regions state space 
precisely lie dimension major variation space 
say dimensions variation 
sub branches may divide similar ways 
example human human animates may branches large large elements 
impossible know large human large human elements differ large counterparts exactly way lying common axis corresponding size 
clustering tells distance relationships organization space underlies relationships 
elman page unit activation space time extremely high dimensional space dimensions current simulation 
representations distributed typically consequence interpretable information obtained examining activity single hidden units 
information encoded dimensions represented multiple hidden units 
say information course simply needs discover proper viewing perspective get 
way doing carry principal components analysis pca hidden unit activation vectors 
pca allows discover dimensions variation vectors possible visualize vectors coordinate system aligned variation 
new coordinate system effect giving somewhat localized description hidden unit activation patterns 
dimensions ordered respect amount variance accounted look trajectories hidden unit patterns selected dimensions state space 
insert figures figures see movement time various plans hidden unit state space trained network processes various test sentences 
compares path state space second principal component network processes sentences boys hear boys boy hears boy 
pca encodes number main clause subject noun 
limitations pca analyze hidden unit vectors 
pca yields rotation original coordinate system requires new axes orthogonal 
need case dimensions variation hidden unit space orthogonal especially true output units receive hidden unit vectors input nonlinear 
preferable carry nonlinear pca technique relaxed requirement orthogonality took account effect hidden output nonlinearity 
elman page difference position dimension correlates subject singular plural 
compares trajectories sentences verbs different argument expectations chases requires direct object sees permits walks precludes 
seen differences argument structure reflected displacement state space upper left lower right 
illustrates path state space various sentences differ degree embedding 
actual degree embedding captured displacement state space embedded clauses sentences multiple embeddings appear somewhat spirals 
trajectories illustrate general principle network 
network learned represent differences lexical items different regions hidden unit state space 
sequential dependencies exist words sentences captured movement time space network processes successive words sentence 
dependencies encoded weights map inputs current state plus new word state 
weights may thought implementing grammatical rules allow formed sequences processed yield valid expectations successive words 
furthermore rules general 
network weights create attractors state space network able respond sensibly novel inputs unfamiliar words encountered familiar contexts 
discussion image language processing just outlined look traditional picture began 
dictionary lexicon state space partitioned various regions 
symbolic rules elman page phrase structure trees dynamical system grammatical constructions represented trajectories state space 
consider implications approach understanding aspects language processing 
sentences focused processing sentences obviously language processing real situations typically involves discourse extends sentences 
clear traditional scheme information represented sentence structures kept available discourse purposes 
problem just hand clearly limitations information stored obviously preserved hand aspects sentence level processing may crucially affected prior sentences 
include anaphora things argument structure expectations verb give normally requires direct object indirect object certain contexts need appear overtly understood plan give money united way 
gave week 
network approach language processing handles requirements natural manner 
network system characterized highly opportunistic 
learns perform task case prediction doing just needs 
notice example information number subject noun maintained verb agrees subject processed 
point sentences identical 
happens verb encountered subject number longer relevant aspect prediction task 
emphasizes importance task presumably tasks prediction easily require elman page subject number maintained longer 
approach preserving information suggests networks readily adapt processing multiple sentences discourse particular reanalysis re representation information required sentence boundaries reason information preserved sentences 
st john harris elman demonstrated networks kind readily adapt processing paragraphs short stories 
emphasis functionality reminiscent suggestions agre chapman brooks 
authors argue animals need perfectly represent environment store indefinitely 
need merely able process relevant task hand 
types tokens consider simulation network state space represent words 
directly relevant way system addresses types token problem arises symbolic systems 
symbolic systems representations context free binding mechanism required attach instantiation type particular token 
network hand tokens distinguished virtue producing small potentially discriminable differences state space 
john john john subscripts indicate different occurrences lexical item physically different vectors 
identity tokens type captured fact located region may designated john space contains vectors 
speak bounded region corresponding elman page lexical type john 
differences context create differences state 
furthermore differences systematic 
clustering tree carried mean vector word averaged contexts 
actual hidden unit activation patterns tree course quite large hundreds tokens word 
inspection tree reveals important facts 
tokens type similar type tokens boy dog mix pointed overlap impossible may circumstances desirable 
second substructure spatial distribution tokens true multiple types 
tokens boy subject occur closely tokens boy object 
true tokens girl 
spatial dimension subject tokens vary object tokens nouns 
subject tokens nouns positioned region dimension object tokens positioned different region 
means proliferating undesirable number representations tokenization types encodes grammatically relevant information 
note tokenization process involve creation new syntactic semantic atoms 
systematic process 
state space dimensions token variation occurs may interpreted meaningfully 
token location state space functionally compositional sense described van gelder 
polysemy accommodation polysemy refers case word multiple senses 
accommodation describe phenomenon word meanings contextually altered langacker 
network approach language processing provides elman page account phenomena shows may related 
clear instances phonological form entirely different meanings bank instance cases polysemy matter degree 
may senses different metaphorically related 
arturo runs fast 
clock runs slow 
dad runs grocery store block 
cases differences far subtle just real 
frank shorter runs faster 
rabbit runs road 
young runs mother 
runs slightly different depending doing running 
just way verb interpreted depends context 
langacker described process emphasized syntagmatic combination involves simple addition components 
composite structure integrated system formed coordinating components specific elaborate manner 
fact properties go expect components 
ne component may need adjusted certain details integrated form composite structure refer accommodation 
example meaning run applied humans adjusted certain respects extended legged animals horses dogs cats technical sense extension creates new semantic variant lexical item 
pp 

see network representations words context demonstrates just sort accommodation 
trajectories shown various sentences contain main verb burn 
representation verb elman page varies depending subject noun 
simulations shown exploit variants verb clear basic property networks 
insert leaky recursion processing complex sentences sensitivity context illustrated occurs levels organization 
network able represent constituent structure form embedded sentences true representation embedded elements may affected words syntactic levels 
means network implement stack pushdown machine classical sort implement true recursion information level processing encapsulated unaffected information levels 
bad 
designed programming language sort leaky recursion highly undesirable 
important value variables local call procedure affected value levels 
true recursion provides sort encapsulation information 
suggest appearance similar sort recursion natural language deceptive natural language may require aspect recursion provides constituent structure self embedding may require sort informational firewalls levels organization 
embedded material typically function 
relative clauses example provide information head noun phrase higher level organization 
adverbial clauses perform similar function main clause verbs 
general subordination involves conceptual dependence clauses 
may important language processing mechanism facilitate impede interactions levels elman page information 
specific consequences processing may observed system sort loosely approximates recursion 
finite bound precision means right branching sentences processed better center embedded sentences 
woman saw boy heard man left 
man boy woman saw heard left 
known years sentences sort processed humans easily accurately sentences second kind number reasons suggested miller isard 
case network asymmetry arises right branching structures require information carried forward embedded material center embedded sentences information matrix sentence saved intervening embedded clauses 
true center embedded sentences equally difficult comprehend 
intelligibility may improved presence semantic constraints 
compare 
man woman boy saw heard left 
claim horse entered race minute absolutely false 
subject nouns create strong different expectations possible verbs objects 
semantic information expected help hearer quickly resolve possible subject verb object associations assist processing king just 
verbs hand provide help 
nouns plausible subject verbs 
series simulations elman demonstrated elman page simple recurrent network exhibited similar performance characteristics 
better able process right branching structures compared center embedded sentences 
center embedded sentences contained strong semantic constraints processed better compared center embedded sentences constraints 
essentially presence constraints meant internal state vectors generated processing distinct apart state space preserved information better vectors sentences nouns similar 
immediate availability lexically specific information question generated considerable controversy concerns processing certain information may available process sentence processing 
proposal pass parse category general syntactic information available frazier rayner 
major position considerably information including lexically specific constraints argument structure available processing taraban mcclelland 
trueswell tanenhaus kello press empirical evidence variety experimental paradigms strongly suggests listeners able subcategorization information resolving syntactic structure noun phrase ambiguous 
example verb noun phrase complement sentential complement point time solution read possible 

student forgot solution back book 
student forgot solution 
hand hope strongly biased sentential elman page complement 

student hoped solution back book 
student hoped solution 
trueswell colleagues subjects appeared sensitive preferred complement verbs behavior significantly correlated statistical patterns usage determined corpus analysis 
insofar actual usage verb biased particular direction subjects expectations consistent usage 
exactly pattern behavior expected model processing described currently attempting model data 
years considerable attempting understand various aspects speech language terms dynamical systems 
elegant developed focused motor control particularly domain speech fowler kelso 
explicit consequences theories phonology goldstein pierrehumbert pierrehumbert 
attention turned systems operate called higher levels language processing 
principal challenges dynamical systems deal satisfactory way apparently recursive nature grammatical structure 
attempted show chapter networks possess dynamical characteristics number properties capture elman page important aspects language including embedded nature 
framework appears differ traditional view language processors way represents lexical grammatical information 
networks exhibit behaviors highly relevant language 
able induce lexical category structure statistical regularities usage able represent constituent structure certain degree 
perfect imperfections strongly resemble observed human language users 
close obvious caveat 
described qualifies full model language 
range phenomena illustrated suggestive limited 
linguist note questions remain unanswered 
models disembodied way difficult capture natural semantic relationship world 
networks essentially exclusively language processors language unconnected ecologically plausible activity 
related prior point view language networks deficient solely reactive 
networks input output devices 
input produce output appropriate training regime 
networks tightly coupled world manner leaves little room endogenously generated activity 
possibility spontaneous speech reflective internal language 
put networks think 
criticisms may levelled course current traditional models language taken inherent deficiencies approach 
suspect view linguistic behavior deriving dynamical system probably allows greater opportunities shortcomings 
exciting approach involves embedding networks environments activity subject evolutionary pressure viewing examples artificial life nolfi elman elman page parisi press 
event obvious remains done 
elman page guidelines reading collection connectionist models language may sharkey ed connectionist natural language processing readings connection science 
oxford intellect 
initial description simple recurrent networks appears elman 

finding structure time 
cognitive science 
additional studies srns reported cleeremans servan schreiber mcclelland 

finite state automata simple recurrent networks 
neural computation elman 

distributed representations simple recurrent networks grammatical structure 
machine learning 
discussion recurrent networks dynamical systems pollack 

induction dynamical recognizers 
machine learning 
elman page agre chapman 

pengi implementation theory activity 
proceedings aaai 
los altos ca morgan kaufmann 
altmann steedman 

interaction context human sentence processing 
cognition 


cognitive basis linguistic structure 
hayes ed cognition development language 
new york wiley 


short term memory limitations decoding self embedded sentences 
journal experimental psychology 
brooks 

robot walks emergent behaviors carefully evolved network 
neural computation 
goldstein 

dynamic modeling phonetic structure 
ed phonetic linguistics 
new york academic press 
chomsky 

reflections language 
new york pantheon 
churchland sejnowski 

computational brain 
cambridge ma mit press 
elman 

finding structure time 
cognitive science 
elman 
representation structure connectionist models 
gerald altmann ed computational psycholinguistic approaches speech processing 
new york academic press 
elman 

distributed representations simple recurrent networks grammatical structure 
machine learning 
ferreira henderson 

verb information syntactic parsing comparison evidence eye movements word word elman page self paced reading 
journal experimental psychology learning memory cognition 
fodor 

language thought 
sussex harvester press 
fodor pylyshyn 

connectionism cognitive architecture critical analysis 
pinker mueller eds 
connections symbols 
cambridge ma mit press 
forster 

accessing mental lexicon 
wales walker eds new approaches language mechanisms 
amsterdam northholland 
fowler 

timing control speech production 
bloomington indiana university linguistics club 
fowler 

coarticulation theories extrinsic timing control 
journal phonetics 
frazier 

sentence processing tutorial review 
coltheart ed attention performance xii psychology reading 
hillsdale nj erlbaum 
frazier rayner 

making correcting errors sentence comprehension eye movements analysis structurally ambiguous sentences 
cognitive psychology 
harris elman 

representing variable information simple recurrent networks 
proceedings tenth annual conference cognitive science society 
hillsdale nj erlbaum 
jordan 

serial order parallel distributed processing approach 
institute cognitive science report 
university california san diego 
jordan rosenbaum 

action 
technical report 
elman page department computer science university massachusetts amherst 
kelso 

dynamical theory speech production data theory 
journal phonetics 
king just 

individual differences syntactic processing role working memory 
journal memory language 
langacker 

foundations cognitive grammar theoretical perspectives 
volume 
stanford stanford university press 


motor control serial ordering speech 
psychological review 
marslen wilson 

speech understanding psychological process 
simon ed spoken language understanding generation 
dordrecht reidel mcclelland elman 

trace model speech perception 
cognitive psychology 
mcclelland rumelhart 

interactive activation model contexts effects letter perception part 
account basic findings 
psychological review 
miller chomsky 

finitary models language users 
luce bush eds handbook mathematical psychology vol 
ii 
new york wiley 
miller isard 

free recall self embedded english sentences 
information control 
morton 

word recognition 
morton marshall eds psycholinguistics structures processes 
cambridge ma mit press 
mozer 

focused back propagation algorithm temporal pattern elman page recognition 
complex systems 
nolfi elman parisi 
press 
learning evolution neural networks 
adaptive behavior 
pearlmutter 

learning state space trajectories recurrent neural networks 
proceedings international joint conference neural networks washington ii 
pierrehumbert pierrehumbert 

attributing grammars dynamical systems 
journal phonetics 
pollack 

induction dynamical recognizers 
machine learning 
rumelhart hinton williams 

learning internal representations error propagation 
rumelhart mcclelland eds parallel distributed processing explorations microstructure cognition vol 

cambridge ma mit press 
selfridge 

pandemonium paradigm learning 
mechanisation thought processes proceedings symposium held national physical laboratory november 
london 
siegelmann sontag 

neural networks real weights analog computational complexity 
report 
rutgers center systems control rutgers university 
simon 

physical symbol systems 
cognitive science 
st john 

story gestalt model knowledge intensive processes text comprehension 
cognitive science 
st john mcclelland 

learning applying contextual constraints sentence comprehension 
artificial intelligence 
taraban mcclelland 

constituent attachment thematic role elman page expectations 
journal memory language 
trueswell tanenhaus kello 
press 
verb specific constraints sentence processing separating effects lexical preference 
journal experimental psychology learning memory cognition 
van gelder 

connectionist variation classical theme 
cognitive 
elman 

pdp approach processing sentences 
proceedings fourteenth annual conference cognitive science society 
hillsdale nj erlbaum 
wiles 

operators curried functions training analysis simple recurrent networks 
moody hanson lippman eds advances neural information processing systems 
san mateo ca morgan kaufmann 
legends 
feed forward network represents time space 
circles represent nodes arrows layers indicate full connectivity nodes adjacent layers 
network feed forward activations level depend input received 
processing input activations lost 
sequence inputs represented architecture associating node left element sequence second node second element 

simple recurrent network srn 
solid lines indicate full connectivity layers weights trainable 
dotted line indicates fixed connection hidden context layers 
context units save activations hidden units time step 
time step hidden units activated new input information context units just hidden units activations prior time step 
input sequence processed presenting element sequence time allowing network activated step time proceeding element 
note hidden unit activations may depend prior inputs virtue prior inputs effects recycled hidden unit context unit activations hidden units record input sequence veridical manner 
task network learn encode temporal events manner allows network perform task hand 

hierarchical clustering diagram hidden units activations elman page simulation simple sentences 
training sentences passed network hidden unit activation pattern word recorded 
clustering diagram indicates similarity structure patterns 
structure reflects grammatical factors influence word position inferred network patterns represent actual inputs orthogonal carry information 

predictions network simulation complex sentences network processes sentence boys mary chases feed cats 
panel displays activations output units successive words outputs summed groups purposes displaying data 
refer verbs nouns sg pl refer singular plural prop refers proper nouns refer transitive verbs intransitive verbs optionally transitive verbs 

trajectories hidden unit state space network processes sentences boy hears boy boys hear boy 
number singular vs plural subject indicated position state space second principal component 

trajectories hidden unit state space network processes sentences boy chases boy boy sees boy boy walks 
transitivity verb encoded position axis cuts third principal components 

trajectories hidden units state space principal components network processes sentences boy chases boy boy chases boy chases boy boy chases boy chases boy boy chases boy chases boy chases boy assist reading plots final word sentence terminated 

trajectories hidden units state space principal components elman page network processes sentences john mary lion tiger boy girl burns house museum house burns final word sentences terminated 
internal representations word burns varies slightly function verb subject 
elman page 
output units input units hidden units time elman page output unit hidden units input unit context units linear hidden units fixed connections elman page smell move see think exist sleep break smash chase eat mouse cat monster lion dragon woman girl man boy car book rock sandwich cookie bread plate glass verbs nouns animates humans animals food transitive intransitive transitive dog distance elman page sg pl prop sg sg sg pl pl pl boys 
summed activation sg pl prop sg sg sg pl pl pl boys 
summed activation sg pl prop sg sg sg pl pl pl boys mary 
summed activation sg pl prop sg sg sg pl pl pl boys mary chases 
summed activation sg pl prop sg sg sg pl pl pl boys mary chases feed 
summed activation sg pl prop sg sg sg pl pl pl boys mary chases feed cats 
summed activation elman page time boy hears boy boys hear boys elman page pca pca boy chases boy boy sees boy boy walks elman page pca pca boy chases boy pca pca boy chases boy chases boy pca pca boy chases boy chases boy pca pca boy chases boy chases boy chases boy elman page pca boy burns house girl john burns john mary lion burns lion tiger house burns museum 
