solving ill conditioned singular linear systems tutorial regularization arnold neumaier 
shown basic regularization procedures finding meaningful approximate solutions ill conditioned singular linear systems phrased analyzed terms classical linear algebra taught numerical analysis course 
apart rewriting known results elegant form derive new parameter family merit functions determination regularization parameter 
traditional merit functions generalized cross validation gcv generalized maximum likelihood gml recovered special cases 
key words 
regularization ill posed ill conditioned generalized cross validation generalized maximum likelihood tikhonov regularization error bounds ams subject classifications 
primary secondary 

applications linear algebra need arises find approximation vector ir satisfying approximate equation ax ill conditioned singular ir thetan ir usually result measurements contaminated small errors noise 
may discrete approximation compact integral operator unbounded inverse operator relating tomography measurements underlying image matrix basis function values points relating vector approximate function values coefficients unknown linear combination basis functions 
frequently ill conditioned singular systems arise iterative solution nonlinear systems optimization problems 
importance problem seen glance probably incomplete list applications numerical differentiation noisy data nonparametric smoothing curves surfaces defined scattered data image reconstruction deconvolution sequences images wiener filtering shape shading computer assisted tomography cat pet indirect measurements testing multivariate approximation radial basis functions training neural networks inverse scattering seismic analysis parameter identification dynamical systems analytic continuation inverse laplace transforms calculation relaxation spectra air pollution source detection solution partial differential equations nonstandard data backward heat equation cauchy problem parabolic equations equations mixed type multiphase flow chemicals 
surveys engl book contain pertinent 
situations vector gamma full rank overdetermined case pseudo inverse gamma exists usually meaningless bad approximation 
seen analysis terms singular value decomposition see section 
vector reasonable approximation vector ax usual error estimates kx gamma xk ka gamma kax gamma yk square case kx gamma xk ka kax gamma yk overdetermined case pessimistic 
called regularization techniques needed obtain meaningful solution estimates ill posed problems parameters ill determined institut fur mathematik universitat wien wien austria 
email cma univie ac www cma univie ac neumaier squares methods particular number parameters larger number available measurements standard squares techniques break 
typical situation means parameters state vector function values function points suitable grid coefficients discretization function 
refining coarse grid increases number parameters finite amount grid independent data fine grid standard squares requires course expects additional parameters introduced refinement grid closely related parameters coarse grid underlying function expected regularity properties continuity differentiability 
get sensible parameter estimates case necessary able additional qualitative information 
underlying continuous problem integral equation kind see wing easy read 
frequently needed applications adequate handling ill posed linear problems hardly touched numerical analysis text books 
golub van loan topic briefly discussed heading ridge regression statisticians name tikhonov regularization book bj squares problems section regularization 
main reason lack covering discussion regularization techniques literature tikhonov engl hanke wahba usually phrased terms functional analytic language geared infinite dimensional problems 
notable exception hansen see hansen hanke hansen book hansen closer spirit 
tends treatments unduly application specific simplicity arguments irrelevant details distracting notation 
summarize functional analytic approach section mainly give familiar tradition guide recognizing happens rest 
unfamiliar functional analysis may simply skip section main purpose show regularization discussed elementary elegant linear algebra accessible numerical analysis students level 
linear algebra setting closer way regularization methods implemented practice 
results derived major exception theory section leading new family merit functions known similar form problems function spaces 
new derivation assumptions sense finite dimensional setting proofs standard linear algebra 
section motivates general approach smoothness conditions form sw vector reasonable norm suitable smoothness matrix derivable coefficient matrix assumed order differentiability 
section derives discusses basic theorem giving deterministic error bounds show regularization possible 
shown general problems behave way completely analogous simplest problem numerical differentiation cure understood long time 
section specializes theorem obtain specific regularization tech tutorial regularization niques 
particular approximate inverses regularization derived modifying standard squares formula 
traditional tikhonov regularization means gamma iterated version covered basic theorem 
section invokes singular value decomposition compute flexible approximate inverses including familiar truncated singular value decomposition 
section discuss inherent limitation regularization techniques deterministic models reliable ways determine regularization parameters absence information size residuals degree smoothness 
situation frequent practice finding valid choices regularization parameter current frontiers research 
remainder discusses stochastic framework possible rigorously study techniques selection optimal regularization parameter absence prior information error level 
prove sections stochastic results parallel obtained deterministic case significant differences 
particular section shows expected squared norm residual minimized explicitly result due leads simplest case tikhonov regularization 
section expresses optimal estimator general situation terms singular value decomposition discusses attainable limit accuracy 
resulting formulas similar arising deconvolution sequences images wiener filters wiener earliest practical regularization techniques 
modern treatment practical point view katsaggelos 
stochastic approach regularization parameter turns reappear variance quotient permits estimation variance component estimation techniques 
section elementary rest derive family merit functions minimizers give approximations ideal regularization parameter merit functions contain generalized cross validation approach generalized maximum likelihood approach special cases 
section extends stochastic approach situation smoothness condition sw replaced condition vector jx usually composed suitably weighted finite differences function values reasonably bounded 
smoothness condition judicious choice smoothness requirements better adapted particular problems 

regularization function spaces 
section assume reader familiar concepts functional analysis unfamiliar concepts harm skipping section remainder completely independent 
shall give short sketch traditional theory detailed depth treatments applications refer surveys mentioned 
particular shall section notation book engl hanke neubauer 
objective computing inverse radon transform tomography solve linear operator equation form tx compact neumaier linear operator hilbert spaces wanted 
typically closed subsets sobolev spaces functions integral operator 
numerical applications needs discretized finite dimensional 
replaced discrete versions consisting function values coefficients expansions basis functions matrix structure inherited continuous problem 
choice hilbert spaces amounts fixing norms measure discretized version analogue choice suitable scales norms assess accuracies 
ir norms equivalent numerical purposes correct scaling may crucial difference magnitude norms 
function space setting useful error estimates obtained satisfies smoothness restrictions 
restrictions take main forms 
form due tikhonov cf 
chapter assumes range smoothing operator product alternating factors adjoint problems involving univariate functions roughly interpreted number weak derivatives bounded norm 
obtain error bounds assume knowledge bound norm bw 
number interpreted rough measure magnitude pth derivative choice smoothness operator setting uniquely determined priori modeling decision amounts selecting smoothness reconstructed solution 
note analytical problems appropriate choose large grows exponentially consider sin large 
priori modeling decision selection appropriate level ffi accuracy approximation tx 
results obtained particular regularization methods form large gamma yk ffi regularized solution ffi error kx ffi gamma xk ffi 
reduced exponent reflects fact inverse compact operator unbounded resulting ill posedness problem 
standard results cf 
proposition imply best exponent ffi achievable 
construction approximation satisfying error bound depends knowledge ffi constant involving ffi ffi theorem theorem technique choosing regularization parameters absence information error level defeated suitably constructed counterexamples pseudo inverse unbounded 
practice information available provide adequate values ffi limits deterministic approach 
results stochastic functional analytic setting context smoothing splines wahba chapter 
second form smoothness restrictions due phillips cf 
chapter assumption differential operator integer norm finite 
theory gives results limitations similar smoothness restriction 
discretized version second form smoothness restrictions usually modeled imposing condition norm vector jx composed suitable finite differences solution vector assumed moderate size tutorial regularization cf 
section 

modeling smoothness 
rank deficient case rank smaller dimension clear additional information needed find satisfactory solution ax infinitely solutions exist 
numerical point view ill conditioned systems behave just singular ones additional information needed 
applications correct near solutions characterized additional properties commonly requiring smoothness function curve surface constructed qualitative knowledge modeled follows 
vector representing continuous function example list function values points grid may apply form numerical integration arrive approximate representation differentiable function derivative repeating times get representation times differentiable function 
algebraically sw suitable coefficient matrix smoothing matrix 
formulate smoothness requiring represented form sw vector reasonable norm 
smoothing matrix characterizes additional information assumed 
flexible alternative smoothness conditions discussed section 
practice discretized integral operator augmented boundary conditions 
convenient way constructs directly works detailed knowledge smoothness requirements 
motivate recipe rewrite numerical differentiation times continuously differentiable function ir infinite dimensional regularization problem 
situation simple functional analysis needed simplify assuming derivatives vanish 
finding derivative ry posed problem solving linear system ax integral operator defined ax differentiability assumption says continuous bounded gamma may write form rewrite form capable generalization note adjoint integral operator defined formula ax terms inner product dt 
ax ax gamma partial integration boundary conditions conclude gammaa 
sign may absorbed write condition sw ae gamma odd product alternating factors simple powerful general way choosing smoothing matrix general transposed matrix conjugate transposed complex case adjoint operator infinite dimensional situation works generally sufficient practical problems 
neumaier assume applications operator smoothing sense ir discretized version times differentiable function ax ir discretized version times differentiable function operator generally smoothing 
ir discretized version times differentiable function ir discretized version times differentiable function 
typically related integral operator adjoint integral operator 
get smoother smoother functions repeating applications vector corresponding bounded continuous function 
suggests require smooth represented sw vector reasonable norm 
need ir final matrix applied case choice 
requiring represented form sw smoothing matrix qualitative knowledge smoothness modeled 
note implies ss sw frequently assumption discussion find suitable value virtually missing literature 
shall return point section 

error estimate 
key treatment ill posed linear systems term shall linear systems standard techniques inadequate process called regularization replaces gamma family approximate inverses way product converges appropriately restricted sense 
parameter called regularization parameter 
generally parameter entering definition may referred regularization parameter 
order may approximate ill conditioned infinite dimensional case unbounded inverses allow norm grows infinity 
usually possible choose suitable exponent constants fl sup fl sup gammap gamma sk finite reasonable size 
kc fl gamma sk fl feasibility regularization techniques consequence fundamental deterministic error bounds 
result valid arbitrary norms shall apply euclidean norm 
cf 
implicit scaling may needed adjust data euclidean norm 
theorem 
suppose sw kax gamma yk tutorial regularization delta 
implies kx gamma yk fl delta fl kwk proof 
kx gamma yk gamma ax gamma gamma xk kc kax gamma yk gamma swk kc gamma sk kwk kc fl kwk fl 
implies 
note assumptions different status 
assumption problem solved requirement may satisfy choosing suitable way 
violated said problem violation implies particular algorithm choosing unsuitable solving problem 
proof shows may allow general relation sw arbitrary satisfying gamma affecting 
applications allows flexible treatment boundary conditions 
traditionally assumes residual bound kax gamma yk ffl form bound motivated fact optimal depends ffl quotient delta ffl kwk 
bound smallest derivative right hand side vanishes giving fl delta fl delta choice fl delta fl find error bound kx gamma yk fl delta reasonable factors hidden landau symbol delta 
landau symbols refer behavior tiny delta 
results valid value delta 
important regularization available data assumed model low accuracy 
posed data fitting problem conditioned normal equation matrix squares estimate error order delta 
follows satisfies gamma ka fl fl independent 
allow blowing bound right exponent approached 
ill posed problem reduced exponent shown optimal assumptions cf 
proposition implies loss precision due ill posed nature problem 
roughly speaking means number correct digits expected approximate solution fraction expect posed problem 
particular familiar numerical differentiation accuracy approximation neumaier roughly half number digits data accurate 
see proof error bounded sum terms 
term divergent proportional error level data vector second term divergent reflects error approximative inverse 
situation completely analogous encountered numerical differentiation 
thorough discussion chapter gill approximated inaccurate function values absolute errors bounded delta 
approximation formulas fi fi fi fi fi gamma gamma fi fi fi fi fi delta kf forward differences fi fi fi fi fi gamma gamma gamma fi fi fi fi fi delta kf central differences terms opposite behavior arise 
optimal choice delta case delta second case gives approximation orders delta error forward differences delta error central differences 
order evaluate specific problem needs know fl fl delta 
usually assumed fixed typically fl fl determined formula define cf 
section 
hand practice hardly knows precise values delta choose optimal value value fl ffi guess ffi delta positive constant fl find kx gamma yk const 
max delta ffi ffi gamma immediately shows relative error ffi choice ffi delta influence size bound choice ffi delta 
doubt ffi taken conservatively large 
context solving nonlinear systems optimization problems overestimation usually additional iterations ffi chosen excessively large small choice ffi may disastrous 
note implies generally problems larger solved larger value long fl delta fl remains smaller 
choosing approximate inverses 
conditioned approximation problem ax residual kax gamma yk smallest choice gamma rank deficient increasingly ill conditioned choice impossible increasingly useless 
may improve condition modifying 
simplest way achieve adding small multiple identity 
symmetric positive semidefinite matrix eigenvalues tutorial regularization kak condition number kak smaller increases 
replacement formula turns gamma formula derived tikhonov 
derived solving modified square problem kax gamma yk kxk min 
formula corresponds family approximate inverses defined gamma generally may consider matrix functions approximate inverses 
arbitrary functions defined ir extended functions gamma example continuous may approximate sequence polynomials lim uniformly jtj ktk lim 
theorem 
smoothing matrix satisfying bounds hold norm family approximate inverses gamma gamma function ir ir constants fl sup fl sup gamma tjt finite 
proof 
fact norm matrix square root supremum eigenvalues matrix bb denote eigenvalues gamma nonnegative 
eigenvalues gamma gamma gamma gamma fl kc gamma fl similarly gamma gamma gamma gamma ss eigenvalues rs rs rss gamma gamma fl gamma sk fl particular tikhonov regularization obtained choosing choice find fl sup neumaier fl sup tikhonov solution formula exploit smoothness degree smoother solutions treated 
particular tikhonov regularization approximate better delta cf 

iterated tikhonov regularization 
obtain smoother solutions may formula iterative refinement system gamma gamma gamma gamma ax commonly called iterated tikhonov regularization considered preconditioned landweber iteration 
landweber iteration defined dropping factor gamma treated similar way slow useful practice 
predates tikhonov riley 
note iterated tikhonov regularization iterative refinement solving regularized normal equations 
obtained place gamma vector gamma gamma gamma gamma gamma inferior approximation powers 
proposition 
gamma gamma gamma delta gamma proof 
holds 
assuming gamma place gamma gamma gamma ax gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma tutorial regularization gamma gamma gamma gamma gamma gamma holds generally 
theorem applies theorem gives error bound defined 
easy see choice leads fl fl sup able exploit smoothness orders 
inspection proof theorem shows supremum needs taken gamma kak implies may get reasonable values fl fl choosing suitable polynomials 
allows computed iteratively matrix vector multiplications example context conjugate gradient methods 
regularization viable technique large scale problems 
details see nemirovski hanke 
different technique largescale case allowing general linear nonlinear regularization conditions discussed kaufman neumaier 
scaling 
applications nonlinear systems optimization linear systems need solved may badly scaled advisable apply preceding results scaled variables dx related diagonal transformation matrix ensures common magnitude variables 
equivalent theorem scaled euclidean norm corresponding error estimates valid 
appropriate scaling matrix known transformed system solved ad gamma iteration gamma gamma gamma gamma terms original variables gamma rewritten gamma gamma gamma gamma ax note computed efficiently single matrix factorization 
appropriate scaling matrix available natural choice diagonal matrix diagonal entries kk kk ka neumaier denotes kth column choice results invariant rescaling right hand side variables corresponding scaling rows columns 
scale place amounts multiplying diagonal entries doing factorization 
iteration allows bound higher higher values neglect second term parenthesis close term small choosing largest subject requirement 
value corresponding compromise 
context inaccuracies solution corrected stages simple stopping criterion iteration accept kr sufficiently small kx exceeds certain bound 
suitable thresholds usually available context 

singular value decomposition 
assume know singular value decomposition svd sigmav orthogonal sigma diag oe oe ir thetan square orthogonal theta matrix square orthogonal theta matrix sigma diag oe oe rectangular diagonal theta matrix diagonal entries sigma ii oe details see golub van loan 
known minimum norm solution squares problem kax gamma yk min 
sigma oe oe ith column singular vector corresponding ith singular value oe sigma diag oe oe ae oe oe ill conditioned matrices characterized presence tiny singular values oe clear representation errors show corresponding components drastically magnified 
minimum norm squares solution useless problems tiny nonzero singular values 
svd give meaning irrational functions 
sigma sigmav sigma sigma gamma gamma delta diag oe polynomials limiting process arbitrary continuous 
sigma sigma diag oe oe tutorial regularization differs replacement ill conditioned diagonal matrix sigma better behaved diagonal matrix sigma particular choice ae removes small singular values sum leads sigma diag ae oe oe 
referred regularization truncated singular value decomposition 
fl fl independent degree smoothness 
needed find optimal 
alternative choice max works fl smaller constant fl suggesting slight superiority 
applications scaling problems may arise advisable apply formulas scaled problem defined 

difficulty estimating delta 
far assumed delta approximation ffi delta known 
having chosen compute fl fl theorem choose optimal regularization parameter 
complete discussion sufficient consider ways estimate delta information sight intractable problem delta unknown assumption virtually vacuous 
theorem theorem engl technique choosing regularization parameters absence information error level defeated suitably constructed counterexamples techniques fail small proportion problems simulations right hand side perturbed random noise 
consequence literature choosing regularization parameters see hanke hansen review surrounded sense mystery typically heuristics study model cases special distributions singular values special right hand sides 
arguments rigorous nature obtained context smoothing splines summarized chapter wahba stochastic assumptions 
discussed arguments inaccessible people trained statistics approach difficult understand 
shall show stochastic approach feasible general situation elementary machinery 
view remarks hope construct estimates delta time 
formalize stochastic setting 
byproduct able find useful estimates order differentiability 

stochastic error analysis 
suppose solving particular problem ax ffl sw neumaier class problems characterized stochastic assumptions ffl specifically assume components ffl uncorrelated random variables mean zero variance oe respectively 
terms expectations assumption implies hffl hffl ffl hw hffl oe hw denotes expectation random variable random vector vector form conditions oe expectation operator deltai linear compute expectations arbitrary quadratic forms ffl particular ss ffl ss hyy ffl ffl ass oe oe measures size gamma ax measures size quotient delta oe appropriate measure relative noise level stochastic case cf 
miller 
stochastic assumptions delta estimated available right hand side see section 
delta known result due gives best hope get 
tr denotes trace square matrix 
theorem 
error term gamma cyk minimal ss delta gamma ss choice optimal estimator cy computable solution positive definite symmetric linear system delta gamma xk tr ss gamma cass proof 
look behavior error term replaced perturbed matrix kx gamma yk kx gamma cy gamma kx gamma cyk gamma gamma cy ey kx gamma cyk gamma tr ey gamma cy tutorial regularization gamma yk gamma cyk gamma tr gamma cy trace term vanishes choice gamma ss ass oe gamma hy gamma cy gamma hyy gamma yk gamma cyk gamma cyk putting gamma see gamma cyk minimal ass oe gamma solution ass oe multiplying gamma ss find ss ass delta ss ss ss ss delta gamma ss inserting get formula 
gamma cyk gamma cy gamma cy gamma tr tr gamma tr tr ss gamma cass giving 
course practice may need scale system cf 

special case recover tikhonov regularization 
tikhonov regularization stochastically optimal regularization method weak qualitative assumption reasonably bounded corresponding case section 
shall see estimators accuracy possible assuming tikhonov regularization longer optimal 
applications oe small large guarantee delta inequality guarantees regularization term optimality result dominate information system coefficient matrix 
note delta corresponds roughly factor delta deterministic discussion 
particular estimates delta section may calculate theorem appears dubious theorem optimal computed 
fixed delta may solve direct iterative method 
singular value decomposition computationally feasible find explicit formulas give cheaply solution delta simultaneously important consideration delta unknown iteratively 
neumaier 
singular value decomposition 
standard choice ss note matrix defined form 
singular value decomposition simplify formulas 
theorem 
ss sigmav singular value decomposition optimal linear estimator cy computed delta oe oe oe delta gamma xk oe oe oe delta hcc diag oe delta proof 
sigmav singular value decomposition write covariance matrix hyy calculated hyy oe aa oe sigma sigma oe vector diagonal covariance matrix hcc hyy iu sigma sigma oe takes form gamma aa gamma sigma sigma sigma sigma sigma oe gamma du diagonal matrix sigma sigma sigma sigma sigma delta gamma du dc components oe oe delta tutorial regularization remains express residual term 
ss sigma sigma ca du sigmav sigmav get desired result gamma cyk tr gamma ca ss tr gamma sigma sigma sigma tr gamma sigma sigma sigma tr gamma sigma sigma sigma gamma ii oe oe delta oe oe delta oe oe oe delta formulas related known long time classical technique wiener filtering wiener deconvolution sequences images probably earliest application regularization techniques 
singular values oe correspond absolute values jh fourier coefficients point spread function defining convolution operator deconvolution sequence image done multiplying fourier transform fourier transform filter function jh pn pn noise power power model sequence image operations elementwise 
assumes jh implicit smoothness assumption intensities true image density writes arg pn delta product takes form 
finds natural explanation noting singular value decomposition circulant matrix written explicitly terms fourier transforms 
details see katsaggelos 
order convergence 
result optimal convergence order reveals difference stochastic deterministic situation 
assume oe delta small 
theorem 
gamma xk ff gamma oe delta ff gamma gammaq oe gamma proof 
rewrite gamma xk oe oe delta oe delta delta gamma oe gamma oe sup delta gamma oe gamma neumaier bound follows oe delta gamma oe oe gamma gamma oe sup gamma gammaq attained zero gamma gradient 
situations need regularized tiny oe constant ff reasonable particular need order bound goes zero model error oe goes zero strictly positive 
choice yields ff proportional rank number nonzero singular values 
recover deterministic convergence order rank large 
typical discretized function space problems full rank dimension large usually singular values oe approach singular values oe continuous problem oe oe cf 
hansen 
bounded ff limit reduced exponent gamma number oe gamma cyk delta independent dimension 

estimating regularization parameter 
return problem estimating delta 
base discussion svd adapt estimation case svd available postponed section 
key relation expresses covariance matrix high dimensional vector terms unknown parameters delta oe relation get hc oe oe general relations form hc allow low dimensional parameter vector estimated single realization high dimensional random vector estimation variance components random vectors classical problem statistics see cox 
shall give direct elementary derivation family estimators 
tutorial regularization theorem 
suppose random vector satisfying 
strictly concave function define ff gamma gamma delta weights ff hf bounded global minimizer hf satisfies proof 
write hf ff gamma hf ff hf gamma hf ff gamma gamma gamma strictly concave ff kth term sum nonnegative vanishes hf hf equality iff corollary 
addition determined uniquely values unique global minimizer hf case parameters 
hc oe case interest application estimation problem reduces global optimization problem variables oe 
introduce shall need section 
possible reduce optimization problem single variable delta oe restricting attention family concave functions defined ae gamma log parameter 
exp denotes exponential function 
theorem 
suppose random vector satisfying 
define weights ff fl ff gamma neumaier fi ff ae log fl gammaq log fi log fl ff log ff bounded oe unique global minimizer oe hfl fi proof 
condition equivalent oe apply theorem consider optimal point associated oe minimizer hf gradient ff hc gamma gamma rv ff hc gamma rv vanishes 
oe rv oe find ff hc gamma gamma gamma gamma choice find ff gamma hc gamma equation gamma hfl gamma fi giving hfl fi oe hfl fi oe expressed terms single parameter imply hf ff limiting case find hf ff log ff log log fi gamma log fi ff log fi gamma fi log fi fi ff independent minimizing hf equivalent minimizing ff ff ff ff fi hfl fi fi hfl fi gammaq tutorial regularization expression minimized maximized get optimal logarithm dividing reverse monotony behavior minimizing hf equivalent minimizing theorem follows previous result 
choice weights 
consider choice weights 
weights random ff independent desirable objective function change condition rescaled positive scale factors weights chosen way transformation transforms ff ff gammaq ff case choices ff gammar gamma ensure tiny values cause trouble need 
determined convenient index pair rs place find generalized maximum likelihood gml merit function log log additional assumption independent gaussian random variables zero mean global minimizer traditional maximum likelihood estimator find generalized cross validation gcv merit function rs log fl rs gamma log fi rs fi rs fl rs important special case discussed section global minimizer gcv merit function familiar gcv estimate regularization parameter 
practical 
regularization problem results apply oe compare 
practice expectation rs computed single realization known 
theorem suggests estimate finding global minimizer exp rs equivalently rs neumaier table failure rates percent case ne alg 
case ne exp case ne alg 
case ne exp terms 
unknown minimize set values find best order differentiability 
assuming independent gaussian random variables zero mean technical additional assumptions proved resulting estimate asymptotically unbiased estimate see welsh 
particular holds maximum likelihood estimator asymptotic properties known see barndorff nielsen cox 
find best values note results depend distribution quotients oe performed simulation study estimation properties various choices quotients 
looked different kinds distributions representative realistic problems algebraic decay gammaa exponential decay chosen range gammae tutorial regularization table number second third places case gold silver alg 
case gold silver exp constants varied independently different signs estimation extremely unreliable 
chose oe gammae oe minimized case curves rs samples various sizes independently distributed gaussian noise mean zero variance 
simulation minimization done evaluation oe gamma gamma 
smallest function value occured jgj values study estimation classified failure 
failure rates decrease increasing dimension reported table 
expected dimension small larger range distributions neumaier estimates unreliable choices 
minimum occured jgj value ranked pairs value jgj giving gold smallest jgj silver smallest 
gold gold silver 
ranking number test cases table fact fewer silver shows choices share best position 
gml merit function clear winner 
smaller smaller values threshold defining failures behavior similar fewer gold gml remains uniformly top list terms gold terms total number 
may recommend estimate delta minimizing gml merit function theorem get suitable starting point minimization median starting point local univariate minimizer bracket search local parabolic interpolation cf 
gill usually takes function values typically find local minimizer assess completely impact various methods estimate needs study sensitive accuracy computed solution depends choice investigation direction known wahba shows circumstances gcv robust gml misspecified 
unknown persists estimated gcv gml 

flexible smoothness constraints 
look regularization general linear stochastic models ax ffl oe ir thetan oe ir thetam symmetric positive definite covariance matrix oe typically unknown 
qualitative constraint assumed form certain components linear combinations state vector large formulate suitable matrix ir thetan assuming jx reasonable norm 
applications usually matrix suitably weighted second order differences apply part containing function values densities 
problems involving images contains grey values color intensities interpreted movies multiple slices object dimensional function position 
smoothness condition judicious choice smoothness requirements adapted particular problems 
modeling smoothness advantage smoothness condition may unrelated particular problems conditioned solution ax lead unacceptably rough solution important case identity matrix curve tutorial regularization image smoothing 
contains function values noisy curve pixel intensities noisy image wants reconstruct smooth original 
relaxed requirement piecewise smooth needs nonlinear regularization terms handled iteratively 
see 
assumptions lead conditions reasonable size ax gamma gamma ax gamma oe traditionally interprets adding multiple penalty solving associated squares system oe gamma gamma ax gamma gamma ax gamma min 
delta oe regularization parameter solution normal equations gamma gamma tj equations generalization tikhonov regularization obtained special case square nonsingular may rewrite sw gamma enforced transformation may stochastic setting obtain theorem optimal estimator cy ss delta gamma ss delta formula agrees 
natural generalization optimal situation considered context 
large scale problems solved cholesky factorization value show factorizations find appropriate regularization parameter 
elden observed significant amount saved reducing problem form 
full matrices done subsequent function evaluation costs operations 
unfortunately method limited dense problems banded problems sparsity destroyed 
problems banded save little reducing banded triangular form see section bj 
sparsity structure computation cholesky factorizations practical iterative methods 
details see section 
generalized svd 
show choose special coordinate system separable easy analyze 
theorem 
nonsingular matrices ir thetan ir thetam nonnegative diagonal matrices ir thetan ir thetan gamma na dm em ii holds transformed variables mx ny neumaier satisfy ax gamma gamma ax gamma gamma ck proof 
give fully constructive proof directly translates algorithm computing factoring ll consider ae qr factorization gamma aej qr ir thetan upper triangular ir thetan gamma aej singular value decomposition orthogonal ir thetam ir thetan nonnegative diagonal matrix ir thetan define gamma find diagonal matrix gamma gamma positive semidefinite 
entries nonnegative form nonnegative diagonal matrix ae gamma gamma component wise square roots 
gammat uu gamma gammat gamma ll gamma gamma na gamma dw dm aej ww wm ae wm wm gamma ae em square diagonal matrix 
proves 
conclude jx em em ax gamma gamma ax gamma ax gamma ax gamma kn ax gamma gamma ck tutorial regularization factorization generally referred generalized singular value decomposition 
see van loan stewart paige properties generalized svd 
case discussed 
implementation transformation may proceed 
choose ae kl gamma ak kjk similar expression order ensure matrices left hand side similar magnitude 
guarantees stable computation conditioned 
see van loan stability algorithms computing 
counterexamples stability computed apply matrices implicitly indicated 
stiff models ill conditioned arise path tracking applications noise different time scales small system noise larger measurement noise 
case numerical solution may suffer instability stable formulation unknown 
diagonal entries lie interval replace computed singular values produced finite precision calculations 
calculation causes problem 
having determined variances find solving squares problem oe gamma kc gamma gamma min 
corresponding 
multiplication oe find solution normal equations te oe diagonal obtain kk kk te kk note computed gamma gamma gamma gammat gamma solution estimate recovered gamma matrices need formed explicitly 
vectors efficiently computed triangular solves matrix vector multiplications 
neumaier finding regularization parameter 
new coordinates model equation implies ffl gamma dz ny gamma gamma ax ffl satisfies ffl ffl oe nv oe particular ffl oe hand qualitative condition says reasonable size 
analogy section consider class problems characterized random vectors ez satisfies hw 
natural assumption ffl uncorrelated hw ffl variances oe determined theorem 
kk kk kk kk ffl kk kk ffl kk kk oe min theorem applies kk kk kk place respectively 
large scale problems generalized svd prohibitively expensive compute store 
important formulas rewritten terms original matrices 
allows problems large sparse provided diagonal block diagonal 
case large number applications 
rewrite merit functions rs finding regularization parameter note implies gamma tj te nab gamma te gamma diagonal matrix 
get diag kk kk te kk diag tutorial regularization gamma diag kk kk te kk diag barred quantities yields fi rs gammas tr gamma fl rs gammas gamma ny gamma ny get rs log ny gamma ny gamma log tr gamma gamma log special case merit function log gamma gamma log tr gamma log gcv gcv gamma tr gamma known generalized cross validation gcv formula craven wahba 
need generally full matrix causes computational difficulties sparse matrices 
product ny trace tr computed times cost factorization forming explicitly making formula useful long sparse factorization details sections bj 
case new handled way 
unknown fast methods exist permit sparse case efficient evaluation merit functions hand limiting case simpler 
write formula terms untransformed matrices assume simplicity log det log det log det te log det log kk te kk log det log irrelevant constant log ny gamma ny gamma log log det nonsingular expression written log det const log gamma log det gamma showing irrelevant constant log gml gml ny gamma ny det gamma neumaier generalized maximum likelihood formula wahba 
holds general case expression gml modified take account zero eigenvalues gamma modification needed 
function values gml merit function computed efficiently sparse cholesky factorization gamma tj log gamma gamma ku gamma log log det solution triangular linear system gamma function evaluation requires new factorization efficient univariate minimizer 
suitable starting value minimization tr gamma tr regularization parameter determined solution squares problem completing back substitution solving triangular linear system evaluation gcv formula expensive requires addition factorization needed compute ny significant additional computation tr gml computationally simplest formula previous section efficient 
general regularization problems 
number applications qualitative constraints different origin 
formulates constraint condition linear expression assumed large may take account constraints penalty terms squares problem 
analogy get solution gamma gamma contains regularization parameters qualitative constraint 
cases may assume proportional known constants reducing problem single regularization parameter proportionality factor 
frequently information available relative size regularization parameters determined simultaneously 
gml criterion generalizes natural way remain valid cholesky factor vector regularization parameters may multivariate minimization 
expensive gcv merit function extends similar way situation wahba 
see gu wahba 
tutorial regularization acknowledgment 
part research done author enjoyed stimulating year bell laboratories murray hill nj 
want careful reading draft lead significant extensions results merit functions regularization parameter 
remarks choosing regularization parameter ratio criterion ussr comput 
math 
math 
phys 
pp 

barndorff nielsen cox inference asymptotics chapman hall london 
de mol stability inverse problems inverse scattering optics ed springer pp 

bj numerical methods squares problems siam philadelphia 
ill posed problems method conjugate gradients inverse ill posed problems engl eds academic press boston pp 

craven wahba smoothing noisy data spline functions estimating correct degree smoothing method generalized cross validation numer 
math 
pp 

en algorithms regularization ill conditioned squares problems bit pp 

engl regularization methods stable solution inverse problems surveys math 

pp 

engl hanke neubauer regularization inverse problems kluwer dordrecht 
gill murray wright practical optimization academic press london 
golub van loan matrix computations john hopkins univ press baltimore 
generalized inverses linear operators dekker new york 
chong gu wahba minimizing gcv gml scores multiple smoothing parameters newton method siam sci 
stat 
comput 
pp 

hanke conjugate gradient type methods ill posed problems pitman res 
notes math longman harlow essex 
hanke hansen regularization methods large scale problems surveys math 

pp 

hanke general heuristic choosing regularization parameter ill posed problems siam sci 
comput 
pp 

hansen analysis discrete ill posed problems means curve siam review pp 

hansen rank deficient discrete ill posed problems 
numerical aspects linear inversion siam philadelphia 
katsaggelos digital image restoration springer berlin 
kaufman neumaier pet regularization envelope guided conjugate gradients ieee trans 
medical imag 
pp 

kaufman neumaier regularization ill posed problems envelope guided conjugate gradients comput 
graph 
stud appear 
van loan generalizing singular value decomposition siam numer 
anal 
pp 

van loan computing cs generalized singular value decomposition numer 
math 
pp 

miller squares methods ill posed problems prescribed bound siam math 
anal 
pp 

error bounds tikhonov regularization hilbert scales appl 
anal 
pp 

nemirovski regularization properties adjoint method ill posed problems ussr comput 
math 
math 
phys 
pp 

paige computing generalized singular value decomposition siam sci 
stat 
comput 
pp 

neumaier phillips technique numerical solution certain integral equations kind assoc 
comp 
mach 
pp 

riley solving systems linear equations positive definite symmetric possibly ill conditioned matrix math 
tables aids comput 
pp 

stewart method computing generalized singular value decomposition matrix pencils ruhe eds springer new york pp 

tikhonov solution incorrectly formulated problems regularization method soviet math 
dokl 
pp 

vogel oman iterative methods total variation denoising siam sci 
comput 
pp 

wahba comparison gcv gml choosing smoothing parameter generalized spline smoothing problem ann 
statist 
pp 

wahba spline models observational data siam philadelphia 
welsh processes estimation ann 
statist 
pp 

correction ann 
statist 

wiener cybernetics mit press cambridge ma 
wing primer integral equations kind siam philadelphia 
