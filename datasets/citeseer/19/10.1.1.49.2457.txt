bagging boosting quinlan university sydney sydney australia quinlan cs su oz au breiman bagging freund schapire boosting methods improving predictive power classifier learning systems 
form set classifiers combined voting bagging generating replicated bootstrap samples data boosting adjusting weights training instances 
reports results applying techniques system learns decision trees testing representative collection datasets 
approaches substantially improve predictive accuracy boosting shows greater benefit 
hand boosting produces severe degradation datasets 
small change way boosting combines votes learned classifiers reduces downside leads slightly better results datasets considered 
designers empirical machine learning systems concerned issues computational cost learning method accuracy intelligibility theories constructs 
research learning tended focus improved predictive accuracy performance new systems reported perspective 
easy understand accuracy primary concern applications learning easily measured opposed intelligibility subjective rapid increase computers performance cost ratio de emphasized computational issues applications 
active subarea learning decision tree classifiers examples methods improve accuracy ffl construction multi attribute tests logical combinations rendell arithmetic combinations utgoff brodley extremely large datasets learning time remain dominant issue catlett chan stolfo 
heath kasif salzberg counting operations murphy pazzani zheng 
ffl error correcting codes classes dietterich bakiri 
ffl decision trees incorporate classifiers kinds brodley ting 
ffl automatic methods setting learning system parameters kohavi john 
typical datasets shown lead accurate classifiers cost additional computation ranges modest substantial 
renewed interest increasing accuracy generating aggregating multiple classifiers 
idea growing multiple trees new see instance quinlan buntine justification methods empirical 
contrast new approaches producing classifiers applicable wide variety learning systems theoretical analyses behavior composite classifier 
data classifier learning systems consists attribute value vectors instances 
bootstrap aggregating bagging breiman boosting freund schapire manipulate training data order generate different classifiers :10.1.1.32.8918
bagging produces replicate training sets sampling replacement training instances 
boosting uses instances repetition maintains weight instance training set reflects importance adjusting weights causes learner focus different instances leads different classifiers 
case multiple classifiers combined voting form composite classifier 
bagging component classifier vote boosting assigns different voting strengths component classifiers basis accuracy 
examines application bagging boosting quinlan system learns decision tree classifiers 
brief summary methods comparative results substantial number datasets reported 
boosting generally increases accuracy leads deterioration datasets experiments probe reason 
small change boosting voting strengths component classifiers allowed vary instance instance shows improvement 
final section summarizes tentative reached outlines directions research 
bagging boosting assume set instances belonging classes learning system constructs classifier training set instances 
bagging boosting construct multiple classifiers instances number repetitions trials treated fixed freund schapire note parameter determined automatically cross validation 
classifier learned trial denoted composite bagged boosted classifier 
instance classes predicted respectively 
bagging trial training set size sampled replacement original instances 
training set size original data instances may appear appear 
learning system generates classifier sample final classifier formed aggregating classifiers trials 
classify instance vote class recorded classifier class votes ties resolved arbitrarily 
cart breiman friedman olshen stone learning system breiman reports results bagging moderate sized datasets 
number replicates set average error bagged classifier ranges corresponding error single classifier learned 
breiman introduces concept order correct classifier learning system training sets tends predict correct class test instance frequently class 
order correct learner may produce optimal classifiers breiman shows aggregating classifiers produced order correct learner results optimal classifier 
breiman notes vital element instability prediction method 
perturbing learning set cause significant changes predictor constructed bagging improve accuracy 
boosting version boosting investigated adaboost freund schapire :10.1.1.32.8918
drawing succession independent bootstrap samples original instances boosting maintains weight instance higher weight instance influences classifier learned 
trial vector weights adjusted reflect performance corresponding classifier result weight misclassified instances increased 
final classifier aggregates learned classifiers voting classifier vote function accuracy 
denote weight instance trial trial classifier constructed instances distribution weight instance reflects probability occurrence 
error ffl classifier measured respect weights consists sum weights instances misclassifies 
ffl greater trials terminated altered 
conversely correctly classifies instances ffl zero trials terminate weight vector trial generated multiplying weights instances classifies correctly factor fi ffl gamma ffl renormalizing equals 
boosted classifier obtained summing votes classifiers vote classifier worth log fi units 
provided ffl freund schapire prove error rate examples original uniform distribution approaches zero exponentially quickly increases 
succession weak classifiers fc boosted strong classifier accurate usually accurate best weak classifier training data course gives guarantee generalization performance unseen instances freund schapire suggest mechanisms vapnik structural risk minimization maximize accuracy new data 
requirements boosting bagging methods utilizing multiple classifiers different assumptions learning system 
bagging requires learning system stable small changes training set lead different classifiers 
breiman notes poor predictors transformed worse ones bagging 
boosting hand preclude learning systems produce poor predictors provided error distribution kept 
boosting implicitly requires instability bagging gamma weight adjustment scheme property ffl 
freund schapire specification adaboost force termination ffl fi case classifiers votes zero bagged boosted boosting vs vs vs bagging err err ratio err ratio ratio anneal audiology auto breast chess colic credit credit diabetes glass heart heart hepatitis hypo iris labor letter lymphography phoneme segment sick sonar soybean splice vehicle vote waveform average table comparison bagged boosted versions 
weight final classification 
similarly overfitting learner produces classifiers total agreement training data cause boosting terminate trial 
experiments modified produce new versions incorporating bagging boosting 
facility deal fractional instances required attributes missing values easily adapted handle instance weights boosting 
versions referred bagged boosted evaluated representative collection datasets uci machine learning repository 
datasets summarized appendix show considerable diversity size number classes number type attributes 
parameter governing number classifiers generated set experiments 
breiman notes improvement bagging evident replications interesting see performance improvement bought single order magnitude increase computation 
parameters default values pruned unpruned trees reduce chance boosting terminate prematurely ffl equal zero 
complete fold cross validations carried dataset 
results trials appear table 
dataset column shows mean error rate cross validations 
second section contains similar results bagging class test instance determined voting multiple trees obtained bootstrap sample 
figures number complete cross validations bagging gives better worse results respectively ties omitted 
section shows ratio error rate bagging error rate value fold stratified cross validation training instances partitioned equal sized blocks similar class distributions 
block turn test data classifier generated remaining blocks 
chess number trials error boosting bagging colic number trials error boosting bagging comparison bagging boosting datasets represents improvement due bagging 
similar results boosting compared third section bagging fourth 
clear datasets bagging boosting lead markedly accurate classifiers 
bagging reduces classification error approximately average superior datasets 
boosting reduces error improves performance datasets degrades performance 
tailed sign test bagging boosting superior significance level better 
bagging boosting compared head head boosting leads greater reduction error superior bagging datasets significant level 
effect boosting erratic leads increase error iris dataset colic 
bagging risky worst performance auto dataset error rate bagged classifier higher 
difference highlighted compares bagging boosting datasets chess colic function number trials boosting identical better bagging instances bagging employs sample omissions repetitions 
increases performance bagging usually improves boosting lead rapid degradation colic dataset 
boosting fail 
experiment carried order better understand boosting leads deterioration generalization performance 
freund schapire put overfitting large number trials allows composite classifier complex 
discussed earlier objective boosting construct classifier performs training data constituent classifiers weak 
simple alteration attempts avoid overfitting keeping small possible impacting objective 
adaboost stops error drops zero address possibility correctly classify training data 
trials situation offer gain increase complexity improve performance training data 
experiments previous section repeated adding condition stopping trials complete 
cases requires boosted trials produce classifier performs perfectly training data average number trials datasets 
despite fewer trials prone overfitting generalization performance worse 
overfitting avoidance strategy results lower cross validation accuracy datasets higher unchanged degradation significant better level 
average error datasets higher reported boosting table 
results suggest undeniable benefits boosting attributable just producing composite classifier performs training data 
calls question hypothesis overfitting sufficient explain boosting failure datasets benefit realized boosting caused overfitting 
changing voting weights freund schapire explicitly consider adaboost confidence estimates provided learning systems 
instance classified number represents informal measure reliability prediction 
freund schapire suggest ing estimate give flexible measure classifier error 
alternative confidence estimate combining predictions classifiers fc give final prediction class instance fixed weight log fi vote classifier plausible allow voting weight vary response confidence classified 
tweaked yield confidence estimate 
single leaf classify instance belonging class denote set training instances mapped leaf subset belong class confidence prediction instance belongs class estimated laplace ratio theta sk theta unknown values attributes leaves making prediction 
similar confidence estimate constructed situations 
note confidence measure determined relative boosted distribution original uniform distribution instances 
experiments repeated modified form boosting change log fi voting weight classifying instance results show improvement datasets error rate dataset higher error rate datasets chess 
average error rate approximately obtained original voting weights 
modification necessarily ad hoc confidence estimate intuitive meaning 
interesting experiment voting schemes see give error bounds similar proved original boosting method 
trials diverse collection datasets confirmed boosted bagged versions produce noticeably accurate classifiers standard version 
boosting bagging sound theoretical base advantage extra computation require known advance classifiers generated require times computational effort 
experiments fold increase computation buys average reduction classification error 
applications improvements magnitude worth computational cost 
cases improvement dramatic largest dataset letter instances modified boosting reduces classification error 
boosting effective bagging applied performance bagged variable boosted counterpart 
voting weights aggregate component classifiers boosted classifier altered reflect confidence individual instances classified better results obtained datasets investigated 
adjustment decidedly ad hoc undermines theoretical foundations boosting extent 
better understanding boosting fails clear desideratum point 
freund schapire put overfitting degradation occur low values shown 
cases boosting increases error noticed class distributions weight vectors skewed 
iris dataset example initial weights classes equal weight vector fifth trial setosa virginica 
skewed weights lead undesirable bias predicting classes concomitant increase error unseen instances 
especially damaging case classifier derived skewed distribution high voting weight 
may possible modify boosting approach associated proofs weights adjusted separately class changing class weights 
written freund schapire applied adaboost bagging datasets 
results confirm error rates boosted bagged classifiers significantly lower single classifiers 
find bagging competitive boosting superior datasets equal inferior 
important differences experiments reported account discrepancy 
freund schapire higher number boosting bagging trials 
second modify weighted instances resampling training data manner analogous bagging probability selecting instance draw trial resampling negates major advantage enjoyed boosting bagging viz 
training instances produce constituent classifier 
manfred warmuth rob schapire stimulating tutorial winnow boosting 
research supported australian research council 
appendix description datasets name cases classes attributes cont discr anneal audiology auto breast chess colic credit credit diabetes glass heart heart hepatitis hypo iris labor letter lymph phoneme segment sick sonar soybean splice vehicle vote waveform breiman 
bagging predictors 
machine learning forthcoming 
breiman friedman olshen stone 
classification regression trees 
belmont ca wadsworth 
brodley 
addressing selective superiority problem automatic algorithm model class selection 
proceedings th international conference machine learning 
san francisco morgan kaufmann 
buntine 
learning classification trees 
hand 
ed artificial intelligence frontiers statistics 
london chapman hall 
catlett 
test flight 
proceedings th international workshop machine learning 
san francisco morgan kaufmann 
chan stolfo 
comparative evaluation voting meta learning partitioned data 
proceedings th international conference machine learning 
san francisco morgan kaufmann 
dietterich bakiri 
solving multiclass learning problems error correcting output codes 
journal artificial intelligence research 
freund schapire 
decisiontheoretic generalization line learning application boosting unpublished manuscript available authors home pages www research 
att com orgs ssr people 
extended appears computational learning theory second european conference eurocolt springer verlag 
freund schapire 
experiments new boosting algorithm 
unpublished manuscript 
heath kasif salzberg 
learning oblique decision trees 
proceedings th international joint conference artificial intelligence 
san francisco morgan kaufmann 
kohavi john 
automatic parameter selection minimizing estimated error 
proceedings th international conference machine learning 
san francisco morgan kaufmann murphy pazzani 
id constructive induction concepts discriminators decision trees 
proceedings th international workshop machine learning 
san francisco morgan kaufmann 
quinlan 
inductive knowledge acquisition case study 
quinlan 
ed applications expert systems 
wokingham uk addison wesley 
quinlan 
programs machine learning 
san mateo morgan kaufmann 
rendell 
lookahead feature construction learning hard concepts 
proceedings th international conference machine learning 
san francisco morgan kaufmann 
ting 
problem small disjuncts remedy decision trees 
proceedings th canadian conference artificial intelligence 
utgoff brodley 
incremental method finding multivariate splits decision trees 
proceedings th international conference machine learning 
san francisco morgan kaufmann 
vapnik 
estimation dependences empirical data 
new york springer verlag 
zheng 
constructing nominal attributes 
proceedings th international joint conference artificial intelligence 
san francisco morgan kaufmann 
