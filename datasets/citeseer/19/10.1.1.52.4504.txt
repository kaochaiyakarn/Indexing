detection exploitation file working sets carl tait tait cs columbia edu dan duchamp duchamp cs columbia edu computer science department columbia university new york ny cucs habits individuals yield file access patterns quite pronounced regarded defining working sets files particular applications 
describes client side cache management technique detecting patterns exploiting successfully prefetch files servers 
trace driven simulations show technique substantially increases hit rate client file cache environment client workstation dedicated single user 
successful file prefetching carries major advantages applications run faster burst load placed network properly loaded client caches better survive network outages 
technique requires little extra code simply augmentation standard lru client cache management algorithm easily incorporated existing software 
supported new york state science technology foundation center advanced technology computer information systems foundation ibm hewlett packard 
views contained document authors interpreted representing official policies expressed implied sponsoring agencies 

investigates automatically prefetch files file server single user client workstation 
form prefetching old idea computer architecture 
major impact computer architecture tight time complexity constraints paging hardware software 
prefetching files promising endeavor reasons 
file accesses frequent page accesses speed decision prefetch essence 
penalty faulty prefetching severe wasting space client file cache physical memory 
resource needed arrange intelligent file prefetching client cpu cycles resource excess distributed systems 
addition hypothesize patterns individuals yield file access patterns quite pronounced regarded defining working sets files particular applications 
hypothesis true possible capitalize promise file prefetching 
section describe algorithm automatically detecting application working set files forms prefetching working set application executed 
method fact detect exploit multiple distinct working sets generated different executions application 
capability important application executed frequently different input compiler run different similar modules system build 
successful file prefetching carries major advantages 
applications run faster experience higher cache hit rate 

burst load placed network prefetching done background demand 

properly loaded client caches better survive network outages 
advantages extra impact client small cache network slow 
expect technique particularly useful small portable workstations connected radio network slow relative modern wired networks 
design file system environment motivation 
implemented prefetching algorithm portable environment section reports encouraging results simulations driven real file access traces 
section argues despite lack demonstration implementation algorithm straightforward quick implement imposes little overhead regular file system activity 
technique desirable small cache environments effective practical 

algorithm unix style operating systems program gives rise tree forked child processes programs access open create files 
files may accessed executed different programs graph results necessarily pure tree may cross edges back edges 
ease expression refer tree structure tree 
algorithm simple idea save distinct trees generated program compare trees built current activity saved trees detect saved tree executed prefetch remainder files 
subtle problems creep seemingly simple algorithm 
happens saved tree large cache 
cross edges back edges trees destroy pure tree structure 
describes address practical details argues resulting algorithm effectively file prefetching distributed file system 

data structure working graph user working graph built reflects current file access patterns 
file program data node graph 
conceptually types arcs 
program forks program draw arc 
program accesses creates file draw arc files need distinct 
fact observed cases program accesses data file 
represent distinct arc types need distinguish types graph 
detect cases different ways edges considered equivalent graph 
order reflect chronology file accesses arc order preserved allow multiple arcs exist consecutive accesses ignored nonconsecutive accesses preserved phase algorithm 
place upper bound currently number links nodes 
subsequent links nodes ignored 
working graph constantly pruned rebuilt links sufficient virtually case seen 
form mark sweep garbage collection periodically invoked reclaim dead nodes working graph 
see diagram working graph series accesses 
program executed sample program tree 
program forks program 
accesses files order 
accesses 
forks 
accesses note file draw arcs node graph 
shows conceptual structure tree represented memory 
unix shells invoked wide variety circumstances treated differently programs 
shell uses file draw arc closest non shell ancestor shell file accessed executed 
essence draw arcs shells effectively cutting working graph 
justification ignoring shells shell acting command interpreter simply extension program forked 
special cases need mentioned 
temporary files names tmp usr tmp included working graph 
nature files occur repeated patterns computation prefetching files waste 
devices files names dev really conventional files place working graph 

saving loading trees program executed working graph checked tree formed previous execution program rooted program 
tree exists copied working graph top stack trees saved program 
links emanating program working graph deleted 
different trees saved program 
result process st execution program begins tree formed nth execution unlinked working graph moved program stack 
moment st execution begins working set nth execution defined 
notice moment definition may late fact time working set information exploited 
st execution begins form tree working graph tree compared trees saved stack 
tree working graph match saved trees prefetching saved tree initiated 
order choose tree prefetch program executes algorithm follows simple guideline wait observed file usage pattern matches saved trees 
trees match wait file accesses 
trees match prefetch execution 
order forming tree working graph match saved tree initial file match exactly files program directly touched far touched root saved tree 
happens guess wrong 
may prefetch saved tree discover terrible match files accessed 
program executes time compare tree built tree prefetched 
match close discard saved tree replace tree just built assume newer better match reasonable 
constructed tree tree prefetched save stack trees program 
definition close heuristic effective data seen tree files tree deem match successful 
allows imperfect matches want rarely trees touch exactly files 
contains example traces 
trees rooted cc unix compiler 
tree responsible creating object files second links object files executables 
start way cc executes reads file containing language information 
algorithm waits set files current cc execution touches exactly saved trees 
cc access previously unknown file tree chosen give prefetching cc execution apparently matching cc lang info cpp cc ld lang info 
different cc trees tree 
note consider access order important 
reason cc touched doing tree selected spot 
important note entire process repeated program tree loaded 
idea program prefetches tree tree selected may may subtree larger tree prefetched earlier 
effectively allows minor prefetch corrections level tree reduces cost bad guess high level 
means concept tree currently prefetched prefetch set constantly modified constituent program executed 
coupling needed working graph saved trees fact substantially eases implementation 

tree splitting prefetched tree big fit cache resort tree splitting scheme 
step tree preorder order files subtree fit comfortably cache 
hard part 
stepping tree take nodes subtrees severed original tree point directly single new root creating single subtree saved 
form path compression 
file accessed pointers saved subtree root load subtree splitting necessary 
note take precedence tree selection scheme described earlier load saved subtrees 
shows tree broken cache hold files 
note pointed directly new root due path compression 

common prefix trees assuming unique saved tree load split subtree previous load available immediately compute prefetch common prefix tree program 
prefix tree level tree root program children root file accesses common saved trees 
require strict matching case root prefix tree children means children root saved tree identical 
added prefix tree feature algorithm discovering oftentimes file accesses saved trees common prefix 
case unoptimized algorithm typically file access prefix despite fact prefix perfectly predictable 
prefix tree computed loaded tree destroyed 
avoid problem tree splitting construction ensures prefix trees large cache extra files common prefix ignored 

cycle trees case requiring special attention long running programs establish repeated file access patterns single execution 
basic algorithm handle case tree matching code triggered program loaded saved loaded saved 
tree splitting begins execution 
get problem cycle trees 
tree consists cyclic pattern file accesses detected running program 
algorithm checks cycles load tree type 
compares access patterns file just touched builds level tree includes longest common prefix patterns 
cycle trees common prefix trees 
include immediate children program question contain exactly file accesses common previous executions access patterns construction large cache 
loaded tree destroyed immediately 
shows cycle tree constructed program pattern file accesses 
generates cycle tree 
cycle tree construction 
prefetch confidence final point loading trees sort order avoid destroying entire cache due bad prefetching guess fill cache loading tree 
prefetch confidence arbitrary 
varying little effect hit rate number small 

results results obtained trace driven simulation 

trace data traces gathered sun running sunos 
version sunos offers secure computing facility includes ability produce system call audit trail 
feature gathered large traces single user file access activity 
monitored operations process creation fork execve file access creation open creat mkdir 
operations needed build working graph 
traces report file access activity volunteer user performing normal activity period weeks 
trace contains events cap tured hours 
second trace contains events captured hours numbers third trace respectively 
hours activity varied widely included compilations document production data analysis display large searches certain files unix find commands usenet news reading printing operations 

simulation methodology task algorithm simulator step trace files managing cache accordance graphical prescribed algorithm 
basis comparison simulator manages separate cache simple lru replacement policy time 
trace data provides information concerning file sizes simply define cache size number files example files kilobytes 
sunos particular files involved dynamic linking process opened program executes 
files accessed frequently reasonable method certainly keep locked cache 
want see method compares lru cases algorithms reasonably expected differ filtered heavily files trace data simulation 
order fair comparisons algorithm lru despite adjustment report ratio hit ratio 
reason significant numbers summarizing simulation run number hits number misses number total accesses number misses unaffected removing certain accesses assumed hit cache 
ratio ratios algorithms remains despite adjusted data ratio ratios simply ratio misses 
current parent directory files accessed frequently 
names represent fixed files simple matter cache current parent directories file filtered 
left interesting cases provide reliable measure algorithm effectiveness vis vis lru 

simulation results expected increased intelligence method effective versus lru smaller caches 
extreme case infinitely large cache doesn matter replacement policy uses files replaced 
optimal column table shows ratios obtain situation 
bearing mind ran simulator cache sizes files 
kept track cache misses lru tree algorithm 
summary results traces appears table 
tree algo rithm substantially better ratio lru cache size files 
larger caches tree method performs lru 
trace cache size lru tree optimal lru tree table file cache rate practice algorithm approach optimal ratio repeated find operations 
addition monitored burst hit ratios method burst accesses 
run checked see method won bursts 
shown table tree method won comparison easily consistently 
shows tree algorithm superiority lru steady stable simply result exceptionally fruitful prefetch sequences 
ties burst level attributable large number new files suddenly brought cache methods happen large find example 
tree method losses due bad prefetching guesses offset wins 
trace cache size wins losses ties table tree vs lru bursts accesses measured rate periods immediately tree prefetch 
loading tree size monitored ratios methods accesses root counted initiated prefetch 
table shows results measurements 
desired rate tree method quite low periods prefetches indicating algorithm effective choosing files prefetch 
trace cache size lru tree lru tree table rates tree prefetches important note 
assume prefetching done background user single file triggered prefetch 
prefetched files treated cache reality user need files background prefetch brought client cache 
anticipate significant problem working implementation tell certainty 
entire simulator consists approximately lines table gives name purpose code size approximate running time major pieces simulator 
routines called quite infrequently timing figures milliseconds call average milliseconds file access calling frequency taken account 
code section lines call avg 
add links working graph compare graph saved trees decide prefetch determine prefetch succeeded split saved tree cache sized pieces garbage collection old graph nodes miscellaneous utility routines simulator skeleton part algorithm declarations constants total table size running time code sections determining prefetch succeeded occurs times accesses 
tree splitting rarer required times accesses 
garbage collection performed accesses 
simulation timed sun mips machine 

limitations algorithm depends unix style model process forks order build trees 
fork information vital method define hierarchical chronology file accesses leading tree working sets files 
dependence fork information mandates algorithm run client side 
really problem interests scalability certainly desirable minimize server resources 
certain file access patterns limit effectiveness algorithm 
trace half misses caused data analysis program repeated executions accessed different subsets files directory accessed files different order 
simulator deals poorly case 
working implementation problem reduced eliminated prefetching entire directories measurements indicate user directory high percentage files accessed 
simulator implicitly file caching 
acceptable studies indicate great majority files read entirety 
furthermore minor trend file systems file caching 
large files shared files remain server addressed problem deal files 
gather trace data different environments 
traces currently provide encouraging evidence effectiveness algorithm 

implementation issues implementation algorithm function quite simulation 
data structures described maintained file cache manager analyses data structures take place times 
common data structure occupies bytes thousands created long runs 
particular data garbage collection effect inducing equilibrium number nodes existence result total program size remained relatively constant kb 
smaller caches required program space large ones due greater number trees split 
note prefetch recommendations produced analysis working graph treated hint 
cache management policy remains lru 
working graph stacks saved trees time reduced thrown away negative effect reduced hit rate bounded hit rate lru produce 
algorithm sensitive particular choices size data structures performance resource traded necessary 
desirable maintain information long possible 
avoid incurring learning curve costs new login reboot data structures checkpointed disk idle periods 
cache manager need extra mechanism perform prefetching advantageous interface client cache server augmented include way ask call files prefetched 
disadvantage algorithm requires information parts operating system increasingly separated process management file system 
fortunately information needed cache manager minimal user id individual file access behavior snooped notice fork exec call individual processes 
algorithm provides hints information batched provided late file system reduced performance cost 
information needed construct trees user id process id open creat mkdir typically available file system implementation needed check permissions establish open file tables 
way user needs involved point process turn cache manager snooping 
system call available turn snooping prefetching 
summary implementation include changes simple lru cache manager 
interface process manager file system include way process manager pass parent child information file system 

user interface file system include call turn snooping prefetching 

client server interface file system include way specify prefetching files time 

client cache manager able checkpoint data structures 
changes essential third change desirable useful 

related file prefetching new idea 
design distributed file systems reached basic level sophistication designers began contemplate increasing client cache hit rates various means labeled prefetching 
infrequent annoying loss file servers led related desire client sites independent servers 
previous practical topic discussed 
argue substantial advantages previous direction 
early discussion concept occurred workshop 
idea simple gather client files need near survive communication failures 
large cache help clearly anticipatory fetches play role 
shortly afterward alonso group listed number methods contemplated inclusion face file system 
order increasing sophistication 
users responsible files 
various methods proposed 
enumerating files file 
making explicit command available user 
user tell system record files selected points time 

application needs 

engineer file system read understand certain key files indicate files needed perform task makefiles 

system files commands user 
notions performing application assuming implicit opens requiring application altered include call saving result commands idea 
face report explain idea achieved 
regard mechanism depends users correct timely information action unsatisfactory reasons 
users refuse necessary regular basis required keep primed correct contents 
second users unable fully describe 
cases users forget certain input files required users ignorant certain files required implementation application necessarily unknown 
task modern file systems remove sort burden user 
proposal specification files makefile guide intuitively appealing limited 
applications employ files 
second usually files specify files needed input know files application implementation 
coda highly available file system part approach availability uses brute force sort prefetching 
coda cache management software performs volume fetching entire volume brought user cache user begins 
volume portion file system coda forms administrative unit 
coda environment user files typically placed single volume coda prefetching scheme boils retrieving user files time 
sort prefetching obvious disadvantages depends existence large client cache method tremendously wasteful 
studies published studies shown overwhelming percentage user files touched preceding month 
wasteful volume prefetching clearly ensure user personal files inaccessible workstation 
half coda high availability strategy widely replicate read copies system volumes 
provided assumptions operating environment met coda reliably give access needed files 
summarize coda assumptions applications rarely need files lying outside user volume system portion file space read 
applications function properly read copies system files 
non read sharing user files uncommon 
network connectivity permits constant contact system file volume 
client cache large accommodate volume user needed files 
closely related korner detecting exploiting block access patterns individual files 
style similar patterns detected exploited 
important aspects korner different 
detection patterns line process performed expert systems method provides line real time detection exploitation 

method adjustment lru access patterns korner discusses mru alternatives lru cache manager require substantial re coding benefit noted access patterns 

korner objective limited problem detecting exploiting block access patterns file file access patterns application 
large proportion files accessed entirety clear percentage files benefit korner prefetch rules 
method benefits application file access behavior predictable 
addition korner proposed exploitation file server memory cache disk blocks client side cache server contents 
propose better korner techniques 
korner method provide impressive speedups prefetching file blocks method offers help prefetching files 
korner complement ways 
way file prefetching method client file cache korner technique server side block cache 
better couple techniques remote open file systems move blocks files client server 
case client side block cache technique determine files blocks cache korner technique pick right blocks files 

algorithm detects exploits file working sets 
add prefetching intelligence basic lru cache management strategy 
technique applicable unix style system independent cache consistency algorithm imposes little overhead easily added existing standard software simulation suggests especially small caches effective improving client cache rate obtained lru 
decreasing cache rate speeds applications renders clients independent file server loss 
simulation results interpreted carefully identified total ways simulation accurately mirror events plausible real life implementation 
commonly files filtered trace data 

certain commonly files ignored algo rithm 
files tmp hit lru method rarely method 
names files tmp highly structured hard distinguish process temporary files certain session temporary files 

assumed prefetching accomplished background cache misses 

tree algorithm suffers learning curve 
results included section suggest traces learning curve penalty raises algorithm rates 

cache size measured files bytes 
simulation inaccuracies tend value algorithm inaccuracies tend value 
promise shown simulations existence inaccuracies motivate start implementation 
algorithm trait spending client cycles lesser extent client memory return effective client cache space fewer demand network operations leads believe effective environment radio connected notebook workstations 
environment characterized features limited client cache space constraints size weight power consumption 
plenty client cycles microprocessors 
slow network commercially available state art mb sec 
occurrence transient network outages fades occurring radio null spots 
strong industry trend portable notebook computers expect portable wireless environment increasingly common important 
algorithm offer especially impressive advantages circumstances 

prefetching technique depends repeated executions application programs exhibiting similar file behavior 
suppose condition satisfied personal workstation machine aimed current effort case client single user workstation 
open question technique help hurt file system user machine 
substantial concern algorithm stands limited definition constitutes computation tree forked processes 
strong trend operating systems providing multiplicity ways spread computation processes message passing clients servers shared memory multiple threads control process trend continues harder track locations actions computation 
uncertainty confound method 
consider difficulty tracing file accesses computation accomplished partly calling multithreaded server machine 
expect tracking control distributed computations distributed computing equivalent network management area research near 
bill schilit labor produced traces study 
alonso barbara 
face enhancing distributed file systems autonomous computing environments 
technical report cs tr princeton univ march 
alonso barbara 
augmenting availability distributed file systems 
technical report cs tr princeton univ october 
birrell 
position autonomy storage section third european sigops workshop 
operating systems review april 
floyd ellis 
directory patterns hierarchical file systems 
ieee trans 
knowledge data engineering june 
howard scale performance distributed file system 
acm trans 
computer systems february 
korner 
intelligent caching remote file services 
proc 
tenth intl 
conf 
distributed computing systems pages 
ieee may 
ousterhout trace driven analysis unix bsd file system 
proc 
tenth acm symp 
operating system principles pages 
december 
satyanarayanan coda highly available file system distributed workstation environment 
ieee trans 
computers april 
smith 
analysis long term file patterns application file migration algorithms 
ieee trans 
software engineering se july 
staelin garcia molina 
file system design large memories 
technical report cs tr princeton univ june 
