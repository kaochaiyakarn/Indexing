boosting algorithms gradient descent function space mason jonathan baxter peter bartlett marcus frean research school information sciences engineering australian national university canberra act australia anu edu au jonathan baxter anu edu au peter bartlett anu edu au department computer science electrical engineering university queensland brisbane qld australia elec uq edu au may attention experimental theoretical focussed classification algorithms produce voted combinations classifiers 
theoretical shown impressive generalization performance algorithms adaboost attributed classifier having large margins training data 
algorithms finding linear convex combinations functions minimize arbitrary cost functionals functionals necessarily depend margin 
existing voting methods shown special cases algorithms 
previous theoretical results bounding generalization performance convex combinations classifiers terms general cost functions margin new algorithm doom ii performing gradient descent optimization cost functions 
experiments data sets uc irvine repository demonstrate doom ii generally outperforms adaboost especially high noise situations 
margin distribution plots verify doom ii willing give examples hard order avoid overfitting 
show overfitting behavior exhibited adaboost quantified terms proposed cost function 
considerable interest voting methods pattern classification predict label particular example weighted vote set base classifiers 
example freund schapire adaboost algorithm breiman bagging algorithm give significant performance improvements algorithms corresponding base classifiers led study related algorithms :10.1.1.105.6964:10.1.1.29.9093:10.1.1.30.3515:10.1.1.33.353:10.1.1.133.1040:10.1.1.156.2440:10.1.1.131.1931
theoretical results suggest effectiveness algorithms due tendency produce large margin classifiers 
margin example defined difference total weight assigned correct label largest weight assigned incorrect label 
interpret value margin indication confidence correct classification example classified correctly positive margin larger margin viewed confident correct classification 
results show loosely speaking combination classifiers correctly classifies training data large margin error probability small 
mason bartlett baxter improved upper bounds misclassification probability combined classifier terms average training data certain cost function margins 
describes experiments algorithm directly minimizes cost function choice weights associated base classifier 
algorithm exhibits performance improvements adaboost suggests margin cost functions appropriate quantities optimize 
general algorithm marginboost choosing combination classifiers optimize sample average cost function margin 
marginboost performs gradient descent function space iteration choosing base classifier include combination maximally reduce cost function 
idea performing gradient descent function space way due breiman 
turns adaboost choice base classifier corresponds minimization problem involving weighted classification error 
certain weighting training data base classifier learning algorithm attempts return classifier minimizes weight misclassified training examples 
simpler way view marginboost algorithm 
section describe class algorithms called anyboost gradient descent algorithms choosing linear combinations elements inner product space minimize cost functional 
component linear combination chosen maximize certain inner product 
marginboost inner product corresponds weighted training error base classifier 
section give convergence results class algorithms 
marginboost convex cost function results show particular choice step size base classifier minimizes appropriate weighted error algorithm converges global minimum cost function 
section show general class algorithms includes special cases number popular successful voting methods including freund schapire adaboost schapire singer extension adaboost combinations real valued functions friedman hastie tibshirani logitboost :10.1.1.30.3515
algorithms implicitly minimize margin cost function gradient descent 
section review theoretical results bounding error combination classifiers terms sample average certain cost functions margin 
cost functions suggested results significantly different cost functions implicitly minimized methods described section 
section experimental results marginboost algorithm cost functions motivated theoretical results 
experiments show new algorithm typically outperforms adaboost especially true label noise 
addition cost functions provide estimates error adaboost sense predict overfitting behaviour 
similar techniques directly optimizing margins related quantities described authors 
ratsch show versions adaboost modified regularization robust noisy data 
friedman describes general boosting algorithms regression classification various cost functions presents specific cases boosting decision trees 
duffy helmbold describe algorithms attempt produce combined classifiers uniformly large margins training data 
freund presents new boosting algorithm uses example weights similar suggested theoretical results 
optimizing cost functions margin notation 
assume examples randomly generated unknown probability distribution theta space measurements typically space labels usually discrete set subset 
algorithms section apply different machine learning settings primary interest voted combinations classifiers form sgn sigma base classifiers fixed class classifier weights 
margin example respect classifier sgn defined yf 
set labelled examples generated wish construct voted combination classifiers form described pd sgn small 
probability incorrectly classifies random example small 
unknown training set take approach finding voted classifiers minimize sample average cost function margin 
training set want find minimized suitable cost function note symbol denote cost function real margin yf cost functional function interpretation meant clear context 
anyboost way produce weighted combination classifiers optimizes gradient descent function space idea proposed breiman 
treatment shows existing voting methods may viewed gradient descent suitable inner product space 
level view base hypotheses combinations elements inner product space 
case linear space functions contains lin set linear combinations functions inner product defined hf gi lin 
anyboost algorithms defined section convergence properties studied section valid cost function inner product 
example hold case hf gi dp marginal distribution input space generated suppose function lin wish find new add cost fflf decreases small value ffl 
viewed function space terms asking direction fflf rapidly decreases 
viewing cost functional lin desired direction simply negative functional derivative rc ff ff fi fi fi fi ff indicator function restricted choosing new function general possible choose search greatest inner product 
choose maximize gamma hrc fi motivated observing order ffl fflf ffl hrc fi greatest reduction cost occur maximizing gamma hrc fi 
preceding discussion motivates algorithm iterative algorithm finding linear combinations base hypotheses minimize cost 
note allowed base hypotheses take values arbitrary set restricted form cost inner product specified step sizes 
appropriate choices things apply algorithm concrete situations 
note algorithm terminates gamma hrc weak learner returns base hypothesis longer points downhill direction cost function 
algorithm terminates order step function space direction base hypothesis returned increase cost 
anyboost anyboost algorithm return arbitrary linear combination elements base hypothesis class 
flexibility potential cause overfitting 
theorem section provides guaranteed generalization performance certain classes cost functions provided algorithm returns elements convex combinations elements base hypothesis class consideration motivates algorithm anyboost normalized version anyboost returns functions convex hull base hypothesis class stopping criterion anyboost gamma hrc gamma gamma hrc 
see notice iteration lie 
convenience assume class contains zero function equivalently denotes convex cone containing convex combinations functions zero function 
algorithm anyboost require ffl inner product space containing functions mapping set ffl class base classifiers ffl differentiable cost functional lin ffl weak learner accepts lin returns large value gamma hrc fi 


gamma hrc return choose return incorporating new component update fff gamma ff ff 
clearly gamma hrc fff gamma ff gamma hrc ff hrc gamma non increasing function ff added convex combination 
geometrically gamma hrc gamma implies change gamma associated addition ffi rc 
anyboost anyboost enforces constraint size combined hypotheses returned algorithm 
certain classes cost functionals theoretical guarantees generalization performance algorithms see section aesthetic perspective constraint natural inner product space setting 
particular ask algorithm perform gradient descent regularized cost functional form kfk algorithm anyboost require ffl inner product space containing functions mapping set ffl class base classifiers ffl differentiable cost functional ffl weak learner accepts returns large value gamma hrc gamma 

gamma hrc gamma return choose jw return regularization parameter needing refer individual weights combination contrast anyboost 
constraint freedom allow weak learner return general linear combinations base hypothesis class just single hypotheses general linear combination lin closer negative gradient direction single base hypothesis stepping direction lead greater reduction cost function ensuring hypothesis constructed element lin 
weak learner accepts direction attempts choose maximizing hg fi easily converted weak learner attempts choose lin optimal direction move anyboost pure direction current combined hypothesis convex hull weak learner produces linear combinations powerful weak learner returning single hypothesis case 
true case 
maximizing hg hi details algorithm 
substituted anyboost algorithm 
algorithm weak learner returning linear combinations require ffl inner product space associated norm kfk hf containing functions mapping set ffl class base classifiers ffl differentiable cost functional lin ffl weak learner accepts direction returns large value hg fi 
ffl starting function lin 
gammar kr 

ffh constraints kh hh maximal 
gamma return anyboost margin cost functionals main aim optimization margin cost functionals section specialize anyboost anyboost algorithms previous sections restricting attention inner product cost sigma 
case rc derivative margin cost respect gamma hrc fi gamma sensible cost function margin monotonically decreasing gammac positive 
dividing gamma see finding maximizing gamma hrc fi equivalent finding minimizing weighted error distribution making appropriate substitutions anyboost yields algorithm marginboost 
anyboost require weak learner maximizes gamma hrc gamma current convex combination 
setting equivalent minimizing gamma 
making appropriate substitutions anyboost yields algorithm marginboost gradient descent view voting methods successful voting methods appropriate choice cost function step size specific cases anyboost algorithm described derivatives 
adaboost algorithm arguably important developments practical machine learning past decade 
studies demonstrated adaboost produce extremely accurate classifiers base classifiers simple decision stumps complex neural networks decision trees :10.1.1.133.1040
interpretation adaboost algorithm performs gradient descent optimization sample average cost function margins examined authors 
see adaboost algorithm shown algorithm fact marginboost cost function ff gammaff need verify distributions stopping criteria identical 
distribution adaboost rewritten gammay distribution gammay algorithm marginboost require ffl differentiable cost function ffl class base classifiers containing functions sigma 
ffl training set theta sigma 
ffl weak learner accepts training set distribution training set returns base classifiers small weighted error 


return choose gamma delta gamma delta return clearly gammay gammay substituting gives marginboost distribution cost function ff gammaff definition ffl stopping criterion adaboost equivalent gamma algorithm marginboost require ffl differentiable cost function ffl class base classifiers containing functions sigma 
ffl training set theta sigma 
ffl weak learner accepts training set distribution training set combined classifier returns base classifiers small weighted error gamma 


gamma return choose jw gamma delta gamma delta return identical stopping criterion marginboost 
chosen wish choose minimize differentiating respect setting solving gives ln exactly setting adaboost algorithm 
choice cost function possible find closed form solution line search optimal step size round 
adaboost performing gradient descent cost function gammay step size chosen line search 
algorithm adaboost require ffl class base classifiers containing functions sigma 
ffl training set theta sigma 
ffl weak learner accepts training set distribution training set returns base classifiers small weighted error 


ffl 
ffl return ln gamma gamma ffl ffl delta ffl gamma ffl 
gammaw return schapire singer examine adaboost general setting classifiers produce real values gamma indicating confidence sigma valued classification 
algorithm cost function step size adaboost gammayf line search arc gamma yf gammayf line search logitboost ln gammayf newton raphson table summary existing voting methods viewed gradient descent optimizers margin cost functions :10.1.1.30.3515
general algorithm essentially anyboost cost function yf gammayf base classifiers gamma 
breiman describes arc algorithm 
arc anyboost cost function ff gamma ff decreasing step size friedman examine adaboost approximation maximum likelihood :10.1.1.30.3515
viewpoint develop direct approximation logitboost exhibits similar performance 
logitboost anyboost cost function ff log gamma ff step size chosen single newton raphson step 
table summarizes cost function step size choices anyboost derivatives reduce existing voting methods 
theoretically motivated cost functions definition gives condition cost function cn delta suffices prove upper bounds error probability terms sample averages cn yf 
condition requires cost function cn ff lie strictly mistake indicator function sgn gammaff 
close cn ff sgn gammaff depends complexity parameter definition 
family fcn ng margin cost functions admissible interval ae length function psi gamma satisfies sgn gammaff ez qn ff psi cn ff ff gamma ez qn ff delta denotes expectation chosen randomly sigma pr ff 
base learning algorithm decision trees directly optimizes exponential cost function margin iteration 
variant boosting reduce gradient descent optimization 
theorem 
admissible family fcn ng margin cost functions finite hypothesis class distribution theta sigma probability gamma ffi random sample labelled examples chosen satisfies pr yf yf ffl ffl ln jf ln ffi similar result applies infinite classes finite vc dimension 
theorem complexity parameter increases sample error estimate yf decreases training error proportion misclassified training examples hand complexity penalty term ffl increases choosing effective complexity combined classifier trade terms 
smaller cost functions give favourable trade 
illustrates family cn delta cost functions satisfy admissibility condition 
notice functions significantly different exponential logit cost functions adaboost logitboost 
particular large negative margins value cn ff significantly smaller 
cost margin cost margin theoretically motivated function exponential function logit function cost functions cn ff compared function sgn gammaff 
larger values correspond closer approximations sgn gammaff 
theoretically motivated cost function ff exponential logit cost functions plotted comparison 
convergence results section prove convergence results algorithms anyboost anyboost quite weak conditions cost functional prescriptions step sizes results convergence guarantees practice smaller necessary fixed small steps form line search 
convergence anyboost theorem supplies specific step size anyboost characterizes limiting behaviour step size 
theorem 
lin lower bounded lipschitz differentiable cost functional exists gamma rc lkf gamma lin 
sequence combined hypotheses generated anyboost algorithm step sizes gamma hrc lkf anyboost halts round gamma hrc converges finite value case lim hrc proof 
need general lemma 
lemma 
inner product space norm kfk hf differentiable functional gamma rc lkf gamma wg gamma hrc gi lw kgk proof 
define wg 
hrc wg gi jg gamma hrc wg gamma rc gi wg gamma rc cauchy schwartz lipschitz continuity rc 
hrc gi implies gamma ff dff hrc gi dff hrc gi lw kgk substituting wg left hand side gives result 
write gamma gamma gammaw hrc gamma lw kf lemma kf hrc anyboost terminate 
greatest reduction occurs right hand side maximized gamma hrc lkf step size statement theorem 
stated step size gamma hrc lkf gamma hrc anyboost terminates 
bounded gamma implies hrc 
theorem shows weak learner find best weak hypothesis round anyboost cost functional convex anyboost guaranteed converge global minimum cost 
ease exposition assumed terminating gamma hrc anyboost simply continues return subsequent time steps theorem 
lin convex cost functional properties theorem sequence combined hypotheses generated anyboost algorithm step sizes 
assume weak hypothesis class negation closed gammaf round anyboost algorithm finds function maximizing gamma hrc accumulation point sequence satisfies sup gamma hrc fi inf lin furthermore lim inf lin proof 
accumulation point suppose sup gamma hrc fi ffl 
continuity infinite number sup gamma hrc ffl gamma contradicts prove suppose exists lin 
convexity ffl fflg ffl gamma ffl limit ffl yields hg gamma rc gamma lin gamma coefficients elements negation closure imply exists gamma hf rc contradicting 
accumulation point follows immediately fact monotonically decreasing 
theorem sup gamma hrc fi convexity implies 
convergence anyboost theorem supplies specific step size anyboost characterizes limiting behaviour step size regime 
theorem 
cost function theorem 
sequence combined hypotheses generated anyboost algorithm step sizes gamma hrc gamma lkf gamma hrc gamma anyboost terminates finite time gamma hrc gamma converges finite value case lim hrc gamma proof 
note step sizes positive 
addition clearly second case apply 
loss generality assume 
applying lemma gamma gamma gamma gamma gamma hrc gamma gamma kf gamma gamma hrc gamma algorithm terminates 
right hand side maximized gamma hrc gamma lkf gamma hrc gamma step size statement theorem 
stated step size gamma hrc gamma lkf gamma lower boundedness implies hrc gamma 
theorem shows weak learner find best weak hypothesis round anyboost cost function convex anyboost guaranteed converge global minimum cost 
theorem assumed terminating hf gamma rc anyboost simply continues return subsequent time steps theorem 
convex cost function properties theorem sequence combined hypotheses generated anyboost algorithm step sizes 
assume weak hypothesis class negation closed round anyboost algorithm finds function maximizing gamma hrc gamma accumulation point sequence satisfies inf hf gamma rc note assumption negation closure theorem ensures hf gamma rc 
inf set convex combinations weak hypotheses furthermore lim inf proof 
proof follows lines proof theorem 
omit details 
experiments adaboost perceived resistant overfitting despite fact produce combinations involving large numbers classifiers 
studies shown case base classifiers simple decision stumps 
grove schuurmans demonstrated running adaboost hundreds thousands rounds lead significant overfitting number authors showed adding label noise overfitting induced adaboost relatively classifiers combination 
theoretical motivations described sections propose new algorithm doom ii marginboost performs gradient descent optimization gamma tanh restricted convex combination classifiers base class adjustable parameter cost function 
henceforth refer normalized sigmoid cost function normalized weights normalized convex combination 
family cost functions parameterized qualitatively similar family cost functions parameterized shown 
family practice may cause difficulties gradient descent procedure functions flat negative margins margins close 
normalized sigmoid cost function alleviates problem 
choosing value corresponds choosing value complexity parameter theorem 
data dependent parameter measures resolution examine margins 
large value corresponds high resolution high effective complexity convex combination 
choosing large value amounts belief high complexity classifier obtain large margins overfitting 
algorithm doom ii require ffl class base classifiers containing functions sigma 
ffl training set theta sigma 
ffl weak learner accepts training set distribution training set combined classifier returns base classifiers small error gamma 
ffl fixed small step size ffl 


gamma return ffl 
jw gamma tanh gamma tanh conversely choosing small value corresponds belief high complexity classifier obtain large margins overfitting 
implementation doom ii fixed small step size ffl experiments ffl 
practice fixed ffl replaced line search optimal step size round 
worth noting norm classifier weights fixed iteration cost function property gammaff gamma ff choice equivalent choosing norm weights cost function ff gamma tanh ff 
normalized sigmoid cost function non convex doom ii algorithm suffer problems local minima 
fact result shows cost functions satisfying gammaff gamma ff marginboost algorithm strike local minimum step 
lemma 
cost function satisfying gammaff gammac ff 
marginboost find optimal weak hypothesis time step terminate time step returning proof 
hrc fi assumption satisfy inf gammaff gamma ff gammaff ff takes values sigma hrc gamma gamma hrc gamma marginboost terminate returning simple technique avoiding local minimum apply notion randomized initial conditions hope gradient descent procedure avoid single classifier local minimum basin attraction 
initial margins randomized random initial classifier chosen initial experiments showed techniques somewhat successful guarantee avoidance single classifier local minimum random initial conditions tried computationally intensive prospect 
principled way avoiding local minimum remove round continue algorithm returning cost goes round 
local minimum cost guaranteed increase round 
continue step best available direction uphill direction eventually crest hill defined basin attraction classifier start decrease cost 
cost decreases classifier safely return classifier class available base classifiers 
course guarantee cost decrease classifier round 
practically problem small values cost function linear gamma case classifier corresponds global minimum anyway 
order compare performance doom ii adaboost series experiments carried selection data sets taken uci machine learning repository 
simplify matters binary classification problems considered 
experiments repeated times examples randomly selected training validation test purposes respectively 
results averaged repeats 
experiments axis orthogonal hyperplanes known decision stumps weak learner 
fixed complexity weak learner avoided problems complexity combined classifier dependent actual classifiers produced weak learner 
adaboost validation set perform early stopping 
adaboost run rounds combined classifier round corresponding minimum error validation set chosen 
doom ii validation set set data dependent complexity parameter doom ii run rounds optimal chosen correspond minimum error validation set rounds 
typical behaviour test error doom ii proceeds various values seen 
small values test error converges value worse adaboost test error 
increased optimal value test errors decrease 
case sonar data set test errors adaboost doom ii optimal similar 
course adaboost adaptive step size converges faster doom ii fixed step size 
test error rounds lambda lambda lambda adaboost test error sonar data set rounds adaboost doom ii 
adaboost doom ii run data sets varying levels label noise applied 
summary experimental results provided table 
attained test errors shown data set single stump adaboost applied stumps doom ii stumps applied stumps label noise 
graphical representation difference test error adaboost doom ii shown 
improvement test error exhibited doom ii adaboost standard error bars shown data set noise level 
results show doom ii generally outperforms adaboost improvement pronounced presence label noise 
sonar cleve ionosphere vote credit breast cancer pima indians hypo splice examples attributes stump label adaboost noise doom ii stump label adaboost noise doom ii stump label adaboost noise doom ii table summary test errors single stump adaboost stumps doom ii stumps varying levels label noise uci data sets 
effect normalized sigmoid cost function exponential cost function best illustrated comparing cumulative margin distributions generated adaboost doom ii 
shows comparisons data sets label noise applied 
margin value curve corresponds proportion training examples margin equal value 
curves show trying increase margins negative examples adaboost willing sacrifice margin positive examples significantly 
contrast doom ii gives examples large negative margin order reduce value cost function 
adaboost suffer overfitting guaranteed minimize exponential cost function margins cost function certainly relate test error 
value proposed cost function correlate adaboost test error 
theoretical bound suggests right value data dependent complexity parameter cost function test error closely correlated 
shows error advantage data set sonar cleve ionosphere vote credit breast cancer pima indians hypo splice noise noise noise summary test error advantage standard error bars doom ii adaboost varying levels noise uci data sets 
margin breast cancer wisconsin noise adaboost noise doom ii noise adaboost noise doom ii margin splice noise adaboost noise doom ii noise adaboost noise doom ii margin distributions adaboost doom ii label noise breast cancer splice data sets 
variation normalized sigmoid cost function exponential cost function test error adaboost uci data sets rounds 
values curves averaged random train validation test splits 
value case chosen running doom ii various values choosing corresponding minimum error validation set 
curves show strong correlation normalized sigmoid cost right value adaboost test error 
data sets minimum adaboost test error minimum normalized sigmoid cost nearly coincide 
sonar labor data sets adaboost test error converges overfitting occur 
data sets normalized sigmoid cost exponential cost converge case sonar data set exponential cost converges significantly test error 
cleve vote data sets adaboost initially decreases test error increases test error overfitting set 
data sets normalized sigmoid cost mirrors behaviour exponential cost converges 
rounds sonar adaboost test error exponential cost normalized sigmoid cost rounds cleve adaboost test error exponential cost normalized sigmoid cost rounds labor adaboost test error exponential cost normalized sigmoid cost rounds vote adaboost test error exponential cost normalized sigmoid cost adaboost test error exponential cost normalized sigmoid cost rounds adaboost sonar cleve labor vote data sets 
costs scaled case easier comparison test error 
examine effect step size compare adaboost modified version adaboost fixed step sizes 
adaboost classifier weight weight 
comparison test errors algorithms various values shown 
expected changing value fixed step size simply translates test error curve log scale doesn alter minimum test error 
test error rounds adaboost epsilon epsilon epsilon epsilon test error vote data set rounds adaboost adaboost 
shown existing boosting type algorithms combining classifiers viewed gradient descent appropriate cost functional suitable inner product space 
anyboost algorithm type generating general linear combinations base hypothesis class related algorithm anyboost generating convex combinations base hypothesis class 
prescriptions step sizes algorithms guaranteeing convergence optimal linear convex combination 
cost functions depending margins classifier training set anyboost anyboost marginboost marginboost showed existing algorithms combining classifiers viewed special cases marginboost algorithm differing choice margin cost function step size 
particular adaboost marginboost gammaz cost function margin step size equal line search 
main theoretical result provides bounds generalisation performance convex combination classifiers terms training sample averages certain cost functions margin 
theorem shows algorithms adaboost optimize exponential margin cost function placing emphasis examples large negative margins explanation overfitting behaviour particularly presence label noise 
motivated result derived doom ii specialization marginboost gamma tanh cost function margin experimental results uci datasets verified doom ii generally outperformed adaboost boosting decision stumps particularly presence label noise 
doom ii cost training data reliable predictor test error adaboost exponential cost 
plan investigate properties anyboost mentioned briefly section 
theoretical results generalization performance algorithm viewed inner product space setting constraint combined hypothesis considerably natural constraint 
addition inner product perspective boosting applied inner product space just spaces functions done 
opens possibility applying boosting machine learning settings 
acknowledgments research supported australian research council 
mason supported australian postgraduate research award 
jonathan baxter supported australian postdoctoral fellowship 
peter bartlett marcus frean supported institute advanced studies australian universities collaborative 
shai ben david stimulating discussion 
bartlett 
sample complexity pattern classification neural networks size weights important size network 
ieee transactions information theory march 
bauer kohavi 
empirical comparison voting classification algorithms bagging boosting variants 
machine learning 
appear 
breiman 
bagging predictors 
machine learning 
breiman 
prediction games arcing algorithms 
technical report department statistics university california berkeley 
keogh blake merz 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
dietterich 
experimental comparison methods constructing ensembles decision trees bagging boosting randomization 
technical report computer science department oregon state university 
drucker cortes 
boosting decision trees 
advances neural information processing systems pages 
duffy helmbold 
geometric approach leveraging weak learners 
computational learning theory th european conference 
appear 
frean downs 
simple cost function boosting 
technical report department computer science electrical engineering university queensland 
freund 
adaptive version boost majority algorithm 
proceedings twelfth annual conference computational learning theory 
appear 
freund schapire 
experiments new boosting algorithm 
machine learning proceedings thirteenth international conference pages 
freund schapire 
decision theoretic generalization line learning application boosting 
journal computer system sciences august 
friedman 
greedy function approximation gradient boosting machine 
technical report stanford university 
friedman hastie tibshirani :10.1.1.30.3515
additive logistic regression statistical view boosting 
technical report stanford university 
grove schuurmans 
boosting limit maximizing margin learned ensembles 
proceedings fifteenth national conference artificial intelligence pages 
maclin opitz 
empirical evaluation bagging boosting 
proceedings fourteenth national conference artificial intelligence pages 
mason bartlett baxter 
improved generalization explicit optimization margins 
machine learning 
appear 
quinlan 
bagging boosting 
proceedings thirteenth national conference artificial intelligence pages 
ratsch onoda 
muller 
soft margins adaboost 
technical report nc tr department computer science royal holloway university london egham uk 
schapire freund bartlett lee 
boosting margin new explanation effectiveness voting methods 
annals statistics october 
schapire singer 
improved boosting algorithms confidence rated predictions 
proceedings eleventh annual conference computational learning theory pages 
schwenk bengio 
training methods adaptive boosting neural networks 
advances neural information processing systems pages 
