new metric approach model selection dale schuurmans institute research cognitive science university pennsylvania philadelphia pa linc cis upenn edu nec research institute independence way princeton nj dale research nj nec com appears proceedings fourteenth national conference artificial intelligence aaai providence ri july 
introduce new approach model selection performs better standard hold error estimation techniques cases 
basic idea exploit intrinsic metric structure hypothesis space determined natural distribution unlabeled training patterns metric detect empirical error estimates derived small labeled training sample trusted region empirically optimal hypothesis 
simple metric intuitions develop new geometric strategies detecting overfitting performing robust responsive model selection spaces candidate functions 
new metric strategies dramatically outperform previous approaches experimental studies classical polynomial curve fitting 
technique simple efficient applied function learning tasks 
requirement access auxiliary collection unlabeled training data 
standard problem learning prediction function training examples hx hx idea take small set labels extend total prediction function entire domain goal produce function predicts labels possibly unseen accurately possible measure accuracy predictions specified error function err 
simplest prototypical approach problem conjecture suitable class hypothesis func copyright fl american association artificial intelligence www aaai org 
rights reserved 
note research function learning considers specific representations domain objects attribute vectors range predictions binary real valued label prediction functions feedforward neural networks decision trees focuses specific error functions zero loss squared error ky gamma yk take simple view encompasses choices 
tions specifying neural net architecture representation class choose hypothesis minimizes empirical error err training set 
course key making approach choose right hypothesis class argue example advantageous expressive possible afford greatest chance representing reasonable hypothesis 
making expressive run risk overfitting training data producing hypothesis function predicts poorly test examples see 
fact developed statistical theory supports intuition saying reliably near best function require training sample size proportional complexity hypothesis class vapnik pollard haussler 
suggests restrict complexity hypothesis class 
course introduce opposite problem underfitting 
restrict severely eliminate reasonable hypotheses perfectly acceptable prediction functions exist 
fundamental dilemma machine learning need hypothesis classes expressive possible maximize chances representing hypothesis need restrict classes ensure reliably distinguish bad hypotheses geman bienenstock doursat 
tradeoff ability represent function ability identify function exists 
question face dilemma dominates machine learning research 
successful applied machine learning systems employ sort mechanism balance hypothesis complexity data fit 
common strategy coping dilemma practice form automatic model selection stratify hypothesis class sequence nested classes ae ae choose class appropriate complexity training data 
understand choice note training set hx hx obtain corresponding sequence empirically optimal functions successive subclass 
basic model selection problem choose functions basis observed empirical errors 
note errors monotonically decreasing choosing function minimum training error simply leads choosing function largest class 
trick invoke criteria empirical error minimization choice 
previous approaches currently model selection strategies predominate 
common strategy complexity penalization 
assign increasing complexity values successive function classes choose hypothesis minimizes prior combination complexity empirical error additive combination err 
variants basic approach including minimum description length principle rissanen bayesian maximum posteriori selection structural risk minimization vapnik generalized cross validation craven wahba different real cross validation regularization moody 
strategies differ specific complexity values assign particular tradeoff function optimize basic idea 
common strategy hold testing 
ask set training data hypothesis class generalizes best 
answer partitioning training set set hold test set pseudo training set obtain sequence pseudo hypotheses hold test set obtain unbiased estimate true errors pseudo hypotheses 
note training set errors tend gross underestimates general 
unbiased estimates simply choose hypothesis class yields pseudo hypothesis smallest estimated error 
selected return function obtains minimum empirical error entire training sequence 
variants basic strategy having repeating pseudo train pseudo test split times averaging results choose final hypothesis class fold cross validation leave testing bootstrapping 
efron weiss kulikowski 
repeated testing manner introduce bias error estimates results generally better considering single hold partition weiss kulikowski 
sequence empirically optimal functions determined stratification ae ae training set 
dotted lines indicate decreasing empirical errors 
idea propose fundamentally different approach model selection better complexity penalization hold testing cases 
basic idea exploit intrinsic geometry function learning task arises simple statistical model problem assume training test examples independent random observations drawn joint distribution xy theta decompose distribution conditional distribution yjx marginal distribution note learning prediction function really interested approximating conditional yjx approach try exploit knowledge help better decisions hypothesis choose 
fact assume know see far gets 
knowing help 
thing give natural measure distance hypotheses fact obtain natural pseudo metric space hypotheses definition err dp measure distance functions average discrepancy random objects reason quotes explained 
extend definition include target conditional yjx definition yjx err dp yjx dp means interpret true error function distance target object yjx importantly definitions compatible sense defined metric satisfies standard axioms fp yjx notice gives nice geometric view problem nested sequence spaces ae ae closest function target yjx distances decreasing 
note information require obtained unlabeled training examples 
key leverage approach having access collection unlabeled data stabilize model selection behavior 
geometric view model selection nested sequence spaces ae ae try find closest function yjx estimated distances 
get observe real distances 
training sample choose hypothesis sequence empirically closest functions decreasing estimates yjx err 
key point information disposal estimated distances yjx know true distances functions sequence 
indicated bold lines 
idea additional information choose better hypothesis 
intuition inter function distances help detect overfitting 
example consider hypotheses small estimated distances yjx large true distance 
claim worry choosing second function 

true distance large functions close yjx simple geometry 
means estimates wrong know trust earlier estimate 
fact yjx yjx really accurate estimates satisfy triangle inequality known distance yjx yjx empirical distances eventually gross underestimates general explicitly chosen minimize empirical distance training set triangle inequality test detect estimates untrustworthy 
fact forms basis simple model selection strategy tri choose function sequence violate triangle inequality preceding function 
simple procedure turns surprisingly experimental situations 
case study polynomial curve fitting explore effectiveness simple model selection procedure considered classical problem fitting polynomial set points 
specifically considered function learning problem ir ir goal minimize squared prediction error err gamma considered polynomial hypotheses ir ir natural stratification ae ae polynomials degree motivation studying task classical studied problem attracts lot interest vapnik cherkassky vapnik vapnik 
polynomials create difficult model selection problem strong tendency produce catastrophic overfitting effects 
benefit polynomials interesting nontrivial class efficient techniques computing best fit hypotheses 
apply metric strategy tri task need define suitable metric distance presumed distribution squared error measure define distance functions gammar gamma dp delta distance yjx yjx gammar gamma dp yjx dp delta establishes verifiable pseudo metric fp yjx 
notice take square roots get metric earlier need quotes 
training set hx hx define corresponding empirical distance estimate yjx gamma determine efficacy tri compared performance number standard model selection strategies including known penalization strategies generalized cross validation gcv craven wahba structural risk minimization srm vapnik formulations reported cherkassky vapnik fold cross validation cv standard hold method efron kohavi 
conducted simple series experiments fixing uniform domain distribution unit interval fixing various target functions ir 
generate training samples drew sequence values computed target function values added independent gaussian noise obtain labeled training sequence hx hx training sample computed series best fit polynomials degree sequence model selection strategy choose hypothesis basis observed empirical errors 
implement tri assumed access known uniform distribution order compute true distances polynomials sequence 
return issue estimating 
main emphasis experiments minimize true distance final hypothesis target conditional yjx primarily concerned choosing hypothesis ob minimum squared error polynomials degrees set training points 
notice high degree polynomial demonstrates erratic behavior training set 
tains small prediction error test examples independent complexity level 
determine effectiveness various selection strategies measured ratio true error polynomial selected best true error polynomials sequence 
rationale doing wish measure model selection strategy ability approximate best hypothesis sequence find better function outside sequence 
experiment tables show results obtained fitting step function step corrupted gaussian noise 
strategy tables explained 
obtained results repeatedly generating training samples fixed size recording approximation ratio achieved strategy 
tables record distribution ratios produced strategy training sample sizes respectively trials 
results quite dramatic 
tri achieved median approximation ratios training sample sizes respectively 
compares favorably median approximation ratios achieved srm achieved cv gcv dramatically criteria imagine optimizing 
example interested finding simple model underlying phenomenon gives insight fundamental nature simply producing function predicts test examples heckerman chickering 
focus traditional machine learning goal minimizing prediction error 
consider elaborate strategies choose hypotheses outside sequence averaging hypotheses opitz shavlik breiman 
pursue idea 
percentiles approximation ratios method tri cv srm gcv theta table fitting step oe 
distribution approximation ratios achieved training sample size showing percentiles approximation ratios achieved repeated trials 
percentiles approximation ratios method tri cv srm gcv theta theta table table examples 
percentiles approximation ratios method tri cv srm theta gcv theta table table examples 
fitting step oe training examples showing percentiles approximation ratios achieved trials 
percentiles approximation ratios method tri cv srm theta theta gcv theta theta theta table table 
percentiles approximation ratios method tri cv srm theta gcv theta table table sin 
percentiles approximation ratios method tri cv srm gcv theta table table sin 
worse trials 
striking difference tri robustness overfitting 
fact penalization strategy srm performed reasonably fairly prone making catastrophic overfitting errors 
normally wellbehaved cross validation strategy cv significant overfitting errors time time 
evidenced fact trials training sample size table tri produced maximum approximation ratio cv produced worst case approximation ratio penalization strategies srm gcv produced worst case ratios theta theta respectively 
th percentiles tri cv srm gcv theta fact tri robustness overfitting surprise prove tri produce approximation ratio greater assume tri best hypothesis sequence ii empirical error underestimate 
proof simple geometry appendix 
basic flavor results remains unchanged different noise levels different domain distributions fact stronger results obtained wider tailed domain distributions gaussian table difficult target functions sin table 
srm gcv forced regime constant catastrophe cv noticeably degrades tri retains performance levels shown table 
experiment course step function pathological target fit polynomial important consider natural targets better suited polynomial approximation 
fact repeating previous experiments benign target function sin obtain quite different results 
table shows procedure tri fare case obtaining median approximation ratios training sample sizes respectively compared srm cv 
close examination data reveals reason performance penalization strategies appear performing worse larger training sample sizes performance improves sample sizes greater 
gamma gamma gamma gamma 
yjx real estimated distances successive hypotheses target yjx solid lines indicate real distances dashed lines indicate empirical distance estimates 
drop tri systematically gets stuck lower degree polynomials 
fact simple geometric explanation degree polynomials give reasonable fits sin odd degree fits tail wrong direction 
creates huge distance successive polynomials causes triangles break odd degree fits large degree polynomial approximation 
metric tri strategy strongly robust overfitting prone systematic underfitting seemingly benign cases 
similar results obtained polynomial polynomial target functions 
problem leads consider reformulated procedure 
strategy adjusted distance estimates final idea explore observation fact dealing metrics true metric defined joint distribution xy empirical metric determined labeled training sequence 
metrics consider triangle formed hypotheses target conditional yjx 
notice distances involved real estimated true distances yjx care don 
key observation real estimated distances hypotheses give observable relationship local vicinity 
fact adopt naive assumption observed relationship holds yjx note case obtain better estimate yjx simply adjusting training set distance yjx observed ratio 
fact adopting simple heuristic leads surprisingly effective model selection procedure hypothesis sequence multiply note expect underestimate general expect ratio typically larger 
estimated distance yjx largest observed ratio choose function sequence smallest adjusted distance estimate yjx simple motivated procedure overcome underfitting problems associated tri retaining tri robustness overfitting 
demonstrate efficacy repeated previous experiments including new competitor 
results show robustly outperformed standard complexity penalization hold methods cases considered spanning wide variety target functions noise levels distributions space limitations preclude complete systematic exposition results tables demonstrate typical outcomes 
particular table shows avoids underfitting problems plague tri selects high order approximations supported data 
table shows extremely robust overfitting situations standard approaches catastrophic errors 
results reported anecdotal full suite experiments strongly suggest outperforms standard techniques wide variety polynomial regression problems 
best model selection strategy observed polynomial regression tasks 
estimating course argue results terribly useful metric strategies tri require knowledge true domain distribution clearly unreasonable assumption practice 
trivial observe obtain information unlabeled training instances 
fact important function learning applications large collections unlabeled training data available image speech text databases metric techniques apply wide range practical situations provided robust estimated distances 
explore issue repeated previous experiments gave tri small sample estimate inter hypothesis distances 
strategies fact extremely robust approximate distances 
table shows unlabeled examples just times number labeled examples sufficient tri perform nearly 
table shows techniques significantly break consider fewer unlabeled labeled training examples 
evidence anecdotal robustness observed wide range problems 
remains important direction research systematically characterize range sample sizes percentiles approximation ratios method tri tri tri table table small number unlabeled examples parentheses estimate holds 
note yields reasonably efficient model selection procedure computing distances involves making single pass list unlabeled examples 
strong advantage standard hold techniques cv repeatedly call hypothesis generating mechanism generate pseudo hypotheses extremely expensive operation applications 
introduced new approach classical model selection problem exploiting intrinsic geometry function learning task 
new techniques significantly outperform standard approaches wide range polynomial regression tasks 
primary source advantage metric strategies able detect dangerous situations avoid making catastrophic overfitting errors responsive adopt complex models supported data 
accomplish attending real distances hypotheses 
note strategies completely ignore information result heavily punished experiments 
hold methods implicitly take information account indirectly effectively metric strategies introduced 
free lunch general schaffer claim obtain universal improvement model selection problem schaffer claim able exploit additional information task knowledge obtain significant improvements wide range problem types conditions 
empirical results polynomial regression support view 
important direction research develop theoretical support strategies 
progress direction reported companion schuurmans ungar foster develops general characterization difficulty model selection problems standard bias variance decomposition expected hypothesis er ror geman bienenstock doursat 
characterize model selection problems shapes approximation error variance profiles delineate conditions traditional techniques prone catastrophic mistakes techniques obtain greatest advantage 
remains open tri best possible ways exploit hypothesis distances afforded plan investigate alternative strategies effective regard 
note approach specific polynomial curve fitting 
techniques developed easily applied hypothesis classes familiar ai research including neural networks radial basis functions decision trees 
fact metric approach easily generalizes classification learning tasks classification loss function err fy yg directly gives metric definitions yjx xy 
discussed schuurmans ungar foster expect achieve dramatic successes classification involves bounded loss permit catastrophic errors distances greater 
applying techniques classification tasks important direction research 
hope compare results earlier study kearns 
performed national research council canada 
rob holte joel martin peter turney help developing nascent ideas adam grove lyle ungar dean foster geoff hinton rob tibshirani insightful comments 
vladimir vapnik suggesting polynomial curve fitting suitable test problem providing useful 
ircs generous hand aravind joshi paid extra page 
appendix prove tri exhibit approximation ratio larger assume tri best hypothesis sequence ii empirical error underestimate 
consider hypothesis follows sequence assume yjx yjx 
show fail triangle test notice error triangle inequality imply yjx yjx yjx yjx 
recall yjx yjx assumption yjx yjx 
yjx yjx yjx contradicts 
tri consider breiman 
bagging predictors 
technical report statistics department uc berkeley 
cherkassky vapnik 
comparison vc method classical methods model selection 
preprint 
craven wahba 
smoothing noisy data spline functions 
numer 
math 

efron 
computers theory statistics 
siam review 
vapnik 
applications model selection techniques polynomial approximation 
preprint 
geman bienenstock doursat 
neural networks bias variance dilemma 
neural comp 

haussler 
decision theoretic generalizations pac model neural net learning applications 
infor 
comput 

heckerman chickering 
comparison scientific engineering criteria bayesian model selection 
technical report msr tr microsoft research 
kearns mansour ng ron 
experimental theoretical comparison model selection methods 
proceedings colt 
kohavi 
study cross validation bootstrap accuracy estimation model selection 
proceedings ijcai 
moody 
effective number parameters analysis generalization regularization nonlinear learning systems 
proceedings nips 
opitz shavlik 
generating accurate diverse members neural network ensemble 
proceedings nips 
pollard 
convergence stochastic processes 
new york springer verlag 
rissanen 
stochastic complexity modeling 
ann 
statist 

schaffer 
overfitting avoidance bias 
mach 
learn 

schaffer 
conservation law generalization performance 
proceedings ml 
schuurmans ungar foster 
characterizing generalization performance model selection strategies 
proceedings ml 
appear 
vapnik 
estimation dependences empirical data 
new york springer verlag 
vapnik 
nature statistical learning theory 
new york springer verlag 
weiss kulikowski 
computer systems learn 
san mateo morgan kaufmann 
