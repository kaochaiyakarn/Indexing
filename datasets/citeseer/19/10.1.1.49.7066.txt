kolmogorov entropy time series information theoretic functionals milan palus technique identification quantification chaotic dynamics experimental time series 
evaluation informationtheoretic functionals redundancies estimated data generated lowdimensional chaotic dynamical system specific properties reflecting positive information production rate 
rate measured metric kolmogorov sinai entropy directly estimated redundancies 
key words kolmogorov sinai metric entropy time series analysis dynamical systems ergodic theory information theory received december revised accepted may 
results theory nonlinear dynamical systems deterministic chaos applicable analysis experimental time series see brought new alternative generally linear stochastic methods significantly changed theoretical paradigms interpretation obtained results 
purely phenomenological parameters stochastic methods replaced new approach invariants characterizing dynamical properties systems study 
extraction invariants may regarded step building model system dynamics 
applications approach analysis experimental time series reported fields natural social sciences 
significant portion devoted classification chaotic behaviour oriented estimations geometric static invariants dimensions number degrees freedom 
algorithms proposed estimation dynamical entropies derived dimensional algorithms related concept enyi entropies 
kolmogorov introduced theoretical concept classification dynamical system information milan palus institute computer science academy sciences czech republic pod vod vez prague czech republic mail mp cas cz cr neural network world rates inspired information theory generalized notion entropy information source 
possibility ideas methods information theory field nonlinear dynamics applied analysis experimental data demonstrated shaw fraser 
considers method estimating metric kolmogorov sinai entropy experimental time series information theoretic functionals proposed fraser 
method represents interface ergodic theory dynamical systems information theory 
original fraser analyses information aspects chaotic dynamics detail concentrate attributes dynamical systems studied ergodic theory mixing generating partitions demonstrate reflected behaviour information theoretic functionals estimated chaotic data 
necessary elements ergodic theory introduced section 
books details proofs theorems 
basic concepts information theory section 
details refer books 
relation entropy dynamical systems information functionals explained section 
technique estimating metric entropy relation described section application known chaotic systems section 
section applications approach 
influence additive noise quantification identification chaotic dynamics proposed information theoretic method studied section 
section 
elements ergodic theory study long term average behaviour systems 
collection states system form space time evolution system represented transformation tx taken state time system time state case continuous time consider parameter family ft 
set real numbers maps 
suppose ft flow restrict cases measure space measure preserving transformation 
oe algebra measurable subsets countably additive non negative set function measure contains subsets measure 
form complete probability space 
measurable map gamma gamma holds 
system called measure preserving transformation abbreviated 
typical set ergodic theory 
reader familiar theory dynamical systems differentiable dynamics diffeomorphism differentiable manifold question existence invariant measure measure arises 
problem discussed simply suppose measure exists 
palus kolmogorov entropy time series 
properties important considerations definition 

called ergodic gamman theorem 

ergodic iff lim gamma gammai definition 
probability space 
weakly mixing lim gamma gammai gamma ii strongly mixing lim gamman clearly strongly mixing transformation weakly mixing weakly mixing transformation ergodic 
theorem 
measure space semi algebra generates ergodic iff lim gamma gammai ii weakly mixing iff lim gamma gammai gamma iii strongly mixing iff lim gamman neural network world theorem 
probability space set positive numbers 
weakly mixing iff pair elements subset ae density zero lim gamman intuitively say strongly mixing set sequence sets gamman asymptotically independent set ergodicity means gamman independent average pair sets considering theorem 
say weakly mixing sequence gamman asymptotically independent set provided neglect instants time 
theorem 
gives possibility apply considerations elements finite partitions definition 
partition disjoint collection elements union definition 
ff fa finite partition 
entropy partition ff ff gamma log interested finite partitions considerations extended countable partitions finite entropy 
definition 
ff fa fi fb bm partitions 
gamma ff partition ft gamma gamma 
ii fi refinement ff written fi ff set measure subset 
iii ff fi common refinement ff fi partition fa mg 
iv common refinement ff partitions ff ff ff ff ff ff conditional entropy ffjfi ff fi ffjfi gamma log omitting terms 
palus kolmogorov entropy time series 
definition 
ff finite partition 
entropy transformation respect partition ff ff lim gamma gammai ff definition 

entropy transformation sup ff ff supremum taken finite partitions 
entropy called metric entropy measure theoretic entropy kolmogorov sinai entropy 
theorem 
ff fa finite partition 
ff ff log theorem 
ff fi partitions ff fi ff fi 
theorem 
finite partition 
lim gammai theorem 
continuous flow equality holds 
definition 
finite partition ff called generating partition generator respect 
gamma gammai ff sets measure 
theorem 
kolmogorov sinai theorem ff generator respect ff 
important theorems existence generating partitions theorem 
krieger generator theorem ergodic lebesgue space finite generator 
neural network world 
elements information theory definition 
continuous random variables probability distribution densities px 
entropy distribution single variable say gamma px log px dx ii joint distribution px joint entropy gamma px log px dxdy iii conditional entropy jy jy gamma px log px dxdy iv entropy distribution discrete random variable values probability distribution gamma log definitions ii iii discrete variables derived straightforwardly 
definition 
mutual information quantifying average amount information variable bears variable gamma theorem 
iff px px iff statistically independent 
generalization definition joint entropy variables xn straightforward definition 
joint entropy distribution px xn variables xn xn gamma px xn log px xn dx dxn mutual information generalized ways palus kolmogorov entropy time series 
definition 
redundancy xn variables xn quantifying average amount common information contained variables xn defined xn xn gamma xn definition 
marginal redundancy xn gamma xn quantifying average amount information variable xn contained variables xn gamma defined xn gamma xn xn gamma xn gamma xn relations redundancies entropies obtained simple manipulation theorem 
xn gamma xn xn gamma xn gamma theorem 
xn gamma xn xn gamma xn jx xn gamma 
numerical set relation practical applications deals time series fy considered realization stochastic process fy stationary ergodic 
due ergodicity subsequent information theoretic functionals estimated time averages ensemble averages variables substituted gamma time lag 
due stationarity redundancies gamma gamma gamma functions number variables time lag independent measure preserving system correspond stochastic process fy orbit ft zg point represents single evolution system 
oe algebra thought family observable events invariant measure specifying time independent probabilities occurrence 
measurable function 
represents measurement system values physically observable variable measured successive instants time experimental time series 
conversely stationary stochastic process corresponds measure preserving system standard way construct map phi mapping variables neural network world stochastic process sequence points fq measure space define shift transformation oe sequence fq oeq due stationarity original process system 
details see ref 

identify entropies distributions stochastic variables partition entropies corresponding measure space especially conditional entropies theorems 

gamma parameter independent clearly dependent partition entropy continuous transformation respect partition corresponding probability distribution 
generating partition respect precisely respect large range see sec 
considering theorems lim gamma jh assertion originally conjectured fraser 
mutual information estimation metric entropy dimensional maps firstly shaw 
detailed study estimation technique bring numerical support conjecture 
exact equality relation partition entropy entropy related distribution provided defined measure space 
want estimate metric entropy dynamical system evolving dimensional state space sequence dimensional stochastic variables providing images projections single trajectory dynamical system see considerations mapped map 

estimation technique study behaviour function low dimensional chaotic processes 
consider studied time series generated dimensional dynamical system 
continuous time dynamical system fulfilling conditions existence uniqueness theorem particular trajectory mapped 
unique trajectory passing point evolution particular trajectory fully determined dimensional point hand theorem takens tuples successive samples gamma form mapping process fy space sequence fq images tuples gamma topologically equivalent original trajectory fs particular tuple gamma equivalent point determines rest series fy means redundancies finite redundancy diverge 
theoretic consideration providing infinite precision 
experimental numerical practice measurement noise finite palus kolmogorov entropy time series 
precision cause estimated redundancies finite increasing suppose increase lower independent intuitively explain supposition fact adding variable variables common information measured increased specific dynamical information increase gamma depends addition variable considering increase gamma common information approximately equivalent addition noise term contributing non specific information related noise finite precision 
considerations biased actual amount noise studied data see figs 

expect curves functions shape just shifted 
const 
limit behaviour case dimensional dynamical system attained small 
consider probability distribution xn estimation corresponds generating partition studied dimensional dynamical system certain range limit behaviour gamma attained 
behaviour low dimensional dynamical systems 
range marginal redundancies approach linearly decreasing function usually bounded equality gamma holds considering determines bounds need explain technique estimating redundancies 
redundancy algorithm practical computation mutual information redundancies continuous variables connected problem quantization 
quantization understand definition finite size boxes covering state space 
probability distribution estimated relative frequencies occurrence data samples particular boxes 
naive approach estimate redundancies continuous variables finest possible quantization computer memory measurement precision 
remember usually finite number data samples 
quantization fine estimation entropies redundancies heavily biased estimating joint entropy variables marginal bins obtains boxes covering state space 
approaches number data samples estimate xn equal log case determined number data samples number distinct data values structure data properties system study 
say case data 
see natural quantization experimental data converter usually fine reliable estimation redundancies 
emergence number boxes covering state space higher space dimension number variables lower neural network world number marginal quantization levels cause 
recalling def 

redundancy variables see estimate joint entropy saturated value number data samples number distinct data values estimates individual entropies increase quantization 
causes overestimation redundancy obscures dependence recalling gamma gamma see causes overestimation marginal redundancy attenuation decrease increasing lead paradoxical unreal result increasing formally implies negative metric entropy figs 

careful defining quantization 
fraser proposed algorithm constructing locally data dependent quantization details see 
computations simple box counting method marginal 
means marginal boxes defined approximately number data points marginal bin 
choice number bins crucial 
proposed computing variables number marginal bins exceed st root number data samples 
extensive numerical experimentation strongest rule necessary preventing special cases finer quantizations give unbiased results 
depends level measured metric entropy level mixing system study 
weaker mixing lower finer quantization 
cf 
results lorenz rossler systems see sec 
figs 

probably general rule exists determining ideal quantization arbitrary data 
propose compute redundancies numbers equi quantization levels recommended value defining quantization boxes construct partition experimental state space estimate probability distribution 
conjectured partition corresponds partition original state space generated data 
simplicity considerations distinguish partitions 
region linear decrease return problem bounds determining region gamma 
consider measure space ae coarse generators respectively 
clearly refinement generator generator 
chaotic mean 
intuitively say sequence gammai ae palus kolmogorov entropy time series 
changing refining slowly sequence gammai able generate gammai sets zero measure coarse partitions ae generator respect computation series generated attaining limit behaviour need finer partition quantization series generated 
hand stated greater tolerance 
example considerations lorenz rossler systems bits time unit bit time unit generating partition emerge marginal quantization bins see sec 
figs 

consider time continuous transformation dynamical system theorem 
equality holds 
analogy transformations generating partition need generating limit behaviour chosen partition start decreases refining partition 
approach right boundary recall theorem 
states ergodic sequence sets gammai asymptotically average independent set 
go ergodic transformation weak mixing generic property considering theorem 
say providing neglect instants time strong mixing generic property ergodic systems 
theorems sequence gammai independent set 
means large partition 
practice right boundary region linear decrease smaller lags vanishes 
recall generating partition 
generating generating bounded interval inequality holds 
case behaves respect partition bernoulli iid process vanish 
fact approaching partition looses generating property linear decrease stops decreases slowly stops numerical zero value 
clearly partition right neural network world bound region linear decrease extends 
phenomenon illustrated fig 

redundancies lorenz system data increasing number marginal quantization levels reading bottom top partition 
increasing straight line graph gives value metric entropy clearly entropy partition 
see linear region gamma extends partition slightly left apparently right ends 
effects emerge 
phenomenon interpreted terms uncertainty prediction states system metric entropy 
suppose know state system time precision partition know system state lies 
prediction uncertainty system state increases time 
maximum uncertainty reached entropy partition 
prediction uncertainty system state knowledge previous system states 
time time total loss system memory observed precision partition 

numerical results illustrates typical time lag dependence marginal redundancies computed time series generated component lorenz system appendix referred lorenz different quantization levels quantization insufficient limit behaviour appear quantization defines partition generating range sufficient reliable estimation metric entropy 
case linear region extended results biased due 
distorted 
depicts computed conditions fig 
data generated rossler system 
see chaotic system quantizations fine approach behaviour 
quantizations correct value metric entropy bit time unit estimated suffers esp 
illustrates dependence region linear decrease lorenz system number marginal quantization levels 
discussed sec 

see objective assessment region linear decrease particular quantization particular data simple straightforward 
strategy looking redundancies computed various numbers quantization levels define reasonable range range quantizations estimate metric palus kolmogorov entropy time series 
marginal redundancy time lag time lag marginal redundancy time lag time lag marginal redundancy time lag time lag marginal redundancy time lag time lag fig 
time lag plots marginal redundancies lorenz system lorenz see appendix computed different numbers marginal equi quantization levels 
different curves represent different numbers lagged series reading bottom top 
redundancies bits time lags units system time variable time units 
marginal redundancy time lag time lag marginal redundancy time lag time lag marginal redundancy time lag time lag marginal redundancy time lag time lag fig 
plots marginal redundancies fig 
rossler system 
neural network world marginal redundancy time lag time lag fig 
time lag plots marginal redundancy lorenz system computed various numbers marginal quantization levels reading bottom top 
increasing straight line represents dependence values metric entropy lorenz system sampled lag lag 
thick lines arrows indicate find lag metric entropy system sampled lag equal partition entropy particular partition quantization 
note region linear decrease finishes point 
entropy robust linear regression 
estimates typically underestimated lower numbers quantization levels increase increasing partition particular generating sufficient range estimates metric entropy approximately saturate correct value 
increasing occur leading decrease entropy estimates 
see discussion sec 
clearly insufficient amount data occur partition generating sufficient range studied system general partition estimates metric entropy increase maximum lower correct value system metric entropy emerging decrease 
examples types behaviour fig 

illustrates estimation metric entropy lorenz system 
estimates obtained data samples upper line particular values denoted asterisks saturate value bits time unit 
estimates decrease due 
estimates obtained data samples lower line particular values denoted crosses reach maximum value decrease due 
results obtained occurred earlier 
palus kolmogorov entropy time series 
presents estimation results fig 
rossler system 
recalling discussion sec 
considering chaotic systems compare numbers quantization levels coarse partitions generating respect lorenz rossler systems systems vulnerable lorenz data rossler data considering number data samples 
metric entropy estimate quantization quantization metric entropy estimate quantization quantization fig 
dependence estimate metric entropy lorenz system lorenz rossler system number marginal quantization levels 
number data samples estimation upper line asterisks estimated values lower line crosses estimated values 
note scales different 
metric entropy bits time unit 
known theorem pesin states metric entropy dynamical system equal sum positive lyapunov exponents assess validity method estimating metric entropy comparing values obtained samples values positive lyapunov exponent published systems examined 
see appendix definition systems 
results follow number estimate metric entropy standard deviation second number positive lyapunov exponent bits time unit related lorenz sigma lorenz sigma rossler sigma 
results support assertion 
large amount data necessary obtain reliable estimate metric entropy 
experimental practice data amounts usually recorded 
cases estimations redundancies useful important tasks neural network world ffl relative quantification studied system called coarse grained entropy rates suitable estimating metric entropy provide classification different systems system states equivalent classification metric entropy 
ffl qualitative characterization data systems study applications nonlinearity tests assessment possible existence low dimensional chaos searching region linear decrease see refs 
section 

qualitative characterization experimental time series analyzing experimental data quantification question qualitative character solved ask reasonable hypothesize data generated low dimensional chaotic dynamical system 
number papers type evidence existence chaos obtaining finite value correlation dimension published decade 
far know strict evidence chaos obtained experimental data possible 
typical properties chaotic systems reflected time series generated systems necessary sufficient conditions chaos 
speak evidence chaos signatures chaos detectable experimental data 
correlation type dimension represents geometrical signature chaos characterizes geometry system attractor dynamics 
extracting data geometric signature erroneously consider special type stochastic dynamics formally exhibiting finite correlation dimension chaotic 
prevent mistake dynamical signatures metric entropy lyapunov exponents applied 
done number extensive numerical studies possibility correctly detect chaotic dynamics time series 
redundancy method discussed cohen procaccia algorithm estimating metric entropy estimating lyapunov exponents applied algorithm higher order fitting jacobian matrices 
reported similar methods able estimate correct values metric entropy lyapunov exponents significantly smaller amounts data necessary redundancy method 
priori knowledge system underlying data methods give qualitatively incorrect results positive value metric entropy lyapunov exponent non chaotic data 
happen estimating redundancies limited amounts data 
qualitative character underlying system assessed simple look time lag traces marginal redundancies 
illustration results obtained data addition lorenz rossler systems 
torus periodic time series additive uniformly distributed noise figs 
respectively 
palus kolmogorov entropy time series 
marginal redundancy time lag time lag marginal redundancy time lag time lag fig 
marginal redundancy torus periodic time series torus series additive uniformly distributed noise 
time lag number samples redundancies bits 
note scales ordinates different 

time series generated double system periodic fig 
fig 
chaotic fig 
states details see appendix 

time series generated dynamics strange attractor fig 
introduced 
strange authors describe fractal geometry attractor means system zero negative lyapunov exponents zero metric entropy 

gaussian random processes spectra lorenz rossler systems investigated obtained forward fft fast fourier transform original data followed randomization phases backward fft time domain fig 

time lag plots marginal redundancies time series figs 

see non chaotic systems tendency linearly decreasing trend specific production information chaotic systems 
means estimate metric entropy clearly zero dynamical signature chaos 
holds random process spectrum rossler system fig 
representing highly correlated coloured noise 
case random process spectrum lorenz system fig 
redundancies decrease quickly zero exponential power law way 
hand chaotic state double system region linear decrease apparent metric entropy system clearly positive fig 

see redundancy analysis useful qualitative characterization experimental time series 
see 
neural network world marginal redundancy time lag time lag marginal redundancy time lag time lag fig 
marginal redundancy periodic state double system 
time lag system time variable redundancies bits 
marginal redundancy time lag time lag fig 
marginal redundancy gamma chaotic state double system 
time lag system time redundancies bits 
palus kolmogorov entropy time series 
marginal redundancy time lag time lag fig 
marginal redundancy gamma strange non chaotic attractor see appendix 
time lag number samples redundancies bits 
marginal redundancy time lag time lag marginal redundancy time lag time lag fig 
marginal redundancy gamma stochastic processes spectra lorenz system rossler system 
time lag system time redundancies bits 
neural network world 
influence noise influence additive noise chaotic time series estimation metric entropy character time lag dependence marginal redundancy general studied adding various portions uniformly distributed noise chaotic time series 
typical examples marginal redundancies noisy lorenz data fig 

general additive noise induces decrease magnitudes redundancies 
see fig 
note scales different 
noise induces decrease estimates metric entropy 
sight paradoxical result explained terms prediction uncertainty system states 
stated experimental numerical situation maximum uncertainty partition entropy reached production information evolution chaotic system 
noisy data basic uncertainty level higher noise free data redundancies lower 
causes decrease effective rate dynamical increase uncertainty 
holds moderate amount noise behaviour fig 
noise 
starting critical amount noise individual different chaotic systems typically chaotic behaviour detected 
marginal redundancy decreases exponential powerlaw way fig 
noise case ergodic stochastic process 
metric entropy estimated case noisy chaotic data recognized stochastic data 
redundancy method 
conclude production information typical property chaotic dynamical systems detected noisy data certain level data 
level noise chaotic properties system obscured noise system recognized stochastic process 
marginal redundancy time lag time lag marginal redundancy time lag time lag fig 
marginal redundancy lorenz time series contaminated additive uniformly distributed noise 
time lag system time redundancies bits 
note scales ordinates different 
palus kolmogorov entropy time series 

method analysis experimental time series suitable identification quantification underlying chaotic dynamics 
examination time lag dependence marginal redundancy informationtheoretic functional computed studied time series lagged versions 
theoretic approach consisting translation properties chaotic dynamical systems language ergodic theory information theory followed extensive numerical study supporting assertions derived theoretically 
demonstrated method able discern chaotic dynamics noisy quasi periodic stochastic processes provides quantitative characterization chaotic systems measuring information production rates terms metric sinai entropy 
technique entropy algorithms proposed related dimensional algorithm independent method extracting lyapunov exponents 
quantification macroscopic property information production rate mentioned methods related microscopic properties systems study heavily biased finite precision measurement noise 
believe redundancy analysis play important role analysis dynamical data 
author albrecht ak fraser stimulating discussions 
support agency czech republic gratefully acknowledged 
contains part author csc phd thesis entropy information properties chaotic systems eeg institute computer science academy sciences prague 
appendix time series related systems ordinary differential equations generated numerical integration method rossler system dx dt dy dt dz dt gammaz gamma gamma integration step accuracy lorenz system lorenz dx dt dy dt dz dt gamma gamma gamma xy gamma integration step accuracy lorenz neural network world dx dt dy dt dz dt gamma gamma gamma xy gamma integration step accuracy double system dx dt dy dt dz dt ffl sin gamma gamma gamma gamma gamma xy yz ffl periodic state ffl state ffl chaotic state integration step accuracy 
cases component recorded 
time series strange attractor obtained iterating system theta theta mod un cos theta vn sin theta gamma cos theta gamma vn cos theta gamma 
component theta recorded 
periodic noisy data generated formula sin 
oe sin 
oe random numbers uniformly distributed gamma xi xi 
term noise means xi 
torus series noise formula holds 
mayer kress ed dimensions entropies chaotic systems 
springer berlin 
abraham albano rapp eds measures complexity chaos 
plenum press new york 
grassberger procaccia phys 
rev 
cohen procaccia phys 
rev 
phys 
rev 
renyi trans 
nd prague conf 
information theory statistical decision function random processes 
academia prague 
kolmogorov dokl 
akad 
nauk sssr 
sinai ya dokl 
akad 
nauk sssr 
shaw 

shaw model chaotic system 
aerial santa cruz 
palus kolmogorov entropy time series 
fraser ieee trans 
information theory 
fraser 
fraser 
petersen ergodic theory 
cambridge university press cambridge 
walters ergodic theory 
springer berlin 
sinai ya 
ergodic theory 
princeton university press princeton 
sinai ya 
ergodic theory 
springer new york 
shannon weaver mathematical theory communication 
university illinois press urbana 
gallager information theory reliable communication 
wiley new york 
mathematical foundations information theory 
dover publications new york 
billingsley ergodic theory information 
wiley new york 
kullback information theory statistics 
wiley new york 
eckmann ruelle rev mod 
phys 

martin england mathematical theory entropy 
addison wesley london russian mir moscow 
und leipzig russian nauka moscow 
arnold ordinary differential equations 
mit press cambridge 
takens rand young eds dynamical systems turbulence warwick lecture notes mathematics 
springer berlin 
fraser phys 
rev 
palus albrecht ak information theoretic test nonlinearity time series 
phys 
lett 

press flannery teukolsky vetterling numerical recipes art scientific computing 
cambridge univ press cambridge mass 
pesin ya 
russian math 
surveys 
ya sinai ed dynamical systems 
collection papers 
world scientific singapore 
progr 
theor 
phys 

wolf swift physica 
grassberger procaccia phys 
rev lett 

grassberger procaccia physica 
osborne physica 
palus ak david ak holden eds mathematical approaches brain functioning diagnostics 
manchester university press manchester new york 
takens joseph eds nonlinear dynamics turbulence 
pittman 
briggs phys 
lett 

bryant brown phys 
rev lett 

parisi physica 
zeng phys 
rev lett 

schultz bull 
math 
biol 

ott pelikan physica 
rossler phys 
lett 

neural network world lorenz atmos 
sci 

palus identifying quantifying chaos information theoretic functionals 
weigend gershenfeld 
eds time series prediction forecasting understanding past 
santa fe institute studies sciences complexity proc 
xv addison wesley reading mass 
palus testing nonlinearity weather records 
phys lett 

palus testing nonlinearity redundancies quantitative qualitative aspects 
physica 
palus detecting nonlinearity multivariate time series 
phys 
lett 

palus coarse grained entropy rates characterization complex time series 
physica 
palus nonlinearity normal human eeg cycles temporal asymmetry nonstationarity randomness chaos 
biol 
cybern 

