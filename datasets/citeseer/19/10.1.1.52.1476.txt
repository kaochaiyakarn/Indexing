technical report jonathan oliver shortened appeared ai statistics decision graphs extension decision trees jonathan oliver cs monash edu au department computer science monash university clayton victoria australia examine decision graphs generalization decision trees 
inference scheme construct decision graphs minimum message length principle 
empirical tests demonstrate scheme compares favourably decision tree inference schemes 
provides metric comparing relative merit decision tree decision graph formalisms particular domain 
examine problem inferring decision procedure set examples 
examine decision graph generalization decision tree propose method construct decision graphs wallace minimum message length principle 
related rissanen minimum description length principle mdlp 
reader unfamiliar minimum encoding methods mml mdl area georgeff 
formalize problem inferring decision procedure set examples follows 
data represents set objects 
object described terms independent variables attributes 
object class dependent variable associated 
data consists vectors give values attributes class object 
object supervised learning find specified family functions function attributes best predicts class object 
consider data training set data determine function highest chance accurately predicting class new objects 
functions wish consider partition set objects categories nearly possible objects category class 
section overview decision trees examine decision graph 
argue decision functions simply described decision graphs decision trees 
section discuss minimum description length principle hill climbing inductive inference algorithm 
section adapt hill climbing algorithm design algorithm construct decision graphs 
section results comparing decision graph scheme decision tree inference schemes 
section describe decide decision tree decision graph appropriate particular domain 
overview decision trees decision tree proposition depicted 
consists decision nodes drawn ovals leaves drawn rectangles 
decision nodes specify attribute test object arcs decision node specifying possible values attribute take 
leaf decision tree specifies category 
decision tree leaves partitions space objects disjoint categories 
object categorized determining unique leaf associated object 
leaf associated object defined path tree arc decision node value object value attribute 
leaf decision tree labelled default class typically frequent class example objects associated leaf 
new object classified determining leaf associated classifying default class leaf 
attribute attribute attribute attribute attribute attribute class class class decision tree replication problem decision tree shown gives inefficient representation proposition term described efficiently term requires identical subtrees represented 
reason duplication disjunction proposition 
general conjunctions described efficiently decision trees disjunctions require large tree describe 
representational shortcoming decision trees identified pagallo haussler rendell termed replication problem 
pagallo haussler demonstrate boolean functions small dnf disjunctive normal form descriptions inefficiently described decision trees 
effect replication problem decision tree learning algorithms require large amount data learn disjunctive functions 
example decision tree learning algorithm requires data items learn subterm algorithm require theta data items learn function 
follows replications subtree describes solution replication problem allow decision nodes contain features function attributes 
feature consists conjunction literals literal attribute negated attribute 
pagallo haussler rendell construct decision trees analyse tree determine candidate features 
add features list attributes build tree 
groups repeat process features added 
need extend results cases data significant noise data classes 
attempts resolve replication problem include partitioning attribute values described section allowing expressive structure decision graphs described section 
fragmentation problem aspect replication problem occur data contains attributes values 
tree uses high arity attributes say arity quickly fragment data partitions see 
commonly method avoid fragmentation training data construct subsets attribute values see 
similar argument applies subterm appears higher tree 
training items leaves average training items 
decision tree demonstrating fragmentation problem noted searching possible way subsets computationally expensive operation 
attribute arity gamma gamma non trivial way subsets explore 
approach infeasible dealing high arity attributes 
example problem protein secondary structure prediction amino acid possible values gamma way subsets explore 
breiman linear time algorithm find locally optimal binary partition classes 
chou fast clustering algorithm finds locally optimal way subsets 
method offer method selection different sized partitions 
example offer metric choosing way way subsetting attribute values see 
way way way partitioning attribute values decision graph examine consider elegant solution replication fragmentation problems 
solution involves merging duplicated subtree join operator shown 
decision graphs depicted generalizations decision trees having decision nodes leaves 
feature distinguishes decision graphs decision trees decision graphs may contain joins 
join represented nodes having common child specifies subsets common properties considered subset 
manner objects categorized decision graphs decision trees 
decision tree decision graph define categorization partitioning object space disjoint categories 
set decision functions representable graphs exactly set representable trees 
set categorizations enter definition decision function different 
example categorizations different decision tree partitions object space categories decision graph partitions object space categories 
attribute attribute attribute class class attribute decision graph decision graphs previously examined literature chou bahl mahoney mooney 
mahoney mooney method construct decision graphs 
grow decision tree 

search decision tree related subtrees 

merge related subtrees build decision graph 
reported limited success doing concluded searching similar subtrees need collapsed shared structure combinatorially explosive 
class class test test test test decision chou bahl approached construction decision graphs different method mahoney mooney 
groups determined gross structure decision graph priori 
example gross structure decision bahl termed structures decision determined priori look shown 
groups applied partitioning algorithms discussed section split took structure graph constructed outside predetermined structure 
fixing structure decision graph priori considerable disadvantage shape decision graph provides insight domain consideration 
decision graphs section highlight disadvantage 
contribution applies technique mml construction decision graphs 
method described avoids problems encountered mahoney mooney search decision trees related subtrees 
furthermore method obviates need arbitrarily gross structure decision graph graph grown gross structure determined data 
minimum message length principle stated intention algorithm grow decision graphs minimum message length principle 
minimum message length criterion comparing inductively derived theories explanations theories provide 
shall going theory minimum message length principle give overview design algorithms inductive learning 
prescribe general algorithm inductive inference 
provides metric allows search model space acceptable theories 
section hill climbing algorithm iteratively modifies extends theory 
section apply hill climbing algorithm problem inferring decision graphs algorithm grows decision graphs 
induction principle output program performs inductive inference output decision tree probabilistic finite state automaton class structure estimator parameter regarded theory input data 
identify subset input data explained theory subset input data dependent learning paradigm wish 
example performing supervised learning theory explain class example performing classification theory explain attribute values example inferring probabilistic finite state automata theory explain sequence symbols input data 
explanation explanation null explanation smallest explanation theory data data theory theory data data explanation explanation null explanation smallest explanation theory data data theory theory data data range explanations define explanation subset message 
message encoded binary string consists parts shown 
part states theory second part states full code gives minimum expected message length true 
standard coding theory length second part message gammalog rob assume prior probability distribution set theories encode part message length gammalog rob may obvious probability distribution assign complex theories decision trees 
cases design code express theory binary string 
non redundant code fact implies prior probability distribution theories 
total length explanation gammalog rob gamma log rob null explanation shown message describing data null theory 
theory unacceptable shown explanation data longer null explanation 
shortest explanation shown considered best explanation 
data totally random sense chaitin best theory null theory 
comparing theories theories data may calculate message length explanations provided may say theory explains data better theory 
necessary encode theory data need calculate length explanation 
finite number possible theories find best theory best done calculating length explanation provided theory theory shortest explanation best considered best theory 
normally feasible consider possible theory 
circumstances classification infinite number theories 
circumstances decision tree inference finite number theories theories explore reasonable amount time 
hill climbing algorithm overcome problem may help search suitable theory 
allows compare theories say superior may theory iteratively propose improvements theory improvements shown 
hill climbing inductive inference null theory modify newt nil return calculate message length message length newt min index minimum fl ln message length lmin return goto step mml hill climbing algorithm algorithm requires define procedures ffl modify constructs list modifications ffl message length returns length message explain message length procedure assume existence message length procedures theories consideration decision trees decision graphs 
procedures encoding decision trees described wallace patrick quinlan rivest 
procedure encoding decision graphs described oliver wallace 
lookahead hill climbing algorithm proposed considers step modifications 
algorithm may get stuck local minimum find global minimum 
overcome modify procedure may designed additional lookahead parameter specifies number base level modifications willing consider stage 
constructing decision graphs section algorithm inferring decision trees proposed quinlan rivest 
examine algorithm inappropriate construction decision graphs 
section examine differences inference decision trees inference decision graphs 
go section adapt hill climbing algorithm design decision graph inference algorithm 
overview decision tree generation algorithm quinlan rivest proposed algorithm generation decision trees consists phases growing phase pruning phase 
growing phase begins root decision tree leaf training set associated 
extends decision tree iteratively performing procedure 
leaf possible replace decision node 

attribute specified decision node replace compute communication cost change 

replace decision node minimum communication cost 
pruning phase consists repeatedly replacing decision nodes children leaves leaves improves total communication cost 
requirements decision graph inference scheme considerations approach quinlan rivest unsuitable construction decision graphs 
ffl firstly leaf necessary decide replace decision node introduce join ffl secondly attempt provide order decision tree grown order grow decision tree immaterial 
order leaves expanded growing decision graph affects resultant graph 
propose algorithm develop decision graphs addresses considerations 
uses determine leaf split leaf introduce join leaf choose leaf expanded split join 
decision graph generation algorithm root graph leaf 
extend graph iteratively performing procedure grow graph graph perfect grown 
grow graph 
leaf determine attribute split 
record perform alteration split saving message length 

pair leaves perform tentative join 
record perform alteration join saving message length 

choose alteration step split step join greatest saving 
alteration creates savings message length perform alteration graph 
note procedure grow graph operations performing alterations increase message length 
stipulate process terminate 
grow graph performed operation split node iteration grow graph join back joining back save message length 
test results decision wallace quinlan data set graph patrick rivest hypo discordant led endgame xd table error rates variety methods standard data sets order test feasibility minimum message length principle generate decision graphs implemented compared results results produced decision tree inference schemes 
compared pessimistic pruning tree generation schemes minimum message length principle quinlan rivest wallace patrick 
results data sets hypo discordant led endgame xd table 
data sets described quinlan 
learning disjunctive concepts tested algorithm xd data set join operator extensively 
fact xd data set artificial set attributes generated division categories boolean function attributes decision graph xd data set give categories purity 
smallest possible decision tree function decision nodes leaves 
function hard decision tree inference schemes learn data partitioned subsets 
graph inference scheme re constructed function shown error rate decision graph noise level 
number decision leaves error data items nodes rate table varying size xd data set decision tree number decision leaves error data items join nodes rate table varying size xd data set decision graph table table give results varied number items training set xd data set 
decision graph inference scheme re constructed original function data items wallace patrick decision tree inference scheme required data items reconstruct function 
furthermore decision graph produced decision procedure easier identify human perspective 
similar success st monk data set reported 
data set artificial data set constructed function jacket color red head shape body shape id error rate decision graph inference scheme reconstructed original function shown error rate 
true red lo head shape body shape body shape body shape round square round square true fa lse decision graph st monk data set protein data set tested decision graphs data sets generated protein structure database 
data consisted amino acid chains secondary structure specified point 
determine amino acid chain protein finding secondary structure extended helix related shape protein quite difficult 
constructed decision graphs predicted secondary structure point protein window centered point interest amino acid chain attributes 
shows abbreviated form decision graph generated window amino acid chain attributes 
example arity attributes decision tree inference schemes quickly fragment data small sets learning difficult 
example complete height tree partition data subsets 
leaf height tree little data associated tree unintelligible biologists 
decision graph displays splits subsets amino acids values formed 
example root graph splits data subsets subsets amino acids values exhaustive search successive joins 
split attribute central amino acid window graph leaves 
grow graph procedure explored possibility leaf joining leaf case leaves gives cases explore 
general leaves grow graph procedure explore theta gamma possible joins 
attr attr attr attr 
abbreviated decision graph protein structure data set determining type model reader note decision graph scheme gave results identical decision tree scheme proposed wallace patrick 
cases decision graph scheme mml metric determine decision tree appropriate data set decision graph 
section describe determination 
message length function described oliver wallace parameter probability join describes disjunctive decision graph effects message length decision graph 
high example joins cost small number bits bits binary join 
low example join relatively expensive bits binary join cost join infinite 
procedure grow graph grow tree grow graphs joins increased 
message length bits bits bits bits bits bits table st monks data set null explanation bits tables vary parameter data sets 
select value minimizes message length 
st monks data set disjunctive nature decision trees inappropriate data set 
decision tree excellent explanation hypothyroid message length bits bits bits bits bits bits table hypothyroid data set null explanation bits data set saving bits better decision graph explanation 
gives measure compare theories distinct structures allows determine theory superior 
scope decision graphs decision trees minimum encoding framework ffl allows compare modifications totally different nature 
example decision graph postulate modifications splits node joins nodes sensibly compare modifications say modification superior 
ffl provides theoretical guarantee 
barron cover show data drawn population probability distribution classes exactly represented decision tree decision graph function sufficient data mml tree graph inference algorithm guaranteed recover function 
representing theory directed acyclic graph tree resulted expressive language theories 
additional expressiveness results search larger search space benefits significant 
ffl firstly graph formalism describes large set disjunctive functions simply tree formalism 
decision graph algorithm general infer functions set fewer data required decision tree algorithm 
ffl secondly graph formalism attributes values fragmenting data small partitions 
resultant graphs form subsets attribute values interest domain experts 
ffl thirdly applicability decision graphs particular domain determined 
done comparing message lengths best decision graph best decision tree domain 
acknowledgments supervisor ingrid zukerman valuable advice chris wallace david dowe wray buntine valuable discussions phil chou critical reading manuscript 
bahl brown desouza mercer 
tree statistical language model natural language speech recognition 
ieee transactions acoustics speech signal processing 
barron cover 
minimum complexity density estimation 
ieee transactions information theory 
breiman friedman olshen stone 
classification regression trees 
wadsworth belmont 
chaitin 
length programs computing finite sequences 

chou 
applications information theory pattern recognition design decision trees 
phd thesis department electrical engineering stanford university june 
chou 
optimal partitioning classification regression trees 
ieee transactions pattern analysis machine intelligence 
chou 
personal communication 

dowe oliver allison wallace dix 
decision graph explanation protein secondary structure prediction 
proceedings hawaii international conference system sciences hicss biotechnology computing track pages 
fayyad irani 
attribute selection problem decision tree generation 
proceedings aaai pages 
georgeff wallace 
general criterion inductive inference 
proceedings th european conference artificial intelligence 
kononenko bratko 
experiments automatic learning medical diagnostic rules 
technical report jozef stefan institute ljubljana yugoslavia 
mahoney mooney 
initializing id domain theory negative results 
department computer science university texas austin taylor hall austin texas usa march 
rendell 
constructive induction decision trees 
ijcai pages 
oliver 
decision graphs extension decision trees 
proceedings fourth international workshop artificial intelligence statistics pages 
extensive version available tr computer science department monash university vic australia 
oliver dowe wallace 
inferring decision graphs minimum message length principle 
adams sterling editors proceedings th australian joint conference artificial intelligence pages 
world scientific singapore 
oliver wallace 
inferring decision graphs 
proceedings workshop evaluating changing representation machine learning ijcai 
available tr computer science department monash university vic australia 
pagallo haussler 
boolean feature discovery empirical learning 
machine learning 
quinlan 
induction decision trees 
machine learning 
quinlan 
simplifying decision trees 
international journal man machine studies pages 
quinlan rivest 
inferring decision trees minimum description length principle 
information computation 
rissanen 
universal prior integers estimation minimum description length 
annals statistics 
rissanen 
stochastic complexity statistical inquiry 
world scientific singapore 
thrun monk problems performance comparison different learning algorithms 
cmu cs carnegie mellon university december 
wallace boulton 
information measure classification 
computer journal 
wallace patrick 
coding decision trees 
appear machine learning 
available tr computer science department monash university vic australia 
