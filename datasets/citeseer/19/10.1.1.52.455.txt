combining nearest neighbor classifiers multiple feature subsets stephen bay department information computer science university california irvine irvine ca usa ics uci edu combining multiple classifiers effective technique improving accuracy 
general combining algorithms bagging error correcting output coding significantly improve classifiers decision trees rule learners neural networks 
unfortunately combining methods improve nearest neighbor classifier 
mfs combining algorithm designed improve accuracy nearest neighbor nn classifier 
mfs combines multiple nn classifiers random subset features 
experimental results encouraging datasets uci repository mfs significantly improved nn nearest neighbor knn nn classifiers forward backward selection features 
mfs robust corruption irrelevant features compared knn classifier 
show mfs able reduce bias variance components error 
nearest neighbor nn classifier oldest simplest methods performing general nonparametric classification 
represented rule classify unknown pattern choose class nearest example training set measured distance metric 
common extension research performed university waterloo department systems design engineering waterloo ont canada 
choose common class nearest neighbors knn 
despite simplicity nn classifier advantages methods 
example learn small set examples incrementally add new information runtime gives competitive performance modern methods decision trees neural networks 
inception fix researchers investigated methods improving nn classifier concentrated changing distance metric manipulating patterns training set dasarathy 
researchers begun experimenting general algorithms improving classification accuracy combining multiple versions single classifier known multiple model ensemble approach 
outputs classifiers combined hope accuracy greater parts 
unfortunately combining methods improve nn classifier 
example breiman experiments bagging difference accuracy bagged nn classifier single model approach 
results suggest combining methods involve significant degree resampling replication patterns nn classifier 
kong dietterich concluded error correcting output coding ecoc method combining classifiers decomposing multi class problems multiple class problems improve classifiers local information high error correlation 
example nn classifier predict class closest pattern 
pattern class problems gives incorrect prediction predictions ecoc ensemble correct new method combining nearest neighbor classifiers goal improving classification accuracy 
approach manipulates features individual classifiers 
contrast combining algorithms may manipulate training patterns bagging boosting class labels ecoc 
section describe mfs algorithm combining multiple nn classifiers 
section evaluate algorithm datasets uci repository accuracy computational complexity robustness irrelevant features 
section analyze algorithm bias variance components error 
section discuss related follow section 
classification multiple feature subsets start describing mfs algorithm discuss motivation dangers 
explain set algorithm parameters 
mfs algorithm algorithm nearest neighbor classification multiple feature subsets mfs simple stated simple voting combine outputs multiple nn classifiers having access random subset features 
select random subset features sampling original set 
different sampling functions sampling replacement sampling replacement 
sampling replacement feature selected equivalent increasing weight 
nn classifiers uses number features 
parameter algorithm set cross validation performance estimates tuning dataset see section 
time pattern ricci aha developed method combining nn classifiers ecoc solves correlation problem 
discuss section 
classification select new random subset features classifier 
example mfs classification consider fisher iris plant classification problem fisher duda hart 
domain try classify iris plants specific species iris setosa iris features petal length petal width length width 
mfs nn classifiers random subset features 
nn classifier length width second width petal length third width width treat width theta 
idea random subset features may counter intuitive throwing away potentially valuable information 
accuracy nn classifiers decrease compared classifier access features 
information classifier accurate possible 
create set classifiers accurate single trained information 
answer questions lies dynamics simple voting set classifiers 
individual models need accurate system achieve high accuracy models different errors 
particular hansen salamon showed simple voting models independent errors error decrease monotonically increasing numbers classifiers 
ali pazzani verified empirically combining models uncorrelated errors significantly reduce error 
selecting different features attempt force nn classifiers different uncorrelated errors 
trading accuracy error diversity 
guarantee different feature sets nn classifiers decorrelate error 
tumer ghosh neural networks selectively removing features decorrelate errors 
unfortunately error rates individual classifiers increased result little improvement ensemble 
successful able combine neural networks different hand selected features achieve human expert level performance identifying volcanoes images 
method generating diverse ensemble classifiers perturb aspect training inputs classifier unstable 
example bagging breiman perturbs training patterns available classifier ensemble :10.1.1.30.8572:10.1.1.32.9399
decision trees unstable patterns bagging generates diverse effective ensemble 
nearest neighbor classifiers stable patterns bagging generates poor nn ensembles 
nearest neighbor classifiers extremely sensitive features 
example langley iba adding just irrelevant features drastically change nn classifier outputs reduce accuracy mfs attempts instability generate diverse set nn classifiers uncorrelated errors 
discussion hopefully provides motivation expect mfs improve accuracy nearest neighbor classifier 
major dangers aware mfs 
simple voting improve accuracy classifiers select correct class class 
breiman refers order correctness 
classifiers order correct simple voting increase expected error 
class problems require slightly accuracy voting classifiers improve accuracy 
multiple classes required accuracy may drop low number classes 

bayes error rate increase subset features 
may difficult nn classifiers mfs meet requirements point 
example parity problem domain highly interacting features bayes error rate proper subset features opposed full feature space 
guarantee random subsets necessary information accurate classification 

nearest neighbor classifier mfs scheme lose asymptotic optimality properties 
specifically number training examples approaches infinity nn classifier bounded twice bayes error rate cover 
knn classifier bayes optimal limit proper choice fix hodges 
claims mfs 
parameter selection mfs algorithm parameter values need set size feature subsets number classifiers combine 
set mfs subset size parameter crossvalidation accuracy estimates training set entire ensemble 
evaluated evenly spaced intervals size original feature set 
example domain features subset sizes 
evaluated 
case ties smaller value chosen 
set number classifiers evaluating performance mfs development datasets varying number classifiers 
results set number classifiers reasonable trade computational expense accuracy 
experiments methods evaluated performance mfs different sampling functions sampling replacement mfs sampling replacement mfs 
compared algorithms nearest neighbor nn nearest neighbor knn nearest neighbor forward fss backward bss sequential selection features aha bankert 
fss bss provide interesting contrast mfs 
fss bss try find single subset features mfs uses multiple random subsets regard performance 
classifiers unweighted euclidean distance continuous features hamming distance symbolic features 
missing values treated informative considered specific symbolic value 
case continuous features normalized missing value considered distance non missing values 
knn classifier value set cross validation performance estimates training set 
feature selection cross validation accuracy training set objective function known wrapper approach kohavi john 
evaluated algorithms datasets uci repository machine learning databases merz murphy 
normalized datasets continuous features ranged ran trials training set contained patterns randomly selected test set contained remaining 
exceptions procedure 
waveform training cases test cases maintain consistency reported results quinlan 
satimage original division training test set results represent run algorithm 
musk dataset features fss bss took long run hours single trial results obtained 
accuracy accuracy parameter selection results average number features selected shown table 
datasets development mfs algorithm 
default accuracy frequency common class 
results show mfs promising mfs mfs accurate domains nearest competitor knn 
mfs best domains including mfs 
mfs best domains tied including mfs 
formal comparison wilcoxon signed rank test mfs mfs significantly better confidence level greater 
mfs performed poorly datasets iris tic tac toe 
iris mfs mfs gave lowest accuracy classifiers 
possibly explained small number features iris dataset 
features feature subsets identical 
lead identical errors high error correlation 
toe mfs performed extremely poorly having error rate times nn knn classifiers 
mfs probably performed poorly tic tac toe domain features high amount interaction 
need examine features determine side won 
random subset features sense probably lead greatly increased bayes error rate individual classifiers 
mfs experience degradation mfs sampling replacement degenerated selecting features performing identically nn 
comparing mfs mfs clear classifier performed better 
mfs better mfs domains worse tied 
mfs slightly better average accuracy catastrophic failure tic tac toe 
wilcoxon test detect significant difference 
computational complexity nearest neighbor classifier criticized slow runtime performance briefly comment complexity mfs actual running times experiments 
nn classifier computes distance test pattern pattern training set 
requires ef time number examples number features 
mfs nn classifiers complexity 
training cross validation mfs requires ne fv time number folds bay 
analysis shows computational requirements mfs change function number examples features 
give indication actual running times real datasets 
table list actual running times intel pentium pro processor nn mfs slowest datasets 
table time requirements nn mfs classification training domain nn mfs mfs satimage pat pat segment pat pat annealing pat pat note combining classifiers mfs times slow nn classifier 
attribute speed caching difference feature values test pattern patterns training set gamma cache gamma 
robustness irrelevant features major drawback nn classifier sensitivity irrelevant features 
concerns mfs algorithm uses multiple nn classifiers raises question ensemble behave 
accuracy individual nn classifiers drops low simple voting increase error rate 
table accuracy parameter selection results average number features selected accuracy average parameter settings domain pat def 
nn knn fss bss mfs mfs knn fss bss mfs mfs glass hepatitis ionosphere iris liver disorders pima diabetes sonar annealing automobile breast cancer credit german horse colic labor lymphography musk na na na na primary tumor satimage segment soybean large tic tac toe vehicle vote waveform wine average unsure ensemble behave experimentally investigated robustness mfs irrelevant features 
basic procedure section 
added boolean irrelevant features datasets measured accuracy knn mfs 
chose boolean irrelevant features difficult nearest neighbor methods handle continuous irrelevant features 
range mean boolean variables greater variance 
table shows results domains 
remaining results bay shown space reasons follow similar pattern 
expected irrelevant features hurt knn mfs degree 
results surprising reveal domains knn critically sensitive mfs stable 
example vehicle wine added irrelevant features knn drops accuracy mfs drops 
general mfs minor degradations accuracy occasionally robust 
example mfs accuracy ionosphere degrades little better dataset corrupted irrelevant features classifiers original dataset 
possible explanation mfs performance lies random voters affect margins victory simple voting 
simplicity divide voters types informed relevant features uninformed random voters 
informed voters cast ballots winner margin votes compared closest competitor 
uninformed random voters cast ballots 
random voters vote equal probability equal expectation competitors multinomial distribution 
order random voting change outcome number random votes class meet inequality gamma margin 
margins informed voters small occur 
numerical example consider class problem informed voters random voters 
informed voters cast ballots outcome votes class votes class uninformed voters cast ballots 
order uninformed voters change outcome vote class wins vote class probability decision change approximately 
situation analogous occurs mfs applied domains irrelevant features 
nn classifiers voters uninformed random conditions met randomly selected features irrelevant occurrence classes training set roughly equal true uci datasets 
note condition met nn classifier random choose classes roughly proportion frequencies training set 
bias variance analysis error expected error algorithm divided components bias consistent error algorithm different runs variance error fluctuates run run 
decomposition useful method explaining changes algorithm affect final error rates 
allows decompose error meaningful components see error components change variations algorithm 
researchers bias variance analysis error show multiple model approaches 
example breiman schapire 
showed bagging improves performance reducing variance component error 
kong dietterich showed ecoc reduce bias variance 
bias variance decomposition error originated squared error regression 
classification loss misclassification rate commonly straightforward unique decomposition 
authors proposed similar decompositions kong dietterich breiman james hastie tibshirani kohavi wolpert :10.1.1.30.8572:10.1.1.32.9399
kong dietterich definitions 
define bias error ideal voted hypothesis result get combining infinite number classifiers trained independent set examples 
variance difference expected error rate ideal voted hypothesis error rate 
formally algorithm training set size unknown test point class ideal voted hypothesis algorithm error expected error algorithm training sets size bias variance bias ae variance error gamma bias note bayes error incorporated bias error 
variance negative 
occurs algorithm usually wrong lucky guess predicts correct class 
investigated bias variance components error datasets originally breiman schapire evaluate multiple model approaches 
datasets class problems individual classes composed dimensional gaussians 
compared classifiers nn knn mfs classifier mfs mfs classifiers 
nn classifier control compare knn mfs algorithms 
mfs allow determine changes error components caused random feature selection changes caused voting multiple classifiers 
test set instances independent training sets size estimate bias variance error classifiers 
approximated voting classifiers trained independent training sets 
results shown table 
twonorm selecting single random subset features mfs nn classifier causes variance error significantly increase 
voting mfs variance error reduced smaller value variance original nn classifier reducing error significantly 
ringnorm feature selection process dramatic trade bias variance 
bias error drops variance increases table accuracy knn mfs corruption irrelevant features knn mfs domain breast cancer german ionosphere soybean large vehicle vote wine table bias variance decomposition error domain opt 
nn mfs mfs knn twonorm bias variance error bias variance error ringnorm bias variance error 
voting drops variance greatly improving accuracy 
datasets see mfs modes operation decreasing variance voting trading bias variance random feature selection 
taken mfs able reduce bias variance components error 
comparison mfs knn classifier reduced variance 
twonorm error nn dominated variance bias error nearly optimal mfs knn able decrease error reducing variance 
fact knn better job mfs variance reduction 
ringnorm error nn classifier dominated bias knn able improve performance 
value bias greater equal bayes error rate estimation error finite sample sizes possible obtain bias estimates lower optimal bound 
related large body research multiple model methods classification little specifically deals combining nn classifiers 
aware skalak combining nn classifiers small prototype sets alpaydin condensed nearest neighbor cnn classifiers hart ricci aha combining nn feature selection ecoc 
skalak alpaydin approach problem combining nn classifiers similarly 
drastically reduce size classifier prototype set destabilize nn classifier 
skalak investigates different strategies finding reduced prototype set pursues approach called radical nn classifier just single prototype class 
able improve accuracy baseline nn classifier uci domains 
interestingly mfs glass lymphography average increase compared nn classifier domains skalak reported combining algorithm improved performance 
alpaydin uses dataset partitioning bootstrap disjoint combination cnn classifier edit reduce prototypes 
reported improvements nn classifier training sets sufficiently small able generate diverse classifiers 
ricci aha applied ecoc nn classifier nn ecoc 
normally applying ecoc nn errors class problems highly correlated applying feature selection class problems decorrelated errors different features selected 
method able improve performance domains tested noted ecoc accuracy gains tended increase diversity features selected class problems 
nn ecoc similar mfs nn classifiers different features 
differ nn ecoc uses active selection features output coding mfs uses random selection 
head head comparison useful determine nn ecoc mfs achieve accuracy gains areas feature space 
ricci aha analyzed nn ecoc bias variance concluded nn ecoc reduces bias slightly increases variance 
unfortunately different definition bias variance results directly comparable 
regardless method better accuracy mfs appears main advantages nn ecoc mfs simpler algorithm mfs constrained ecoc multiclass problems 
introduced mfs new algorithm combining multiple nn classifiers 
mfs nn classifier access patterns original training set random subset features 
experiments showed mfs effective improving accuracy 
accuracy improvements mfs significant advance allows incorporate desirable properties nn classifier multiple model framework 
example primary advantages nn classifier ability incrementally add new data remove old data requiring retraining 
mfs maintains property new data added old data removed runtime 
useful property nn classifier ability predict directly training data intermediate structures 
result matter classifiers combine mfs require memory single nn classifier 
combined nn classifiers share common dataset features selected randomly runtime 
mfs disadvantages 
particular mfs loses asymptotic optimality properties nn knn classifiers 
additionally domains highly interacting features tic tac toe error rate increase feature subsets resulting poor ensemble performance 
multiple model approaches lose comprehensibility compared single model 
individual judge potential accuracy increases worth disadvantages 
mfs attempt random feature selection generate effective nn ensembles successful improving accuracy unanswered questions open areas 
mfs 
initial attempt answering question analysis irrelevant features bias variance decomposition error 
clearly needs done characterize domains mfs 

application classifiers 
showed random feature selection useful generating ensembles nn classifiers 
apply technique learning algorithms 

implications feature selection feature weighting 
experimental results showed combining multiple random feature subsets significantly improve performance single best subset features fss bss 
implies searching single best set features searching multiple feature sets 

improvements 
kept design mfs simple possible number obvious improvements may help accuracy speed 
particular investigate different weighting schemes varying number features classifier uses ensemble combining sophisticated versions nn classifier editing prototypes 
michael pazzani support encouragement 
cathy blake yang wang anonymous reviewers providing comments improved 
partially supported nserc pgs scholarship 
aha bankert 

feature selection case classification cloud types empirical comparison 
proceedings aaai workshop case reasoning pages 
ali pazzani 

error reduction learning multiple descriptions 
machine learning 
alpaydin 

voting multiple condensed nearest neighbors 
artificial intelligence review 
bay 

nearest neighbour classification multiple data representations 
master thesis university waterloo department systems design engineering 
breiman 

bagging predictors 
machine learning 
breiman 

bias variance arcing classifiers 
technical report statistics department university california berkeley 


human expert level performance scientific image analysis task system combined artificial neural networks 
chan editor working notes aaai workshop integrating multiple learned models pages 
available www cs fit edu 
cover hart 

nearest neighbor pattern classification 
ieee transactions information theory 
dasarathy 

nearest neighbor nn norms nn pattern classification techniques 
ieee computer society press los alamitos ca 
duda hart 

pattern classification scene analysis 
john wiley new york 
fisher 

multiple measurements taxonomic problems 
annual 
fix hodges 

discriminatory analysis nonparametric discrimination consistency properties 
technical report project report number usaf school aviation medicine field texas 
hansen salamon 

neural network ensembles 
ieee transactions pattern analysis machine intelligence 
hart 

condensed nearest neighbor rule 
ieee transactions information theory 
james hastie 

generalizations bias variance decomposition prediction error 
stat stanford edu gareth 
kohavi john 

wrappers feature subset selection 
artificial intelligence 
kohavi wolpert 

bias plus variance decomposition zero loss functions 
machine learning proceedings thirteenth international conference 
kong dietterich 

errorcorrecting output coding corrects bias variance 
proceedings twelfth national conference artificial intelligence pages 
langley iba 

average case analysis nearest neighbor algorithm 
proceedings thirteenth international joint conference artificial intelligence pages 
merz murphy 

uci repository machine learning databases 
university california irvine dept information computer science 
www ics uci edu mlearn 
quinlan 

bagging boosting 
proceedings thirteenth national conference artificial intelligence pages 
ricci aha 

error correcting output codes local learners 
proceedings th european conference machine learning 
schapire freund bartlett lee 

boosting margin new explanation effectiveness voting methods 
machine learning proceedings fourteenth international conference 
skalak 

prototype selection composite nearest neighbor classifiers 
phd thesis department computer science university massachusetts 
tibshirani 

bias variance prediction error classification rules 
technical report department statistics university toronto 
tumer ghosh 

error correlation error reduction ensemble classifiers 
connection science 
special issue combining artificial neural networks ensemble approaches 
