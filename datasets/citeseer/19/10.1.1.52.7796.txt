statistical ideas selecting network architectures ripley professor applied statistics university oxford oxford uk choosing architecture neural network important problems making neural networks practically useful accounts applications usually sweep details carpet 
hidden units needed 
weight decay 
type output units chosen 

address issues framework statistical theory model choice provides number workable approximate answers 
principally concerned architecture selection issues feed forward neural networks known multi layer perceptrons 
issues arise selecting radial basis function networks recurrent networks widely 
problems occur wider context statistics applied statisticians selecting combining models decades 
discussions 
discuss neural networks statistical perspective 
smoothing example shows dataset simulation example illustrates smoothing splines 
assume true noise variance oe known 

data points curve fitting problem true curve 
curve fitted neural network sigma standard error bands true curve dotted 
shows fitted curve neural network linear output unit squares fitting hidden units 
limits shown sigma standard errors obtained theory non linear regression local linearization non linear regression problem 
clearly fitting shows local minima fitting function 
local minima hessian confirmed positive definite 
weight decay provides way combat fitting squares replaced minimizing theta gamma ij ij kwk possibly omitting bias weights giving proportionally lower penalty 
curves neural networks fitted different starting weights 
curve fitted neural network sigma standard error bands true curve 
gamma 
shows result weight decay 
knowing truth sure part correct 
weight decay figures picked thin air bayesian argument gives guidance suggesting oe regression network 
weight decay multiple minima occur 
effect weight decay reduce variability fit cost bias fitted curve smoother true curve 
model selection choosing point bias variance compromise terminology 
surprising standard errors tighter 
fact optimistic local linearization breaks quickly case take account local minima 
ways control smoothness fit number hidden units interact moderate values number hidden units little effect fitted function 
better direct regularization penalty spline smoothing see 
choose degree smoothness data 
cross validation nic eff cross validation widely method select architectural parameters term mis neural networks field 
language pattern recognition available examples may divided training validation test sets 
training set train network choose parameters weights 
performance validation set select competing family networks combination 
test set measure performance selected network 
note validation set distinct test set assessed performance valid 
examples normally abundant large validation test sets needed measure performance cross validation divides available samples re uses 
simplest form fold cv examples divided roughly comparable sets turn validation set remainder training set 
allows example validation 
cross validation loocv takes predicts example network trained remaining examples 
cross validation test set method network wish validate test need double layer cross validation 
cross validation cpu consuming neural networks especially loocv 
presence local minima causes difficulties attempt track local minima starting solution full problem removing outlying point change dramatically structure local minima 
take approach generalization akaike aic criterion 
deviance minus twice log likelihood regression problem known oe squares measure divided oe 
oe known deviance log 
aic deviance total number parameters 
appropriate maximum likelihood squares fitting regularizer weight decay replaced trace kj gamma gammae var averages done true distribution false parameter best possible fit true distribution log density plus regularization divided 
follows standard statistical theory 
regularization 
quantities estimated derivatives hessian evaluated fit training set replacing expectations averages training set 
moody eff version aic called nic 
underlying idea nic aic estimate deviance test set size compensating fact weights chosen fit training set 
stone proved state large training sets loocv nic equivalent practice 
nic gives effect loocv computational effort 
theory nic relies single defined minimum fitting function unreliable local minima 
leave cross validation leave single local minimum order perturbations small assess fit accurately 
consequence loocv reputation variable fold cross validation usually better guide 
size ssq nic bayes gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma table shows various quantities range fits data 
ssq fitted sum squares fold cv prediction fit dataset 
clearly hidden units optimizer got stuck local minimum 
table shows nic usually similar effectively controlled shows small differences models small nic small 
fitted curves supported models 
curves fitting models true curve added noise 
hidden unit model shows bias 
bayesian model selection bayesian framework established years edition way choose models bayes factor ratios probability density data model 
prior density parameters weights integrable 
weight decay corresponds assumption independent normal distributions weights known context ridge regression 
finding bayes factor implies integration weight space normally impossible approximations known 
fitting maximize log data log 
approximation log model log gamma log jhj hessian easily computed fit 
regression problem log data essentially sum squares divided oe converted corrected sum squares shown table 
agreement methods results available weight decay 
approach difficulty multiple local minima combined integration weight space need summed local minimum 
markov chain monte carlo methods explore model space compute bayes factors example 
model averaging bayesians long known logical endpoint theory average predictions models bayes factors weights computationally feasible 
particular choosing hyperparameter integrate proper distribution 
feasible coarse grid insists average predictions local minima computing weights 
model averaging discussed non bayesian contexts example wolpert stacked generalization 
missed part stone cross validation pp 

suggestion take convex combination predictions models cross validation choose weights combination 
averaging unweighted proposed neural networks 
little argument model averaging idea predictions neural networks explanatory purpose 
main argument computational pattern recognition problems difficult get network predict fast 
computationally feasible long established practice applied statistics average models 
bayesian view averages models fit applied madigan raftery advocate occam window exclude models fits exclude models subsume included 
simplest models fit included averaging process 
besag green higdon mengersen 
bayesian computation stochastic systems 
statistical science 
bishop improving generalization properties radial basis function neural networks 
neural computation 
cheng titterington neural networks review statistical perspective discussion 
statistical science 
draper assessment propagation model uncertainty discussion 
journal royal statistical society series 
gelman carlin stern rubin bayesian data analysis 
chapman hall new york 
geman bienenstock doursat neural networks bias variance dilemma 
neural computation 
huber behavior maximum 
le cam neyman 
eds proceedings fifth berkeley symposium mathematical statistics probability 
university california press berkeley jeffreys theory probability 
third edition 
clarendon press oxford 
lincoln synergy clustering multiple backpropagation networks 
touretzky 
ed advances neural information processing systems 
morgan kaufmann san mateo ca pp 

madigan raftery model selection accounting model uncertainty graphical models occam window 
journal american statistical association 
madigan york bayesian graphical models discrete data 
technical report department statistics university washington 
moody note generalization regularization architecture selection nonlinear learning systems 
ieee sp networks signal processing 
ieee computer society press pp 

moody effective number parameters analysis generalization regularization nonlinear learning systems 
moody hanson lippmann 
eds advances neural information processing systems 
morgan kaufmann san mateo ca pp 

moody principled architecture selection neural networks application corporate bond rating prediction 
moody hanson lippmann 
eds advances neural systems 
morgan kaufmann san mateo ca pp 

moody architecture selection strategies neural networks application corporate bond rating prediction 

ed neural networks capital markets 
wiley chichester pp 

murata amari criterion determining number parameters artificial neural network model 
kohonen simula kangas 
eds artificial neural networks 
north holland amsterdam pp 

murata amari learning curves model selection complexity neural networks 
hanson cowan giles 
eds advances neural information processing systems 
morgan kaufmann san mateo ca pp 

murata amari network information criterion determining number hidden units artificial neural network models 
ieee transactions neural networks 
perrone cooper networks disagree ensemble methods hybrid neural networks 

ed artificial neural networks speech vision 
chapman hall london pp 

ripley statistical aspects neural networks 
barndorff nielsen jensen kendall 
eds networks chaos statistical probabilistic aspects 
chapman hall london pp 
ripley neural networks related methods classification discussion journal royal statistical society series ripley neural networks flexible regression discrimination 
mardia 
ed statistics images 
pp 
advances applied statistics 
ripley flexible non linear approaches classification 
cherkassky friedman wechsler 
eds statistics neural networks 
theory pattern recognition applications 
springer berlin pp 

ripley pattern recognition neural networks 
cambridge university press cambridge 
wild nonlinear regression 
wiley new york 
stewart hierarchical bayesian analysis monte carlo integration computing posterior distributions possible models 
statistician 
stone cross choice assessment statistical predictions discussion 
journal royal statistical society series 
stone asymptotic equivalence choice model cross validation akaike criterion 
journal royal statistical society series 
wahba wold completely automatic french curve 
communications statistics 
wolpert stacked generalization 
neural networks 
