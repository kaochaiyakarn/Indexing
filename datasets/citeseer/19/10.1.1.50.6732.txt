mimic finding optima estimating probability densities jeremy de bonet charles isbell jr paul viola artificial intelligence laboratory massachusetts institute technology cambridge ma optimization problems structure solutions reflects complex relationships different input parameters 
example experience may tell certain parameters closely related explored independently 
similarly experience may establish subset parameters take particular values 
search cost landscape take advantage relationships 
mimic framework analyze global structure optimization landscape 
novel efficient algorithm estimation structure derived 
knowledge structure guide randomized search solution space turn refine estimate structure 
technique obtains significant speed gains randomized optimization procedures 
advances neural information processing systems mit press cambridge ma cost function local minima may search optimal ways 
variations gradient descent popular 
minima far optimal search include brute force component incorporate randomization 
classical examples include simulated annealing sa genetic algorithms gas kirkpatrick gelatt vecchi holland 
cases process optimizing thousands millions samples evaluated 
optimization algorithms take millions pieces information compress single point current estimate solution notable exception gas return shortly 
imagine splitting search process parts time steps 
parts structurally identical description start search initial point 
sole benefit enjoyed second part search initial point closer optimum 
intuitively additional information learned half search warn second half avoidable mistakes pitfalls 
optimization algorithm called mutual information maximizing input clustering mimic 
attempts communicate information cost function obtained iteration search iterations search directly 
efficient principled way 
main components mimic randomized optimization algorithm samples regions input space contain minimum second effective density estimator capture wide variety structure input space computable simple second order statistics data 
mimic results simple cost functions indicate order magnitude improvement performance related approaches 
experiments color map coloring problem yield similar improvements 
related known optimization procedures represent utilize structure optimization landscape 
contrast genetic algorithms ga attempt capture structure ad hoc embedding parameters line chromosome 
intent crossover operation standard genetic algorithms preserve propagate group parameters partially responsible generating favorable evaluation 
groups exist offspring generated preserve structure groups choice crossover point random 
problems benefit parameter completely independent value parameters population simply encodes information probability distribution parameter 
case crossover operation equivalent sampling distribution crossovers better sample 
problems fitness obtained combined effects clusters inputs ga crossover operation beneficial randomly chosen clusters happen closely match underlying structure problem 
rarity fortuitous occurrence benefit crossover operation greatly diminished 
result gas history function optimization baum boneh garrett lang 
goals incorporate insights gas principled optimization framework 
attempts capture advantages gas 
population incremental learning pbil attempts incorporate notion candidate population replacing single probability vector baluja caruana 
element vector probability particular bit solution 
learning process probability vector thought simple model optimization landscape 
bits values firmly established probabilities close 
unknown probabilities close 
structure components candidate particular values components determines fares difficult move pbil representation viable solution 
sorts problems pbil performs genetic algorithms algorithms hindered fact random crossovers infrequently beneficial 
distinct related technique proposed jordan reinforcement learning task jordan 
framework learner generate actions reinforcement function completely explored 
simultaneously learner exploit learned optimize long term reward 
jordan chose construct boltzmann distribution reinforcement function exp zt reinforcement function action temperature normalization factor 
distribution generate actions 
high temperatures distribution approaches uniform distribution results random exploration 
low temperatures actions garner large reinforcement generated 
reducing learner progresses initially randomized search directed search true optimal action 
interestingly estimate extent model optimization landscape constructed learning process 
knowledge jordan attempted optimization high dimensional spaces attempted fit complex model 
mimic knowing unreasonable search minimum generating points uniform distribution inputs 
search allows information generated previous samples effect generation subsequent samples 
surprisingly necessary samples generated distribution uniformly distributed probability 
example access min single sample sufficient find optimum 
insight suggests process successive approximation collection points density estimator constructed 
density estimator additional samples generated new threshold established gamma ffl new density estimator created 
process repeated values cease improve 
mimic algorithm begins generating random population candidates choosen uniformly input space 
population median fitness extracted denoted algorithm proceeds 
update parameters density estimator sample 

generate samples distribution 

set equal nth percentile data 
retain points validity approach dependent critical assumptions successfully approximated finite amount data gammaffl jjp small samples samples gammaffl kullback liebler divergence 
bounds conditions prove convergence finite number successive approximation steps 
performance approach dependent nature density approximator 
chosen estimate conditional distributions pair parameters representation total numbers 
section show conditionals distributions construct joint distribution closest kl sense true joint distribution 
approximator capable representing clusters highly related parameters 
similar intuitive behavior crossover representation strictly powerful 
importantly clusters learned data pre defined programmer 
generating events conditional probabilities joint probability distribution set random variables fx jx xn jx xn xn gamma xn pairwise conditional probabilities jx unconditional probabilities faced task generating samples match closely possible true joint distribution 
possible capture possible joint distributions variables unconditional pairwise conditional probabilities describe true joint distribution closely possible 
derive algorithm choosing description 
permutation numbers define class probability distributions jx jx gamma jx distribution uses ordering pairwise conditional probabilities 
goal choose permutation maximizes agreement true distribution 
agreement distributions measured kullback liebler divergence log gamma log dx log gamma log gammah gamma log jx jx gamma jx gammah jx jx gamma jx divergence non negative equality case identical distributions 
optimal defined minimizes divergence 
distribution completely described pairwise conditional probabilities optimal generate distribution identical true distribution 
insofar true distribution captured way optimal diverge distribution 
term divergence depend 
cost function wish minimize jx jx gamma jx optimal produces lowest pairwise entropy respect true distribution 
searching 
permutations possible determine optimal 
interests computational efficiency employ straightforward greedy algorithm pick permutation 
arg min 

arg min jx gamma gamma 
empirical entropy 
distribution chosen generating samples straightforward 
choose value empirical probability 
gamma gamma choose element empirical conditional probability jx 
algorithm runs time second time 
experiments measure performance mimic performed benchmark experiments compared results obtained standard optimization algorithms 
algorithms comparisons 
mimic algorithm samples iteration 
pbil standard population incremental learning 
rhc randomized hill climbing 
ga standard genetic algorithm single crossover mutation rate mimic pbil rhc ga inputs thousands evaluations function evaluations required maximize peaks number evaluations peak cost function different algorithms plotted variety problems sizes 
peaks peaks problem taken baluja caruana 
dimensional input vector peaks evaluation function defined max tail head tail number trailing head number leading ae tail head global maxima function 
achieved leading followed trailing preceded 
suboptimal local maxima occur string 
large values problem increasingly difficult basin attraction inferior local maxima larger 
results running algorithms shown 
trials set total number inputs 
mimic algorithm consistently maximizes function approximately tenth number evaluations required second best algorithm 
peaks peaks problem slight variation peaks tail head tail head function additional global maxima leading followed trailing preceded 
mimic pbil rhc ga inputs thousands evaluations function evaluations required maximize peaks mimic pbil rhc ga inputs thousands evaluations function evaluations required maximize coloring number evaluations peak cost function left color cost function right variety problem sizes 
case values candidates important structure positions take value positions take value groups take different values middle positions take value 
results problem shown 
expected pbil performed worse peak problem tends oscillate middle space contradictory signals pull back forth 
random crossover operation ga occasionally able capture underlying structure resulting improved relative performance ga expected mimic algorithm able capture underlying structure problem combine information maxima 
mimic consistently maximizes peaks function approximately number evaluations required algorithms 
max coloring graph colorable possible assign colors nodes graph adjacent nodes color 
determining graph colorable known np complete 
define max coloring task finding coloring minimizes number adjacent pairs colored 
results problem shown 
subset graphs single solution permutations color optimal solution dependent structure parameters 
pbil performs poorly 
ga perform better crossover point representative underlying structure graphs 
mimic performs best able capture structural regularity inputs 
described mimic novel optimization algorithm converges faster reliably existing algorithms 
mimic accomplishes ways 
performs optimization successively approximating con ditional distribution inputs bound cost function 
process optimum cost function gradually 
result mimic directly communicates information cost function early stages stages search 
second mimic attempts discover common underlying structure optima computing second order statistics sampling distribution consistent statistics 
acknowledgments research jeremy de bonet supported dod multidisciplinary research program university research initiative charles isbell fellowship granted labs research paul viola office naval research 

greg helped preparation 
baluja caruana 

removing genetics standard genetic algorithm 
technical report carnegie mellon 
baum boneh garrett 

genetic algorithms excel 
proceedings conference computational learning theory new york 
association computing machinery 
holland 

adaptation natural artificial systems 
michigan university press 
kirkpatrick gelatt vecchi 

optimization simulated annealing 
science 
lang 

hill climbing beats genetic search boolean circuit synthesis problem koza 
twelfth international conference machine learning 
jordan 

reinforcement learning probability matching 
david touretzky perrone editors advances neural information processing volume denver 
mit press cambridge 
