perceptron algorithm vs winnow linear vs logarithmic mistake bounds input variables relevant kivinen manfred warmuth ucsc crl october center computer engineering information sciences university california santa cruz santa cruz ca usa give adversary strategy forces perceptron algorithm gamma mistakes learning literal disjunctions variables 
experimentally see simple random data number mistakes perceptron algorithm grows linearly number relevant variable remains small constant 
contrast littlestone algorithm winnow log mistakes problem 
algorithms thresholded linear functions hypotheses 
winnow multiplicative updates weight vector additive updates perceptron algorithm 
supported academy finland 
done author visiting university california santa cruz 
address department computer science box fin university helsinki finland cs helsinki fi 
supported nsf iri 
address computer information sciences university california santa cruz santa cruz ca usa manfred cse ucsc edu 

addresses familiar problem predicting linear classifier instances tries predict binary classification dimensional real vectors 
linear classifier represented pair dimensional weight vector threshold linear classifier represented pair value instance delta value 
instance thought value input variables value ith input variable 
monotone disjunctions special case linear classifiers 
monotone literal disjunction corresponds linear classifier represented pair delta delta delta disjunction variables disjunction called relevant remaining variables irrelevant study performance known learning algorithms linear classifiers applied learning monotone disjunctions number relevant variables smaller total number variables 
analyze algorithms simple line prediction model learning 
learning proceeds trials 
trial learning algorithm instance produces prediction current hypothesis linear classifier algorithm current weight vector threshold algorithm receives binary outcome may update weight vector threshold outcome differs prediction say algorithm mistake 
littlestone lit lit goal minimize total number mistakes learning algorithm certain sequences trials 
standard line algorithm learning linear classifiers simple perceptron algorithm rosenblatt ros 
alternate algorithm called winnow introduced littlestone lit lit 
see algorithms consider binary vector instance assume algorithm predicted outcome 
algorithms increment weights corresponding input change weights 
delta delta dot product increases 
difference algorithms increment weights question 
perceptron algorithm adds positive constant winnow multiplies constant larger 
similarly prediction outcome weights decremented subtracting positive constant dividing constant larger 
choice constants updates initial weights thresholds significantly affect performance algorithms 
call choosing parameters tuning linear classifier delta say trial sequence consistent classifier say classifier target trial sequence 
easy tune winnow log mistakes lit lit sequence literal disjunction target 
tuning allowed depend tighter bound log obtainable 
upper bound optimal constant factor vapnik chervonenkis vc dimension vc class literal disjunctions omega gamma log lit dimension lower bound optimal mistake bound :10.1.1.130.9013
best upper bound know learning literal monotone disjunctions perceptron 
algorithm kn mistakes 
bound comes perceptron convergence theorem dh suspect tight case particularly large 
main result give simple adversary strategy forces perceptron algorithm gamma mistakes assuming initial weight vector algorithm zero 
perceptron algorithm allowed choose arbitrary initial weight vector prove lower bound gamma 
lower bound gamma holds algorithms property weight vector algorithm obtained adding initial weight vector linear combination instances seen far gamma ff scalars ff call algorithms additive 
class additive algorithms includes classical perceptron algorithm complicated algorithm ellipsoid method linear programming mt 
additive algorithms include algorithms change predictions instances rotated instance replaced ax thetan orthonormal 
contrast weight vectors winnow satisfy gamma fi positive scalars fi winnow called multiplicative algorithm 
perceptron algorithm special property scalars ff satisfy ff ff similarly scalars fi winnow satisfy fi fi algorithms new weight vector obtained knowledge past instances small mistake bound perceptron algorithm exponential optimal mistake bound case essentially vc dimension 
may seen instance called curse dimensionality literature neural networks statistics dh 
may surprising perceptron algorithm performs badly monotone disjunctions concepts simple linear classifiers small weights threshold 
difference performances algorithms shows algorithms different bias search hypothesis 
intuitively winnow favors weight vectors sense sparse wins target weight vector sparse disjunction case 
target weight vector dense omega gamma disjunction case advantage winnow worst case bounds smaller 
experiments seen perceptron algorithm fewer mistakes winnow target dense learning rate threshold set theorems guarantee log mistake bound 
experiments instances sparse instance components set zero 
assures roughly half instances positive 
believe sparseness instances advantageous perceptron algorithm 
note known close winnow tuned simulates classical elimination algorithm learning disjunctions val case gamma mistakes literal monotone disjunctions robust noise 
trade winnow able take advantage sparse targets dense instances perceptron algorithm able take advantage sparse instances 
dense targets similar situation line linear regression kw :10.1.1.30.7849
regression problem classical gradient descent algorithm perceptron style additive updates new family exponentiated gradient algorithms multiplicative winnow style updates 
exponentiated gradient algorithms win sparse targets dense instances gradient descent algorithm wins dense targets sparse instances 
regression problem algorithm brought clearly classification problem 
proven worst case loss bounds incomparable experimental performance simple artificial data 
lower bounds perceptron algorithm proven adversary argument instances obtained simple transformation rows hadamard matrix 
rows hadamard matrix orthogonal perceptron algorithm updates weight vector adding multiple current instance result predictions perceptron algorithm independent preceding trials 
similar proofs devised linear regression 
proofs case linear classification slightly involved 
wish point behavior similar predicted worstcase lower bounds takes place random data 
case number mistakes perceptron algorithms gets close significantly lower winnow number relevant variables small 
possibly explained fact large dot product random dimensional binary vectors close fixed value 
argue random inputs natural 
genuine random inputs may lead large number pseudo random variables inputs expanded form new variables 
case number mistakes perceptron algorithm may grow linearly number pseudo random variables 
perceptron algorithm susceptibility irrelevant input variables batch setting 
perceptron algorithm trained instances derived half rows theta hadamard matrix wrong roughly half time instances derived second half hadamard matrix 
winnow opens new venue algorithm design 
additional irrelevant input variables degrade algorithm prediction performance badly extend algorithm capabilities nonlinear prediction introducing additional inputs values large number nonlinear basis functions 
ignoring moment computational efficiency extreme case consider learning dnf winnow 
introducing new input variable possible conjunctions dnf formula gets worst case bound kn mistakes term dnf formulas 
note mistake bound light fact logarithm number formulas kn 
computational cost maintaining weights soon prohibitive 
introducing new variables heuristic introduce 
weights tested variables guide choosing variables iteration 
experiments method learning dnf noticed random instances respect uniform distribution number mistakes winnow get close theoretical bound 
simple artificial data perceptron algorithm outperformed winnow tuned upper bound theorems littlestone lit :10.1.1.130.9013

prediction model algorithms introduce details line prediction model algorithms consider section 
section gives adversarial lower bound constructions class additive algorithms 
experimental results section 
prediction model algorithms basic setting pair represent linear classifier weight vector threshold 
classifier represented denoted phi defined phi delta phi 
concerned special case dimensional trial sequence game played players learner teacher 
purposes restrict learners predict linear classifiers manner shall soon describe detail 
game rounds trials positive integer 
trial sequence trial proceeds follows 
learner chooses hypothesis 
teacher presents instance 
learner prediction defined phi 

teacher presents outcome trial teacher target phi goal learner minimize number mistakes trials teacher hand tries force learner mistakes 
seemingly strong worst case model prediction adversarial teacher justified fact algorithms guaranteed reasonable number mistakes learners model 
soon introduce algorithms perceptron algorithm winnow mistake bounds 
model adversarial allowing teacher number classification errors trials phi hand restrict teacher restricting target 
consider case target required monotone literal disjunction exactly components value 
line linear prediction algorithm deterministic algorithm act learner game described 
general line prediction algorithm allowed choose hypothesis mapping linear classifier 
restriction linear classifiers effective essential learner required fix hypothesis instance 
learner emulate arbitrary line algorithm choosing hypothesis constant threshold function gamma depending prediction algorithm instance term trial sequence sequence gives teacher part game 
fixed deterministic learning algorithm learner part completely determined trial sequence 

prediction model algorithms algorithms perceptron algorithm winnow new hypothesis depends old hypothesis observed instance outcome call dependence update rule algorithm 
addition update rule give initial hypothesis characterize algorithm 
usual initial weight vectors zero vector vector 
note definition linear line prediction algorithm allows new hypothesis depend earlier instances outcomes sophisticated algorithms dependencies 
perceptron algorithm winnow families algorithms parameterized initial hypothesis learning rate 
give update rules algorithms denote oe sign prediction error trial oe gamma basic forms perceptron algorithm winnow maintain fixed threshold instance sign oe learning rate update perceptron algorithm written componentwise gamma joe update winnow note basic version winnow algorithm winnow lit uses positive weights assuming initial weights positive :10.1.1.130.9013
algorithm generalized negative weights simple reduction lit :10.1.1.130.9013
see littlestone lit discussion learning rates parameters winnow 
just point standard method allowing threshold fixed cost increasing dimensionality problem 
instance xn replaced xn 
linear classifier nonzero threshold replaced wn gamma 
useful technique gives method effectively updating threshold components weight vector 
known target monotone literal disjunction winnow log mistakes lit :10.1.1.130.9013
algorithms multiplicative weight updates achieve similar mistake bounds lit 
best upper bound know perceptron algorithm comes perceptron convergence theorem duda hart dh pp 

assuming target monotone literal disjunctions instances satisfy value bound kx mistakes 
note maass tur mt pointed linear programming methods transformed efficient linear line prediction algorithms 
notably applies khachiyan ellipsoid algorithm newer algorithm due vaidya vai 
vaidya algorithm achieves upper bound log mistakes arbitrary linear classifier target instances perceptron algorithm winnow suitable learning arbitrary linear classifiers domain maass tur show worst case number mistakes algorithms exponential proof log mistake bound general linear classifiers observing arbitrary real weights linear classifier replaced integer weights larger changing classification 
prediction model algorithms point monotone disjunctions weights threshold directly chosen leads better bound log mistakes 
follows assume arithmetic operations various algorithms performed exactly rounding errors 
special classes algorithms main theoretical results lower bounds class additive algorithms 
definition linear line prediction algorithm additive algorithm tth weight vector written gamma ff fixed initial weight vector coefficients ff considering line prediction algorithms coefficients ff course depend instances outcomes lower bounds shall prove valid algorithm allowed set coefficients ff best possible values instances outcomes 
perceptron algorithm additive 
comparing see take ff perceptron algorithm 
consider winnow initial weights learning rate ln threshold 

consistent target gives 
vector gamma span see winnow additive 
recall square matrix thetam orthogonal columns orthogonal orthonormal orthogonal columns euclidean norm 
orthogonal matrix product diagonal matrix orthonormal matrix theta identity matrix 
consider orthonormal matrix thetam think vector list coordinates point dimensional space ax considered list coordinates point new coordinate system 
basis vectors new coordinate system represented original coordinate system column vectors orthonormal matrices represent rotations coordinate system 
write ax 
rotations preserve angles delta aw ax delta situation geometric interpretation meaningful natural assume choice coordinate system irrelevant changes systematically replaces 
definition linear line prediction algorithm rotation invariant orthonormal matrices thetan trial sequences predictions algorithm trial sequence predictions trial sequence ax ax 

prediction model algorithms general rotation invariant necessarily natural property algorithm 
instance components instances represent physical quantities different may entirely different units 
common scale instances example gamma hold cases original coordinate system clearly special meaning 
common algorithms rotation invariant 
perceptron algorithm rotation invariant 
linear line prediction algorithm obtains applying reduction maass tur ellipsoid method linear programming rotation invariant 
initial ellipsoid algorithm ball centered origin updates ellipsoid done rotation invariant manner 
uses vaidya algorithm linear programming reduction gets algorithm rotation invariant 
vaidya algorithm uses polytope updated rotation invariant manner initialization polytope rotation invariant 
winnow rotation invariant 
see consider dimensional trial sequence 
assume winnow uses weight vector threshold 
trials winnow weight vector 
consider orthonormal matrix gamma gamma seeing counterexamples ax ax winnow hypothesis 
linear clear winnow rotation invariant 
specific consider instance gammar delta delta ax 
values predictions winnow rotated original instances 
general result 
theorem linear line prediction algorithm rotation invariant additive algorithm zero initial weight vector 
proof hypothesis rotation invariant algorithm seen instances outcomes claim subspace spanned set easy construct orthonormal matrix thetan ax ax gammax vector orthogonal ax definition rotation invariant algorithm implies ax ax choose vector orthogonal ax gammaw subspace spanned conversely consider algorithm additive zero initial weight vector 
algorithm thresholds coefficients ff depend outcomes dot products delta algorithm easily seen rotation invariant 

lower bounds additive algorithms lower bounds additive algorithms vectors gamma gamma denote hamming distance number indices proofs basic properties hadamard matrices hadamard matrix orthogonal matrix element gamma multiplying row column hadamard matrix gamma leaves hadamard matrix 
note different rows theta hadamard matrix 
definition gives straightforward way obtaining high dimensional hadamard matrices 
definition theta hadamard matrix obtained recursive construction gammah result 
lemma positive integer theta hadamard matrix defined definition 
vector gamma index holds jth column consider additive algorithm hypothesis 
prediction instance depend dot products delta delta adversary helpful different candidates dot products differ 
motivates definition 
definition say sequence pairwise constant dot products delta delta delta delta basic idea form sequence pairwise constant dot products choosing tth row theta hadamard matrix gammaz simple transformations necessary instances binary 
merely having pairwise constant dot products sufficient generating mistakes 
adversary needs target suitably different algorithm initial hypothesis 
get idea consider instance candidates say delta delta depending algorithm threshold algorithm may predict predict predict target delta delta choosing tth instance adversary force algorithm mistake regardless choice note adversary choosing instances sequence pairwise constant dot products algorithm additive condition delta delta equivalent delta delta independent updates algorithm 
leads definition 
definition weight vector theta linear classifier 
say weight vector classifier differ trial sequence delta delta delta delta delta delta delta delta basic idea prove result 

lower bounds additive algorithms theorem sequence pairwise constant dot products 
consider additive linear line prediction algorithm initial weight vector target adversary choose instances way algorithm mistakes trials differ proof consider trial sequence phi 
assume gamma sequence pairwise constant dot products 
hypothesis additive linear line prediction algorithm trial write gamma ff assume initial weight vector target differ trial sequence consider case delta delta delta delta pairwise constant dot products delta delta delta adversary chooses case algorithm mistake 
adversary chooses algorithm mistake 
case delta delta delta delta similar 
proving lower bounds reduced finding initial weight vector sequence pairwise constant dot products target initial weight vector target differ sufficiently 
sequence definition 
definition gamma positive integers write gamma 
theta hadamard matrix defined definition tth row define sequence bh setting gammah gammah lemma sequence bh defined definition pairwise constant dot products 
proof follows facts delta gamma 
lemma gamma positive integers write gamma bh definition 
monotone literal disjunction zero vector differ trial sequence bh proof vector 
delta delta 
sequence bh delta delta lemma gamma positive integers write gamma bh definition 
vector monotone literal disjunction differ trials sequence bh 
experiments trials perceptron winnow cumulative mistake counts perceptron algorithm winnow random instances monotone literal disjunction target 
proof define vector gamma gamma delta delta 
lemma choose index ith column hadamard matrix choose 
construction bh delta delta gamma delta delta 
vector literal disjunction differ trial combining theorem lemma lemmas get main results 
theorem gamma positive integers write gammak 
additive linear line prediction algorithm dimensional trial sequence monotone literal disjunction target algorithm mistakes trial sequence 
additive linear line prediction algorithm zero initial weight vector dimensional trial sequence monotone literal disjunction target algorithm mistakes trial sequence 
comments section theorem gives lower bound gamma mistakes ellipsoid algorithm perceptron algorithm zero initial weight vector 
obtain lower bound gamma mistakes perceptron algorithm arbitrary initial weight vectors 
lower bounds perceptron algorithm allow algorithm arbitrary thresholds trial 
experiments section describe simple experiments perceptron algorithm winnow 
experiments intended thorough empirical evaluation algorithms 
illustrate fact certain circumstances actual behavior algorithms qualitatively similar expected considering worst case bounds 

experiments trials winnow perceptron cumulative mistake counts perceptron algorithm winnow sparse random instances literals target 
shows results experiments target monotone disjunction literals 
instances generated uniform distribution number variables experiment second 
winnow parameters ln tuning winnow guaranteed log mistakes target literal monotone disjunction algorithm robust noise lit 
perceptron algorithm zero initial weights eliminated threshold transformation section 
case choice learning rate perceptron algorithm difference 
see sparse case winnow clearly fewer mistakes 
notable little winnow performance degraded dimensionality instances doubled 
hand number mistakes perceptron algorithm increased drastically 
shows results experiments number literals target monotone disjunction dimensional instances 
targets denser ones 
experiments 
instances chosen random component having probability gamma gamma having value 
probability having positive example 
values considered gave approximately average number input variables value 
experiments perceptron algorithm outperformed winnow 
note tuning gives winnow log worst case bound robust noise 
tuning large algorithm gamma mistakes noise free setting 
bound lower number mistakes perceptron algorithm experiments 
issues need addressed experimental 
experiments reported noiseless data 
winnow perceptron algorithm tolerate noise reasonably presence noise affect comparison 
remains see algorithm behavior data actual applications similar behavior random data 

open problems open problems far evaluation algorithms random data experimental 
possible obtain closed formulas expected total number mistakes perceptron algorithm thermodynamic limit see sst 
wish study closed formulas relate worst case upper bounds adversary lower bounds 
studying behavior lead deeper understanding high dimensionality hurts perceptron algorithm additive algorithms 
extensive experimental comparison various line algorithms learning disjunctions presence attribute noise done littlestone lit 
bounds worst case number mistakes earlier obtained perceptron algorithm winnow 
upper bound proofs interpreted amortized analysis potential function 
different potential functions generalized version entropy function winnow lit aw squared euclidean distance perceptron convergence theorem dh 
observations analogous case line linear regression 
algorithms worst case loss bounds proved potential functions kw :10.1.1.30.7849
case linear regression possible derive updates potential functions addition worst case analysis 
open problem devise framework deriving updates potential functions linear classification case 
perceptron algorithm specialized learn disjunctions 
purpose show number mistakes additive algorithm grow linearly number variables simple linear classifiers small disjunctions 
variables relevant additive algorithms dimensions futile search predictor 
cleaner comparison related algorithms obtained linear regression case kw :10.1.1.30.7849
targets small disjunctions ellipsoid method exhibits similar linear growth number mistakes computationally trivial perceptron algorithm 
know vaidya algorithm linear programming exhibits linear growth 
applying linear programming methods learning simple disjunctions remains subject study 
implementation issues need addressed general linear programming methods perform optimally restricted problem 
case algorithms require significantly computation time update perceptron algorithm winnow 
considered case target disjunction consistent trial sequence 
instances outcomes corrupted noise target usually exist 
noisy case considered littlestone lit auer warmuth aw 
auer warmuth prove worst case loss bounds situation target may change time 
aw auer warmuth 
tracking best disjunction 
proc 
th symposium foundations comp 
sci 
ieee computer society press los alamitos ca 
blumer ehrenfeucht haussler warmuth 
learnability vapnik chervonenkis dimension 
acm 
dh duda hart 
pattern classification scene analysis 
wiley 
khachiyan 
polynomial algorithm linear programming russian 
doklady 
english translation soviet mathematics doklady 
kw kivinen warmuth :10.1.1.30.7849
exponentiated gradient versus gradient descent linear predictors 
report ucsc crl university california santa cruz june 
extended appeared stoc 
lit littlestone :10.1.1.130.9013
learning quickly irrelevant attributes abound new linear threshold algorithm 
mach 
learning 
lit littlestone 
mistake bounds logarithmic linear threshold learning algorithms 
phd thesis report ucsc crl university california santa cruz 
lit littlestone 
redundant noisy attributes attribute errors linear threshold learning winnow 
proc 
th workshop comput 
learning theory pages 
morgan kaufmann 
lit littlestone 
comparing linear threshold learning algorithms tasks involving superfluous attributes 
proc 
th international machine learning conference ml pages 
littlestone long warmuth 
line learning linear functions 
proc 
rd acm symposium theory computing pages 
mt maass tur 
fast threshold gate learn 
computational learning theory natural learning systems volume pages 
mit press 
ros rosenblatt 
perceptron probabilistic model information storage organization brain 
psych 
rev 
reprinted neurocomputing mit press 
sst sompolinsky seung tishby 
learning curves large neural networks 
proc 
th workshop comput 
learning theory pages 
morgan kaufmann 
vai vaidya 
new algorithm minimizing convex functions convex sets 
proc 
th symposium foundations computer science pages 
ieee computer society 
val valiant 
theory learnable 
commun 
acm 
vc vapnik chervonenkis 
uniform convergence relative frequencies events probabilities 
theory probab 
applications 
rau 
statistical mechanics learning rule 
rev mod 
phys 
