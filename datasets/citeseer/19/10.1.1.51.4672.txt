vldb journal springer verlag data placement shared parallel database systems mehta david dewitt st san jose ca usa mail com dayton st madison wi usa mail dewitt cs wisc edu edited 
received may accepted april 
data placement shared database systems studied extensively past various placement algorithms proposed 
consensus efficient data placement algorithm placement performed manually database administrator periodic reorganization correct mistakes 
presents comprehensive simulation study data placement issues shared system 
results show current hardware technology trends significantly changed performance tradeoffs considered past studies 
simplistic data placement strategy new results developed shown perform variety workloads 
key words declustering disk allocation resource allocation resource scheduling decade seen significant change characteristics database applications 
demands traditional applications transaction processing grown dramatically 
addition emerging applications geographical information systems multimedia database mining pose new performance challenges existing database systems 
shared sn database systems bubba bora gamma dewi tandem tand teradata tera volcano promise scalability availability evolved answer new challenges :10.1.1.113.6798
existing configurations contain hundreds processors multiple disks 
efficient resource management essential high performance large systems 
important resource management issue sn parallel database systems layout database system data placement 
earlier studies shown performance scalability sn parallel database research done author student university wisconsin madison 
research supported ibm research initiation 
systems contingent physical layout data nodes system 
data placement serves important load balancing mechanism 
absence remote data access sn system data placement determines distribution data distribution operators select directly access data 
poor data placement strategy result non uniform distribution load formation bottlenecks 
relatively static nature data placement decisions increases need efficient placement algorithm 
resources processors memory re allocated run time allowing design dynamic policies adapt workload transients brow rahm brow 
data placement changed expensive reorganization relations database 
factors data placement extremely important issue highperformance sn systems 
order exploit parallelism parallel sn database system tuples belonging single relation typically placed multiple disks 
relation said horizontally partitioned declustered ries cases issue needs addressed selecting data placement strategy sn database system number nodes partition decluster tuples relation called degree declustering relation 
declustering exploits parallelism leads higher startup termination costs process started terminated nodes relation declustered order read tuples relation 
degree declustering relation chosen increase startup termination costs offset benefits increased parallelism 
choosing degree declustering data placement algorithm select particular nodes decluster relation 
load balancing important objective phase placement algorithm 
relation placement algorithm determine mapping individual data tuples nodes 
os community generally uses term striping declustering horizontal partitioning 
addresses data placement issues choosing degree declustering selecting set nodes place relation 
explicitly address third issue mapping individual tuples nodes explore effect processing overhead system performance 
detailed discussion declustering policy interested reader see hua 
data placement strategy cope decided placement relations memory feel caching relations memory better managed dynamic memory management policy fragment algorithm brow 
policies adapt runtime changes static data placement strategy 
data placement refers placement relations disks 
data placement studied extensively past various algorithms proposed consensus best placement strategy 
commercial systems tandem teradata simply full declustering relations declustered nodes system 
hand previous placement studies cope rahm rahm concluded partial declustering better 
unfortunately studies advocated partial declustering data placement algorithms partial declustering developed algorithms depend static analysis workload 
assume database workloads complex change dynamically static workload analysis 
developments processor network technologies significantly changed performance tradeoffs considered previous data placement studies 
goal develop data placement strategy sn parallel database systems static workload analysis takes hardware trends consideration 
remainder organized follows 
section describes architecture sn parallel database system sect 
presents detailed discussion various factors determine performance data placement algorithms 
simulation model workload performance analysis sect 

section contains results experiments selecting degree declustering algorithms select nodes placing relations studied sect 

section explores effect workload changes performance declustering algorithms multi class workloads discussed sect 

section presents related data placement presents comparison previous declustering studies 
sect 
contains suggestions 
system architecture presents schematic description typical sn parallel database system 
system consists set external terminals transactions submitted 
transactions sent randomly selected scheduling node 
execution transaction processing nodes coordinated specialized process called scheduler 
scheduler node processing nodes interconnection network 
memory cpu disks 
memory cpu disks 
memory cpu disks terminals 
memory cpu disks nodes fig 

shared database system scheduler allocates resources memory processors transaction responsible starting terminating operators transaction 
processing nodes composed cpu memory disk drives nodes interconnection network communication 
data placement issues section discusses factors affect choice degree declustering relations placement system startup termination costs communication costs workload isolation data skew result collection 
startup termination costs mentioned previously degree declustering relation chosen benefits parallelism offset costs operator startup termination 
startup termination operators handled specialized process called query scheduler 
time consumed startup termination dependent startup termination protocols query scheduler 
consider alternative startup protocols study 

parallel protocol scheduler sends startup messages nodes executing operator waits acknowledgments 
node initiates instance operator sends acknowledgment back scheduler 
startup process complete acknowledgments received nodes 

sequential sequential protocol scheduler sends startup message node waits acknowledgment sending startup message 
sequential protocol obviously slower parallel protocol included simulate systems high startup costs 
termination protocol modeled study 
rest term node collectively refer processor local memory attached set disks 
instance operator completes sends termination message query scheduler 
operator terminates scheduler receives termination message operator instances 
communication costs cope rahm rahm cite communication costs important factor determining degree declustering 
degree declustering determines degree parallelism operators access disk resident data select 
higher degree declustering implies higher degree parallelism operators turn implies data read possibly re distributed nodes concurrently 
bandwidth interconnection network network bottleneck re distribution lower degree parallelism lower degree declustering may desirable 
network bandwidth may factor determining degree declustering past assert longer case 
scalable interconnects designed provide extremely high bandwidths mb node para 
comparison data transfer rates disks reaching mb ibm 
data transmitted simultaneously large number nodes network bottleneck 
workload isolation typically large database configurations shared users performing variety tasks 
instance large system may execute workloads multiple independent databases 
problem environment workload interference 
users want prevent fluctuations workload affecting concurrent workloads 
data placement mechanism achieve workload isolation environments 
storing database disjoint set nodes interference essentially eliminated disjoint placement provides complete workload isolation sn database system major drawback partitioned system responsive workload changes 
consider workload consisting classes class called contains low priority queries consists high priority queries 
partitioning isolate classes set nodes dedicated processing queries disjoint set nodes process queries 
sudden increase arrival rate class increased contention corresponding part system possible distribute additional load nodes processing queries 
order deal cases data placement strategy allow flexible workload isolation mechanisms adapt workload changes 
assuming adequate network bandwidth eliminate network contention 
skew variance response times multiple instances parallel relational operator called skew wolf hua kits walt dewi 
degree declustering determines degree parallelism operators select skew important factor selecting degree declustering relation 
taxonomy data skew parallel databases walt 
types skew identified skew initial tuple placement tuple placement skew skew due variations predicate selectivity nodes selectivity skew skew redistribution tuples preparation join redistribution skew imbalance number output tuples join product skew 
wolf walt dewi assume relations distributed tuple placement selectivity skew 
redistribution skew considered techniques ones dewi easily eliminate 
hand remote data access capabilities required reducing join product skew 
example distributed virtual shared memory handling join product skew 
remote data access assumed join product skew may 
section presents experiment explores effect join product skew data placement 
result collection output parallel operation needs merged single stream transmission external processor 
size output relation large collection result tuples bottleneck 
single output stream may required result sizes vary dynamically query query feel result size determining factor selecting degree declustering relation 
assume join results declustered multiple disks result collection bottleneck experiments 
simulator model performance studies detailed simulation model sn database system 
simulator written csim process oriented simulation language models database system closed queueing system 
sections describe configuration database workload models simulator detail 
configuration model terminals model external workload source system 
terminal sequentially submits stream transactions 
terminal exponentially distributed thinktime create variations arrival rates 
experiments configuration consisting nodes 
nodes modeled cpu buffer pool mb kb data pages disk drives 
cpu uses round robin scheduling policy ms timeslice 
buffer pool models set main memory page frames replacement controlled lru policy extended love hate hints haas 
hints provided various relational operators fixed pages 
example love hints index scan operator keep index pages memory hate hints sequential scan operator prevent buffer pool flooding 
addition memory reservation system control scheduler task allows buffer pool memory reserved particular operator 
memory reservation mechanism hash join operators ensure memory available prevent hash table frames stolen operators 
simulated disks model fujitsu model gb disk drive 
disk provides cache divided kb cache contexts prefetching pages sequential scans 
disk model slightly simplifies actual operation disk cache managed follows request required page number specifies prefetching desired 
prefetching requested pages read disk cache context part transferring page originally requested disk memory 
subsequent requests prefetched blocks satisfied incurring operation 
simple round robin replacement policy allocate cache contexts number concurrent prefetch requests exceeds number available cache contexts 
disk queue managed elevator algorithm 
interconnection modeled infinite bandwidth network network contention messages 
previous experience gamma prototype dewi showed network contention minimal typical sn :10.1.1.113.6798
messages incur transmission delay messages point point broadcast mechanism communication 
table contains configuration parameters cpu processing costs various database operations table 
kinds configurations considered disk intensive node configuration mips cpu disk 
configuration represents systems workloads typically bound 
cpu intensive nodes cpu intensive configuration mips cpu disks 
configuration models systems performance typically cpu bound 
simulated buffer pool size smaller buffer pools typical configurations 
unfortunately simulating larger buffer pool size require enormous amounts resources 
simulations took ran days ibm rs mb memory node 
hand configuration realistic previous simulation studies rahm studied configuration nodes mb node memory 
table 
simulator parameters values configuration node parameter value number nodes memory node mb cpu speed mips number disks node page size kb disk seek factor disk rotation time ms disk settle time ms disk transfer rate mb disk cache context size pages disk cache size contexts disk cylinder size pages wire delay message table 
cpu cost parameters operation instructions initiate select operator terminate select operator initiate join operator terminate join operator apply predicate read tuple buffer probe hash table insert tuple hash table start copy byte memory send receive message database model sizes input relations chosen explore large range query execution times 
database consists sets relations sizes number tuples relation 
addition extra relations tuples achieve disk occupancy configuration 
tuple size bytes relations tuples kb page 
model clustered non clustered tree indices relations 
index key sizes bytes key pointer pairs bytes long 
relations indices declustered nodes particular declustering algorithm performance study 
table summarizes database parameters 
workload model key workload characteristic study access pattern 
real workloads contain numerous transaction types varied behaviors simplified class workload designed capture table 
database parameters parameter value number tuples tuple size tuples page index key size index key pointer pair size essence real workloads 
class designed model short interactive transactions 
transactions class perform single tuple non clustered index selections randomly selected relations 
selection executes nodes relation declustered 
process selection transaction reads pages index pages data page 
transaction reads total pages total number os performed may differ due variations buffer hit rate 
selections executes randomly chosen single node 
response time transactions affected relation degree declustering 
class referred transaction class rest 
second class called join class captures behavior long running batch queries consists binary hybrid hash joins dewi experiments 
binary join queries chosen issues pipelining intra query parallelism arise processing complex queries ignored 
reasonable simplification commercial database systems execute queries comprised multiple joins series binary joins pipeline tuples adjacent joins query tree 
simplified query workloads previously ng yu 
consequently complex multiple join queries included experiments sect 

order maximize join processing costs declustering attribute assumed different join attribute complete redistribution relation needed join processing 
simplification stated selection predicates joins selectivity indices processing input relations 
effect indices examined separately sect 

sect 
queries allocated maximum memory requirement 
order simplify processor allocation joins join executed nodes inner relation declustered 
degree declustering inner relation determines degree join parallelism response time proportional degree declustering inner outer relation 
higher degree declustering decreases amount data read node improves response time increases overhead associated parallel execution operator 
number transaction class terminals fixed simulation experiments number query terminals varies 
terminal think time exponentially distributed different mean values change load offered class 
selecting degree declustering series experiments determines degree declustering relations 
results cpu intensive configurations combination primary factor importance large number os performed queries join methods nested loop sort merge change results qualitatively 
degree declustering average join response time secs terminals terminals terminals terminals fig 

query response times joins relations disk intensive configuration parallel startup sequential parallel startup protocols 
sect 
queries allocated maximum memory requirement 
disk intensive configuration parallel startup experiment examines effect increasing degree declustering queries various sizes 
inner outer relations number tuples size input relations label query 
example query refers join different randomly chosen tuple relations 
shows response times queries various system loads degree declustering increased 
declustering nodes relation randomly chosen 
note degree declustering represents full declustering 
expected average query response time increases number terminals system load increased 
terminals response times initially decrease degree declustering increased start increasing slowly 
behavior observed higher query loads terminals sharper increases higher degrees declustering 
startup termination costs increase higher degree declustering response time increases degree declustering due startup termination communication costs 
degree declustering disk cache context worth data disk recall disk node configuration disk kb cache divided kb cache contexts 
disk caches fully utilized nodes disk fetches kb pages disk cache 
reduces cache hit rate increases disk arm contention 
resulting loss bandwidth causes average query response time increase 
increase disk contention exacerbates effect lower bandwidth increase response time higher query terminals 
similar behavior ex input relation size tuples thousands optimal degree declustering fig 

optimal degree declustering disk intensive configuration parallel startup pected multiple page os cache reduce disk arm contention 
order verify relationship cache context size optimal degree declustering experiment performed relations different sizes 
shows degree declustering achieves lowest average response times called optimal degree declustering relations tuples 
results show optimal degree declustering increases linearly relation size increased tuples 
relation size optimal degree declustering occurs just disk cache context worth data node 
optimal degree declustering greater system size nodes optimal degree declustering constant relation size tuples 
results experiments demonstrate simple cache context disk rule disk intensive configuration parallel operator startup determine maximum degree declustering relations 
sequential startup shows optimal degree declustering operator instances started sequentially scheduler solid line 
compared results parallel startup dotted line optimal degree declustering relations lower longer linear relationship relation size 
instance relation size increased tuples tuples optimal degree declustering increases 
increasing relation size tuples increases optimal degree declustering 
non linear relationship optimal degree declustering opt relation size explained follows 
startup termination costs significant response time query modeled startup termination cost node degree declustering input relation processing cost tuple number tuples relation 
input relation size tuples thousands optimal degree declustering sequential parallel fig 

optimal degree declustering disk intensive configuration input relation size tuples thousands optimal degree declustering cpu intensive disk intensive fig 

optimal degree declustering parallel startup optimal degree declustering relation tuples opt differentiating equation respect equating 
leads formula opt shows opt non linear function sequential startup opt rises rapidly size input relations increases 
result relations tuples mb fully declustered disk intensive configuration 
cpu intensive configuration parallel startup shows optimal degree declustering relations parallel startup algorithm configuration mips cpu disks node 
optimal degree declustering configuration shown comparison 
cpu intensive configuration relations tuples fully declustered solid line compared tuples disk intensive configuration dashed line 
startup termination take longer slower cpu constitute larger fraction total execution time query 
result number tuples processed node increased order compensate increase startup termination costs 
input relation size tuples thousands optimal degree declustering cpu intensive disk intensive fig 

optimal degree declustering sequential startup degree declustering average join response time secs terminals terminals terminals terminals fig 

joins relations minimum memory allocation sequential startup experiment explores effects sequential startup cpu intensive configuration 
shows relations tuples fully declustered 
reason sequential startup increases query startup time considerably slower cpu configuration effect 
number tuples full declustering optimal increases tuples 
shows non linear relationship relation size optimal degree declustering saw sequential startup disk intensive configuration 
discussion previous experiments explored optimal degree declustering disk intensive cpu intensive situations sequential parallel startup algorithms 
optimal degree declustering highest configuration parallel startup 
lower cpu intensive system sequential startup algorithm 
slowest system cpu intensive sequential startup relation tuples mb fully declustered 
kb data needed node overcome startup termination costs configuration 
results indicate relations typical parallel database system fully declustered best performance 
previous experiments demonstrate optimal degree declustering complex function hardware software parameters 
optimal degree declustering linear relationship relation size disk intensive system parallel startup non linear relationship configurations 
means startup termination costs low system disk intensive simple rule cache context disk sect 
determine optimal degree declustering 
fortunately current hardware trends indicate database systems disk intensive 
cpu speeds increasing average year 
contrast disk bandwidths growing year 
systems disk intensive fast cpus networks reduce startup termination costs 
consequently determining optimal degree declustering easier systems 
remainder cache context disk rule calculate degree declustering relation 
furthermore results disk intensive configuration parallel startup 
minimum memory allocation startup termination costs join queries maximum minimum memory allocation declustering results previous section maximum allocation applicable minimum memory allocation 
shows effect degree declustering response time joins tuple relations minimum memory allocation various system loads 
results confirm full declustering degree declustering provides best response time system loads 
experiments larger relations produced result full declustering best performance effect indices previous experiments full relation scans input relations 
experiment examines effect index scans degree declustering 
figures show performance joins tuple relations clustered index read relation 
index selectivities considered 
results experiment quite different previous ones 
shows selectivity factor optimal degree declustering relation tuples compared previous section 
implies degree declustering reduced index scans read relations 
shows selectivity increased optimal degree declustering increases 
results experiments explained follows 
optimal experiments done smaller relation sizes previous studies yu shown minimum memory allocation justified large relations leads substantial reduction memory consumption 
degree declustering average join response time secs terminals terminals terminals terminals fig 

index selectivity 
effect clustered index scans degree declustering degree declustering average join response time secs terminals terminals terminals terminals fig 

index selectivity 
effect clustered index scans degree declustering degree declustering depends number tuples processed operator 
query clustered index scan selectivity relation processes number tuples file scan tuple relation selectivity optimal degree declustering cases 
similarly optimal degree declustering query clustered index scan selectivity similar optimal degree declustering file scan tuple relation 
general clustered index scan selectivity read relation tuples optimal degree declustering optimal degree declustering file scan relation tuples 
optimal degree declustering relation changes depending selectivity index scans read relation 
optimal degree declustering increases rapidly increase relation size fig 
full declustering provide best performance index selectivity small 
experiments unclustered index scans showed full declustering provides best performance selectivity small omitted 
effect skew experiment explores effect data skew cache context disk rule 
mentioned sect 
degree declustering average join response time secs terminals terminals terminals terminals fig 

skew factor 
effect skew degree declustering degree declustering average join response time secs terminals terminals terminals terminals fig 

skew factor 
effect skew degree declustering effect join product skew considered 
join product skew modeled manner 
assumed skew processing nodes chosen randomly similar scalar skew model walt dewi 
number tuples produced selected node calculated skew factor times average number tuples produced nodes 
example node produces result tuples average skew factor means node experiences join product skew produce tuples 
order ensure number tuples produced irrespective degree declustering skew factor increased linearly degree declustering 
instance skew factor degree declustering changes degree declustering increases respectively 
linearly increasing skew factor degree declustering ensures number tuples produced skew node degree declustering 
figures show effect different degrees skew average response time join queries indices 
cases response time decreases increase degree declustering 
reduction response time small low loads ter represents extremely high degree skew 
realistic distribution scalar skew model increase skew degree declustering gradual 
join join join join store left deep join join join join store right deep fig 

complex query schedules 
reason response time queries dominated response time node experiences join product skew 
number tuples produced skewed node change degree declustering increases response time change significantly 
evident fig 
queries higher skew response time reduction lower 
higher loads performance improves slightly higher degree declustering query load gets spread nodes 
experiment shows full declustering improves performance presence low degrees skew 
improvement higher degrees skew full declustering best performance 
presence skew full declustering helps reducing average response time leads higher response times 
complex queries far considered binary join queries 
section examines effect declustering strategies complex queries 
workload consists single way join query input relation consists tuples 
selection selectivity join selectivity joins allocated maximum memory allocation 
multiple ways scheduling complex query differing amount parallelism pipelining exploited schn chen chen 
experiment left deep right deep scheduling schn considered 
represent extremes query scheduling strategies left deep schedules parallelism limited pipelining schedules highest parallelism maximum pipelining 
query schedules shown fig 

dark edges schedules represent build operation hash join lighter edges represent probe operation 
shows response time complex queries degree declustering input relations increased 
results show increasing degree declustering improves performance irrespective query scheduling strategy full declustering provides best performance left deep trees 
reason increase parallelism higher degrees declustering compensates small increase startup termination costs 
experiment illustrates full declustering input relation size tuples thousands optimal degree declustering cpu intensive disk intensive fig 

complex query response times way join queries tuples input relation improves performance simple binary join queries complex join queries 
similar experiments complex queries containing different numbers multiple joins produced results 
selecting nodes placement relations section presents evaluates alternative strategies deciding placement fragments declustered relation 
previous sections showed simple disk rule determine degree declustering relations 
placement needs determined relations fully declustered rule relation fully declustered simply placed nodes placement decision needed 
part section presents alternative algorithms placing relations fully declustered 
followed detailed performance evaluation resulting complete data placement algorithm variety workloads 
workloads divided partitioned non partitioned workloads 
partitioned workload consists disjoint sets queries access mutually exclusive sets relations 
workload consisting queries submitted separate organizations accessing private databases example partitioned workload 
queries organization isolated placing relations disjoint sets nodes 
disjoint placement relations partition workloads workloads called non partitioned 
example workload partitioned tpc benchmark workload 
workload consists transaction types access data database composed relations 
relation shared multiple transactions making impossible partition workload placing relations disjoint nodes 
section examines effect declustering non partitioned workloads partitioned workloads considered sect 

table 
workload database parameters workload database class terminals think time relation size number join partially declustered tuples fully declustered tuples handling small relations small relations declustered nodes potential source load imbalance placement leads increased load subset nodes 
experiments section compare algorithms placing small relations 
algorithm degree declustering determined cache context disk rule 
algorithms differ order relations chosen placement method select nodes placing relation 
placement algorithms considered random algorithm simplest completely workload independent 
randomly chooses relation placed set nodes place 
round robin round robin algorithm order relations placed random placement relations nodes performed round robin fashion relation placed processors placement relation starts processor 
algorithm workload independent tends spread data uniformly random 
bubba final placement algorithm bubba data placement heuristic cope 
scheme relations placed decreasing order access frequency heat 
node algorithm maintains sum relations placed far node 
relation nodes accumulated heat chosen declustering 
algorithm efficient load balancing requires detailed knowledge workload accurate prediction relation heat 
placement algorithms compared non partitioned workload results determine data placement partitioned workloads 
non partitioned workloads relations non partitioned workload divided categories fully declustered relations partitioned nodes partially declustered relations declustered subset nodes 
experiment compares performance algorithms placing small relations disk intensive configuration 
workload database experiment described table 
database consists tuple relations fully declustered variable number small relations partially declustered 
number tuples small relations vary uniformly tuples 
degree declustering partially number partially declustered relations average join response time secs random round robin bubba fig 

effect number partially declustered relations declustered relations determined cache context rule 
example tuple relation pages declustered disks tuple relation pages declustered disks 
workload consists number concurrently executing hash join queries number queries chosen average disk utilization 
selection inner outer relation query performed follows 
relation partially fully declustered relation equal probability 
relation selected partially declustered relation skewed zipf distribution choose particular partially declustered relation 
distribution selected vary heat partially declustered relations 
hand placement fully declustered relations fixed independent heat relation 
input relation chosen fully declustered chosen randomly uniform distribution set tuple relations 
skewed probabilities defined zipfian distribution randomly assigned various partially declustered relations 
order remove effect random assignment access probabilities relations result point represents average simulation runs different assignment access probabilities partially declustered relations 
shows performance algorithms number partially declustered relations varied 
order stress performance algorithms data read disk memory buffering switched 
effect memory buffering examined separately experiment 
tuples page disk cache context pages disk 
number relations zipf 
shows partially declustered relations random algorithm results highest average query response time 
random algorithm places relations random set nodes order placement randomized 
number partially declustered relations small random algorithm place fragments multiple high heat relations node 
leads load imbalance performance random algorithm worst 
round robin algorithm distributes small relations evenly nodes better performance 
order relations placed random fragments relations high heat placed node 
heat relations spread uniformly 
hand bubba algorithm places relations decreasing order heat attempts place fragments high heat relations disjoint nodes 
technique achieves better load balancing performs best algorithms 
number partially declustered relations increased performance random round robin algorithms improves effect randomness decreases 
just small relations performance round robin algorithm quite similar bubba algorithm performance algorithms nearly identical relations 
experiment shows bubba best placement algorithm simpler algorithms random roundrobin perform number small relations increases 
previous experiment assumed data disk resident 
practice data kept memory resident improve performance lru gray min rule gray bubba memory management cope fragment brow 
shows performance algorithms percentage partially declustered relations kept memory varied 
relations kept memory resident pre reading pinning appropriate percentage data pages small relations memory system startup 
number partially declustered relations fixed value showed largest difference algorithms previous experiment 
point curves residency equal value shown experiment shows maximum difference algorithms 
residency increases performance algorithms similar 
residency algorithms identical performance 
experiments examined performance different placement algorithms handling relations small fully declustered disk rule 
results show bubba algorithm best performance followed round robin random 
detailed workload knowledge required bubba algorithm may available bubba algorithm cases 
cases simplistic round robin algorithm 
algorithm knowledge workload seen perform quite 
addition experiments showed small relations percentage relations memory average join response time secs random round robin bubba fig 

effect memory residency partially memory resident quite case large number small relations performance round robin algorithm nearly identical bubba algorithm 
partitioned workloads experiments shown far included workloads 
section examines partitioned workloads evaluate effect declustering workload isolation 
initially simple class workload studied give insight problem 
results sections study workloads shift changes general class workload 
partitioned workloads consist disjoint components access mutually exclusive sets relations 
workload partitioned placing relations disjoint sets nodes 
determining size partitions created workload component difficult problem 
experiment section studies different partitioning strategies class workload compares performance non partitioned system full declustering 
results study workload shift changes 
workload consists classes transactions queries described detail sect 

experiments conducted node system mb memory disk node 
study performance number nodes dedicated transaction processing increased nodes 
number nodes dedicated query processing decreased 
partitioning method performance compared non partitioned system queries transactions execute nodes 
experiment explores systems percentage system devoted transactions queries varies 
table describes various parameters classes workload 
think times transaction terminals set disk utilization 
example number nodes executing transactions think time think time reduced nodes execute transactions table 
partitioned workload workload database class terminals think time number nodes relations size transaction transaction tuples gb join join tuples gb number transaction nodes partitioned system average transaction response time msecs partitioned non partitioned fig 

transactions number transaction nodes partitioned system average join response time secs partitioned non partitioned number query nodes partitioned system fig 

joins higher transaction load needed achieve disk utilization 
mpl multi programming level query class fixed think time effect varying query mpl explored experiment 
simplified database ease exposition results relations size contain tuples 
class accesses mutually exclusive set relations number relations set chosen occupy disks 
rest experiments simplified database 
selectivity join queries reduced enable inner relations multiple join queries fit memory 
performance classes partitioned non partitioned systems shown figs 

partitioned system transaction load configured disk utilization response time transactions relatively flat number nodes executing transactions increases transaction throughput increases significantly 
performance non partitioned case different 
number nodes processing transactions small average transaction response time lower non partitioned system partitioned system 
number transaction nodes increases transaction load increases interference join query causes average transaction response time increase 
average transaction response time non partitioned system partitioned system nodes dedicated transaction processing partitioned system 
consider performance join query fig 

response time join increases significantly number nodes processing transactions increases consequently number nodes processing queries decreases non partitioned system join response time increases transaction load increases due interference transactions 
increase response time smaller partitioned system join executes nodes reduction join parallelism 
experiment demonstrates executing workload partitioned system non partitioned system improve performance transactions queries 
transaction response times may degrade non partitioned system due interference concurrently executing queries 
experiment explores increase cause system perform worse partitioned system 
fixed number dedicated transaction nodes configuration worst transaction performance non partitioned case previous experiment 
figures show performance declustering schemes number queries increased 
partitioned case transactions execute separate set nodes joins performance remains unaffected query mpl increases 
query response time increases steeply mpl increases 
queries non partitioned system perform better partitioned case result increased parallelism 
interference queries causes transactions suffer average transaction response time increases increase query mpl 
experiment shows query interference issue low query mpl additional mechanism needed control interference higher loads 
evaluated mechanisms limit interference 
disk priority scheduling mechanism gives higher priority short interactive transactions disk queries perform long sequential scans lower priority 
high priority requests serviced disk low priority requests serviced join query mpl partitioned non partitioned fig 

transactions join query mpl average join response time secs partitioned non partitioned fig 

joins absence high priority requests 
significantly reduces query interference perceived transactions disk 
mechanism tandem non sql product engl 

dynamic delay second mechanism limits interference dynamically delaying queries execution 
reduces average execution mpl queries system leaving disk bandwidth transactions real system delay period dynamically set match expected response time user specified determined automatically system optimizer estimates 
purposes experiment queries delayed non partitioned system average response time equal corresponding query response time partitioned system 
query delayed amount time previous query exceeded expected average response time 
detailed discussion setting delay times brow 

combined mechanism combines previous schemes 
priority scheduling disk queries delayed dynamically reduce interference 
note reduction average execution mpl just due closed system 
delay mechanism maximum mpl limit result reduced execution mpl open systems 
join query mpl average transaction response time msecs partitioned non partitioned non part priority non part delay non part combined fig 

transactions join query mpl average join response time secs partitioned non partitioned non part priority non part delay non part combined fig 

joins performance mechanisms shown figs 
show average transaction join query response times function query mpl 
performance partitioned non partitioned schemes shown 
disk priority scheme dashed line diamonds slows queries reduces query interference disk giving higher priority transactions 
reduction interference transactions higher response time partitioned system query mpl greater 
delay scheme dashed line triangles delays queries response times close query response time partitioned case 
reduces interference lowers average transaction response time 
low query mpls scheme achieves lower transaction response times partitioned case 
reduction interference higher query mpls transactions higher response time 
combined scheme dashed line crosses delays queries uses priority disk scheduling performs best able limit interference high query mpls previous experiments demonstrated placement full declustering performs average response time figures reported compared th percentile values results similar qualitatively 
table 
partitioned workload shift changes workload transaction terminals transaction think time query terminals query think time shift shift partitioned data placement high load conditions transactions perform poorly due query interference 
cases simple mechanisms priority disk scheduling dynamic delay queries limit interference outperform partitioned placement 
handling workload shifts mentioned sect 
important factor performance declustering scheme ability provide performance presence workload shifts 
experiment compares performance partitioned placement class workload shift changes 
previous partitioning experiment classes workload transactions joins 
table shows structure classes workload shift 
workload models typical shifts mixed workloads higher transaction processing shift higher query processing second 
shift transaction workload previous experiment terminals think time queries system 
transaction workload halves second shift query workload doubles 
experiment compares performance partitioning scheme allocates nodes transaction processing nodes query processing strategies 
non partitioning strategy labeled non partitioned match similar method previous experiment combined scheme disk priority scheduling query admission delay match query response times partitioned scheme 
second method called non partitioned constant trans uses combined mechanism maintain constant transaction response time shifts 
performance schemes shown figs 

partitioned system transaction response times decrease second shift transaction load decreases query response times increase query workload increases 
partitioned system statically partitions system workloads prevents system adapting dynamic workload changes 
example decrease transaction workload second shift improve performance query class 
hand delay mechanism non partitioned system multiple ways modify performance classes shift 
non partitioned join match scheme achieves query response times identical query response times partitioned system transaction response times better 
non partitioned constant trans scheme hand delays queries transaction response time maintained shifts 
resulting reduction transaction workload second shift allows declustering strategy average transaction response time secs shift shift partitioned non partitioned join match constant trans fig 

effect shift changes transactions declustering strategy average join response time secs shift shift partitioned non partitioned join match constant trans fig 

effect shift changes joins lower join query response times second shift compared response times achieved second shift partitioned non partitioned join match schemes 
experiment shown data placement full declustering perform better partitioned system static workloads better strategy dynamically varying workloads schemes query delaying priority scheduling adapt performance system workload changes 
partitioned placement obtains workload isolation placing data disjoint nodes prevents dynamic mechanisms adapt changes workload 
handling multiple classes final experiment compares performance partitioned non partitioned systems general workload 
database experiment previous experiment relations average transaction response time secs partitioned non partitioned average response time secs partitioned non partitioned average response time secs partitioned non partitioned fig 

multiclass workload tuples 
addition transactions workload query classes different query response times resource requirements 
query class represents class low memory allocation high response times 
queries class allocated minimum memory requirement mb need passes hybrid hash join algorithm execute 
queries second class higher memory allocations lower response times 
queries selection predicates selectivity quarter input tuples selected join queries class allocated maximum memory requirement size inner relation mb require single pass join algorithm execute 
similar previous experiment compare performance partitioned system nodes dedicated transaction processing nodes dedicated query processing non partitioned system 
non partitioned system uses combined delay mechanism control query interference delays queries match response times query class partitioned system 
shows performance schemes 
seen graphs non partitioning achieves query response time partitioning scheme significantly better transaction response time 
results demonstrate flexibility query delay mechanism 
non partitioned system average execution time joins class execution time joins class order match corresponding response times partitioning system delay periods classes changed accordingly queries delayed seconds queries delayed result slowing queries transaction response times improve significantly compared partitioned system 
experiment demonstrates delay mechanism greater flexibility multi class workload different delay period tune performance multiple classes workload 
multi class workload system processors note mentioned sect 
selectivity joins 
selectivity chosen joins class ensure hash table fit memory 
partitioned different workload classes perform significantly better partitioned system 
related wealth research area data placement 
sn database systems teradata dbc tera tandem non sql tand gamma dewi full declustering data placement strategy :10.1.1.113.6798
studies argued partial declustering data placement cope rahm rahm 
result studies partial declustering considered performance studies pena baru 
section examine earlier declustering studies compare 
followed discussion related data placement context system architectures sn parallel database systems 
case partial declustering copeland copeland cope data placement algorithm bubba parallel database system 
scheme copies maintained relation database 
base relation called direct copy 
addition system maintained replica called copy recovery purposes 
data placement bubba simplified choosing degree declustering called relation 
nodes split direct copy relation relative heat access frequency copies 
note splitting nodes direct copies implies partial declustering relation relation declustered nodes actual placement relations performed steps 
step relations highest temperatures heat unit size cached memory 
second step remaining relations placed decreasing order heat nodes accumulated heat 
performance bubba declustering algorithm investigated analytical queueing model 
shown increasing degree declustering improves performance load balancing system cpu 
case increase degree declustering degrades performance due high startup termination communication costs 
results cope long interpreted showing partial declustering preferred declustering technique 
closer look results lead different 
instance analytical study cope mips processors processors bottleneck performance degrades degree declustering increased 
faster processors degree declustering increased performance degrades 
fact authors note processor speeds continue increase respect disk speeds coming decade 
overheads associated declustering diminish 
parallel database machines installed fewer nodes faster processors results cope interpreted advocating full declustering database relations 
drawbacks data placement algorithm cope 
bubba algorithm requires presence copies direct relation 
provide mechanism selecting degree declustering relations degree declustering input parameter placement algorithm 
degree declustering identical relation results show optimal degree declustering varies relation size 
performance bubba placement algorithm depends accuracy access frequencies relations predicted 
database workloads complex dynamic static workload analysis bubba data placement algorithm cases 
padmanabhan padmanabhan studied data placement sn database system 
showed optimal degree declustering different relation degree declustering placement relation performed single integrated algorithm order obtain best possible performance 
data placement shown np complete randomization techniques develop data placement heuristics 
analytical model show partial declustering superior full declustering randomized algorithms perform variety workloads 
study examining performance small database relations 
order show results hold larger relation sizes duplicated results small relation sizes repeated experiments larger relation sizes 
shows response time indexed scan tuple relation selectivity tuples selected 
experimental parameters experiment established consultation author sriram padmanabhan 
number nodes response time secs fig 

index selectivity 
comparison clustered index scan tuple relation number nodes response time secs fig 

index selectivity 
comparison clustered index scan tuple relation results similar results fig 
show response time increases degrees declustering 
note results agree results sect 
show optimal degree declustering clustered index scan selectivity optimal degree declustering file scan relation tuples 
clustered index scan tuple relation selectivity optimal degree declustering 
results show partial declustering desirable declustering 
shown fig 
scan selectivity increases full declustering best option system sizes 
experiments show results hold small query sizes full declustering best option non trivial relation sizes 
rahm marek rahm marek studied processor allocation sn database system rahm rahm 
experiments compared full partial declustering 
results showed partial declustering outperforms full declustering query workloads mixed workloads containing update transactions read que table 
simulation parameter configuration database workload parameter value parameter value number pes relation size tuples cpu speed size tuples page size kb tuple size bytes avg 
disk access time ms selectivity send message instructions result size tuples receive message instructions size result tuples bytes copy kb message access method clustered tree number nodes response time secs full declustering partial declustering fig 

index selectivity 
comparison rahm number nodes response time secs full declustering partial declustering fig 

index selectivity 
comparison rahm ries 
results workloads consisting small queries binary joins input relations tuples selectivity tuples relation selected join 
order show results valid larger relation sizes simulation environment duplicate results rahm re ran similar experiments larger query sizes 
experimental parameters experiments taken rahm shown table 
workload consists multiple binary join queries join relations performance full declustering compared partial declustering strategy relations declustered disjoint set disks proportion size declustered nodes declustered 
number nodes response time secs full declustering partial declustering fig 

index selectivity 
comparison rahm number nodes response time secs full declustering partial declustering fig 

index selectivity 
comparison rahm examine multi user workload selectivity varied 
figures show performance average query response time obtained full partial declustering schemes various selectivities number nodes system increased 
order simulate constant throughput system node selectivity rahm query arrival rates adjusted system size 
shows selectivity corresponding rahm fig 
full declustering performs worse partial system saturates full declustering number nodes increased 
similar phenomenon observed selectivity fig 

rahm marek rahm conclude results similar ones shown figs 
partial declustering better full declustering 
relative performance partial full declustering different number tuples processed queries increased 
compares performance declustering schemes selectivity increased tuples selected relation respectively 
partial declustering better full declustering degree declustering increases kb data node 
selectivity increased fig 
shows full declustering performs partial declustering config 
experiments shown partial declustering option queries extremely small low selectivities rahm rahm valid larger sized queries 
related addition sn systems data placement studied computing environments 
performance full declustering compared scheme places entire relations single disk centralized multi disk file system 
results showed full declustering improves performance high utilization 
data placement file systems studied wolf 
results applicable sn parallel database systems place copies files single disks declustering 
data placement disk arrays studied 
authors developed analytical formulas decide striping unit files 
striping unit file analogous degree declustering relation sn systems formulas directly applicable 
degree declustering sn systems determines degree intra operator parallelism correlation absent disk arrays 
consequently average size requests file determine striping unit intra operator parallelism ignored 
data placement distributed database systems deals issues partitioning assignment ceri 
due high cost accessing remote data main emphasis placement enhance locality increase parallelism 
study considered horizontal partitioning database relations divides tuples relation disjoint sets 
authors proposed vertical partitioning divides attributes relation parallel database systems ceri cope 
performance impact vertical partitioning carefully evaluated current parallel database system uses 
study ignored vertical partitioning issues 
data placement important issue achieving high performance sn parallel database systems 
intelligent data placement enhances performance exploiting parallelism serves powerful tool load balancing 
explored data placement issues detail determining degree declustering placement declustered data system nodes 
results demonstrate current state trends hardware technology degree declustering relations increased penalizing performance disk cache context worth data disk 
rule implies small relations relations database fully declustered 
performance analysis algorithms placing relations small fully declustered 
simple round robin algorithm shown perform variety conditions 
results albeit simple contrary prevailing view research community 
numerous studies shown full declustering leads reduced performance lead believe partial declustering correct solution 
shown study changes technology significantly changed performance tradeoffs 
result full declustering data placement strategy parallel sn database systems 
addition non partitioned workloads examined effect data placement schemes partitioned configurations 
results indicate full declustering conjunction dynamic delay scheme achieve variety response times workload classes 
experiments showed schemes leads flexible system adapt successfully workload changes 
summary results conclusively demonstrate full declustering viable strategy placing relations sn parallel database system 
full declustering provides high parallelism efficient load balancing 
mechanisms query delay conjunction full declustering adapt performance system workload changes resorting expensive strategy data reorganization 
results significant impact query processing issues 
startup termination communication costs dominant factor query processing processor allocation issues simplified queries executing nodes system 
similarly low processing costs affect selection declustering strategy map tuples relation partitions 
various declustering strategies differ degree parallelism database operation 
processing costs low effect different degrees parallelism small 
relative performance different declustering algorithms may similar 
want explore effect data placement complex query processing 
complex query scheduling strategies right deep joins read multiple base relations simultaneously may cause disk interference full declustering 
cases left deep scheduling reads relation time lead interference may better strategy scheduling complex queries 
want validate results simulation study real parallel database systems 

authors kurt brown helpful comments earlier draft 
bitton gray disk shadowing proc 
vldb conf los angeles calif 
bora boral prototyping bubba highly parallel database system ieee transactions knowledge data engineering march 
brow brown carey dewitt mehta naughton resource allocation scheduling mixed database workloads computer sciences technical report de computer sciences university wisconsin madison july 
brow brown carey livny managing memory meet multiclass workload response time goals proc 
vldb conf dublin ireland august 
brow brown mehta carey livny automated performance tuning complex workloads proc 
vldb conf santiago chile september 
ceri ceri distributed databases principles systems mcgraw hill new york ny 
chen chen ming segmented right deep trees execution pipelined hash joins proc 
vldb conf vancouver canada august 
chen chen ming scheduling processor allocation parallel execution multi join queries proc 
th ieee data engineering conf phoenix feb 
cope copeland khoshafian decomposition storage model proc 
acm sigmod conf 
cope copeland data placement bubba proc 
acm sigmod conf chicago ill june 
dewi dewitt implementation techniques main memory database systems proc 
acm sigmod conf boston mass june 
dewi dewitt gamma database machine project ieee transactions knowledge data engineering march :10.1.1.113.6798
dewi dewitt gray parallel database systems high performance database systems cacm june 
dewi dewitt practical skew handling parallel joins proc 
pdis conf san diego calif january 
foster file assignment computer network acm computer surveys 
engl load balancing batch interactive queries highly parallel environment proc 
ieee compcon conf san francisco calif february 
faloutsos bhagwat declustering fractals proc 
pdis conf san diego calif january 
gerber dewitt multiprocessor hash join algorithms proc 
vldb conf stockholm sweden august 
ghandeharizadeh physical database design multiprocessor systems phd thesis university 
ghandeharizadeh dewitt performance analysis alternative multi attribute declustering strategies proc 
acm sigmod conf san diego calif june 
graefe volcano extensible parallel dataflow query processing system computer science technical report oregon graduate center beaverton ore june 
gray gray minute rule trading memory disk accesses byte rule trading memory cpu time proc 
acm sigmod conf san francisco calif may 
haas haas starburst mid flight dust clears ieee trans 
knowledge data eng march 
hua hua lee adaptive data placement scheme parallel database computer systems proc 
vldb conf brisbane australia 
hua hua lee handling data skew multiprocessor database computers partition tuning proc 
vldb conf barcelona spain september 
ibm ibm product announcement disk drives ibm november 
kits kitsuregawa ogawa bucket spreading parallel hash new robust parallel hash join method data skew super database computer scd proc 
vldb conf brisbane australia august 
livny khoshafian boral multi disk management algorithms proc 
acm sigmetrics conf alberta canada may 
mehta dewitt dynamic memory allocation multiple query workloads proc 
vldb conf dublin ireland august 
mehta resource allocation parallel shared database systems phd 
thesis university wisconsin 
navathe ra vertical partitioning database design graphical algorithm proc 
acm sigmod conf portland ore june 
ng ng faloutsos sellis flexible buffer allocation marginal gains proc 
acm sigmod conf denver may 
omiecinski performance analysis load balancing hash join algorithm shared memory multiprocessor proc 
vldb conf barcelona spain september 
valduriez principles distributed database management systems prentice hall 
padmanabhan data placement shared parallel database systems phd 
thesis cse tr university michigan 
para release release notes paragon xp system intel beaverton ore october 
rahm rahm marek analysis dynamic load balancing strategies parallel shared database systems proc 
vldb conf dublin ireland august 
rahm rahm parallel query processing shared disk database systems proc 
th international workshop asilomar calif september 
ries ries epstein evaluation distribution criteria distributed database systems ucb erl technical report uc berkeley may 
schn schneider dewitt tradeoffs processing complex join queries hashing multiprocessor database machines proc 
vldb conf melbourne australia august 
csim users guide mcc technical report 
act microelectronics computer technology austin tex march 
selinger predictions challenges database systems year proc 
vldb conf dublin ireland august 
sell sellis multiple query optimization acm tods march 
naughton shared virtual memory parallel join processing proc 
acm sigmod conf washington dc may 

ncr personal communication 
tand tandem performance group benchmark non sql debit credit transaction proc 
acm sigmod conf chicago ill june 
tera teradata dbc data base computer system manual teradata document 
release november 
walt walton taxonomy performance model data skew parallel joins proc 
vldb conf barcelona spain september 
weikum dynamic file allocation disk arrays proc 
acm sigmod conf denver may 
weikum tuning striping units disk array file systems proc 
ride workshop phoenix february 
wilschut apers parallelism mainmemory dbms performance prisma db proc 
vldb conf vancouver canada august 
wolf wolf placement allocation program practical solution disk file assignment problem proc 
acm sigmetrics conf 
wolf wolf effective algorithm parallelizing hash joins presence data skew proc 
th ieee data engineering conf kobe japan april 
young swami family round robin partitioned parallel external sort algorithms research report rj ibm research division november 
yu yu cornell buffer management return consumption multi query environment vldb journal january 
zipf zipf human behavior principle effort addison wesley reading mass 
