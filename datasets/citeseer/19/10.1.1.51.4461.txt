constructive theory refinement knowledge neural networks rajesh parekh honavar artificial intelligence research group department computer science hall iowa state university ames ia 
cs iastate edu knowledge artificial neural networks offer approach connectionist theory refinement 
algorithm refining extending domain theory incorporated knowledge neural network constructive neural network learning algorithms 
initial domain theory comprising propositional rules translated knowledge network threshold logic units 
domain theory modified dynamically adding neurons existing network 
constructive neural network learning algorithm add train additional neurons sequence labeled examples 
propose novel hybrid constructive learning algorithm tiling pyramid constructive learning algorithms allows knowledge neural network handle patterns continuous valued attributes 
results experiments non trivial tasks ribosome binding site prediction financial advisor show algorithm compares favorably algorithms connectionist theory refinement terms generalization accuracy network size 
inductive learning systems attempt learn concept description sequence labeled examples 
presence domain specific prior knowledge domain theories concept learned greatly enhance performance inductive learning system 
prior knowledge assist cutting training time result system superior generalization capability compared system learned inductively examples 
prior knowledge inaccurate incomplete 
inductive theory refinement process modifying domain theories help labeled examples 
inductive theory refinement systems proposed literature broadly classified symbolic inductive logic programming connectionist approaches depending underlying learning model :10.1.1.35.951
focus connectionist theory refinement approaches 
knowledge artificial neural networks offer attractive framework combining domain specific prior knowledge usually form propositional rules connectionist inductive learning 
knowledge neural networks find applications datamining initial domain theory available 
kbann algorithm proposed towell shavlik provides framework translating initial domain knowledge called domain theory neural network topology refines theory backpropagation learning algorithm 
practice domain theories incomplete inaccurate 
kbann limited fact modify network topology 
prevents incorporation new rules restricts algorithm ability compensate existing inaccuracies 
constructive neural network learning algorithms offer approach dynamically constructing near minimal neural network architectures pattern classification tasks 
constructive algorithms offer advantages conventional backpropagation style learning approaches ffl obviate need ad hoc priori choice network topology 
ffl guaranteed converge zero classification errors finite non contradictory datasets 
ffl elementary threshold logic units trained perceptron style weight update rules 
ffl involve extensive parameter fine tuning 
constructive algorithms lend design knowledge neural networks 
domain theory translated initial network topology comprising 
new rules incorporated inaccuracy existing rules corrected dynamically adding new network 
new trained sequence labeled examples 
discuss approach constructive learning algorithms knowledge neural networks 
section ii places perspective related knowledge neural networks 
section iii describes incorporation domain theory initial network topology constructive learning algorithm dynamically grow knowledge network 
section iv describes results experiments non trivial datasets viz 
ribosome financial advisor 
section concludes summary gives directions research 
ii 
related constructive learning method dynamically adding neurons initial knowledge network studied fletcher 
approach starts initial network representing domain theory modifies theory training single hidden layer labeled training data 
resultant network topology depicted fig 

method uses hyperplane detection examples construct single hidden layer trains new output unit pocket algorithm 
output unit additional units domain theory input units fig 

knowledge neural network approach similar fletcher method 
constructing single hidden layer allow constructive learning algorithm build network hidden layers necessary initial network representing domain theory 
fig 

provides general framework incorporating domain knowledge constructive neural network learning algorithm 
performance different constructive learning algorithms differs quite significantly different datasets advantageous scheme allows construction appropriate network topology augment initial network limiting network construction single hidden layer 
neural network domain theory input units constructive learning fig 

constructive learning knowledge networks opitz shavlik extensively studied constructive approaches theory refinement :10.1.1.121.3039
topgen algorithm introduces new rule kbann heuristically searching possible ways adding neurons determine network topology best refines initial domain theory 
topgen performs beam search space network architectures 
domain theory translated initial network called kbann 
kbann trained backpropagation placed queue 
step best network terms cross validation accuracy picked queue 
new networks created strategically adding nodes different locations original network 
networks trained placed queue 
best network topology returned prespecified time interval 
regent algorithm broadens types networks searched topgen performing genetic search 
idea create population networks perturbing initial kbann different ways 
step evolution new networks created suitably designed genetic operators 
offsprings scored cross validation set placed population 
network highest score reported current best network 
topgen regent evaluated datasets human genome project perform better compared standard backpropagation kbann algorithm 
approach considerably simpler topgen regent 
construct single network population networks constructed topgen regent 
impact training time knowledge networks 
topgen regent reportedly taken days search networks report best 
hand approach requires minutes cpu time training 
related issue training time topgen regent expensive backpropagation style training simple perceptron type weight update rules approach 
backpropagation algorithm effective networks large number layers propagated error tends diffuse considerably layer 
topgen regent allow weight changes part network incorporates original domain theory 
possibility weight changes completely alter original rules embedded neural network 
approach leaves initial neural network representing domain theory unchanged 
domain theory revision performed constructively adding new neurons network training 
approach preserves original domain theory 
datasets available university wisconsin madison site www ftp ftp cs wisc edu machinelearning shavlik group datasets 
iii 
constructive knowledge neural network learning algorithm embedding domain theory neural network symbolic knowledge encoding procedure translates domain theory form propositional rules network 
weights individual chosen satisfy rules 
example consider simple financial advisor rule base due table sav invest stocks dep sav sav assets hi sav dep earn steady debt lo sav dep dep sav assets income assets hi income dep dep debt pmt income debt lo table financial advisor rule base fig 
depicts initial network topology corresponding domain theory 
network computes bipolar function outputs gamma weighted sum inputs 
neurons hidden layer encode rules rule base 
lone second hidden layer computes logical function encodes rule 
constructive neural network learning algorithm constructive neural network learning algorithm augment initial network topology 
approach allows commonly constructive neural network learning algorithms 
purpose experiments novel hybrid algorithm combines features tiling pyramid learning algorithms 
tiling algorithm constructs strictly layered network 
layer maintains master neuron responsible classifying training patterns 
master neuron correctly classify training patterns ancillary neurons added trained achieve faithful representation pattern set 
faithful representation pattern set indicates patterns belonging different output classes output representation 
faithful layer constructed tiling algorithm trains master neuron layer 
master neuron connected neurons previous layer 
new layer similarly trained achieves faithful representation patterns 
convergence algorithm demonstrated showing successive master neuron reduces number mis classifications training set 
interested reader referred detailed description tiling algorithm extensions handle pattern sets real valued attributes multiple output categories 
pyramid algorithm successively adds new layers network 
newly added layer network new output layer 
neurons new layer connected input layer previously added layers network 
convergence pyramid algorithm proved demonstrating total classification error training set decreases added layer 
original pyramid algorithm relies patterns binary bipolar attributes 
extension pyramid algorithm handle real valued attributes requires pre processing pattern set 
individual patterns normalized projected parabolic surface guarantee convergence 
alternatively continuous valued features discretized 
discretization quantization involves mapping pattern space equivalent bipolar binary representation gamma typically 
hybrid constructive learning algorithm careful observation operation tiling algorithm indicates layer tiling algorithms performs quantization input patterns 
outputs layer discrete valued gamma 
faithful representation pattern set ensures quantization scheme introduce inconsistencies 
approach preserves nearness properties input patterns sense quantized representations patterns similar attribute values similar 
layer tiling algorithm quantization layer achieve faithful discrete representation training patterns 
discretized representation training set pyramid learning algorithm 
experiments unpublished shown hybrid tiling pyramid algorithm works practice outperforms tiling pyramid algorithms independently 
algorithm input initial domain theory embedded neural network set training patterns 
output trained augmented neural network incorporates modified domain theory 

train layer tiling algorithm create faithful representation training patterns 
connected input neurons neurons belonging initial network topology 

compute output representation training patterns faithful layer constructed step 
quantized representation training patterns train pyramid network tiling layer step 
iv 
experimental results report results experiments sites recognition financial advisor rule base problems 
recognition ribosome binding sites dna studied part human genome project 
ribosome dataset comprises imperfect domain theory income dependents assets savings earn steady debt pmt assets hi invest stocks savings dep sav dep debt lo fig 

embedding financial advisor domain theory neural network developed dna expert set labeled examples indicating particular dna sequence contains ribosome binding site 
financial advisor rule base shown table set patterns training testing correctly match financial advisor rule base randomly generated case experiments performed fletcher 
hybrid tiling pyramid constructive learning algorithm augment initial domain knowledge 
hybrid network trained thermal perceptron learning rule 
trained epochs initial weights chosen randomly gamma initial temperature thermal perceptron set 
ribosome binding sites ribosome dataset contains imperfect domain theory 
fold cross validation exactly folds experiments performed opitz shavlik augment domain theory 
fold cross validation generalization accuracy measured network representing imperfect domain theory prior performing theory refinement 
report average generalization accuracy average network size standard deviations runs table ii 
seen theory refinement provide improvement generalization accuracy 
compare results results topgen regent reported :10.1.1.121.3039
note standard deviations available experiments topgen regent 
tiling pyramid topgen regent test size test size test size sigma sigma table ii experiments ribosome dataset financial advisor incomplete knowledge financial advisor rule base modeled pruning certain rules antecedents rule base 
incomplete domain theory augmented constructive learning 
experiments follow performed fletcher 
table iii summarize average generalization test patterns average network size runs different pruning points 
generalization accuracy corresponding network prior theory refinement rules reported 
assets hi rule see substantial increase generalization accuracy theory refinement 
generalization accuracy network assets hi rule significantly high slight worsening generalization performance refining theory assets hi rule pruned attributed fitting 
table iv results experiments note standard deviations results experiments available 
pruning point tiling pyramid rules test size test dep sav sigma sigma assets hi sigma sigma dep sigma sigma debt lo sigma sigma sav sigma sigma sigma sigma table iii experiments financial advisor rule base pruning point rules test hidden units test constructed dep sav assets hi dep debt lo sav table iv experiments financial advisor rule base summary discussion approach constructive learning knowledge networks 
knowledge networks allow domain theory embedded initial neural network architecture 
approach uses constructive learning algorithm dynamically add train augment initial domain theory 
experimental results demonstrate performance hybrid tiling pyramid algorithm compares favorably algorithm financial advisor rule base terms generalization accuracy network size 
generalization accuracies theory refinement exhibit significant increase performance compared generalization accuracy imperfect domain theory 
algorithm generalization accuracy ribosome binding site dataset slightly worse achieved topgen regent 
hybrid tiling pyramid algorithm generates significantly smaller networks compared topgen regent 
train single network opposed population networks training time significantly training times topgen regent 
merits framework constructive theory refinement knowledge neural networks 
ffl commonly approaches connectionist theory refinement approach allows potentially constructive neural network learning algorithm knowledge framework 
specifically hybrid tiling pyramid algorithm experiments gave satisfactory results 
ffl training simple perceptron style learning rule 
training time networks trained approach considerably compared connectionist theory refinement methods backpropagation type learning algorithms 
ffl extraction rules trained networks actively pursued area research finds direct applicability datamining 
approach uses elementary operation easily translated rules sigmoid neurons typically backpropagation type algorithms 
original rules left uncorrupted approach comprehensibility rules extracted trained network improve significantly 
results rule extraction trained neural networks reported 
topgen heuristically finds effective places add nodes knowledge bases 
idea training new neurons compensate errors existing ones learning algorithm 
interest similar constructive learning approach conjunction current approach 
serve twin objectives refining inaccuracies domain theory adding new rules original theory 
type rules incorporated network limited propositional rules 
mechanism handling uncertainty rules 
extension knowledge neural networks handle rules predicate logic handle uncertainty adjusting weights individual connections clearly interest 
research partially supported national science foundation iri iri honavar 
ourston mooney theory refinement combining analytical empirical methods artificial intelligence vol 
pp 

thompson langley iba background knowledge concept formation proceedings eighth international machine learning workshop pp 

muggleton inductive logic programming academic press san diego :10.1.1.35.951
towell shavlik refinement approximate domain theories knowledge neural networks proceedings eighth national conference artificial intelligence boston ma pp 

fu integration neural heuristics knowledge inference connection science vol 
pp 

fu knowledge connectionism refining domain theories ieee transactions systems man cybernetics vol 

towell shavlik knowledge artificial neural networks artificial intelligence vol 
pp 

rumelhart hinton williams learning internal representations error propagation parallel distributed processing explorations microstructure cognition vol 
foundations 
mit press cambridge massachusetts 
honavar generative learning structures processes generalized connectionist networks ph thesis university wisconsin madison 
honavar uhr generative learning structures processes information sciences vol 
pp 

parekh yang honavar constructive neural network learning algorithms multi category real valued pattern classification tech 
rep isu cs tr department computer science iowa state university submitted review ieee transactions neural networks 
fletcher combining prior symbolic knowledge constructive neural rk learning connection science vol 
pp 

gallant perceptron learning algorithms ieee transactions neural networks vol 
pp 
june 
opitz shavlik dynamically adding symbolically meaningful nodes knowledge neural networks knowledge systems vol 
pp 

opitz shavlik connectionist theory refinement genetically searching space network topologies journal artificial intelligence research vol :10.1.1.121.3039
pp 

artificial intelligence design expert systems benjamin cummings redwood city ca 
nadal learning feed forward networks tiling algorithm phys 
math 
gen vol 
pp 

yang parekh honavar constructive neural network learning algorithm multi category pattern classification proceedings world congress neural networks san diego pp 

frean thermal perceptron learning rule neural computation vol 
pp 

towell shavlik extracting rules knowledgebased neural networks machine learning vol 
pp 

frean algorithm method constructing training feedforward neural networks neural computation vol 
pp 

