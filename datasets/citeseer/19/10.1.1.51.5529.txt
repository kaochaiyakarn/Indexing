recurrent networks state machines iterated function systems 
john kolen laboratory ai research department computer information science ohio state university columbus oh kolen cis ohio state edu feedforward neural networks process information performing fixed transformations representation space 
recurrent networks hand process information quite differently 
understand recurrent networks confront notion state recurrent networks perform iterated transformations state representations 
researchers recognized difference suggested parallels recurrent networks various automata 
demonstrate common notion deterministic information processing necessarily hold deterministic recurrent neural networks dynamics sensitive initial conditions 
second link mathematics recurrent neural network models iterated function systems 
link points model independent constraints recurrent network state dynamics explain universal behaviors recurrent networks internal state clustering 
recurrent networks state machines current models human intellectual competence demand potential store act infinite collection internal representations 
inspired pioneering mcculloch pitts simulation proofs universal computation recurrent networks appeared years 
projects clearly supports claim representational adequacy recurrent networks showing recursive computation arise design 
projects provide little insight detection computation occurring system 
words don tell recognize computation happening eyes 
attribution computational behavior system traditionally rests discovery isomorphism states physical dynamical system information processing states computational model 
certain voltage levels particular wires cpu instance isomorphic bits processed machine 
modeler measure current state process sufficient resolution justify isomorphism 
models capable recursive computation information processing state demand unbounded accuracy modeler measurements 
researchers studying computational power recurrent networks followed tradition 
researchers clustered internal state vectors drew correspondences clusters deterministic state machine generating training data 
extended summarized variants dynamic state partitioning 
giles collaborators arbitrary partitions state space build clusters employ statistical clustering techniques extract state clusters 
methods shares problem multiple inductions single recurrent network produce state machine interpretations 
crutchfield young suggests information processing states assigned highly precise examination internal dynamics 
assuming stream noisy numerical measurements construct models periodic samplings single decision boundary 
binary sequence collect requires computational description kind automaton generated sequence numerical measurements described mathematically 
minimal finite state automaton induced sequence discrete measurements provides realistic assessment intrinsic computational complexity physical system observation 
important distinction drawn methods crutchfield young multitude researchers studying recurrent networks 
dynamical systems theory particularly study systems producing chaotic behavior demonstrates significant state information buried deep system initial conditions 
sensitivity initial conditions implies best way measure system state simply observe behavior long periods time retroactively determine state 
methods system identification employ approach 
instance takens method embedding single dimensional measurement higher dimensional space provide judgment underlying system dimensionality 
crutchfield young extended method computational understanding systems reconstructing computational dynamics system sequence boolean measurements 
counterexample methods assessing computational ability system described 
relies identification isomorphism information processing ip states computational model observables system 
method induces computational model sequence behaviors 
example demonstrates clustering observable states lead extraction nonexistent deterministic states recurrent networks 
recall conditions deterministic ip states 
states generate output responses possible inputs 
clustering methods shall see establish existence ip states dealing systems dynamics sensitive initial conditions 
consider recurrent network state units vector dynamics specified equation output network zero thresholding state unit 
current state right boundary output positive 
output negative left 
illustrates happens network iterates large set initial conditions selected small ball radius 
iteration points migrate lower corner state space dimension 
iterations original ball shape longer visible 
points spread dimensional sheet state space 
iterations see set network states reaching extent state space 
behavior ubiquitous systems sensitive initial conditions 
assumption initial neighborhood defined information processing state deterministic system clearly invalidated vast set unique output sequences follow state assistance inputs 
iterated function systems section provides brief overview new branch mathematics known iterated function theory 
limit behavior single affine transformations determined analytically examining eigenvalues transformations iterated function theory account limit behavior systems affine transformations 

indicates inner product vector appended vector 
tanh ax basic iterated function systems theory formally iterated function system ifs metric space finite collection affine transformations attractor ifs set points ifss fascinating limit behavior single transformation just point limit set union transformations extremely complex recursive structure 
ifss responsible structure seen sierpinski triangle fractals 
metric space combination set distance metric consider unit square euclidean distance function metric space 
functions equation map unit square quadrants unit square 
focus single transform find exists limit iterating function starting point unit square 
instance fixed point transform 
attracting fixed point iteration draws points closer time 
system transformation collapse single point 
attracting set infinite recursive structure 
single transformation shrinks entire limit set fourth sized copy original union operation different copies form original image 
process illustrated 
attractor structure depends transients transformation 
words limit behavior individual transformations contributes little emergent shape attracting set 
simple example demonstrate fact 
transformations comprising sierpinski triangle ifs attracting limit point 
points limits transformations equation attractor set transformations finite set looks sierpinski triangle 
node node node node node node start iterations iterations iterations state space recurrent network state transitions sensitive initial conditions 
important concept understanding iterated function systems space 
space complete metric space set compact subsets excluding empty set 
compact set set closed totally bounded 
distance points space equation 
metric measures distance sets maximal distance point nearest point space allows consider subsets single points 
lets view set iterated transformations metric space single transformation space 
theory laid barnsley shows ifss composed transformations go fixed points produce fixed point dynamics space 
instance shown sierpinski triangle point sierpinski ifs reduces distance starting subset sierpinski triangle 
random iteration interesting side effect unique attractor barnsley ifss approximations attractor easy construct 
random iteration known chaos game produces attractor approximations rapidly 
take initial point metric space create sequence points randomly selecting ifs transformation iteration 
initial portion sequence ignored transients resulting union sequence remainder finite approximation attractor 
easy show iteration distance current point attractor geometrically decreases approaches zero 
random transformation enforces coverage attractor infinite sequence points generated way dense attractor 
random iteration algorithm sensitive method selecting transformations iteration 
probability selecting particular transformation lead abnormal distribution points approximating attractor 
second selection generator ergodic selection independent previous selections 
language theory provides alternative way describing random iteration algorithm 
sequence transformations produced random iteration compactly described stochastic finite state generator 
generator probabilities associated transitions produce variations sequence necessary production approximation 
probabilities generators produce uniform distribution sequences transform transform transform transform difference limit single transformation point limit collection transformations sierpinski triangle 
actual affine transformations defined equation 
infinity superscript denotes limit infinite sequence function compositions 
subset metric space sup inf set possible infinite sequences 
attractors possible defining generators produce subsets recurrent networks iterated function systems behavior recurrent networks symbolic processing applications explained terms iterated function systems 
section provides demonstration emergence complex recurrent network state space interaction simple state spaces 
recurrent network selected demonstration second order recurrent network know sequential cascaded network scn 
scn described dynamic equations show equation 
output vector time state vector input vector dimensional weight matrices describing state state state output transformations performed scn 
function sigmoid function 
learning takes place scn adjusting weight matrices modified form back propagation 
training continues network successfully generates desired outputs overtraining forces network qualitative change fixed point attractor limit cycle 
pollack initially applied task formal language recognition networks apparently induced regular languages 
internal states dynamical recognizers finite 
dimensional analysis revealed fractal dimension set possible internal states indicating infinitely structured set 
symbolic applications set predefined input vectors finite may write dynamic individual transformations reduced copies attractor 
union individual copies original attractor 
attractor form shown equation 
parameters dimensional matrices selected input pattern transfer function linear set inputs finite scn mathematically equivalent set affine transformations ifs 
scn adds sigmoid squash internal state small subregion preventing activations growing infinity allowing non fixed point attractors limit cycles chaos individual transformations 
display attractor formed selection transformations play chaos game plotting sequence state points transformed sequences transformations length 
pollack trained scn recognize regular languages set positive negative examples 
shows state space dissection scn trained examples language 
network learned target language exhibit state space marked scn states 
breaking scn transformations input symbol examine individual transformations produced magic mushroom 
iteration transformation maps state space thin sheet diagonal iteration transformation pushes state space ceiling sixth twist 
limit transformation produces fixed point transformation gets closer closer period attractor 
mushroom example demonstrates existence fixed point limit cycle attractors individual inputs 
chaotic aperiodic attractors possible scn state space 
illustrates effects increasing parameter asymptotic trajectories function shown equation 
resulting plot known bifurcation diagram captures underlying structure period doubling phenomena 
limits period doubling systems attractor chaotic implying infinite state behavior 
function expressed scn input selected transformation spreading iterated computation time steps equation third element second elements dissection sequential cascaded network trained accept 
state space transform transform transform transform scn states ax tanh ax tanh tanh ax tanh ax tanh discussion learning systems relying recurrent networks carry difficult burden unclear networks processing learning 
assumed networks learning simulate finite state machines state dynamics begun extract finite state machines networks state transition dynamics 
extraction methods employ various clustering techniques partition internal state space recurrent network finite number regions corresponding states finite state automaton 
demonstrated part clustering hidden unit activations recurrent network state space provides incomplete information regarding ip state network 
ip states determine behavior encapsulate input history 
network state transformations exhibit sensitivity initial conditions generate disparate futures state clusters sizes 
second part presents ifs theory shows explain recurrent network state dynamics 
linking ifss recurrent networks existing constraints network dynamics independent network models evident 
assuming finite set inputs case symbolic domains describe recurrent network models finite collection nonlinear state transformations interaction transforms produces complex state spaces recursive structure 
limit behavior collection transformations recurrent networks symbolic applications complex union individual transformations 
input driven recurrent network behaves random iteration algorithm 
infinite input sequence generates sequences points dense state space attractor transformations contractive 
demonstration scn models produce similar ifs behaviors long network input selects transformations 
ifs approach explains phenomena state clustering recurrent networks 
report significant clustering simple recurrent networks training grammar prediction task 
set random transformations normally reduce volume recurrent networks state space place upper bound distance transformed points 
upper bound significant effect clustering especially transformations map small regions state space 
task required clusters arranged particular structure constrained single layer network generating predictions 
utility network states directly related complexity mechanism measuring state 
computational system requirements 
input driven 
second computational system needs dynamically change state 
third generate output 
written importance state dynamics requirements capable elevating behavioral complexity 
systems produce perform transductions periodic oscillations complex signals 
different measurement devices sample mapping function bifurcation diagram results changes control parameter produce increases number apparent ip states observed complexity class system 
words recurrent network states ip states require appropriate context elevate ip hood 
context consists set input sequences observation method generating outputs 
recurrent network state dynamics may described ifs ip interpretation involve holistic combination set possible inputs state dynamics output generation mechanism network 
acknowledgments supported office naval research 
inspiration came endless discussions jordan pollack 
giles miller chen sun chen lee extracting learning unknown grammar recurrent neural networks advances neural information processing systems pp 

watrous kuhn induction finite state automata second order recurrent networks advances neural information processing systems pp 

giles das sun learning context free grammars capabilities limitations recurrent neural network external stack memory proceedings fourteenth annual conference cognitive science conference pp 

barnsley fractals academic press san diego ca 
chomsky syntactic structures hague 
newell simon computer science empirical inquiry symbols search communications association computing machinery vol 
pp 

mcculloch pitts logical calculus ideas nervous activity bulletin mathematical biophysics vol 
pp 

franklin garzon neural network implementation turing machines proceedings second institute electrical electronics engineers international conference neural networks 
pollack connectionist models natural language processing university illinois urbana champaign ph thesis 
siegelmann sontag computational power neural networks report rutgers center systems control 
field computation brain technical report cs university tennessee 
ashby cybernetics chapman hall london 
cleeremans servan schreiber mcclelland finite state automata simple recurrent networks neural computation vol 
pp 

crutchfield young computation onset chaos entropy complexity physics information zurek ed addison reading 
takens detecting strange attractors turbulence lecture notes mathematics springer verlag berlin 
crutchfield farmer packard chaos scientific american vol 
pp 

pollack induction dynamical recognizers machine learning vol 
pp 

mandelbrot fractal geometry nature freeman san francisco ca 
kolen ph thesis progress ohio state university 
servan schreiber cleeremans mcclelland encoding sequential structure simple recurrent networks technical report cmu cs 
elman finding structure time cognitive science vol 
pp 

kolen pollack apparent computational complexity physical systems proceedings fifteenth annual conference cognitive science society laurence erlbaum 
