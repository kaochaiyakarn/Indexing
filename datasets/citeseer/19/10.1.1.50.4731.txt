fast robust fixed point algorithms independent component analysis hyv rinen helsinki university technology laboratory computer information science box fin hut finland email hyvarinen hut fi appear ieee trans 
neural networks january independent component analysis ica statistical method transforming observed multidimensional random vector components statistically independent possible 
combination dioeerent approaches linear ica comon information theoretic approach projection pursuit approach 
maximum entropy approximations dioeerential entropy introduce family new contrast objective functions ica 
contrast functions enable estimation decomposition minimizing mutual information estimation individual independent components projection pursuit directions 
statistical properties estimators contrast functions analyzed assumption linear mixture model shown choose contrast functions robust minimum variance 
introduce simple xed point algorithms practical optimization contrast functions 
algorithms optimize contrast functions fast reliably 
central problem neural network research statistics signal processing nding suitable representation transformation data 
computational conceptual simplicity representation sought linear transformation original data 
denote xm zero mean dimensional random variable observed dimensional transform 
problem determine constant weight matrix linear transformation observed variables wx suitable properties 
principles methods developed nd linear representation including principal component analysis factor analysis projection pursuit independent component analysis transformation may de ned criteria optimal dimension reduction statistical interestingness resulting components simplicity transformation criteria including application oriented ones 
treat problem estimating transformation linear independent component analysis ica 
name implies basic goal determining transformation nd representation transformed components statistically independent possible 
method special case redundancy reduction 
promising applications ica blind source separation feature extraction 
blind source separation observed values correspond realization dimensional discrete time signal 
components called source signals usually original uncorrupted signals noise sources 
sources statistically independent signals recovered linear mixtures nding transformation transformed signals independent possible ica 
feature extraction th feature observed data vector ica feature extraction motivated results neurosciences suggest similar principle redundancy reduction explains aspects early processing sensory data brain 
ica applications exploratory data analysis way closely related method projection pursuit 
new objective contrast functions algorithms ica introduced 
starting information theoretic viewpoint ica problem formulated minimization mutual information transformed variables new family contrast functions ica introduced section 
contrast functions interpreted viewpoint projection pursuit enable sequential extraction independent components 
behavior resulting estimators evaluated framework linear mixture model obtaining guidelines choosing contrast functions contained introduced family 
practical choice contrast function discussed statistical criteria numerical pragmatic criteria section 
practical maximization contrast functions introduce novel family xed point algorithms section 
algorithms shown appealing convergence properties 
simulations con rming usefulness novel contrast functions algorithms reported section real life experiments methods 
drawn section 
contrast functions ica ica data model minimization mutual information projection pursuit popular way formulating ica problem consider estimation generative model data observed dimensional vector dimensional latent random vector components assumed mutually independent constant theta matrix estimated :10.1.1.131.165
usually assumed dimensions equal assumption rest 
noise vector may 
matrix de ning transformation obtained pseudo inverse estimate matrix non gaussianity independent components necessary identi ability model see 
comon showed obtain general formulation ica need assume underlying data model 
de nition concept mutual information 
de ne dioeerential entropy random vector yn density follows gamma log dy dioeerential entropy normalized give raise de nition negentropy appealing property invariant linear transformations 
de nition negentropy gauss gamma gauss gaussian random variable covariance matrix negentropy interpreted measure 
concept dioeerential entropy de ne mutual information scalar random variables 
mutual information natural measure dependence random variables 
particularly interesting express mutual information negentropy constraining variables uncorrelated 
case yn gamma mutual information information theoretic measure independence random variables natural criterion nding ica transform 
de ne ica random vector invertible transformation wx matrix determined mutual information transformed components minimized 
note mutual information independence components multiplication components scalar constants 
de nition de nes independent components multiplicative constants 
constraint adopted 
constraint strictly necessary simpli es computations considerably 
negentropy invariant invertible linear transformations obvious nding invertible transformation minimizes mutual information roughly equivalent nding directions negentropy maximized 
formulation ica shows explicitly connection ica projection pursuit 
fact nding single direction maximizes negentropy form projection pursuit interpreted estimation single independent component 
contrast functions approximations negentropy de nition ica simple estimate negentropy dioeerential entropy needed 
new approximations developed maximum entropy principle 
shown approximations considerably accurate conventional cumulant approximations 
simplest case new approximations form efg gamma efg practically non quadratic function irrelevant constant gaussian variable zero mean unit variance standardized 
random variable assumed zero mean unit variance 
symmetric variables generalization approximation obtained choice function deferred section 
approximation negentropy gives readily new objective function estimating ica transform framework 
nd independent component projection pursuit direction maximize function jg jg efg gamma efg dimensional weight vector constrained ef scale arbitrarily 
independent components estimated scheme see section 
second approach minimizing mutual information unit contrast function simply extended compute matrix 
recall mutual information minimized constraint decorrelation sum components maximized 
maximizing sum unit contrast functions account constraint decorrelation obtains optimization problem maximize jg wrt 
constraint ef ffi jk maximum vector gives rows matrix ica transformation wx 
de ned ica estimator optimization problem 
analyze properties estimators giving guidelines choice propose algorithms solving optimization problems practice 
analysis estimators choice contrast function behavior ica data model subsection analyze behavior estimators data follows ica data model square mixing matrix 
simplicity consider estimation single independent component neglect decorrelation 
denote vector obtained maximizing jg 
vector estimator row matrix gamma consistency prove locally consistent estimator component ica data model 
prove theorem theorem assume input data follows ica data model suoeciently smooth function 
set local maxima jg constraint ef includes th row inverse mixing matrix corresponding independent component ful lls efs gamma efg gamma efg derivative standardized gaussian variable 
theorem considered corollary theorem 
condition theorem true reasonable choices distributions particular condition ful lled distribution non zero kurtosis 
case proven spurious optima 
asymptotic variance asymptotic variance criterion choosing function contrast function 
comparison say traces asymptotic covariance matrices estimators enables direct comparison mean square error estimators 
evaluation asymptotic variances addressed related family contrast functions 
fact seen results valid case theorem theorem trace asymptotic variance minimized form opt log density function arbitrary constants 
simplicity choose opt log 
optimal contrast function obtained maximum likelihood approach infomax approach 
identical results obtained algorithm 
theorem treats unit case multi unit case treated authors 
robustness attractive property estimator robustness outliers 
means single highly erroneous observations inaeuence estimator 
obtain simple form robustness called robustness estimator bounded inaeuence function 
adapt results 
turns impossible completely bounded inaeuence function simpler form robustness stated theorem theorem assume data whitened sphered robust manner see section form preprocessing 
inaeuence function estimator bounded ug bounded inaeuence function bounded sets form fx kxk fflg ffl derivative particular chooses function bounded bounded robust outliers 
possible choose function grow fast juj grows 
practical choice contrast function performance exponential power family shall treat question choosing contrast function practice 
useful analyze implications theoretical results preceding sections considering exponential power family density functions ff exp jsj ff ff positive parameter normalization constants ensure ff probability density unit variance 
dioeerent values alpha densities family exhibit dioeerent shapes 
ff obtains sparse super gaussian density density positive kurtosis 
ff obtains gaussian distribution ff sub gaussian density density negative kurtosis 
densities family examples dioeerent non gaussian densities 
theorem sees terms asymptotic variance optimal contrast function estimating independent component density function equals ff form opt juj ff arbitrary constants dropped simplicity 
implies roughly supergaussian resp 
sub gaussian densities optimal contrast function function grows slower quadratically resp 
faster quadratically 
recall section grows fast juj estimator highly non robust outliers 
account fact independent components encountered practice super gaussian reaches general purpose contrast function choose function resembles opt juj ff ff problem contrast functions ff 
better approximating functions kind qualitative behavior 
considering ff case double exponential density function log cosh constant 
note derivative familiar tanh function 
case ff highly super gaussian independent components approximate behavior opt large gaussian function minus sign gamma exp gammaa constant 
derivative function sigmoid small values goes larger values 
note function ful lls condition theorem providing estimator robust possible framework estimators type 
regards constants experimentally provide approximations 
choosing contrast function practice theoretical analysis gives guidelines choice practice criteria important particular 
computational simplicity contrast function fast compute 
noted polynomial functions tend faster compute say hyperbolic tangent 
non polynomial contrast functions replaced piecewise linear approximations losing bene ts non polynomial functions 
second point consider order components estimated estimation 
inaeuence order basins attraction maxima contrast function dioeerent sizes 
ordinary method optimization tends rst nd maxima large basins attraction 
course possible determine certainty order suitable choice contrast function means independent components certain distributions tend rst 
point application dependent say general 
account criteria reach general 
basically choices contrast function give derivatives log cosh tanh gamma exp gammaa exp gammaa constants piecewise linear approximations may 
bene ts dioeerent contrast functions may summarized follows ffl general purpose contrast function 
ffl independent components highly super gaussian robustness important may better 
ffl computational overhead reduced piecewise linear approximations may 
ffl kurtosis justi ed statistical grounds estimating sub gaussian independent components outliers 
emphasize contrast ica methods framework provides estimators practically distributions independent components choice contrast function 
choice contrast function important wants optimize performance method 
fixed point algorithms ica preceding sections introduced new contrast objective functions ica minimization mutual information projection pursuit analyzed properties gave guidelines practical choice function contrast functions 
practice needs algorithm maximizing contrast functions 
simple method maximize contrast function stochastic gradient descent constraint taken account feedback 
leads neural adaptive algorithms closely related related introduced 
show appendix modify algorithms minimize contrast functions 
advantage neural line learning rules inputs algorithm enabling faster adaptation non stationary environment 
resulting trade ooe convergence slow depends choice learning rate sequence step size iteration 
bad choice learning rate practice destroy convergence 
important practice learning faster reliable 
achieved xed point iteration algorithms introduce 
algorithms computations batch block mode large number data points single step algorithm 
respects algorithms may considered neural 
particular parallel distributed computationally simple require little memory space 
show xed point algorithms appealing convergence properties making interesting alternative adaptive learning rules environments fast real time adaptation necessary 
note basic ica algorithms require preliminary sphering whitening data versions non sphered data 
sphering means original observed variable say linearly transformed variable qv correlation matrix equals unity transformation possible accomplished classical pca 
details see 
fixed point algorithm unit shall derive xed point algorithm unit sphered data 
note maxima jg obtained certain optima efg conditions optima efg constraint ef kwk obtained points gamma fiw fi constant easily evaluated give fi xg value optimum 
try solve equation newton method 
denoting function left hand side obtain jacobian matrix jf jf gamma fii simplify inversion matrix decide approximate rst term 
data sphered reasonable approximation efg gi 
jacobian matrix diagonal easily inverted 
approximate fi current value obtain approximative newton iteration gamma gamma fiw efg gamma fi kw denotes new value fi xg normalization added improve stability 
algorithm simpli ed multiplying sides rst equation fi gamma efg gives xed point algorithm gamma efg gw kw introduced heuristic derivation :10.1.1.131.165
earlier version kurtosis derived xed point iteration neural learning rule name comes 
retain name algorithm light derivation newton method xed point iteration 
due approximations derivation xed point algorithm may wonder really converges right points 
jacobian matrix approximated convergence point algorithm solution kuhn tucker condition 
appendix proven algorithm converge right extrema corresponding maxima contrast function assumption ica data model 
proven convergence quadratic usual newton methods 
fact densities symmetric convergence cubic 
convergence proven appendix local 
special case kurtosis contrast function convergence proven globally 
derivation enables useful modi cation xed point algorithm 
wellknown convergence newton method may uncertain 
ameliorate may add step size obtaining stabilized xed point algorithm gamma gamma fiw efg gamma fi kw fi xg step size parameter may change iteration count 
smaller unity say algorithm converges certainty 
particular strategy start case algorithm equivalent original xed point algorithm 
convergence problematic may decreased gradually convergence satisfactory 
note continuum newton optimization method corresponding gradient descent method corresponding small 
xed point algorithms may simply original sphered data 
transforming data back non sphered variables sees easily modi cation algorithm works non sphered data gamma gamma efg gw cw covariance matrix data 
stabilized version algorithm modi ed follows non sphered data gamma gamma gamma fiw efg gamma fi cw algorithms obtains directly independent component linear combination need sphered 
modi cations presuppose course covariance matrix singular 
singular near singular dimension data reduced example pca 
practice expectations xed point algorithms replaced estimates 
natural estimates course corresponding sample means 
ideally data available idea computations may demanding 
averages estimated smaller sample size may considerable eoeect accuracy nal estimates 
sample points chosen separately iteration 
convergence satisfactory may increase sample size 
reduction step size stabilized version similar eoeect known stochastic approximation methods 
fixed point algorithm units unit algorithm preceding subsection construct system neurons estimate ica transformation multi unit contrast function 
prevent dioeerent neurons converging maxima decorrelate outputs iteration 
methods achieving 
methods assume data sphered 
covariance matrix simply omitted formulas 
simple way achieving decorrelation scheme gram schmidt decorrelation 
means estimate independent components 
estimated independent components vectors run unit algorithm iteration step subtract projections previously estimated vectors renormalize 
gamma cw 
cw certain applications may desired symmetric decorrelation vectors privileged 
accomplished classical method involving matrix square roots wcw gamma matrix wn vectors inverse square root wcw gamma obtained eigenvalue decomposition wcw ede wcw gamma ed gamma simpler alternative iterative algorithm 
repeat 
convergence 
gamma wcw norm step ordinary matrix norm norm largest absolute row column sum frobenius norm 
convergence method may considered variation potter formula see proven appendix 
note explicit inversion matrix avoided identity gamma valid decorrelating gives raise xed point algorithm sphering inversion covariance matrix needed 
fact algorithm considered xed point algorithm maximum likelihood estimation ica data model see 
properties fixed point algorithm xed point algorithm underlying contrast functions number desirable properties compared existing methods ica 
ffl convergence cubic quadratic assumption ica data model proof see convergence proof appendix 
contrast gradient descent methods convergence linear 
means fast convergence con rmed simulations experiments real data see section 
ffl contrary gradient algorithms step size parameters choose original xed point algorithm 
means algorithm easy 
stabilized version reasonable values step size parameter easy choose 
ffl algorithm nds directly independent components practically non gaussian distribution nonlinearity contrast algorithms estimate probability distribution function rst available nonlinearity chosen accordingly 
ffl performance method optimized choosing suitable nonlinearity particular obtain algorithms robust minimum variance 
ffl independent components estimated roughly equivalent doing projection pursuit 
ffl xed point algorithm inherits advantages neural algorithms parallel distributed computationally simple requires little memory space 
stochastic gradient methods preferable fast adaptivity changing environment required 
simulation experimental results investigated robustness contrast functions 
generated arti cial source signals sub gaussian super gaussian 
source signals mixed dioeerent random matrices elements drawn standardized gaussian distribution 
test robustness algorithms outliers values sigma added random locations 
xed point algorithm sphered data dioeerent contrast functions eq 
symmetric orthogonalization 
robust estimation covariance matrix classical problem independent robustness contrast functions simulation hypothetical robust estimator covariance simulated estimating covariance matrix original data outliers 
runs observed estimates kurtosis essentially worse estimates slightly better 
results con rm theoretical predictions robustness section 
investigate asymptotic variance eoeciency estimators performed simulations dioeerent contrast functions estimate independent component mixture identically distributed independent components 
dioeerent distributions independent components uniform double exponential laplace distribution third power gaussian variable 
asymptotic mean absolute deviations ed measure error components obtained vectors correct solutions estimated averaged runs combination non linearity distribution independent component 
results basic noiseless case depicted fig 

see estimates kurtosis essentially worse super gaussian independent components 
especially strongly super gaussian independent component cube gaussian estimated considerably worse kurtosis 
sub gaussian independent component kurtosis better contrast functions 
clear dioeerence performances contrast functions experiments repeated added gaussian noise energy energy independent components 
results shown fig 

time kurtosis perform better case sub gaussian density 
robust contrast functions somewhat robust gaussian noise 
studied speed convergence xed point algorithms 
independent components dioeerent distributions supergaussian arti cially generated symmetric version xed point algorithm sphered data 
data consisted points data iteration 
observed contrast functions iterations necessary average achieve maximum accuracy allowed data 
illustrates fast convergence xed point algorithm 
fact comparison algorithm algorithms performed showing xed point algorithm gives approximately statistical eoeciency algorithms fraction computational cost 
experiments dioeerent kinds real life data performed contrast functions algorithms introduced 
applications include artifact cancellation eeg meg decomposition evoked elds meg feature extraction image data 
experiments validate ica methods introduced 
matlab tm implementation xed algorithm available world wide web free charge 
problem linear independent component analysis ica form redundancy reduction addressed 
comon ica problem formulated search linear transformation minimizes mutual information resulting components 
roughly equivalent nding directions negentropy maximized considered projection pursuit directions 
novel approximations negentropy introduced constructing novel contrast objective functions ica 
resulted generalization kurtosis approach enabled estimation independent components 
statistical properties contrast functions analyzed framework linear mixture model shown suitable choices contrast functions statistical properties superior kurtosis approach 
new family algorithms optimizing contrast functions introduced 
family xed point algorithms neural sense non adaptive share bene ts neural learning rules 
main advantage xed point algorithms convergence shown fast cubic quadratic 
combining statistical properties robustness new contrast functions algorithmic kurtosis ln cosh gaussian contrast function log estimation error finite sample estimation errors plotted dioeerent contrast functions distributions independent components noiseless case 
asterisk uniform distribution 
plus sign double exponential 
circle cube gaussian 
kurtosis ln cosh gaussian contrast function log estimation error noisy case 
finite sample estimation errors plotted dioeerent contrast functions distributions independent components 
asterisk uniform distribution 
plus sign double exponential 
circle cube gaussian 
properties xed point algorithm appealing method ica obtained 
simulations applications real life data validated novel contrast functions algorithms introduced 
extensions methods introduced problem noisy data addressed deals situation independent components observed variables 
amari cichocki yang 
new learning algorithm blind source separation 
advances neural information processing pages 
mit press cambridge ma 
barlow 
possible principles underlying transformations sensory messages 
editor sensory communication pages 
mit press 
bell sejnowski 
information maximization approach blind separation blind deconvolution 
neural computation 
bell sejnowski 
independent components natural scenes edge lters 
vision research 

cardoso laheld 
equivariant adaptive source separation 
ieee trans 
signal processing 
cichocki unbehauen 
neural networks signal processing optimization 
wiley 
comon 
independent component analysis new concept 
signal processing 
cover thomas 
elements information theory 
john wiley sons 

adaptive blind separation independent sources approach 
signal processing 
fastica matlab package 
available www cis hut fi projects ica fastica 
friedman tukey 
projection pursuit algorithm exploratory data analysis 
ieee trans 
computers 
friedman 
exploratory projection pursuit 
american statistical association 
karhunen oja 
experimental comparison neural ica algorithms 
proc 
int 
conf 
arti cial neural networks icann pages sk vde sweden 
hampel 
robust statistics 
wiley 
harman 
modern factor analysis 
university chicago press nd edition 
huber 
projection pursuit 
annals statistics 
hyv rinen :10.1.1.131.165
family xed point algorithms independent component analysis 
proc 
ieee int 
conf 
acoustics speech signal processing icassp pages munich germany 
hyv rinen 
unit contrast functions independent component analysis statistical analysis 
neural networks signal processing vii proc 
ieee workshop neural networks signal processing pages amelia island florida 
hyv rinen 
new approximations dioeerential entropy independent component analysis projection pursuit 
advances neural information processing systems pages 
mit press 
hyv rinen 
fast independent component analysis noisy data gaussian moments 
proc 
int 
symp 
circuits systems orlando florida 
appear 
hyv rinen 
xed point algorithm maximum likelihood estimation independent component analysis 
neural processing letters 
appear 
hyv rinen oja 
fast algorithm estimating overcomplete ica bases image windows 
proc 
int 
joint conf 
neural networks washington 
submitted 
hyv rinen oja 
fast xed point algorithm independent component analysis 
neural computation 
hyv rinen oja 
independent component analysis general nonlinear hebbian learning rules 
signal processing 
hyv rinen oja hoyer 
image feature extraction sparse coding independent component analysis 
proc 
int 
conf 
pattern recognition icpr pages brisbane australia 
jones sibson 
projection pursuit royal statistical society ser 

jutten herault 
blind separation sources part adaptive algorithm neuromimetic architecture 
signal processing 
karhunen oja wang 
class neural networks independent component analysis 
ieee trans 
neural networks 
luenberger 
optimization vector space methods 
john wiley sons 
oja 
principal components minor components linear neural networks 
neural networks 
oja 
nonlinear pca learning rule independent component analysis 
neurocomputing 
olshausen field 
emergence simple cell receptive eld properties learning sparse code natural images 
nature 
papoulis 
probability random variables stochastic processes 
mcgraw hill rd edition 

pham jutten 
separation mixture independent sources maximum likelihood approach 
proc 
eusipco pages 
van hateren van der schaaf 
independent component lters natural images compared simple cells primary visual cortex 
proc 
royal society ser 

rio 
extraction ocular artifacts eeg independent component analysis 

clin 
neurophysiol 
rio ki inen hari oja 
independent component analysis identi cation artifacts recordings 
advances neural information processing proc 
nips pages cambridge ma 
mit press 
rio rel oja 
independent component analysis wave decomposition auditory evoked elds 
proc 
int 
conf 
arti cial neural networks icann pages sk vde sweden 
appendix proofs proof convergence algorithm convergence proven assumptions rst data follows ica data model second expectations evaluated exactly 
technical assumption efs gamma considered generalization condition valid kurtosis contrast kurtosis independent components non zero 
true subset independent components estimate just independent components 
change variable assume neighbourhood solution say 
shown proof theorem change lower order change coordinates due constraint kzk 
expand terms taylor approximation rst obtaining gamma gamma gamma gamma gamma gamma kz gamma gamma gamma gamma gamma kz gamma gamma gamma vectors rst components 
obtain independence doing tedious straight forward algebraic manipulations efs gamma kz gamma skew efg gz kurt efg gz kz gamma obtain kz shows clearly assumption algorithm converges locally vector sigma 
means gamma converges sign rows inverse mixing matrix implies converges efg symmetric distribution usually case shows convergence cubic 
cases convergence quadratic 
addition local approximations exact convergence global 
proof convergence denote result applying iteration step wcw ede eigenvalue decomposition wcw cw ede ed ed gamma note due normalization division eigenvalues wcw interval 
eigenvalue wcw say cw corresponding eigenvalue de ned gamma iterations eigenvalues wcw obtained applied times eigenvalues wcw original matrix iterations 

clear eigenvalues wcw converge means wcw dioecult see convergence quadratic 
appendix adaptive neural algorithms consider sphered data 
instantaneous gradient approximation negentropy respect normalization kwk account obtains hebbian learning rule deltaw normalize efg gamma efg equivalent learning rule self adaptation constant dioeerent 
nd dimensional transform wx network neurons learns eq 

course kind feedback necessary 
shown add feedback learning rule 
denoting wn weight matrix rows weight vectors neurons obtain diag gamma learning rate sequence function applied separately component vector 
general version learning rule estimated separately neuron see 
may xed prior knowledge 
