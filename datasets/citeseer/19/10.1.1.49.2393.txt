parametric optimization method machine learning kristin bennett erin bredensteiner math report january classification problem constructing plane separate members sets formulated parametric bilinear program 
approach originally created minimize number points misclassified 
novel interpretation algorithm subproblems represent alternative error functions misclassified points 
subproblem identifies specified number outliers minimizes magnitude errors remaining points 
tuning set select best result subproblems 
parametric frank wolfe method solve bilinear subproblems 
computational results number datasets indicate results compare favorably linear programming simulated annealing approaches 
algorithm part decision tree algorithm create nonlinear classifiers 
fundamental problem machine learning discrimination elements sets dimensional real space simplest case linear function consisting linear combination input attributes separate sets 
linear function determines separating plane 
practice uncommon sets strictly linearly separable 
important find linear function discriminates best sets error minimization criterion 
linear function utilized decision tree attain nonlinear separation 
decision tree nonlinear separation achieved recursively applying linear functions decisions partition disjoint regions corresponding set set goal find linear separator generalizes best correctly classifies points 
example approximate separating plane department mathematical sciences rensselaer polytechnic institute troy ny 
email rpi edu rpi edu 
material research supported national science foundation 
minimizes distances misclassified points separating plane bm 
misclassification minimization problem minimize number misclassified points 
problem different error functions may result better worse separators terms generalization 
plane obtained minimizing distances misclassified points separating plane plane misleading 
clear underlying separation sets plane 
plane formed minimizing number points misclassified 
clear division plane 
practice choice error functions clear 
propose hybrid approach parametric misclassification minimization identifies outliers minimizes distances remaining misclassified points 
parametric approach includes linear program minimizes average misclassification error subproblem bm 
research investigates mathematical programming methods constructing decisions decision trees 
classical decision tree algorithms cart id qui exhaustive search find decisions single input attribute 
decisions multivariate linear functions input attributes exhaustive search longer feasible 
linear programming perceptron algorithms construct decisions minimize distances misclassified points separating plane 
linear programming approaches bm ben glo find optimal decisions criterion polynomial time 
decision tree methods heuristic variants perceptron algorithms bu worked practice algorithms may fail converge may find optimal solutions 
problem creating linear function minimizes number points misclassified np complete hea 
algorithms oc simulated annealing minimize functions number misclassified points 
mathematical programming developed algorithm combines error criteria exploits mathematical structure underlying problem order find better solutions 
section investigate parametric bilinear programming formulation misclassification minimization program proposed mangasarian man 
discuss novel interpretation suboptimal solutions alternative error criterion 
section propose algorithm frank wolfe method discussed bm solving parametric bilinear programming problem 
algorithm attractive half subproblems closed form solutions 
computational results number practical problems section 
notation 
sets points dimensional real space cardinality respectively 
theta matrix rows points theta matrix rows points th point th row denoted likewise th point th row vectors xy denotes dot product 
set minimizers set denoted arg min 
vector denote vector components maxfx step function denote vector components misclassification minimization section investigate misclassification minimization problem man minimizes number points misclassified plane discuss benefits limitations 
primary limitations problem np complete hea infinitely local minima man 
problem may require extensive computational time 
parametric bilinear 
plane plane planes minimizing different error functions 
plane minimizing distance misclassified points plane 
plane minimizing number points misclassified 
programming formulation misclassification minimization program requires solution series subproblems 
subproblems produce alternate solutions related different error criteria 
attractive property known priori criterion produce plane generalizes best dataset 
definition linear separability 
definition linear separability sets points sets linearly separable exists plane wx fl normal plane fl determines distance plane origin aw efl efl bw vector ones appropriate dimension 
normalization aw gamma efl gamma gamma bw efl gamma step function defined gamma system definition linear separability equivalent efl bw gamma efl case linearly separable problem minimizing left hand side merely counts number points misclassified current plane fl 
shown mangasarian man problem reformulated linear program equilibrium constraints min fl er es aw gamma efl gamma gamma bw efl gamma aw gamma efl gamma gamma bw efl gamma gammar gammas gammar gammas demonstrated man linear program stationary point fl 
avoid problem consider equivalent parametric bilinear program find non negative integer ffi ffi min ffi ffi ffi min fl aw gamma efl gamma gammar gamma bw efl gamma gammas aw gamma efl gamma gamma bw efl gamma er es ffi ffi parametric bilinear formulation represents combination error functions mentioned previously distance misclassified points separating plane number points misclassified 
ffi subproblem precisely linear program bm minimizes average magnitude misclassification errors min fl eu ev subject aw gamma efl gamma gamma bw efl gamma parametric bilinear problem requires solution series subproblems different values ffi 
show subproblem identifies ffi outliers difficult classify points minimizes average magnitude misclassification errors remaining points 
optimal ffi exactly fewest number outliers removed remaining points classes linearly separable 
showing optimality variables measure error terms distance misclassified points optimal plane xw fl 
error metric lp 
theorem characterization optimal solution fl optimal solution bilinear subproblem value ffi gammaa fl gamma fl proof 
optimal dual variables problem 
optimal fl feasible satisfy remaining optimality conditions lue aw gamma efl gamma gamma bw efl gamma gamma gamma er es gamma ffi ra gamma sb gamma se gamma gamma gamma gamma gamma aw gamma efl gamma gamma el efl gamma gamma el gamma complementarity gamma fl gamma 
gammaa fl gamma fl gamma 
gammaa fl 
argument holds points set bilinear subproblem ffi ffi exist optimal exactly ffi components equal remaining components equal 
proved theorem 
theorem existence integer solution 
fl optimal solution bilinear subproblem integer value ffi ffi ffi exists fl optimal solution ffi ffi 
proof 
know theorem gammaa fl gamma fl feasible optimal 
gamma fl gamma 
similarly optimal gamma fl gamma 
fixed fl problem simplifies min gammar gammas subject er es ffi ffi ffi largest values optimal solution 
show ffi ffi nonzero components identify ffi outlying points classes plane xw fl chosen minimize average distance remaining misclassified points plane 
theorem optimality alternative error functions fl dual variables optimal solution ffi ffi ffi binary vectors 
matrix formed removing rows matrix formed removing rows vector formed removing components vector formed removing components fl optimal solutions lp min fl eu ev subject aw gamma efl gamma gamma bw efl gamma proof 
consider optimality conditions lue bilinear program lp 
clearly primal constraints lp satisfied subset constraints 
dual variables satisfy optimality conditions 
vector formed removing components vector formed removing components note 
complementary slackness lp holds gamma fl gamma gamma fl gamma 
remains shown shown theorem 
gamma gamma gamma 
gamma gamma similarly gamma gamma misclassification minimization algorithms section provide mismin mismin algorithms 
describing common parts approaches 
subproblem solved ffi 
corresponds solving lp find plane minimizes average distance misclassified points plane 
ffi frank wolfe type algorithm bm solve bilinear subproblem 
algorithm beneficial property decomposes problem linear programs closed form integer solution 
note ffi bilinear subproblem nonconvex may local minima 
careful choice sequence ffi solution subproblem starting point subproblem better solutions reduced computation time 
secant method proposed implemented man select values ffi mismin parametric misclassification minimization algorithm selects best bilinear subproblem solved mismin algorithm tuning set 
plane performs best reserved set points selected final plane 
necessarily plane corresponding lowest value ffi ffi 
details algorithms 
bilinear subproblems parametric bilinear programming formulation uncoupled bilinear program 
frank wolfe algorithm applied uncoupled bilinear program converge global solution stationary point bm 
applying frank wolfe algorithm problem obtain algorithm algorithm frank wolfe algorithm uncoupled bilinear programs fixed ffi step fl arg min fl eu aw gamma efl gamma ev efl gamma aw gamma efl gamma gamma bw efl gamma step arg min gamma ru gamma sv er es ffi step repeat improvement objective 
subproblem contained step closed form integer solution shown theorem 
mismin bilinear program parametric programming algorithm searches smallest integer value ffi ffi 
subproblem ffi may local minima choices ffi affect final solution 
ordering ffi values may drive solution local minimum ordering find global optimizer 
choice ffi linear programs may need solved making large number guesses true value ffi results computational inefficiency 
demonstrates function value ffi cancer problem described computational results section changes ffi increases 
shows variation secant method excellent approximator ffi ffi practice employ algorithm algorithm misclassification minimization ffi max denote fewest number points misclassified plane time 
ffi min denote largest ffi value attempted far algorithm ffi 
step ffi solve bilinear subproblem algorithm 
ffi max number points misclassified new plane 
ffi ffi max step solve bilinear subproblem algorithm 
step ffi max minimum ffi max number points misclassified current plane 
step ffi ffi ffi min ffi max calculate secant method update ffi gamma ffi ffi gamma ffi min ffi gamma ffi min ffi min ffi ffi min ffi max ffi ffi ffi min ffi max step ffi max ffi min go step 
parametric misclassification shown theorem optimal solutions bilinear subproblem ffi corresponds minimizing problem error function selects ffi outliers minimizes ffi ffi function values varying values ffi wisconsin breast cancer data average distance separating plane remaining misclassified points 
select plane minimizes classification error points 
estimate accuracy plane points tuning set 
points reserved training data bilinear programs 
points evaluate optimal planes solving subproblem value ffi 
plane minimizes testing set error returned best solution 
note plane minimizes number points misclassified plane minimizes average magnitude errors candidate solutions 
mismin mismin lp 
computational results section results computational experiments performed mismin real world data sets cleveland heart disease database djs wisconsin breast cancer database wm star galaxy dim bright data sets 
mismin implemented ampl fgk mathematical programming software package utilizing cplex cpl solver 
results lp mismin mismin 
computational results tabulated choices 
mismin testing set tuning set order see best answer 
practice tuning set include testing data regarded optimistic estimate 
oc simulated annealing algorithm applied comparisons dependability mismin results 
oc algorithm generates multivariate decision trees deterministic randomized procedures 
results obtained oc represent accuracy root hyperplane decision tree constructed algorithm 
defaults chosen parameters oc pruning portion iterations chosen value 
correspondence misclassification minimization sum minority impurity measure applied 
training testing set accuracies measured dataset 
training testing cancer heart star galaxy dim star galaxy bright training testing training testing training testing training testing mismin oc lp mismin table average accuracy set accuracies calculated fold cross validation 
training points find hyperplane 
testing remaining points test hyperplane generalizes unseen points 
process repeated times partition establish average testing training set accuracy 
partitions methods 
cleveland heart disease database cleveland heart disease database patients listed numeric attributes 
patient classified presence absence heart disease 
patients presence heart disease 
data set available anonymous file transfer protocol ftp university california irvine uci repository machine learning databases ma 
wisconsin breast cancer database data set classify set patients breast cancer 
patient represented integral attributes ranging value 
classes represented benign malignant patients benign malignant 
data set available anonymous ftp university california irvine uci repository machine learning databases ma 
star galaxy databases star galaxy database consists data sets dim bright 
dim data set examples bright data set examples 
example represents star galaxy described numeric attributes 
bright data set nearly linearly separable dim data set significantly difficult 
data sets generated large set star galaxy images collected university minnesota 
table shows mismin performs substantially better oc 
differences statistically significant trends clear 
version oc algorithm mismin optimizing error function number misclassified points 
mismin achieved greater training testing set accuracies datasets 
indicates simulated annealing algorithm oc prematurely stopping local minima 
mismin superior solutions 
possible adjustment oc obtain better solutions 
advantage mismin parameter adjustment needed 
expected training set accuracy lp lower mismin lp minimizes magnitudes misclassification errors number points misclassified 
surprisingly lp better training set accuracy oc 
addition testing set accuracy lp higher oc mismin 
mismin parametric error approach provides improvement lp results 
shows priori know error function appropriate data set 
data method average average accuracy decisions cancer mismin oc star galaxy mismin dim oc star galaxy mismin bright oc table comparison mismin best reported oc results additional testing needed explore choices error functions optimization algorithms decision tree construction 
promising results table single plane 
oc designed construct decision trees decisions 
oc construct simpler trees generalized better univariate tree approaches 
table compare best results reported oc mismin 
fold cross validation estimate average testing set accuracy number decisions 
significant difference testing set accuracies methods 
oc planes average star galaxy dim data mismin required 
mismin produced dramatically simpler trees 
certainly mismin perform better single plane decision tree oc algorithm datasets 
results indicate simulated annealing approach searching space possible decisions effectively mismin 
believe superior approach decision tree construction selects alternate error functions finds better solutions respect error functions 
investigated parametric bilinear programming formulation proposed mangasarian minimizes number points misclassified hyperplane mismin uses secant method select bilinear subproblems 
subproblem solved frank wolfe method involving sequence uncoupled linear programs 
computationally useful result half linear programs closed form solution 
misclassification minimization problem closely related minimizing average sum distances misclassified points separating plane 
mismin subproblem identifies outliers data set removes problem minimizes average sum distances remaining misclassified points separating plane 
evaluation series subproblems leads naturally parametric misclassification minimization program mismin 
computational results demonstrate mismin performs better simulated annealing algorithm mismin provides improvement linear programming solution 
single error metric best combination metrics mismin led better results 
ben bennett 
decision tree construction linear programming 
evans editor proceedings th midwest artificial intelligence cognitive science society conference pages illinois 
breiman friedman olshen stone 
classification regression trees 
wadsworth international california 
bm bennett mangasarian 
neural network training linear programming 
pardalos editor advances optimization parallel computing pages amsterdam 
north holland 
bm bennett mangasarian 
bilinear separation sets space 
computational optimization applications 
bu brodley utgoff 
multivariate decision trees 
coins technical report university massachusetts amherst massachusetts 
appear machine learning 
cpl cplex optimization incorporated incline village nevada 
cplex callable library 
djs schmid sandhu guppy lee 
international application new probability algorithm diagnosis coronary artery disease 
american journal cardiology 
fgk gay kernighan 
ampl modeling language mathematical programming 
boyd massachusetts 
glo glover 
improved linear programming models discriminant analysis 
decision sciences 
hea david heath 
geometric framework machine learning 
phd thesis john hopkins university 
heath kasif salzberg 
learning oblique decision trees 
proceedings thirteenth international conference artificial intelligence pages chambery france 
morgan kaufmann 
lue luenberger 
linear nonlinear programming 
addison wesley reading massachusetts 
ma murphy aha 
uci repository machine learning databases 
department information computer science university california irvine california 
man mangasarian 
misclassification minimization 
journal global optimization 
murthy kasif salzberg 
system induction oblique decision trees 
journal artificial intelligence research 
murthy kasif salzberg beigel 
oc randomized induction oblique decision trees 
proceedings eleventh national conference artificial intelligence pages boston ma 
mit press 
pennington humphreys 
automated star galaxy discrimination neural networks 
astronomical journal 
qui quinlan 
induction decision trees 
machine learning 
utgoff 
perceptron trees case study hybrid concept representations 
connection science 
wm wolberg mangasarian 
method pattern separation medical diagnosis applied breast 
proceedings national academy sciences 
