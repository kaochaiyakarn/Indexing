knot selection regression splines lasso osborne centre mathematics applications crc cma australian national university canberra act australia key words convex programming dual problem knot selection regression splines 
tibshirani proposes absolute shrinkage selection operator lasso method regression estimation combines features shrinkage variable selection 
algorithm allows efficient calculation lasso estimator 
particular algorithm number variables exceeds number observations 
algorithm applied problem knot selection regression splines 
performance regression spline smoothing governed choice knots calculating estimator research effort devoted difficult problem knot selection see wand denison 
knot selection variable selection linear regression tibshirani proposes absolute shrinkage selection operator 
lasso estimator solution constrained estimation problem minimise fi fi gamma xfi gamma xfi subject fi gamma vector observations design matrix 
lasso solves squares regression problem constraint norm vector coefficient estimates 
assume data univariate assume loss third author wishes acknowledge carried cooperative research centre advanced computational systems established australian government centres program 
generality sorted increasing order 
may lasso estimate knot location regression splines order constructing design matrix columns gamma gamma gamma xn gamma gamma gamma note matrix gamma columns number columns may exceed number rows 
osborne 
analyse convex programming methods derive dual problem 
development includes case columns rows show exist solutions non zero entries 
show solution fi equation holds gammax fi gamma xfi vm satisfies fi gamma fi gamma fi 
results osborne 
develop efficient algorithm calculate lasso estimator particularly suited case rows columns 
algorithm described section method choosing automatically knot selection problem 
performance algorithms demonstrated examples section 
section discuss methodology extended problems 
section summarises 
algorithms calculating lasso estimator osborne 
propose iterative algorithm local linearisation current iterate fi 
stage algorithm oe denote set indices nonzero components fi oe fi fi 
permutation matrix collects nonzero components fi joej positions fi gamma fi oe delta oe entry corresponding entry fi oe non negative gamma 
step algorithm fi oe feasible kfi oe oe fi oe obtain iterate current fi optimisation problem solved minimise fi subject oe fi oe oe gamma oe delta fi fi solution 
sign fi oe oe fi called sign feasible 
fi sign feasible current solution updated follows 
move new zero component direction find smallest fl fl exists oe fi 
possibilities setting gamma fi fi recomputing solving yields descent direction consistent revised oe update oe deleting setting fi fi resetting fi oe oe accordingly feasible recomputing solving 

iterate sign feasible fi obtained 
fi sign feasible tested optimality verifying 
calculate kx rk gamma delta oe gamma oe fi solution 
proceed follows 
determine violated condition find maximal absolute value 

update oe adding 
fi oe updated appending zero element append sign oe 
solve iterate 

solving problem readily solved calculating qr factorisation oe factorisation easily updated oe changes 
oe augmented column factorisation yields result unconstrained fit current set knots see section 

starting iteration iteration started fi oe 
starting problem advantages ffl builds optimal oe starting small base pruning large ill conditioned ffl permits computation proceed time building mentioned 
lasso estimate calculated values automatic choice discussed section start smallest value fi oe 
values take starting point solution previous smaller value 
automatic choice lasso methodology select knots regression spline clear constraint norm parameters necessarily retained knots determined 
demonstrated examples section unconstrained regression spline knots may yield better fit data particularly number knots selected relatively small 
addition implementation lasso algorithm qr factorisation oe unconstrained fit residual sum squares readily available 
suggests lasso parameter chosen convenient automatic way minimising criterion aic bic fpe eubank miller ripley penalises residual sum squares unconstrained fit number knots retained number nonzero fi residual sum squares unconstrained fit immediately available qr decomposition little required computing lasso estimator various values implementation approach start algorithm described press 
bracket minimum 
minimum bracketed brent algorithm brent press find minimum 
method demonstrated examples section 

numerical observed lasso frequently chooses knots close 
crit 
crit shows effect minimum bracketing routine 
solid line constrained fit dotted line unconstrained fit knot locations indicated vertical lines 
proximity 
problem alleviated lasso select initial set knots final set chosen best subset selection backward deletion similar methods residual sum squares unconstrained fit 
numerical properties minimum bracketing routine described press 
depends tuning parameters chosen user 
shows parameters significant influence region minimum bracketed chosen automatically outlined section 
influence final outcome procedure 
top panel demonstrates unconstrained fit may excellent constrained fit approximates data poorly 
joined plot 
crit 
crit demonstrates roughness criterion function try minimise 
bracketing minimum part problem appears criterion function minimised typically rough 
connection note error description minimisation algorithm brent 
points illustrated 
middle panel shows 
crit 
crit 
crit example fit motorcycle data silverman 
figures show effect tuning parameters chosen minimum bracketing routine 
third shows influence scaling method 
vertical lines indicate knot locations 
solid line constrained fit dotted line unconstrained fit 
aic 
bic bic aic example blocks function donoho johnstone 
shows simulated data dots solid line constrained fit dotted line unconstrained fit 
second shows true curve unconstrained fit 
rug shows knot locations 
chosen automatically bic 
third shows resulting fit aic 
crit 
example doppler function donoho johnstone 
shows result chosen automatically aic 
second theta simulated data shown dots solid line constrained fit dotted line unconstrained fit 
third shows true curve unconstrained fit 
rug shows knot locations 
result minimisation process brent algorithm correcting error mentioned bottom panel shows result obtained corrected algorithm 
top panel shows values considered minimisation processes corresponding criterion values aic example dots denote values calculated corrected algorithm crosses evaluated uncorrected version 
roughness criterion function evident 
shows selecting correct tuning parameters minimum bracketing routine important 
applied algorithm section motorcycle data silverman 
parameters minimum bracketing routine changed top panel middle panel 
bottom panel uses parameters minimum bracketing routine top panel data rescaled observed lie zero 
demonstrates result sensitive scaling data 
examples order regression spline 
evidence function piecewise linear current approach choosing automatically may overfit seriously far knots selected 
case aic criterion selecting estimating blocks function donoho johnstone 
bic example hand 
illustrated 
doppler example donoho johnstone demonstrates current implementation automatic selection fail bracketing minimum early 
shown top panel 
chosen user reasonable fits obtained 
development examples indicate automatic selection necessary especially assume function piecewise linear high spatial variance doppler function 
obvious extension methodology generalisation multivariate setting 
question choose suitable set basis functions 
univariate case intend investigate methodology works basis functions splines varying location scale 
allow different orders time denison interesting challenge 
truncated power basis different scale truncated power basis shows lasso approach knot selection sensitive scaling 
situation probably necessary group parameters belonging truncated power basis functions power separate penalties group 
generalisation additive models jx possible 
estimated regression spline 
question arises single penalty basis function scaling problems 
separate penalty absolute shrinkage selection operator powerful subset variable selection method 
context knot selection regression splines naively selects knots close proximity 
evidence excellent tool choose initial set knots considered 
set final set knots may determined best subset selection backward elimination similar techniques 
osborne 

mars improved splines teubner gill eds computational techniques applications world scientific singapore pp 

brent 

algorithms minimization derivatives series automatic computation prentice hall englewood cliffs new jersey 
denison mallick smith 

automatic bayesian curve fitting journal royal statistical society series 
donoho johnstone 

ideal spatial adaptation wavelet shrinkage biometrika 
donoho johnstone 

adapting unknown smoothness wavelet shrinkage journal american statistical association 
eubank 

smoothing splines nonparametric regression marcel dekker new york basel 


computer intensive statistical methods validation model selection bootstrap chapman hall london 


selection best subset regression variables ralston wilf eds statistical methods digital computers vol 
mathematical methods digital computers john wiley sons new york chapter pp 

miller 

subset selection regression vol 
monographs statistics applied probability chapman hall london 
osborne 

note absolute shrinkage selection operator 
unpublished manuscript 
press flannery teukolsky vetterling 

numerical recipes art scientific computing edn cambridge university press cambridge 
silverman 

aspects spline smoothing approach non parametric regression curve fitting discussion journal royal statistical society series 
tibshirani 

regression shrinkage selection lasso journal royal statistical society series 
ripley 

modern applied statistics plus statistics computing edn springer verlag new york 
wand 

comparison regression spline smoothing procedures 
unpublished manuscript 
url www harvard edu gz 
