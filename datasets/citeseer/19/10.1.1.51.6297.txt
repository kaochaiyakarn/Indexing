estimating attributes analysis extensions relief igor kononenko university ljubljana faculty electrical engineering computer science ljubljana slovenia mail igor kononenko fer uni lj si 
context machine learning examples deals problem estimating quality attributes dependencies 
kira rendell developed algorithm called relief shown efficient estimating attributes 
original relief deal discrete continuous attributes limited class problems 
relief analysed extended deal noisy incomplete multi class data sets 
extensions verified various artificial known real world problem 
deals problem estimating quality attributes strong dependencies attributes key issue machine learning general 
particular problems parity problems higher degrees discovering dependencies attributes may unfeasible due combinatorial explosion 
cases efficient heuristic algorithms needed discover dependencies 
information gain proposed measure estimating attribute quality hunt 
authors quinlan 
idea estimate difference prior entropy classes posterior entropy values attribute gain gamma log gamma gammap theta log information gain similar estimates gini index breiman distance measure mantaras measure smyth goodman assume attributes independent applicable domains strong dependencies attributes 
kira rendell developed algorithm called relief shown efficient estimating attributes 
key idea relief estimate attributes values distinguish instances near 
purpose relief instance searches nearest neighbours class called nearest hit different class called nearest 
fact relief estimate attribute approximation difference probabilities different value instance different class gammap different value instance class rationale attribute differentiate instances different classes value instances class 
original relief deal discrete continuous attributes limited class problems 
clear relief extended deal incomplete data deal problems classes 
straightforward extensions give satisfactory results 
relief analysed extended deal noisy incomplete multi class data sets 
extensions verified various artificial known real world problem 
section relief described extended nearest neighbours search 
section extend relief deal missing data section deal multi class problems 
results experiments extended versions relief artificial data sets real world problem discussed section 
estimating probabilities relief original algorithm relief kira rendell set weights randomly select instance find nearest hit nearest attributes diff diff diff attribute instance instance calculates difference values attribute instances 
discrete attributes difference values different values equal continuous attributes difference actual difference normalized interval 
normalization guarantees weights interval gamma 
function diff calculating distance instances find nearest neighbours 
total distance simply sum differences attributes 
obviously algorithm tries approximate difference 
parameter represents number instances approximating probabilities 
larger implies reliable approximation 
exceed number available training instances 
obvious choice relatively small number training instances set upper bound run outer loop learning algorithms available training instances 
experiments simplification algorithm 
selection nearest neighbours crucial importance relief 
purpose find nearest neighbours respect important attributes 
redundant noisy attributes may strongly affect selection nearest neighbours estimation probabilities noisy data unreliable 
increase reliability probability approximation relief extended search nearest hits misses near hit extended version algorithm called relief averages contribution nearest hits misses 
estimate contribution nearest neighbours generated data sets 
attributes data sets binary equal prior probabilities values random attributes various prior probability values independent class 
equally probable classes data set instances 
compared intended information gain attributes estimates generated relief calculating standard linear correlation coefficient 
correlation coefficient show close intended quality estimated quality attributes 
intended information gain calculated probabilities generate artificial data sets 
note due random generator sections due added noise incomplete data factual information gain may differ intended information gain 
experiments correlation intended information gain factual information gain greater 
interested true probability distribution responsible generation data consider intended information gain target ideal estimator 
data set contained random binary attributes different prior probability values independent informative attributes 
degrees information gains determined probability value class jc class jc gamma jc 
second data set obtained data set replacing informative attribute binary attributes altogether theta attributes values pair new attributes determined value original attribute parity relation second order exor relation example original attribute value new attributes equal values different values 
intended information gain new attributes equal information gain original attribute 
note information gain calculated zero new attributes intended information gain new attribute half information gain original attribute 
third data set obtained data set replacing informative attribute binary attributes altogether attributes values determined value original attribute parity relation third order 
new attribute data set third information gain original attribute 
data set corrupted class noise 
noise means class changed randomly selected instances 
nearest neighbours corr 
coeff 
independent attributes class noise 


results experiments relief data set independent informative attributes 
results data set figures 
results clearly show higher number nearest neighbours relief improves estimates attributes noise free data sets 
improvement drastic data sets noise 
independent attributes quality estimate increases number nearest neighbours 
formally explained derivation 
number nearest neighbours increases equation different value class gammap different value class nearest neighbours corr 
coeff 
parity class noise 


results experiments relief data set pairwise informative attributes 
nearest neighbours corr 
coeff 
parity class noise 


results experiments relief data set triplets informative attributes 
equal value class gammap equal value class rewrite equal value class value obtain bayes rule gamma gamma gamma equalities theta obtain theta gini gamma gini theta gamma highly correlated gini index breiman classes values attribute difference factor gini index uses gini index impurity functions turn highly correlated information gain defined 
denominator equation constant attributes influence correlation factor 
factor kind normalization factor multi valued attributes 
experiments attributes binary equal prior probabilities values gives constant 
increasing number nearest neighbours estimates relief highly correlated gini index information gain 
dependent attributes quality increases maximum decreases number nearest neighbours exceeds number instances belong peak distribution space class 
effect seen parity problem observed effect larger number nearest neighbours 
interesting smaller number nearest hits misses algorithm noise drastically affects results independent data sets results data sets dependent attributes 
may explained fact incorrect class label implies incorrect attribute values half third attributes parity parity problems 
higher number nearest neighbours helps drastically reduces effect 
incomplete data sets enable relief deal incomplete data sets function diff attribute instance instance extended missing values attributes 
compared versions relief relief instances unknown value attribute diff set gamma values attribute relief relief updating estimates contributions differences calculated instances unknown value attribute ignored appropriate normalization 
idea unknown values ignored estimates training instances provided resulting estimates converge correct estimates 
relief calculate probability instances different values attribute instance unknown value diff gamma value jclass instances unknown value diff gamma values gamma jclass theta jclass delta conditional probabilities approximated relative frequencies training set 
estimate performance algorithms values informative attributes replaced unknown values 
turned noise free data significant difference performance algorithms 
incomplete noisy data relief performed significantly better 
typical picture shown data set independent informative attributes 
number nearest hits misses set unknown values 
data sets values parameters pictures results similar 
class noise corr 
coeff 
independent atts unknown vals nearest neighbours 


results different versions relief incomplete data set unknown attribute values independent attributes 
number nearest hits misses 
multi class problems kira rendell claim relief estimate attributes data sets classes splitting problem series class problems 
solution unsatisfactory 
order relief practice able deal multi class problems prior changes knowledge representation affect final outcomes 
experimented extensions relief multi class problems relief near instance defined nearest neighbour different class 
straightforward generalization relief 
relief finding near different class algorithm finds near different class averages contribution updating estimates 
average weighted prior probability class class theta diff idea algorithm estimate ability attributes separate pair classes regardless classes closest 
order compare performance algorithms generated additional data sets 
data sets equally probable classes respectively random attributes informative attributes pair classes 
data set classes theta theta binary attributes classes theta theta attributes 
attributes informative means prior probability attribute values class 
attribute informative separating class jc jc gamma jc jc jc 
data sets obtained replacing informative attribute binary attributes way second data set described section 
third data set attributes attributes 
results figures 
results show clear advantage relief noise free noisy data 
discussion relief efficient heuristic estimator attributes able deal data sets dependent independent attributes 
extensions incorporated relief enable deal noisy incomplete data sets probably important contribution relief efficiently deal multi class problems 
verify drawn experiments artificial data sets ran different versions relief known medical data set 
real world data sets intended true information gain attributes unknown 
primary tumor data set physicians claim attributes independent confirmed experiments semi naive bayesian classifier kononenko 
data set acceptable information gain calculated estimate target attribute quality 
estimate performance different versions relief calculated correlation coefficient factual information gain attributes relief estimates 
results 
results real classes corr 
coeff 
independent attributes nearest neighbours 
noise 
noise 
noise 
noise results different versions relief multi class problems independent attributes noise class noise 
number nearest hits misses 
classes corr 
coeff 
parity nearest neighbours 
noise 
noise 
noise 
noise results different versions relief multi class problems dependent attributes noise class noise 
number nearest hits misses 
nearest neighbours corr 
coeff 
primary tumor 

results different versions relief primary tumor medical data set 
versions relief algorithm unknown values relief relief correlation coefficients 
world data set confirm advantage relief versions support drawn experiments artificial data sets 
address problem multi valued attributes 
information gain gini index tend overestimate multi valued attributes various normalization heuristics needed avoid tendency gain ratio quinlan binarization attributes kononenko 
relief nearest hits misses implicitly uses prior probability instances equal values see equation normalization appropriate 
major difference information gain estimates relief primary tumor problem estimates significant attributes 
information gain overestimates attribute values opinion physicians specialists 
relief normalized versions information gain correctly estimate attribute important 
inductive learning algorithms typically variants greedy search strategy overcome combinatorial explosion search hypotheses 
major role greedy search heuristic function estimates potential successors current state search space 
relief promising heuristic function may overcome current inductive learning algorithms 
kira rendell relief preprocessor eliminate irrelevant attributes data description learning 
relief general efficient reliable inside learning process guide search 
part done author stay california institute technology pasadena ca 
padhraic smyth prof rodney goodman enabling micro systems research laboratory 
supported ministry science 
kovacic comments earlier versions 
reviewers slightly imprecise suggesting read strongly related papers journals ieee trans 
pami pattern recognition pattern recognition letters years 
appreciate reader information precise 

breiman friedman olshen stone classification regression trees 
international group 
hunt martin stone experiments induction 
new york academic press 
kira rendell practical approach feature selection 
proc 
intern 
conf 
machine learning 
aberdeen july sleeman edwards eds morgan kaufmann pp 
kira rendell feature selection problem traditional methods new algorithm 
proc 
aaai 
san jose ca july 
kononenko bratko experiments inductive learning medical diagnostic rules 
proc 
international school synthesis expert knowledge workshop 
bled slovenia august 
kononenko semi naive bayesian classifier 
proc 
european working session learning porto march kodratoff ed springer verlag pp 
mantaras id revisited distance criterion attribute selection 
proc 
int 
symp 
methodologies intelligent systems 
charlotte north carolina oct 
quinlan induction decision trees 
machine learning 
smyth goodman rule induction information theory 

piatetsky shapiro frawley eds knowledge discovery databases 
mit press article processed macro package llncs style 
