statistical perspective knowledge discovery databases john elder iv computational applied mathematics department center research parallel computation rice university pregibon statistics data analysis research bell laboratories quest find models usefully characterizing data process central scientific method carried fronts 
researchers expanding number fields designed algorithms discover rules equations capture key relationships variables database 
task chapter provide perspective statistical techniques applicable kdd accordingly review major advances statistics decades 
highlight may called statistical viewpoint 
overview influential classical modern statistical methods practical model induction 
statistical contributions unfortunate kdd community dismissed statistical methods basis courses took statistics years ago 
provides rough chronology significant contributions statistics relevant kdd community 
noteworthy fact time period coincides significant increases computing horsepower memory powerful expressive programming languages general accessibility computing propelled elder pregibon information age 
effect started slow deliberate shift statistical community important influences enablers come computing mathematics 
era robust resistant statistical methods 
ideas box tukey huber hampel formalized notion usual estimators location regression coefficients sensitive outliers leverage values unreasonably small amounts contamination 
key concepts ffl influence function hampel essentially derivative estimator respect data ffl estimators huber called generalize maximum likelihood estimators require probability distribution closely related class estimating equations ffl diagnostics implicit observations afforded robust estimators replaced empirical derivatives quantify effects small changes data important aspects regression models see example kuh theory supporting ideas elegant important unifies seemingly unrelated concepts trimmed means medians reflects realism data usually obey assumptions required mathematical theorems 
robustness era freed statisticians narrow models depending unrealistic assumptions normality 
downside era effort placed deriving new estimators deviated slightly qualitatively quantitatively 
needed leadership direction methods practice dealing plethora alternatives available 
partly misguided effort techniques era commercial software mainstream methods 
early term exploratory data analysis eda characterizes notion statistical insights modeling driven data 
john tukey mosteller tukey basically reflecting fisher insight statistical methods researchers easier inventing new statistical estimator 
statistical perspective knowledge discovery databases reinforced notions early battery ultra simple methods done pencil 
deeper message traditional dogma stating allowed look data prior modeling 
side argument hypotheses biased choosing basis data indicating 
side belief pictures numerical summaries data necessary order understand rich model data support 
key notion era characterized statistical modeling decomposing data structure noise data fit residual examining residuals identify move additional structure fit 
fitting process repeated followed subsequent residual analyses 
iterative process described roots general statistical paradigm partitioning variability distinct parts explained unexplained variation classification group group variation 
eda notion simply uses observed scale response somewhat unnatural squared units variability 
trivial distinction difference critical observed scale diagnosis treatment possible 
example component variance indicate nonlinearity prescribe accommodate 
graphical methods confused graphical models bayes nets enjoyed renaissance period statisticians re discovered outperforms human visual capabilities pattern recognition 
specifically statistical tests models focus expected values cases unexpected invalidates model outliers 
tukey argued graphical methods allow unexpected values highlighted models expanded changed account 
important contribution data description respectable 
statistics roots earlier times descriptive statistics mathematical statistics eye 
data description concerned simplicity accuracy overly formal quantifying terms important area research tries just mallows akaike rissanen 
key notion popularized era seldom single right answer nearly situations answers 
effective data description highlights simple concise reasonably accurate 
simple transformations dataset effect descriptions common elder pregibon ones data log age age data splitting setting aside outliers simplify description bulk data 
late outsider statistical literature fragmented disjoint 
fact matter closely related specific details individual contributions hide real similarities 
late review papers book elegantly captured essence numerous prior publications 
generalized linear models nelder mccullagh nelder extended usual normal theory linear model wider class models included probability models normal distribution structural models nonlinear 
theory accomplished decomposing variation response variable systematic random components allowed capture covariate effects strictly monotone link function fi allowing member exponential family distributions oe 
doing models provided unifying theory regression models binary count data continuous data asymmetric distributions 
second major review known outside statistics em algorithm dempster laird rubin 
neatly pulled numerous ways solving estimation problems incomplete data 
beauty general treatment concept data complete useful treat missing value problem computational purposes 
analysis nominal discrete data specifically counts disconnected streams literature inconsistent ways describe relationships 
bishop fienberg holland pulled material class loglinear models 
associated theory allowed researchers draw analogies models continuous data example analysis variance ideas provided computational strategies estimation hypothesis testing 
noteworthy anticipated current called graphical models subset class loglinear models nominal data 
early resampling methods late jackknife named tukey general purpose tool eliminating bias estimator 
essence procedure replace original observations possibly correlated estimates quantity interest called 
obtained systematically leaving observations recomputing estimator 
precisely parameter statistical perspective knowledge discovery databases interest ith defined gamma gamma gammai quantity estimator leaving ith subset size 
jackknife estimate arithmetic mean jackknife originally proposed bias reduction tool quickly recognized ordinary standard deviation provides honest estimate error estimate 
empirical means deriving measure uncertainty virtually statistical estimator available 
interpretation procedure construction repeatedly systematically sampling replacement data hand 
led efron generalize concept repeated sampling replacement called bootstrap allowed pick oneself bootstraps constructing confidence interval standard error 
seemingly trivial insight opened flood gates comprehensive analytic study understanding resampling methods 
focus estimating precision estimators bias removal coupled advance computing resources allowed standard errors highly nonlinear estimators routinely considered 
unfortunately robustness bulk research effort directed theoretical study resampling ideas kdd researchers regard uninteresting situations 
nonlinear procedures resulting combining model identification model estimation see section received cursory effort efron gong faraway 
late characterize classical statistical methods globally linear explanatory prediction classification variables affect distribution response variable linear combinations 
effect summarized single regression coefficient fi nonlinear relationships modeled specifically including appropriate nonlinear terms model log cleveland helped seed notion globally linear procedures replaced locally linear ones employing scatterplot smoothers interesting ways 
scatterplot smoother data dependent curve defined pointwise range example moving average smoother defined unique mean symmetric nearest neighbors ordered sequence pointwise estimates traces smooth curve scatter points 
originally smoothers simply enhance scatterplots clutter changing density plotted points hindered visual interpretation trends nonlinear features 
elder pregibon interpreting scatterplot smoother estimate conditional mean yjx obtains adaptive nonlinear estimate effect distribution nonlinearity simultaneously reducing bias caused effects enforcing local linearity smoothing procedure opposed local constants provided moving averages medians 
moving window data fitting linear regressions window globally nonlinear fit obtained sequence predictions point coefficients determined squares regression points window centered notion applied contexts regression classification discrimination error distributions generalized additive model hastie tibshirani 
reduced emphasis strict linearity explanatory variables models ameliorate need having previously identified relevant variables 
early statistics community friedman tukey pioneered notion allowing model adapt nonlinearly letting data determine interesting structure projection pursuit methods section 
restrictive related nonlinear methods neural networks section supposing model form yjx fi jk regression coefficients fi jk squashing functions unknown 
important algorithmic developments theory resulted models failed achieve widespread statistics community 
part reason models regarded flexible sense arbitrarily complex functions provably recovered big 
community back additive models limited flexibility afforded greater interpretability 
interpretability focus area alternative formulations locally linear model derived penalized likelihood bayesian formulations sullivan 
ambitious methods helped nudge community focusing model estimation model selection modern methods see sections modeling search structure space parameter space 
uncommon thousands candidate structures considered modeling run statistical perspective knowledge discovery databases forces conservative judging improvements significant measure model quality optimized search optimistic see miller context regression subset selection 
considering plethora candidates usually clear wide variety models different structures inputs score nearly single best 
ancient statistical adage safety researchers explicitly blending outputs viable models obtain estimates reduced variance better accuracy new data wolpert breiman 
techniques especially promising models merged completely different families example trees polynomials kernels splines local influence function estimated accuracy region design space 
statistical viewpoint interpretability researchers different fields emphasize different qualities models seek 
example breiman noted neural network community appears variations approach may experiment wide variety techniques overriding goal developing model minimally misclassifying new data 
statisticians hand usually interested interpreting models may sacrifice performance able extract meaning model structure 
accuracy acceptable reason model decomposed revealing parts useful black box system especially early stages investigation design cycle 
characterizing uncertainty randomness sampled data inherited estimated model parameters functions data 
statisticians summarize induced randomness called sampling distributions estimators 
judicious assumptions exact sampling distributions analytically tractable typically asymptotic arguments invoked 
net result estimated parameters approximately normally distributed 
distribution characterizes uncertainty estimated parameters owing normality uncertainty succinctly captured standard deviation sampling distribution termed standard error estimate 
standard elder pregibon statistical practice requires stating standard errors estimated model parameters 
parameters associated estimates small comparison standard errors fi fi part true underlying process generating data prudent drop parameters model 
term term analysis breaks presence collinear variables weakened nonlinear models stretch applicability asymptotic normal sampling theory 
basic insight useful estimates accompanied uncertainty measures error bars useful 
bayesian paradigm provides different related perspective 
treats parameter random variable merges prior beliefs parameter observed data 
resulting posterior distribution approximated normal distribution single number summary parameter uncertainty available 
course computational advances ingenious algorithms markov chain monte carlo obviate need analytically derived normal approximations 
lacking picture posterior distribution second moment summarize spread induced posterior distribution 
disciplines deal unavoidable variation 
instance electrical engineers design circuits filter noisy signals components inexact values resistors tolerances 
similarly financial analysts know potential investments need evaluated expected return risk usually standard deviation returns 
investments higher historical implied deviations sense accompanied appropriate risk premium higher expected return 
hand logicians computer scientists slow appreciate importance explicitly handling uncertainty 
arguably statistics head start problem natural language probability calculus propagate characterize uncertainty models 
certainty factors early expert systems fuzzy logic ones weak attempts probability done centuries 
modeling contexts emphasis prediction estimation model parameters 
change emphasis reduce need characterize report uncertainty 
properly formulated models provide prediction point design space yjx associated variance var yjx 
monitoring local variance useful confidence estimates 
example cox john elder employ conditional variances response surface guide global search algorithms efficiently low dimensions 
unfortunately nonlinear inductive modeling techniques see section explicitly incorporate conditional variance estimates clear area potential improvement 
statistical perspective knowledge discovery databases borrowing strength case statistical problems inferences desired data sparse 
consider example retail marketing 
stock keeping unit unique label assigned retail product example men size blue socks 
predictions required store level large chain department stores build sufficient inventory promotions seasonal demand predictable events 
problem detailed historical data individual sales store chain available example may men size blue socks sold florida store november 
concept borrowing strength allows build forecasts site level exploiting hierarchies problem possibly ways 
aggregating stores sufficient information available build site independent prediction 
prediction add stability predictions regions turn add stability site level 
similar types decompositions allow borrow strength looking sales say blue socks independent size socks men 
hierarchical models origins empirical bayes models called inferences truly bayesian maximum likelihood estimates place hyperparameters parameters prior distributions highest levels hierarchy data numerous 
typically results estimates form ff gamma ff estimate specific ith level hierarchy parent data abundant 
mixing parameter ff captures similarity individual estimate parent relative tightness distribution 
explicit assumptions statisticians typically aware explicit implicit assumptions associated models 
appeal non traditional models methods undoubtedly stems apparent ability bypass statistical analysis stages see cumbersome clear matching assumptions method characteristics problem beneficial solution 
statistical analysts usually take useful step checking assumptions chiefly examining 
residuals model errors 
diagnostics model sensitivity perturbations 
parameter covariances redundancy violations assumptions equally bad 
example assumptions stochastic behavior data typically important structural behavior elder pregibon lead inefficient estimates inaccurate standard errors result biased estimates 
broad classes normality independence assumptions typically important constancy homogeneity variance var yjx constant 
single outlier structural model bias fit 
likewise leverage values observations undue influence fit example deleting ith observation resulted large change estimate key parameter 
important distinction leverage values need correspond large residuals virtue leverage bias fit resulting small negligible residuals 
predictor variables confuses interpretation associated parameters harmful prediction new data strictly abide interrelationships reflected training data model extrapolating confines training space interpolating 
regularization aim statistical inference inductive modeling infer general laws specific cases summarize observations phenomenon coherent model underlying data generating mechanism tested explanatory power new cases 
perform data seen training models need appropriate structure complexity powerful approximate known data constrained generalize successfully 
ockham razor invoked guiding principle model selection suggests select competing hypotheses similar explanatory power simplest 
cases simpler accurate model generalize better new data arising process generated training set 
statistical terms tradeoff model bias overfit variance imposition modeling restraint called regularization 
data plentiful model overfit avoided reserving representative subsets data testing model constructed 
performance test set systematically worsens model growth curtailed 
common scenario design space densely populated data cases employed training model complexity number parameters roughness integrated squared slope response surface constrain fit 
criterion minimized weighted sum training error measure model complexity roughness 
note nonlinear adaptively selected parameters influence model typical linear terms inclusion accompanied correspondingly greater increase training accuracy pay way 
regularization methods employed statistics statistical perspective knowledge discovery databases communities 
method parameter shrinkage uses variables constrains influence models robust 
example collinear variables infinite solutions linear estimation problem 
nearly collinear variables estimated optimal parameters may huge variances clear type overfit 
shrinking parameters singular value decomposition ill conditioned design matrix relatively robust weight solution selected near origin parameters zeroed 
likewise ridge regression pushes unstable solutions direction smaller values effectively reducing complexity model 
shrinkage performed trees pregibon hastie neural networks known optimal weight decay 
shrinkage procedures bayesian interpretation user defined prior guides direction degree regularization 
second method applies iterative procedures considered relatively crude approach statisticians halt adjustment procedure time convergence 
primary means artificial neural network training weight modification halted reported contexts taming em algorithm positron emission tomography 
summarize hallmark statistical approach regularize models employ simplifying constraints alongside accuracy measures model formulation order best generalize new cases true goal empirical modeling activities 
reservations automatic modeling statistics experienced statistician capable guiding development automated tools data analysis may acutely aware difficulties arise dealing real data 
hesitation bred skepticism automated procedures offer contributed strong focus statistical community model estimation neglect logical predecessor step model identification 
culprit underlying benign neglect close historical connection mathematics statistics statisticians tend problems theorems analytical solutions attainable sampling distributions asymptotics 
solutions necessarily conditional underlying model specified small number identifiable parameters summarize relationship predictor variables response variable moments conditional distribution yjx 
example common regression model takes form elder pregibon yjx fi oe yjx constant implicit parameter part explicit formulation precise specification define model mean parameter 
traditional statistics provides useful information sampling distribution estimates fi fixed set formalism saying needed 
relatively small effort statistical community model identification focused computing horsepower human judgment opposed fully automated procedures 
general problem deciding large complex model available data support 
directly explicitly focusing mean squared prediction error statisticians long understood basic tradeoff bias small model variance large model model selection 
algorithms wilson methods mallows extensively identifying candidate models summarized model accuracy model size 
primary reason human judgment crucial process algorithmic optimality include qualitative distinctions competing models similar size example accuracy availability cost variables differ 
largely human expertise select validate model models potentially large pools candidate models 
statistician tendency avoid complete automation respect challenges data historical emphasis models interpretable structure led community focus problems manageable number variables dozen say cases typically may encountered kdd problems orders magnitude larger outset 
increasingly huge amorphous databases clear methods automatically hunting possible patterns worthy fuller interactive attention required 
existence tools free instance posit wider range candidate data features basis functions building blocks wish deal specifying model structure hand 
obvious need gaining sympathy precious little resulted 
subsections highlight areas underlie hesitation automating model identification statistical community 
final models similar complexity magnitude initial candidate set variables cases usually larger kdd 
statistical perspective knowledge discovery databases statistical significance versus practical significance common approach addressing complexity size model space limit model growth model fitting learning stage 
accomplished statistical test significance step incremental model building stage 
example standard test independence nominal variables means limit growth model searches significant association 
main problem approach significance levels depend critically sample size increases trivial differences attain statistical significance 
statisticians ameliorate problem introducing context better qualify findings significant 
simpson paradox related problem automated search procedures completely fooled anomalous association patterns small datasets 
accessible easily understood example freedman concerns admission graduate school uc berkeley 
major departments female applicants admitted male applicants admitted 
disparate fractions indicate sex bias 
face applicants admissions broken department fractions admitted shows different story argue reverse sex bias 
paradox choice major confounded sex females tend apply majors harder get males apply easy majors 
implication paradox kdd tools attack large databases looking interesting associations pairs variables contain methods search potential 
computationally changes problem operation higher considers 
computational burden avoided providing knowledge potential discovery algorithm 
principle possible sufficient common sense knowledge suggests operating 
statisticians long brought common sense insights problem delegate automata 
selection bias automated knowledge discovery systems applied databases expectation translating data information 
bad news available data representative population interest worse news data contains hint potential bias 
issue elder pregibon data set information contains 
example suppose white house press secretary kdd information retrieval tool browse email messages president gov concern health care reform 
suppose finds ratio pro reform messages leading assert americans favor reform ratio followed government fix 
may people dissatisfied health care system sound views satisfied 
true distribution views health care reform mean score zero self selected samples heavily biased tails distribution give misleading estimate true situation 
realistic expect automated tools identify instances 
probably realistic expect users lawyers systems critically question interesting facts 
quantifying visual capabilities today data analyst dependent interactive analysis numerical graphical summaries computed displayed fly 
successful instances data mining statisticians sprinkled cries aha subject matter context information unexpected behavior plot discovered course interaction data 
discovery change intended course subsequent analysis steps quite unpredictable ways 
assuming hard problem include common sense context information automated modeling systems leaves automated interpretation plots promising area explore 
problems served barrier statisticians regard 
hard quantify procedure capture unexpected plots 

accomplished need describe maps analysis step automated procedure 
needed statisticians way represent meta knowledge problem hand procedures commonly 
suggests opportunity kdd statistical communities complement skills provide acceptable powerful solution 
statistical methods modern realistic situation occurred politics major polls overwhelmingly projected dewey presidential election bad dewey republican discrepancy voting public phone service 
statistical perspective knowledge discovery databases section review classical methods knowledge discovery problems interest centers single response variable collection predictors 
models assume availability training data goal find model predict performs new data 
problem defined solution squares decades computing advances possible relax classical assumptions 
statisticians feeding devising new estimation methods estimates models additive models exploit restricted formulation 
caught race develop increasingly flexible models encouraged famous result kolmogorov multi dimensional functions represented composition dimensional functions 
statisticians result class models far flexibility useful practice finite noisy data prevail 
need models scale real data due size complexity missing values simplest analyses 
discussion classical linear methods nonparametric techniques briefly describe modeling algorithms selected span statistical method space projection pursuit neural networks polynomial networks decision trees adaptive splines 
treated black box knobs set performs variable selection feature extraction set candidate inputs recommend abandon 
careful modeling familiarity subject matter domain lead greatly improved performance 
recommended information subject 
friedman provides excellent overview major issues involved building models data applicable induction techniques 
weiss kulikowski describe accessible manner basics major inductive machine learning classification techniques including linear discriminant analysis decision trees neural networks expert systems 
useful advanced surveys focusing neural networks statistical properties include ripley cheng titterington 
barron barron provide unifying view methods statistical learning networks 
comparison approximately public domain classification algorithms summarized number diverse applications european statlog project michie spiegelhalter taylor 
lastly modern developments statistical density estimation data visualization clearly tree methods dominate kdd machine learning expert system communities reason 
trees mapped rules easily handle categorical data missing values usually far interpretable 
smooth methods deserve consideration applicable basis functions appropriate data lead improved performance 
elder pregibon effectively scott 
linear models classical models prediction classification linear regression linear discriminant analysis respectively 
term linear models pertains primarily fact regression classification surface plane linear combination available predictors equations may nonlinear functions original data 
flexibility straightforward computation involved linear regression leads wide techniques 
example radial basis function networks merely linear regression set kernel features nonlinear functions separation case potentially adaptively selected data centroids section 
lowe webb employ neural network architecture section compute nonlinear data features feed final linear regression stage polynomial networks section linear regression node combine previous nonlinear polynomial data transformations 
linear discriminant analysis appropriate pre post processing formulated problem linear regression stage hastie tibshirani buja 
allows replace linear regression module advanced nonlinear nonparametric estimation method greatly increasing types patterns handled classification techniques 
models linear second important respect estimated parameters model linear response variable example usual linear regression model fi ij type linearity enables exact sampling theory estimated model parameters section selected course analysis case hard nonlinearity involved 
nonparametric methods linear models parametric methods replace sample data model representing global consensus pattern data represents degree patterns captured particular building blocks typically lines quadratic curves 
nonparametric model free code book methods keep data refer estimating response class new point 
simplest method nearest neighbors returns response statistical perspective knowledge discovery databases closest known point measured input variable design space distance metric euclidean 
resulting estimation surface discrimination boundaries extremely responsive local variations 
smooth somewhat data set pared unusual points responses nearest neighbors averaged 
simple method quite competitive performance asymptotically data density increases results worse twice bayes optimal error cover hart 
kernel estimation parzen provides smoothed generalized weighting near neighbors 
density function uniform triangular normal centered point predicted 
prediction kernel weighted average data 
single parameter representing spread kernel adjusted govern roughness local prediction 
methods appear model free type model implicit choice distance function 
considers mahalanobis distance points ij gamma sigma gamma gamma considerable latitude deciding enter distance calculation form log 
scaling issues reflected sigma break resulting prediction 
modeling methods issues need carefully considered experimented training data 
parametric methods nonparametric techniques essentially constrained operate low dimensions depend heavily local structure high dimensional data sparse local neighborhoods empty non empty neighborhoods local scott 
example data uniformly distributed dimensional unit cube data histogram bin width local neighborhood 
dimension grows nearly point views outlier respect rest training data closer outer boundary space nearest neighbor friedman 
methodological intuition gained experience low dimensions thoroughly place high spaces phenomenon known curse dimensionality 
intelligently selecting variables reduce dimensionality samples reasonably densely populate predictor space simple methods outperforming parametric methods 
accordingly automated induction methods described employed useful examine perfor rectangular kernel leads type histogram flexible bin edges 
elder pregibon mance simple kernel weighted nearest neighbors subset variables selected adaptive algorithms 
projection pursuit low dimensions human ability recognize patterns matched automata 
straightforward visual examination data histograms scatterplots rotating plots reveal structure missed automated induction algorithms elder 
grand tour strategy asimov data rotated smoothly views allowing discover interesting perspectives 
number different views explodes exponentially dimension limiting visual coverage methods problems moderate number candidate predictor variables 
accordingly statisticians sought quantify measures interestingness optimized computer identify revealing views high 
procedure known exploratory projection pursuit friedman tukey searches projections maximally deviate normality robustly smoothes data projection subtracts smooth response 
process repeated projection projection error reduction justify added complexity 
anti normal projection index reasonable employ regardless true density random projections high data normal diaconis freedman 
exploratory projection indices designed seek holes clusters 
projection pursuit regression friedman stuetzle utilizes maximal correlation index maximal class separation building classifier 
difficult capture interestingness single criterion structure obvious analyst missed see elder 
particular quality quantified index analyst employing visualization advantage multiple points recognizing wide variety patterns encountered explicitly choosing goal 
weakness shared automated modelling techniques varying degrees maximize performance automated search structure high space complemented visualization lower manifolds discovered 
practice techniques producing models interpretable components additional advantage speeding design cycle entire iterative process model development 
neural networks artificial neural networks anns useful class models consisting layers nodes implementing linearly weighted sum inputs adjustable sigmoidal statistical perspective knowledge discovery databases shaped output transformation bounded squashing function 
outputs node layer feed node subsequent layers inputs 
backpropagation weight adjustment procedure werbos cases fed time errors adjust weights final output node degree proportional contribution magnitude 
weights nodes feed similarly adjusted forth back layer 
initial weights typically set randomly 
statisticians suspicious anns due promotion appear parameterized weight adjustment procedure local gradient method missing global optima sequential allowing early cases influence leading crude type regularization runtime principle way avoid overfit 
weaknesses cancel somewhat slow local search doesn allow excess parameters overfit easily 
note true degrees freedom employed ann usually fewer glance 
danger overfit depend training duration random node weights lead essentially linear functions nodes operating middle extreme sigmoid linear functions absorbed subsequent layers 
nodes get pushed curved part sigmoids training parameters active 
may explain common observation performance ann problem surprisingly robust respect changes network structure 
polynomial networks regression terms adaptively selected candidate pool forward stepwise greedy manner choose single best term add term best combines add third term works best pair forth occasionally deleting term useful accuracy improvement small justify increment complexity 
polynomial network algorithm group method data handling see expanded idea considering chunks terms simultaneously 
uses linear regression fit quadratic polynomial nodes output variable input variable pairs turn 
best nodes retained layer outputs candidate inputs layer complexity impairs performance checking set data name 
best node final layer nodes feeding model forming hierarchical composition functions feed forward network 
considerable improvements approach introduced versions polynomial network training algorithm barron elder pregibon algorithm synthesis polynomial networks elder 
details history methodology algorithms elder brown 
anns polynomial network estimation surfaces smooth global nonlinearities entering higher order polynomial terms cross products sigmoids 
structure adaptive fixed parameters adjusted sets data globally case time 
polynomial networks take orders magnitude time train back propagation anns typically achieve better results lee 
anns polynomial networks opaque difficult dissect trees interpreted straightforwardly 
decision trees neural polynomial network methods global estimators poorly estimate function sufficiently badly behaved 
decision trees recursively divide space different regions sharp breaks estimation surfaces allowing great local 
variables selected splits may different adaptively partitioned region space 
flexibility method practice crude basis function constant 
note classification problem variable different constant value class decision tree capture rule perfectly classical linear discriminant analysis encounter numerical instabilities due negligible class pooled variance variable 
decision trees statistical community pioneering classification regression trees cart breiman 

authors neatly describe problem provide theory methods grow tree validate 
depart previous propose expanding nodes reach prescribed minimal size pure 
cost complexity parameter introduced characterizes nested sequence subtrees cross validation decide far back prune overly large initial tree 
statistical models usual precautions careful pre post fitting analysis required 
advantage held trees regard tree metaphor exploited graphical analysis 
trees natural classification useful difficult estimation problems simple piecewise constant response surface lack smoothness constraints highly robust outliers predictors response variable 
automatically select variables construct models quite rapidly adaptive method 
importantly trees probably easiest model form interpret statistical perspective knowledge discovery databases long shallow experience greatly improves model chances 
main problem trees data rate exponential depth uncover complex structure extensive data required 
adaptive splines extreme local responsiveness trees disadvantage 
friedman multiple adaptive regression splines mars model employs recursive partitioning locate product spline basis functions adjustable degree constants 
results smooth adaptive function approximation opposed crude steps plateaus provided regression trees 
method considers splines involving interactions previously selected variables orient basis functions original data axes 
aid interpretation model terms collected inputs influence reported anova manner effects individual variables pairs variables collected graphically function plots 
cart mars employs cross validation prunes terms growing handle categorical variables 
new somewhat complex method accumulated experience favorably compared anns expect enforcing continuity response surface slope useful applications design space densely populated support require local spline basis functions 
statistical computing arguably matter brilliant model method describe summarize data software essential methodology 
kdd machine learning communities implicitly provide software methods largely described algorithmically 
statisticians hand perfectly capable generating scores models methods defined operating characteristics asymptotically writing line code 
days largely rare statistical methods described promoted application data 
early general purpose statistical packages included spss biomedical social sciences respectively 
important newer systems sas sas similar style containing special procedures standard statistical models 
designed language express statistical computations complete package 
example sample tests built sas simply provided high level language express relevant computations elder pregibon assigned function repeated usage 
sas widely exploratory data analysis modeling graphics 
provides degree data management remove burden users extended tailor methods specific applications 
emerging useful packages include lisp stat tierney inspired object oriented common lisp system research circles host pc systems demonstrate remarkable breadth accessible interfaces 
case specialized methods discussed previous section appear isolation part bigger system 
useful repository statistical research algorithms statlib archive 
example cart fits trees trees 
features stand programs usually eventually way general systems losing efficiencies gaining capabilities integrated data analysis environment essential analysis quality analyst productivity 
cart inspired implementation tree models language clark pregibon allows users manipulate data variety ways prior fitting provides interactive graphical interface model opportunity explore alternatives additive models 
story somewhat similar graphical front 
stand statistical graphics systems provide real time dynamic motion find essential exploring complex high dimensional data sets 
general purpose systems adequately handle plots lack degree specialization allowing friendly user interfaces state ofthe art graphics 
lisp stat exception certain advanced features case linking multiple plots provided 
xgobi system cook buja provides comprehensive projection real time motion tool set includes grand tours guided tours graphics system stand cooperative statistics system 
want emphasize computing statistics vehicle data analysis 
revolutionized field computational methodologies statisticians take granted resampling methods cross validation markov chain monte carlo 
expect influence computer science statistics increase 
send line message send index statlib lib stat cmu edu contents retrieval instructions 
offered optional module popular pc package 
statistical perspective knowledge discovery databases tendency statistical community propagate uncertainty models sampling distributions familiarity need regularize models trade accuracy complexity checking model assumptions stability residual graphical analyses strengths 
alternative heuristic modeling techniques gained popularity partly way avoid statistics address challenging induction tasks 
statisticians learn need better job communicating value considerations clarifying streamlining ways injecting extra data information modeling process 
great deal goes identifying gathering cleaning labeling data specifying question asked finding right way view literally discover useful patterns 
despite central importance modeling data focus chapter stage take small proportion project effort 
hard conceive entire process automated 
increased automation researchers need think statistical terms including matching model assumptions problem seeking interpretability quantifying variance regulating complexity improve generalization keeping lookout unexpected 
modern statistical modeling tools possible analyst think problem higher level handling routine massive tasks try numerous approaches estimate uncertainty arising complex processes iterate stages solution design settling representation scheme blend 
comparing kdd techniques attempting extract database sense try accessible modern statistical algorithms 
bibliography akaike 
information theory extension maximum likelihood principle 
proceedings second international symposium information theory eds 
petrov budapest academy 
asimov 
grand tour tool viewing multidimensional data 
siam journal scientific statistical computing 
barron barron 
statistical learning networks unifying view 
proceedings twentieth symposium interface computing elder pregibon science statistics reston virginia 
barron cook craig barron 
adaptive learning networks development application united states algorithms related ch 
self organizing methods modeling type algorithms ed 

new york marcel dekker 
kuh 
regression diagnostics identifying influential data sources collinearity 
new york john wiley sons 
bishop fienberg holland 
discrete multivariate analysis theory practice 
cambridge massachusetts mit press 
breiman 
comment neural networks cheng titterington statistical science 
breiman 
stacked regressions technical report dept statistics uc berkeley 
breiman friedman olshen stone 
classification regression trees 
monterey california wadsworth brooks 
cheng titterington 
neural networks review statistical perspective discussion 
statistical science 
clark pregibon 
tree models 
ch 
statistical models eds 
chambers hastie 
pacific grove california wadsworth brooks cole advanced books software 
cleveland 
robust locally weighted regression smoothing scatterplots 
journal american statistical 
association 
cover hart 
nearest neighbor pattern classification 
ieee transactions information theory 
cox john 
statistical method global optimization 
proceedings ieee systems man cybernetics society chicago oct 
practical guide splines 
new york springer verlag 
dempster laird rubin 
maximum likelihood incomplete data em algorithm discussion 
journal royal statistical society 
ungar 
comparison nonparametric estimation schemes mars neural networks 
computers chemical engineering 
statistical perspective knowledge discovery databases diaconis freedman 
asymptotics graphical projection pursuit 
annals statistics 
efron gong 
look bootstrap jackknife cross validation 
american statistician 
efron 
bootstrap methods look jackknife 
annals statistics 
elder iv 
comment views furnas buja 
journal computational graphical statistics 
elder iv 
global optimization probes expensive algorithm 
ph diss dept systems engineering university virginia may elder iv 
assisting inductive modeling visualization 
proceedings joint statistical meeting san francisco california aug 
elder iv 
user manual algorithm synthesis polynomial networks barron associates virginia 
th edition 
elder iv brown 
induction polynomial networks 
ch 
advances control networks large scale parallel distributed processing models vol 
ed 
fraser 
norwood new jersey ablex 
forthcoming 
available technical report ipc tr university virginia 
faraway 
cost data analysis technical report dept statistics univ michigan ann arbor 
ed 

self organizing methods modeling type algorithms 
new york marcel dekker 
freedman 
statistics 
new york ww norton friedman 
overview predictive learning function approximation 
ch 
statistics neural networks theory pattern recognition applications eds 
cherkassky friedman wechsler springer 
friedman 
multiple adaptive regression splines discussion 
annals statistics 
friedman stuetzle 
projection pursuit regression 
journal american statistical association 
friedman tukey 
projection pursuit algorithm exploratory data analysis 
ieee transactions computers 
elder pregibon wilson 
regression leaps bounds 
technometrics 
hampel 
influence curve role robust estimation 
journal american statistical association 
hastie pregibon 
shrinking trees technical report bell laboratories 
hastie tibshirani 
generalized additive models 
london chapman hall 
hastie tibshirani buja 
flexible discriminant analysis optimal scoring 
journal american statistical association 
huber 
robust estimation location parameter 
annals mathematical statistics 

group method data handling rival method stochastic approximation 
soviet automatic control 
kolmogorov 
representation continuous functions variables superpositions continuous functions variable addition 

lowe webb 
optimized feature extraction bayes decision feed forward classifier networks 
ieee transactions pattern analysis machine intelligence 
mallows 
comments cp 
technometrics 
mccullagh nelder 
generalized linear models nd ed 
london chapman hall 
michie spiegelhalter taylor eds 

machine learning neural statistical classification 
new york ellis horwood 
miller 
subset selection regression 
new york chapman hall 
mosteller tukey 
data analysis regression 
reading massachusetts addison wesley 
nelder 
generalized linear models 
journal royal statistical society 
sullivan jr 
automatic smoothing regression functions generalized linear models 
journal american statistical association 
statistical perspective knowledge discovery databases parzen 
estimation probability density function mode 
annals mathematical statistics 
ripley 
statistical aspects neural networks 
chaos networks statistical probabilistic aspects eds 
barndorff nielsen cox jensen kendall london chapman hall 
rissanen 
modeling shortest data description 
automatica 

estimator 
encyclopedia statistical science 
new york john wiley sons 
scott 
multivariate density estimation theory practice visualization 
new york wiley 

neural network tool 
ieee spectrum february 
cook buja 
xgobi interactive dynamic graphics window system link proceedings american statistical association meetings 
lee 
self organizing neural networks identification problem 
advances neural information processing systems ed 
touretzky 
san mateo california morgan kauffman 
tierney 
lisp stat new york john wiley sons 
tukey 
exploratory data analysis 
reading massachusetts addisonwesley 
weiss kulikowski 
computer systems learn classification prediction methods statistics neural networks machine learning expert systems 
san mateo california morgan kaufmann 
werbos 
regression new tools prediction analysis behavioral sciences 
ph diss harvard august 
wolpert 
stacked generalization 
neural networks 
