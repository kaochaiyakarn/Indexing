appeared proceedings ml twelfth intern 
conf 
machine learning morgan kaufmann 
ant reinforcement learning approach traveling salesman problem luca gambardella idsia corso lugano switzerland luca idsia ch www idsia ch marco dorigo iridia universit libre de bruxelles avenue franklin roosevelt cp bruxelles belgium eu ulb ac iridia ulb ac dorigo dorigo html introduce ant family algorithms similarities learning watkins apply solution symmetric asymmetric instances traveling salesman problem tsp 
ant algorithms inspired ant system distributed algorithm combinatorial optimization metaphor ant colonies proposed dorigo dorigo maniezzo colorni 
show particular instance ant family instances family perform better 
experimentally investigate functioning ant show results obtained ant symmetric tsp competitive obtained heuristic approaches neural networks local search 
apply ant difficult asymmetric tsp obtaining results ant able find solutions quality usually specialized algorithms 
new distributed algorithm combinatorial optimization introduced dorigo maniezzo colorni dorigo dorigo maniezzo colorni colorni maniezzo dorigo 
basic idea underlying algorithm called ant system colony cooperating ants find shortest hamiltonian tours weighted complete graph called traveling salesman problem tsp 
realized interpreted particular kind distributed reinforcement learning rl technique 
propose ant family algorithms strengthen connection rl particular learning 
family algorithms section different members family discussed experimentally compared section 
experiments run symmetric asymmetric traveling salesman problems 
section experimentally study properties ant 
particular show computation goes agents converge common path 
continue search solutions space 
show ant learns aq values favor agents discovery short tours 
section comparison results obtained ant obtained heuristic approaches sets standard tsp problems 
comparison ant resulted majority data sets best performing compared elastic net simulated annealing self organizing map farthest insertion 
applied ant difficult atsp problems finding results 
example ant able find iterations seconds pentium pc optimal solution city asymmetric problem balas 
problem solved optimality hours computation workstation best published code available atsp assignment problem relaxation fischetti toth atsp solved optimality fischetti toth algorithm polyhedral cuts branch cut scheme 
conclude section briefly discussing related 
ant family algorithms introduce ant algorithm application traveling salesman problem 
set cities pair cities distance rs tsp stated visited tours 
mean performance trials sec 
problem finding minimal length closed tour visits city 
instance tsp graph set cities set edges cities fully connected graph euclidean tsp 
case rs sr general asymmetric traveling salesman problem atsp 
ant algorithms apply problems 
aq read ant value positive real value associated edge 
ant counterpart learning values intended indicate useful move go city city aq changed run time 
ant applied symmetric tsp aq aq 
ant applied asymmetric tsp aq different aq 
heuristic value associated edge allows heuristic evaluation moves better tsp chose inverse distance 
agent task tour visit cities return starting 
associated list cities visited current city 
list implements kind memory constrain agents feasible tours tours visit cities 
agent situated city moves city rule called action choice rule state transition rule arg max aq parameters weigh relative importance learned aq values heuristic values value chosen randomly uniform probability parameter higher smaller probability random choice random variable selected probability distribution function aq 
equation case equation multiply aq value aq corresponding heuristic value 
choice meant favor aq values belonging shorter edges mainly motivated previous ant system 
composition functions different multiplication possible subject 
ant agents cooperate learn aq values favor probability discovery tsp solutions 
aq values updated rule aq aq daq max aq update term composed reinforcement term discounted evaluation state 
parameters learning step discount factor 
ant algorithms reinforcement daq zero agent completed tour 
delayed reinforcement daq computed discussed section 
update rule formula learning fact set available actions state set function previous history agent fig report generic ant algorithm 
algorithm called generic structural parameters instantiated 
action choice rule formula particular form probability distribution variable ii way delayed reinforcement daq formula computed distributed 
section study compare algorithms result different instantiations structural parameters 
iteration generic ant algorithm described words follows 
step initialization phase initial value aq values agent placed city chosen policy discussed section 
set visited cities initialized 
step cycle agents move aq updated discounted state evaluation repeated agent finished tour back starting city 
step length tour done agent computed compute see discussion section delayed reinforcements daq 
aq updated formula 
step checks termination condition met case algorithm returns step 
usually termination condition verified fixed number cycles improvement obtained fixed number cycles 
experiments optimal value known priori algorithm stopped soon optimum 
experimental comparison ant algorithms start section experimental investigation ant performance different instantiations structural parameters action choice rule way delayed reinforcement daq computed 
conclude brief description ant system inspired ant experimental comparison ant 
call parameters structural value changes form algorithm 
differ standard parameters formula need numerically optimized 
algorithm performance evaluated repeating trial times 
report means variances necessary significance comparisons means computed mann whitney tests kruskal wallis anova siegel castellan 
tables report average best performances 
average performance computed best result obtained trials computing mean 
best performance best result obtained trials 
experiments reported differently indicated value parameters set aq average length edges general number agents belong range range experimentally experiments necessary better understand role cooperation see dorigo gambardella preliminary results 
regarding initial positioning placed agent city 
initialization phase pair aq aq starting city agent set visited cities agent city city agent located 
step agents build tours 
tour agent stored tour local reinforcement null state evaluation update aq values 
choose city formula tour cycle agents go back initial city tour aq aq max aq formula reinforcement daq null new city agent 
step delayed reinforcement computed aq values updated formula state evaluation term max aq null compute length tour done agent edge compute delayed reinforcement daq delayed reinforcement daq function update aq values applying formula 
condition true print shortest goto step ant algorithm table comparison action choice rules 
type delayed reinforcement iteration best 
city problems stopped iterations 
oliver stopped iterations ry iterations 
averaged trials 
pseudo random pseudo random proportional random proportional mean std dev best mean std dev best mean std dev best city set city set city set city set city set oliver ry values set benchmark problems grid problems oliver city symmetric problem see example whitley starkweather ry city asymmetric problem see tsplib reinelt set city symmetric problems cities coordinates randomly generated durbin willshaw 
action choice rule tested ant algorithms action choice rules pseudo random pseudo random proportional random proportional 
obtained formula follows 
pseudo random rule formula random variable set set cities visited agent situated city selected uniform distribution 
action choice rule strongly resembles pseudo random action choice rule learning 
pseudo random proportional rule formula random variable set selected distribution formula gives probability agent city chooses city move 
aq aq random proportional rule pseudo random proportional 
grid problem problem cities evenly distributed squared grid 
results obtained city grid 
choice city done random selection edges chosen probability distribution formula 
action choice rule ant system 
table report results obtained action choice rules clearly shows pseudorandom proportional rule far best choice 
differences means resulted significant value kruskal wallis anova tests 
results obtained iteration best type delayed reinforcement see section 
similar results obtained global best type delayed reinforcement introduce section 
delayed reinforcement tested types delayed reinforcement called global best iteration best 
discussed 
experiments run pseudo random proportional action choice rule 
global best 
delayed reinforcement computed formula daq gb tour done agent gb parameter experiments set value optimized ant gb agent globally best tour trial gb tour length 
formula says aq values correspond edges belonging globally best tour receive reinforcement 
iteration best 
delayed reinforcement computed formula daq ib tour done agent ib ib agent best tour current iteration trial ib tour length 
results table show average methods give similar results 
reasons prefer iteration best type delayed reinforcement 
slightly faster finding solutions quality global best 
second important sensitive changes value parameter decided iteration best type delayed reinforcement rest stated 
case results best way computing delayed reinforcement aren definitive subject require research 
table comparison different ways compute delayed reinforcement 
city problems stopped iterations 
oliver stopped iterations ry iterations 
averaged trials 
ant global best ant iteration best mean std 
dev 
best mean std 
dev 
best city set city set city set city set city set oliver ry ant system ant system presents respect ant major differences 
delayed reinforcement computed formulas daq daq daq tour done agent length tour done agent number agents 
case delayed reinforcement edges belonging shorter tours chosen agents receive reinforcement belong longer tours chosen smaller number agents 
ant system opposed ant algorithms agents contribute delayed reinforcement 
second formula simplified aq aq daq applied edges visited agent 
choice due fact inspired observation ant colonies aq intended represent amount pheromone edge 
aq values updating formula meant simulate change amount pheromone due addition new pheromone deposited agents ants visited edges pheromone evaporation 
table indicates difficult problems ant outperforms ant system 
particular similar better results obtained fewer iterations 
table comparison best ant algorithm ant system 
ant algorithm pseudo random proportional action choice iteration best delayed reinforcement 
experiments stopped iterations optimal solution 
ant experiments stopped optimal solution iterations grid iterations oliver iterations ry 
results averaged trials 
ant ant system mean std 
dev 
best mean std 
dev 
best grid oliver ry interesting properties ant section highlight characteristics ant 
ant agents making tour ii learned aq values exploited agents find short tours 
results obtained ant algorithm pseudo action choice delayed reinforcement com puted iteration best method 
test problem ry asymmetric tsp 
qualitatively analogous results obtained problems 
experiments reported section computed performance learning test sessions 
learning performance computed running basic algorithm reported fig test sessions updating aq values switched set agents deterministically chose best edge available 
cases performance length shortest tour done agents considered iteration 
test sessions run learning iterations 
ran kinds test sessions 
called test session agent deterministically chose edge aq highest leading visited city 
second called test session agent deterministically chose edge product aq highest leading visited city 
ant agents converge common path characteristic ant system find ant agents converge common path 
observed running experiments 
see fig observed mean length agents tours standard deviation 
mean tour length diminishes algorithm runs clear see standard deviation fig agents converging common path 
ant algorithm continues improve best tour see mean length best tour fig 
iterations mean length best tour mean length agents tour mean mean std dev 
mean std dev 
mean length best tour mean length agents tour std 
dev 
problem ry 
averaged trials 
mean mean trials mean length tours agents 
second experiment observed mean branching factor changes computation goes 
branching factor node defined follows 
aq max aq min largest smallest respectively aq values edges exiting node aq max aq min 
parameter branching factor node number edges exiting associated aq value greater aq min 
mean branching factor gives indication dimension search space 
experiment results reported fig showed branching factor diminish monotonically computation goes 
ant maintains property typical agents reduce drastically search space reduction branching factor phases computation converge common path 
agents continue explore subset search space computation goes 
desirable property agents explore different paths higher probability find improving solution case tour useless agents 
behavior agents test session interesting thing observe behavior agents test session 
test session step agents deterministically choose edge highest aq value tours test session indicate aq values exploited agents find shortest tours 
fig show test session performances 
iterations lambda lambda lambda lambda branching factor 
problem ry 
averaged trials 
iterations mean length test session mean length test session best tour test session aq values test session aq values heuristic values test session 
test session run learning iterations 
problem ry 
averaged trials 
interesting observe early stages computation heuristic improves performance iterations performance slightly better performance 
indication fact computation goes aq values useful direct agents finding solutions 
aq values special meaning fact value iteration 
heuristic function useful early stages computation useless slightly detrimental aq values learned 
comparisons heuristics results difficult problems compared average behavior ant known heuristic methods elastic net en simulated annealing sa self organizing map som farthest insertion fi 
compared best result obtained ant obtained improved versions previous heuristics som improved version som consisting different runs som processing cities various orders sa fi plus local optimization opt opt lin comparison run set city problems durbin willshaw 
table reports average results obtained heuristic best average result heuristics bold font 
interesting note ant best performing algorithm 
ap opt opt known edge exchange procedures purpose improve tsp feasible tour hamiltonian tour 
edge exchange procedures arcs removed hamiltonian tour producing disconnected paths 
paths reconnected produce best possible tour 
tour optimal exchange produces tour lower length 
plied opt opt heuristics result produced ant local optimization heuristics able improve result 
solutions produced ant resulted locally optimal respect opt opt heuristics 
table compare best results obtained som sa opt farthest insertion plus local optimization opt opt 
case ant sa opt best performing algorithms 
comparison ant known heuristics gave encouraging results believe ant real strength solving asymmetric version tsp 
fact ant iteration complexity order quickly infeasible application big tsp problems exist heuristic exact methods 
exact methods optimal solutions instances hundreds cities largest tsp solved optimally cities padberg rinaldi 
hand atsp problems difficult tsp exact methods optimal solutions instances cities maintains iteration complexity applied tsp 
run experiments compare ant exact methods proposed fischetti toth 
table report best result time needed find exact algorithms mean best result obtained ant trials iterations 
test problems available tsplib reinelt 
problem ft exact algorithm able find optimal solution hours computation 
ant new family algorithms inspired learning algorithm observation ant colonies behavior 
ant authors knowledge application learning related technique combinatorial optimization problem traveling salesman problem tsp 
results obtained application tsp particular asymmetric version shown ant effective finding optimal solutions hard problem instances 
direction better understanding ant dynamics 
particular suspect connection ant learning tighter theoretical results obtained learning extended reformulated ant 
plan extend realm ant applications applying combinatorial optimization problems 
considered best exact methods atsp 
tsplib problem called atsp 
differ obtained atsp doubling length arcs 
table comparisons average result obtained city problems 
en elastic net sa simulated annealing som self organizing map fi farthest insertion fi opt best solution fi distinct runs opt fi opt best solution fi distinct runs opt 
results en sa som durbin willshaw 
fi results averaged trials starting different initial cities 
ant pseudo random proportional action choice iteration best delayed reinforcement 
run iterations results averaged trials 
city set en sa som fi fi opt fi opt ant table comparison best results obtained sa opt best solution simulated annealing distinct runs opt som best solution som different runs processing cities various orders fi locally optimized versions ant 
opt opt heuristics result fi starting configuration local optimization 
results sa opt som durbin willshaw 
ant pseudo random proportional action choice iteration best delayed reinforcement 
run iterations best result obtained trials 
city set sa opt som fi fi opt fi opt ant table comparison exact methods ant difficult atsp problems 
numbers parenthesis seconds 
type delayed reinforcement global best 
problem set 
ant run iterations results obtained trials 
problem ft ft ant mean ant best result ry wish hugues bersini stimulating discussions subjects 
research partially supported individual cec human capital mobility programme fellowship marco dorigo years 
balas 
lift cutting plane algorithm mixed programs mathematical programming 
colorni dorigo maniezzo 
distributed optimization ant colonies 
proceedings ecal european conference artificial life paris france varela bourgine eds elsevier publishing 
colorni dorigo maniezzo 
investigation properties ant algorithm 
proceedings parallel problem solving nature conference ppsn brussels belgium nner manderick eds elsevier publishing 
dorigo 
optimization learning natural algorithms 
ph thesis politecnico di milano italy eu 
italian 
dorigo gambardella 
ant reinforcement learning approach combinatorial optimization 
tech 
rep iridia universit libre de bruxelles belgium 
dorigo maniezzo colorni 
ant system optimization colony cooperating agents 
ieee transactions systems man cybernetics press 
durbin willshaw 
analogue approach travelling salesman problem elastic net method 
nature 
fischetti toth 
additive bounding procedure asymmetric travelling salesman problem 
mathematical programming 
fischetti toth 
polyhedral approach exact solution hard atsp instances 
technical report deis universit di bologna italy april 
lin 
computer solutions traveling salesman problem 
bell syst 
journal 
padberg rinaldi 
facet identification symmetric traveling salesman problem 
mathematical programming 

traveling salesman problem neural network 
orsa journal computing 
reinelt 
traveling salesman computational solutions tsp applications 
springer verlag 
siegel castellan 
nonparametric statistics behavioral sciences 
mcgraw hill 
watkins 
learning delayed rewards 
ph 
dissertation psychology department university cambridge england 
whitley starkweather 
scheduling problems travelling salesman genetic edge recombination operator 
proceedings third international conference genetic algorithms morgan kaufmann 
