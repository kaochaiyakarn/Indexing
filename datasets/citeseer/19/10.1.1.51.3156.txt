predicting binary sequence optimal biased coin yoav freund laboratories yoav research att com www research att com orgs ssr people yoav april apply exponential weight algorithm introduced littlestone warmuth vovk problem predicting binary sequence best biased coin 
show case logarithmic loss derived algorithm equivalent bayes algorithm jeffrey prior studied xie barron probabilistic assumptions 
derive uniform bound regret holds sequence 
show empirical distribution sequence bounded away length sequence increases infinity difference bound corresponding bound average case regret algorithm asymptotically optimal case 
show gap necessary calculating regret min max optimal algorithm problem showing asymptotic upper bound tight 
study application algorithm square loss show algorithm derived case different bayes algorithm better prediction worst case 
show methods developed computational line learning theory applied basic statistical inference problem give think surprisingly strong results 
order results context find necessary review standard statistical methods problem 
consider simple prediction problem 
observe sequence bits bit time 
observing bit predict value 
prediction tth bit terms number 
outputting close corresponds confident prediction close corresponds confident prediction 
outputting corresponds making vacuous prediction 
formally define loss function theta non negative real numbers value loss associate making prediction observing bit shall consider loss functions square loss gamma log loss log gammax log gamma gamma log gamma absolute loss jx gamma pj 
goal prediction algorithm predictions incur minimal loss 
course assumption sequences order hope making predictions better predicting turns 
popular simple assumption sequence generated independent random coin flips coin fixed bias goal find algorithm minimizes total loss incurred sequence 
minimal achievable loss depends informative consider difference incurred loss minimal loss achievable omniscient statistician knows true value chooses optimal prediction distribution 
denote binary sequence length denote distribution sequences bit chosen independently random probability average regret prediction algorithm average difference total loss incurred total loss incurred omniscient statistician 
symbols av gamma term average regret differentiate worst case regret define 
general optimal algorithm minimizing average regret bayes algorithm 
variants bayesian methodology call variants subjective bayesianism logical bayesianism ffl subjective bayesianism view distribution set models defined axiomatically representation knowledge state mind statistician regarding identity correct model model class 
choosing appropriate prior distribution case act compiling knowledge exists seeing data form prior distribution 
chosen goal prediction algorithm minimize expected average regret chosen random prior strictly speaking exist non symmetric loss function vacuous prediction 
concentrate symmetric loss functions vacuous prediction 
sake simplicity restrict discussion deterministic prediction algorithms 
case prediction function previously observed bits 
distribution generated find minimizes av 
easy show bayes algorithm prior achieves optimality criterion respect log loss 
ffl logical bayesianism view attributed wald assumption model chosen optimality criterion average regret worst case choice minimized 
interestingly number trials fixed ahead time min max optimal strategy adversary chooses value choose fixed distribution worst see blackwell ferguson haussler 
min max optimal prediction strategy case bayes prediction algorithm prior set worst prior distributions peculiar see calculating hard importantly optimal known advance 
attractive option find algorithm need know advance 
bernardo suggested bayes algorithm jeffrey prior denote bj clarke barron proved choice asymptotically optimal models interior set 
xie barron performed detailed analysis bj algorithm model class biased coins shown ffl ff lim max ffl ff gammaffl ff av bj gamma ln ln methodologies prediction methodologies general statistical assumption 
assumes sequence predicted generated independent random draws unknown distribution selected known class distributions 
difference bayesian worst case methodologies regards way specific distribution chosen class 
assumption particular source sequences really called random source problematic real world cases impossible verify particular data source random source 
lack data computing power wants simple stochastic model situations known sequence generated complex random process mechanism random 
jeffrey prior case known prior dirichlet prior see equation exact definition 
xie barron give results slightly stronger form results form 
accepted approach measuring randomness particular sequence measure kolmogorov complexity see words compare length sequence length shortest program generating 
random sequences program significantly shorter sequence 
measure mathematically elegant usually practical measure compute 
question weaker formal assumption correspond intuition biased coin approximate model data 
assumption study exists fixed model total loss sequence non trivial 
direction builds ideas prediction worst case line learning universal coding universal portfolios universal prediction 
define trivial trial loss maximal loss incurred best prediction 
formally inf max easy verify prediction minimizes loss log loss square loss absolute loss corresponding values trivial loss log respectively 
say prediction algorithm non trivial predictions particular sequence xt total loss algorithm incurs sequence significantly smaller tl 
case biased coins assumption exists fixed value total loss significantly smaller tl 
define directly concept significant difference 
define requirements prediction algorithm relative total loss incurred optimal value say ffl significant difference total prediction loss exists prediction algorithm total loss guaranteed smaller tl tl gamma ffl 
measure performance prediction algorithm specific sequence xt difference gamma min worst case regret prediction algorithm maximum difference sequences length defined rwc max gamma min denote argument minimizes second term note log loss square loss equal fraction refer fraction empirical distribution denote 
clearly rational number form integer range absolute loss 
shall see prove slightly better bounds regret allow dependence therefor define quantity rwc max gamma min consider situation sequence generated random source analysis done terms worst case regret 
rwc max max gamma gamma delta max max gamma gamma delta max gamma gamma delta max av inequality follows replacing maximum average second inequality follows replacing optimal sequence choice global choice see worst case regret upper bound regret 
relation surprising 
surprising result prove class biased coins respect log loss worst case regret slightly larger average case regret described equation 
prove bound worst case regret bj algorithm problem 
ffl ff lim max ffl ff gammaffl ff rwc bj gamma ln ln observe difference bound bound equation just 
show tiny gap necessary 
calculating min max optimal algorithm worst case regret showing asymptotic difference ln ln 
asymptotic advantage min max algorithm sequences empirical distribution close case regret larger additional 
algorithm suggested logical bayesian analysis optimal respect worst case regret 
result complements results xie barron 
result merges nicely method stochastic complexity advocated rissanen 
prediction algorithm translated coding algorithm vice versa example arithmetic coding 
length code sequence equal bit cumulative log loss corresponding prediction algorithm sequence 
result means bayes method jeffrey prior optimal universal coding algorithm models class biased coins including additive constant expression min max redundancy sequences empirical distribution extreme 
goal minimize cumulative log loss total code length stochastic complexity logical bayesianism worst case prediction suggest algorithm achieve essentially identical bounds 
interested loss function different log loss worst case methodology suggest algorithm differs significantly bayes algorithm 
specifically show algorithm suggested different algorithm suggested bayesian approach 
give example worst case regret bayesian algorithm significantly larger exponential weights algorithm 
prediction algorithms efficient 
prediction rule function number bits observed number bits equal 
suggested prediction rule cumulative log loss possible prediction rule square loss ln ln erf iq erf iq gamma erf iq erf iq gamma erf gammax dx cumulative distribution function normal distribution 
prediction rules close large slightly different initial part sequences 
organized follows 
section review exponential weights prediction algorithm known bound case number models finite 
section show algorithm analysis extended case class models uncountably infinite 
section basic bound laplace method integration 
section apply method case log loss section compare bound bounds regarding cumulative log loss 
section apply method square loss section briefly review known absolute loss 
conclude general comments open problems section 
details proof appendix 
trivial observation prediction algorithm viewed bayesian algorithm prior distribution defined set sequences set models 
observation little value suggest interesting way finding distribution calculating predictions generated 
algorithm algorithm study direct generalization aggregating strategy vovk weighted majority algorithm littlestone warmuth 
refer exponential weights algorithm denote ew 
denote class models section assume finite set values element pn denote cumulative loss model time 
algorithm simple 
receives positive real numbers parameters 
model class time step associates weight exp gamma delta initial weights sum set models finite initial weights usually set models prediction algorithm time denote oe function weights associated models time 
describing predictions chosen describe bound total algorithm 
backwards choice prediction trivial bound 
bound total loss algorithm time step form oe gammac ln want bound hold times sequences clear predictions 
prediction chosen possible values bound hold holds specifically means prediction oe chosen gammac ln oe gammac ln gamma ln oe gammac ln way bound usually observe total loss best model observing min weight associated best model total weight lower bounded exp 
plugging equation find simple algebra oe ln cjl cj immediately get nice simple bound worst case regret algorithm rwc ln clear small possible 
haussler kivinen warmuth studied problem combining models predicting binary sequence detail 
give formula calculating minimal value loss function broad class bound holds results log loss square loss absolute loss 
uncountably infinite sets models apply exponential weights algorithm case set models set biased coins 
natural extension notion weights associated model finite class assume measure defined set models initial weights sum initial measure denoted probability measure 
shall refer initial probability measure prior 
define measure follows denotes integration respect measure similarly prediction rule equation prediction time oe satisfies oe gammac ln gamma oe gammac ln gamma bound guaranteed case oe gammac ln interestingly set pairs bound equation guaranteed identical set equation holds 
proofs identical replace sums integrals proofs haussler immediately clear relate bound total loss worst case regret 
number models infinite bound form logn meaningless need different bound 
rest develop bound appropriate model class biased coins method laplace integration 
measure space function set measurable sets case borel sets real numbers range 
probability measure measure assigns value set consists domain 
equation get bound worst case regret rwc ew max gammac ln gamma notice case biased coin cumulative loss model sequence written form gamma expression rewriting second term positive exponential form get rwc ew max gammac ln exp gamma gamma ij gammac ln exp gamma ijo combine exponents terms get rwc ew max gammac ln gammat gamma gamma delta gamma gamma gamma delta refer gap function 
gap function additional loss trial model suffers model optimal model sequence empirical distribution 
exponent integral equation zero optimal model negative 
laplace method integration section describe general method calculating integral equation 
derivation section done independently general scenario yamanishi 
shall see method sufficient prove bound worst case regret 
describe additional steps required 
require loss function properties 
easy verify log loss square loss model class biased coins properties 
proof property holds loss functions haussler 

achievable 
symbol denote minimal value satisfies criterion 

values continuous second derivative function 
exists function unique optimal model sequence empirical distribution 
denote clear context 
setting ew algorithm suggest constants defined condition initial probability measure density measure dx gamma dx theorem gives bound performance algorithm theorem fixed loss suffered exponential weights algorithm described sequence empirical distribution satisfies rwc ln gamma ln defined 
bound bound worst case regret 
asymptotic result applies sets finite sequences sequences empirical distribution 
course sequence empirical distribution belongs set sequences theorem holds 
term hidden dependence 
need uniform bound bound dependence properties sequence 
get bound need refined analysis point know special cases described sections 
theorem important bounds regret important sets sequences suggests choice initial probability measure 
proof theorem laplace method integration method approximating integrals form gammat dt show dependence exist vanishes ffl gamma ffl fixed ffl 
large values sufficiently smooth functions reals 
intuition method large contribution small neighborhood min argmin dominates integral 
taylor expansion min get estimate integral 
dependence contribution maximum depends maximum point derivative zero 
case min case concentrate case 
laplace method formally watson lemma gives asymptotic approximation integral case theorem watson functions segment reals 
assume dt dt exist continuous 
assume exists min min theta dt min 
assume taylor expansion neighborhood min gammat dt min gamma dt min gamma prove theorem proof theorem fix empirical distribution sequence empirical distribution 
fact defined density function rewrite bound equation maximum sequence gamma gammac ln exp gammat dp integral form defined equation min watson lemma get fixed value gammat gamma dp gamma order minimize bound want integral large 
precisely want choose maximize minimal value achieved choices 
distribution easy see minimum term description laplace method see chapter murray book 
see derivation equation 
fix need consider values form find slightly better choices fixed values goal choose single distribution large consider rational second derivative continuous equivalent considering 
bound maximized value term equal values 
arrive choice equation exactly cancels dependence term 
get max gamma dp gamma gamma gamma delta plug estimate bound equation get statement theorem 
move show suggested exponential weights algorithm achieve strong bound worst case regret log loss square loss class biased coins 
log loss loss function case log gamma log optimal parameters optimal value sequence 
easy check case prediction rule oe exp gammat ln gamma ln gamma jj satisfies equation log loss 
note rule equivalent bayes optimal prediction rule prior distribution gap function case minus kl divergence 
gamma log gamma gamma log gamma gamma gammad kl jjp second derivative fisher information gamma optimal prior gamma prior jeffrey prior model class algorithm suggested case bayes algorithm jeffrey prior bj 
bound worst case regret calculate integral equation case equal gammat dkl jjp dp details calculation appendix state resulting bound theorem regret exponential weights algorithm class biased coins uses prior distribution described equation respect log loss bounded rwc ew ln ln gamma min gamma gamma implies rwc ew ln inequality shows regret exponential weights algorithm holds uniformly sequences 
worthwhile consider precise bound inequality 
fixed ffl ffl gamma ffl terms converge zero bound converges ln ln bound holds ffl ff gamma ffl ff ff 
get slightly larger bound ln ln get intermediate bound 
comparison results regarding log loss interesting compare bounds ones xie barron 
analyze algorithm similar problem consider expected regret worst case regret 
shown worst case regret upper bounds average case regret 
definition regret stronger theirs probabilistic assumption mechanism generating sequence 
therefor surprising bounds get close bounds 
arguments previous section get theorem implies bound equation 
difference bound bound derived xie barron described equation ln gamma ln bits words knowing sequence generated independent random draws random coin worth bit 
xie barron show asymptotically optimal algorithm respect average prior 
endpoints loss larger interior points difference vanish 
order achieve asymptotic min max suggest multiplying jeffrey prior gamma gammaff ff putting probability mass actual asymptotic value regret worst case average case bj algorithm slightly smaller ln ln 
points gamma constant reduces asymptotic average regret endpoints asymptotically optimal value changing asymptotic average regret interior points 
similar observations fix hold worst case regret 
show section regret algorithm bj interior ln ln larger optimal performance respect average regret 
show small gap removed 
calculating regret min max optimal algorithm worst case regret respect log loss 
known lemma stated instance cesa bianchi 
lemma states min max optimal defines distribution set sequences length max max regret suffered optimal algorithm sequence equal ln explicitly calculate worst case regret min max optimal algorithm result previously shown 
lemma worst case regret min max optimal prediction algorithm sequences length denote mm respect class biased coins log loss rwc mm ln gammat ln ln gamma proof appendix square loss section consider loss function gamma shown vovk haussler optimal parameters case optimal model 
gap function case gamma gamma gamma gamma gamma gamma gamma second derivative constant optimal prior uniform distribution bound worst case regret calculate integral equation case equal exp gamma gamma gamma gamma gamma gamma gamma jj dp details appendix resulting bound theorem regret exponential weights algorithm class biased coins uses uniform prior distribution respect squared loss bounded rwc ew ln ln erf erf gamma gamma ln implies rwc ew ln ln erf gamma ln similar detailed analysis log loss case inequality gives uniform upper bound depend sequence assume ffl gamma ffl constant ffl inequality gives slightly better asymptotic bound 
second case erf delta functions converges second term vanishes left negative term gamma ln 
erf delta terms converges remains get additional term ln regret 
square loss restriction distance bit stronger log loss case 
ffl ff gamma ffl ff ff better bound holds get bound interior bound bound 
comments order 
concerned computational efficiency algorithms log loss version version require small constant time calculate predictions formulas equations 
second hard give examples finite model classes ew algorithm better bayes data generated model outside class see 
conjecture example exists continuous model class 
absolute loss section consider absolute loss different properties log loss square loss 
haussler finite value bound holds shown cesa bianchi prediction algorithm worst case regret depend loss best model 
hand choices equation holds cesa bianchi shown fact exponential weights algorithm finite model classes devised 
worst case regret algorithm bounded ln nl 
choose finite multiplicative weights algorithm technique theorem case 
need infinite set models analysis case 
case optimal model class 
consider ew algorithm combines models get close optimal bounds regret 
open problems demonstrated simple case bayes algorithm shown xie barron optimal respect average case regret optimal respect worst case regret 
bound worst case regret slightly worse average case regret 
shown different algorithm results interested square loss log loss 
results give evidence accurate statistical inference done assuming world random 
currently working extended general classes models sequences larger alphabets 
acknowledgments special sebastian seung help understanding solving integral equations 
andrew barron meir feder david haussler rob schapire vovk xie kenji yamanishi helpful discussion suggestions 
abramowitz stegun 
handbook mathematical functions 
national bureau standards 
bernardo 
posterior distributions bayesian inference 
roy 
statistic 
soc 
ser 

david blackwell 
theory games statistical decisions 
dover publications new york 
cesa bianchi yoav freund 
helmbold david haussler robert schapire manfred warmuth 
expert advice 
proceedings fifth annual acm symposium theory computing pages 
appear journal association computing machinery 
bertrand clarke andrew barron 
jeffrey prior asymptotically favorable entropic risk 
stat planning inference 
cover ordentlich 
universal portfolios side information 
unpublished manuscript 
thomas cover 
behavior sequential predictors binary sequences 
thomas cover 
universal portfolios 
mathematical finance january 
davisson 
universal noiseless coding 
ieee trans 
inform 
theory 
feder merhav gutman 
universal prediction individual sequences 
ieee transactions information theory 
thomas ferguson 
mathematical statistics decision theoretic approach 
academic press 
dean foster 
prediction worst case 
annals statistics 
david haussler 
general minimax result relative entropy 
unpublished manuscript 
david haussler kivinen manfred warmuth 
tight worst case loss bounds predicting expert advice 
computational learning theory second european conference eurocolt pages 
springer verlag 
ming li paul vitanyi 
kolmogorov complexity applications 
texts computer science 
springer verlag 
nick littlestone 
learning irrelevant attributes abound 
th annual symposium foundations computer science pages october 
nick littlestone manfred warmuth 
weighted majority algorithm 
information computation 
murray 
asymptotic analysis 
springer verlag 
rissanen 
stochastic complexity statistical inquiry volume series computer science 
world scientific 
rissanen glen langdon jr universal modeling coding 
ieee transactions information theory january 
yu 
shtarkov 
coding sources unknown statistics 
csiszar elias editors topics pages 
north holland amsterdam 
yu 
shtarkov 
universal sequential coding single messages 
problems information transmission july september 
vovk 
game prediction expert advice 
proceedings eighth annual conference computational learning theory 
vovk 
aggregating strategies 
proceedings third annual workshop computational learning theory pages 
watson 
theory bessel functions 
cambridge university press 
xie andrew barron 
minimax redundancy class memoryless sources 
unpublished manuscript 
kenji yamanishi 
decision theoretic extension stochastic complexity applications learning 
unpublished manuscript 
zhang 
discrete noninformative 
phd thesis yale university 
proof theorem want calculate integral gammat dkl jjp dp expand write follows gamma exp log gamma log gamma gamma dp gamma gamma gamma gamma gamma gamma dp luckily integral studied quantity called beta function 
specifically gamma expressed terms gamma function 
relations get gammat dkl jjp dp gamma gamma gamma gamma gamma gamma plugging formula equation get gamma ln gamma gamma gamma ln ln ln gamma ln gamma gamma ln gamma ln gamma asymptotic expansion ln large values give upper lower bounds function positive values see equations 
gamma ln gamma ln gamma ln gamma ln gamma ln bounds get statement theorem details appendix 
ln gamma ln gamma ln gamma ln ln gamma ln gamma ln gamma ln gammat ln gamma ln gamma gammat gamma ln gamma gamma gamma ln gamma gamma gamma ln ln gamma ln gamma essentially gamma function extension factorial reals beta function extension reciprocal binomial function 
ln ln ln gamma ln gamma gamma gamma ln ln gamma min gamma gamma ln proof lemma 
bound regret min max optimal algorithm equal ln normalization factor min max distribution sequences length goal calculate lim ln zt lim ln zt max probability assigned sequence model depends number 
gamma gamma gamma gamma value maximized get zt gamma gamma gamma delta sequences length fraction achieves values rewrite equation form 
gammat gammap ln gamma gamma ln gamma binary entropy approximate value equation replace gamma delta equivalent expression gamma move log expression exponent get zt exp gamma ln gamma ln gamma ln gamma gamma th delta replacing delta series expansion definition get exponent expression ln gamma ln gamma ln gamma ln gamma gamma ln gamma gamma gamma ln ln gamma ln gamma gamma ln ln gamma ln gamma gamma ln gamma ln gamma ln gamma gamma ln gamma ln ln gamma ln gamma gamma ln gamma ln gamma line expression terms constant third term 
terms second line range 
term equal gamma ffl gamma ffl fixed ffl second third term asymptotic behavior 
dominant term case 
returning equation separate sum parts follows zt gammat fflt gammat gammaffl fflt gammat gammaffl gammat approximations shown upper bound summands third sums gammat gamma ln gamma estimate summands second term gammat gamma ln gamma convenient way writing common factor gamma gammai equalities write second major summation follows gammaffl fflt gammat gammaffl fflt gammai observe sum easy see riemann sum finite approximation integral gammaffl ffl gamma dp function gamma riemann integrable sum approaches value integral get gammaffl fflt gammat gammaffl ffl gamma dp similarly get sum corresponds terms approach fflt gammat ffl gamma dp gammaffl gammat gammaffl gamma dp integrals ffl take limit take limit ffl get lim lim ffl lim fflt gammat gammaffl fflt gammat gammaffl gammat lim ffl ffl gamma dp gammaffl ffl gamma dp gammaffl gamma dp gamma dp log get lim ln zt gamma ln ln completes proof theorem 
proof theorem need calculate lower bound integral exp gamma gamma gamma gamma gamma gamma gamma jj dp integral simplifies exp gamma gamma dp known integral multiplicative constant integral normal distribution part real line 
integral closed algebraic form expressed error function erf delta 
exp gamma gamma gamma gamma dp gamma gamma gamma dp erf erf gamma jj erf gammax dx cumulative distribution function normal distribution 
erf increasing function find erf erf gamma minimized erf concave find minimum achieved 
get integral uniformly lower bound exp gamma gamma erf plug lower bound equation get regret uniformly upper bounded gamma ln erf ln ln erf gamma ln example exponential weights algorithm better bayes algorithm suppose model class consists just biased coins sequence generated flipping random coin bias 
consider bayes algorithm choice prior distribution algorithm observes zero sequence posterior distribution concentrated predictions necessarily oe 
causes algorithm average loss trial gamma 
hand consider ew algorithm uses uniform prior distribution models 
total loss algorithm guaranteed larger best model class ln 
large high probability best model average loss trial gamma 
average loss trial ew algorithm guaranteed quickly approach making clearly better choice bayes algorithm problem 
conjecture gap performance algorithms exists class models set biased coins 
able calculate optimal prior optimal bayes algorithm case 
