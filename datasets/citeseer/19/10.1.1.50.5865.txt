improving file system performance predictive caching james griffioen randy appleton randy dcs edu department computer science university kentucky lexington ky supported part nsf number ccr cda cda improving file system performance predictive caching despite impressive advances file system throughput resulting technologies high bandwidth networks disk arrays file system latency improved little cases worse 
consequently file system remains major bottlenecks operating system performance ous 
investigates automated predictive approach improves file cache performance utilizes increased device bandwidth reduces file system latency 
automatic prefetching uses past file accesses predict file system requests 
objective effectively masking access latencies 
short want transform file cache repository accessed data staging area data accessed 
designed implemented system measure performance benefits automatic prefetching 
current results show prefetching results improvement file system performance compared lru especially smaller caches 
alternatively prefetching reduce cache size 
file systems historically primary performance bottleneck general purpose computing systems ous 
rapid improvements processor speeds memory sizes network speeds solving file system bottleneck urgent 
importance file system problem evoked large amount research years 
efforts primarily focused improving file system bandwidth quite successful improving file system performance improved software hardware techniques ho wil pg 
despite advances high latencies plague conventional file systems 
result file systems continue bottleneck system performance 
latency worse better due emerging technologies 
campus corporate scale distributed file systems wireless computing systems environments isdn ppp slip cd rom worm storage systems certain raid systems exhibit relatively high latencies 
new features offered systems mobility access home massive storage capacity extremely popular despite bandwidth latency limitations 
short count hardware higher bandwidth techniques devices solve file system bottleneck especially light increasing popularity low performance environments 
develop techniques fully utilize available bandwidth resources hide avoid latency 
file caching enormous performance boost effectively eliminating large percentage disk accesses smi nwo bad 
conventional file systems exhibit cache misses file system remains performance bottleneck ous 
unfortunately increasing cache sizes yield type performance improvements 
experience shows relative benefit caching decreases cache size cache cost increases och nwo 
large caches feasible affordable emerging computer devices pdas notebooks workstations pc 
complicate things natural cache size rapidly substantially larger fraction forth third total memory due part larger files big applications databases video audio bhk 
means larger caches necessary just maintain current hit rates 
note cache high hit ratio necessarily mean fast access 
distributed file systems open close functions represent synchronization points shared files 
file may reside client cache open close call executed server consistency reasons 
latency calls quite large tends dominate costs file file cache 
simply stated goal research transform conventional file system cache repository accessed data staging area data accessed 
pull data machine local memory attempt keep pace cpu demand data 
optimize file system performance deliver data just time 
delivery insures data available cpu needs avoids removing necessary data unnecessarily consuming valuable system resources network bandwidth disk bandwidth excessive amounts physical memory file cache 
develop new methods anticipate data requirements achieve just time delivery fully utilize high bandwidth provided hardware 
requesting data advance need data benefit emerging low bandwidth systems 
low bandwidth devices significant idle periods unused bandwidth 
anticipating data needs sufficiently far advance need file system effectively utilize idle bandwidth high low bandwidth devices 
investigated methods successfully reducing perceived latency associated file system operations general purpose computing system 
describe new method masking file system latency called automatic prefetching attempts achieve just time delivery 
automatic prefetching takes heuristic approach knowledge past accesses predict access user application intervention 
result applications automatically receive reduced perceived latencies better available bandwidth batched file system requests improved cache utilization 
describes automatic prefetching system 
section provides overview automatic prefetching algorithm ga motivation design choices 
section uses trace base simulations show potential performance performance benefits automatic prefetching 
verify results implemented prefetching file system sunos 
section describes results tests verifies simulation results 
section briefly discusses related 
prefetching certainly new idea wide variety settings ranging processor caches databases large scale scientific computations 
differs past focus completely automated prefetching system targeted general purpose multiuser computer systems 
automatic prefetching investigating approach call automatic prefetching operating system predicts file requirements attempts data available cache needed 
basic idea automatic prefetching file activity successfully predicted past file activity 
research explores methods correctly predict file access limited past information 
assuming predictions system employ policies algorithms decide prefetch file prefetch ordering prefetch requests 
automatic prefetching appealing characteristics 
existing applications need modified rewritten take advantage prefetching system 
applications receive benefits automatic prefetching including existing software 
second operating system automatically performs prefetching application behalf application writers concentrate solving problem hand worrying optimizing file system performance 
third operating system monitors file access application boundaries detect access patterns span applications 
consequently operating system prefetch files substantially earlier file needed application begins execute 
fourth predictions improve cache management algorithm effectively reducing cache misses prefetching occurs 
operating system predictions far advance need data distributed file system client batch multiple file requests single request server send idle times 
goal research determine approach viable 
second goal develop effective prefetch algorithm evaluate performance simulations 
third goal confirm results working prototype 
sections consider objectives turn describe results 
automatic prefetching viable 
determine viability automatic prefetching analyzed current file system usage patterns 
gather specific information needed analysis modified sunos kernel monitored file system activity systems day interval 
addition recording file system calls system kernel gathers precise information issuing process time taken operation 
timing information serves indicator system performance provides information prefetching substantial effects performance 
accurate timing information allowed compute read complete time trace 
define read complete time total amount time applications spent waiting file system reads complete 
read complete time easily calculated summing read wait times reads issued trace 
read complete times accurate measure file system performance cache rates 
cache rates measure amount missing data delay required bring data 
read complete times incorporate delay access data recording amount time applications spent waiting file system 
gathered variety traces including normal daily usage researchers various synthetic workloads 
traces collected sun sparcstation supporting users executing variety tasks 
traces collected varying time periods longest traces spanning days containing operations 
users restricted way 
typical daily usage included users processing email editing compiling preparing documents executing task typical academic environment 
traces post processed eliminate certain file accesses interfere results 
example sunos executables load standard set libraries including libc startup 
practice libraries physical memory 
counting loading libraries file accesses cache hit rate file cache 
eliminated libraries 
eliminated accesses devices appear file system example dev null dev tty dev zero 
eliminated files tmp usr tmp temporary files unsuitable prefetching 
brevity results trace 
results traces workloads app ga 
workloads differed trace trace significant percentage recurring predictable access patterns traces 
particular trace contains normal daily usage students database activity 
characteristics trace table 
trace duration days ratio write read ratio exec read number reads total kb total unique files avg 
app 
read complete time ms table characteristics trace trace analysis initial analysis trace data indicates typical file system usage realize substantial performance improvements prefetching provides guidelines successful prefetching policy 
data shows relatively little time moment file opened moment read occurs see 
fact median time traces milliseconds 
consequently prefetching occur significantly earlier open operation achieve significant performance improvement 
prefetching open time provide minor improvements 
second data shows median amount time successive opens substantial ms 
operating system accurately predict file accessed exists sufficient amount time prefetch file 
percent opens time ms histogram times open read file 
prediction algorithm designed implemented prediction algorithm informed guesses file accesses past access patterns 
past file access information obtained trace data predictor dynamically creates logical graph called probability graph 
node graph represents file file system 
describing probability graph define lookahead period construct graph 
lookahead period defines means file opened soon file 
analyzer defines lookahead period fixed number file open operations occur current open 
file opened period open considered occurred soon current open 
physical time measure virtual time measure measure easily obtained argued better definition soon unknown execution times file access patterns applications 
results show measure works practice 
say files related files opened lookahead period 
example lookahead period file opened file considered related current file 
lookahead period file opened files current file considered related current file 
predictor allocates node probability graph file interest file system 
unix exec system calls treated opens included probability graph 
graph node stored disk meta data associated file 
alternatively graph stored main memory results app compact graph 
arcs probability graph represent related accesses 
open file follows lookahead period open second file directed arc drawn second 
larger lookaheads produce arcs 
arc weighted number times second file accessed file 
node associated total open count describing total number times associated file opened 
graph represents ordered list files demanded file system arc represents probability particular file opened soon file 
host dynamically builds single probability graph containing files accessed host 
considered graph user file system nfs local disk single system wide graph acceptable times preferable ga 
files prefetched certain file systems file systems preferable unified probability graph 
having graph system reduces total amount space taken probability graph 
building system wide graph allows access patterns shared users 
example mosaic started reads large file start information 
pattern discovered system user user gains advantage faster mosaic startup times 
applications exhibit behavior 
behavior tends quite consistent long periods time change 
illustrates structure example probability graph 
conf passwd login nodes example probability graph 
probability graph provides information necessary intelligent prefetch decisions 
likelihood file file estimated probability graph ratio number arcs file file divided total open count file say prediction follows reasonable estimated likelihood prediction tunable parameter minimum chance algorithm willing accept 
say prediction correct file predicted opened lookahead period 
establishing minimum chance requirement crucial avoid wasting system resources 
absence minimum chance requirement predictor produce predictions file open consuming network cache resources prediction incorrect 
measure success predictor define accuracy value 
accuracy set predictions number correct predictions divided total number predictions 
accuracy large minimum chance practice substantially higher 
accuracy tradeoff number predictions open call varies required accuracy predictions 
requiring accurate predictions predictions wrong means limited number predictions 
typical set data trace relatively low minimum chance value probability graph algorithm averages files predicted file open 
higher minimum chance values algorithm averages files predicted open 
relatively low minimum chance algorithm prediction time correct approximately predictions 
shows tradeoff 

predictions open predictions lookahead graph tradeoff accuracy quantity 
prefetch algorithm prefetch algorithm works follows 
time file accessed arcs corresponding node probability graph examined 
files pointed arcs estimated likelihood greater added candidate list files prefetch 
candidate list built information current open added probability graph described section 
candidate list empty prefetch algorithm useful predictions 
assuming candidate list non empty cache searched file list 
file candidate list file cache associated lru access time changed current time 
changing access time current time causes files remain cache longer preventing flushed accessed 
note just updating access time candidate files improve cache performance data prefetched 
file cache system decide file prefetched 
time file prefetch occur relative time file needed controlled lookahead parameter probability graph algorithm 
avoid wasting valuable cpu time memory space predicting files intention prefetching lookahead value control number predictions prefetch files predicted 
consequently prefetch algorithm prefetch files predicted predictor 
larger lookaheads cause file predictions effectively increasing time file prefetch actual need data 
setting lookahead small causes files predicted prefetched closer actual need 
section discusses effects lookahead parameter settings 
amount data prefetch predicted file controlled parameter 
portion file bytes brought prefetch 
set kb results shown 
experimented settings large changes produce minimal changes performance 
note files prefetched entirety files significantly smaller irl 
remaining portions brought read ahead described section 
read ahead works large files 
simulation system measure performance algorithms built simulator emulates file system cache read ahead local disk remote disk intervening network 
simulator uses timestamps traces compute times file accesses occur adjusted file system model simulated 
result simulator calculate rates total runtimes prefetching variety local remote file system models 
local disk model parameters latency throughput 
time needed access file sum access latency result dividing length file bandwidth device 
file access may device time 
file accesses near simultaneously second access queued finishes 
simulations local disk latency seek time assumed ms including software latencies 
disk throughput assumed mb sec includes software delays time spent copy buffers 
remote disk model parameters local disk model additional network latency network throughput network portion model 
experiments reported simulated network round trip latency ms realizable bandwidth mb sec 
real computer networks simulated network allows multiple outstanding requests 
concurrent requests serialized remote disk drive 
remote file system model possible prefetching component file system request file initiate transfer receive application request file prefetch transfer completes 
case resulting access considered cache file access completes significantly faster prefetching 
simulated file cache organized basic lru fashion kb cache line 
amount space needed hold prefetching graph varies dynamically graph built 
results simulator shown sections take account space required probability graph 
smallest cache sizes space needed hold graph small fraction total cache size 
initial results indicate space consumed algorithm sufficiently limited significantly altering algorithm performance app 
notice current disk subsystem re ordering requests 
particular preempt defer prefetch requests satisfy subsequent application requests 
reordering prioritizing requests represents area potential performance improvements 
simulation results performed simulation tests measure performance improvements achieved automatic prefetching 
results prototype system shown section 
particular set tests described trace taken day period containing unrestricted activity multiple users 
determine performance benefits prefetching ran simulations varying cache size lookahead value minimum chance measured lru performance case comparison purposes 
timing results recall section time open file read small prefetching effective 
shows simulator able predict prefetching files sufficiently far advance read file 
measurements indicate files predicted subsequently accessed prefetched ms actual need resulting cache hits time read 
percent prefetches time ms histogram times prefetch read access 
performance compared lru primary goal automatic prefetching bring necessary file data cache needed 
automatic prefetching successful expect read complete time prefetching experienced standard lru cache management 
shows read complete time file system incurred lru prefetching various cache sizes 
tuning lookahead prefetching performs better lru cache sizes cases outperforming lru 
note cache sizes shown prefetching provided performance lru cache half size 
machines similar file system performance half cache memory lru freeing memory applications enhancing performance 
particularly important machines limited amounts memory available file caching 
large memory machines benefit additional available free memory 
performance boost indicates number correctly prefetched pages offsets pages incorrectly forced cache prefetching small cache sizes 
read complete time ms cache size kb lru prefetch read complete time ms cache size kb lru prefetch application read complete time function cache size local disk networked disk 
prefetch parameters effect performance parameters significantly affect predictions predictor lookahead values 
recall lookahead represents close file opens need files considered related 
setting value large increases number files considered related file open may potentially cause files prefetched 
large lookaheads increase number files prefetched predictions response open request 
large lookaheads result files prefetched substantially earlier predictions advance 
result large lookaheads inappropriate smaller cache sizes perform larger caches case small caches large lookaheads tend prefetch files far advance need 
result data necessary current computation may forced cache replaced useless data needed far 
larger cache sizes cache may sufficient space load file data required disturbing file data required current computation 
minimum estimated probability file needed near 
larger cache sizes smaller values perform better 
setting low results aggressive terms small large relative measures cache size meaning small large depend workload 
small cache cache misses large cache misses 
prefetching 
cache large incorrect prefetches minimal affect performance 
small cache sizes somewhat higher 
small caches suffer cache misses 
low file system prefetch incorrect files 
small caches application reads incorrect prefetches compete space disk queue slowing application reads 
large cache suffer fewer cache misses effect pronounced large caches 
summary low aggressive large caches somewhat higher moderate small caches 
lookahead increase increasing cache size 
figures illustrate tradeoffs kb cache kb cache respectively 
clearly lookahead parameters highly sensitive cache size adjusted accordance cache size 
multiple settings particular cache size may result approximately equal read complete times 
case factors network congestion processing overhead aid selection appropriate parameter settings 
minimum chance percent lookahead read complete time ms minimum chance percent lookahead read complete time ms application read complete times function lookahead cache local disk networked disk 
prototype system confirm results simulator modified sunos kernel support automatic prefetching 
prototype system supports file prefetching local file system ufs remote file systems nfs 
approximately lines kernel code modified added implement prefetching 
modifications standard sunos file cache 
new read call added perform prefetch reads 
prefetch read call pulls prefetched data cache updates lru timestamp data cache 
obtain information past accesses added tracing code open exec system calls 
open exec logged pseudo device file system 
accesses certain files minimum chance percent lookahead read complete time ms minimum chance percent lookahead read complete time ms application read complete times function lookahead cache local disk networked disk 
interfere prefetching shared libraries temporary files pseudo devices logged 
user level predictor daemon tracks current file usage reading pseudo device containing file access log 
predictor daemon constructs probability graph described section 
periodically receipt appropriate signal user predictor daemon writes newly computed prefetch table kernel 
kernel immediately begins new table prefetching decisions 
prefetch table list tuples consisting file names 
element tuple name file opened generate prefetch requests 
remainder tuple specify files prefetched 
file open ed exec ed prefetch table consulted 
file opened table entry tuple files tuple prefetched cache 
differing prefetch strategies varying lookahead parameters tested computing appropriate prefetch table writing prefetch table kernel 
alternatively prefetching disabled writing null table prefetch table buffer 
second daemon called prefetch daemon asynchronously prefetch predicted files parallel running program 
time running program issues open call kernel consults current prefetch table constructs candidate list 
kernel awakens prefetch daemon load cache files candidate list 
prefetch daemon invokes new prefetch read call load cache copying data user space 
tested prototype writing shell scripts represent activities typical user may execute repeatedly time 
script compilation runs large compile prefetching kernel create new executable kernel 
compilation test indicates type performance user compiling source code 
second script typesetting large papers latex bibtex grep gnuplot fig dev miscellaneous programs 
typesetting test shows type performance author expect 
overhead required compute store prefetch table prototype just working system 
prefetch table occupies portion physical memory kernel consumes cpu cycles examining prefetch table calculating candidate list 
modified kernel record read complete times read operation defined section 
prototype results evaluated performance prototype prefetching system compilation typesetting shell scripts described earlier 
executed test scripts sparcstation running modified sunos kernel 
user files tex files typesetting test files compilation test resided network file server 
configuration reflects increasingly common situation user files reside file server accessible clients 
system files executables font files header files place local disk tests remote file server tests 
tests local disk swap space 
examine prefetching performs various cache sizes executed system mb memory mb memory 
test user applications running test script plus operating system associated system processes sole memory users 
ran sets tests measure performance system wide variety conditions 
particular varied lookahead total amount free memory location system files 
test recorded total run time wall time read complete time test 
test repeated times obtain average standard deviation runs 
tables show results 
cases prefetching significantly reduced application read complete time 
improvement ranged depending test 
general higher latency higher total read time implies larger performance gain 
prefetch read complete times compilation test average better non prefetch times prefetch read complete times typesetting test average better non prefetch times 
part improvements read complete times directly reflected total run time wall time performance improvements 
cases improved read complete times completely carried total run time 
profiling showed prefetching occasionally increase number virtual memory page faults offset prefetching gains 
addition algorithm incurs albeit minimal overhead store prefetch table search table prefetch opportunities context switch prefetcher daemon 
despite added overheads performance gains prefetching outweigh overheads ultimately reducing total run time 
note mb system prefetching exhibits read complete times better mb system prefetching 
similarly cases remote typesetting case mb system prefetching exhibits total run times equal better mb system prefetching 
means similar performance achieved half total memory 
achieving performance similar better memory systems twice size particularly important increasing number notebook users pda systems memory readily available 
free mem sys files prefetching read comp time std dev wall time std dev memory location 
seconds seconds seconds seconds local mb remote local mb remote table performance results compilation test 
free mem sys files prefetching read comp time std dev wall time std dev memory location 
seconds seconds seconds seconds local mb remote local mb remote table performance results typesetting test 
related concept prefetching variety environments including microprocessor designs virtual memory paging databases file read ahead 
long term prefetching file systems support disconnected operation sat sk 
prefetching improve parallel file access mimd architectures ke 
unix perform type intra file prefetching known read ahead 
operating systems notice file sequentially cause substantial parts file brought disk disk access 
read ahead substantially boost performance particularly large files 
read ahead little small files majority files och bhk 
achieve performance improvements provided caching read ahead develop aggressive innovative techniques 
strong performance improvement come prefetching files application opened file 
allows system time properly prefetch large files allows system prefetch numerous small files applications 
unfortunately current system prefetch file opened prefetch file opened application 
great difficulty determining files application need application opened 
relatively straight forward idea application inform operating system requirements 
proposed currently researched researchers including patterson pgg 
approach application program informs operating system file requirements operating system attempts optimize accesses 
basic idea approach application knows files needed needed 
step right direction see disadvantages solution app ga 
particular approach assumes application know file requirements sufficiently far advance real file requirements 
applications frequently know files access need access file consider compiler opening include files 
approach requires applications rewritten take advantage new prefetching system calls 
application prefetching decisions knowledge activity concurrently running processes 
tait duchamp td proposed approach prefetches files application working sets soon specific instance application executed uniquely identified 
algorithm employs trees processes recording file activity working sets trees 
operating system attempts find unique patterns trees prefetch entire working set subtree soon identified 
automatic prefetching takes fundamentally different approach 
periodically prefetching entire working set automatic prefetching continuously pulls files system rate sustainable machine cache 
automatic prefetching supports prefetching completely unrelated applications 
tait assumption prefetching working set occurs background completes data needed admit uncertain assumption 
experience ga results indicate assumption rarely true implying amount data prefetched carefully regulated 
despite limitations simulated rate analysis assumes prefetching overhead fixed number files cache size indicates prefetching significantly reduce cache misses 
kroeger long begun investigate tree algorithm predict file access patterns kl 
approach motivated fact study past events determine probability events problem quite similar problem data compression 
consequently employ trees data compression method called prediction partial match 
research early stage report simulated performance gains standard lru rates 
results significantly worse results 
research may preliminary draw firm 
results show reasonable predictions past file activity 
result automatic prefetching substantially reduce latency better available bandwidth improved cache utilization 
simulator prototype show prefetching improve file system performance 
wide area distributed file systems raids wireless communication high latency possibly low bandwidth systems prevalent predictive caching prefetching increasingly important reducing latency achieving high performance file system app randy appleton 
automatic file system prefetching 
technical report university kentucky may 
app randy appleton 
automatic prefetching file system 
phd thesis university kentucky lexington ky 
bad baker ousterhout seltzer 
non volatile memory fast reliable file systems 
proceedings th international conference architectural support programming languages operating systems pages october 
bhk mary baker john hartman michael kupfer ken shirriff john ousterhout 
measurements distributed file system 
proceedings th acm symposium operating systems principles pages 
association computing machinery sigops october 
ga james griffioen randy appleton 
reducing file system latency predictive approach 
proceedings summer usenix conference june 
ga james griffioen randy appleton 
performance measurements automatic prefetching 
proceedings isca international conference parallel distributed computing systems pages september 
ho john hartman john ousterhout 
zebra striped network file system 
proceedings usenix file system workshop pages may 
irl gordon 
unix file size survey 
technical report usenet community trust 
ke kotz ellis 
prefetching file systems mimd multiprocessors 
ieee transactions parallel distributed systems 
kl thomas kroeger darrell long 
predicting file system actions prior events 
technical report submitted usenix fall 
kotz 
parallel file system bibliography august 
seventh edition www cs dartmouth edu cs archive bib html 
kuenning 
design seer predictive caching system 
proceedings mobile computing systems applications december 
samuel leffler marshal mc michael karels john quarterman 
design implementation bsd unix operating system 
addison wesley 
msc morris satyanarayanan conner howard rosenthal smith 
andrew distributed personal computing environment 
cacm march 
nwo nelson welch ousterhout 
caching sprite network file system 
acm transactions computer systems february 
och ousterhout da costa harrison kunze kupfer thompson 
trace driven analysis unix bsd file system 
proceedings th symposium operating systems principles pages december 
ous john ousterhout 
aren operating systems getting faster fast hardware 
proceedings summer usenix conference pages june 
pg hugo patterson garth gibson 
exposing concurrency informed prefetching 
proceedings third international conference parallel distributed information systems pages september 
pgg hugo patterson garth gibson daniel jim zelenka 
informed prefetching caching 
proc 
th symposium operating systems principles december 
david patterson garth gibson randy katz 
case redundant arrays inexpensive disks raid 
acm sigmod pages june 
pgs hugo patterson garth gibson satyanarayanan 
informed prefetching converting high throughput low latency 
proceedings dags pc symposium pages hanover nh june 
dartmouth institute advanced graduate studies 
sat satyanarayanan 
coda highly available file system distributed workstation environment 
ieee trans 
computers april 
sk peter gail kaiser 
disconnected operation multi user software development environment 
proceedings ieee workshop advances parallel distributed systems pages october 
smi smith 
cache memories 
computing surveys september 
td carl tait dan duchamp 
detection exploitation file working sets 
proceedings ieee th international conference distributed computing systems pages may 
wil john wilkes 
house building distributed systems technology 
acm operating systems review april 
