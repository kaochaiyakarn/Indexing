machine learning fl kluwer academic publishers boston 
manufactured netherlands 
mixed memory markov models decomposing complex stochastic processes mixtures simpler ones lawrence saul research att com labs florham park nj michael jordan jordan cs berkeley edu university california berkeley ca editor padhraic smyth 
study markov models state spaces arise cartesian product discrete random variables 
show parameterize transition matrices models convex combination mixture simpler dynamical models 
parameters models admit simple probabilistic interpretation fitted iteratively expectation maximization em procedure 
derive set generalized baum welch updates factorial hidden markov models parameterization 
describe simple iterative procedure approximately computing statistics hidden states 
give examples mixed memory models provide useful representation complex stochastic processes 
keywords markov models mixture models discrete time series 
modeling time series fundamental problem machine learning widespread applications 
include speech recognition rabiner natural language processing nadas traffic surveillance forbes protein modeling haussler musical analysis synthesis weigend visual gesture recognition yamato numerous 
probabilistic models discrete time series typically start form markov assumption independent past 
purpose statistical estimation problems arise system possesses large number degrees freedom ii window knowledge required predict extends time steps 
cases number parameters specify markov model overwhelm amount available data 
particular system possible states memory length number free parameters scales exponentially difficulties compounded latent variable models markov assumption applies hidden state space 
case may computationally intractable infer values hidden states 
example order hidden markov models hmms computing likelihood sequence observations scales number hidden states rabiner 
practice exact probabilistic inference limited hmms relatively small tightly constrained state spaces 
saul jordan technical note propose principled way investigate markov models large state spaces 
done representing transition matrix convex combination mixture simpler dynamical models 
refer resulting models mixed memory markov models 
mixture distributions parameterize higher order markov models known raftery ney macdonald apply methodology generally factorial models models large state spaces represented cartesian product smaller ones 
note builds earlier describing factorial hmms ghahramani jordan dynamic probabilistic networks binder 
papers show complex stochastic processes graphically represented sets markov chains connected directed links common set observable nodes 
models arise naturally study coupled time series observations priori decomposition cartesian product random variables 
factorial hmms aim combine power latent distributed representations richness probabilistic semantics williams hinton 
capturing type probabilistic reasoning fundamental problem artificial intelligence dean kanazawa 
believe mixed memory markov models advantages representing complex stochastic processes learning examples 
parameters models admit simple probabilistic interpretation fitted iteratively expectationmaximization em procedure dempster 
em algorithm desirable properties including monotone convergence log likelihood lack step size parameters naturalness handling probabilistic constraints 
situations provides compelling alternative gradient learning methods baldi chauvin binder 
mixed memory models express rich set probabilistic dependencies making appropriate modeling complex stochastic processes 
applied factorial hmms generalize ghahramani jordan important directions introducing coupled dynamics considering non gaussian observations 
give rise simple iterative procedure making inferences hidden states 
describe procedure practical value cleanly illustrates idea exploiting tractable substructures intractable probabilistic networks saul jordan 
main significance lies application factorial hmms modeling coupled time series 
principle mixed memory markov models applied large state spaces arise cartesian product random variables 
take advantage generality mixed memory models number different settings 
doing goal build gradual way somewhat involved notation needed describe factorial hmms 
organization note follows 
order increasing complexity consider higher order markov models large state spaces arise cartesian product time slices factorial markov models dynamics order observations componential structure factorial hmms markov dynamics apply hidden states opposed observations 
mixed memory markov models conclude mixed memory models provide valuable tool understanding complex dynamical systems 

higher order markov models ng denote discrete random variable take possible values 
kth order markov model specified transition matrix ji gamma gamma gammak 
avoid having specify elements matrix consider parameterizing model convex combination raftery ney ji gamma gamma gammak ji gamma ji elementary theta transition matrices 
model eq 
specified kn parameters opposed full memory model 
note weight influence past observations distribution type weighted sum defining characteristic mixed memory models 
mixture model eq 
distinguished models approximate higherorder markov models gram smoothing employing linear combination nth order transition matrices chen goodman 
model gram smoother approximates higher order markov model linear combination non adjacent bigram models 
model eq 
differs mixture experts models applied continuous time series predictions different nth order regressors combined weights softmax gating function 
purpose parameter estimation convenient interpret index eq 
value latent variable 
denote latent variable time step consider joint probability distribution ji gamma gammak ji gamma note marginalizing summing recovers previous model transition matrix eq 

expressed dynamics mixture model parameters prior probabilities 
likewise view parameters ji conditional probabilities ji gamma gammak 
fi denote observed time series length sufficient statistics full memory markov model transition frequencies 
fit mixed memory markov model avail em procedure dempster 
general terms em algorithm calculates expected sufficient statistics sets equal observed sufficient statistics 
procedure iterates guaranteed increase likelihood step 
model eq 
em updates ney ji ji ji gamma ji gamma iji saul jordan table 
entropy character computed various markov models 
order memory english italian finnish th st full nd mixed nd full case multiple time series available training data sums interpreted sums series 
em updates model easy understand iteration model parameters adjusted statistics joint distribution match statistics posterior distribution 
expectations eqs 
may straightforwardly computed bayes rule ji ji gamma ji gamma note algorithm requires fine tuning step sizes gradient descent 
terms representational power model eq 
lies order markov model kth order markov model 
demonstrate point fitted various markov models word spellings english italian finnish 
state space models alphabet english training data came long lists words letters 
matrices ji initialized count bigram models predicting letter th preceding 
type initialization component sub models trained independently useful avoid poor local maxima learning procedure 
table give results measured entropy character 
results show mixed memory model noticeably better order model 
course capture structure full second order model times parameters 
mixture model accordingly viewed intermediate step higher order models 
envision situations model eq 
may applied 
dynamics process generating data faithfully described mixture model 
case expect mixture model perform full higher order model requiring substantially data parameter estimation 
real world example modeling web sites visited session world wide web 
modeling sequences applications web page prefetching resource management internet bestavros cunha cunha 
typically choice web page conditioned previous site necessarily visited 
recall necessary retrace steps back option 
model eq 
captures type conditioning explicitly 
states markov model correspond web pages matrices ji links web page web page index number backward steps taken activating new link 
second situation model may appropriate amount training data extremely sparse relative size state space 
case parameterization eq 
poor approximation true model may mixed memory markov models desirable avoid overfitting 
ney investigated models form large vocabulary language modeling 
ability discern sequences words sequences important component automated speech recognition 
large vocabularies tens thousands words sufficient data estimate robustly statistics second higher order markov models 
practice models smoothed interpolated chen goodman lower order models 
interpolation lower order models forced practitioners enormous size state space words small relative terms amount training data words 
applied sophisticated version eq 
large vocabulary language modeling saul pereira 
cpu hours possible fit parameters statistics word corpus 
smoothed combination mixed memory higher order markov models led significantly lower entropies sample predictions 

factorial markov models section saw large state spaces arose result higher order dynamics 
section consider source large state spaces factorial representations 
time series natural componential structure 
consider example voices alto tenor bass bach fugue weigend 
model voice separate markov model capture correlations due harmony 
straightforward way model coupling voices write markov model dynamical state cartesian product voices 
combinatorial structure state space leads explosion number free parameters imperative provide compact representation transition matrix 
mixed memory models especially geared sorts situations 
denote tth element vector time series th component vector components component take values state space size model coupling components compact way simplifying assumptions components time vector gamma ji gamma ji gamma ii conditional probabilities ji gamma expressed weighted sum cross transition matrices ji gamma ji gamma parameters ji elementary theta transition matrices parameters positive numbers satisfy :10.1.1.131.5458
number free parameters eq 
opposed full memory saul jordan table 
portion component time series generated bach fugue 
alto tenor bass model 
allowing non square transition matrices model generalized case different components take different numbers values 
parameters measure amount correlation different components time series 
particular correlation identity matrix th component independent rest 
hand nonzero components time step influence th component 
matrices ji provide compact way parameterize influences 
previous section convenient introduce latent variables view eq 
mixture model 
may write ji gamma ji gamma ji gamma ji gamma role select component gamma determines transition matrix derive em algorithm fit parameters model 
case em updates ji ji ji gamma ji gamma ji stands observed time series 
naturally structure updates quite similar model previous section 
test algorithm learned model component time series generated bach fugue 
fugue rich history weigend 
time series beats long public santa fe competition time series prediction 
table shows portion time series element represents sixteenth note numerical value codes pitch 
help avoid poor local maxima learning procedure transition matrices ji initialized count bigram models predicting th voice time th voice previous time step 
examining parameters fitted model see extent voice enables predictions 
general observed mixture coefficients close zero 
reason voices typically change pitch sixteenth note 
voice note previous beat predictor note current 
voices transition move pitch coupling voices evident 
see look posterior mixed memory markov models 
plot tenor correlations versus time measured posterior probabilities mixed memory markov model 
probabilities latent variables reveal extent voices interact specific moments time 
shows plot posterior probabilities ji versus time calculated fitted model 
framework mixture model probabilities measure relative degree note time predicted tenor note previous time step 
moments probability acquires non zero value indicate times tenor tightly coupled 
surprisingly pulses coupling viewed time series discernible local rhythm regularity 

factorial hmms building results section consider generalization factorial hidden markov models hmms 
hmms states observations internal combinatorial structure ghahramani jordan binder 
structure arise 
suppose trying model processes give rise speech signal 
number unobserved variables interact generate signal ultimately observe 
articulatory model speech production variables encode positions various organs lip tongue jaw 
recognizer variables encode current phonemic context speaker accent gender presence background noise 
case hidden state models naturally decomposed cartesian product random variables 
motivation factorial representations applications observations priori componential structure 
case example audiovisual speech recognition bregler omohundro information different modalities combined recognizer 
case frequency subband speech recognition bourlard dupont different recognizers trained sub bands speech signal combined global decision 
simple ways integrate different components collapsing data single time series reweighting combining likelihood scores independent hmms 
hope sophisticated integration joint model looks correlations actual time scale observations 
manner arise factorial hmms pose concrete problems 
representation 
applications sufficient data estimate elements full transition emission matrices formed cartesian product individual factors 
parameterize matrices saul jordan making restrictive inelegant assumptions 
ideally representation unjustified assumptions conditional independence force give desirable properties em algorithm monotone convergence log likelihood 
second problem factorial hmms computational complexity 
algorithm parameter estimation scales number hidden states 
hidden state cartesian product random variables degree effective number hidden states small may prohibitively large calculate statistics step em algorithm 
naturally led consider approximations statistics 
return development factorial hmms issues mind 
see mixture models provide compromise problem representation efficient deterministic approximations exist problem parameter estimation 
concreteness suppose trained simple hmms separate time series length wish combine hmms single model order capture may useful correlations different time series 
individual hmm hidden states types observations hidden state space combined model size likewise observation space combined model size time step denote spaces cartesian products omega omega delta delta delta omega hidden omega omega delta delta delta omega observed 
hmm hidden states opposed observations markov dynamics 
accordingly setting eq 
model hidden state transition matrix 
analogy eqs 
parameterize emission probabilities ji oe ji jji elementary theta emission matrices 
note model capture correlations hidden states th markov chain observations th time series 
purposes parameter estimation convenient introduce latent variables encode mixture components eq 

analogy eqs 
ji oe ji ji having encoded mixture components hidden variables apply em algorithm estimate model parameters 
case updates form jj jj ji gamma jj gamma ijj mixed memory markov models oe jj jj jji jjj ijj denotes observed time series 
viterbi approximation obtained conditioning probable sequence hidden states arg max ji gamma ji note computing posterior probabilities updates requires ln operations true computing viterbi path 
avoid computational burden approximation estimating statistics factorial hmms outlined saul jordan 
basic idea approach simple structure factorial hmm intractable gives rise efficient approximations exploit tractability underlying components 
note discuss approximations estimate viterbi path 
general ideas may extended approximate full statistics posterior distribution example ghahramani jordan 
factorial hmm dynamic programming procedures compute viterbi path algorithm require ln steps 
practical alternative consider iterative procedure returns possibly sub optimal path polynomial time 
iteration subroutine finds optimal path hidden states th chain fixed values hidden states 
note instantiate hidden variables chains effective size hidden state space collapses perform optimization respect remaining hidden states ln steps 
factor picked converting right hand side eq 
form standard viterbi algorithm applied elementary viterbi operation requires lk steps 
algorithm approximately computing full viterbi path factorial hmm obtained piecing subroutines obvious way 
initial guess viterbi path component hmm 
typically done ignoring intercomponent correlations computing separate viterbi path chain 
viterbi algorithm applied turn component hmms 
viterbi algorithm applied times chain cycle repeats iteration process involves lk steps 
note iteration results sequence hidden states probable preceding process guaranteed converge final possibly suboptimal path 
practice process typically converges stable path iterations 
viterbi algorithm guaranteed find truly optimal sequence hidden states factorial hmm 
success algorithm depends quality initial guess judgment modeler 
approximation assumption model describes set weakly coupled time series particular auto correlations time series strong stronger saul jordan 
plot tenor correlations versus time measured posterior probabilities mixed memory hmm 
cross correlations 
view approximation computationally cheap way integrating hmms trained parallel data streams 
main virtue exploits modeler prior knowledge separate hmms weakly coupled 
assumption holds approximation quite accurate 
test ideas fitted mixed memory hmm bach fugue section 
hopes model hidden states reflect musical structure longer time scales single note 
experiments voice component hmm hidden states previous notation 
employed viterbi approximation full em algorithm meaning posterior probabilities eqs 
conditioned observations viterbi path probable sequence hidden states estimated iterative procedure described 
interesting see model discovered correlations different voices fugue 
shows plot posterior probabilities ji versus time calculated factorial hmm training 
frequent pulses indicate framework model moments strong coupling tenor themes fugue 

discussion parameterizations proposed probabilistic models time series 
mixed memory models note distinguishing features 
express rich set probabilistic dependencies including coupled dynamics factorial models 
second fitted em algorithms avoiding potential drawbacks gradient descent 
third compact easy interpret notably ordinary markov models parameter defines simple 
features enable researchers build sophisticated models dynamical systems 
acknowledgments smyth retrieving word lists tommi jaakkola helping finnish fernando pereira pointing application web page prefetching 
acknowledge useful discussions zoubin ghahramani yoram singer 
initiated ls member center biological computational learning mit 
time supported nsf cda onr 
mixed memory markov models 
baldi chauvin 
hybrid modeling hmm nn architectures protein applications 
neural computation 

baum 
inequality associated maximization technique statistical estimation probabilistic functions markov process 
ed inequalities 
academic press new york ny 

bestavros cunha 
client speculation www 
boston university department computer science technical report tr 

binder koller russell kanazawa 
adaptive probabilistic networks hidden variables 
machine learning 

bourlard dupont 
new asr approach independent processing recombination partial andw 
eds proceedingsof th speech language processing 

bregler omohundro 
nonlinear manifold learning visual speech recognition 
grimson ed proceedings th international conference computer vision 
ieee computer society press los alamitos ca 

chen goodman 
empirical study smoothing techniques language modeling 
proceedings th annual meeting association computational linguistics 

cunha bestavros crovella 
characteristics www client traces 
boston university department computer science technical report tr 

dean kanazawa 
model reasoning persistence causation 
computational intelligence 

dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 

weigend 
baroque forecasting completing bach fugue 
weigend gershenfeld eds time series prediction forecasting understanding past 
addison wesley reading ma 

forbes huang kanazawa russell 
bayesian automated taxi 
proceedings th international joint conference artificial intelligence 
montreal canada 

ghahramani jordan 
factorial hidden markov models 
machine learning 

haussler krogh mian 
protein modeling hidden markov models analysis globins 
proceedings hawaii international conference system sciences 
ieee computer society press los alamitos ca 

macdonald 
hidden markov models discrete valued time series 
chapman hall 

nadas 
estimation probabilities language model ibm speech recognition system 
ieee transactions assp 

ney essen kneser 
structuring probabilistic dependences stochastic language modeling 
computer speech language 

rabiner 
tutorial hidden selected applications speech recognition 
proceedings ieee 

raftery 
model high order markov chains 
journal royal statistical society 

ron singer tishby 
power amnesia learning probabilistic automata variable memory length 
machine learning 

saul jordan 
exploiting tractable substructures intractable networks 
touretzky mozer hasselmo eds advances neural information processing systems 

saul pereira 
aggregate mixed order markov models statistical language processing 
proceedings nd conference empirical methods natural language processing 

williams hinton 
mean field networks learn discriminate temporally distorted strings 
proceedings connectionist models summer school 

yamato ohya ishii 
action time sequential images hidden markov models 
proceedings international conference computer vision 

meir adler 
time series prediction mixtures experts 
mozer jordan petsche eds advances neural information processing systems 
