structurally adaptive localized mixtures experts non stationary environments viswanath joydeep ghosh department electrical computer engineering university texas austin austin tx 
introduces neural network capable dynamically adapting architecture realize time variant non linear input output maps 
network roots mixture experts framework uses localized model gating network 
modules experts grown pruned depending complexity modeling problem 
structural adaptation procedure addresses model selection problem typically leads better parameter estimation 
batch mode learning equations extended obtain line update rules enabling network model time varying environments 
simulation results support proposed techniques 
keywords mixture experts network growing pruning model complexity line learning non stationarity localized gating function approximation 
research supported part aro contracts daah nsf ecs 
list figures mixture experts network :10.1.1.136.9119
regions expertise determined softmax gating network 
hme different sizes trying approximate sinc function sinc function approximation localized model layer experts localized gating network 
number experts vs test set mse 
noisy sinc approximation 
noisy sinc 
true sinc approximation experts growing algorithm 
mackey glass data mode mackey glass data transition mode mode mackey glass data mode mse comparison modeling dual mode mackey glass data set growing localized mixture experts network mlp network 
number experts network time modeling dual mode mackey glass data set 
global data set 
overlapping subsets global data set 
mse comparison growing localized mixture experts network mlp network modeling rapidly changing dual mode mackey glass data set 
number experts time network modeling rapidly changing dual mode mackey glass data set 
mse comparison growing localized mixture experts network mlp network modeling modified building data set 
number experts time growing network modeling modified building data set situations learning system capable adapting changing environments desirable 
example may want model plant process undergoes seasonal variations input output behavior 
neural network community problem realizing time varying mapping commonly encountered sequential learning problem 
neural network models multi layered perceptrons mlps known exhibit catastrophic interference tend forget previously learned mappings quickly exposed new mappings 
catastrophic interference exhibited mlps mainly networks highly coupled global parameter updates learn 
surprisingly efforts alleviate reduced coupling mlp networks met limited success 
promising alternative networks cells localized responses 
order model nonstationary environments networks ability adapt parameters line respond localized changes input output map adjusting parameters 
network able dynamically adjust complexity pruning growing order properly match complexity problem modelled currently 
popular architecture exhibits desired characteristics modeling sequential learning tasks resource allocation network ran constructive version radial basis function rbf network 
novel input pattern encountered additional rbf unit added network provided output error sample threshold 
times learning performed gradient descent 
growing algorithm respond changes output space gradient descent learning rule pose problems quick changes tracked 
original ran pruning mechanism 
pruning policies subsequently proposed principle projection unit relevance 
outlier sensitivity 
modular architecture mixture experts framework capable growing pruning modules experts maintain suitable model complexity times adjust network parameters line 
key motivation mixture experts architecture function approximation problems softly decomposed different regions input space sub problem region modelled simple expert little interaction experts mapping regions 
powerful approximation properties mixtures experts hierarchical versions established networks applied variety real life problems 
original formulation soft partitioning input space achieved softmax gating network 
argue section situations architecture localized leads sub par performance 
localized gating network normalized gaussians 
alternative gating network proposed xu jordan hinton primarily motivated computational ease pass em algorithm structure 
show localized gating model provides natural mechanism network growing pruning corresponding batch mode em equations readily converted line ones 
independently stimulating article murray smith johansen pointed problems global learning methods mixtures experts network locally parameterized poorly structured 
theoretical analysis experiments normalized gaussian gating networks fixed centers widths demonstrate effect local learning relative robustness 
noteworthy schaal atkeson introduce system experts architecture local experts allocated depending spatial distribution input samples 
architecture expert fixed center point associated input space expert outputs weighted distance input sample different center points 
architecture related localized mixture experts network 
center point expert operation fixed weighting gating factors function distance input sample center points associated different experts 
gating network outputs localized mixture experts network depend prior probability term ff equation attuned current task scenario modeling tasks change 
training algorithm developed mixture experts network capable modeling inverse problems system experts network model :10.1.1.27.993
broader sense architectures draws strength localized modeling multiple model approaches 
class includes decision tree type methods cart mars localized subspaces 
employing different technique parameter estimation approaches differ splits limited parallel ordinate axes normalized ellipsoids 
includes ensemble methods 
schemes aim combine results obtained individual networks 
key difference member ensemble trained entire data set expertise localized space intent 
detailed description differences ensemble modular approaches see 
try solve problem determining appropriate model approximate static interpolation task 
localized form gating network considered mixture experts network perform function approximation tasks layer experts 
techniques proposed overcome model selection problem mixture experts architecture 
technique efficient way grow expert networks come appropriate number experts problem 
second approach starts certain initial number experts methods training prune experts useful add experts effective 
simulation results show techniques yield better results standard static approach final network sizes chosen 
section briefly describes mixture experts network motivates localized gating network popular softmax 
techniques counter model selection problem structural adaptation network growing pruning proposed 
algorithms described far batch mode derived expectation maximization em algorithm 
learning system capable adapting changing environments line training algorithm necessary proceed convert batch algorithms line ones 
line techniques structural adaptation techniques proposed earlier 
empirical findings proposed method summary type experiments reported provided table help reader better understand intent scope 
table summary experiments section number purpose experiments data sets evaluate network growing procedure sinc noise building heartac multivariate fn 
evaluate pruning growing gabor fn 
approximation procedure 
multivariate fn 
approximation 
compare line training algorithm gabor fn 
approximation batch algorithm 
multivariate fn 
approximation 
pruning growing gabor fn 
approximation line algorithm model slowly varying environments 
dual mode mackey glass time varying gabor fn 
approximation 
model rapidly changing environments 
dual mode mackey glass multimodal version building data set 
case localized gating mixture experts networks generic mixture experts architecture mixture experts model shown 
model consists set expert networks gating network 
gating network mediates outputs expert networks 
expert networks look input vector form outputs 
gating network looks input vector produces outputs weight expert network outputs form output 
expert networks typically single layer linear networks simple models desired localized fits 
introduced gating network single layer network softmax output non linearity 
th output gating network prior output non linearity ji corresponding output softmax non linearity exp exp softmax function ensure expert gating network expert expert 
mixture experts network :10.1.1.136.9119
mixture experts network viewed modeling conditional probability density target output network input training data 
conditional density cast mixture conditional probability densities expert modeling conditional density yjx target conditioned expert picked network input 
entire network represents yjx yjx expert outputs means corresponding densities yjx gating network output interpreted conditional probability selecting expert input vector component densities gaussian distributed identity covariance mixture density function written form yjx gamman expf gamma gamma gamma equation viewed likelihood function unknown parameters gating expert network weights 
problem estimating network parameters viewed maximum likelihood problem 
gradient ascent algorithm derived maximize log likelihood function 
bayes rule defines posterior probabilities yj yj learning rules gradient ascent weight matrix theta th expert delta theta gamma jth weight vector gating network delta gamma em algorithm derived evaluating network parameters :10.1.1.136.9119:10.1.1.136.9119
em algorithm faster algorithm train network compared gradient ascent learning 
em iterations step turns just computation posterior probabilities step reduces separate maximization problems 
theta arg max theta ln yj arg max ln equations denotes iteration number 
superscript running variable corresponding individual input patterns 
level mixture experts model extended hierarchy replacing expert mixture experts model :10.1.1.136.9119
resulting level hierarchy similarly extended form deeper trees 
drawbacks global gating network gating network mixture experts network defined equation divides input space overlapping regions soft hyperplanes region assigned experts 
mathematically shown cost function mixture experts network contains entropy term increases number experts active input see equation :10.1.1.27.993
reason gating network equation leads difficulties modeling function approximation task non trivial 
explained help 
assume function approximated inputs falling regions shows hyperplanes bs corresponding gating weight vectors realization involving experts 
idea gating outputs high inputs falling region high inputs falling region ideally experts model approximation region experts model approximation region unfortunately happen high inputs falling regions high inputs falling regions interference experts half subspaces covered different hyperplanes overlap 
interference happens constructive approximation task performed 
probability finding constructive interference decreases rapidly number experts increases 
top regions expertise determined softmax gating network 
employs hierarchy experts problem circumvented hierarchy provides isolation sub spaces 
example level hierarchy employed task described top level gating network provide partition top isolates regions individual mixture experts networks act 
illustrates effect hierarchy 
task approximate sinc function 
simple mixture experts network consisting experts level binary tree hierarchy level binary tree hierarchy employed 
seen deeper network better performs 
phenomena reported classification type hme networks 
unfortunately computational cost increases tree height increases gating expert networks need trained 
depth tree leads question right size structure tree best solve problem faced model selection problem 
localized model gating network xu jordan hinton proposed alternative form gating network divides input space soft hyper ellipsoids opposed soft hyperplane divisions created original gating network 
localized regions expertise single layer linear experts adequate practice function sinc function hme network true sinc level exps level bin tree level bin tree hme different sizes trying approximate sinc function approximation 
shows performance localized model layer experts sinc function 
sinc localized gating network true sinc level exps sinc function approximation localized model layer experts gating network proposed form ff xj ff xj xj gamman sigma gamma expf gamma gamma sigma gamma gamma jth expert influence predominant region 
note normalization performed eq 
strictly local need sum 
compared original formulation region influence experts tends ellipsoidal center location 
localized gating network 
em formulation estimating gating network parameters ff set training patterns indexed step jx expf gamma gamma gamma expf gamma gamma gamma step gating network ff sigma gamma gamma comments localized gating network employed localized gating network solve number regression problems network performs just single layer experts 
requiring gating network model significantly faster train compared hierarchical model 
step gating network exact pass analytical solution opposed approximate solution train step pass original gating network algorithm described :10.1.1.136.9119
couple things need attention training localized model practice 
em iterations step gating network requires inverting covariance matrices sigma obtained step 
covariance matrices obtained turn close singular 
avoid problem consider diagonal elements covariance matrices diagonal elements zero 
mean variances different input elements considered cross terms ignored 
affect function approximation capability network 
worst case network needs experts needed 
effect change em derivation calculating sigma step terms corresponding diagonal entries need computed 
lower bound diagonal entries imposed entries small 
algorithm sensitive initial random means associated experts 
initializing means standard means algorithm works practice 
capability localized model illustrated simulation results building data set function approximation task proben set benchmark data sets 
building data set input vector includes date time day outside temperature outside air humidity solar radiation wind speed output vector consisting hourly consumption electrical energy hot water cold water 
task perform function approximation training samples samples test approximation 
single layer mixture experts network trained 
network gives average test set mse training set mse best results obtained fully connected mlp including short cut connections slightly better results quoted short cut connections mlp 
model selection mixture experts mixture experts framework provides modular flexible approach function approximation 
approach key issue determining model complexity case appropriate number complexity experts needs addressed 
common practice counter model selection problem train sufficiently large network training monitor mean squared error validation set parallel 
high computational cost training possibly redundant experts ends complicated model approximate function making parameter estimation model interpretation difficult 
possible approach form regularization ridge regression reduce effective number parameters training 
experiments may needed determine suitable value regularization parameter 
neural networks including mixture experts early stopping similar effect regularization 
model selection network growing pruning widely studied feedforward networks 
mixture experts framework fritsch 
proposed growing technique hme expert likelihood selected replacement multiple experts 
waterhouse developed techniques growing pruning hierarchical mixture experts principles classification regression trees 
hierarchical network grown layer time splitting expert leaf tree experts gate 
expert split time maximizes increase log likelihood due split 
scheme nice theoretical framework parallels known results growing decision trees leads deep trees 
aware structural adaptation single layered mixtures experts 
section observe localized gating network introduced section provides basis overcoming model selection problem 
propose techniques counter model selection problem 
technique starts training network consisting expert corresponds linear regression grows experts sequentially fit complexity problem 
second technique starts training adequately powerful network training prunes away experts useful subsequently grows effective experts need 
final network yielded second approach typically provides better results compared training fixed network final size 
growing mixture experts network researchers proposed algorithms dynamically adapt network structure adding weights nodes appropriate network size obtained function approximation task 

section constructive algorithm building mixture experts network exploiting localized nature chosen gating network 
idea initially start expert add experts time systematically reduce output error 
assume instant time experts network experts having added 
mixture experts trained em algorithm mse validation set fails decrease 
network parameters obtained saved 
define weighted mse expert validation set samples gamma obtained evaluating step expression validation set samples gives measure th expert performed validation set 
partitioning crisp indicator functions sample test set associated expert mse expert trying approximate sub function samples associated expert mixture experts architecture reality correspond soft version mean squared error 
large indicates possibilities localized model able approximate target function weighted samples associated expert localized model overfit training data able perform validation set data 
case addition expert localized space spanned expert distinguished training set examples indexed reduce output error due added flexibility 
case addition overfit training data 
add effective expert ranked largest smallest 
denote expert network current highest rank largest 
try add expert tries reduce error due expert new expert created follows ffl weights expert network weights expert network ffl perform weighted means training samples weights 
means obtained new means associated experts ffl set ff ff new ff old ffl set sigma sigma weighted means algorithm performed follows ffl initialize means training samples having largest ffl assign point training set subset depending closer ffl determine new new ffl iterate convergence 
crisp case weighted means procedure reduces finding means set samples associated expert means algorithm 
newly formed network trained em iteration 
decrease mse validation data set proceed 
revert earlier saved network try adding new expert reduce error due expert highest rank 
procedure terminated stage adding new expert reduce error existing experts reduce mse validation data set 
time growing procedure overfitting expert detected corresponding expert network parameters mean covariance matrix frozen 
simulation results algorithm tested synthetic real life problems 
task approximate sinc function gaussian noise variance added 
training samples training set 
shows results growing algorithm 
test mse case computed measuring error network output true sinc function 
mse obtained growing operation better mse obtained training static networks size experts 
shows performance final trained network 
algorithm tried building data set described earlier 
results typical run shown table 
multiple runs average experts network gives average validation set par reported earlier 
worked heartac data proben set benchmark data sets 
task predict heart disease single output input vector personal data age sex smoking habits results various medical examinations 
data originally obtained cleveland clinic foundation robert ph 
problem obtained test set mse expert 
addition experts help reduce validation set mse 
happens task linear regressor single layer perceptron network solve problem 
procedure overfit data 
multivariate function approximation task considered 
function approximated training set consisted samples validation set samples 
results static networks growing algorithm shown table 
growing algorithm run network grew final size experts run network grew final size experts 
static networks mse values averaged runs 
seen table growing procedure performs static networks 
computational cost train static networks roughly equal cost performing initial means algorithm number experts plus cost proportional number experts times average number em iterations converge 
average number em iterations depends problem usually small iterations 
growing procedure observed usually takes rare occasions em iterations validation set mse converge addition expert 
computational cost growing procedure roughly equal cost proportional square final number experts plus number experts times cost involved performing means algorithm 
cost means algorithm roughly proportional number samples clustered large 
long final network structure large computational cost involved employing growing algorithm cost involved training static networks 
final network structure growing algorithm typically seen perform comparably sized static network 

experts 
experts vs test set mse sinc noise number experts vs test set mse 
table growing mixture experts network building data set 
number experts test set mse pruning growing mixture experts popular approach structural adaptation start adequately powerful model simplify training data 
various weight decay strategies pruning links proposed especially mlp network 
simple efficient technique structurally adapt prune grow mixture experts network localized nature chosen gating network exploited 
section observed prior ff computed step ff ff seen proportional sum patterns training set posterior probability selecting expert input corresponding output see iteration em algorithm ff directly gives measure important network feels expert relation experts 
needs prune expert obvious candidate expert value ff 
pruning growing combined training mixture experts network serve purposes sinc function approximation experts test mse noisy sinc approximation 
noisy sinc 
true sinc approximation experts growing algorithm 
table growing mixture experts multivariate data set 
number experts ave mse static architecture network growth remove redundant experts ii avoid local minima perform better generalization 
pruning growing train mixture experts network certain initial number experts convergence 
prune expert corresponding lowest ff 
significant change network performance continue pruning 
perform network growth described previous section 
strict order pruning growing performed generally useful prune initially grow 
simulation results simultaneous pruning growing technique applied function approximation problems approximate dimensional gabor function delta exp gamma delta cos consisting training set samples validation set samples 
approximate multivariate function described previous section 
gabor function initial network configuration chosen experts 
network trained em algorithm described 
network converged validation set mse 
network pruning performed pruning method described 
significant change mse observed till number experts brought 
mse experts 
network growth performed 
shown table significant performance gains obtained addition experts lowering mse 
noted result better simply pruning expert network experts 
table shows results multivariate data set 
table batch mode pruning growing gabor function 
number experts mse pruning growing table batch mode pruning growing multivariate data set 
number experts mse pruning growing modeling time varying maps modeling time varying maps broad field encountered virtually major branch engineering 
example signal processing control communities extended kalman filters may fruitful slow variations fast changes may addressable detecting changes switching models response 
line updating parameters heart adaptive filtering 
related issues convergence studied 
time series analysis popular approaches include adjusting identifying regimes relatively stationary 
example weigend mixture mlp experts divide time series segments different distinct sub segments individual experts act :10.1.1.27.993
tuning experts different noise levels local model complexity attempts match local complexity data 
model applied static time series data sets 
kohlmorgen developed method analyzing non stationary time series multiple operating modes able segment data different modes 
ensemble radial basis function network predictors specializes respective operating modes 
method applicable cases time series data hand batch learning methods applied 
sections examine localized mixtures experts network line modeling slowing rapidly changing maps 
line training training algorithm described section batch algorithm 
order employ network learning non stationary maps needs line algorithm 
em iterations train gating network parameters bear close resemblance procedure training parameters mixture gaussian densities model unknown probability density attracted attempts line versions 
take approach derive suitable line algorithm 
equations em update rule batch mode growing technique general form fl fl fl fl fl fl fl fl fl fl gamma fl rearranging fl fl fl gamma fl considering values denominator approximated recursion gamma gating network parameter update equations ff ff gamma ff denote discrete time gamma sigma sigma gamma gamma gamma sigma expert networks linear parameters updated lms rls algorithms 
rls algorithm converges faster compared lms algorithm 
rls algorithm sensitive forgetting factor rls robust lms algorithm rls algorithm train ith expert network line described 
initialize ffi gamma ffi small positive constant 
small random values 
gamma rls gamma gamma gamma gamma rls gamma gamma ae gamma gamma gamma ae simulation results comparison line algorithm batch algorithm synthetic data sets described earlier shown table 
results averaged runs data sets 
table seen performance line algorithm batch algorithm comparable 
terms number epochs train networks batch algorithm takes fewer number epochs train pseudo inversion technique solve weighted squares problem step em iterations 
terms total time taken train networks line algorithm slightly faster training time epoch shorter compared batch algorithm 
table batch vs line results static networks 
number batch line experts av 
mse std 
av 
mse std 
gabor data set multivariate data set modeling slowly varying environments algorithm developed section structural adaptation batch algorithm 
similar approach extended line setting incoming training data come fixed distribution rls algorithm robust making rls converge unity training proceeds 
distribution changes slow respect time 
section line extension batch mode growing technique 
line setting error estimate experts gamma gamma error corresponding particular expert increases predetermined threshold need additional expert detected 
threshold chosen prior knowledge modeling complexity problem 
order grow additional expert follow procedure batch mode create new parameters exception gating network means 
create gating network means employ line version weighted means algorithm follows 
ffl initialize mean vector corresponding new expert equal mean vector corresponding expert split 
ffl add small random perturbation means 
ffl window length input patterns parameter updates experts expert split new expert 
ffl window length input patterns posterior corresponding new experts highest posteriors experts input pattern parameter updates expert 
window length chosen way separate means 
window length samples experts normal experts 
pruning performed monitoring parameter ff ff small corresponding expert pruned 
simulation results algorithm applied static gabor data set section 
lms algorithm expert network updates sub section 
initial network configuration experts gabor data set 
table shows results line structural adaptation algorithm applied data set 
expert network initially attains mse 
experts subsequently pruned 
seen table mse value displays little change 
addition experts mse value improves considerably 
comparison table shows online method provides significant performance improvements batch method 
algorithm applied drifting mackey glass system model employed 
mackey glass chaotic time series generated delay differential equation dx dt gamma gamma gamma table line pruning growing gabor function 
number experts mse pruning growing stationary operating modes established different delays respectively 
operating steps mode system drifts second mode 
drift takes steps system operates second mode steps 
drift performed mixing equations 
mixing accomplished formula gamma denoting exponential drift exp gamma sample number mackey glass td mackey glass data mode samples mode drift mode shown figures respectively 
expert network initially chosen model data 
task predict sample time series past samples 
training time steps network examined see pruning growing performed 
pruning growing performed conservatively expert pruned grown inspection 
threshold pruning expert set ff 
expert ff reached value expert pruned 
grow expert sample number mackey glass transition mackey glass data transition mode mode sample number mackey glass td mackey glass data mode error threshold fixed 
expert grown ff corresponding expert split greater 
decided experts initial number experts time 
shows performance network 
sake comparison performance hidden node mlp network plotted averaged runs 
little variance observed performance mlps 
number experts time plotted 
intial high error value displayed adaptive network due larger initial random weights compared mlp networks 
structurally adaptive network able model time series quite rapidly track transitions problems 
transitioning mackey glass little change input probability distributions modes input output map different 
inspect time varying version gabor function sample number mlp moe mse comparison modeling dual mode mackey glass data set growing localized mixture experts network mlp network 
approximation problem see adaptation capabilities sudden shifts input distribution 
uniformly spaced samples gabor function delta exp gamma delta cos obtained range gamma gamma grid 
data set divided overlapping subsets consisting samples shown 
randomly ordered samples subsets network epochs 
restrictions imposed training 
ffl number experts network 
ffl threshold pruning expert ff 
ffl error threshold growing expert 
expert grown corresponding ff 
initial number experts network 
staying constraints data set network gives mse experts 
network subsequently exposed second data set network prunes away experts adds experts give mse value 
network exposed third data set network prunes away experts gives mse experts 
data sets duration epochs independent mlp networks hidden units 
average mse values averaged runs data sets respectively 
sample number number experts network time modeling dual mode mackey glass data set 
modeling rapidly changing environments setting input space output space change time quickly rapidly adaptable scheme required 
alternative way grow mixture experts novelty training patterns described 
approach growing neural network new 
similar approach input novelty 
approach differs cited incorporating output novelty framework novelty detection 
network instant time experts 
new expert added network current training pattern min gamma sigma gamma gamma ffi sigma means covariances corresponding current set experts respectively ffi small fixed constant 
mean new expert initialized current input pattern 
ff set equal ff corresponds expert mean closest input pattern 
ff set ff ff sum 
new expert random initial weights 
covariance matrix initialized constant diagonal matrix 
new expert added network network parameter updates continued rls algorithm 
growing algorithm capable adapting changes input space creating new experts input samples regions seen earlier network 
modeling task changing environments occasions output space changes input space change 
happens catastrophic interference takes place networks adapt changes input space 
order take account changes output space training network gating network fed combined data input output space 
way distinct experts created changes output space 
expert networks continue see data input space 
global data set 
overlapping subsets global data set 
recall testing network new input pattern target output available 
case treat gating network input input vector missing elements missing elements corresponding output space 
pointed line setting training testing take place alternately 
easiest way deal missing feature element choose mean value missing feature substitute missing feature recall 
mixture experts network expert specific mean input feature vector computed gating network equation 
extra computation required determine mean missing features 
simulation results growing procedure applied mackey glass drifting system described previous subsection 
simulate rapid changes modeling task system parameters changed operate mode time steps followed drift time steps operate mode time steps 
process repeated cycles 
shows performance network hidden node mlp network 
average mse value time steps plotted number experts network time plotted 
time steps 
mlp network network mse comparison growing localized mixture experts network mlp network modeling rapidly changing dual mode mackey glass data set 
time steps number experts time network modeling rapidly changing dual mode mackey glass data set 
application growing procedure real life data set described 
data set employed adapted building data set proben benchmark suite 
problem predict hourly consumption electrical energy hot water cold water date day week time computing mse values sample output errors ignored networks networks started training random initializations 
day outside temperature outside air humidity solar radiation wind speed 
data set spread months september february 
observing data seen hot water requirement strong correlation outside temperature 
ignoring temperature parameter hot water requirement varies months 
order test growing technique problem involving changes input output space data set divided segments containing data corresponding consecutive months 
outside temperature assumed measured network predict hot water requirement 
data segments contain samples respectively 
different sets random samples extracted data segments 
different sets samples fed sequentially network set segment second segment third segment fourth segment 
purpose experiment see quickly network able adapt changing environments 
feeding different sets data termed phases 
experiment conducted multiple cycles cycle phases involves data points 
average mse values averaged phase plotted 
performance mlp network having hidden nodes plotted number experts network phase training plotted 
seen growing mixture experts network yields low mse values couple cycles mlp network perform cycles feeding data 
experiment entire set samples data segments september march sequentially fed growing mixture experts network 
done examine performance network actual daily data spread months fed naturally network 
network grew experts starting expert gave average mse 
samples fed second time 
think data year year starts september ends march 
addition just expert network gives average mse 
mlp network hidden nodes gives average mse years 
concluding remarks methods address model selection problem mixture experts network 
localized model employed gating network training algorithm extended line settings model environments non stationary 
note computational efficiency orders magnitude performance level localized approaches compared mlp documented see example 
advantages carry methods reason emphasized point providing timing comparisons experiments 
approach compared localized connectionist computing mse value phase cycle sample output errors ignored networks networks started training random initializations 
phases growing localized mixture experts network vs mlp growing 
mlp mse comparison growing localized mixture experts network mlp network modeling modified building data set 
architectures radial basis function rbf networks 
view localized mixture experts network normalized rbf network output layer weights provided individual experts functions inputs network constants 
added flexibility useful alleviating curse dimensionality associated rbf networks 
argued localized model gating network better standard softmax gating network single layer mixture experts 
repeatedly observed data sets find little mention literature 
note comparison linear experts 
way non linear experts rbfs mlps 
problem expert may dominate performing input space spirit having simple localized fits lost 
careful initialization partitioning experts coupled successes non linear complex experts example mlps individual experts forecasting kalman filters trajectory estimation markov models speech achieved :10.1.1.27.993
weigend mlp gating network :10.1.1.27.993
mlp network alleviates problems mentioned single layer softmax gating network allowing complex regions expertise 
mlp employed gating expert network advantage able train network pass step em iterations lost 
expert domains localized data distribution may uniform possible experts localized mixture experts network get 
indicates regularization techniques applied gating expert networks improve generalization performance 
shown linear experts coupled soft decomposition input space provides effective way perform regularization obtain confidence intervals network phases number experts growing network number experts time growing network modeling modified building data set outputs 
simulation results encouraging necessarily need qualified space possible time varying problems enormously rich varied 
extensive comparative empirical studies years needed definitely establish advantages lack thereof 
example study situations behavioral regime experienced earlier may recur 
example time series cyclic behavior months 
modeling non stationary processes worth incorporating memory scheme store ff equation regime recalling corresponding ff switch different previously encountered regime detected 
unfortunately public domain benchmarking datasets situations lacking 
efforts taken create benchmarks nonlinear time series modeling 
machine learning community research context sensitivity context drift resulting creation somewhat related datasets 
efforts benchmarking put service comparable say uci proben data sets static regression classification problems created invaluable resource enabler empirical studies time varying problems 
sharkey sharkey 
understanding catastrophic interference neural nets 
technical report cs department computer science university sheffield 
typically contexts symbolic relatively defined conveniently break problem determining context modeling 
issue best deal slowly varying partially contexts resolved 
mccloskey cohen 
catastrophic interference connectionist networks sequential learning problem 
psychological review 
roger ratcliff 
connectionist models recognition memory constraints imposed learning forgetting functions 
psychological review 
french 
semi distributed representations catastrophic forgetting connectionist networks 
connection science 
jacobs jordan nowlan hinton 
adaptive mixtures local experts 
neural computation 
jordan jacobs :10.1.1.136.9119
hierarchical mixture experts em algorithm 
neural computation 
xu jordan hinton 
alternative model mixture experts 
tesauro touretzky leen editors advances neural information processing systems pages 
mit press 
schaal atkeson 
isolation cooperation alternative view system experts 
touretzky mozer hasselmo editors advances neural information processing systems pages 
mit press 
bishop 
neural networks pattern recognition 
clarendon press oxford 
weigend srivastava :10.1.1.27.993
nonlinear gated experts time series discovering regimes avoiding overfitting 
international journal neural systems 
breiman friedman olshen stone 
classification regression trees 
belmont ca wadsworth international group 
friedman 
multivariate adaptive regression splines 
annals statistics 
tumer ghosh 
analysis decision boundaries linearly combined neural classifiers 
pattern recognition feb 
perrone cooper 
learning learned supervised learning multi neural network systems 
proceedings world congress neural networks pages iii 
inns press 
hanson 
neural network ensembles 
ieee transactions pattern analysis machine intelligence 
sharkey 
combining artificial neural networks 
connection science dec 
ghosh 
advances hierarchical mixture experts signal classification 
proceedings icassp pages 
waterhouse robinson 
classification hierarchical mixture experts 
neural networks signal processing iv proceedings ieee workshop pages 
ieee press new york 
ghosh 
structural adaptation mixture experts 
proceedings icpr track pages 
prechelt 
proben set benchmarks benchmarking rules neural network training algorithms 
technical report fakultat fur informatik universitat karlsruhe karlsruhe germany september 
anonymous ftp pub papers techreports ps ftp ira uka de 
reed 
pruning algorithms survey 
ieee transactions neural networks 
waterhouse robinson 
constructive algorithms hierarchical mixture experts 
touretzky mozer hasselmo editors advances neural information processing systems pages 
mit press 
kohlmorgen muller pawelzik 
segmentation identification drifting dynamical systems 
principe giles morgan wilson editors neural networks signal processing pages 
ieee 
hans 
neural network approach statistical pattern classification semiparametric estimation probability density functions 
ieee transactions neural networks may 
roberts tarassenko 
probabilistic resource allocating network novelty detection 
neural computation 
tresp hofmann 
missing noisy data nonlinear time series prediction 
girosi makhoul wilson editors neural networks signal processing pages 
ieee 
bishop ghosh 
hierarchical adaptive kalman filtering interplanetary orbit determination 
ieee trans 
aerospace electronic systems 
