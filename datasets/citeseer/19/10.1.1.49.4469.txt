markov chain monte carlo related topics jun liu department statistics stanford university sequoia hall stanford ca email stat stanford edu summary article provides brief review developments markov chain monte carlo methodology 
methods discussed include standard metropolis hastings algorithm gibbs sampler various special cases interest practitioners 
devotes section strategies improving mixing rate mcmc samplers simulated tempering parallel tempering parameter expansion dynamic weighting multigrid monte carlo generalizations 
related topics simulated annealing reversible jump method multiple try metropolis rule 
theoretical issues bounding mixing rate diagnosing convergence conducting perfect simulations briefly mentioned 
computer simulation techniques indispensable tools solving difficult computational problems scientific disciplines 
wide applications range biology leach karplus lawrence chemistry wainwright computer science kirkpatrick ullman economics finance engineering geman geman material science smit physics metropolis goodman sokal statistics 
simulation methods monte carlo methodology especially markov chain monte carlo mcmc provides enormous scope realistic statistical modeling attracted attention statisticians 
fundamental step monte carlo methods generate pseudo random samples follow target probability distribution function 
variable interest usually takes value occasionally take values topological group diaconis liu wu 
applications directly generating independent samples distribution interest infeasible 
case generated samples dependent distribution generate samples different 
rejection sampling von neumann importance sampling marshall sampling importance resampling rubin schemes dependent independent samples generated trial distribution differs similar target distribution 
metropolis hastings algorithm metropolis hastings basic building block mcmc generates dependent samples markov chain equilibrium distribution 
view mcmc essentially monte carlo integration procedure random samples produced evolving markov chain 
increasing complexities statistical models encountered practice mcmc provides needed unifying framework complex problems analyzed 
bayesians need integrate possibly high dimensional probability distributions missing data nuisance parameters inference parameter interest predictions 
basic need underlies potential role mcmc methodology statistical modeling inference 
past years witnessed explosive growth interest mcmc methodology researchers areas statistics 
gilks richardson spiegelhalter provided survey mcmc 
steve brooks administered useful website www stats bris ac uk mcmc entertaining new research papers mcmc 
prelude random variable generation order generate random variables follow general pdf need generate uniformly distributed random variables 
simple looking task achievable computer 
generate pseudo random numbers 
formally define uniform pseudo random number generator algorithm starting initial value seed produces sequence values 
values reproduce behavior iid sample uniform random variables 
pseudo random number generators available 
refer reader knuth 
assume uniform random variables satisfactorily produced computer 
simple result proof left exercise reader 
lemma suppose unif dimensional cumulative distribution function cdf 
gamma distribution define gamma ug 
lemma provides explicit way generating dimensional random variable cdf available 
distributions gaussian distribution closed form cdf difficult directly apply inversion method 
overcome limitation von neumann proposed popular rejection method applied draw multi dimensional distributions 
lemma van neumann 
suppose pdfs defined sample space exists mg 
output algorithm follows distribution 
generate unif 
accept mg go back step 
target distribution implement rejection method employing distribution easy generate sample finding envelop constant clearly efficiency method depends large comparisons method approaches importance sampling liu 
metropolis hastings algorithms expf gammah target probability distribution function investigation presumably pdfs written form 
metropolis 
introduced fundamental idea evolving markov process achieve sampling 
hastings provided general form type algorithms 
metropolis algorithm starting configuration metropolis algorithm iterates steps 
step propose random perturbation current state seen generated symmetric probability transition function calculate change deltah gamma 
step generate random number uniform 
exp gamma deltah 
metropolis scheme extensively statistical physics past years cornerstone markov chain monte carlo mcmc techniques adopted developed statistics community 
simulation balls movements closed box metropolis algorithm 
left iterations right iterations 
example consider simulating uniformly distributed positions hard shell balls box theta 
balls assumed equal diameter kg denote positions balls 
target distribution interest equal positive constant balls box overlaps equal zero 
metropolis algorithm implemented follows pick ball random say position move tentative position ffi ffi ffi oe accept proposal violate constraints box overlap 
stay put 
starting positions balls regular grids adjusted oe gave acceptance rate 
shows snap shots simulation taken iterations second taken iterations 
hastings generalization mathematical formulation metropolis hastings algorithm prescribes transition rule markov chain equilibrium distribution chain 
start algorithm gives arbitrary easy sample transition function called proposal distribution 
proposal distribution implement iteration metropolis hastings algorithm current state ffl draw proposal distribution 
ffl draw uniform update ae metropolis 
hastings suggested ae min ae oe baker suggested acceptance rejection function ae general formula ae charles stein personal communication ae ffi ffi symmetric function ae note foregoing formulas cancels symmetric proposal originally required metropolis 

intuition ratio compensate flow bias proposal chain 
rejection function form actual transition probability implied metropolis hastings rule ae ffi gamma ffi ffi ffi implies markov chain induced metropolis hastings rule reversible invariant distribution 
convergence rate chain highly dependent target distribution 
see roberts tweedie 
discrete state spaces showed optimal choice ae terms statistical efficiency metropolis 

issue clear terms convergence rate induced markov chain liu 
gibbs sampling data augmentation statistics gibbs sampler geman geman special mcmc scheme 
prominent feature underlying markov chain constructed sequence conditional distributions chosen invariant respect conditional moves 
gibbs sampler effectively reduces high dimensional simulation problem series lower dimensional ones 
gibbs sampler suppose 
gibbs sampler randomly systematically choose coordinate say update new sample drawn conditional distribution delta gamma gammaa refers fx subset coordinates 
algorithmically describe gibbs sampler follows random scan gibbs sampler 
suppose currently 
ffl randomly select dg probability vector ff ff 
ffl draw conditional distribution delta gammai gammai gammai systematic scan gibbs sampler 
currently 
ffl draw conditional distribution gamma easy check individual conditional update leaves invariant 
suppose currently 
gammai follows marginal distribution 
gammai theta gammai gammai means conditional update joint distribution gammai 
regularity conditions show gibbs sampler chain converges geometrically convergence rate related variables correlate schervish carlin liu tierney 
argued grouping highly correlated variables gibbs update greatly speed sampler liu liu 
researchers shown random scan outperform systematic scan terms convergence speed roberts sahu 
simple restatement conditional updates gibbs sampler potentially useful seen way move point direction drawn appropriate distribution 
difficult show drawn gammai move leaves invariant 
view critical generalizing gibbs sampler transformation group setting liu wu useful designing efficient mcmc samplers 
see section discussions 
gibbs sampler popularity statistics community stems extensive conditional distributions iteration 
tanner wong data augmentation see chapter em algorithm linked gibbs sampling structure missing data problems em type algorithms see chapter em algorithm 
gelfand smith pointed conditionals needed gibbs iterations commonly available bayesian likelihood computations 
data augmentation component gibbs sampler suppose random variable partitioned parts current state 
component gibbs sampler updates follows ffl draw conditional distribution delta ffl draw conditional distribution delta 
sampler especially interesting reasons 
firstly corresponds data augmentation algorithm tanner wong designed handling bayesian missing data problems 
problems components say corresponds parameter interest corresponds missing data 
gibbs iterations ones drawing parameter value conditional currently imputed missing data missing data conditional current parameter value 
idea closely related em algorithm dempster laird rubin multiple imputation rubin long appealing applied statisticians 
secondly component gibbs sampler nice theoretical properties 
weak regularity conditions liu 
showed sampler converges geometrically monotonically 
convergence rate sampler equal maximal correlation components closely related statistical concept faction missing information rubin liu bayesian missing data problems 
stationarity cov var fh holds function derive expression lag auto covariances cov var delta delta delta ft delta delta delta cov var delta delta delta fs delta delta delta right hand sides expectation signs conditioned alternately formulas compare different imputation schemes show rao blackwellization increases efficiency monte carlo estimates 
example efron morris empirical bayes method analyze data bats middle season major league players shown column table 
estimated true betting probabilities data set predictions person betting average remainder season 
apply hierarchical bayes model data augmentation task 
denote observed betting average column table bats ith person denote true betting percentage 
variance stabilizing transformation performed efron morris arcsin gamma arcsin gamma approximately regarded random variable 
build hierarchical model assume iid oe 
furthermore assume prior distribution oe uniform gamma theta improper 
exercise reader may try priors note prior oe singular 
implemented gibbs sampler problem iterates steps draw conditional oe draw oe conditional values shows posterior density gibbs sampling approximation shrinkage table averages estimates average average stein efron morris player bats remainder estimator estimator estimates fact gibbs procedure modified improve efficiency 
see liu gelfand 
liu discussions 
special markov chain monte carlo algorithms illustrate metropolis hastings rule gibbs sampler adopted practice describe algorithms appeared frequently literature 
independence chain special choice proposal transition metropolis hastings algorithm independent trial density 
proposed move generated delta independent previous state method proposed hastings appears alternative rejection sampling importance sampling 
convergence properties studied liu 
independence sampler mis current state ffl draw ffl simulate uniform min 
theta 

left posterior density gibbs sampling solid versus normal dotted approximation 
solid curve indistinguishable true posterior 
right graphical view shrinkage estimates related respective mle 
usual importance sampling weight 
rejection method efficiency mis depends close trial density target 
ensure robust performance advisable delta relatively distribution 
tierney gelman rubin suggested insert couple mis steps gibbs iteration correctly sampling conditional distribution difficult 
idea useful bayesian computations conditional density approximated reasonably gaussian distribution 
accommodate irregular tail behaviors essential long tailed distribution 
random walk metropolis exactly algorithm simulating position distribution balls box 
suppose defined interest 
current state algorithm iterates follows ffl draw ffl oe set ffl 
oe spherically symmetric distribution oe controlled user 
distribution oe 
ffl simulate uniform update 
gelman roberts gilks gave interesting results heuristic guidance choose oe achieve fast convergence 
hit run algorithm current sample uniformly select random direction sample scalar density re update algorithm behaves random direction gibbs sampler allows complete exploration randomly chosen direction 
tends especially helpful modes comparable sizes target distribution 
main difficulty implementing algorithm rarely able draw practice 
may single step metropolis update chen schmeiser renders algorithm equivalent random walk metropolis 
adaptive directional sampling gilks roberts george proposed multiple chain mcmc method adaptive directional sampling ads allows exchange information different chains 
iteration ads algorithm population samples say fx size generation produced follows member selected random random direction generated gamma kx gamma anchor point chosen random scalar generated appropriate distribution update gilks 
roberts gilks show form jrj gamma re gave general form algorithm provided cautionary advice 
main difficulty ads practice sampling infeasible 
liu liang wong proposed way overcome 
slice sampler suppose density function interest drawing equivalent generating uniformly distributed region surface fz generating uniformly distributed random variables arbitrary region equally difficult simulation problem 
apply gibbs iteration achieve sampling ffl draw unif ffl draw uniformly region fx region iteration difficult deal 
written product functions theta delta delta delta theta edwards sokal introduced auxiliary variables described gibbs sampler sampling uniformly region ffl draw unif ffl draw uniformly region fx damien wakefield walker showed cases find decomposition intersection set easy compute leads easily implemented sampler 
noticed convergence may slowed presence auxiliary variables 
applications approach image analysis discussed besag green higdon 
gibbs sampler state space interest discrete liu suggested way improve ordinary gibbs sampler relaxation proved superiority 
suppose takes possible values distribution interest 
random scan gibbs sampler described section coordinate randomly chosen current value replaced value drawn corresponding full conditional distribution 
consider modification procedure value different drawn probability gammai gamma gammai replaces hastings acceptance probability min gamma gammai gamma gammai retained 
liu proves modified gibbs sampler discrete random variables defined statistically efficient random scan gibbs sampler 
gibbs sampler essentially barker method modified procedure metropolis 
algorithm 
general comparisons samplers 
besag 
note superiority metropolis binary systems results increased mobility state space 
rationale applies generally modified gibbs sampler 
convergence diagnosis view issue carlin combination gelman rubin geyer usually provide effective simple method monitoring convergence mcmc sampling 
approaches typically consume times computing time provide marginal improvements 
perfect simulation method proposed propp wilson exciting theoretical breakthrough value assessing convergence mcmc schemes noticed robert 
method ready general practical routine mcmc computation 
interested reader may find robert inspiring provided extensive study convergence diagnostics 
normal theory approximations target distribution gelman rubin proposed method involves steps 
sampling begins obtain simple trial distribution dispersed relatively target distribution 
generate say iid samples 

start independent samplers starting values obtained step 
run chain iterations 

scalar quantity interest appropriate transformation approximate normality say sample iterations compute average variances variance means parallel chains 

compute shrink factor gamma mn df df gamma df refers degree freedom distribution approximation empirical distribution 
gelman rubin suggested log general diagnosis benchmark 
choices reviewed carlin 
geyer main criticism gelman rubin approach difficult mcmc computation concentrate resources single chain iteration samples single run iterations come target distribution samples parallel runs iterations 
addition convergence criterion autocorrelation time physics produced single chain 
concerning generic mcmc methods advocate variety diagnostic tools single plot statistic 
run parallel chains relatively scattered starting points 
inspect chains comparing aspects histogram parameters autocorrelation plots gelman rubin efficient mcmc sampler optimizer section describe innovative ideas built fundamental mcmc framework global optimization efficient monte carlo simulations 
ideas tremendous impact scientific research areas computer industry 
simulated annealing condensed matter physics annealing known thermal process obtaining low energy states solid heat bath 
process steps ffl raise temperature heat bath high solid metal melt 
ffl decrease carefully temperature heat bath particles arrange ground state solid high temperature phase solid metal liquid particles flow rearrange freely low temperature particles gradually forced line attain lowest energy state 
realizing metropolis algorithm simulate particle movements various temperature reach thermal equilibrium kirkpatrick gelatt vecchi proposed computer imitation annealing process called simulated annealing si applied solve combinatorial optimization problems 
algorithm 
suppose task find minimum target function 
equivalent finding maximum expf gammah temperature delta delta delta delta delta delta sequence monotone decreasing temperatures reasonably large lim 
temperature run iterations metropolis hastings gibbs sampler expf gammah equilibrium distribution 
increases puts probability mass converging vicinity global maximum surely vicinity global optimum number iterations sufficiently large 
algorithmically ffl initialize arbitrary configuration temperature level ffl run steps mcmc iterations target distribution 
pass final configuration iteration 
ffl increase 
shown global maximum reached sa probability temperature decreases sufficiently slowly speed order log gamma delta delta delta geman geman 
practice afford slow annealing schedule 
frequently people linear exponential temperature decreasing schedule longer guarantee global optimum reached 
researchers experiences past fifteen years sa attractive general purpose optimization tool 
see aarts korst analysis 
simulated tempering parallel tempering increase mixing rate mcmc scheme parisi geyer thompson proposed technique simulated tempering st spirit simulated annealing 
implement st constructs family distributions pi ig varying single parameter temperature target distribution 
distribution corresponds member family highest temperature 
new target distribution st defined augmented space theta controllable constant role allow temperature level reasonable chance visited 
mcmc sampler draw samples st intuition st heating distribution repeatedly new sampler escape local mode increase mixing rate 
initiated space interest st algorithm consists steps st algorithm 
current state draw unif 
ffl ff drawn mcmc transition leaves invariant 
ffl ff propose temperature transition usually simple nearest neighbor random walk reflecting boundary probability min order st adjacent distributions need sufficient overlap ff need tuned carefully 
requirement demands prescribe temperature levels adversely affect efficiency algorithm 
optimization purpose applied relaxed version st vlsi design problem obtained results cong 
dynamic weighting method described section overcome steep energy barriers encountered temperature transitions 
parallel tempering geyer interesting powerful twist st augmenting theta geyer suggested augmenting product space theta delta delta delta theta identical copies suppose theta delta delta delta theta family distributions pi ig define joint probability distribution product space pt run parallel mcmc schemes space index swapping operation conducted place temperature transition st pt algorithm rigorously defined follows suppose current state theta draw unif 
ffl ff conduct parallel step 
update respective mcmc scheme 
ffl ff conduct swapping step 
randomly choose neighboring pair say propose swap accept swap probability scheme powerful simulating complicated systems bead polymers molecular structures 
popular dealing statistical physics models 
compared st pt need fine tuning adjust normalizing constants ff utilize information multiple mcmc chains 
dynamic weighting mcmc wong liang introduced dynamic weighting variable controlling markov chain simulation 
scheme able obtain better results optimization problems traveling salesman problem neural network training high dimensional integration problems ising model simulation 
start dynamic weighting scheme augment sample space theta include weight variable 
similar metropolis algorithm need proposal function space suppose iteration 
type move defined ffl draw compute metropolis ratio ffl choose draw uniform 
wr wr wr wr 
easy check type move equilibrium distribution 
wong liang propose invariance respect importance weighting justifying scheme 
joint distribution said correctly weighted respect wf 
transition rule said satisfy maintains correctly joint distribution 
clearly type move satisfies 
purpose introducing importance weights dynamic monte carlo process provide means system large transitions allowable standard metropolis transition rules 
weight variable updated way allows adjustment bias induced non metropolis moves 
reversible jump applications bayesian model selections green need sampler jumps different dimensional spaces 
principle follow rule guide design sampler 
suppose corresponds higher dimensional space lower 
communicate spaces needs proposals lower dimensional transition degenerate density respect dominant measure implying moves reversed transition overcome difficulty matching space dimension matching sampling distribution 
matched space come nondegenerate proposals theta theta follows metropolis hastings rule design jumps 
example jump draw follows draw delta 
accept move probability ff min ae oe jumping achieved proposing 
accepting fi min ae oe green formal treatment type move named reversible jumps 
method combining reversible jump simulated tempering speed mcmc sampling discussed liu 
multigrid monte carlo generalized gibbs multigrid method developed field computational mathematics solving partial differential equations mccormick 
main idea alternately apply iterative algorithm gauss seidel method different grid discretizations space 
doing slow varying components error damped rapidly coarser grid iterations high frequency components removed finer grid iterations 
goodman sokal applied idea monte carlo computation statistical physics models named multigrid monte carlo 
translated multigrid idea way designing progressively global moves 
updating component time gibbs sampler suggests moving highly correlated ones simultaneously certain subspace 
liu generalized key step statistical applications 
liu wu discovered fact generalization gibbs sampler move understood transformation group viewpoint 
locally compact group transformations space starting initial point group move new point achieved drawing conditional distribution jj jh dg jacobian transformation left haar measure letting 
shown move leaves invariant 
gibbs sampling update corresponds translation group coordinate time 
multiple try metropolis mtm section interesting generalization metropolis hastings transition rule 
new rule smit liu enables mcmc sampler large step size jumps 
particularly useful identifies certain directions interest difficulty implement gibbs sampling type move unfavorable conditional distributions 
mtm readily combined hit run ads algorithms section 
versions similar equivalent symmetric 
algorithm ffl draw trials proposal distribution 
compute ffl select probability proportional draw gamma distribution ffl accept probability min delta delta delta delta delta delta reject remaining probability 
algorithm ii ffl draw trials symmetric proposal distribution 
ffl select probability proportional draw gamma distribution 
denote ffl accept probability min delta delta delta delta delta delta reject remaining probability 
easily shown new transitions satisfy detailed balance condition induce reversible markov chains invariant distribution 
final remarks limited space able provide sketchy biased review developments markov chain monte carlo methodology 
lot interesting new theoretical methodological developments unable cover 
monte carlo methods left article sequential importance sampling significant development 
interested reader referred article liu chen 
provided general framework methodology reviewed connections related methods bootstrap filter sequential imputation rejuvenation kong liu chen 
theoretical convergence rates mcmc algorithms convergence diagnostics omitted 
lot useful monte carlo techniques developed physicists structural biologists included 
connections mcmc neural network training genetic algorithms commented 
applications mcmc left reader reading 
defects hope reader find mcmc methodology exciting methods described article useful 
hope reader join effort discover new efficient mcmc techniques understand theoretical properties 
aarts korst 

simulated annealing boltzmann machines 
wiley new york 
wainwright 

studies molecular dynamics 
chem 
phys 

barker 

monte carlo calculations radial distribution functions plasma 
austral 
phys 

besag green 

spatial statistics bayesian computation discussion 
roy 
statist 
soc 

besag green higdon mengersen 

bayesian computation stochastic systems discussion 
statist 
sci 

chen 
schmeiser 

performances gibbs hit run metropolis samplers 
comput 
graph 
statist 

cong kong xu liang liu wong 

simulated tempering vlsi floorplan designs 
proc 
asia south pacific design automation conference accepted 
carlin 

markov chain monte carlo convergence diagnostics comparative study 
amer 
statist 
assoc 

damien wakefield walker 

gibbs sampling bayesian hierarchical models auxiliary variables 
technical report business school university michigan 
dempster laird rubin 

maximum likelihood estimation incomplete data em algorithm discussion 
roy 
statist 
soc 

diaconis 

group representations probability statistics 
lecture notes monograph series ims hayward california 
efron morris 

data analysis stein estimator generalizations 
amer 
statist 
assoc 

smit 

understanding molecular simulation 
academic press new york 
hwang di stefano sheu 

convergence rates gibbs sampler metropolis algorithm single site updating dynamics 
roy 
statist 
soc 

gelfand sahu carlin 

efficient parameterizations normal linear mixed models 
biometrika 
gelfand smith 

sampling approaches calculating marginal densities 
amer 
statist 
assoc 

gelman rubin 

inference iterative simulation multiple sequences discussion 
statist 
sci 

gelman roberts gilks 

efficient metropolis jumping rules 
bayesian statistics bernardo eds oxford university press 
geman geman 

stochastic relaxation gibbs distributions bayesian restoration images ieee trans 

anal 
mach 
intell 

geyer 

markov chain monte carlo maximum likelihood 
computing science statistics proceedings rd symposium interface ed 

fairfax interface foundation 

practical monte carlo markov chain discussion 
statist 
sci 

geyer thompson 

annealing markov chain monte carlo applications ancestral inference 
amer 
statist 
assoc 

gilks richardson spiegelhalter 

markov chain monte carlo practice 
new york chapman hall 
gilks roberts george 

adaptive direction sampling 
statistician 
goodman sokal 

multigrid monte carlo method 
conceptual foundations 
physical review 
gordon salmon smith 

novel approach nonlinear non gaussian bayesian state estimation 
iee proc 
radar signal processing 
green 

reversible jump markov chain monte carlo computation bayesian model determination 
biometrika 
hastings 

monte carlo sampling methods markov chains applications 
biometrika 
higdon 

auxiliary variable methods markov chain monte carlo applications 
amer 
statist 
assoc 



exchange monte carlo method application spin glass simulations 
phys 
soc 
jpn 

karplus 

molecular dynamics simulations biology 
nature 
kirkpatrick gelatt jr vecchi 

optimization simulated annealing 
science 
knuth 
art computer programming vol 
algorithms 
addisonwesley reading 
kong liu wong 

sequential imputation method missing data problems 
amer 
statist 
assoc 



empirical analysis yield curve information data viewed window cox ross 
technical report department finance univ arizona 
lawrence altschul boguski liu neuwald 

detecting subtle sequence signals gibbs sampling strategy multiple alignment 
science 
leach 

molecular modelling principles applications 
addison wesley longman singapore 
liu 

collapsed gibbs sampler applications gene regulation problem 
amer 
statist 
assoc 


independent sampling comparisons rejection sampling importance sampling 
statistics computing 

theorem modified discrete state gibbs sampler 
biometrika 
liu chen 

blind deconvolution sequential imputations 
amer 
statist 
assoc 


sequential monte carlo methods dynamic systems 
amer 
statist 
assoc 

liu liang wong 

multiple try method local optimization metropolis sampling 
tech 
rep department statistics stanford university 
liu 

generalized multigrid monte carlo bayesian computation 
technical report department statistics stanford university 

simulated markov chain monte carlo spaces varying dimensions discussion 
bayesian statistics bernardo berger dawid smith eds 
new york oxford university press 
press 
liu wong kong 

covariance structure gibbs sampler applications comparisons estimators augmentation schemes 
biometrika 

covariance structure convergence rate gibbs sampler various scans 
roy 
statist 
soc 

liu wu 

parameter expansion scheme data augmentation 
am 
statist 
assoc tentatively accepted 
parisi 

simulated tempering new monte carlo scheme 
letters 


kiss generator 
tech 
rept dept statistics florida 
marshall 

multi stage sampling schemes monte carlo computations 
symposium monte carlo methods edited meyer wiley new york 
mccormick 

multilevel adaptive methods partial differential equations 
society industrial applied mathematics pa metropolis rosenbluth rosenbluth teller teller 

equations state calculations fast computing machines chem 
phys 



optimal monte carlo sampling markov chains 
biometrika 
propp wilson 

exact sampling coupled markov chains applications statistical mechanics 
random structures algorithms 
robert 

discretization mcmc convergence assessment 
springer new york 
roberts gilks 

convergence adaptive direction sampling 
mult 
anal 

roberts sahu 

updating schemes correlation structure blocking parameterization gibbs sampler 
roy 
statist 
soc 

roberts tweedie 

geometric convergence central limit theorems multidimensional hastings metropolis algorithms 
biometrika 
rubin 

multiple imputation surveys new york wiley 

sampling importance resampling alternative data augmentation algorithm creating imputations fractions missing information modest sir algorithm 
amer 
statist 
assoc 

schervish carlin 

convergence successive substitution sampling 
comp 
graph 
statist 

tanner wong 

calculation posterior distribution data augmentation discussion 
amer 
statist 
assoc 

tierney 
markov chains exploring posterior distributions discussion 
ann 
statist 

ullman 

computational aspects vlsi computer science press rockville md 
von neumann 

various techniques connection random digits 
natl 
bureau standards appl 
math 
ser 

wong liang 

dynamic importance weighting monte carlo optimization 
proc 
natl 
acad 
sci 

