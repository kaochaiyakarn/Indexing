sequential prediction individual sequences general loss functions david haussler computer science university california santa cruz santa cruz ca usa kivinen department computer science box fin university helsinki finland manfred warmuth computer science university california santa cruz santa cruz ca usa consider adaptive sequential prediction arbitrary binary sequences performance evaluated general loss function 
goal predict individual sequence nearly best prediction strategy comparison class possibly adaptive prediction strategies called experts 
general loss function generalize previous universal prediction forecasting data compression 
restrict case comparison class finite 
sequence define regret total loss entire sequence suffered adaptive sequential predictor minus total loss suffered predictor comparison class performs best particular sequence 
show large class loss functions minimax regret theta log omega gamma log depending loss function number predictors comparison class length sequence predicted 
case preliminary results appeared computational learning theory eurocolt pp 
clarendon press oxford computational learning theory second european conference eurocolt pp 
springer technical report ucsc crl university california santa cruz 
supported nsf iri doe de fg er mail haussler cse ucsc edu 
supported emil foundation academy finland esprit project neurocolt mail cs helsinki fi supported nsf iri ccr mail manfred cse ucsc edu 
shown previously vovk give simplified analysis explicit closed form constant minimax regret formula give probabilistic argument shows constant best possible 
weak regularity conditions imposed loss function obtaining results 
extend analysis case predicting arbitrary sequences take real values interval 
assume data consists sequence binary outcomes revealed outcome time 
time step trial seeing outcomes gamma predict outcome producing number 
actual outcome revealed suffer loss fixed loss function 
example scenario producing sequential probability assignments individual binary sequences 
estimate probability previous outcomes gamma case loss function logarithmic loss function log defined letting log gamma ln log gamma ln gamma 
loss negative logarithm probability predicted closely related number bits required encode gamma optimal sequential adaptive coding method sequential probability assignments 
possible loss functions square loss sq gamma literature sequential forecasting absolute loss abs jy gamma pattern recognition computational learning theory term line describe sequential procedure type :10.1.1.37.1595
absolute loss interpreted probability error predicting randomized strategy predicting probability probability gamma line prediction setup play role adaptive algorithm learning algorithm produces predictions trial 
nature provides sequence outcomes unknown process 
universal worst case prediction individual outcome sequences assumed process nature produce sequence outcomes 
performance learning algorithm judged worst case possible outcome sequences length 
order problem nontrivial considers performance learning algorithm relative performance best prediction strategy specified class line prediction strategies call comparison class set experts 
specifically possible sequence total loss incurred learning algorithm trials measured subtract infimum experts total loss incurred expert sequence 
difference represents regret suffered learning algorithm measured total loss suffers minus total loss suffered advice expert performed best particular sequence outcomes 
particular constant predictor predicts class constant sequential probability assignments memoryless encoding schemes 
minimax regret logarithmic loss essentially redundancy adaptive code total number bits needed encode sequence outcomes adaptively minus number bits required best constant prediction particular outcome sequence 
extensions include case consists markov predictors order finite state predictors number states 
course learning algorithm predictions line know ahead time expert perform best sequence outcomes produced nature 
remarkably universal prediction forecasting data compression shown cases learning algorithm achieve surprisingly small regret predictions knew ahead time expert advice take 
particular shown lempel ziv algorithm universally achieves quite small regret compared best finite state predictor 
general analysis universal prediction comparison class smooth parametric family general cases 
closely related done area mathematical finance seeks stock portfolio re balancing strategy performs best strategy comparison class market 
general decision theoretic scenarios considered 
related line competitive algorithms computer science see 
focus case comparison class finite 
goal develop general results possible finite case 
previous papers exception focused single loss function different different disciplines give unified treatment general class loss functions including usual ones 
previous papers specific forms comparison class studied kth order markov models finite state predictors certain size obtain bounds hold arbitrary finite set sequential prediction mechanisms 
allowed dependent sense prediction expert trial depend previous outcomes gamma predictions experts including time 
term experts statistical models predictors comparison class distinguish setting setting comparison class consists simpler statistical models 
standard universal prediction setting viewed game nature learner 
nature selects sequence outcomes learner predictions line suffers regret defined outcomes seen 
interested minimax value game minimum possible prediction strategies maximum regret possible outcome sequences 
call minimax regret 
exact minimax regret depends strongly comparison class simple comparison classes usually nice closed form formula loss function length play 
focus obtaining upper lower bounds minimax regret 
upper bounds minimax regret quite general depend number experts comparison class 
fact hold challenging game nature chooses predictions experts comparison class adversarial manner addition choosing outcomes 
game type defined analyzed cesa bianchi absolute loss extend analysis general loss functions 
upper bounds obtain minimax regret challenging game provide upper bounds minimax regret standard universal prediction game 
lower bounds show leading constants general upper bounds improved 
adversary construction lower bounds require full power available nature challenging game 
particular lower bounds show large set non adaptive experts predetermined predictions times known advance line prediction algorithm algorithm suffer minimax regret close general upper bound respect worst case outcome sequence 
vovk introduced line prediction algorithm applicable loss functions outcomes binary 
algorithm obtain general upper bounds minimax regret 
large class loss functions vovk proved algorithm minimax regret bounded ln independent number trials positive constant determined loss function number experts comparison class 
instance square loss vovk algorithm achieves bound logarithmic loss natural logarithm define loss function 
hand absolute loss abs cesa bianchi shown best general bounds minimax regret obtained theta log best possible constant bound approaches large number experts natural logarithm 
strong dependence number trials 
slightly weaker results absolute loss obtained earlier littlestone warmuth :10.1.1.37.1595
instructive compare general results obtained finite comparison class size logarithmic loss universal prediction results rissanen smooth parametric families models form infinite comparison classes 
rissanen shown purposes universal prediction logarithmic loss essentially loss performance suitable smoothness conditions replace continuous dimensional comparison class models finite approximation class parameters precision roughly number data points trials see equation preceding equation 
gives finite comparison class size 
case vovk bound ln minimax regret logarithmic loss gives bound roughly ln obtained rissanen optimal apart additive constant supplied deeper argument rissanen 
give simplified analysis vovk general algorithm yields explicit definition constant formula wide class loss functions including usual loss functions exception absolute loss 
provide probabilistic argument shows best constant obtained 
define class loss functions includes absolute loss prove general upper bound minimax regret loss function class smaller theta log loss functions class minimax regret general depend strongly number trials 
weak regularity assumptions loss function 
possible construct loss functions classes know bounds 
open problem provide nontrivial bounds minimax risk apply loss functions 
classes define cover broad range functions conclude asymptotic forms minimax regret obtained theta log theta log sense generic problem 
section gives formal description framework analysis 
bounds subsection discussion regularity conditions assumed loss function 
subsection vovk algorithm upper bound proof simplified purposes 
lower bound proof subsection generating outcome sequence simple randomized adversary simple randomly defined experts showing expected regret algorithm approaches worst case upper bound 
sense see particular setting average case difficult worst case 
proof technique randomized adversary previously cesa bianchi special case absolute loss 
subsection show certain loss functions square logarithmic loss vovk algorithm achieves worst case regret outcomes allowed arbitrary real numbers interval 
case logarithmic loss generalized relative entropy loss defined ent ln gamma ln gammay gamma combined lower bounds shows minimax regret case binary outcomes 
absolute loss worst case regret bounds proven binary outcomes achieved continuous valued outcomes slightly complicated algorithm show subsection 
line prediction loss bounds consider predictive performance line learning algorithm sequence outcomes 
algorithm performance compared best expert set en experts arbitrary line prediction strategy 
prediction expert outcome denoted real number interval 
prediction depend previous current predictions experts previous outcomes 
vector predictions experts trial called prediction vector defined 
algorithm prediction outcome assume access previous outcomes previous predictions experts including predictions current trial true algorithm access previous outcomes simulate predictive mechanisms experts 
upper bound results hold general cases algorithm simulate experts see discussion 
algorithms considered predictions independently length trial sequence situations consider algorithms fine tuned known advance 
define expert trial sequence pair trial 
consider separately cases binary outcomes outcomes continuous valued outcomes real number interval 
performance learner trial measured loss function range 
binary outcomes suffices consider functions defined 
example relative entropy loss ent defined ent ln gamma ln gammay gamma usual convention ln gives gamma ln gamma gamma ln ent binary case relative entropy loss better known logarithmic loss 
square loss sq defined sq gamma sq gamma hellinger loss lh lh gamma gamma gamma gamma lh gamma gamma gamma absolute loss abs abs jy gamma yj gamma abs worth noting properties loss functions example important 
case function increasing decreasing loss increases prediction moves away outcome functions differentiable previous absolute loss second derivatives positive means errors progressively expensive difference prediction outcome increases 
consider loss function line prediction algorithm expert trial sequence prediction algorithm trial sequence define loss loss algorithm loss loss ith expert sequence define loss gamma min loss regret additional loss algorithm amount loss algorithm exceeds loss best expert 
sup worst case regret outcomes expert trial length restricted binary 
formalizing challenging game nature allowed select outcomes predictions experts adversarial fashion 
vl inf smallest regret obtainable line prediction algorithm minimax value challenging game 
goal study vl general loss functions generalize results continuous valued outcomes 
general mathematical notation need follows 
var denote expected value variance random variable want emphasize underlying probability measure write var 
probability event probability measure denoted pr 
denote set positive integers denote set real numbers 
binary outcomes consider case binary outcomes results include upper lower bounds minimax regret 
section show usual loss functions upper bounds generalized allow continuous valued outcomes 
main results summarized subsection 
subsection gives algorithm obtains upper bounds proof 
algorithm analysis originally vovk able simplify considering continuous loss functions 
subsection contains main lower bound proofs 
subsection consider possibilities lower bound proofs 
main results proofs upper lower bounds require loss function satisfies certain constraints 
state main result necessary restrictions discuss meaning restrictions 
loss functions twice differentiable define function gamma function gamma define constant sup write 
main result concerns case finite 
finite loss function satisfies certain conditions prove upper bound ln show bound asymptotically tight 
theorem loss function times differentiable 
assume constant defined finite defined positive 
line prediction algorithm ln holds 
vl gamma ln denotes quantity approaches approach 
algorithm obtains bound proof bound vovk 
algorithm predictions independently length trial sequence 
give algorithm simplified proof subsection 
note length sequence appear right hand side 
lower bound probabilistic proof subsection 
lower bound holds algorithms get knowledge 
example consider loss functions example 
loss functions clearly 
logarithmic loss gamma gamma 
identically 
square loss gamma 
hellinger loss gamma gamma gamma gamma gamma straightforward show maximized 
gamma absolute loss identically theorem applicable absolute loss 
typical loss functions conditions hold 
theorem non applicable implies reasons occur 
subsection prove lower bounds show denominator strictly positive regret vl upper bound independent 
theorem loss function times differentiable 

vl omega log 
values vl omega log special case absolute loss considered cesa bianchi 
show optimal algorithm theta ip ln absolute loss denominator lower bound generalizes lower bound general loss functions 
unfortunately case general loss functions know corresponding upper bound 
possible value infinite denominator positive construct example show behavior possible usual loss functions literature exhibit 
loss functions results implications whatsoever 
example define loss function gamma gammaff gamma gammaff gamma positive value ff 
ff ff gammaff gamma gamma gammaff approaches approaches infinite 
results give upper bound vl 
denominator ff ff gamma gammaff gamma strictly positive 
lower bound 
loss function open problem define bounds vl 
ignoring artificially constructed special case example results specific loss functions divided sign function logarithmic loss square loss hellinger loss value positive theorem applies 
absolute loss zero theorem applies 
conclude section clarify intuitive meaning function connecting predictions simple probabilistic prediction game 
probability measure pr prediction expected loss probability measure bias gamma ql 
define delta 
example logarithmic loss expected loss prediction defined bias 
biases infinite 
prediction bayes optimal bias minimizes expected loss 
note assume continuous closed interval expected loss minimum value holds allow infinite losses 
increasing decreasing prediction bayes optimal bias prediction bias 
value local extremum point expected loss gamma ql gamma implies gamma gamma generally nonzero value unique value holds bayes optimal prediction bias 
gamma ql holds addition local minimum point 
may bayes optimal predictions bias 
lemma loss function times differentiable 
biases unique bayes optimal prediction biases bayes optimal prediction unique interval 
proof lemma subsection 
close section applying lemma specific loss functions 
example logarithmic square hellinger losses loss function example unique bayes optimal prediction bias actual bayes optimal predictions determined straightforward calculations 
logarithmic square losses hellinger loss gammaq loss function considered example gamma gammaff gamma gammaff gamma positive value ff gammaq ff absolute loss identically zero bias bayes optimal predictions 
easy calculations show unique bayes optimal prediction biases biases 
bias prediction bayes optimal 
algorithm upper bound consider algorithm introduced vovk 
give general algorithm analysis applied situation loss function continuous 
examples details interesting loss functions 
algorithm positive real valued parameters introduce algorithm somewhat open form leaving parameters unspecified defining prediction giving condition satisfy 
moment leave open possibility prediction satisfies condition case say algorithm fails 
parameter vaguely characterized measure error allowed algorithm 
smaller value tighter upper bound get regret assuming algorithm fail 
applying algorithm need find value algorithm guaranteed fail learning rate chosen suitably 
turns loss function satisfies assumptions theorem suitable choice gives bound ln main part proof showing choice algorithm guaranteed fail give direct way choosing prediction satisfies required conditions provided prediction exists 
examples show seemingly complicated conditions quite simple usual loss functions 
algorithm uses dimensional weight vector internal state 
weight nonnegative summarizes performance ith expert previous trials 
tth trial gamma ln consists trials note weights invariant permutations trial sequence predictions algorithm independent total length trial sequence 
algorithm generic algorithm loss function positive constants 
initialization set weights initial values 
prediction trial compute value delta gammac ln receiving tth input predict value satisfies condition delta value exists algorithm fails 
update receiving tth outcome understand algorithm note write delta gamma gammac ln consider gammac ln potential function condition delta means trial increase potential large loss algorithm 
case logarithmic loss key quantities generic algorithm natural statistical interpretation 
particular turns optimal set gamma gammay quantity interpreted likelihood probability model ith expert 
update interpreted bayesian update posterior probabilities set experts 
additivity logarithmic loss associated statistical interpretation chain rule analysis special loss convenient pointed 
bounds logarithm loss obtained certain inequalities derive bounds losses 
obtain better results vovk generalization likelihood directly obtain analogous chain rule general loss 
basic idea proving upper bound loss generic algorithm relating total potential increase gamma total loss best expert 
upper bound vovk 
theorem loss function 
expert trial sequence outcomes binary 
assume trial sequence generic algorithm parameters fail produces trial prediction total loss satisfies loss gammac ln gammac ln proof condition implies gammac ln gammac ln gammac ln get theorem follows 
values say loss function realizable condition satisfied suitable choice prove upper bound theorem suffices show loss function satisfies assumptions theorem realizable result follows theorem setting rest subsection gives formulation vovk proof results 
develop equivalent version condition 
write delta delta delta delta condition expressed delta delta obtain explicit bounds conditions need notion inverse assume continuous strictly increasing continuous strictly decreasing implied assumptions theorem 
continuous strictly increasing inverse gamma defined continuous strictly decreasing inverse gamma defined 
consider case delta delta 
values gamma delta gamma delta defined equivalently written gamma delta gamma delta prediction satisfies gamma delta gamma delta holds prediction chosen arbitrary value bounds gamma delta gamma delta 
instance mean gamma delta gamma delta valid choice consider possibility value delta delta outside range respectively 
instance delta larger condition delta holds equivalence maintained nonnegative delta inverse gamma extended way condition gamma delta holds delta 
say gamma generalized inverse gamma gamma delta delta 
similarly gamma generalized inverse gamma gamma delta delta 
instance square loss sq generalized inverses gamma gamma gamma delta delta relative entropy loss ent gamma gamma gammaz gamma gammaz get gamma delta gamma delta absolute loss abs gamma gamma gamma need delta delta definitions generalized inverses show equivalence values delta delta lemma assume loss function continuous strictly increasing continuous strictly decreasing 
generalized inverses gamma gamma condition equivalent proof delta delta gamma delta hold 
delta delta gamma delta hold 
may assume delta range delta range case equivalent strictly increasing strictly decreasing 
ready show algorithm value defined set algorithm fails 
lemma loss function times continuously differentiable hold 
assume value defined finite defined positive condition holds proof define exp gammal exp gammal range define exp gammal gamma gammac ln note 
assume holds 
going show fact true delta gammac ln delta gammac ln 
assumption implies 
get delta gammac ln gammac ln gamma gammac ln gamma delta condition follows gamma decreasing 
show assumptions imply function non positive second derivative range 
differentiating obtain gamma gammal gamma 
substituting gammal gammal similar expressions see gammal gamma assumptions imply gamma conclude holds 
necessary sufficient condition having note argument shows necessary condition 
positive interval placing values interval making equal get gamma delta gamma delta 
particular see generic algorithm fail parameters get upper bound claimed theorem applying theorem initial weights theorem loss function constant finite 
generic algorithm parameters initial weights regret algorithm satisfies ln ready write generic algorithm explicit form particular loss functions 
example logarithmic loss take generic algorithm 
simple manipulations get delta gamma ln gamma delta gamma ln weighted average experts predictions 
gamma delta gamma delta prediction holds choice loss bound obtain previously shown de santis vovk 
example square loss 
vovk shown square loss realizable 
result follows lemma example 
note proof lemma implies square loss realizable 
take generic algorithm square loss 
condition gamma gamma ln gamma gammax gamma ln gamma numerically substituting random values see seemingly natural choice usually satisfy 
generally function choosing guarantee hold 
see consider set 
evaluating left hand side values yields bound 
hand evaluating right hand side values gives contradictory condition 
algorithm needs information provided merely weighted average experts predictions 
proved restricted case experts predictions guarantee square loss 
gives slightly improved bound 
restricting experts predict binary values allowing algorithm predict continuous values natural setting 
example take absolute loss 
know absolute loss realizable arbitrary see values absolute loss realizable 
bound gamma gamma gammaj holds obtain gamma delta gamma gamma delta gammac ln gamma ln gammaj gammax gamma ln gamma gamma gammaj gamma ln gamma gamma gammaj gamma gamma gamma ln gamma gammaj gamma ln gamma gammaj gamma jensen inequality positive ln gammaj gamma prediction condition ln gammaj gammax ln gammaj gamma ln ln gammaj cesa bianchi noted holds choose ln gamma gammaj ln gamma gammaj ln gamma gammaj general hold weighted average experts prediction provides sufficient information prediction directly 
bound obtained applying theorem absolute loss choice ln gammaj gamma loss gamma ln ln gammaj proven vovk 
choose learning rate way loss bound right hand side minimized 
tuning learning rate discussed detail cesa bianchi 
just cite basic results 
initial weights chosen ln iq ln ln generic algorithm absolute loss satisfies ln log note necessary know trial order choose learning rate appropriately 
similar results obtained basing choice upper bound loss min loss best expert 
consider variations generic algorithm cesa bianchi special case absolute loss 
update write generally ff delta gammac ln ff consider choices factors ff addition choice ff exp gammaj jy gamma generic algorithm 
note gamma ln ff jy gamma proof theorem easily generalized yield loss bound 
second note proof inequality gamma delta gamma delta valid assuming ff gamma gamma gammaj jy gamma algorithm works gives worst case loss bound choice gammax ff gamma gamma gammaj jy gamma interestingly weights obtained ff gamma gamma gammaj jy gamma bayesian interpretation 
lower bounds subsection contains proofs lower bounds vl stated theorems subsection 
lower bounds hold algorithms receive input trial 
lower bound proofs probabilistic method 
consider trial sequences outcomes independent identically distributed random variables distribution experts predictions independent identically distributed random variables distribution 
derive arbitrary algorithm lower bound expected regret trial sequence drawn distribution 
clearly holds yields lower bound minimax regret 
surprisingly turns lower bound derived simple probabilistic setting tight matches asymptotically upper bounds derived assuming arbitrary adversarial choice experts outcomes 
outline proof 
consider arbitrary fixed distributions pr probability drawing outcome bias distribution define gamma ql denote expected loss prediction bias 
recall section minimized called bayes optimal prediction bias assume bayes optimal prediction 
expected loss loss obtains minimum algorithm expected regret minimized purposes bounding expected regret loss generality assume note true regardless experts predictions allow algorithm know experts predictions 
proving stronger result fixed choice experts predictions choice lower bound achieved set outcomes matter prediction algorithm 
consider experts choosing predictions independently distribution parameters need lower bound calculation oe var 
lower bound proof giving theorem large lower bound form vl gamma ln ff positive independent term right hand side simply expected loss optimal prediction algorithm second term expected loss fixed expert distribution final term shows better best expert expected perform compared fixed single expert 
obviously final term large variance experts predictions variance measured parameter oe 
bound form theorem holds useful carefully chosen 
consider choosing fixed 
simple case distinct bayes optimal predictions bias say covered lemma 
setting pr pr yields distribution oe substituted gives desired result vl omega gamma log 
interesting case considered theorem unique prediction bias unique minimum point case choose pr gamma pr close zero 
estimating loss functions second order taylor expansions obtain values oe substituted yield vl gamma ln 
notice bound implicit dependence assume bayes optimal bias remains choice bias bias bayes optimal prediction directly get lower bound omega gamma log bias 
lemma shows varying prediction vary range 
suitable choice allows replace bound supremum minor technical complication third case bias unique bayes optimal prediction get bound slightly weaker omega gamma log 
actual proof 
provide bound holds arbitrary distributions theorem probability measure probability measure assume condition pr holds constant bayes optimal prediction oe var 
assume variance var strictly positive 
vl gamma gamma oe ln lim 
proof theta define expert trial sequence length hx yi 
line prediction algorithm consider hx yi random variable drawn product measures theta respectively 
expected value random variable clearly lower bound supremum 
combining linearity expectation get theta hx yi gamma theta min loss hx yi gamma theta min loss hx yi holds obtain prove theta min loss hx yi gamma gamma oe ln basic method estimating expectation left hand side consists steps 
apply central limit theorem random variables loss hx yi see large approximately normal distribution 
second apply known results directly give expectation minimum set identical independent normal random variables 
steps relatively simple 
unfortunately random variables loss hx yi give losses various experts independent outcome sequence affects 
proof rigorous need add inelegant details considering arbitrary fixed outcome sequence pr 
gamma qe oe gamma var sequence define gamma oe gamma var var estimates obtained oe true probability theta ij loss expert trial sequence experts predictions sequence outcomes 
consider ij random variable domain theta define random variable domain theta theta ij denote loss expert trials 
define sequence random variable ij 
underlying probability measures random variables product measures defined fixed random variables ij independent 
study distribution define suitably normalized random variable gamma ij var ij var 
assumed pr jt ij lindeberg form central limit theorem implies sequences converges distribution standard normal random variable 
fn independent standard normal random variables 
known min gammaa ln lim 
sequence converges distribution apply various convergence theorems show lim theta min gammaa ln lim 
quite need 
really interested expected minimum variables give losses experts normalized variables denominator right hand side complicated dependence expectations normalized variables readily transformed back expectations original ones 
get desired result show considering expectations limit large get results replace var ij expected value oe 
define oe oe 
jr oe strong law large numbers lim apply equation lim theta min min obtain directly applying lemma proved appendix 
intuitively merely changed order limit expectations minimum random variables 
words lim oe oe theta min gammaa ln probability theoretic part proof rest straightforward 
partitioning summations parts write gamma gamma gamma var var gamma oe substituting obtain lim theta min gamma oe gammaa ln value theta min gamma theta min loss hx yi gamma gamma gamma oe ln implies desired 
see theorem implies lower bound vl probability measure experts chosen suitably 
consider case prediction unique 
bayes optimal prediction minimum point expected loss result cases depending second derivative expected loss positive zero minimum point 
lemma loss function times differentiable hold 
assume bayes optimal prediction bias 

gamma ql vl gamma ln denotes quantity approaches values ratio ln approach 
gamma ql vl omega log proof probability measure pr arbitrary line prediction algorithm 
probability measure theorem sufficiently large bound gamma gamma oe ln lim 
positive parameter define give gamma probability probability 
simple calculus approximate right hand side function accuracy 
choose value maximizes approximated value 
see resulting value terms safely ignored approach infinity manner stated lemma 
expand sigma sigma sigma denotes quantity lim similarly substitute expansions various quantities 
note var gamma similarly var oe gamma ql gamma gamma gamma gamma ql gamma rh gamma sh gamma gamma ln gamma ql gamma ql consider case gives part theorem 
main part rh gamma sh bound maximized choosing 
value get gamma gamma gamma ql gamma ql ln gamma log gamma ln theta log gamma applying eliminate get claimed result lim 
consider case gives second part theorem 
ah ln gamma gamma gamma ql choosing gamma get ln 
bayes optimal prediction bias unique get asymptotically stronger bound grows grow 
lemma loss function strictly increasing strictly decreasing 
assume bias distinct bayes optimal predictions vl gamma oe ln lim oe gamma gamma gamma proof distinct bayes optimal predictions probability measure strictly monotone bias 
define probability measure pr pr apply theorem 

get var gamma gamma gamma similarly var gamma oe 
results follows theorem note strictly monotone right hand side strictly positive 
absolute loss apply lemma 
gives oe vl gamma ln result obtained cesa bianchi 
recall lemma lower bound terms assuming unique bayes optimal prediction bias 
show value bayes optimal prediction bias allows replace supremum bias multiple bayes optimal predictions gives stronger lower bound omega gamma log lemma 
lemma prediction bayes optimal bias predictions bias bayes optimal 
proof consider prediction bayes optimal bias 
set biases bayes optimal prediction set biases bayes optimal prediction show done 
bayes optimal 
closed intersection empty 
suppose closed 
monotone sequence points converges point bayes optimal prediction bias 
sequence monotone converges limit bayes optimal prediction bias define gamma ql 
bayes optimal bias continuous implies 
bayes optimal bias contradiction 
similar argument works assume closed 
ready combine lower bounds theorem 
wish replace various assumptions concerning bayes optimal predictions assumptions function defined 
purpose apply lemma 
proof lemma assume strictly increasing strictly decreasing unique bayes optimal prediction bias unique bayes optimal prediction bias 
assume bayes optimal predictions bias expected loss gamma ql local minima local maximum value 
condition implies gamma gammal substituted gives 
assume bias unique bayes optimal prediction 
lemma implies bias bayes optimal know bias unique 
denote bias bayes optimal prediction 
know strictly increasing 
gammal 
gamma 
strictly increasing derivative negative zero continuous interval 
gamma claim follows 
lower bounds theorem theorem follow directly theorem 
theorem loss function times differentiable hold 


vl gamma ln 

vl omega log ff 
values continuous interval vl omega log proof bias distinct bayes optimal predictions lemma bound vl omega log strongest bounds claimed 
need consider case bias bayes optimal prediction 
lemma predictions bias bayes optimal 
lemma value nonnegative zero continuous interval 
recall bayes optimal condition implies gamma ql sign 
applying lemma bias bayes optimal gives bound vl omega log ff 
lemma gives vl gamma ln vl gamma ln follows 
alternative lower bound methods lower bounds proved sufficient show improve constant upper bound theorem 
lower bounds having approach infinity 
interesting get bounds say constant approaching infinity 
special cases really results lines 
give ideas arguments useful 
notice logarithmic loss simple argument shows lower bound vl ln example arbitrary positive integer arbitrary line prediction algorithm 
trials choose binary prediction vectors way set experts prediction sequences contains possible binary sequences length 
outcomes chosen adversary way prediction algorithm satisfies 
trial algorithm incurs loss ln total loss algorithm ln ln expert total loss obtain ln matches exactly upper bound theorem example generic algorithm 
way thinking lower bound argument follows 
trial half experts predict half experts predict 
trial mistake eliminated correct remain 
subsequent trials half remaining experts predict half predict 
trial gamma experts remaining cumulative loss rest experts cumulative loss eliminated 
note considering single trial easily gives logarithmic loss bound vl ln 
general lower bound vl ln logarithmic loss obtained applying theorem lower bound vl 
theorem proven lemma 
lemma assume line prediction algorithms expert trial sequence length line prediction algorithms expert trial sequence length line prediction algorithms expert trial sequence length proof expert coupled trial sequence sequence instance property expert simple trial sequence sequence instance property delta delta delta delta delta delta note expert coupled trial sequences essentially expert trial sequences expert simple trial sequences essentially expert trial sequences 
assumed prediction algorithms expert trial sequence length follows line prediction algorithms expert coupled trial sequence length similarly assumed prediction algorithms expert trial sequence length follows line prediction algorithms expert simple trial sequence length arbitrary line prediction algorithm trial sequences length trial sequence length denote algorithm trial sequences length simulates algorithm processes trial sequence actual trial 
assumptions imply expert coupled trial sequence length expert simple trial sequence length expert trial sequence length obtained concatenating complete proof show loss gamma loss holds note loss loss loss 
know loss loss holds coupled trial sequence implies loss loss know loss loss holds simple trial sequence implies loss loss holds loss loss loss loss proves claim 
proof lemma remains valid algorithms allowed know length trial sequence 
obvious induction lemma gives result 
theorem loss function positive integer vl 
particular lim vl ln constant theorem implies lim vl log ln ln able prove lim vl ln constant defined obtain asymptotic lower bound vl gamma ln stated theorem 
new bound stronger term approaches approaches form bound theorem term stated approach approach 
obtain lower bound vl gamma ln theorem example square loss applying theorem need show lim vl ln conjecture true 
numerically obtained lower bounds vl ln 
obviously vl increasing function vl ln upper bound theorem example numerical results recurrence able solve closed form 
note square loss simple construction logarithmic loss yield optimal lower bound 
algorithm predicts bound falls short required ln 
preceding remarks show logarithmic loss lim vl lim vl 
interesting open question see loss functions property 
theorem gives lim vl lim vl loss functions 
show equality sufficient show lim vl ln conjecture true square loss 
continuous valued outcomes applying generic algorithm show certain assumptions generic algorithm works continuous valued outcomes 
assumptions hold square relative entropy loss absolute loss considered subsection 
consider general situation values range 
lemma assume function defined gamma jl satisfies holds binary values holds values 
proof write gamma delta 
exponentiating sides applying get denote left hand side 
second derivative get assumption implies nonnegative maximum value interval occurs 
equivalent proves claim 
theorem loss function constant finite condition holds generic algorithm parameters initial weights trial sequence hold algorithm fail trial sequence regret satisfies ln proof note lemma algorithm fail 
lemma predictions algorithm satisfy delta 
proceed proof theorem obtain claimed bound choosing example relative entropy loss ent ln gamma ln gamma gamma ln ln gamma second derivative gamma depend second derivative function lemma holds 
recall relative entropy loss 
theorem generic algorithm ln expert trial sequence outcomes continuous valued 
example square loss sq second derivative constant second derivative function lemma trivially holds 
generic algorithm 
theorem ln trial sequence contains continuous valued outcomes 
consider general case trial experts predictions outcome known range 
gamma gamma prediction generic algorithm scaled inputs outcomes theorem applies scaled sequence trials 
algorithm predicts loss bound choose initial weights equal gamma min gamma ln consider giving loss bound similar loss function changes dynamically ranges vary 
note achieving bound requires known prediction 
case instance outcome assumed range defined smallest largest expert prediction trial special case trial know range 
take equivalent gamma min gamma ln note range bounded loss bounds form attained 
see consider trial sequence prediction vector gammar 
outcome chosen adversary gammar gamma depending algorithm prediction negative 
loss best expert loss algorithm 
grow regret algorithm grows omega ip absolute loss abs derivative technique lemma give results loss function 
subsection devise new algorithm particularly problem 
vee algorithm show loss bounds obtained absolute loss binary outcomes achieved outcomes continuous valued 
results section obtained independently vovk private communication 
call algorithm vee algorithm 
choosing prediction necessary explicitly consider outcomes just 
show prediction computed time log 
algorithm vee algorithm generic algorithm fixed loss function absolute loss parameter ln gammaj gamma predicting done follows prediction receiving tth input predict value satisfies condition max gamma delta min delta delta gamma ln gammax ln gammaj easy see prediction obtained time values gammax obtained choices vector contains components prediction vector sorted ascending order 
gamma 
vector obtained time log 
vector obtained applying permutation applied gives exp jy gamma exp fi fi fiy gamma fi fi fi 
show sums obtained time sorted prediction vector example graphs functions delta abs 
unify notation write 
note write gammaj gammax gammaj gammax computed time 
obtain time gammaj gammax gammaj gammax gamma gammaj gammax prediction exists total time log 
see lemma prediction satisfies implies jy gamma delta merely requirement generic algorithm 
get continuous valued outcomes bound previously obtained binary outcomes note holds holds provided 
moving outside range experts predictions increases jy gamma increases jy gamma coefficient ln gammaj appears front jy gamma greater 
parameter tuned mentioned example scaling method example values range 
absolute loss simple geometric interpretation 
gives example graphs left hand side jy gamma yj right hand side delta functions fixing 
left hand side inequality represented vee curve tip 
graph delta non differentiable tip value condition states vee curve graph delta continuous valued outcomes wish hold vee curve graph delta 
move tip vee left right arm vee intersect delta curve value 
value maximum left hand side roughly 
similarly minimum right hand side moving tip vee value left arm intersect delta curve 
binary outcomes required hold gives weaker condition vee curve graph delta endpoints 
binary outcomes loss bound previously shown family algorithms defined number different prediction update factors ff briefly explained example 
continuous case freedom 
suppose ff gamma gamma gammaj jy gamma 
delta satisfy jy gamma yj delta choose 
delta jy gamma yj delta 
algorithm wmc continuous case allowed update satisfies :10.1.1.37.1595
worst case bound gamma gammaj denominator ln gammaj slightly worse bounds 
noticed example binary outcomes possible choose prediction function weighted average experts predictions 
outcomes allowed continuous valued possible 
see function guarantees hold consider cases 

value left hand side approximately obtain constraint hand considering right hand side gives contradictory constraint 
show prediction satisfies exists satisfies conditions theorem 
lemma 
prediction satisfies exists 
implies jy gamma delta 
proof prove existence showing gamma delta delta holds define exp gammaj jy gamma jz gamma gamma ln gammaj equivalent 
second derivative defined positive suffices show 
restricted case second derivative positive furthermore delta trivially holds suffices show 
second derivative positive left case case original inequality rewritten ln gamma gammaj ln gamma pe gammaj ln gammaj holds function ln concave 
similar argument second derivatives shows value gamma delta obtains value delta lemma immediately implies result 
theorem trial sequence absolute loss vee algorithm 
loss gamma ln ln gammaj challenging open problems give tight bounds regret prediction algorithm compared loss best expert general classes loss functions considered 
outcomes binary possible produce bounds arbitrary loss functions 
challenge extend results continuous valued outcomes general loss functions 
direction worth exploring outcomes discrete valued choices 
results chung address problems 
restricted predictions experts lie zero specific examples indicated scaling tricks 
nice thorough investigation scaling range variables affects results 
bounding norm prediction vector lead interesting problems 
restricting range predictions individual experts related bounding infinity norm prediction vectors 
interesting see alternative update rules defined absolute loss loss functions 
seen possible obtain prediction function weighted average experts predictions 
know exactly simplification possible weakening bounds weakening slightly 
bounds regret algorithms loss best expert 
challenging problem bound regret algorithms best linear combination experts 
worst case loss bounds case obtained square loss function 
hopefully results generalized linear combination case 
intermediate case worth exploring case bounding regret algorithm compared best stretched expert original expert multiplied positive constant 
acknowledgments david helmbold significant help developing vee algorithm andrew barron insightful comments 
appendix lemma probability measure probability measure independent identically distributed random variables var 
assume independent identically distributed random variables fn sequence converges distribution functions lim holds probability drawn jr holds constant lim min min proof write min min show sequence converges distribution pr gamma gamma pr gamma gamma lim pr lim gamma gamma pr lim pr proves claim 
see ju holds 
see note value distribution depend implies pr pr measurable 
implies ju ne ju ju dp ju ju dp ij sequence converges distribution bound guarantees corollary lim lim probability drawn bound implies jr bn bounded convergence theorem thm 
lim claimed 
auer cesa bianchi freund schapire gambling casino adversarial multi armed bandit problem proc 
th annual symposium foundations computer science los alamitos ca ieee computer society press pp 

barron xie asymptotic minimax loss data compression gambling prediction proc 
th annual conference computational learning theory new york ny acm press 
patrick billingsley probability measure second edition 
new york wiley 
blackwell controlled random walks proc 
internat 
congress mathematicians vol 
iii amsterdam pp 

blackwell analog minimax theorem vector payoffs pacific journal mathematics vol 
pp 

cesa bianchi freund haussler helmbold schapire warmuth expert advice journal acm vol 
pp 

cesa bianchi freund helmbold warmuth line prediction conversion strategies machine learning vol 
pp 

cesa bianchi helmbold bayes methods line boolean prediction proc 
th annual conference computational learning theory new york acm press pp 

cesa bianchi long warmuth worst case quadratic loss bounds line prediction linear functions gradient descent ieee transactions neural networks vol 
pp 

chung approximate methods sequential decision making expert advice proc 
th annual acm workshop computational learning theory new york acm press pp 

cover behavior sequential predictors binary sequences proc 
th prague conference information theory statistical decision functions random processes publishing house academy sciences pp 

cover ordentlich universal portfolios side information ieee transactions information theory vol 
pp 

dawid prequential analysis stochastic complexity bayesian inference bayesian statistics oxford clarendon press markowsky wegman learning probabilistic prediction functions proc 
th annual ieee symposium foundations computer science los alamitos ca ieee computer society press pp 

feder merhav gutman universal prediction individual sequences ieee transactions information theory vol 
pp 

foster prediction worst case annals statistics vol 
pp 

freund predicting bits optimal biased coin proc 
th annual conference computational learning theory new york acm press pp 

asymptotic theory extreme order statistics second edition 
fl krieger 
hannan dynamic statistical decision problem component problem involves finite number distributions annals mathematical statistics vol 

hannan approximations bayes risk repeated play ann 
math 
stud vol 
pp 

princeton university press 
haussler barron bayes methods line prediction gamma values proc 
rd nec symposium computation cognition siam pp 

helmbold schapire singer warmuth line portfolio selection multiplicative updates proc 
th international conference machine learning san francisco morgan kaufmann pp 

kivinen warmuth additive versus exponentiated gradient updates linear prediction information computation vol 
pp 

littlestone long warmuth line learning linear functions journal computational complexity vol 
pp 

littlestone warmuth weighted majority algorithm information computation vol :10.1.1.37.1595
pp 

merhav feder universal sequential learning decisions individual data sequences proc 
th annual workshop computational learning theory new york acm press pp 

learning algorithm linear operators proceedings american mathematical society vol 
pp 

opper haussler worst case prediction sequences log loss mathematics information coding extraction distribution springer verlag 
rissanen universal coding information prediction estimation ieee transactions information theory vol 
pp 

rissanen stochastic complexity statistical inquiry 
volume series computer science world scientific 
rissanen fisher information stochastic complexity ieee transactions information theory vol 
pp 

shtarkov coding discrete sources unknown statistics topics information theory amsterdam north holland pp 

shtarkov universal sequential coding single messages prob 

inf vol 
pp 

vitter krishnan optimal prefetching data compression proc 
nd ieee symposium foundations computer science los alamitos ca ieee computer society press pp 

vovk aggregating strategies proc 
rd annual workshop computational learning theory san mateo ca morgan kaufmann pp 

vovk universal forecasting algorithms information computation vol 
pp 

vovk game prediction expert advice proc 
th annual conference computational learning theory new york acm press pp 

weinberger lempel ziv sequential algorithm universal coding finite memory sources ieee transactions information theory vol 
pp 

weinberger merhav feder optimal sequential probability assignment individual sequences ieee transactions information theory vol 
pp 

