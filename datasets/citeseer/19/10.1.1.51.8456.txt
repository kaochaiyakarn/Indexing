implementation dual affine scaling algorithm minimum cost flow bipartite uncapacitated networks mauricio resende veiga 
describe implementation dual affine scaling algorithm linear programming specialized solve minimum cost flow problems bipartite uncapacitated networks 
implementation uses preconditioned conjugate gradient algorithm solve system linear equations determines search direction iteration interior point algorithm 
preconditioners considered diagonal preconditioner preconditioner incidence matrix approximate maximum weighted spanning tree network 
dual spanning tree allows early identification optimal solution 
applying ffl perturbation cost vector optimal extreme point primal solution produced presence dual degeneracy 
implementation tested solving large instances randomly generated assignment problems comparing solution times network simplex code relaxation algorithm code relax 
interior point algorithm greatly benefits implemented parallel architecture 
largest instances tested interior point code competitive simplex relaxation codes 
key words 
linear programming interior point algorithm network flows assignment problem conjugate gradient preconditioning ams mos subject classifications 


consider directed graph set vertices set edges denoting directed edge vertex vertex underlying graph define network attaching certain numerical quantities vertices edges 
represent units flow produced consumed vertex associated edge define quantities ij ij ij representing respectively unit cost lower bound upper bound 
solution network flow problem referred flow represented jej dimensional vector component ij stands flow edge 
minimum cost network flow mcnf problem consists finding flow minimum cost subject flow conservation constraints vertices constraints lower upper bounds flow edges 
required optimal flow consist integer quantities 
implementation describe restricted problems bipartite vertex partition ij ij 
edge vertex vertex techniques described applied general mcnf problem described previous paragraph 
limit focus sub class implementation considerations 
november revised version january bell laboratories murray hill nj usa department university california berkeley ca usa mauricio resende veiga integer programming formulation mcnf problem minimize ij ij ij subject ik ik kj kj ij integer assume furthermore assuming connected single redundant constraint remove formulation 
resulting constraint matrix correspondence basic sequences spanning trees assume data integer 
constraint matrix totally unimodular basic solutions integer integrality constraint dropped problem solved linear programming algorithm produces vertex solution 
variations simplex method customized solve mcnf problem 
mature implementations algorithms widely solve large scale problems 
combinatorial nature simplex method variants results rapid growth number iterations problem dimensions grow 
instances vertices require simplex iterations 
furthermore primal degeneracy certain classes network flow problems causing simplex iterations degenerate 
main motivation study practice number iterations interior point algorithms linear programming appears grow slowly problem size 
direct comparisons interior point algorithms simplex method conclude problem size increases advantage increasingly tilts interior point methods :10.1.1.48.8163
may conjecture large problems interior point algorithms outperform simplex methods network flow problems despite implemented double precision arithmetic network simplex codes implemented integer arithmetic 
furthermore interior point methods appear affected degeneracy simplex method 
dual affine scaling das algorithm interior point methods shown competitive alternative simplex method 
adler karmarkar resende veiga described implementation dual affine scaling algorithm compared implementation simplex code minos :10.1.1.48.8163
data structures programming techniques implementation described 
theta matrix dimensional vectors dimensional vector 
dual affine scaling algorithm solves linear network interior point algorithm program minimize subject ax indirectly solving dual maximize subject dimensional vector slacks dimensional vector 
algorithm starts initial interior solution fy gamma obtains iterate ff ad gamma diag sn ff gamma 
iteration tentative primal solution ad gamma easy verify ax guaranteed feasible optimal dual solution assuming dual 
adler monteiro shown continuous version dual affine scaling algorithm assumption needed convergence iterates primal estimates 
bulk dual affine scaling algorithm related building updating matrix ad solving system linear equations ad determines ascent direction iteration algorithm 
adler consider approaches solve 
approach cholesky factorization 
matrix ad factored upper triangular matrix lower triangular matrix system ll solved applying forward substitution lz mauricio resende veiga back substitution approach considered satisfactory number nonzero elements factors small 
may case large fill fill difference number nonzeros ad ll factors problem large absence fill number nonzeros factors large 
constraint matrix mcnf problem sparse factorization ad produce considerable fill 
adler preconditioned conjugate gradient algorithm dense columns matrix consequently making dense 
approach dense columns matrix dropped resulting incomplete cholesky factors ad preconditioners 
karmarkar ramakrishnan conjugate gradient algorithm dual affine scaling algorithm solving large linear programs 
large linear programs number nonzero elements factors large regardless fill direct factorization methods slow 
compare implementation minos class randomly generated minimum cost network flow problems suggest interesting compare special purpose interior point implementation special purpose network simplex code 
mehrotra developed code solved numerous netlib problems 
studies concluded interior point methods competitive network simplex codes solving network flow problems 
show examples large scale network flow problems interior point method competitive mature network flow codes 
describe test special purpose implementation das algorithm mcnf problems built general purpose implementation das algorithm described adler 
outline 
section describe implementation preconditioned conjugate gradient algorithm including conjugate gradient stopping criteria 
section consider preconditioners mcnf problem 
describe diagonal preconditioner spanning tree preconditioner 
section consider early stopping dual affine scaling algorithm identifying primal optimal basis 
early stopping avoids problems experienced conjugate gradient algorithm iterate dual affine scaling algorithm close face 
section discuss way avoid numerical problems dropping dual constraints 
section consider dual degeneracy perturbation scheme allows primal iterates dual affine scaling algorithm converge vertex 
section consider parallel implementation algorithm 
section test implementation large randomly generated assignment problems compare results network simplex code relaxation method code relax 
concluding remarks section 
computing ascent direction 
implementation dual affine scaling algorithm network flow problems described preconditioned conjugate gradient algorithm solve direction finding system iteration 
differ slightly preconditioned conjugate gradient algorithm described adler 
preconditioned conjugate gradient algorithm consists solving gamma ad gamma network interior point algorithm procedure pcg ffl cg ad gamma gamma ffl cg ad ff ff gamma ff gamma fi fi od pcg fig 

preconditioned conjugate gradient algorithm positive definite matrix 
objective preconditioned matrix gamma ad ill conditioned ad improving convergence conjugate gradient algorithm 
hx yi inner product respect positive definite matrix 
gamma ad symmetric respect hx gamma ad yi hm gamma ad yi solve standard conjugate gradient algorithm inner products norms replaced hx yi resulting algorithm described pseudo code 
computationally intensive steps preconditioned conjugate gradient algorithm lines pseudo code 
lines correspond matrix vector multiplications systems linear equations 
lines computed lines computed conjugate gradient iteration 
multiplications carried form ad vector line line 
computational advantage decomposing matrix vector multiplication steps step step step az mauricio resende veiga forming ad explicitly requires jej multiplications jej additions 
performing matrix vector multiplication requires jv jej multiplications jv jej additions 
decomposing matrix vector multiplication requires jej multiplications jej additions 
enhancement computation carried parallel 
subject section 
preconditioned residual computed lines amounts solving system linear equations mz system easily solved 
usual stopping criterion conjugate gradient algorithm terminate norm residue kr ad gamma bk tolerance ffl cg implementation suggestion compute angle ad gamma cos ffl cos ffl cos small tolerance typically ffl cos gamma 
computation cos jb ad kbk delta ad complexity conjugate gradient iteration carried conjugate gradient iteration 
implementation computed iterations conjugate gradient algorithm 
stopping criterion effectively halts conjugate gradient algorithm direction hand 

preconditioners 
diagonal preconditioners preconditioners conjugate gradient algorithm 
simple compute lead jej multiplications steps preconditioned conjugate gradient algorithm 
practice effective mcnf problems initial iterations dual affine scaling algorithm 
instances mcnf problems dual affine scaling iterations progress tend lose effectiveness 
yeh exclusively diagonal preconditioning implementation dual affine scaling algorithm conjugate gradient assignment problem 
diagonal preconditioner implementation diag ad preconditioner computed jej additions multiplications 
preconditioned residue systems mz lines pseudo code solved jv divisions 
karmarkar ramakrishnan vaidya suggested spanning tree preconditioner network flow problems 
spanning tree preconditioner suggestions 
submatrix corresponding maximum weighted spanning tree appropriately defined weights 
spanning tree preconditioner network interior point algorithm diag stm tm edge indices spanning tree 
edge weight vector primal estimates ad gamma reciprocal estimates jej vector ones 
known absence degeneracy weight vectors identify primal basic sequence mcnf problem 
primal degenerate dual nondegenerate problems weights identify primal basic sequence 
reader see section random perturbation scheme avoid dual degeneracy reciprocal estimates weights tree preconditioner 
computing spanning tree find approximate maximum weighted spanning tree 
variant kruskal greedy algorithm carry approximate bucket sort place usual exact sorting weights 
min wmax values minimum maximum weights respectively 
partition min wmax interval number subintervals classify edge bucket 
approximate scheme time construct preconditioner small compared time required iterations conjugate gradient algorithm 
conjugate gradient iteration preconditioned residue system solved 
substituting spanning tree preconditioner matrix spanning tree permuted triangular form jv time 
consequently solved jv time forward substitution solving diagonal system back substitution entries back forward substitutions involve additions 
general mcnf problem corresponds incidence matrix directed spanning tree 
mauricio resende veiga das iterations ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi 
ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi 
cg iterations cg cpu time secs fig 

cg iterations cpu time theta assignment problem edges observed instances diagonal preconditioner effective initial iterations dual affine scaling algorithm dual affine scaling iterations progress tends lose effectiveness 
hand initial iterations dual affine scaling algorithm dual slacks provide little information consequently spanning tree preconditioner effective diagonal preconditioner 
dual affine scaling iterations progress spanning tree preconditioner increasingly effective 
implementation preconditioners 
diagonal monitor number iterations required conjugate gradient algorithm 
conjugate gradient iterations surpass specified number iterations code switches spanning tree preconditioner 
spanning tree preconditioner point code returns back diagonal preconditioning diagonal preconditioning may effective iterations dual affine scaling algorithm 
may case switch diagonal spanning tree preconditioner 
observed large instances solved 
illustrates switch theta assignment problem jej 
example change preconditioner triggered conjugate gradient algorithm exceeds iterations time 

identifying optimal primal basis 
practical experience direct factorization implementations interior point methods shown implementations numerically quite stable iterations condition number ad may large 
iterative methods conjugate gradient method sensitive ill conditioning matrix 
effect observed conjugate gradient algorithm substantial increase number iterations 
remedy problem allow dual affine scaling iterations proceed point ill conditioning occurs 
able network interior point algorithm identify primal optimal basis early dual affine scaling iterations iterations interior point algorithm optimal solution hand 
discussion assume exists unique optimal primal solution 
section consider case dual degenerate mcnf problems 
dual das algorithm converges optimal dual solution primal solution estimate converging unique optimal primal solution 
early detection optimal primal solution iteration attempt build primal basis includes edges nonzero flow primal optimal solution 
approach identify basis dual solution iteration 
compute approximate spanning tree reciprocals dual slacks 
reciprocals dual slacks alternate method estimating primal solution dual interior point methods 
spanning tree selected iteration solution linear system solved integer arithmetic jv operations 
primal feasible integer solution mcnf problem 
spanning tree correctly picked feasible basis additional dual affine scaling iterations required 
assuming initial interior dual feasible solution available optimality feasible flow confirmed simply checking dual gap current interior dual iterate tentative primal basic feasible solution gamma optimal flow 
somewhat better bound max max obtained line segment passes gamma way boundary polytope max ffd ff minf gammas gamma max optimal integer primal basic solution das halted 
initial interior dual solution readily available big scheme described :10.1.1.48.8163
optimality testing carried iterations das algorithm 
implementation compute primal flow dual convergence criterion jb gamma jb gamma satisfied 
sk cost subvector corresponding spanning tree basis guarantee sk gamma feasible dual solution 
implementation provide optimal primal dual pair optimal integer primal solution 
way proceed dual optimal vertex needed jump network simplex algorithm starting current optimal primal vertex 
yeh reported small instances tested network simplex mauricio resende veiga iterations needed find primal dual optimal pair post processing idea 
approach obtaining optimal vertex interior solution described 
linear programming problems unimodular coefficient matrix dual assumption fractional solution rounded optimal integer solution gamma optimal objective value 
algorithm generates feasible primal solutions bounds optimal objective value property early termination 
example primal dual variants interior point methods result 

dropping dual constraints 
dual affine scaling algorithm converges dual constraints optimal solution little influence computation search direction 
elements scaling diagonal matrix corresponding constraints converge zero coefficients matrix ad dominated inner products involving columns corresponding binding constraints optimal solution 
dropping columns computation coefficient matrix advantages 
firstly immediate reduction computational effort matrix vector multiplications carried conjugate gradient algorithm 
furthermore due primal degeneracy matrix ad ill conditioned algorithm converges 
circumstance rounding errors associated matrix vector multiplications involving small coefficients induces perturbation may slow convergence conjugate gradient algorithm 
section describe dual constraint dropping strategy implementation 
submatrices corresponding dropped columns submatrices corresponding remaining columns 
coefficient matrix system linear equations expressed ad dropping strategy coefficients small values coefficient matrix linear system approximated term right hand side 
primal degeneracy das algorithm converges approximate matrix may rank deficient 
conjugate gradient algorithm case singular matrices long system linear equation solution 
computation preconditioners requires special attention 
implementation computation diagonal preconditioner unchanged 
allow number columns dropped may result overdetermined system 
spanning tree preconditioner prevent columns corresponding spanning tree basis dropped 
iteration das algorithm implementation tries identify dual constraints optimal solution 
current dual slack iterate partition dual slacks sets small slacks large slacks 
follow strategy karmarkar ramakrishnan conjugate gradient implementation 
compute arithmetic mean elements scaling matrix jej jej jej network interior point algorithm das iterations dropped primal columns ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi 
fig 

primal column dropping theta assignment problem edges harmonic mean jej jej jej take geometric mean means oe jej delta jej constraint dropped ffl drop oe case spanning tree preconditioner edge current spanning tree basis 
ffl drop small tolerance typically ffl drop gamma 
scheme allows dropped constraints reconsidered iterations 
practice observed columns dropped iteration das dropped iteration 
illustrates column dropping theta assignment problem edges 
primal columns dropped iterations significant number columns excluded iteration 
just prior halting iteration columns dropped 

avoiding degeneracy 
dual degeneracy existence multiple primal optimal solutions mcnf problems 
affine scaling algorithms shown converge relative interior optimal face consequently primal estimates das algorithm converge solutions presence dual degeneracy 
integer nature mcnf formulation extreme point optimal solution 
classes mcnf problems assignment problem integer requirement relaxed 
early stopping scheme described section effective presence dual degeneracy 
das algorithm correctly identifies optimal face maximum spanning tree procedure potentially produce infeasible primal basis 
circumvent problem classical idea known ffl perturbation originally due charnes 
general purpose linear programs perturbing primal cost vector megiddo chandrasekaran show polynomial time algorithm find ffl ffl ffl perturbed problem nondegenerate 
mauricio resende veiga table effect degeneracy assignment problems dual degenerate dual nondegenerate problem iterations problem iterations jv jej opt total jv jej opt total specializing result mcnf problems theoretical value ffl small practice 
classical perturbation vector ffl ffl ffl ffl implementation perturb cost entry randomly ffi ffl ffi uniform random variables interval ffl small real parameter 
implementation ffl jv gamma ju jej jg 
heuristic may lead sub optimal solutions occur experiments described 
table shows iteration counts dual degenerate dual nondegenerate problems 
table identifies problems number vertices edges shows number iterations find optimal basic primal sequence opt total number iterations 
perturbation scheme successful avoiding dual degeneracy test problems considered 
total number iterations similar problem classes 
phenomena practice interior point methods sensitive degeneracy previously observed studies 
expected dual nondegenerate problems primal basis finding scheme section requires fewer iterations dual degenerate problems 
mehrotra described similar approach reports success scheme finding vertex solutions problems netlib suite 

parallel implementation 
computationally intensive steps conjugate gradient algorithm matrix vector multiplications steps 
matrix vector multiplications natural candidates parallel implementation 
implemented matrix vector multiplications parallel processor alliant fx parallel computer 
network interior point algorithm concur vector ia ia ja enddo enddo fig 

parallel matrix vector multiplication alliant fortran assume data structures similar described adler representing matrices 
arrays fia ja iat store matrix 
consider matrix vector multiplication carried fortran code 
compiler directives execute outer loop concurrently parallel vector mode execute inner loops vector processor concurrently 
figures illustrate effect parallel processors theta assignment problem edges 
gives total cpu times alliant fx seconds conjugate gradient algorithm cg dual affine scaling algorithm das 
matrix vector multiplication conjugate gradient algorithm computation implemented parallel 
consequently single processor conjugate gradient responsible total cpu time processors takes 
shows speedup attained das cg 
processors speedup observed interior point algorithm conjugate gradient algorithm sped factor approximately 
computational results 
section experimental results implementation das algorithm bipartite uncapacitated mcnf problems described 
report tests randomly generated assignment problems right hand side constraints unit vector 
das code general purpose solver bipartite uncapacitated mcnf problems specific features tailored solution assignment problems 
furthermore little modification fitted handle general uncapacitated problems 
problems generated random network generator 
allows control dual degeneracy generation process provides priori optimal value objective function 
briefly describe section 
compare conjugate gradient implementation das network simplex code relaxation code relax larger assignment problems 
das code tailored handle assignment problems compare implementation auction algorithm variation relaxation method specific solution assignment problems 
computational results indicate auction algorithm substantially faster relax solution randomly generated assignment problems 
auction algorithm implemented massively parallel computer 
mauricio resende veiga processors total cpu time secs ffi ffi ffi ffi ffi ffi ffi ffi 
ffi ffi ffi ffi ffi ffi ffi ffi 
das cg fig 

cpu times parallel implementation theta assignment problem edges processors speedup ffi ffi ffi ffi ffi ffi ffi ffi 
ffi ffi ffi ffi ffi ffi ffi ffi 
cg das fig 

speedup parallel implementation theta assignment problem edges network interior point algorithm run codes problems having linear programming formulations constraints variables 
results described section 
runs carried alliant fx parallel vector computer 
configured parallel processors numerically intensive computations microprocessors intensive tasks mbytes main memory kbytes cache memory gbytes disk storage 
parallel processor vector registers capable operating double precision numbers simultaneously 
experiments code written fortran compiled alliant fortran compiler flags das 
special care taken vectorize code implement parallel matrix vector multiplication conjugate gradient algorithm 
times reported user times system call times 

random network generator 
generator random minimum cost network flow problems 
generators purpose generates problems known optimal solution offers control degree degeneracy optimality 
preliminary version intended replacement trying limitations provide user controls problem structure parameters 
currently generate uncapacitated mcnf problems including special sub classes bipartite graphs assignment transportation problems 
user control topology underlying graph setting numbers vertices edges sources sinks 
context sources sinks nodes zero degree zero degree respectively 
consistent information generates random spanning tree determines optimal basis mcnf problem 
optimal flow generated randomly distributing total flow supplied user way unit flow produced consumed source sink 
supplying appropriate values number vertices sources sinks total supply user generate common special sub classes minimum cost network flow problem assignment problem classical transportation problem 
generating remaining edges underlying graph generate cost coefficients basic edges sampled uniform distribution 
basic cost coefficients sampled uniform distribution range values supplied user 
lastly remaining edges sampled set possible edges way necessary number degenerate nondegenerate edges requested user satisfied 
nondegenerate edge cost coefficient sampled uniform distribution range values requested user intersected range values preserving dual feasibility 
number potential degenerate nondegenerate edges inconsistent user request resulting major limitation current version 

large assignment problems 
section report testing parallel implementation network das algorithm large dual nondegenerate assignment problems generated 
problems cost structure edges corresponding optimal primal basic variables cost uniformly distributed nonbasic variables costs uniformly distributed 
compare das code mature network optimization codes relax 
total problems generated ranging vertices edges 
interior point code run problems 
instances run codes 
mauricio resende veiga theta theta theta theta jv ave cpu time secs ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl pi pi pi pi pi pi pi pi pi pi pi 
ffi ffl pi das relax fig 

running times das relax instances generated dimension jv jej generated different random number generator seeds 
tables summarize runs 
tables abbreviations dnr na mean record applicable respectively 
table shows performance results interior point code 
problems identified names dimensions jv jej 
average number conjugate gradient iterations conjugate gradient cpu times interior point iteration total number das iterations number das iterations needed identify optimal primal basic sequence opt total das cpu time 
times exclude problem input codes 
table presents results network simplex code code relax 
instance table gives number simplex iterations total simplex cpu time cpu times code relax 
table summarizes runs grouping instances identical dimensions gives averages das iterations identify primal optimal basic sequence total das cpu time iterations cpu times relax cpu times 
cpu time ratios das relax das 
figures illustrate numerical results 
observations regarding results ffl exists trend relative performance interior point code codes 
problem size increases relative performance interior point code improves 
behavior similar observed comparisons interior point methods simplex method general linear programming 
network interior point algorithm table das runs problem conjugate gradient das iterations das name jv jej ave iters ave time total opt time dnr dnr dnr dnr dnr dnr dnr dnr dnr dnr mauricio resende veiga table relax runs problem relax name jv jej iterations time time na na na na na na na network interior point algorithm table summary runs problem das relax cpu ratios type average iters average average average average ntf xi xi jv jej opt total time iters time time das das dnr na dnr na dnr na dnr na dnr na dnr dnr na ffl problems vertices interior point code faster faster relax 
problems vertices das factor solution time relax 
ffl instances interior point code optimal integer solution 
failed find optimal solution iterations problem 
ffl matrix vector multiplication conjugate gradient algorithm das code implemented parallel 
codes implemented parallel 
fair say steepest edge variant simplex method may benefit parallel architecture 
removing parallel matrix vector multiplication slow interior point code factor 
affect main observation section relative performance das code improves problem size changing break point 
processors break point appears vicinity vertices vertices relax 
ffl problems interior point algorithm spends average iterations prove optimality finding feasible basic primal sequence 
instances feasible sequence turned optimal 
suggests may potential jumping network simplex algorithm feasible sequence initial solution suggested yeh 
ffl diagonal preconditioner performs quiet problems tested 
fact largest problems tested need activate tree preconditioner 
commonly accepted criterion goodness preconditioner result conjugate gradient algorithm iterations solve theta linear system 
instances average number conjugate gradient iterations threshold 
example theta system solved das iteration problem took average conjugate gradient iterations 
ffl number iterations network simplex algorithm grows quickly mauricio resende veiga theta theta theta theta jv ave iters ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl 
das fig 

das iterations problem size 
example class theta problems simplex method needed average iterations find optimal solution 
factorization needed network simplex method computation carried integer arithmetic problems grow special characteristics network simplex method sufficient offset large number simplex iterations 
hand number affine scaling iterations grows slowly problem size see 
furthermore number conjugate gradient iterations appears level low value 
consequently ratio cpu times das code increases problem size spite fact interior point code carries computation double precision arithmetic 
determining factor respect performance interior point implementation matrix vector multiplication 
efficiently implemented parallel believe performance enhancements expected das multiprocessor computer architectures 
ffl shows problems fewer vertices code relax fastest followed das 
problems increase size ranking longer valid 
ffl feasible dual interior solution das single iteration problems tested 
ffl problems optimal primal basic sequence time spanning tree built 

concluding remarks 
described implementation dual affine scaling algorithm linear programming solve bipartite uncapacitated minimum cost network flow problems 
excessive computational demand direct factorization interior point methods previously thought network interior point algorithm competitive methods solving problems class 
implementation preconditioned conjugate gradient algorithm compute ascent direction 
efficient direct factorization conjugate gradient algorithm depends heavily matrix vector multiplication implemented parallel 
implement parallel conjugate gradient observe speed interior point code processor parallel computer 
limit experimental study special class minimum cost network flow problems assignment problems 
problems class performed extensive computational experiments concluding problem sizes increase interior point method relative performance commonly mcnf algorithms improves 
largest problems tested code competitive network simplex code relaxation method code relax 
observed trend continues larger problems expect interior point method method choice solving large scale network flow problems 
test code general minimum cost network flow problems implement lower upper bounds flow variables data structures handle directed edges general networks 
acknowledgment 
authors resende acknowledges insightful discussions karmarkar ramakrishnan vaidya 
authors acknowledge anonymous referee suggestions greatly improved 
adler karmarkar resende veiga implementation karmarkar algorithm linear programming mathematical programming pp :10.1.1.48.8163

data structures programming techniques implementation karmarkar algorithm orsa journal computing pp 

adler monteiro limiting behaviour affine scaling continuous trajectories linear programming problems mathematical programming pp 

mehrotra computational comparison network simplex method affine scaling method tech 
report department industrial engineering management sciences northwestern university evanston il september 
aronson barr loh zaki projective transformation algorithm karmarkar computational experiment assignment problems tech 
report department operations research southern methodist university dallas tx august 
bertsekas new algorithm assignment problem mathematical programming pp 

auction algorithm assignment network problems tutorial interfaces pp 

bertsekas tseng relaxation methods minimum cost ordinary generalized network flow problems operations research pp 

hill empirical evaluation algorithms military airlift applications operations research pp 

charnes optimality degeneracy linear programming econometrica pp 

dantzig maximization linear function variables subject linear inequalities activity analysis production allocation ed john wiley sons pp 

iterative solution problems linear quadratic programming soviet mathematics doklady pp 

mauricio resende veiga gay electronic mail distribution linear programming test problems mathematical programming committee algorithms newsletter pp 

golub van loan matrix computations johns hopkins university press baltimore md 
convergence large step primal affine scaling algorithm primal nondegenerate linear programs tech 
report es department systems engineering computer science federal university rio de janeiro rio de janeiro brazil 
stiefel methods conjugate gradients solving linear systems journal research national bureau standards section pp 

karmarkar ramakrishnan implementation computational results karmarkar algorithm linear programming iterative method computing projections tech 
report bell laboratories murray hill nj 
karmarkar ramakrishnan private communication 
algorithms network programming john wiley sons new york ny 
napier stutz program generating large scale capacitated assignment transportation minimum cost flow network problems management science pp 

kruskal jr shortest spanning subtree graph traveling salesman problem proc 
amer 
math 
soc pp 

megiddo chandrasekaran ffl perturbation method avoiding degeneracy tech 
report ibm almaden research center san jose ca 
mehrotra implementation affine scaling methods approximate solution system linear equations preconditioned conjugate gradient methods tech 
report department industrial engineering management sciences northwestern university evanston il august 
finding vertex solution interior point methods tech 
report department industrial engineering management sciences northwestern university evanston il january 
van der vorst iterative method linear equation systems coefficient matrix symmetric matrix mathematics computation pp 

mizuno orlin determination optimal vertices feasible solutions unimodular linear programming tech 
report department prediction control institute statistical mathematics tokyo japan 
monma morton computational experimental dual affine variant karmarkar method linear programming operations research letters pp 

murtagh saunders minos user guide tech 
report sol systems optimization laboratory department operations research stanford university stanford ca 
todd effects degeneracy unbounded variables variants karmarkar linear programming algorithm large scale numerical optimization coleman eds siam pp 

todd extension karmarkar algorithm linear programming dual variables algorithmica pp 

tsuchiya global convergence affine scaling methods degenerate linear programming problems tech 
report institute statistical mathematics tokyo january 
vaidya solving linear equations symmetric diagonally dominant matrices constructing preconditioners tech 
report department computer science university illinois urbana champaign urbana il 
wein zenios massively parallel solutions assignment problem tech 
report department decision sciences wharton school university pennsylvania philadelphia pa 

yeh reduced dual affine scaling algorithm solving assignment transportation problems phd thesis columbia university new york ny 
