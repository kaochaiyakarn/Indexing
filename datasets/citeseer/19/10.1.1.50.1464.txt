natural language engineering fl cambridge university press hierarchical dirichlet language model david mackay cavendish laboratory cambridge cb united kingdom mackay cam ac uk linda department computer science university toronto canada peto cs toronto edu discuss hierarchical probabilistic model predictions similar popular language modelling procedure known smoothing 
number interesting differences smoothing emerge 
insights gained probabilistic view problem point new directions language modelling 
ideas applicable problems modelling speech dna protein sequences molecular biology 
new algorithm compared smoothing word corpus 
methods prove equally accurate hierarchical model fewer computational resources 
contents bigram language model smoothing rational predictive procedure bayesian explicit model dirichlet priors inferences likelihood function prior 
convenient family priors dirichlet distributions definition hierarchical model hd inference prediction hierarchical dirichlet model address box cambridge ontario canada 
mackay peto inferring dirichlet hyperparameters dice factory evidence ffm inferring ffm approximations ff comments far application small corpus discussion generalizations relationship previous empirical bayes approaches gamma function digamma function speech recognition automatic translation depend language model assigns probabilities word sequences 
automatic translation system implemented ibm crude trigram model language impressive results brown 
similar language models speech recognition systems bahl jelinek mercer 
trigram models implemented particular involving smoothing predictions predictions better determined bigram models smoothing coefficients determined deleted interpolation jelinek mercer bahl 
generally language model employs similar procedure known backing katz 
text compression similar prediction task character sequences predicted adaptively 
text compression smoothing technique known blending combine predictions obtained contexts different orders bell 
aim reverse engineer underlying model gives probabilistic meaning smoothing allowing better understood objectively tuned sensibly modified 
objective create rival language model demonstrate bayesian approach language modelling show feasible 
simplicity pretend language model simply bigram model key issues addressed studying smoothing bigram statistics 
assumes bigram model markov process appropriate language model discusses optimal inference subject assumption 
bigram language model smoothing develop predictive model language string words wt observed marginal conditional frequencies observed 
define marginal count number times word occurs conditional count ijj number times word immediately followed word 
ignoring option grouping words common root hierarchical dirichlet language model complications central concept smoothing 
counts estimators marginal probability word conditional probability word word ff ff ijj ijj fi fi initial counts ff fi commonly set bell 
subscripts run total number distinct words language 
initial counts set obtain maximum likelihood estimators ijj ijj assign zero frequency words word pairs occur data 
practical aim language modelling predict word gamma available information data 
prediction described predictive probability jw gamma 
inclined observed conditional frequency jw gamma estimator predictive probability statistics adequate 
typically especially case trigram modelling conditional frequency estimator large variance possible ij small fraction observed data 
adopted jw gamma fw gamma jw gamma noisy bigram statistics smoothed better determined model predictions 
cross validation procedure called deleted interpolation set jelinek mercer bahl 
involves dividing data number blocks computing predictions block blocks training data adjusting optimize predictive performance 
better predictions obtained contexts similar values grouped separate group determined deleted interpolation 
text compression blending combines predictions different models manner similar equation 
parameters equivalent adapted fixed priori choice escape mechanism 
bell 
theoretical justification choosing particular escape mechanism 
agree possible language models making priori assumptions argue possible hierarchical model effectively determine smoothing parameters posteriori data 
rational predictive procedure bayesian smoothing procedure sounds sensible slightly ad hoc 
rational inference mapped probabilities cox aim discover implicit probabilistic model procedure related 
smoothing formula deleted interpolation originally conceived way combining predictions different models 
define single hierarchical model non trivial dirichlet prior gives predictive distributions similar including adaptive expressions mackay peto weighting coefficients equivalent various interesting differences emerge highlighting problems equation 
explicit model dirichlet priors heart bigram model conditional distribution ijw gamma described gamma independent parameters number words language possible conditioning terms right hand side probability distribution gamma independent parameters specified 
parameters denoted ijw gamma ijj theta transition probability matrix 
single row probability vector transitions state denoted jj alternative ways parameterizing model defined example marginal word probabilities joint probabilities gamma 
conditional probability parameterization chosen natural representation markov process marginal distribution independent deterministic function conditional probability matrix principal eigenvector 
parameters perfectly known uncertainty values represented probability distribution possible qs 
inferences model specification model parameters way probability data depends parameters prior probability distribution parameters 
model inferences interested making 
inferences mechanically rules probability theory infer parameters data 
bayes theorem gives probability parameters data terms likelihood function prior distribution qjh 
qjd qjh djh normalizing constant integrating numerator djh qjh dimensionality predict word context 
obtain probability gamma data sum rule probability ajc ajb bjc db marginalize unknown hierarchical dirichlet language model parameters jw gamma jw gamma qjd jw gamma qjd distribution inside integral qjd depends likelihood function prior shown equation 
likelihood function likelihood function written immediately independent assumptions define rest model 
simplifying assumption word data set priori predicted model 
probability string words probability second word times probability third word second forth jw gamma rewrite product counting variable ijj appears product 
conditional count ijj ijj ijj assumed bigram model conditional counts ijj contain relevant information data convey prior 
having defined parameterization model question remains inferences fully defined prior 
particular examines question prior qjh give predictive distributions smoothed form 
convenient family priors dirichlet distributions dirichlet distribution antoniak probability vector components parameterized measure vector coefficients write ffm normalized measure components ff positive scalar ffm ffm gamma ffi gamma dirichlet function ffi dirac delta function simply restricts distribution simplex normalized 
normalizing constant mackay peto fig 

zipf plots random samples dirichlet distributions various values ff 
ff samples standard gamma distribution generated shape parameter ff normalized give sample dirichlet distribution 
zipf plot shows probabilities ranked magnitude versus rank 
dirichlet distribution ffm gamma ffm gamma ff vector mean probability distribution dirichlet role ff characterized ways 
parameter ff measures sharpness distribution measures different expect typical samples distribution mean large value ff produces distribution sharply peaked effect ff visualized drawing typical sample distribution dirichlet set uniform vector making zipf plot ranked plot values components traditional plot vertical axis rank horizontal axis logarithmic scales power law relationships appear straight lines 
shows plots single sample ensembles ff 
large ff plot shallow components having similar values 
small ff typically component receives overwhelming share probability probability remains shared components component receives similarly large share 
limit ff goes zero plot tends increasingly steep power law 
second characterize role ff terms predictive distribution results observe samples obtain counts possible outcomes 
posterior probability conveniently hierarchical dirichlet language model dirichlet distribution ffm fjp ffm gamma ffi gamma ffm ffm gamma ffi gamma ffm dirichlet ffm predictive distribution data ffm dirichlet ffm ffm ffm notice term ffm appears effective initial count bin value ff defines number samples required order data dominate prior subsequent predictions 
ff ae ffm ff ffm 
note equations evidence ffm ffm ffm gamma ffm gamma ff gamma ff gamma ffm important role evidence known marginalized likelihood clear shortly 
additional useful formulae approximations appendix definition hierarchical model hd define prior hierarchical model denote hd dirichlet 
called hierarchical model containing unknown parameters place probability distribution data contains unknown hyperparameters define probability distribution parameters order obtain predictions similar smoothing equation assign coupled prior parameters prior learning probability vector context gives information probability vectors contexts 
introduce unknown measure words ffm define separable prior ffm vectors jj hd dirichlet jj produce dependence vectors jj putting uninformative prior ffm measure ffm precise flat prior broad mackay peto gamma prior ff 
prior defined hierarchical model dirichlet jj ffm ffm hierarchical model effectively find data measure show section 
additional prior knowledge language expect specific structure measure define informative prior ffm improve model predictive performance 
aim simply demonstrate minimal data driven bayesian model emphasis getting information data adding detailed human knowledge 
hierarchical model described puts qualitative prior parameters qualitative form dirichlet distribution specified specifying quantitative values hyperparameters ffm effectively turned quantitative prior consulting data 
general approach called empirical bayes 
method distinguished empirical bayes prescriptions bayesian inference control hyperparameters motivate procedure approximation ideal predictive bayesian approach 
inference prediction hierarchical dirichlet model convenient distinguish levels inference 
interested plausible values level parameters fq ijj level hyperparameters ffm 
results section 
level inference level assume know ff 
easy infer posterior distribution get predictive distribution 
bayes theorem posterior distribution qjd ffm hd hd hd hd distribution separable product contexts prior hd likelihood hd separable 
qjd ffm hd jj jd ffm hd posterior distribution conditional probability vector simply dirichlet distribution jj jd ffm hd ijj ffm gamma ijj ffi ijj gamma dirichlet jj jf ffm hierarchical dirichlet language model posterior prediction ijj ffm hd ijj ffm jj ffm notice precisely role marginal statistics equation 
explicit predictive distribution written ijj ffm hd gamma ijj ijj ijj ff ff note contrast equation quantity constant 
varies inversely frequency context practitioners deleted interpolation mentioned useful divide contexts different groups frequency separate group 
optimized deleted interpolation 
simply turning handle bayesian inference produced smoothing prescription anticipate eliminates need group contexts frequency 
appropriate variation automatically 
new idea blending method text compression bell uses variation level inference second level inference infer hyperparameters data 
posterior distribution ffm bayes theorem hd hd data dependent term hd normalizing constant level inference 
call evidence ffm 
proceed finding maximum ffm mp posterior distribution hd 
ideal bayesian method put proper prior hyperparameters marginalize making predictions ijj hd hd ijj ffm hd ffm expect posterior distribution hd sharply peaked ffm effectively delta function relative ijj ffm hd may approximate ijj hd ijj ffm mp hd marginalizing hyperparameters optimize optimization computationally convenient gives predictive distributions indistinguishable true predictive distribution mackay mackay peto 
assuming noninformative prior posterior probability maximum ffm mp maximizing evidence hd 
accuracy approximation specific case correct marginalization hyperparameters performed example monte carlo methods see example neal neal west 
note passing mode posterior probability distribution fundamental status bayesian inference location changed arbitrarily non linear reparameterization 
maximum evidence hand invariant reparameterization 
question mp turn equal marginal statistics bayesian procedure reproduce predictions smoothing 
answer optimal measure different 
discussed toy example persuade reader equation unsatisfactory 
mathematics bayesian optimization ffm worked detail 
example data set equation evidently unsatisfactory 
imagine see language see see frequently occurring see see second word see follows word high probability see 
marginal statistics see going dominated see words see equal frequency see 
data set conditional probability word novel context occurs 
particular probabilities words see dirichlet model assign probabilities mp smoothing formula assign probabilities proportional smoothing formula predictions come equal see occurred equally times far 
intuitively reasonable 
evidently relatively high probability context see high frequency high probability 
intuitively greater 
probability word relate raw frequency number contexts occurred 
see shortly mp exactly 
emphasized failure smoothing formula inadequacy bigram model markov process easily capture data set 
text compression method known update exclusion bell avoids problem described 
inferring dirichlet hyperparameters dice factory analogy may useful describe inferences 
imagine factory produces biased sided dice 
model probability vector single die coming dirichlet prior unknown hyperparameters ffm characterize factory 
data outcomes rolls dice hierarchical dirichlet language model labeled die rolled number times told counts outcomes ijj give imperfect information parameters fq ijj measurements task infer hyperparameters ffm factory order better predictions rolls individual dice 
problem identical language modelling problem number dice number classes equal number words imagine language generated dice rolling procedure outcome roll determines die rolled time 
inference problems genome modelling example related inference models dice factories 
evidence ffm posterior probability ffm proportional jj obtain equation gamma ijj ffm gamma ff gamma ff gamma ffm terms ffm find probable ffm differentiate digamma functions defined psi log gamma 
motivation evaluating gradient optimization continuous function variables easiest gradient function calculated 
log theta psi ijj gamma psi psi gamma psi gradient may fed optimization program find mp conjugate gradients algorithm press example easily finds optimum 
obtain insight derive explicit optimization algorithm making approximations 
inferring ffm approximations ff assume ff derive algorithm specialized parameter regimes expected language modelling 
expect ff greater ff corresponds rough number data points needed overwhelm dirichlet prior 
times expect need see context learnt principal properties seen context times intuitively expect prior knowledge high frequency word example important 
seen context tens hundreds times expect observed counts differ significantly default distribution 
preliminary experiments find probable ff ranged 
sum typical size vocabulary ffm expected 
mackay peto relationship psi psi combine fourth terms equation psi ijj gamma psi ijj gamma ijj gamma delta delta delta number terms sum ijj assuming smaller approximate sum ijj ijj gamma gamma ijj gamma approximating terms psi ff gamma psi ff equation obtain prescription maximum evidence hyperparameters mp nf number contexts ijj max largest nf 
denote number entries row ijj non zero compute quantities max fi gamma max fi gamma define ff log ff ff ff ff optimal hyperparameters satisfy implicit equation mp ff mp gamma ff mp gamma defines dimensional problem find ff satisfy ff 
optimal ff bracketing procedure reestimation procedure alternately set ff set ff algorithm section 
comments far ffl predictive algorithm similar smoothing derived fully probabilistic model 
ffl smoothing vector marginal distribution traditional language model 
see numerator equation directly related number contexts word occurred satisfying desideratum raised toy example previous section 
ffl weight smoothing vector scales automatically number counts 
need separate words separate categories depending raw frequency 
hierarchical dirichlet language model ffl framework involve cross validation data devoted aspect modelling process 
ffl perspective reveals implicit model underlying smoothing conditional probability vectors matrix modelled coming single dirichlet distribution 
distribution characterized mean probability vector single scalar measure spread mean ff 
plausible bigram model assumed complex distribution give better model 
example discussed section believe contexts come equivalence classes types motivate mixture model vectors application small corpus conducted experiment compare deleted interpolation new method empirically peto 
algorithm construct alternative model training corpus 
compared predictive accuracy algorithms evaluating perplexity test data competing models better model smaller perplexity 
perplexity defined cross entropy unknown true model assumed model case distributions alternatives log bigram models large test corpus perplexity test corpus approximated perplexity jw gamma gamma number words corpus see brown 

training test corpora taken english portion gale church sentence aligned version canadian hansard proceedings canadian parliament 
text separated sentences stripped titles formatting codes speaker identifiers 
removed sentence numbers added sentence sentence markers 
keeping common practice experiments type split punctuation suffixes words followed making separate tokens 
order reduce total number types vocabulary replaced number special token 
resulting sentences distributed blocks megabytes consecutive sentences going different blocks 
interleaving sentences performs important function data significant differences token frequencies result different portions corpus different topics discussed 
blocks training data words test data extracted remaining blocks 
prepared different test samples test data 
algorithms compared assign probabilities bigrams composed tokens mackay peto algorithm deleted interpolation dirichlet sample table 
perplexities test data samples different models 
number tokens sample 
appear training data way dealing previously unseen tokens chose address important zero frequency problem study 
removed sentences contained token occur training data 
left sentences tokens sample 
recognizing hansard contains conventional phrases sentences skew results experiment removed sample sentences duplicated test data training data 
left sentences tokens sample 
test sample large approximation perplexity equation hold pseudo randomly chose half sentences sample sample sentences tokens 
algorithms different numbers parameters optimized 
deleted interpolation method number chosen subjectively 
ran deleted interpolation method judge effect choice 
hierarchical model hyperparameter type training data vocabulary 
experiment conducted follows 
raw frequencies relative frequencies tokens bigrams obtained training data 
probable values parameters model solved iteratively 
dirichlet model meant solving simultaneous equations equation obtain mp smoothing method separate frequencies calculated block obtained deleted interpolation jelinek mercer 
optimization halted average parameter model converged decimal places 
optimized parameter values model compute predictive probabilities ijj bigram test data 
perplexity test data samples evaluated models results compared 
perplexity test sample model table 
samples perplexities deleted interpolation model dirichlet model nearly 
sample deleted interpolation models having different numbers hierarchical dirichlet language model tested 
effect altering number small 
values decreased frequency roughly expected equation 
perplexity results smaller sample close corresponding results sample 
suggests sample large provide meaningful comparison models 
fact perplexity results sample lower sample probably reflects high degree regularity extra conventional data small increase test data size 
regard resource new algorithm advantage 
number iterations required algorithm converge comparable 
single iteration dirichlet model requires time linear size vocabulary iteration deleted interpolation requires time linear size training corpus 
larger training corpus significant advantage dirichlet model 
deleted interpolation requires memory keeps separate count frequency data block training corpus 
implementation blocks 
direct comparison backing algorithm katz results indicate backing indistinguishable performance deleted interpolation similar bigram modelling task 
discussion exercise creating bayesian version smoothing procedure benefits 
dirichlet model identical smoothing differences intuitively reasonable 
dirichlet model away crossvalidation full data requiring fewer computational resources 
distinguish general bayesian method particular hierarchical bayesian model discussed computational approximations implement 
view algorithm bayesian answer language modelling claim particular algorithm necessarily superior deleted interpolation application 
possible bayesian language models studied virtually simplest possible 
discuss possible models 
generalizations language modelling viewed modelling set probability vectors jj drawn coupled density simplex simplex space probability vectors satisfying probability vector jj context model context simply previous word density simplex single dirichlet distribution parameterized ffm 
mackay peto simplest modifications model change functional form density simplex change definition context 
alternative density probabilities dirichlet distribution entropic prior skilling gull exp ff log ffi gamma entropic prior dirichlet prior characterizes language single mean spread distribution conditional probabilities jj contexts ibm maximum entropy language modelling della pietra personal communication interpreted terms entropic prior 
interpretation obtain bayesian prescription ff done dirichlet model 
interesting model assert different types context contexts type conditional probabilities similar 
know priori type context model mixture model 
mixture model hm defines density weighted combination independently parameterized simple distributions mixture component dirichlet entropic distribution 
various algorithms implement mixture models monte carlo methods neal gaussian approximations hanson 
mixture models applied modelling amino acid probabilities mackay 
alternatively model define context words type context defined word 
coupled prior context hyperparameters model give predictions similar smoothed trigram language model 
mixture model able capture clustered structure hierarchical trigram model potential advantage discover relationships contexts example happens case word important word characterizing jj flexible mixture model capture structure data 
believe type context naturally described componential structure hinton personal communication see williams hinton 
imagine example context context verb 
traditional mixture model mixture components capture sources variation general need number mixture components exponential dimensionality context space believe number parameters needed describe probability distribution ought linear number dimensions 
motivates development componential models type latent variable model type context represented continuous discrete dimensions 
componential model described applied modelling amino acid hierarchical dirichlet language model probabilities mackay 
generalized modelling joint distributions multiple amino acids mackay mackay 
relationship previous empirical bayes approaches approach similar spirit advocated described nadas 
empirical bayes approach interprets smoothing formula terms prior hyperparameters determined data 
contrast nadas points chooses estimators arbitrary way fully bayesian approach choices mechanistic inferences 
weakness nadas prior considered technically inappropriate prior neglects normalization probability vectors jj technique smoothing modelling classification trees literature contains similar empirical bayes approach buntine 
approach compromised invocation ad hoc estimators derivation inferences 
estimator fact probable objective procedure setting ff 
fully bayesian approach hyperparameter ff west monte carlo algorithm gibbs sampling hyperparameter 
advantages fully bayesian attitude data modelling firstly forced assumptions explicit secondly model defined inferences predictions mechanically defined rules probability theory 
peter brown radford neal geoff hinton phil woodland david robinson martin steve gull john bridle graeme mitchison helpful discussions isaac newton institute hospitality 
bill gale radford neal peter brown helpful discussions 
gamma function digamma function gamma function defined gamma du gamma gammau 
general gamma gamma integer arguments gamma 
digamma function defined psi dx log gamma 
large practical purposes approximations useful log gamma gamma gamma delta log gamma log psi dx log gamma log gamma mackay peto small practical purposes log gamma log gamma fl psi gamma gamma fl fl euler constant 
digamma function satisfies recurrence relation exactly psi psi formula general algorithm algorithm series expansions psi valid formula part series expansion gives approximation difference psi gamma psi accurate positive integers psi gamma psi log gamma approximation useful gradient optimization dirichlet distributions mackay 
antoniak 
mixtures dirichlet processes applications nonparametric problems 
annals statistics 
bahl brown de souza mercer nahamoo 
fast algorithm deleted interpolation 
proc 
eurospeech genoa pp 

bahl jelinek mercer 
maximum likelihood approach continuous speech recognition 
ieee trans pami 
bell cleary witten 
text compression 
englewood cliffs prentice hall 
brown della pietra della pietra lai mercer 
estimate upper bound entropy english 
computational linguistics 
brown dellapietra dellapietra mercer 
mathematics statistical machine translation parameter estimation 
computational linguistics 
buntine 
learning classification trees 
statistics computing 
cox 
probability frequency reasonable expectation 
am 
physics 
gale church 
program aligning sentences bilingual corpora 
proceedings th annual meeting acl pp 

gull 
developments maximum entropy data analysis 
maximum entropy bayesian methods cambridge ed 
skilling pp 
dordrecht 
kluwer 
hierarchical dirichlet language model hanson stutz cheeseman 
bayesian classification correlation inheritance 
proceedings th international joint conference artificial intelligence sydney australia volume pp 

morgan kaufmann 
jelinek mercer 
interpolated estimation markov source parameters sparse data 
pattern recognition practice ed 
gelsema kanal pp 

north holland publishing 
katz 
estimation probabilities sparse data language model component speech recognizer 
ieee transactions acoustics speech signal processing 
mackay 
bayesian neural networks density networks 
nuclear instruments methods physics research section 
mackay 
density networks protein modelling 
maximum entropy bayesian methods cambridge ed 
skilling dordrecht 
kluwer 
mackay 
hyperparameters optimize integrate 
maximum entropy bayesian methods santa barbara ed 
dordrecht 
kluwer 
mackay models dice factories amino acid probability vectors 
preparation 
nadas 
estimation probabilities language model ibm speech recognition system 
ieee trans assp 
neal 
bayesian mixture modelling 
maximum entropy bayesian methods seattle ed 
smith erickson pp 
dordrecht 
kluwer 
neal 
probabilistic inference markov chain monte carlo methods 
technical report crg tr dept computer science university toronto 
peto 
comparison smoothing methods word bigram models 
technical report csri computer systems research institute university toronto 
press flannery teukolsky vetterling 
numerical recipes cambridge 
skilling 
classic maximum entropy 
maximum entropy bayesian methods cambridge ed 
skilling dordrecht 
kluwer 
west 
hyperparameter estimation dirichlet process mixture models 
working duke inst 
stats 
decision sciences 
williams hinton 
mean field networks learn discriminate temporally distorted strings 
connectionist models proceedings summer school ed 
touretzky elman sejnowski 
morgan kaufmann san mateo ca 
