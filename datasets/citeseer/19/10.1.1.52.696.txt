journal artificial intelligence research submitted published operations learning graphical models wray buntine wray kronos arc nasa gov riacs nasa ames research center mail moffett field ca usa multidisciplinary review empirical statistical learning graphical model perspective 
known examples graphical models include bayesian networks directed graphs representing markov chain undirected networks representing markov field 
graphical models extended model data analysis empirical learning notation plates 
graphical operations simplifying manipulating problem provided including decomposition differentiation manipulation probability models exponential family 
standard algorithm schemas learning reviewed graphical framework gibbs sampling expectation maximization algorithm 
operations schemas popular algorithms synthesized graphical specification 
includes versions linear regression techniques feed forward networks learning gaussian discrete bayesian networks data 
concludes sketching implications data analysis summarizing popular algorithms fall framework 
main original contributions decomposition techniques demonstration graphical models provide framework understanding developing complex learning algorithms 

probabilistic graphical model graph nodes represent variables arcs directed undirected represent dependencies variables 
define mathematical form joint conditional probability distribution variables 
graphical models come various forms bayesian networks represent causal probabilistic processes data flow diagrams represent deterministic computation influence diagrams represent decision processes undirected markov networks random fields represent correlation images hidden causes 
graphical models domains diagnosis probabilistic expert systems planning control dean wellman chan shachter dynamic systems time series dagum horvitz general data analysis gilks statistics whittaker 
shows task learning modeled graphical models 
metalevel graphical models suggested spiegelhalter lauritzen context learning probabilities bayesian networks 
graphical models provide representation decomposition complex problems 
associated set mathematics algorithms manipulation 
graphical models discussed graphical formalism associated algorithms mathematics implicitly included 
fact graphical formalism unnecessary technical development approach conveys important fl ai access foundation morgan kaufmann publishers 
rights reserved 
buntine structural information problem natural visual manner 
graphical operations manipulate underlying structure problem fine detail connecting functional distributional equations 
structuring process important way high level programming language leads higher productivity assembly language 
graphical model developed represent basic prediction done linear regression bayesian network expert system hidden markov model connectionist feed forward network buntine 
graphical model represent reason task learning parameters weights structure representations 
extension standard graphical model allows kind learning represented 
extension notion plate introduced spiegelhalter 
plates allow samples explicitly represented graphical model reasoned manipulated 
data analysis problems explicit way utility decision nodes decision analysis problems shachter 
develops framework basic computational techniques learning directly applied graphical models 
forms basis computational theory bayesian learning language graphical models 
computational theory mean approach shows wide variety learning algorithms created graphical specifications simple algorithmic criteria 
basic computational techniques probabilistic bayesian inference computational theory learning widely reviewed tanner press kass raftery neal bretthorst 
include various exact methods markov chain monte carlo methods gibbs sampling expectation maximization em algorithm laplace approximation 
specialized computational techniques exist handling missing values little rubin making batch algorithm incremental adapting algorithm handle large samples 
creative combination techniques able address wide range data analysis problems 
provides blueprint software toolkit construct data analysis learning algorithms graphical specification 
conceptual architecture toolkit 
probability decision theory decompose problem computational prescription search optimization techniques fill prescription 
version toolkit exists gibbs sampling general computational scheme gilks 
list algorithms constructed form scheme impressive 
real gain scheme arise potential re implementation existing software understanding gained putting common language ability create novel hybrid algorithms ability tailor special purpose algorithms specific problems 
tutorial sense collects material different communities presents language graphical models 
introduces graphical models represent order inference learning 
second develops reviews number operations graphical models 
gives examples 
notion replicated node version developed independently 
adopted notation spiegelhalter colleagues uniformity 
learning graphical models optimizing search methods statistical decision methods basis basis linear gaussian glm software generator developing learning algorithms combinations basic operations 
main original contribution demonstration graphical models provide framework understanding developing complex learning algorithms 
detail covers topics 
graphical models graphical models ways 
graphical models representing inference tasks section reviews basics graphical models 
graphical models provide means representing patterns inference 
adapting graphical representations represent learning section discusses representation problem learning graphical models notion plate 
operations graphical models operations take graphical representation learning problem simplify perform important calculation required solve problem 
operations closed form solutions section covers classes learning problems closed form solutions learning known 
section adapts standard statistical methods graphical models 
basic operations operations graphs required able handle complex problems 
covered section including decomposition breaking learning problem independent components evaluating component 
differentiation computing derivative probability log probability respect variables graph 
differentiation decomposed operations local groups nodes graph popular neural networks 
approximate operations approximate algorithms follow naturally methods 
section reviews gibbs sampling deterministic cousin em algorithm 
buntine example algorithms closed form solutions learning form fast inner loop complex algorithms 
section illustrates graphical models help 
lists common algorithms derivation framework 
proofs lemmas theorems collected appendix 
graphical models section introduces graphical models 
brief tour necessary introducing operations learning 
graphical models offer unified qualitative quantitative framework representing reasoning probabilities independencies 
combine representation uncertain problems techniques performing inference 
flexible toolkits systems exist applying techniques srinivas breese andersen olesen jensen jensen cowell 
graphical models notion independence worth repeating 
definition independent bjc ajc bjc theory independence basic tool knowledge structuring developed dawid pearl 
graphical model equated set probability distributions satisfy implied constraints 
graphical models equivalent probability models corresponding sets satisfying probability distributions equivalent 
directed graphical models basic kind graphical model bayesian network called belief net popular artificial intelligence 
see charniak shachter heckerman pearl 
graphical representation markov chain 
bayesian network graphical model uses directed arcs exclusively form directed acyclic graph dag directed graph directed cycles 
adapted shachter heckerman shows simple bayesian network occupation climate age disease symptoms simplified medical problem simplified medical problem 
graphical model represents conditional decomposition joint probability see lauritzen dawid larsen details interpretations 
decomposition works follows full variable names learning graphical models abbreviated 
age occ clim dis occ clim conditioning context instance expert prior knowledge choice graphical model 
variable written conditioned parents parents set variables directed arc general form equation set variables jm equation interpretation bayesian network 
undirected graphical models popular form graphical model undirected graph called markov network pearl 
graphical model markov random field 
markov random fields statistics advent hammersley clifford theorem besag york 
variant theorem theorem 
markov random fields imaging spatial reasoning ripley geman geman besag various stochastic models neural networks hertz krogh palmer 
undirected graphs important simplify theory bayesian networks lauritzen 
shows simple theta image undirected model image 
model simple theta graphical model degree markov assumption current pixel directly influenced pixels positioned indicated undirected arcs variables node corresponding pixel set neighbors nodes directly connected undirected arc instance neighbors variable node denote neighbors 
general formula undirected graphs terms conditional probabilities corresponding equation bayesian network 
functional decomposition exist form maximal cliques 
maximal buntine cliques subgraphs fully connected strictly contained fully connected subgraphs 
sets theta cliques fp interpretation graph joint probability product functions maximal cliques 
functions defined constant 
formula follows conditionally independent non neighbors neighbors general form equation set variables theorem 
compare equation 
theorem undirected graph variables set set maximal cliques cliques ae distribution probability probability density strictly positive domain theta domain 
distribution independent gamma fxg gamma neighbors neighbors refers condition local markovian functional representation cliques functions 
general form theorem finite discrete domains called theorem geman besag 
equation interpretation markov network 
conditional probability models consider conditional probability occ clim simple medical problem equation 
conditional probability models disease vary values age occupation climate 
class probability trees breiman friedman olshen stone quinlan graphs rules rivest oliver kohavi feed forward networks representations devised express conditional models different ways 
statistics conditional distributions represented regression models generalized linear models mccullagh nelder 
models equation equation show joint distribution composed simpler components 
give global model variables problem 
conditional probability models contrast give model subset variables conditioned knowing values subset 
diagnosis concern may particular direction reasoning predicting disease patient details symptoms full joint model provides unnecessary detail 
full joint model may require extra parameters data learn 
learning graphical models supervised learning applications general view conditional models superior prior knowledge dictates full joint model appropriate 
distinction referred diagnostic versus discriminant approach classification dawid 
number ways explicitly representing conditional probability models 
joint distribution implicitly gives conditional distribution subset variables definition conditional probability 
instance model age occ clim dis symp definition conditional probability conditional model follows occ clim symp age occ clim dis symp dis age occ clim dis symp age occ clim dis symp conditional distributions represented single node labeled identify functional form node takes 
instance graphs follow labeled gaussian nodes linear nodes standard forms 
conditional models rule sets feed forward networks constructed special deterministic nodes 
instance shows model constructs 
case input variables rule rule gaussian unit logistic graphical conditional models conditional models represented shaded 
shading means values variables known 
shading indicates value known value unknown 
presumably predicted represents rule set 
nodes double ovals deterministic functions inputs contrast usual nodes probabilistic functions 
means value rule deterministic function notice implies values rule known 
conditional probability deterministic node required equation treated delta function 
corresponds statement ffi unit unit function specified 
bayesian network constructed entirely double ovals equivalent data flow graph inputs shaded 
analysis deterministic nodes bayesian networks generally influence diagrams considered shachter 
purposes deterministic nodes best treated intermediate variables removed problem 
method doing variable elimination lemma 
buntine logical conjunctive form rule expressed graph presumably formulas accompanying graph basic functional structure rule set exists 
node labeled functional type 
functional type node boolean variable function jx sigmoid logistic gamma maps real value probability binary variable function inverse logistic logit function generalized linear models mccullagh nelder sigmoid function feed forward neural networks 
uses deterministic node reproduce single unit connectionist feedforward network unit activation computed sigmoid function 
simple univariate gaussian normally distributed mean standard deviation oe 
node labeled italics indicate conditional type 
general level networks conditional 
shows conditional versions simple medical problem 
shading nodes ignored joint climate age disease symptoms occupation climate age disease symptoms equivalent conditional models medical problem bility age occ clim dis symp graphs age occ occ clim dis age occ clim occ clim dis nodes shaded means values known 
conditional distributions computed identical occ clim symp occ clim dis dis occ clim dis distinct graphs identical viewed conditional perspective conditional components model corresponding age occupation climate cancel conditional distribution formed 
symptoms node unknown variable disease parent arc age symptoms kept 
generally simple lemma applies derived directly equation 
learning graphical models lemma bayesian network nodes shaded representing conditional probability distribution node parents values bayesian network created deleting arcs represents equivalent probability model bayesian network mean instance graphs just discussed causal influential links variables age occupation climate effects irrelevant conditional model considered values known 
corresponding result holds undirected graphs follows directly theorem 
lemma undirected graph nodes shaded representing conditional probability distribution delete arc nodes common neighbors 
resultant graph represents equivalent probability model graph mixed graphical models undirected directed graphs mixed sequence 
mixed graphs called chain graphs wermuth lauritzen 
chain graphs precise understanding required 
simple chain graph 
case single disease node occupation climate age heart disease lung disease symptom symptom symptom expanded medical problem single symptom node expanded represent case possibly occurring diseases possibly occurring symptoms 
medical specialist may said lung disease heart disease influence may hidden common cause difficult tell cause 
causal model join disease nodes undirected arc represent direct influence 
likewise symptom nodes 
resultant joint distribution takes form age occ clim heart dis lung dis symp symp symp age occ clim heart dis lung occ clim symp symp symp dis lung dis buntine conditional probabilities take arbitrary form 
notice probabilities variable left side 
general chain graph consists chain undirected graphs connected directed arcs 
cycle graph directed arcs going opposite directions 
chain graphs interpreted bayesian networks defined components chain original variables 
goes follows definition subgraph variables chain components subsets maximal undirected connected subgraphs chain graph 
furthermore chain components denote nodes chain component variable chain components graph ordered consistently directed arcs dis lung symp symp cg 
informally chain graph variables chain components set interpreted decomposition corresponding decomposition bayesian networks equation jm parents parents gamma component probability form similar equation 
process graphs form having consider mathematics chain graphs device 
comment set nodes chain graph form clique fully connected subgraph identical children parents set nodes replaced single node representing cross product variables 
operation done get 
furthermore chain graphs occupation climate age heart disease lung disease symptom symptom symptom expanded medical problem bayesian networks 
case comment chain components bayesian network singleton sets individual variables graph 
furthermore chain components learning graphical models chain graphs decomposed chain directed undirected graphs 
example 
shows original chain graph 
shows decomposing chain graph directed undirected components bayesian network right showing 
having done decomposition components analyzed machinery directed undirected graphs 
interpretation graphs terms independence statements implied functional form joint probability combination previous forms equation theorem theorem interpretation conditional graphical models section 
learning graphical models simplified inference problem represented 
nodes var var var shaded 
represents value nodes inference task predict value remaining variable class 
graph matches called idiot bayes classifier duda hart langley iba thompson supervised learning speed simplicity 
probabilities network easily learned data input variables var var var class 
graph matches unsupervised learning problem class class data hidden 
unsupervised learning algorithm learns hidden classes cheeseman self kelly taylor freeman stutz mclachlan basford 
var var var class simple classification problem buntine implied joint variables read graph class var var var class var jclass var jclass var jclass bayesian classifier gets name derived applying bayes theorem joint get conditional formula var var class var jclass var jclass var jclass class class var jclass var jclass var jclass formula predict hidden class objects simple unsupervised learning framework 
formula corresponding formula general classifiers automatically exact methods inference bayesian networks 
consider simple model 
matching unsupervised learning problem model represented sample cases variables observed case var var var th case var var var corresponding hidden classes class class observed interest performing inference parameters needed specify hidden classes 
learning problem represented 
includes added features explicit representation model parameters oe representation sample repeated subgraphs 
parameter oe vector class probabilities var var var class var var var class var var var class 
learning simple classification gives proportions hidden classes parameters give variables distributed hidden class 
instance classes oe vector class probabilities prior probability case class oe var binary variable probabilities class case known class probability var true probability var false gamma yields equations class oe var learning graphical models unknown model parameters oe included graphical model explicitly represent unknown variables parameters learning problem 
bayesian learning useful time introduce basic terminology bayesian learning theory 
field 
introductions bretthorst press loredo bernardo smith cheeseman 
section reviews notions sample likelihood bayes factor important subsequent results 
unsupervised learning problem model hidden class particular graphical structure 
data assumed independently sampled parameters model oe 
order theory assumed model correct 
true distribution data assumed come model parameters 
practice hopefully model assumptions sufficiently close truth 
different sets model assumptions may tried 
typically true model parameters unknown may rough idea values 
models considered instance different kinds bayesian networks assumed just correct 
model selection model averaging methods deal 
bayesian classifier subjective probability placed model parameters form oe jm 
called prior probability 
bayesian statistics decision theory distinguished statistical approaches places initial probability distributions prior probability unknown model parameters 
model feed forward neural network prior probability needs placed network weights standard deviation error 
model linear regression gaussian error linear parameters standard deviation error 
prior probabilities active area research discussed introductions bayesian methods 
important component sample likelihood basis model assumptions set parameters oe says sample data 

model needs completely determine sample likelihood 
sample likelihood basis maximum likelihood principle hypothesis testing methods casella berger 
combines prior form posterior probability oe oe jm equation bayes theorem term derived prior sample likelihood integration sum difficult oe oe jm oe term called evidence model model likelihood basis bayesian model selection model averaging methods bayesian hypothesis testing buntine methods bayes factors smith spiegelhalter kass raftery 
bayes factor relative quantity compare model bayes factor kass raftery review large variety methods available computing estimating evidence model including numerical integration importance sampling laplace approximation 
implementation log bayes factor keep arithmetic reasonable bounds 
log evidence produce large numbers rounding errors floating point arithmetic scales order magnitude log bayes factor preferred quantity consider implementation 
evidence simpler mathematical analysis 
bayes factor bayesian equivalent likelihood ratio test orthodox statistics developed wilks 
see casella berger review 
evidence bayes factors fundamental bayesian methods 
case complex non parametric model statistical term loosely translates varied parameter model problem simple model fixed number parameters 
examples models decision trees neural networks bayesian networks 
instance suppose models proposed bayesian networks suggested domain expert 

multinomial variables var var gaussian variables model additional arc going discrete variable var real valued variable var var model model var var gaussian gaussian gaussian gaussian graphical models 
learning select 
parameters oe oe model parameterize probability distributions bayesian network parameters oe oe second 
task learn set parameters select bayesian network 
bayes factor gives comparative worth models 
simple example extends principle selecting single decision tree rule set bayesian network learning graphical models huge number available attributes domain 
case compare posterior probabilities models 
assuming truth falls model computed bayes theorem bayes factor generally multiple models exist holds bayes factor notice computation requires model prior evidence 
second form reduces computation relative quantity bayes factor ratio priors 
bayesian hypothesis testing corresponds checking bayes factor null hypothesis compared alternative hypothesis small large 
bayesian model building corresponds searching model high value usually involves comparing bayes factors model alternative models search 
making estimate new case estimate fm predictions individual models averaged model posteriors gamma 
general components calculation model priors evidence model bayes factors prediction new case model 
process model averaging happens general 
typical non parametric problem learn class probability trees data 
number class probability tree models super exponential number features 
learning bayesian networks data number bayesian networks best quadratic number features 
doing exhaustive search spaces doing full averaging implied equation computationally infeasible general 
may case models posterior probabilities models posteriors 
select single model representative set models chosen averaged identity general averaging process depicted gibbs sampler generate representative subset models high posterior 
kind computation done class probability trees representative sets trees heuristic branch bound algorithm buntine learning bayesian networks madigan raftery :10.1.1.52.1068
sampling scheme bayesian networks section 
buntine sample gibbs sampler new data sample new data new data sample averaging multiple bayesian networks plates representing learning graphical models current form graphical models allow convenient representation learning problem 
important points observed regarding graphical models improve suitability learning 
consider unsupervised learning system described section 
ffl unknown model parameters oe included graphical model explicitly represent variables problem model parameters 
including probabilistic model explicitly bayesian model constructed 
variable graphical model unknown model parameters defined prior probability 
ffl learning sample repeated set measured variables basic model appears duplicated times cases sample shown 
clearly awkward repetition occur homogeneous data modeled typical learning 
techniques handling repetition form major part 
ffl graph figures represents goal learning 
learning goal directed additional information needs included graph learned knowledge evaluated subsequent performance measured 
role decision theory modeled graphical form influence diagrams shachter 
discussed covered buntine 
ffl possible take graphical representation learning problem goal learning construct algorithm solve problem 
subsequent sections discuss techniques 
consider simplified version unsupervised problem 
fact simplest possible learning problem containing uncertainty goes follows biased coin learning graphical models heads beta heads beta heads heads tossing coin model plate unknown bias heads 
long run frequency getting heads coin fair toss 
coin tossed times time binary variable heads recorded 
graphical model 
heads nodes shaded values node 
node beta prior 
assumes distributed beta distribution parameters ff ff jff ff ff gamma gamma ff gamma beta ff ff beta standard beta function mathematical tables 
prior plotted 
beta prior instance uniform beta slightly favors values closer fairer coin 
equivalent beta prior ff priors graphical model notation plates 
repeated group case heads nodes replaced single node box 
box referred plate 
implies ffl enclosed subgraph duplicated times stack plates ffl enclosed variables indexed buntine ffl exterior interior links duplicated 
section shown bayesian network corresponding form joint probability variables bayesian network 
applies plates 
plate indicates product appear corresponding form 
probability equation read directly graph oe class var var var class var var var oe class joe var jclass var jclass var jclass class joe var jclass var jclass var jclass corresponding equation product notation oe class var var var oe class joe var jclass var jclass var jclass equations equivalent 
differences written form corresponds differences graphical form 
plate converted product joint probability ignoring plates written product added plate index variables inside 
disjoint plates disjoint products 
overlapping plates yield overlapping products 
corresponding transformation unsupervised learning problem 
notice var var var class simple unsupervised learning plate hidden class variable shaded 
corresponding transformation supervised learning problem classes corresponds idiot bayes classifier identical class variable shaded classes part training sample 
learning problems similarly modeled plates 
write graphical model full learning problem single case provided 
put box data part model pull model parameters instance weights network classification parameters ensure unshaded unknown 
add data set size bottom left corner 
notion plate formalized 
formalization included subsequent proofs 
learning graphical models definition chain graph plates variable set consists chain graph variables additional boxes called plates placed groups variables 
directed arcs cross plate boundaries plates overlapping 
plate integer bottom left corner indicating cardinality 
plate indexes variables inside values variable occurs subset plates 
denote set values indices corresponding plates 
cross product index sets plates containing graph plates expanded remove plates 
expanded form 
chain graph plates variables construct expanded graph follows ffl variable add node 
ffl undirected arc variables add undirected arc 
ffl directed arc variables add directed arc identical values index components plate 
parents indexed variables graph plates parents expanded graph 
parents parents graph plates interpreted product form 
product form chain graph plates chain components jm product form chain graph plates product plate jm expanded version graph 
testing independence chain graphs plates involves expanding plates 
cases simplified 

exact operations graphical models section introduces basic inference methods graphs plates exact inference methods graphs plates 
common machine learning algorithms explained section operations explained mathematical basis fast learning algorithms 
importance basic operations underestimated 
known learning algorithms explained sections 
buntine graphical model developed represent problem graph manipulated various exact approximate transformations simplify problem 
section reviews basic exact transformations available arc reversal node removal exact removal plates recursive arc reversal 
summary operations emphasizes computational aspects 
graphical model associated set definitions tables basic functions conditional probabilities implied graph operations effect graphical structure underlying mathematical specifications 
cases process making transformations constructive graphical specification learning problem converted algorithm 
generic approaches performing inference directed undirected networks plates 
approaches mentioned covered detail 
approach exact corresponds removing independent irrelevant information graph attempting optimize exact probabilistic computation finding reordering variables 
second approach performing inference approximate corresponds approximate algorithms gibbs sampling markov chain monte carlo methods hertz neal 
cases complexity approach inherently exponential number variables second efficient 
approaches combined cases appropriate reformulation problem dagum horvitz 
exact inference plates exact inference approach highly refined case variables discrete 
surprising available algorithms strong similarities shachter andersen szolovits major choice points involve ordering summation ordering selected dynamically statically 
special classes inference algorithms include cases model multivariate gaussian shachter whittaker corresponds specific diagnostic structure level believe networks level symptoms connected level diseases henrion 
subsection reviews simple exact transformations graphical models plates 
representative methods covered means optimal arc reversal arc removal 
important building blocks methods graphs plates 
sophisticated variations combinations algorithms exist literature including handling deterministic nodes shachter chain graphs undirected graphs 
arc reversal basic steps inference marginalize nuisance parameters condition new evidence 
may require evaluating probability variables different order 
arc reversal operator interchanges order nodes connected directed arc shachter 
operator corresponds bayes theorem instance automate derivation equation equation 
operator applies directed acyclic graphs chain graphs adjacent chain components 
learning graphical models parents parents parents parents arc reversal reversing nodes consider fragment graphical model left 
equation fragment bja ajb parents suppose nodes need reordered 
assume directed path length greater 
reversing arc create cycle forbidden bayesian network chain graph 
formula variable reordering applying bayes theorem equation 
aja bja corresponding graph right 
notice effect graph nodes share parents 
important point 
parents vice versa excepting parents parents fbg graph unchanged direction arc regardless probability tables formula associated graph need updated 
variables discrete full conditional probability tables maintained operation requires instantiating set fa bg parents parents ways exponential number variables 
arc node removal variables graph part model important goal data analysis 
called nuisance parameters 
unshaded node children outward going arcs action node utility node removed bayesian network 
corresponds leaving term product equation 
bja yja marginalized trivially yield bja buntine generally applies chain graphs chain component nodes unshaded children removed 
node children remove node graph arcs ignore factor full joint form 
consider th case nodes class var var var removed model affecting rest graph 
removal plates exact methods consider simple coins problem 
graph represents joint probability heads heads 
main question interest conditional probability data heads heads obtained repeated arc reversals heads heads data appears directed graph 
doing repeated series applications bayes theorem yields fully connected graph arcs 
corresponding formula posterior simplified lemma simple heads ff ff ff gamma gamma ff gamma beta ff ff number heads sequence gamma number tails 
worthwhile introductory exercise bayesian decision theory howard familiar students statistics 
compare equation 
important points notice result ffl effectively parameter update ff ff ff ff requiring search numerical optimization 
sequence tosses irrespective length ordering heads tails summed numbers 
summary statistics called sufficient statistics assuming model correct sufficient explain important data 
ffl corresponding graph simplified shown 
plate efficiently removed replaced sufficient statistics numbers irrespective size sample 
beta removing plate coin problem ffl posterior distribution simple form 
furthermore moments log log gamma distribution computed simple functions normalizing constant beta ff ff 
instance heads ff ff log log beta ff ff ff heads ff ff beta ff ff beta ff ff learning graphical models heads ff ff gamma beta ff ff beta ff ff gamma beta ff ff beta ff ff result somewhat obscure general property holding large class distributions allows averages calculated symbolic manipulation normalizing constant 
exponential family result generalizes larger class distributions referred exponential family casella berger degroot 
includes standard undergraduate distributions gaussians chi squared gamma complex distributions constructed simple components including class probability trees discrete input domains buntine simple discrete gaussian versions bayesian network whittaker linear regression gaussian error :10.1.1.52.1068
broad insignificant class distributions definition 
general form linear combination parameters data exponential 
definition space independent parameter space remains just changed 
domains independent conditional distribution xjy exponential family xjy exp functions integer 
normalization constant known partition function 
notice functional form equation similar functional form undirected graph equation holds cases markov random field 
previous coin tossing example coin tossing distribution binomial heads posterior distribution model parameters exponential family 
see notice rewrites original probabilities 
components explicit 
exp heads true log heads false log gamma heads ff ff beta ff ff exp ff gamma log ff gamma log gamma table appendix gives selection distributions functional form 
details textbooks probability distributions degroot bernardo smith 
simple graphical reinterpretation pitman koopman theorem statistics jeffreys degroot 
statistic fixed dimension independent sample size corresponding coin tossing example 
theorem says sample summarized statistics shown probability distribution xjy exponential family 
case sufficient statistic 
buntine generalized graph plate removal theorem recursive arc reversal 
consider model represented graphical model sample size 
domain domain domains independent domains components real valued finite discrete 
conditional distribution xjy positive derivatives exist respect real valued components plate removal operation applies samples xn sufficient statistics dimension independent conditional distribution exponential family equation 
case invertible function averages cases extends domains dependent jeffreys 
sufficient statistics distribution exponential family easily read functional form logarithm 
instance multivariate gaussian sufficient statistics normalizing constant sigma sigma det sigma exp sigma coin tossing generally holds sampling distribution binomial heads exponential family posterior distribution model parameters cast exponential family 
useful normalizing constant beta ff ff coin tossing example derivatives readily computed 
lemma conjugacy property 
context theorem assume distribution represented exponential family 
factor normalizing constant components second constant part independent 
assume prior takes form exp log learning graphical models dimensional parameter appropriate normalizing constant function 
posterior distribution xn represented equation parameters function trivial instance uniformly equal distribution equation referred conjugate distribution means mathematical form mirroring sample likelihood 
prior parameters looking update equations lemma thought corresponding sufficient statistics prior sample property useful analytic computational purposes 
posterior distribution assuming standard distributions property easily established 
table appendix gives standard conjugate prior distributions table table gives matching posteriors 
extensive summaries degroot bernardo smith 
parameters priors set standard priors box tiao bernardo smith elicited domain expert 
important consequences pitman koopman theorem recursive arc reversal go unnoticed 
comment discrete finite valued distribution xjy represented member exponential family 
holds positive finite discrete distribution represented extended case statement form xjy exp boolean functions set mutually exclusive exhaustive conditions 
indicator function value boolean true 
main importance exponential family continuous integer domains 
course large class functions log xjy approximated arbitrarily polynomial sufficiently terms exponential family covers broad class distributions 
application exponential family learning earliest published result computational learning theory 
interpretations recursive arc reversal theorem relevant mainly distributions involving continuous variables 
comment incremental learning algorithm finite memory compress information seen far training sample smaller set statistics 
done sacrificing information sample context probabilities positive hypothesis search space learning distribution exponential family 
buntine comment computational requirements learning exponential family distribution guaranteed linear sample size compute sufficient statistics learning proceeds independently sample size 
exponential dimension feature space 
furthermore case functions full rank dimension jacobian respect invertible det dw various moments distribution easily 
situation function gamma exists called link function mccullagh nelder 
lemma consider notation definition 
link function gamma exponential family distribution exists moments functions exp expressed terms derivatives direct applications functions gamma normalizing constant link function closed form moments 
techniques doing symbolic calculations appendix exponential family distributions fall groups 
normalizing constant link function known gaussian 
efficiently compute moments determine functional form conjugate distributions normalizing constant 
case 
markov random field image processing moments generally computed approximation process gibbs sampling section 
linear regression example example consider problem linear regression gaussian error described 
instance generalized linear model linear construction basis basis linear gaussian linear regression gaussian error basis learning graphical models core 
basis functions known deterministic functions input variables typically nonlinear orthogonal functions legendre polynomials 
combine linearly parameters produce mean gaussian 
corresponding learning problem represented plates expressed 
basis basis linear gaussian inverse gamma gaussian linear regression problem joint probability model follows oe joe oe gamma oe gamma basis denotes vector values th datum linear regression gaussian error falls exponential family gaussian exponential family mean simple gaussian linear function regression parameters see lemma 
case correspondence exponential family drawn follows 
individual data likelihoods yjx oe need considered 
expand probability show linear sum data terms parameter terms 
yjx oe oe exp gamma oe gamma basis oe exp gamma oe gamma basis basis oe basis oe data likelihood line seen form general exponential family sufficient statistics various data terms exponential 
link function exist parameters sufficient statistics 
buntine inverse gamma gaussian linear regression problem plate removed model simplified graph usual sample means covariances obtained called normal equations linear regression 
matrix dimension number basis functions vector dimension basis basis basis sufficient statistics read directly data likelihood 
consider formula yjx oe dy differentiating respect shows expected value oe mean expected yjx oe basis differentiating respect oe shows expected error mean oe expected yjx oe gamma oe higher order derivatives give formula higher order moments skewness kurtosis casella berger functions second third fourth central moments 
known gaussian interesting point formula constructed differentiating component functions equation recourse integration 
conjugate distribution parameters linear regression problem multivariate gaussian distribution oe known oe inverted gamma 
learning graphical models 
recognizing exponential family recursive arc reversal applied automatically graphical model 
graphical model subset graphical model falls exponential family needs identified 
conditional distribution bayesian network chain component chain graph exponential family full joint exponential family 
lemma gives additional conditions deterministic nodes 
applies bayesian networks comment 
lemma chain graph single plate 
non deterministic variables inside plate deterministic variables variables outside plate 

arcs crossing plate boundary directed plate 

chain components conditional distribution exponential family data variables model parameters furthermore log polynomial function variables 
variable expressed deterministic function form functions conditional distribution exponential family 
second results model fall exponential family 
categories techniques available context 
cases algorithms concerned constructed graphical specifications 
new classes recursive arc reversal case 
case denotes graphical configuration ii denotes operations simplifications performed algorithm 
various normalization constants known closed form appropriate moments bayes factors computed quickly algorithm schemas reasonable computational properties 
category useful subset model fall exponential family 
represented partial exponential family 
part problem exponential family simplified recursive arc reversal theorem remaining part problem typically handled approximately 
decision trees bayesian networks multinomial gaussian variables fall category 
happens structure tree bayesian network remaining problem composed product multinomials gaussians 
basis various bayesian algorithms developed problems buntine madigan raftery buntine spiegelhalter dawid lauritzen cowell heckerman geiger chickering :10.1.1.52.1068
strictly speaking decision trees bayesian networks multinomial gaussian variables exponential family see comment 
computationally convenient treat way 
category discussed section 
buntine exp family mixture model ii exp family exp family conjugate ss partial exponential model ii ss conjugate ii exponential model categories algorithms exponential family second category hidden variables introduced data problem exponential family hidden values known 
represented mixture model 
mixture models titterington smith makov poland model unsupervised learning incomplete data classification problems robust regression general density estimation 
mixture models extend exponential family rich class distributions second category important practice 
general methods handling problems correspond gibbs sampling markov chain monte carlo methods discussed section deterministic counterpart expectation maximization algorithm discussed section 
shown algorithms cycle back forth process order inference process uses fast exponential family algorithms re estimate 
operations graphical models recursive arc reversal theorem section characterizes plates readily removed sample summarized statistics 
outside cases general classes approximate algorithms exist 
introduced section detail instance tanner 
general algorithms require number basic operations performed graphs decomposition learning problem decomposed simpler subproblems yielding separate analysis 
form decomposition learning problems considered section 
related form applies undirected graphs developed dawid lauritzen 
forms decomposition done modeling level initial model constructed manner requiring fewer parameters heckerman similarity networks 
learning graphical models exact bayes factors model selection averaging methods deal multiple models kass raftery buntine stewart madigan raftery :10.1.1.52.1068:10.1.1.52.1068
require computation bayes factors models constructed search 
exact methods computing bayes factors considered section 
derivatives various approximation search algorithms require derivatives calculated discussed 
derivatives important operation graphs calculation derivatives parameters 
useful conditioning known data approximate inference 
numerical optimization derivatives done search map values parameters apply laplace approximation estimate moments 
section shows compute derivatives operations local node 
computation easily parallelized popular instance neural networks 
suppose graph compile function searches map values parameters graph conditioned known data 
general requires numerical optimization methods gill murray wright 
gradient descent conjugate gradient levenberg marquardt approach requires calculation derivatives 
newton raphson approach requires calculation second derivatives 
done numerically difference approximations accurate calculations exist 
methods symbolically differentiating networks functions piecing results produce global derivatives understood griewank corliss 
instance software available function defined fortran code language produce second function computes exact derivative 
problems understood feed forward networks werbos su buntine weigend graphical models plates add additional complexity 
basic results reproduced section simple examples highlight special characteristics arising chain graphs 
consider problem learning feed forward network 
simple feed forward network 
corresponding learning problem representing feed forward network bayesian network 
sigmoid units network modeled deterministic nodes network output represents mean bivariate gaussian inverse variance matrix sigma 
nonlinear sigmoid function making deterministic mapping inputs means learning problem reasonable component falling exponential family 
rough fallback method calculate map value weight parameters 
method laplace approximation buntine weigend mackay covered tanner tierney kadane 
setting priors feed forward networks difficult mackay nowlan hinton wolpert considered assuming prior 
graph implies buntine sigmoid sigmoid sigmoid sigmoid gaussian gaussian learning feed forward network posterior probability sigma sigma det sigma exp gamma sigma gamma sigmoid sigmoid undirected clique parameters indicates prior term 
suppose posterior differentiated respect parameters result known neural network community kind calculation yields standard backpropagation equations 
calculation look general case 
develop general formula differentiating graphical model concepts needed 
deterministic nodes form islands determinism uncertainty represented graph 
partial derivatives island calculated recursive chain rule instance forward backward propagation derivatives equations 
instance forward propagation network gives called forward propagation derivatives respect propagated forward network 
contrast backward propagation propagate derivatives respect different variables backwards 
island determinism important variables output variables derivatives required 
feed forward network partial derivatives respect required 
learning graphical models definition non deterministic children node denoted set non deterministic variables exists directed path intermediate variables deterministic 
nondeterministic parents node denoted set non deterministic variables exists directed path intermediate variables deterministic 
deterministic children node denoted set deterministic variables children deterministic parents node denoted set deterministic variables parents instance model non deterministic children deterministic nodes removed graph rewriting equations represented remaining variables graph 
graphical operations apply deterministic nodes removal done implicitly theorem 
goes follows lemma chain graph nodes deterministic nodes ae chain graph created adding directed arc node non deterministic children deleting deterministic nodes graphs equivalent probability models nodes gamma general formula differentiating bayesian networks plates deterministic nodes lemma 
chain rule differentiation important notice network structure computation 
partial derivatives computed networks local global partial derivatives different 
consider feed forward network 
place extra arc consider partial derivative respect value influenced directly new arc shows indirectly computing partial derivative involving indirect influences need differentiate direct indirect effects 
various notations werbos buntine weigend 
notation local versus global derivative 
local partial derivative subscripted represents partial derivative computed node direct influences parents 
example partial derivative respect various local partial derivatives combine produce global partial derivative equivalent 
general global partial derivative index variable sum ffl local partial derivative node containing buntine ffl partial derivatives child non deterministic child ffl combinations global partial derivatives deterministic children backward forward propagation derivatives 
lemma differentiation 
model represented bayesian network plates deterministic nodes variables denote known variables unknown variables gamma conditional probability represented graph jk 
unknown variable graph nd non deterministic 
occurs inside plate arbitrary valid index null 
log jk nd log children log log furthermore ae subset unknown variables partial derivative probability known variables jk expected value probabilities log jk gammay jy log jk equation contains global partial derivative inside double sum right side 
partial derivative computed local island determinism chain rule differentiation instance forward propagation deterministic children backward propagation non deterministic parents 
apply differentiation lemma problems feed forward networks unsupervised learning lemma needs extended chain graphs 
means differentiating markov networks bayesian networks handling expected value equation 
extensions explained giving examples 
example consider feed forward network problem 
treating output units feed forward network single variable cartesian product differentiation lemma applied directly feed forward network model 
uses simplification section comment 
joint probability feed forward network model equation 
non deterministic children single chain component consisting variables parents set fm consider differentiation lemma 
children non deterministic middle sum equation differentiation lemma empty 
lemma yields expanding inner sum log log learning graphical models log sigma log sigma sigma dimensional gaussian global derivative evaluates local derivative 
second example reconsider simple unsupervised learning problem section 
likelihood single datum model parameters marginal form var var var joe oe gamma logarithm full case probability class var var var joe reveals vectors components exponential distribution log class var var var joe class log oe class var true log class var false log gamma notice normalizing constant oe case 
consider finding partial derivative log var var var joe done case differentiating posterior likelihood unsupervised learning model 
applying equation yields log var var var joe log class var var oe class var true log gamma class var var oe class var true var true class jvar var var oe gamma var false class jvar var var oe notice derivative computed doing order inference find class jvar var var oe noted russell binder koller 
property holds general exponential family models missing unknown variables 
derivatives calculated order inference followed combination derivatives functions 
consider notation exponential family introduced previously definition functional form xjy exp buntine consider partial derivative marginal probability gamma ae equation differentiation lemma partial derivative log gamma gammau gamma partition function known closed form case boltzmann machine final derivative approximated key formula doing equation appendix 
extend differentiation lemma chain graphs trick illustrated feed forward network 
interpret chain graph bayesian network chain components done equation apply differentiation lemma 
evaluate necessary local partial derivatives respect individual chain component 
undirected graphs necessarily normalized may problem 
general undirected graph variables theorem general form jy cliques cliques local partial derivative respect log jy cliques log gamma xjy cliques log difficulty computing expected value formula comes normalizing constant 
computation forms core early boltzmann machine algorithm hertz 
general done gibbs sampling techniques section applied directly 
decomposing learning problems learning problems decomposed sub problems cases 
material section applies generally sorts decompositions section considers simple example proves general results problems decomposition 
problem decompositions recomputed fly create search space models takes advantage decompositions exist 
general result incremental decomposition 
results simple applications known methods testing independence lauritzen added complication plates 
consider simple learning problem section multinomial variables var var gaussian variables problem specified alternative models model model model additional arc going discrete variable var real valued variable subsequently discuss local search models evaluated evidence 
manipulation conditional distribution model making lemma yields model conditional distribution 
parameters learning graphical models var var var var var simplification model priori independent data likelihoods introduce cross terms parameters posteriori independent 
occurs set oe model simplification implies evidence model decomposes similarly 
denote sample variable likewise var var case result evidence var jm var jvar jvar jx var evidence model similar posterior distribution oe replaced posterior distribution oe result general applies bayesian networks undirected graphs generally chain graphs 
similar results covered dawid lauritzen family models call hyper markov 
general result described application rules independence applied plates 
uses notion nondeterministic children parents introduced definition 
requires notion local dependence called markov blanket pearl generalization equivalent set bayesian networks 
definition chain graph plates 
markov blanket node neighbors non deterministic parents non deterministic children non deterministic parents children chain components markov blanket neighbors chain components follows independent non deterministic variables graph markov blanket 
perform simplification depicted sufficient find finest partitioning model parameters independent 
decomposition represents finest partition model evidence model factor partition model equation 
task theorem depicted graphically 
buntine theorem decomposition 
model represented chain graph plates 
variables graph possibly empty subsets variables unknown partition unknown 
induces decomposition graph subgraphs ffl graph contains nodes arcs plates occurring nodes ffl potential functions cliques equivalent induced decomposition represents unique finest equivalent independence model original graph finest collection sets ignoring plates unknown node markov blanket finest decomposition takes jx compute 
furthermore evidence product subgraph evidence known jm known functions proof 
shows decomposition works unknown nodes 
shows basic problem shows finest decomposition 
notice bottom component decomposed variable unknown 
var var var var var var incremental decomposition model cases functions decomposition theorem equation clean interpretation equal evidence subgraphs 
result obtained corollary 
learning graphical models corollary local evidence 
context theorem suppose exists set chain components graph ignoring plates unknown 
known known denote th subgraph model term conditional evidence model denote maximal subgraph known variables induced cliques proof decomposition theorem 
condition corollary holds follows evidence model equal product evidence subgraph evidence evidence holds general original graph bayesian network learning bayesian networks buntine cooper herskovits :10.1.1.52.1068
corollary equation holds parent graph bayesian network plates 
general consider searching family graphical models 
local search johnson yannakakis numerical optimization find high posterior models markov chain monte carlo methods select sample representative models discussed section 
represent family models shown 
instance similar models arcs hatched 
indicate arcs optional 
var var family models optional arcs hatched instantiate hatched arc removed replaced full arc graphical model represents different models possible instantiations arcs 
prior probabilities models generated scheme buntine heckerman prior probability assigned buntine domain expert different parts model arcs parameters prior full model multiplication :10.1.1.52.1068
family models includes instances 
search sampling important property bayes factor models bayes factor described section 
decomposition theorem corollary bayes factor versus looking local bayes factors 
difference models parent variable bayes factor jvar var jvar bayes factor computed considering models involving oe oe incremental modification evidence bayes factors finest decompositions general follows directly independence test 
similar property undirected graphs dawid lauritzen 
developed case directed arcs non deterministic variables 
handling deterministic variables require repeated application results non deterministic variables may effected adding single arc deterministic variables 
lemma incremental decomposition 
graph context theorem non deterministic variables 
consider adding removing directed arc update finest decomposition unique subgraph containing unknown variables chain component 
subgraph add delete arc add delete subgraph required 
shaded non deterministic parents added nodes graph finest decomposition remains unchanged additional arcs 
hatched arcs contexts causes additional trouble decomposition process 
finest decomposition graph plates hatched directed arcs formed arcs directed arcs 
evidence adjusted search adding different parents required 
bayes factors exponential family decomposition results learning system necessary able generate bayes factors evidence component models 
models exponential family normalization constant known closed form turns easy 
exact computations available various approximation methods compute evidence bayes factors kass raftery discussed section 
conjugate distribution exponential family model derivatives readily computed bayes factor model closed form 
decomposition methods result important basis fast bayesian algorithms considering multiple models 
explicitly implicitly bayesian methods learning decision trees directed graphical models discrete gaussian variables linear regression buntine spiegelhalter :10.1.1.52.1068:10.1.1.52.1068
learning graphical models instance normalizing constant lemma known closed form bayes factor readily computed 
lemma consider context lemma 
model likelihood evidence evidence xn jy computed evidence jy yjx gaussian involves multiplying sets normalizing constants gaussian gamma distributions 
evidence common exponential family distributions appendix table instance consider learning problem 
assume variables var var binary parameters interpreted follows var var jvar var jvar dirichlet priors parameters shown table priors gamma dirichlet ff ff jj gamma jj dirichlet ff jj ff jj priori independent choice priors distributions discussed box tiao bernardo smith 
denote corresponding sufficient statistics equal number data var jji equal number data var var 
terms evidence model read directly table written var jm beta ff ff beta ff ff var jvar beta ff ff beta ff ff beta ff ff beta ff ff assume variables gaussian means var var var var variances oe jj oe jj respectively 
case split data set parts var var 
get parameters sufficient buntine statistics contribution evidence 
conjugate priors table appendix yjx gaussian indexed accordingly ijj joe ijj gaussian ijj sigma ijj oe ijj oe gamma ijj gamma ffi ijj fi ijj notice sigma ijj dimensional dimensional 
suitable sufficient statistics situation read table looking data summaries 
simplified gaussian uniformly 
sufficient statistics means variances different values var denote sample means var respectively corresponding sample variances 
done second case notation table sigma fi table respectively sigma jj jj fi jj change vector making calculations indicated 
sufficient statistics case var jj var jj var jj var gamma jj evidence terms read table 
jvar sigma jj sigma jj gamma ffi jj gamma ffi jj fi ffi jj jj fi jj jj sigma sigma gamma jj ffi jj jx var det sigma jj det sigma jj gamma ffi jj fi ffi jj jj gamma ffi jj fi ffi jj jj final simplification model 

approximate methods graphical models exact algorithms learning reasonable size invariably involve recursive arc reversal theorem section 
learning methods approximate algorithms level 
common uses exponential family approximation algorithms summed 
various methods inference plates learning graphical models full simplification model applied model level parameter level gibbs sampling described section general markov chain monte carlo algorithms em style algorithms dempster laird rubin various closed form approximations mean field approximation laplace approximation berger azevedo shachter 
section summarizes main families approximate methods 
gibbs sampling gibbs sampling basic tool simulation applied probability distributions geman geman gilks ripley long full joint zeros variable instantiations possible 
special case general markov chain monte carlo methods approximate inference ripley neal 
gibbs sampling applied virtually graphical model plates undirected directed arcs variables real discrete 
gibbs sampling apply graphs deterministic nodes put zeroes full joint 
section describes gibbs sampling plates precursor discussing gibbs sampling plates section 
challenging problems forms markov chain monte carlo sampling tried 
literature extensive 
gibbs sampling corresponds probabilistic version gradient ascent goals averaging opposed maximizing fundamentally different 
gradient ascent real valued problems corresponds simple methods function optimization gill discrete problems corresponds local repair local search johnson minton johnson philips laird selman levesque mitchell 
gibbs sampling varies gradient ascent introducing random component 
algorithm usually tries ascend descend strategy exploring search space 
algorithm tends wander local maxima occasional excursions regions space 
gibbs sampling core algorithm simulated annealing temperature held equal van laarhoven aarts 
sample set variables non zero distribution initialize value repeatedly resample variable conditional buntine probability xjx gamma fxg 
simple medical problem suppose value symptoms known remaining variables sampled follows 
initialize remaining variables 

repeat record sample age occ clim dis cycle 
reassign age sampling conditional clim dis symp take values occ clim dis symp compute resulting conditional distribution age 
sample age distribution 
reassign occ sampling conditional clim dis symp reassign clim sampling conditional occ dis symp reassign dis sampling conditional clim occ symp sequence steps depicted 
basic graph climate age disease occupation climate age disease symptoms occupation climate age disease occupation climate age disease gibbs sampling medical example arranged step represent dependencies arise sampling process 
uses arc reversal conditioning operators introduced previously 
effect sampling immediate 
age occ clim dis conditionally dependent age occ clim dis general age occ clim dis effect sampling scheme long run large age occ clim dis learning graphical models approximately generated age occ clim independently age occ clim dis gibbs sampling conditional sampling done accordance original distribution stationary process long run samples converge stationary distribution fixed point process 
methods making subsequent samples independent known regenerative simulation ripley correspond sending temperature back zero occasionally 
sample different quantities probability patient age clim tropical symp estimated 
done looking frequency event generated sample 
justification subject markov process theory theorem 
result informally applies comment sequence discrete variables gibbs sampler distribution 
average approaches expected value probability approaches infinity gamma 
second function ratio sample averages approaches true ratio gamma 
approximate conditional expected values 
complete procedure necessary know gibbs samples take large estimate error estimate 
questions easy answer heuristic strategies exist ripley neal 
bayesian networks scheme easy general requirement sampling xjx gamma fxg conditional distribution nodes connected global probabilities need calculated 
notice instance sampling operations require variables 
general form bayesian networks equation goes follows xjx gamma fxg parents parents notice product subset variables 
include conditional distributions variables parent 
formula involves examining parents children children parents called markov blanket pearl 
notice normalization required single dimension changed current cycle done denominator 
discrete conditional probabilities enumerated direct sampling done kind simplification bayesian networks applies undirected graphs chain graphs plates 
modify equation xjx gamma fxg exp cliques exp cliques buntine formula ignore cliques containing gibbs sampling computes information local node 
troublesome normalization constant computed probability ratio functions cancels 
normalization required single dimension gibbs sampling plates learning problems represented bayesian networks 
instance simple unsupervised learning problem represented bayesian network plate expanded 
follows gibbs sampling readily applied learning general inference algorithm gilks 
consider simplified example unsupervised learning problem 
model assume variable var var belongs mixture gaussians known variance equal 
simple model 
class class var var class unsupervised learning dimensions variables var var distributed gaussian means uniform unit variance case distribution sample var var oe oe var gamma var gamma dimensional gaussian probability density function standard deviation 
model trivial standard deviation vary model corresponds kernel density estimate approximate distribution arbitrarily sufficient number tiny gaussians 
simplified gaussian mixture model sequence steps gibbs sampling goes follows 
initialize variables oe class 
repeat record sample oe class cycle 
reassign class conditional class var var oe reassign vector oe sampling conditional learning graphical models reassign vector sampling conditional jvar class illustrates step language graphs 
illustrates var var class var var class gibbs sampling unsupervised learning problem steps 
step represents standard sampling operation inference bayesian networks plates 
step easy perform case distributions exponential family graph matches conditions lemma 
model parameters oe posteriori independent distribution known closed form sufficient statistics calculated time 
important caveat gibbs sampling learning problem symmetry 
description distinguish class class 
initially class centers process remain distinct 
asymptotically problem definition distinguish class class appear indistinguishable 
problem handled symmetry breaking force gibbs sampling applies variables associated data 
hidden latent variables example 
incomplete data missing values quinlan robust methods modeling outliers various density estimation non parametric methods fall family models titterington 
gibbs sampling generalizes virtually graphical model plates unshaded nodes inside plate sequence sampling operations 
underlying distribution exponential family instance lemma applies shading nodes inside plate full cycle guaranteed linear time sample size 
algorithm exponential family case summed 
shows general learning problem extending mixture model 
structure appears 
sufficient statistics shown 
ii shows algorithm generalizing 
role sufficient statistics shown 
sampling computes sufficient statistics sampling applies 
thomas spiegelhalter gilks gilks taken advantage general applicability sampling create compiler converts graphical representation data analysis problem plates matching gibbs sampling algorithm 
scheme applies broad variety data analysis problems 
buntine exp family exp family exp family ii ii gibbs sampling exponential family closed form approximation happen gibbs sampling number cases training sample large compared number unknown parameters problem 
way think sufficiently large samples model parameters oe tend drift mean posterior variance 
th step sample oe th sample oe conditionally dependent large posterior variance th sample th sample small oe oe oe approximation markov chain monte carlo methods mean field method statistical physics popular neural networks hertz 
sampling sequence parameters scheme deterministic update expected value sampling distribution 
generates deterministic sequence reasonable conditions converges maxima 
kind approach leads naturally em algorithm discussed section 
expectation maximization em algorithm expectation maximization algorithm widely known em algorithm corresponds deterministic version gibbs sampling search map estimate model parameters dempster 
generally considered faster gradient descent 
convergence slow near local maxima implementations switch conjugate gradient methods near solution 
computation find derivative similar computation em algorithm require great deal additional code 
determinism means em algorithm longer generates unbiased posterior estimates model parameters 
intended learning graphical models gain speed accuracy 
em algorithm generally applied exponential family models gibbs sampling applied 
correspondence em gibbs shown 
consider simple unsupervised learning problem represented section 
case sequence steps em algorithm similar gibbs sampler 
em algorithm works means modes unknown variables sampling 
sampling set classes computing sufficient statistics distribution oe sequence class means generated compute expected sufficient statistics 
likewise sampling new parameters oe modes computed expected sufficient statistics 
consider unsupervised learning problem 
suppose classes variables var var var finite valued discrete modeled multinomial probabilities conditional class value class sufficient statistics case counts number cases class kjj number cases class var class kjj class var expected sufficient statistics computed rules probability set parameters oe class var var oe kjj class var var oe var lemma kinds expected sufficient statistics computed exponential family distributions 
sufficient statistics computed distributions posterior means modes model parameters case oe 

initialize parameters oe 
repeat convergence criteria met compute expected sufficient statistics kjj 
recompute oe equal mode conditioned sufficient statistics 
posterior distributions standard tables cases lemma 
instance mean oe gives oe ff ff buntine gibbs sampling algorithms discussed section similarly placed em framework 
mode step ignoring numerical problems em algorithm converges local maxima posterior distribution parameters dempster 
general method summarized comment dempster 
comment conditions lemma apply data variables inside plate model parameters outside 
addition variables ae latent unknown unshaded 
remaining variables missing data variables ae gamma 
means data th datum gamma gamma em algorithm goes follows step contribution expected sufficient statistics datum jx gammau gammav expected sufficient statistic step maximize conjugate posterior expected sufficient statistics place sufficient statistics map approach distribution 
fixed point algorithm local maxima posterior 
ii gibbs sampling section illustrates steps em nicely 
ii corresponds step 
expected sufficient statistics parameters 
sampling computing sufficient statistics expected sufficient statistics computed 
ii corresponds step 
sufficient statistics mean mode parameters computed sampled 
em gibbs mean mode approximation done major sampling steps algorithm 
cases expected sufficient statistics computed closed form 
assume exponential family distribution known normalization constant link function gamma exists 
normalizing constant exponential family distribution jx gamma gamma known closed form 
denote 
notation theorem dw gamma log 
partial exponential models cases initial inner part learning problem handled recursive arc reversal theorem section 
case simplify simplified solve remainder problem generic method map approximation 
section presents examples linear regression heterogeneous variance feed forward networks linear output layer bayesian networks 
general process depicted graphical model partial exponential family 
abstraction represent general process 
consider learning graphical models problem learning bayesian network structure parameters distribution exponential family network structure 
variable discrete variable indicating graphical structure chosen bayesian network 
variable represents full set variables problem 
variable theta represents distributional parameters bayesian network part model conveniently exponential family 
theta treated exponential different theta hold fixed 
sufficient statistics case ss 
case subproblem conveniently falls exponential family theta jx simplified necessary resort general learning techniques previous sections solve remaining part problem jx 
linear regression heterogeneous variance consider heterogeneous variance problem 
shows graphical model linear regression problem section modified situation standard deviation heterogeneous function inputs 
case basis basis weights linear gaussian weights log linear exp linear regression heterogeneous variance standard deviation computed exp weights oe basis exponential transformation guarantees standard deviation positive 
corresponding learning model simplified graph 
compare model 
difference 
case sufficient statistics exist shown deterministically dependent sample ultimately unknown parameters standard deviation weights oe 
parameters standard deviation known graph reduced 
computationally important gain 
says set values weights oe calculation done linear time sample size arrive characterization buntine basis weights basis weights gaussian log linear exp heterogeneous variance problem plate simplified parameters mean weights 
short half problem weights oe understood 
search map solution search space parameters standard deviation weights oe remaining weights 
variation linear regression replaces gaussian error function robust error function student distribution norm 
introducing convolution robust regression models handled combining em algorithm standard squares lange 
feed forward networks linear output layer similar example standard feed forward network final output layer linear 
situation change deterministic functions linear sigmoidal 
case lemma identifies weight vectors assumed distribution exponential family 
simplification possible standard sufficient statistics multivariate linear regression 
algorithmically implies values internal weight vectors assuming conjugate prior holds output weight vectors posterior distribution output weight vectors means variances closed form 
evidence jx computed exact method lemma posterior jy jm jx computed closed form constant 
effectively cuts problem pieces followed provides clean solution second piece 
learning graphical models sigmoid sigmoid linear linear simplified learning feed forward network linear output bayesian networks missing variables class probability trees discrete bayesian networks learned efficiently noticing basic form exponential family buntine cooper herskovits spiegelhalter :10.1.1.52.1068
take instance family models specified bayesian network 
case local evidence corollary corollary applies 
evidence bayesian networks generated graph product nodes bayesian network 
change bayesian network adding removing arc bayes factor simply local bayes factor node mentioned incremental decomposition lemma lemma 
local search quite fast gibbs sampling space bayesian networks possible 
similar situation exists trees buntine :10.1.1.52.1068
results apply bayesian network exponential family distributions node gaussian poisson 
results gaussians instance geiger heckerman 
local search approach map approach searches network structure maximizing posterior probability 
accurate approximation done generating markov chain bayesian networks search space bayesian networks 
bayes factors readily computed case gibbs sampling markov chain monte carlo schemes 
scheme metropolis algorithm ripley 
looks single neighbors successor 
done repeating steps 
initial bayesian network randomly select neighboring bayesian network differing arc buntine 
compute bayes factor making decompositions described theorem doing local computation described lemma bayes factors computed lemma 
accept new bayesian network probability min bayes factor accepted assign remains unchanged 
local maxima bayesian network concurrently scheme generates set bayesian networks appropriate model averaging expert evaluation space potential bayesian networks 
course initialization search local maxima 
sampling scheme illustrated context averaging 
scheme readily adapted learn structure parameters bayesian network missing latent variables 
metropolis algorithm add step resamples missing data latent variables 

current complete data bayesian network compute predictive distribution missing data latent variables 
resample missing data latent variables construct new set complete data subsequent computing bayes factors 

marriage learning graphical models provides framework understanding learning 
provides framework developing learning data analysis toolkit software generator learning algorithms 
toolkit combines important components language representing learning problem techniques generating matching algorithm 
working toolbox demonstrated blueprint provided show constructed construction known learning algorithms demonstrated 
table lists standard problems derivation algorithms operations previous chapters text considered 
notion learning toolkit new seen bugs system thomas spiegelhalter gilks gilks cohen inductive logic programming emerging software handling generalized linear models mccullagh nelder becker chambers wilks 
important role data analysis toolkit 
problem quirks requirements 
knowledge discovery instance vary ways depending user defined notion interestingness 
learning embedded task larger system 
easy applications learning generally learning applications require special purpose development learning systems related support software 
achieved patching existing techniques decomposing problem subproblems 
decomposition patching learning graphical models problem method sections bayesian networks exponential family conditionals decomposition exact bayes factors local search gibbs sampling generate alternative models bayesian networks missing latent variables unsupervised learning models gibbs sampling em making techniques possible feed forward networks map method exact computation derivatives feed forward networks linear output initial removal linear component linear regression extensions squares em map generalized linear models map method exact computation derivatives table derivation learning algorithms learning algorithms inference decision making formalized understood graphical models 
ways system plays role toolkit chambers hastie 
provides system prototyping learning algorithms includes ability handle generalized linear models automatic differentiation expressions includes statistical mathematical functions useful primitives 
language graphical models best viewed additional layer top kind system 
note impractical assume software generator create algorithms competitive current finely tuned algorithms instance hidden markov models 
software toolkit learning prototype algorithm refined hand 
combination learning graphical models shares superior aspects different learning fields 
consider philosophy neural networks 
nonparametric systems composed simple computational components usually readily parallelizable nonlinear 
components tailor systems specific applications 
graphical models learning features 
graphical models expressibility probabilistic knowledge representations developed artificial intelligence knowledge acquisition contexts 
form important basis knowledge refinement 
graphical models learning allow powerful tools statistics applied problem 
learning problems specified common language graphical models associated learning algorithms derivation interrelationships explored 
allows commonalities seemingly diverse pairs algorithms means clustering versus approximate methods learning hidden markov models learning decision trees versus learning bayesian networks buntine gibbs sampling versus expectation maximization algorithm section understood variations :10.1.1.52.1068
framework important educational tool 
buntine appendix proofs lemmas theorems proof theorem useful property independence independent functions result follows directly property 
proof result simple lemma 
independent gamma gamma functions variable sets gamma gamma functions 
notice known gamma gamma functions independence 
instantiate variables value gamma gamma similarly instantiate gamma gamma multiplying sides equalities substitute gamma gamma get defined domain cross product 
lemma holds functions strictly positive gamma gamma final proof result follows applying equation repeatedly 
suppose variables strictly positive function gamma fx fx neighbors denote gamma fx fx neighbors 
repeating application equation variable yields gamma strictly positive functions consider functions 
necessary keep function set gamma maximal contained set 
equivalently keep function set minimal 
minimal sets set cliques undirected graph 
result follows 
learning graphical models proof theorem takes jx operations remove deterministic nodes graph lemma 
nodes removed graph reinserted 
assume graph contains deterministic nodes 
denote unknown variables set unknown gamma known 
loss generality assume contains known variables markov blanket unknown 
showing independence model equivalent amounts showing unknown independent unknown known 
test independence method plate expanded duplicate right number times graph removing nodes test separability 
markov blanket node expanded graph corresponds nodes directly connected moralized expanded graph 
suppose finest unique partition unknown unknown nodes 
reconstructed adding known variables markov variables unknown 
suppose unknown variable plate instances plate expanded 
symmetry element finest partition separate elements 
certain unknown variable markov boundary outside plate symmetry 
element partition 
contradiction separate element element occurs wholly plate boundaries 
finest partition represented plates finest partition identified graph ignoring plates 
operation finding finest separated sets graph quadratic size graph jx complexity 
assume condition holds consider equation 
cliques denote subsets variables parents form cliques graph formed restricting parents placing undirected arc parents 
set chain components jm ind cliques furthermore known variables markov blanket occur clique cliques containing unknown variable partitioned subgraph belong 
cliques fc cliques unknown add remaining cliques wholly contained set far cliques cliques cliques cliques call remaining cliques cliques jm cliques ind buntine distribution functional form dim multinomial yjx gaussian oe oe exp gamma oe gamma gamma ff fi fi ff gamma ff ff gamma gammafi dim dirichlet ff ff beta ff ff ff gamma dim gaussian sigma thetad det sigma exp gamma gamma sigma gamma dim wishart ff sigma thetad det gammaff sigma det ff gammad gamma dff gamma gamma ff gammai exp gamma trace sigma gamma sigma symmetric positive definite table distributions functional form results known unknown cliques ind furthermore potential functions cliques defined described 
proof corollary clique chain component occur cliques cliques ind proof lemma consider definition markov blanket 
directed arc added nodes markov blanket change unknown node enters set non deterministic parents chain components containing non deterministic children effect subsequent graph separability subsequently add arcs node nodes 
appendix exponential family exponential family distributions described definition 
common exponential family exists theorem 
table gives exponential family distributions functional form 
details extensive tables bayesian textbooks probability distributions degroot bernardo smith 
table gives standard conjugate prior distributions table table gives matching posteriors degroot bernardo smith learning graphical models distribution conjugate prior dim multinomial dirichlet ff ff yjx gaussian joe dim gaussian oe sigma oe gamma gamma ff fi gamma gamma ff fi dim gaussian sigma gaussian sigma sigma wishart ffi table distributions conjugate priors distribution conjugate posterior dim multinomial dirichlet ff nc ff yjx gaussian joe dim gaussian oe sigma oe gamma gamma ff fi sigma sigma sigma gamma sigma fi gamma gamma sigma gamma fi gamma gamma ff ff fi dim gaussian sigma gaussian sigma sigma wishart ffi gamma gamma gamma table distributions matching conjugate posteriors buntine distribution evidence dim multinomial beta ff nc ff beta ff ff yjx gaussian det sigma det sigma gamma ff fi ff gamma ff fi ff gamma fi ff gamma ff ff gamma ff gamma fi delta ff ff ff fixed dim gaussian det ffi dn det ffi gamma ffi gamma gammai gamma ffi gamma gammai table distributions evidence 
distributions table priors table table gives matching evidence derived lemma cancelling common terms 
case functions full rank dimension jacobian respect invertible det dw various moments distribution easily xjy dw gamma dz vector function inverse referred link function mccullagh nelder 
yields xjy exp oe gamma oe important normalization constant closed form differentiated divided instance symbolically construct formula various moments distribution xj xj 
furthermore equation implies derivatives normalization constant dz estimating moments sufficient statistics instance markov chain monte carlo methods 
general program shared including peter cheeseman encouraged development inception 
ideas formative stages snowbird neural networks computing april bayesian analysis expert systems group pavia italy june 
feedback group helped develop ideas 
graduate students stanford berkeley received various incarnations ideas 
george john ronny kohavi scott scott roy padhraic smyth peter cheeseman feedback drafts jair reviewers 
brian williams pointed extension decomposition theorems deterministic case 
brian ripley reminded extensive features learning graphical models andersen olesen jensen jensen 

hugin shell building bayesian belief universes expert systems 
international joint conference artificial intelligence detroit 
morgan kaufmann pp 

azevedo shachter 

laplace method approximations probabilistic inference belief networks continuous variables 
de mantaras poole 
eds 
uncertainty artificial intelligence proceedings tenth conference seattle washington pp 

becker chambers wilks 

new language 
pacific grove california wadsworth brooks cole 
berger 

statistical decision theory bayesian analysis 
new york springerverlag 
bernardo smith 

bayesian theory 
chichester john wiley 
besag york 

bayesian image restoration applications spatial statistics 
ann 
inst 
statist 
math 
box tiao 

bayesian inference statistical analysis 
reading massachusetts addison wesley 
breiman friedman olshen stone 

classification regression trees 
belmont california wadsworth 
bretthorst 

model selection probability theory logic 

ed 
maximum entropy bayesian methods kluwer academic 
proceedings santa barbara 
buntine 

classifiers theoretical empirical study 
international joint conference artificial intelligence sydney morgan kaufmann pp 

buntine 

learning classification trees 
hand 
ed artificial intelligence frontiers statistics london chapman hall pp 

buntine 

theory refinement bayesian networks 
ambrosio smets bonissone 
eds uncertainty artificial intelligence proceedings seventh conference los angeles california 
buntine 

representing learning graphical models 
technical report fia artificial intelligence research branch nasa ames research center 
submitted 
buntine weigend 

bayesian back propagation 
complex systems 
buntine weigend 

computing second derivatives feed forward networks review 
ieee transactions neural networks 
buntine casella berger 

statistical inference 
belmont california wadsworth brooks cole 


stochastic processes 
prentice hall 
chambers hastie 
eds 

statistical models pacific grove california wadsworth brooks cole 
chan shachter 

structural controllability observability influence diagrams 
dubois wellman ambrosio smets 
eds 
uncertainty artificial intelligence proceedings conference stanford california pp 

charniak 

bayesian networks tears 
ai magazine 
cheeseman self kelly taylor freeman stutz 

bayesian classification 
seventh national conference artificial intelligence saint paul minnesota pp 

cheeseman 

finding probable model 
langley 
eds computational models discovery theory formation 
morgan kaufmann 
cohen 

compiling prior knowledge explicit bias 
ninth international conference machine learning morgan kaufmann pp 

cooper herskovits 

bayesian method induction probabilistic networks data 
machine learning 
cowell 

probabilistic expert system shell qualitative quantitative learning 
bernardo bernardo berger dawid smith 
eds 

bayesian statistics 
oxford university press pp 

dagum horvitz 

uncertain reasoning forecasting 
international journal forecasting 
submitted 
dagum horvitz 

reformulating inference problems selective conditioning 
dubois wellman ambrosio smets 
eds 
uncertainty artificial intelligence proceedings conference stanford california pp 

dawid 

conditional independence statistical theory 
siam journal computing 
dawid 

properties diagnostic data distributions 
biometrics 
dawid lauritzen 

hyper markov laws statistical analysis decomposable graphical models 
annals statistics 
dean wellman 

planning control 
san mateo california morgan kaufmann 
learning graphical models degroot 

optimal statistical decisions 
mcgraw hill 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society 
duda hart 

pattern classification scene analysis 
new york john wiley 


chain graph markov property 
scandinavian journal statistics 
geiger heckerman 

learning gaussian networks 
de mantaras poole 
eds 
uncertainty artificial intelligence proceedings tenth conference seattle washington pp 

geman 

random fields inverse problems imaging 

ed ecole de probabilit es de saint flour xviii 
springer verlag berlin 
lecture notes mathematics volume 
geman geman 

stochastic relaxation gibbs distributions bayesian relation images 
ieee transactions pattern analysis machine intelligence 
gilks clayton spiegelhalter best mcneil kirby 

modelling complexity applications gibbs sampling medicine 
journal royal statistical society 
gilks thomas spiegelhalter 

language program complex bayesian modelling 
statistician 
gill murray wright 

practical optimization 
san diego academic press 
griewank corliss 
eds 

automatic differentiation algorithms theory implementation application breckenridge colorado 
siam 
heckerman geiger chickering 

learning bayesian networks combination knowledge statistical data 
technical report msr tr revised microsoft research advanced technology division 
submitted machine learning journal 
heckerman 

probabilistic similarity networks 
mit press 
henrion 

efficient inference multiply connected belief networks 
oliver smith 
eds influence diagrams belief nets decision analysis pp 

wiley 
hertz krogh palmer 

theory neural computation 
addison wesley 
buntine howard 

decision analysis perspectives inference decision experimentation 
proceedings ieee 


gibbs sampling bayesian networks 
artificial intelligence 
jeffreys 

theory probability third edition 
oxford clarendon press 
johnson yannakakis 

easy local search 
focs pp 

kass raftery 

bayes factors model uncertainty 
technical report department statistics carnegie mellon university pa submitted 
american statistical association 


computational scheme reasoning dynamic probabilistic networks 
dubois wellman ambrosio smets 
eds 
uncertainty artificial intelligence proceedings conference stanford california pp 

kohavi 

bottom induction oblivious read decision graphs strengths limitations 
twelfth national conference artificial intelligence 
lange 

normal independent distributions applications robust regression 
journal computational graphical statistics 
langley iba thompson 

analysis bayesian classifiers 
tenth national conference artificial intelligence san jose california pp 

lauritzen dawid larsen 

independence properties directed markov fields 
networks 
little rubin 

statistical analysis missing data 
new york john wiley sons 
loredo 

promise bayesian inference astrophysics 
babu 
eds statistical challenges modern astronomy 
springer verlag 
mackay 

practical bayesian framework backprop networks 
neural computation 
mackay 

bayesian non linear modeling energy prediction competition 
report draft cavendish laboratory university cambridge 
madigan raftery 

model selection accounting model uncertainty graphical models occam window 
journal american statistical association 
appear 
mccullagh nelder 

generalized linear models second edition 
chapman hall london 
learning graphical models mclachlan basford 

mixture models inference applications clustering 
new york marcel dekker 


fast improvement em algorithm terms 
roy 
statist 
soc 

minton johnson philips laird 

solving large scale scheduling problems heuristic repair method 
eighth national conference artificial intelligence boston massachusetts pp 

neal 

probabilistic inference markov chain monte carlo methods 
technical report crg tr dept computer science university toronto 
nowlan hinton 

simplifying neural networks soft weight sharing 
touretzky 
ed advances neural information processing systems nips 
morgan kaufmann 
oliver 

decision graphs extension decision trees 
proceedings fourth international workshop artificial intelligence statistics pp 

extended version available tr department computer science monash university australia 
pearl 

probabilistic reasoning intelligent systems 
morgan kaufmann 
poland 

decision analysis continuous discrete variables mixture distribution approach 
ph thesis department engineering economic systems stanford university stanford california 
press 

bayesian statistics 
new york wiley 
quinlan 

unknown attribute values induction 

ed proceedings sixth international machine learning workshop cornell new york 
morgan kaufmann 
quinlan 

programs machine learning 
morgan kaufmann 
ripley 

spatial statistics 
new york wiley 
ripley 

stochastic simulation 
john wiley sons 
rivest 

learning decision lists 
machine learning 
russell binder koller 

adaptive probabilistic networks 
technical report csd july university california berkeley 
selman levesque mitchell 

new method solving hard satisfiability problems 
tenth national conference artificial intelligence san jose california pp 

shachter 

evaluating influence diagrams 
operations research 
buntine shachter 

ordered examination influence diagrams 
networks 
shachter andersen szolovits 

global conditioning probabilistic inference belief networks 
de mantaras poole 
eds 
uncertainty artificial intelligence proceedings tenth conference seattle washington pp 

shachter heckerman 

thinking backwards knowledge acquisition 
ai magazine fall 
shachter 

gaussian influence diagrams 
management science 
smith spiegelhalter 

bayes factors choice criteria linear models 
journal royal statistical society 
spiegelhalter 

personal communication 
spiegelhalter dawid lauritzen cowell 

bayesian analysis expert systems 
statistical science 
spiegelhalter lauritzen 

sequential updating conditional probabilities directed graphical structures 
networks 
srinivas breese 

ideal software package analysis influence diagrams 
bonissone 
ed proceedings sixth conference uncertainty artificial intelligence cambridge massachusetts 
stewart 

hierarchical bayesian analysis monte carlo integration computing posterior distributions possible models 
statistician 
tanner 

tools statistical inference second edition 
springer verlag 
thomas spiegelhalter gilks 

bugs program perform bayesian inference gibbs sampling 
bernardo bernardo berger dawid smith 
eds 

bayesian statistics 
oxford university press pp 

tierney kadane 

accurate approximations posterior moments marginal densities 
journal american statistical association 
titterington smith makov 

statistical analysis finite mixture distributions 
chichester john wiley sons 
van laarhoven aarts 

simulated annealing theory applications 
dordrecht reidel 


likelihood ratio tests model selection non nested hypotheses 
econometrica 
learning graphical models werbos su 

neural networks system identification control chemical process industry 
white 
eds handbook intelligent control pp 

van nostrand reinhold 
wermuth lauritzen 

substantive research hypotheses conditional independence graphs graphical chain models 
journal royal statistical society 
whittaker 

graphical models applied multivariate statistics 
wiley 
wolpert 

bayesian backpropagation functions weights 
tesauro 
ed advances neural information processing systems nips 
morgan kaufmann 
