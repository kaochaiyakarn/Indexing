reducing memory traffic requirements scalable directory cache coherence schemes anoop gupta wolf dietrich weber todd mowry computer systems laboratory stanford university ca multiprocessors scaled single bus systems renewed interest directory cache coherence schemes 
schemes rely directory keep track processors caching memory block 
write block occurs pointto point invalidation messages sent keep caches coherent 
straightforward way recording identities processors caching memory block bit vector memory block bit processor 
unfortunately main memory grows linearly number processors total size directory memory grows square number processors prohibitive large machines 
remedy problem schemes limited number pointers directory entry suggested 
schemes cause excessive invalidation traffic 
propose simple techniques significantly reduce invalidation traffic directory memory requirements 
coarse vector novel way keeping directory state information 
scheme uses little memory limited pointer schemes causes significantly invalidation traffic 
second propose sparse directories directory entry associated memory blocks technique greatly reducing directory memory requirements 
presents evaluation proposed techniques context stanford dash multiprocessor architecture 
results indicate sparse directories coupled coarse vectors save orders magnitude storage slight degradation performance 
critical design issue shared memory multiprocessors cache coherence scheme 
contrast snoopy schemes directory schemes provide attractive alternative scalable high performance multiprocessors 
schemes directory keeps track processors cached memory block 
processor wishes write block directory sends point point messages processors copy invalidating cached copies 
number processors increased amount state kept directory increases accordingly 
large number processors memory requirements keeping full record processors caching memory block prohibitive 
earlier studies suggest memory blocks shared processors time number blocks shared large number processors small 
observations point directory organizations optimized keep small number pointers directory entry able accommodate blocks pointers 
propose methods lowering invalidation traffic directory memory requirements 
coarse vector directory scheme 
common case block shared small number processors directory kept form pointers 
points processor cached copy 
number processors sharing block exceeds number pointers available directory switches different representation 
memory store pointers treated coarse bit vector bit state indicates group processors 
term new directory scheme dir number pointers size region bit coarse vector represents 
bits set equivalent broadcast achieved 
amount memory proposed scheme limited pointer scheme broadcast dir 
second method propose reduces directory memory requirements organizing directory cache having directory entry memory block 
total size main memory machines larger cache memory time memory blocks cached processor corresponding directory entries empty 
idea sparse directory contains active entries appealing 
furthermore need backing store directory cache 
state block safely discarded invalidation messages sent processor caches copy block 
scheme sparse directories brings storage requirements main memory directories close cache linked list directory schemes sci scheme 
avoid longer latencies complicated protocol associated cache directories 
note proposals orthogonal 
sparse directories apply equally directory entry formats coarse vector scheme 
compare full bit vector scheme existing limited pointer schemes coarse vector scheme 
evaluate performance sparse directories 
performance results obtained multiprocessor simulations parallel applications 
multiprocessor simulator stanford dash architecture 
results show coarse vector scheme schemes robust response different applications 
applications cause directory scheme degrade badly coarse vector performance close full bit vector scheme 
sparse directories adds traffic reducing directory memory overhead orders magnitude 
section briefly introduces dash multiprocessor architecture currently developed stanford 
base architecture studies 
dash architecture section followed background information 
processor cache processor cache 
memory remote access cache directory memory processor cache processor cache 
memory remote access cache directory memory dash architecture 
directory cache coherence schemes emphasis memory requirements scheme 
section introduces directory schemes proposed 
section describes experimental environment parallel applications performance evaluation studies 
section presents results studies 
sections contain discussion results 
dash architecture performance analysis different directory schemes depends implementation details multiprocessor architecture 
schemes concrete evaluating context dash multiprocessor currently built stanford 
section gives brief overview dash 
dash architecture consists processing nodes referred clusters interconnected mesh network see 
processing node contains processors caches portion global memory corresponding directory memory controller 
caches clusters kept consistent bus snoopy scheme 
inter cluster consistency assured directory cache coherence scheme 
dash prototype currently built total processors arranged clusters 
prototype implementation uses full bit vector directory entry 
state bit cluster single dirty bit corresponding directory memory overhead bits byte main memory block 
follows brief description protocol messages sent typical read write operations 
information useful understanding message traffic results section 
read cluster read initiated local cluster sends message cluster contains portion main memory holds block home cluster 
directory determines block clean shared sends response local cluster 
block dirty request sent owning cluster replies directly original requestor 
write local cluster sends message home cluster 
directory look occurs appropriate invalidations sent clusters having cached copies remote clusters 
time ownership reply returned local cluster 
reply contains count invalidations sent equals number messages expect 
invalidations reaches destination invalidation messages sent local cluster 
received local cluster write complete 
directory schemes cache coherence existing cache coherent multiprocessors built bus snoopy coherence protocols 
snoopy cache coherence schemes rely bus broadcast medium caches snoop bus keep coherent 
unfortunately bus accommodate small number processors machines scalable 
scalable multiprocessors require general interconnection network scalable bandwidth snooping impossible 
directory cache coherence schemes offer attractive alternative 
schemes directory keeps track processors caching memory block system 
information selectively send invalidations updates memory block written 
directory schemes successful scalable multiprocessors satisfy requirements 
bandwidth access directory information scale linearly number processors 
achieved distributing physical memory corresponding directory memory processing nodes scalable interconnection network 
second requirement hardware overhead directory scheme scale linearly number processors 
critical component hardware overhead amount memory needed store directory information 
second aspect directory schemes focus 
various directory schemes proposed fall broad classes full bit vector scheme ii limited pointer schemes iii cache linked list schemes 
examine directory schemes classes qualitatively discuss scalability performance advantages disadvantages 
quantitative comparison results section 
full bit vector scheme dir bit vector bit processor block main memory 
directory contains dirty bit memory block indicate processor exclusive access modify block cache 
bit indicates memory block cached corresponding processor directory full knowledge processors caching block 
block invalidated messages sent processors caches copy 
terms message traffic needed keep caches coherent best invalidation directory scheme 
unfortunately multiprocessor processor block size bytes directory memory requirements square number processors 
fact full bit vector schemes unacceptable machines large number processors 
asymptotic memory requirements look formidable full bit vector directories quite attractive machines moderate number processors 
example prototype stanford dash multiprocessor consist processors organized clusters processors 
snoopy scheme intra cluster cache coherence full bit vector directory scheme inter cluster cache coherence 
block size bytes need bit vector block keep track clusters 
overhead directory memory fraction total main memory quite tolerable dash multiprocessor 
observe way reducing overhead directory memory increase cache block size 
certain point practical approach increasing cache block size undesirable side effects 
example increasing block size increases chances false sharing may significantly increase coherence traffic degrade performance machine 
limited pointer schemes study parallel applications shown kinds data objects corresponding memory locations cached small number processors time 
exploit knowledge reduce directory memory overhead restricting directory entry small fixed number pointers pointing processor caching memory block 
important implication limited pointer schemes exist mechanism handle blocks cached processors number pointers directory entry 
alternatives exist deal pointer overflow discuss 
depending alternative chosen coherence data traffic generated may vary greatly 
limited pointer schemes need log bit sufficed point processor full bit vector scheme 
full bit vector scheme effective bits 
ignore single dirty bit directory memory required limited pointer scheme pointers grows log number processors 
limited pointers broadcast scheme dir pointer overflow problem adding broadcast bit state information block 
pointer overflow occurs broadcast bit set 
subsequent write block cause invalidations broadcast caches 
invalidation messages go processors copy block reduce performance delaying completion writes wasting communication bandwidth 
dir scheme expected poorly typical number processors sharing block just larger number pointers case numerous invalidation broadcasts result invalidations going caches copy block 
limited pointers broadcast scheme dir nb way avoid broadcasts disallow pointer overflows altogether 
dir nb scheme room additional requestor invalidating caches sharing block 
manner block caches time write cause invalidations 
serious degradation performance scheme occurs application read read data objects actively shared large number processors 
data read continuous stream invalidations result objects cache attempt share caches 
special provisions handle widely shared data performance severely degraded section presents example 
superset scheme dir way dealing pointer overflow superset dir scheme terminology suggested 
scheme pointers kept entry 
pointers exhausted memory keep single composite pointer 
bit composite pointer assume states denotes 
entry added bit pattern compared existing pointer 
bit patterns disagree pointer bit flipped state 
write occurs invalidations sent composite pointer expanded states 
set pointers processor caches result superset caches copies block 
unfortunately composite pointer representation produces lot extraneous invalidations 
section show superset scheme marginally better broadcast scheme accurately capturing identities processors caching copies block 
cache linked list schemes different way addressing scalability problem full vector directory schemes keep list pointers processors caches directory memory 
scheme currently formalized scalable coherent interface 
directory entry doubly linked list 
head tail pointer list kept memory 
cache copy block item list forward back pointer remainder list 
cache wants read shared item simply adds head linked list 
write shared block occur list copies caches invalidated 
advantage scheme scales naturally number processors 
processors added total cache space increases space keep directory information 
unfortunately disadvantages 
thing protocol required maintain linked list directory entry complicated protocol memory directory scheme directory updates performed atomically 
secondly write produces serial string invalidations linked list scheme caused having walk list cache cache 
contrast memorybased directory scheme send invalidation messages fast network accept 
thirdly memory directory operate main memory speeds cheap dense dram linked list needs maintained expensive high speed cache memory 
exploration tradeoffs memory cache directories currently active area research 
focus memory directories dash architectures 
dir dir cv dir processors number sharers dir dir dir cv dir processors number sharers average invalidation messages sent function number sharers 
new proposals propose techniques reduce memory requirements directory schemes significantly compromising performance communication requirements 
coarse vector scheme combines best features limited pointer full bit vector schemes 
second technique sparse directory uses cache backing store 
coarse vector scheme dir disadvantages limited pointer scheme losing advantage reduced memory requirements propose coarse vector scheme dir number pointers size region bit coarse vector represents 
dir identical limited pointer schemes processors sharing block 
pointers stores identity processor caching copy block 
pointer overflow occurs semantics switched memory storing pointers store coarse bit vector 
bit bit vector stands group processors 
region size determined number directory memory bits available 
accuracy lost full bit vector representation forced throw entries dir nb go broadcast immediately dir 
different behaviour broadcast coarse vector schemes apparent 
graph assume limited pointer schemes pointers 
graph shows average number invalidations sent write shared block number processors sharing block varied 
invalidation event sharers randomly chosen number invalidations required recorded 
large number events invalidation figures averaged plotted 
ideal case full bit vector line number invalidations identical number sharers 
schemes full knowledge sharers extraneous invalidations need sent 
areas line full bit vector scheme lines schemes represent number extraneous invalidations scheme 
dir scheme go broadcast soon pointers exhausted 
results extraneous invalidations 
dir scheme uses composite pointer pointer overflow occurs graph shows behaviour bad broadcast scheme 
composite vector soon contains xs close broadcast bit 
coarse vector scheme hand retains rough idea processors cached copies 
able send invalidations regions processors containing cached copies having resort broadcast 
number extraneous invalidations smaller 
coarse vector scheme advantages multiprogramming environments large machine divided users 
user set processor regions assigned application 
writes user processor space cause invalidation messages sent caches users 
single application environments take advantage data locality placing processors share data set processor region 
sparse directories typically total amount cache memory multiprocessor total amount main memory 
directory state kept entirety entry memory block 
blocks cached corresponding directory entries empty 
reduce waste memory propose sparse directory 
directory cache needs back store safely replace entry sparse directory invalidating processor caches entry points 
example machine mbytes main memory processor kbytes cache memory processor directory entries time 
directory cache suitable size able drastically reduce directory memory 
machine cost lowered designer choose spend saved memory making entry wider 
example dir scheme sparse directory pointers smaller regions result 
directory cache size chosen large total number cache blocks 
additional factor reduce probability contention sparse directory entries memory access patterns skewed load directory heavily 
contention occurs memory blocks mapping directory entry exist processor caches table sample machine configurations 
number number total main total processor block directory directory clusters processors memory space cache space size scheme overhead mbytes mbytes bytes dir sparse dir sparse dir cv keep sparse directory 
similar reasoning provides motivation making sparse directory set associative 
sparse directories contain large fraction main memory blocks tags need bits wide 
sparse directories expected particularly dash style architecture 
dash directory entries data memory module cached processors cluster 
expect processes allocate non shared data memory cluster directory entries data 
furthermore increasing locality programs fewer data items remotely allocated fewer directory entries needed 
ratio main memory blocks directory entries called sparsity directory 
directory contains entries main memory blocks sparsity 
table shows possible directory configurations machines different sizes 
machines mbytes main memory kbytes cache allocated processor 
directory memory overhead allowed 
processors clustered processing nodes similar dash 
line table close dash prototype configuration 
processors arranged clusters processors 
machine full bit vector scheme dir easily feasible 
machine scaled processors keep directory memory overhead level switching sparse directories 
sparse directories contain entries main memory blocks sparsity 
shall see section sparser directories perform 
processor machine directory memory overhead kept constant entry size kept manageable coarse vector scheme dir cv addition directory sparsity 
note achieved having resort larger cache block size 
evaluation methodology evaluated directory schemes discussed previous sections event driven simulator stanford dash architecture 
studying execution time various applications looked amount type message traffic produced different directory schemes 
simulations utilized tango generate multiprocessor 
tango allows parallel application executed uniprocessor keeping correct global event interleaving intact 
global events shared data synchronization events lock unlock requests 
tango generate multiprocessor traces coupled memory system simulator yield accurate multiprocessor simulations 
case memory system simulator returns timing information generator preserving valid interleaving 
second method simulations 
study uses benchmark applications derived different application domains 
lu comes numerical domain computes factorization matrix 
medical domain string matching program search gene databases 
mp comes aeronautics 
dimensional particle simulator study upper atmosphere 
commercial quality standard cell routing tool vlsi cad domain 
table general application characteristics 
shared shared shared sync shared refs reads writes ops space application mill mill mill thou mbytes lu mp table presents general data applications 
shows total number shared application run breakdown reads writes 
shared defined globally shared data sections applications 
number shared varied slightly run run non deterministic applications mp 
show values full cache non sparse full bit vector runs 
table gives amount shared data touched execution estimate data set size program 
runs done processors cache block size bytes 
processors currently applications achieve speedup processors 
evaluation studies assumed directory memory overhead tolerable allowed bits directory memory entry 
restricts limited pointer schemes pointers coarse vector scheme regions size 
schemes examined study dir cv dir dir nb 
dir full bit vector scheme comparison purposes 
sparse directories introduced overhead naturally drops dramatically orders magnitude depending sparsity 
example full bit vector directory sparsity requires bits keep track processor caches dirty bit bits tag 
bits byte block bits blocks savings factor 
dash simulator configured parameters correspond dash prototype hardware 
processors kbyte primary kbyte secondary caches 
local bus requests take order processor cycles 
remote requests involving clusters take cycles remote invalidations number invalidation events average invalidations event invalidation distribution dir invalidations number invalidation events average invalidations event invalidation distribution dir nb 
requests clusters latency processor cycles 
simulator main memory evenly distributed clusters allocated clusters round robin scheme 
messages classes simulator caches request data ownership 
reply messages sent directories ownership send data 
invalidation messages sent directories invalidate block 
messages sent caches response invalidations 
simulator collects statistics distribution number invalidations sent write request 
invalidation distribution helps explain behaviour different directory schemes 
simulation results results section subdivided follows 
subsection gives invalidation distributions differ invalidations percentage invalidation events number invalidation events average invalidations event invalidation distribution dir invalidations percentage invalidation events number invalidation events average invalidations event invalidation distribution dir cv ent directory schemes 
impart intuitive feel different schemes behave discusses advantages disadvantages 
subsections results main study 
contrasts performance coarse vector scheme limited pointer schemes 
second subsection presents results regarding effectiveness sparse directories 
invalidation distributions figures give invalidation distributions shared data application 
results applications space reasons 
distributions illustrate trends different schemes 
see distribution full bit vector scheme dir intrinsic invalidation distribution best achieved 
case dir scheme writes hit clean block invalidation events 
note writes cause invalidations writes cause large number invalidations 
number invalidation events event average causes invalidations total invalidations 
shows invalidation distribution dir nb 
broadcasts allowed caches share acks exec 
time full vector coarse vector broadcast non broadcast replies requests performance lu 
acks exec 
time full vector coarse vector broadcast non broadcast replies requests performance 
block time 
means see invalidations write 
unfortunately new single invalidations caused replacements block wants shared caches 
dir nb possible reads cause invalidations number invalidation events larger 
average number invalidations event decreased total number invalidations increased 
distribution dir shown 
see number smaller invalidations goes back level seen full vector scheme 
writes caused invalidations full vector scheme broadcast invalidations 
broadcasts clusters invalidated home cluster new owning cluster require invalidation 
serves drive average invalidations event total invalidations 
dir cv scheme shown able respond larger invalidations resorting broadcast 
peaks odd numbers invalidations caused granularity bit vector 
note absence large peak invalidations right edge broadcast scheme 
average invalidations event total invalidations 
see broadcast non normalized execution time traffic acks exec 
time full vector coarse vector broadcast non broadcast replies requests performance mp 
normalized execution time traffic acks exec 
time full vector coarse vector broadcast non broadcast replies requests performance 
broadcast schemes cause invalidation traffic increase 
case broadcast scheme increase due broadcast invalidations relatively frequent small number pointers 
non broadcast scheme extra invalidations caused replacing entries caches sharing block pointers available 
coarse vector scheme strikes balance avoiding drawbacks able achieve performance closer full bit vector scheme 
performance different directory schemes figures show performance achieved data coherence messages produced different directory schemes applications 
runs processors kbyte primary kbyte secondary caches cache block size bytes 
total number messages broken requests include replies invalidation messages 
observe number request reply messages schemes dir dir application 
expected schemes similar request reply behaviour 
dir dir oc send extraneous invalidations difference compared full bit vector scheme 
dir sent read requests pointer overflow occurs 
invalidations cause additional read misses associated increase request reply messages 
look applications individually discuss results 
lu exhibits problem discussed previous paragraph 
see greatly increased number request reply messages large number invalidation messages dir nb scheme 
lu matrix column read processors just pivot step 
data actively shared processors dir nb poorly 
read shared data cause poorer performance dir nb 
pattern library arrays constantly read processes run 
schemes virtually indistinguishable 
mp data shared just processors time 
sharing pattern causes invalidation distribution schemes handle 
coarse vector broadcast schemes show increase execution time message traffic non broadcast scheme takes longer run 
interesting application dir nb scheme outperforms dir central data structure shared processors working geographical region 
number sharers exceeds number pointers dir broadcast results write 
dir nb scheme better kind object invalidations due pointer overflow cause re reads 
section message traffic numbers diverge execution times various schemes 
simulate cluster multiprocessor processors processor cluster 
local cluster bus underutilized 
real dash system processors cluster cluster bus 
consequently expect performance degradation due increased number messages larger shown 
comparing performance different schemes various applications see dir nb worse schemes applications 
perform better schemes 
secondly expect dir scheme perform broadcast scheme see significantly better applications 
note coarse bit vector scheme sends extraneous messages 
worst case application dir sends messages ideal full bit vector scheme 
performance sparse directories method evaluating sparse directories similar evaluate different directory schemes 
key differences 
firstly simulator configured sparse directory keeping complete directory 
secondly scaled processor caches achieve realistic size relationship sparse directories processor caches 
slow speed simulator limited relatively small application data sets 
result regular kbytes cache processor data set fit caches 
case unable experiment sparse directories larger processor caches smaller total memory blocks system 
caches scaled keep ratio data set size cache size runs similar data set size cache size full blown application problem real dash multiprocessor 
example full blown problem processor dash occupy gbyte main memory see table 
times total cache space 
simulation data set size mbytes 
preserve data set cache ratio total cache space processor simulation reduced kbytes kbytes processor 
experimented sparse directories entries times total number cache lines system shown size factor graphs 
entry needs allocated sparse directory look see slot maps empty 
filled 
replace existing entry 
invalidations sent empty slot filled 
empty slots created processor cache replaces writes back dirty line 
effect sparsity figures show effect directory sparsity performance 
chose results lu 
results mp similar lack space omit 
full scale runs data set expected small sparse directories perform non sparse directories 
omit results subsection 
figures show execution times lu directory sparsity varied 
consider cases number directory entries system factor times total number cache blocks system 
runs sparse directories associativity random replacement policy see 
results suggest directories size processor caches perform 
worst case application lu shows increase execution time going non sparse full bit vector directory sparse directory equal size processor caches 
directory size increased times cache size performance degradation sparse directories small 
size factor normalized execution time full bit vector non sparse coarse vector broadcast sparse directory performance lu 
size factor directory lu see large performance difference coarse vector broadcast schemes 
size factor full bit vector non sparse coarse vector broadcast sparse directory performance 
lu pivot column shared processors 
directory replacements frequent case sparse directories processes may get chance access data replacements 
replacement occur sharers exist cause broadcast dir needs send invalidations 
performance fairly flat schemes size factors 
performance vary scheme scheme invalidation behaviour handled equally schemes 
performance flat size factors wave front algorithm relatively small working set moment time 
ensures sparse directories suffer excessive replacements 
effect associativity replacement policy sparse directory fewer entries main memory blocks possible active blocks map directory entry 
set associative sparse directory handle situation entries direct mapped sparse directory keep bumping leading poor directory performance 
acks associativity size factor replies requests effect associativity sparse directory lu 
lu sample application study effect sparse directory associativity replacement policy 
full bit vector scheme studies 
shows message traffic numbers directory size factors 
show traffic numbers show trends better execution time results 
size factors associativity equal slightly better associativity turn better direct mapped larger margin 
benefits set associativity small expect associativity sparse directories robust different application behaviours 
normalized traffic acks repl policy lru rand lra lru rand lra lru rand lra size factor replies requests effect replacement policies sparse directory lu 
set associative directories choice replacement policies 
explored random lru allocated lra schemes 
lru keeps different sets entry ordered time access replaces 
lra keeps track allocation time set entry replaces allocated 
results lu run sparse directory set associativity full bit vector scheme shown 
lru difficult implement performs best 
random easiest implement hardware better lra 
lra possibility replacing entries allocated early frequently exists 
soon leads replacements frequently entries accessed 
discussion question arises proposals introduce additional complexities architecture 
answer 
coarse vector scheme require modification protocol full bit vector scheme 
merely ends sending extraneous invalidations 
sparse directories hand protocol modification required 
entry replaced sparse directory effectively removed system invalidate copies corresponding memory block cached processor caches 
entity keep track invalidations received 
entity exist systems implement weak consistency order keep track outstanding invalidations 
dash remote access cache rac 
block replaced sparse directory rac allocates entry block invalidations sent cached copies 
rac receives messages sent response invalidations 
operation complete received 
hardware issue concerns synchronization 
dash directory bit vectors keep track processors queued lock 
case full bit vector space keep track nodes 
consequently lock released granted exactly waiting nodes 
switch coarse vector scheme longer case 
able keep track processor regions queued lock 
lock released wish node release processors region try regain lock 
mechanism slightly efficient avoids having release waiting processors causing hot spot try obtain lock 
techniques reduce memory requirements directory cache coherence schemes 
example suggested associate small directory entries memory block allow overflow small cache wider entries 
similarly multiple memory blocks share wide entry 
plan evaluate alternative schemes 
techniques reducing memory overhead data coherence traffic directory cache coherence schemes coarse vector scheme sparse directory scheme 
performance new schemes analysed compared existing directory schemes 
results show savings achieved memory overhead traffic reduction significant 
depending application coarse vector scheme produces memory message traffic best limited pointer scheme factors worst limited pointer scheme 
coarse vector scheme robust limited pointer schemes performance closest full bit vector scheme 
sparse directories add memory coherence traffic significantly reduce directory memory overhead orders magnitude depending sparsity 
believe combination techniques allow machines scaled hundreds processors keeping directory memory overhead reasonable 
helen davis steven goldschmidt creating supporting tango 
dan lenoski jim laudon kourosh gharachorloo provided insightful discussions 
dan lenoski provided 
people patiently stood brought machines knees runs 
lastly henk jim laudon dan lenoski margaret martonosi ed rothberg mike smith reviewing early version 
anoop gupta todd mowry supported darpa contract 
wolf dietrich weber supported ibm graduate fellowship 
anant agarwal richard simoni john hennessy mark horowitz 
evaluation directory schemes cache coherence 
th international symposium computer architecture 
james archibald jean baer 
cache coherence protocols evaluation multiprocessor simulation model 
acm transactions computer systems 
james archibald 
cache coherence problem shared memory multiprocessors 
phd thesis department computer science university washington february 

new solution coherence problems systems 
ieee transactions computers december 
davis goldschmidt hennessy 
tango multiprocessor simulation tracing system 
stanford technical report preparation 
eggers katz 
effect sharing cache bus performance parallel programs 
proceedings third international conference architectural support programming languages operating systems pages may 
encore computer 
multimax technical summary 
working group 
part iiia sci cache coherence overview 
technical report revision ieee computer society november 
tom knight march 
talk stanford computer systems laboratory 
lenoski laudon gharachorloo gupta hennessy 
directory cache coherence protocol dash multiprocessor 
proceedings th international symposium computer architecture 
dan lenoski james laudon kourosh gharachorloo anoop gupta john hennessy mark horowitz monica lam 
design scalable shared memory multiprocessors dash approach 
proceedings compcon pages 
tom 
symmetry multiprocessor system 
proceedings international conference supercomputing pages 
patel 
low overhead coherence solution multiprocessors private cache memories 
proceedings th international symposium computer architecture pages 
tang 
cache design tightly coupled multiprocessor system 
afips conference proceedings national computer conference ny ny pages june 
wolf dietrich weber anoop gupta 
analysis cache invalidation patterns multiprocessors 
proceedings third international conference architectural support programming languages operating systems pages april 
john willis 
cache coherence systems parallel communication channels processors 
technical report tr philips laboratories march 
