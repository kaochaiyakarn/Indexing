scaling beowulf class distributed systems john salmon christopher stein thomas sterling beowulf class systems employ inexpensive commodity processors open source operating systems communication libraries commodity networking hardware deliver supercomputer performance lowest possible price 
small medium sized beowulf systems installed planned dozens universities laboratories industrial sites world 
design space larger systems largely unexplored 
investigate interconnection techniques allow scaling beowulf class system unprecedented sizes hundreds thousands processors 
commercially available high speed backplane fast ethernet gigabit ethernet switch modules 
second network combines point point connections commodity switches employing beowulf nodes ip routers computational engines 
method drawback relatively high price tag significant overwhelming fraction cost system devoted switch 
second approach inexpensive introduces extra latency may restrict bandwidth depending topology employed 
tests designed quantify effects 
perform sets tests 
low level tests measure hardware performance individual components bandwidths latencies various combinations network interface cards node routers switches 
system level tests measure bisection bandwidth ability system communicate compute simultaneously conditions designed generate contention heavy loads 
application benchmarks measure performance delivered subset nas parallel benchmark suite 
low level measurements indicate latencies weakly affected node routers bandwidths completely unaffected 
switches introduce microseconds latency linux routers introduce microseconds 
system level measurements indicate load routed network delivers bisection bandwidth near expect simply summing capacities individual components 
large scatter introduced latencies 
application level measurements reveal routed network acceptable applications significant deleterious effect 
successful routed networks large beowulf systems requires deeper understanding issue 
years clusters pcs including beowulf class systems emerged important rapidly expanding approach parallel computing 
beowulf class systems natural consequence trends computing powerful inexpensive mass market microprocessors powerful robust freely available operating systems widespread adoption messagepassing apis engineering scientific supercomputing applications high speed low cost networking 
price performance ratios sustained execution achieved mflops favorable non trivial scientific engineering applications 
linpack performance gflops reported gflops sustained scientific applications 
beowulf class forms clustered pc systems established new platform accomplishing real world moderate scale computation science engineering commerce 
beowulf class systems threshold supercomputers systems larger dozen processors experimental 
beowulf class pc clusters large scale applications determined issues architectural scalability development latency tolerant applications software environments tools 
focuses 
despite success small processor systems processors installed design systems largely unexplored 
machines constructed effectively applied low cost cots technology established 
consider scalability system designs capable integrating hundreds thousands processors 
study systems component level system level perspective application performance 
low level synthetic benchmarks real world application benchmarks employed characterize network components hardware software system scaling characteristics 
purpose take initial look possible implementation strategies large scale beowulf class systems 
beowulf designs necessarily highly restricted 
individual components constrained characteristics available commodity networking systems 
particular choices driven characteristics fast ethernet technologies widely adopted systems due excellent cost adequate performance 
practical scalable networks ultimate network offer high performance reliability inexpensive scalable linearly increasing cost thousands processors significant degradation performance 
unfortunately network exist time soon 
fast ethernet constitutes significant compromise greatest advantage low cost greatest drawbacks high latency modest bandwidth limited scalability 
appropriate include detailed tutorial fast ethernet technology 
details crucial understanding networks propose 
refer computational elements system nodes 
practically speaking node single motherboard memory bus may processors 
nodes equipped network interface controllers nics 
quality nics cost send receive data simultaneously mbit latencies neighborhood microseconds 
nodes connected ways point point inexpensive cable simply connects ports nics 
hub inexpensive component serves shared medium connects nics 
nics connected hub may communicate compete common mbps medium 
switched switches similar hubs allow pair time communicate interference 
wide variety switches commercially available sizes ranging ports 
denote number ports switch price port ethernet switches increases dramatically increasing small switches fewer ports cost port 
larger switches constructed expensive pluggable modules cost considerably port 
switches de facto standard connection component small beowulf systems 
switches connected little difficulty architectural restrictions ethernet resulting network may contain closed loops 
severely limits ability construct network purely interconnected switches 
possible configuration shown 
expensive configuration capable scaling beowulf systems multiples size individual switch 
cost switches ports root switch ports interconnect nodes way processor bisection bandwidth top switch uplink bandwidth 
possible solution problem higher bandwidth technology links switches 
fact switches available today offer small number typically gigabit addition standard mbps ports 
gigabit ethernet new technology reason believe larger gigabit switches available moderate cost commodity components coming years 
suggest mbit switches gigabit offer path scalable beowulf systems 
unfortunately time frame processor performance increase exploit gigabit nics switches nodes beowulf systems 
point gigabit offer advantage quickly saturated small fraction non local communication 
root switch leaf switch nodes tree switches configuration showing root switch connecting leaf switches links nodes 
principle different higher speed technology 
moment mbit gigabit practical see gigabit ethernet distant 
routed networks second way overcome limitation closed loops routing 
router interposed switches isolated run spanning tree algorithm effectively enforces closed loops rule 
dedicated standalone routers expensive possible computer node router 
nodes beowulf act routers network 
simply installs nic routing node os directed forward messages data nics 
routing entails software overhead manifested additional latency associated multi hop messages 
may impact rate intermediate nodes carry user level computations 
ability route nics node opens enormous class network topologies consideration 
key features topologies follows 
assume switch ports 
note nics expensive switch ports factor restrict attention networks node nic connected switch nics connected directly nics processors point point links 
general case allow nic processor connected switch ports different switches offers greater flexibility increase cost 
upper graph representing routed network consisting nodes arranged groups nodes 
lower shows detail groups 
node nics available communicate groups weight edge graph channels available groups 
bisection bandwidth implied graph obtained noting bisection cuts links weight cuts single vertex half internal bandwidth connections 
node bisection bandwidth 
bisection bandwidth available node slightly half single dedicated unshared connection 
networks abstracted 
notice switch effectively defines meta node form traditional beowulf system switch collection nodes 
addition normal internal connections meta node external connections connect meta nodes 
neglect differences arise choices exactly nodes connected meta nodes describe general case weighted graph consisting vertices corresponding edges weighted sum weights incident vertex equal weight edge corresponds number point point links exist vertices meta nodes connected edge 
total number nodes system course fully connected routed network nodes illustrated 
interesting examine diameter bisection bandwidth implied network 
diameter maximum number edges vertices 
related maximum latency experienced messages traversing system 
messages routed vertex vertex incurring additional latency hop 
low diameter networks probably preferable 
fortunately degree vertices large little practical value studying graphs diameter greater 
communication networks characterized bisection bandwidth maximum bandwidth available arbitrarily chosen halves system 
correspondence weight edge graph number individual network connections linking vertices allows compute theoretical bisection bandwidth summing weights edges 
simplest graphs consider homogeneous fully connected meta node connected equal number pointto point links 
weight edge edges crossing bisection 
bisection bandwidth node 
diameter course 
bisection bandwidth node bandwidth single point point connection suggesting bandwidth limited applications may able scale smaller beowulf systems moderate degradation performance 
fully connected network larger size system limited theoretically scalable thousands processors currently available commodity components 
course host problems bear viability large systems principle 
measurements report classes measurements designed evaluate network choices perspective elementary building block performance aggregate network performance application performance 
elementary network components run series low level tests designed measure latency bandwidth switched networks commodity components 
particular wish measure latency bandwidth communication variety paths routed multiple switches nodes 
may expect latency grow intermediate elements introduced rate growth needs established 
expect bandwidth insensitive intermediate elements needs quantitatively established 
constructed small set elementary mini networks run selection lmbench 
suite networking tests 
particular measured tcp udp latencies socket bandwidths table shows results lmbench 
network configurations chosen constitute elementary components larger point point networks consideration 
measured performance lucent switches maximum internal switching capacity gbit available modular slots 
modules switched fast ethernet ports linked switches gigabit ethernet modules 
test network unloaded processors involved test 
tests reveal best case behavior contention resources 
data table closely fit rules udp latency bay networks linux routers tcp latency udp latency bandwidth mbyte single lucent switch introduces additional latency second adds 
aggregate network measurements factors undermine ability predict application performance latency bandwidths reported lmbench 
account additional layers overhead imposed application level communication software mpi 
second contention resources result significantly degraded performance system run load 
code synthetic load generation implemented heavily instrumented synthetic load generator mpi attempt assess effects 
code fairly compact compile difficult ansi mpi platform 
included appendix wish try wish know exactly reporting section 
carries mixture floating point calculation network communication connection tcp latency udp latency socket bandwidth mbyte table latencies bandwidths reported lmbench 
connection types abbreviated follows communication point mhz running linux linux kernel ip routing bay networks switch lucent switch 
link lucent switches implemented links provided gigabit ethernet modules 
uses hardware cycle counters available measure time intervals low granularity cycles 
communication pattern fixed processors communicate chosen neighbor random random list partners drawn measurements commence communication cycles list 
message lengths lengths intervals floating point computation set command line 
range chosen values uniformly distributed logarithm plots effectively cover wide dynamic range 
experience shown distribution times highly irregular simple statistical aggregations arithmetic means linear regressions grossly misleading obscure significant features true behavior system 
program simply reports time interval measures responsibility post processing plot characterize results 
figures section scatterplots time vs message length ping pong message transfers carried processes test configurations 
process sent received approximately messages varying sizes 
message traffic time displayed vertical axis half back time measured code estimate time required unidirectional message length 
processes running simultaneously subject ability ensure starting measurements immediately call mpi barrier 
hardware testbed testbed system consists nodes 
node fast ethernet interface cards 
interface eth connects node switch 
second interface eth connects port port bay networks switch 
nodes distributed evenly bay switches 
third interface eth connects node directly node eth point point link 
point point eth connections 
nodes grouped switch groups bay switch connected 
group nodes connected point point eth nodes switch group 
bay switch remote switch nodes connected node eth 
connected node connected gateway nodes distributed connected nodes source connected nodes gateways non connected nodes 
source destination ip packet traverse gateways 
remote nodes system reachable directly hop reachable hops reachable hops 
script configures kernel ip routing table 
correctness routing verified sequence traceroute tests analysis routing table 
basic configurations study different classes networks ffl routed network described point point nic node meta nodes consisting bay networks switch pentium pro nodes 
ffl network consisting lucent switches connected pair fiber links 
operated system configurations nodes connected switch nodes connected switch connected 
results shows mpi performance different fixed patterns communication switches communication takes place single switch communication takes place gigabit fiber links connecting switches 
reassuring configurations appear indistinguishable confirming trunk connection switches bottleneck 
addition indication conventional formula relating communication time message length may inadequate comm latency length bandwidth solid line plots equation latency bandwidth mb clearly line capture nuances actual data 
features actual data multiples bytes related ethernet mtu maximum amount data transmitted single ethernet frame 
feature kbytes may cache related 
time vs message length different fixed communication patterns switches 
black dots represent configuration nodes connected switch 
red dots configuration nodes paired member pair switch member pair switch 
communication take place gigabit fiber connection switches 
colors dots virtually indistinguishable 
blue line shows time predicted equation latency bandwidth mb interesting features probably great significance application performance 
greater importance spread latencies short messages frequently factor occasional outliers seconds cases 
shows effect random communication pattern 
configurations nodes switch nodes divided equally switches point sets indistinguishable 
case new branch latencies neighborhood 
addition scatter common branch latencies clearly misleading characterize data described single latency bandwidth 
turn attention routed networks discussed section 
shows results separate runs different fixed communication pattern 
different communication patterns exercise pointto point links switch links hop hop virtual connections 
hop hop connections correspond eb lines table 
marked differences different classes connection 
latency higher multi hop connections bandwidth considerably reduced 
furthermore considerably spread actual times messages lengths 
reduction bandwidth surprising established section maximum theoretical node bisection bandwidth network single mbps link 
see performance mbps test code transfers data direction time nodes hardware transfer mbps directions simultaneously 
shows result performing random communication routed network 
colors show behavior interleaved floating point calculation 
apparently indistinguishable indicating minimal degradation communication performance intermediate processors may busy user tasks 
furthermore results similar different classes connections clearly visible highlighted color 
just second branch apparent latency rare outliers require sec 
features occur graphs probably unrelated particular network result interactions processes system subtle details mpi tcp implementations 
surprising user level computations impact communication 
converse unfortunately case 
shows effect routing computational performance 
black dots represent rate processors evaluate logistic function tight unrolled pipelined loop running mflop communication 
red dots show effect communication routing 
computations frequently interrupted occasionally 
time vs message length random communication patterns different network configurations 
black dots represent configuration nodes connected switch 
red dots configuration nodes split equally switches 
colors dots virtually indistinguishable branch values latencies approaching 
time vs message length fixed communication patterns routed network 
colors represent different ways pairing nodes 
black dots show times point point links 
red dots show times nodes switch switches odd number processors point point links included 
green dots show times nodes exactly hops apart point topoint link switch link 
blue dots pairings involving maximally separated nodes switch links point point link 
solid lines correspond bandwidths mb lower mb upper 
time vs message length random communication patterns routed network 
colors represent times run communication black separate run red processors alternate communication floating point calculations 
time vs number floating point operations routed networks red black communication traffic 
class class class test np np np routed routed cg ep ft lu mg table results selected nas parallel benchmarks version 
values mops processor reported benchmark suite 
entries marked complete unknown reasons 
long computations sec suffer performance degradation effect worse short bursts computation 
application level benchmarks ran nas parallel benchmarks network routed network 
tests performed unmodified source code version npb obtained nas web site 
commercial compiler portland group pgf rel compile fortran modules snapshot compile modules 
unable run codes completion suggesting may bugs system 
results successfully verified shown table 
note effort map calculations hardware topology 
dramatic differences latencies bandwidths different communication paths suggests significant difference outcome 
results table encouraging sense routed network greatly expensive network 
clear real performance degradation relatively small networks 
summary examined possible architectural approaches design pc clustered systems processors connected mass market cots switch 
classes networks investigated detail high bandwidth routed topologies 
expensive practical costs dropping rapidly due market forces low cost alternative ultimately probably necessary build systems larger largest available switch 
scalable networks proposed deliver component level latencies bandwidths comparable seen smaller switched networks scale easily nodes 
contention resources network computation introduces additional effects component level measurements 
results indicate switches hold load routed networks problematical 
load large variance introduced time required message delivery 
messages delivered time consistent component level measurements non trivial number suffer delay rare instances delays approaching resulting network performance adequately characterized single latency bandwidth 
important topological effects virtual connections times average latency large spread observed message times topologically equivalent pairs 
effects complex network behavior small suite application benchmarks reveals significant cost routed networks 
effect different programs nas benchmark suite fact probably opportunities improving results altering mapping logical connectivity expressed software physical connectivity expressed wiring point point links 
initial results grounds cautious optimism routed networks larger systems study required determine observed problems reduced overcome 
implementation synthetic load generator measurement code generate data section available electronically www caltech edu johns papers sc 
program written ansi calls mpi libraries 
reasonably portable compiled run linux systems mpich glibc 
david bailey tim harris william rob van der alex woo maurice 
nas parallel benchmarks 
technical report nas numerical aerodynamic simulation facility nasa ames 
science nas nasa gov software npb 
larry mcvoy carl staelin 
lmbench portable tools performance analysis 
usenix winter conference january 
www usenix org publications library proceedings sd mcvoy html 
sterling becker savarese 
beowulf parallel workstation scientific computation 
international conference scientific computation 
michael warren timothy peter david john salmon 
alpha linux cluster achieves gflops 
supercomputing los alamitos 
ieee comp 
soc 
proceedings 

