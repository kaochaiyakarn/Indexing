query learning large margin classifiers colin campbell campbell bris ac uk department engineering mathematics university bristol bristol bs tr united kingdom nello cristianini nello cristianini bris ac uk department engineering mathematics university bristol bristol bs tr united kingdom alex smola alex smola anu edu au department engineering australian national university canberra act australia active selection instances significantly improve generalisation performance learning machine 
large margin classifiers support vector machines classify data informative instances support vectors 
natural candidates instance selection strategies 
propose algorithm training support vector machines instance selection 
give theoretical justification strategy experimental results real artificial data demonstrating effectiveness 
technique efficient dataset learnt support vectors 

labour intensive task labelling data serious bottleneck data mining tasks 
cost time constraints mean fraction available instances labeled 
reason increasing interest problem handling partially labeled datasets 
approach problem query learning learning machine allowed actively interrogate environment data source just passively waiting data 
particular learning membership queries algorithm creates selects unlabeled instances human expert label 
problem creating queries informative ones meaningless impossible human expert label baum informative query may improper character ocr task 
better approach may query learning conjunction partially labeled datasets unlabeled part reserve potential queries freund 
task reduced querying informative points data problem known instance selection selective sampling 
large margin classifiers smola boosting freund schapire support vector machines svms vapnik cristianini shawe taylor property focusing subset informative patterns dataset construct hypothesis 
patterns may called support patterns context svms support vectors 
sense large margin classifiers natural candidate instance selection strategies 
knew priori identity support patterns dataset possible discard patterns recover final hypothesis 
theoretical results artificial examples discussed rivest eisenberg show possible invent malicious distributions number queries comparable sample size removing advantage 
practice adversarial distributions may occur frequently 
find sparsity solution important factor hypothesis requires comparatively support vectors relation total dataset size selective sampling works 
brief overview large margin techniques svms section outline strategy selecting support patterns efficiently considering minimax contribution patterns regularized risk functional section 
efficient stopping criterion considered section 
experimental results section showing algorithm identifies informative datapoints minimal number queries 

large margin classifiers risk functionals space patterns gamma space labels target values probability distribution theta classification problem consists finding mapping minimizes risk misclassification jy gamma jp dx pr practice uses real valued functions 
binary functions substitutes sign 
furthermore readily available labelled training set points order infer suitable function generally done minimizing empirical estimate training error remp jy gamma sign minimization usually ill posed guarantee small 
bounds generalization error vapnik cristianini shawe taylor obtained seeking function achieves large margin training set exists constant fl fl 
goal usually achieved replacing binary loss function jy gamma sign margin type loss function soft margin loss max gamma yf adaptive loss settings loss scholkopf 
minimizing modified loss function may ill posed problem minimizing may just lead poor generalization performance 
consequently term omega gamma controlling class admissible functions added obtains regularized risk functional cf 
smola reg remp omega gamma omega gamma regularization trade constant determines functions minimizing complexity favoured functions minimizing remp 
problem instance selection viewed follows 
minimizer reg selecting set ae indices possible ji minimization reg remp omega gamma omega gamma leads comes close minimizing reg 
index old strategy test old possible combinations labellings minimal improvement taken possible labellings maximised 
clearly complexity operation prohibitively high opt computationally cheaper heuristic strategy 
done section 
going details briefly review special type learning machines experiments 
support vector machines support vector machines svms implement complex decision rules non linear function oe map training points high dimensional feature space optimal hyperplane minimise regularized risk functional reg 
procedure finding optimal hyperplane reduced quadratic programming task involving maximisation corresponding dual objective function 
solution optimisation task remarkable property exclusively expressed subset data support vectors task finding subset omitting labelling rest gives obvious motivation considering query learning svms 
svm considers class non linear functions delta delta oe particular separating hyperplane characterised equation delta gamma 
distance separating hyperplane 
margin fl perpendicular distance separating hyperplane closest points support vectors 
region hyperplanes support vectors side called margin band 
examples support vectors opposite sign 
closest points called margin denoted fl 
margin written fl kwk gamma cf 
vapnik may write sv optimization problem minimization regularized risk functional reg kwk computing wolfe dual yields standard sv optimization task vapnik maximise ff gamma ff ff ff oe delta oe subject ff 
hard margin case ff soft margin loss ff oe delta oe oe delta oe 
weight vector ff oe show see smola smola scholkopf details size loss function corresponding lagrange multiplier ff connected ff ff ff denotes respect ff case soft margin loss means advantage defined infer things 
firstly size ff indicates sample inside margin provided trained 
secondly allows predict adding new sample increase dual objective function decrease primal objective ff default patterns trained gain achieved 
experimental study define sparsity ratio number points function values corresponding zero total number points words fraction points margin larger case soft margin loss function 
svms fraction support vectors total dataset size 
certain datasets modelled sparse hypothesis relatively support vectors decision function datasets hypothesis modelling data dense sparsity ratio nearer unity 
complete implementation svm final task perform implicit mapping feature space replacing inner product oe delta oe closed form expression satisfies mercer condition vapnik cristianini shawe taylor 
possible choices kernel gaussians oe polynomial kernels delta 
algorithm selecting instances selection strategies subset data want infer instance choose 
know labels rest dataset assumptions distribution noted afford solve optimization problem possible sets labellings sigma picking point 
heuristic assume terms new equal influence 
approximation leaves room strategies random labels 
assume predictions far classifier minimizes reg completely uncorrelated labels continuous functions nondifferentiable countable set 
yields interval lhs rhs limit standard derivatives 
words ff jffj yields sign ff ff gamma ff 
unlabelled data 
case want find index corresponding largest expected error contribution arg max gamma soft margin case simplifies arg max max gamma max assumption randomness may initial stage training assume estimate little actual data 
expect sign positively correlated algorithm learned training set assumption randomness bad choice 
worst case 
implies error expected small labelling correct possibly margin small 
smallest 
choose worst case strategy arg max min gamma soft margin case simplified arg min jf words points closest decision boundary chosen 
entails cases pattern nonzero minimax error jf margin empty 
empty margin 
case empty margin conflicting issues 
hand suggests choose large function value 
hand suggest choosing pattern close margin 
strategies appears optimal pick pattern randomly labels random patterns section decide 
hard margin 
hard margin cost function assumes function values 
fail equally 

generalisation error axis percentage versus number patterns learnt axis random sampling top right curve query learning bottom left curve 
majority rule components instance randomly split training test instances averaged samplings gaussian kernels oe 
analysis case limit sv machines 
assume feasible solution optimization problem exists words dataset separable 
analyzing objective function omega gamma kwk subject constraints see training subset equivalent ignoring constraints adding data adding constraints increase objective function kwk step add constraint causes omega gamma increase start feasible solution optimization problem larger closer feasible solution 
largest increase comes large decrease minimum margin fl kwk achieved patterns lying close possible decision boundary 
selection rule identical worst case strategy 
algorithm far omitted description stopping criteria algorithm 
come issue section largely detached description training algorithm 
sake simplicity hard margin case just straightforward implementation strategy discussed previous subsection 
argument strategy start requesting labels random subset instances subsequently iteratively requesting label datapoint closest current hyperplane algorithm hard margin query learning input unlabelled training patterns xm program set 
randomly select initial starting set instances training data 
repeat train set fx ji ig max ff gamma ff ff subject ff ff compute decision function unlabelled members training set ff chosen min delta oe max gamma delta oe pick training pattern arg min jf jf margin band empty select validation set instances random unlabelled training set see section 
query label increment set fi endif labels requested stopping criterion met validation set instances section output number queries query set coefficients ff real life datasets contain noise outliers data 
readily handled soft margin technique loss amounting addition diagonal components kernel matrix shawe taylor cristianini positive constant limiting influence individual lagrange multipliers ff 
practical situations cost time constraints may cause early stopping labelling process 
immediate concern important question stopping criterion effective termination process 
naive strategy 
averaged value objective function versus number instances learnt axis majority rule 
upper solid curve corresponds query learning lower dashed curve random sampling 

generalisation error axis percentage versus number patterns learnt axis random sampling top right curve query learning bottom left curve 
shift detection dataset randomly split training test instances averaged samplings gaussians kernel oe 
dual objective function stops increasing queried points yielding additional information 
illustrate section practice noise free data 
better heuristic margin band empty illustrate criterion section 
query termination criterion 
stopping rule able give upper bound probability error remaining unlabelled training set current hypothesis outcomes previous queries label set fy ig 
main tool subsequent analysis theorem hoeffding hoeffding bounding deviation empirical means expectations 

generalisation error axis percentage versus number patterns learnt axis random selection top curve query learning bottom curve 
uci ionosphere dataset randomly split training test instances averaged samplings gaussian kernel oe 

average minimal distance closest point current separating hyperplane axis versus number instances learnt axis ionosphere dataset averaged samplings 
average instances closest points lie outside margin band 
theorem independent bounded random variables falls interval probability 
denote average sm ffl gamma sm fflg sm gamma sm fflg exp ae gamma ffl oe adapt statement large deviations statement concerning sample averages different sample size 
readily constructed follows 
quote theorem smola mangasarian scholkopf smola mangasarian scholkopf contains exactly statement need 
theorem independent identically distributed bounded random variables falling 
generalisation error axis percentage versus number patterns learnt axis random selection top curve query learning bottom curve 
uci cleveland heart dataset randomly split training test instances averaged samplings hard margin gaussian kernel oe 
query learner begins learning outliers passing minimum generalisation error problem removed soft margin interval probability 
denote average sm denote subset random variables mg mg injective map ffl gamma fflg prfs gamma sm fflg exp gamma ffl gamma case set training errors count denotes size far unlabelled training set gamma ji size subset training performed labels queried 
confidence gamma training error statement holds margin error untested subset bounded sm ffl gamma log ffl gamma words point perform queries updating current hypothesis observe margin error queries applied 
may satisfied test query decide training subsequently perform query 
generalisation error axis percentage versus number patterns learnt axis random selection top curve instance selection bottom curve 
uci cleveland heart dataset randomly split training test instances averaged samplings soft margin ffl gaussian kernel oe stopping algorithm 
exists probability just lucky term terms add 
countered systematically enlarging time perform query bound holds gammak lucky events independent statement gamma gamma gammak gamma confidence 
obtain sm gamma log gamma log sm log gamma log practical algorithm proceed follows train margin band empty stopping point specified user met 
perform random sampling unlabelled samples confidence specified user bound remaining training error stated 
user satisfied continue training querying desired 
confidence rating increase continue sampling fix ffl stated terms remaining error unlabelled data entire dataset 

experiments compare instance selection random sampling artificial real datasets stopping criteria stated 
predictably number queries needed achieve optimal performance depends sparsity ratio artificial data 
artificial example choose dataset generated majority rule majority bits input string gamma majority consists gamma 
see instance selection clear advantage random sampling data points 
particular querying subset samples decision function learnt correctly 
shows rapid increase dual objective function query learning optimum achieved instances 
noiseless datasets case monitoring dual objective function provides stopping criterion shows corresponding performance shift detection problem nowlan hinton 
compared majority rule selective sampling appears dramatic effect 
shift detection sparsity ratio contrast majority rule 
instance selection appears effective target concept sparse 
sense support vectors dual objective function increasing support vectors early process 
real world data 
plot corresponding curves ionosphere dataset uci repository blake 
ionosphere dataset sparsity ratio advantages instance selection clear 
plot averaged distance separating hyperplane indicates closest points outside margin band average instances 
appears stopping criterion 
notice generalisation error minimum selective sampling hard margin loss 
effect pronounced cleveland heart dataset uci repository blake 
instances learnt generalisation error increases system learns outliers data 
average closest points separating hyperplane outside margin band instances justifying earlier heuristic 
note hypothesis change closest points outside margin band correspond points hyperplane current hypothesis predicts incorrectly 
mentioned earlier influence noisy data handled introducing soft margin loss function illustrate effect 
numerical experiments considered instance selection led rapid decrease generalisation error compared random selection advantages instance selection small hypothesis modelling data dense 
surprising cases data needed model hypothesis precisely definition simply achieve result data 
sparsity ratios generally decrease volume data increases reaching lower limit smola 
method particularly useful large datasets 
figures notice instance selection appears useful strategy highlighting erroneous critical samples 
exploited context data cleaning potential outliers detected possibly re labelled appropriate 
angluin 
queries concept learning 
machine learning 
baum baum 
neural networks learn polynomial time examples queries 
ieee transactions neural networks 
blake blake keogh merz 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
cristianini shawe taylor cristianini shawe taylor 
support vector machines 
cambridge university press cambridge uk 
appear 
freund schapire freund schapire 
decision theoretic generalization online learning application boosting 
journal computer system sciences august 
freund freund seung shamir tishby 
selective sampling query committee algorithm 
machine learning 
hoeffding hoeffding 
probability inequalities sums bounded random variables 
journal american statistical association 
nowlan hinton nowlan hinton 
simplifying neural networks soft 
neural computation 
rivest eisenberg rivest eisenberg 
sample complexity pac learning random chosen examples 
proceedings workshop computational pages morgan kaufmann san mateo ca 
scholkopf scholkopf bartlett smola williamson 
shrinking tube new support vector regression algorithm 
kearns solla cohn editors advances neural information processing systems pages cambridge ma 
mit press 
shawe taylor cristianini shawe taylor cristianini 
results margin distribution 
proceedings twelth annual conference computational learning theory colt 
smola smola 
regression estimation support vector learning machines 
master thesis technische universitat munchen 
smola smola 
learning kernels 
phd thesis technische universitat berlin 
smola mangasarian scholkopf smola mangasarian scholkopf 
sparse kernel feature analysis 
technical report university wisconsin data mining institute madison 
smola scholkopf smola scholkopf 
kernel method pattern recognition regression approximation operator inversion 
algorithmica 
smola smola bartlett scholkopf schuurmans 
advances large margin classifiers 
mit press cambridge ma 
forthcoming 
vapnik vapnik 
statistical learning theory 
wiley 
