implementation portable nested data parallel language guy blelloch siddhartha chatterjee jonathan jay marco zagha school computer science carnegie mellon university pittsburgh pa gives overview implementation nesl portable nested data parallel language 
language implementation fully support nested data structures nested dataparallel function calls 
features allow concise description parallel algorithms irregular data sparse matrices graphs 
addition maintain advantages data parallel languages simple programming model portability 
current nesl implementation intermediate language called vcode library vector routines called 
runs connection machine cm cray mp serial machines 
compare initial benchmark results nesl machine specific code machines algorithms squares line fitting median finding sparse matrix vector product 
results show nesl performance competitive machine specific codes regular dense data superior irregular data 
research sponsored avionics laboratory wright research development center aeronautical systems division afsc air force wright patterson afb ohio contract arpa order 
cray connection machine cm connection machine cm time provided pittsburgh supercomputing center asc 
partially supported chair nsf young investigator award 
riacs mail nasa ames research center moffett field ca views contained document authors interpreted representing official policies expressed implied government 
high cost rewriting parallel code resulted significant effort directed developing highlevel languages efficiently portable parallel vector supercomputers 
common approach add data parallel operations existing languages exemplified high performance fortran hpf effort various extensions 
data parallel extensions offer fine grained parallelism simple programming model permitting efficient implementation simd mimd vector machines 
hand generally agreed language extensions ideally suited computations dense matrices regular meshes suited algorithms operate irregular structures unstructured sparse matrices graphs trees :10.1.1.44.9578
languages control parallel constructs better suited problems unfortunately constructs port vector machines simd machines mimd machines vector processors 
nested data parallel languages combine aspects strict data parallel languages languages 
nested data parallel languages allow recursive data structures application parallel functions multiple sets data parallel 
example sparse array represented sequence rows subsequence containing nonzero elements row subsequence may different length 
parallel function sums elements sequence applied parallel sum rows sparse matrix 
nested calls function technique called flattening nested parallelism allows compiler convert form runs vector simd machines 
nested data parallel languages theory maintain advantages data parallel languages fine grained parallelism simple programming model portability suited cray cm dec cray cm dec cray cm dec median sparse performance summary benchmarks 
numbers ratio times taken nesl native code versions benchmark smaller numbers better 
problem size largest fit physical memory 
full performance results section 
describing algorithms irregular data structures 
efficient implementation previously demonstrated 
part carnegie mellon scal project completed implementation nested dataparallel language called nesl 
implementation intermediate language called vcode library vector routines called 
implementation runs connection machine cm cray serial workstations 
currently working version connection machine cm 
describe language implementation provide benchmark numbers analyze benchmark results 
results demonstrate possible get efficiency portability variety parallel machines nested data parallel language 
benchmarks squares line fitting algorithm median finding algorithm sparse matrix vector product 
summarizes benchmark timings 
machine give direct comparisons written native code compiled full optimizations 
nesl benchmark times interpreted version intermediate language discussed section compiled version significantly faster 
line fitting benchmark measures interpretive overhead implementation contains nested parallelism generates near optimal code fortran vectorized cm fortran 
benchmarks demonstrate efficiency dynamic memory allocation nested parallel code 
organized follows section describes nesl illustrates nested parallelism applied simple algorithms sparse matrices 
description nesl wider variety algorithms including computing minimum spanning tree sparse graphs searching patterns strings finding convex hull set points 
section outlines different components current nesl implementation 
section describes benchmarks section discusses running times benchmarks 
nesl nested parallelism nesl high level language designed allow simple concise descriptions nested data parallel programs 
strongly typed applicative sideeffect free 
sequences primitive parallel data type language provides large set built parallel sequence operations 
parallelism achieved ability apply functions parallel element sequence 
application function sequence specified set notation similar set formers setl list comprehensions miranda haskell 
example nesl expression negate read parallel sequence negate 
expression returns 
parallelism applied expression left colon right pipe 
parallel implemented packing techniques 
nesl supplies set parallel functions operate sequences 
lists functions full list nesl manual 
similar operations data parallel languages 
nesl supports nested parallelism permitting sequences elements sequence permitting parallel sequence functions construct 
example sparse matrix represented sequence rows sequence column number value pairs 
matrix gamma gamma gamma gamma represented technique 
operation description length sequence th element sequence sum sum sequence plus scan parallel prefix addition 
permute permute elements sequence positions sequence get get values sequence indices sequence append sequences nesl sequence operations 
sum values row sum row row delete elements eps row eps row add column row row permute rows new positions permute simple operations sparse matrices 
note operation permutes rows 
representation matrices arbitrary patterns non zero elements 
shows examples useful operations matrices 
operations parallelism row rows 
parallelism proportional total number non zero elements number rows outer parallelism average row size inner parallelism 
graphs represented similarly sparse matrices subsequence keep adjacency list 
nested parallelism useful divide andconquer algorithms 
example consider parallel quicksort algorithm 
assignments function qsort pivot rand les fe eql fe grt fe result les grt result eql result nested data parallel quicksort nesl 
les eql grt select elements equal greater pivot respectively 
expression les grt puts sequences les grt nested sequence applies qsort parallel elements sequence 
result sequence sorted subsequences 
function appends sequences 
algorithm parallelism pack required select intermediate sequences parallel execution recursive calls 
flat data parallel language permit recursive calls parallel 
decided define new language adding nested parallel constructs existing language main reasons 
wanted small core language allow guarantee expressed parallel compiles parallel form 
second wanted side effect free language due difficulty implementing nested data parallelism nested function calls interact side effects 
problem probably overcome aggressive compiler analysis determine places possibility interaction 
system overview implementation nesl consists intermediate language called vcode compiler interpreter vcode portable library parallel routines called 
illustrates implementation organized 
split system lines concentrate different aspects system isolation 
section gives overview various components system detailed description full version 
vcode vcode stack intermediate language objects stack vectors atomic values integers floats booleans 
vcode instructions act vectors performing operations elementwise adding vectors summing vector permuting order elements vector 
provide support nested parallelism vcode supplies notion segment descriptors 
objects kept stack describe vectors partitioned segments 
example segment descriptor specifies vector length considered segments lengths respectively segments contiguous overlap 
vcode representation segment descriptor machine dependent serial nesl vcode nesl compiler vcode interpreter vcode compiler length inference access inference instruction clustering storage optimizations serial cray cm coded coded cal coded paris cm coded memory management runtime length checking serial nested data parallel language strongly typed polymorphic order functional flatten nested parallelism type inference type specialization mark variables stack intermediate language library parallel functions multi threaded parts scal project fit 
cal stands cray assembly language paris interface connection machine parallel instruction set 
cm development 
implementation uses sequence lengths segment implementations connection machine cm cray flags mark boundaries segments 
non elementwise vcode instructions require vector segment descriptor arguments 
instruction operates independently segment vector 
example scan instruction perform parallel prefix operation segment starting zero segment boundary 
segmented versions instructions critical implementation nested parallelism 
notion segments appears library routines connection machines cm cm adopted prefix suffix operations high performance fortran 
nesl compiler nesl compiler translates nesl code vcode 
important compilation step technique called flattening nested parallelism 
process converts nested sequences sets flat vectors translates nested operations segmented vcode operations flattened representation 
flattening nested sequences achieved converting sequences pair value vector segment descriptor example sequence represented pair value nested sequence represented value examples value specifies vector segment seemingly redundant data critical implementing nested versions user defined functions 
second example describes segmentation value 
sequences nested deeper represented additional segment descriptors 
sequences fixed sized structures pairs represented multiple vectors slot structure single segment descriptor 
representation nested versions nesl operations vcode counterparts directly converted corresponding segmented vcode instruction 
nested versions user defined functions implemented program transformations called stepping stepping 
transformations convert substeps nested call segmented operations calls functions transformed 
transformations user function called parallel gets placed separate segment computations segments proceed independently 
complex part flattening nested parallelism transforming conditional statements 
main parts transformation 
part inserts code packing take branch data branch completed 
guarantees done take branch similar technique vectorizing compilers vectorize loops conditionals 
results proper load balancing parallel machines 
second part inserts code test branch skip branch 
important guaranteeing termination 
communication costs involved doing packing unpacking quite high area ongoing research see communication minimized packing significant load imbalance processors 
enable quickly implement vcode new machines designed vector library library low level segmented vector routines callable vcode interpreter described 
implementations connection machine cm cray serial tions highly optimized 
originally tried implement operations high level language attain high efficiency forced cray assembly language cal cray parallel instruction set paris connection machine cm 
particularly important cray get fortran compilers vectorize segmented code issues discussed 
achieved better performance connection machine cm going level closer machine time permit 
implementation workstations uses ansi ported easily serial machines 
vcode interpreter main requirements vcode interpreter portability efficient management vector memory 
actual vector instructions supplied vcode mapped calls fairly straightforward manner 
vcode interpreter imposes constant executed instruction overhead vcode programs 
efficient execution primitives guarantees efficient evaluation vcode programs large vectors 
interesting part interpreter manages memory 
interpreter uses free lists varying sized blocks memory counting vectors order create destroy vectors differing dynamically determined sizes 
algorithm fully described 
vcode compiler chatterjee doctoral dissertation discusses design implementation evaluation optimizing vcode compiler sharedmemory mimd machines 
course trivial implementation vcode mimd machine vcode instruction written parallel loop processors synchronize instructions 
precisely vcode interpreter works 
implementation data parallelism mimd machine performance bottlenecks limit asymptotic performance performance small problem sizes 
vcode compiler addresses problems set program optimization techniques cluster computation larger loop bodies reduce intermediate storage relax synchronization constraints 
techniques require compile time knowledge data sizes number processors 
compiler accepts vcode input outputs code thread extensions targeted shared memory multiprocessors 
native compiler invoked produce final machine code 
compiler portable allows best compiler properties commu dynamic nested nication structures parallel line fit low median high sparse high properties benchmarks 
function float xa sum ya sum stt sum xa sum xa tmp yg stt ya xa chi sum yg sqrt xa stt chi sqrt stt chi nesl code fitting line square fit 
function takes sequences coordinates returns intercept slope respective probable uncertainties 
technology available machine 
benchmarks section describes benchmarks leastsquares line fit generalized median find vector product 
particular benchmarks selected diverse computational requirements summarized 
simple reader able fully understand algorithm doing complex bare kernels livermore loops 
performance benchmarks demonstrate advantages disadvantages system 
results benchmarks analyzed section 
benchmark squares line fitting routine algorithm described press section version assumes points equal measurement errors 
straightforward algorithm requires little communication nested parallelism 
furthermore vectors known size function invocation allocated statically 
suited languages function select kth pivot les fe les select kth les grt fe grt select kth grt grt pivot function median select kth nesl code median finding 
function select kth returns kth smallest element median find middle element 
function vect get vect fa vg row row nest sparse matrix vector product 
holds matrix values holds column indices holds length row vect input vector 
function nest takes flat sequence nests lengths sum values equal length 
fortran 
benchmark measure overhead incurred interpreter implementation 
nesl code benchmark 
second benchmark median finding algorithm 
implement median finding general algorithm finds th smallest element set 
algorithm method 
method similar quicksort calls recursively partitions depending contains result recursion removed fortran version 
algorithm requires dynamic memory allocation removed fortran version sizes pivot greater pivot sets data dependent 
order obtain proper load balancing data redistributed iteration 
nesl code algorithm shown 
algorithm selected demonstrate utility efficiency dynamic allocation 
third benchmark multiplies sparse matrix dense vector demonstrates usefulness nested parallelism 
sparse matrix vector multiplication important supercomputer kernel difficult vectorize parallelize efficiently irregular data structures high communication requirements 
algorithms special classes sparse matrices interested supporting operations arbitrary sparse matrices 
challenge matrices number different scientific engineering disciplines average row lengths 
row lengths significantly start overhead vector machines far small divide processors attempt parallelize row row 
hand dividing rows processors load balancing difficult row different length longest rows long 
implementations nesl fortran compressed row format containing number nonzero elements row values nonzero matrix element column index 
shows nesl implementation 
results running times benchmarks variety data sizes table 
times interpreted nesl code native code 
native code fortran cray cm fortran connection machine cm decstation gcc compiler 
cases full optimization case median finding code cray include compiler directives force vectorization 
full listing native code 
nesl timings code shown previous section run vcode interpreter 
cray benchmarks run processor memory 
connection machine cm benchmarks run processors cm kbits memory 
decstation benchmarks run decstation mbytes memory 
discuss main issues exhibited timings advantage nested parallelism implementation sparse matrix vector product overhead incurred interpreter need dynamic load balancing median finding code connection machine cm 
nested parallelism sparse matrix vector multiplication benchmark demonstrates advantages efficiency nested data parallelism 
gives running times cray variety degrees time seconds average row length cray fortran cray nesl running times sparse matrix vector product varying levels sparseness 
number nonzero entries sparse matrix fixed sparseness 
sparse matrices nesl version outperforms native version factor 
compilation nested data parallelism described section generates code running time essentially independent size subsequences 
full efficiency vectorization cray high data processor ratio cm executing full input data achieved nested code 
result consistently high performance regardless sparse matrix note matrix density increases cray fortran performance improves 
eventually fortran achieves superior performance extra element cost interpretation relative compilation 
interpretive overhead main source inefficiency system interpretation vcode generated nesl compiler 
cost interpretation analyzed studying line fitting benchmark benchmark requires little communication native code implementations compile perfect code 
main sources interpretive overhead system 
cost executing interpreter 
line fitting benchmark constant independent input size interpreter executes fixed number vcode steps may computed examining running times small input 
shows percentage run time accounted overhead varying input sizes value implementations attain half asymptotic efficiency 
shows nesl requires fairly large input order attain close peak efficiency 
percent run time overhead problem size cm nesl cm fortran cray nesl interpreter overhead line fitting benchmark 
vertical lines indicate points overhead accounts running time 
percentage overhead cm nesl implementation comparable cm fortran implementation 
cray fortran overhead insignificant data sizes graph shown 
head problem cm processors loss efficiency working small vectors overwhelms interpretive overhead 
second major deficiency interpreter system granularity operations performed library fine 
operation collection data performed distinct call library 
compiled system loops performing separate parallel operations fused 
optimization result better memory locality quantities kept registers reused loaded memory acted written back allow chaining cray 
inefficiencies adversely affect peak performance nesl programs effects seen performance line fitting large data sizes see table 
dynamic load balancing consider native code median algorithm poorly compared nesl code cm 
median algorithm reduces number active elements step 
cm fortran implementation elements get packed bottom array imbalanced processors 
cm additional important source inefficiency cm built top paris instruction set 
working paris advantages forces older representation data efficient representation generated cm fortran compiler 
time element usec problem size cm fortran cm nesl cm median nesl vs cm fortran 
dec dec cm cm nesl nesl nesl line fit median sparse matrix vector multiply table running times seconds benchmarks nesl native code 
sparse matrix vector product uses row length randomly selected column indices 
possible pack elements smaller array require dynamically allocating new vector step awkward cm fortran 
nesl vectors dynamically allocated data automatically balanced processors 
median algorithm requires log steps average nesl implementation requires total step amount data processed cut constant factor 
cm fortran implementation requires step factor log slower large inputs illustrated 
comparison systems numerous flat data parallel languages proposed portable parallel programming mpp pascal lisp uc fortran 
section explained expressibility efficiency limitations imposed type language 
problems discussed 
existing languages permit user describe nested data parallel operations connection machine lisp lisp 
implementations languages able exploit bottom level parallelism sparse matrix example results parallel sum row serial loop rows 
languages data parallel extensions common lisp 
large number features common lisp difficulty extending semantics parallel execution preclude implementation full nested data parallelism 
main reason wanted simple core language 
parallel languages id sisal crystal explicitly data parallel support fine grained parallelism 
support nested data structures little research implementing nested parallelism languages 
serial languages supply dataparallel primitives nested structures 
include setl apl fp 
discussion languages perspective supporting data parallelism see 
approach architecture independent parallel programming control parallel languages provide asynchronous communicating serial processes 
examples include csp linda actors pvm 
languages suited problems including irregular problems specified terms coarse grained sub tasks 
unfortunately high overhead implementation efficiency dependent finding decomposition reasonably sized blocks 
result systems suited exploiting fine grained parallelism 
large grain size renders programs efficient parallel supercomputers won vectorize don expose parallelism take advantage large numbers processors 
extending model capture fine grained parallelism area active research 
purpose nested data parallel languages provide advantages data parallelism ex tending applicability algorithms irregular data structures 
main advantages data parallelism preserved efficient implementation fine grained parallelism simple synchronous programming model 
described implementation nested data parallel language called nesl 
nesl designed allow concise description parallel algorithms structured unstructured data 
course parallel algorithms allowed students quickly implement wide variety programs including systems speech recognition raytracing volume rendering parsing maximum flow singular value decomposition mesh partitioning pattern matching big number arithmetic 
benchmark results shown possible get efficiency nested data parallel language variety different parallel machines 
nesl runs local interactive environment allows user execute programs remotely supported architectures 
portability depends crucially organization system intermediate language 
efficiency nesl large applications requires study 
issues plan examine getting efficiency nested parallel code conditionals specification data layout irregular structures tools profiling nested parallel code interaction higherorder functions nested parallelism porting system architectures 
agha 
concurrent object oriented programming 
communications acm sept 
ansi 
ansi fortran draft version 
backus 
programming liberated von neumann style 
functional style algebra programs 
communications acm aug 
bagrodia mathur 
efficient implementation high level parallel programs 
fourth international conference architectural support programming languages operating systems pages apr 

flip network 
proceedings international conference parallel processing pages 
full implementation nesl available blelloch cs cmu edu 

massively parallel processor system overview 
potter editor massively parallel processor pages 
mit press 
blelloch 
vector models data parallel computing 
mit press 
blelloch 
nesl nested data parallel language 
technical report cmu cs school computer science carnegie mellon university jan 
blelloch chatterjee 
vcode data parallel intermediate language 
proceedings frontiers massively parallel computation pages oct 
blelloch chatterjee zagha 
vector library 
technical report cmu cs school computer science carnegie mellon university feb 
blelloch chatterjee zagha 
implementation portable nested data parallel language 
technical report cmu cs school computer science carnegie mellon university feb 
blelloch 
class notes programming parallel algorithms 
technical report cmu cs school computer science carnegie mellon university feb 
blelloch 
compiling collection oriented languages massively parallel computers 
journal parallel distributed computing feb 
carriero gelernter 
write parallel programs guide 
acm computing surveys sept 
chatterjee 
compiling data parallel programs efficient execution shared memory multiprocessors 
phd thesis school computer science carnegie mellon university oct 
chatterjee blelloch fisher 
size access inference data parallel programs 
acm sigplan conference programming language design implementation pages june 
chatterjee blelloch zagha 
scan primitives vector computers 
proceedings supercomputing pages nov 
chen choo li 
crystal theory pragmatics generating efficient parallel code 
szymanski editor parallel functional languages compilers chapter 
addison wesley reading ma 
chien dally 
experience concurrent aggregates ca implementation programming 
proceedings fifth distributed memory computers conference 
siam apr 
duff grimes lewis 
sparse matrix test problems 
acm trans 
math 
software 
fox :10.1.1.44.9578
architecture problems portable parallel software systems 
technical report sccs syracuse center computational science syracuse university 
hatcher tichy philippsen 
critique programming language communications acm june 
high performance fortran forum 
high performance fortran language specification jan 
hoare 
algorithm partition algorithm find 
communications acm 
hoare 
communicating sequential processes 
communications acm aug 
hudak wadler 
report functional programming language haskell 
technical report yale university apr 
hui iverson mcdonnell whitney 
apl 
apl conference proceedings pages jan 
ibm 
apl programming language edition aug 
order number sh 

essential lisp manual 
thinking machines cambridge ma july 
mcgraw allan thomas 
sisal streams iteration single assignment language language manual version 
lawrence livermore national laboratory mar 
mcmahon 
livermore fortran kernels computer test numerical performance range 
technical report lawrence livermore national laboratory dec 
nikhil 
id version manual 
computation structures group memo laboratory computer science massachusetts institute technology july 
press flannery teukolsky vetterling 
numerical recipes 
cambridge university press cambridge 
quinn hatcher 
data parallel programming multicomputers 
ieee software sept 
rose steele jr extended language data parallel programming 
proceedings second international conference supercomputing vol 
pages may 

model parallel programming 
mit press cambridge massachusetts 
schwartz schonberg 
programming sets setl 
springer verlag new york 

data representation optimizations collection oriented languages 
phd thesis school computer science carnegie mellon university pittsburgh pa appear 
blelloch 
languages 
proceedings ieee apr 
steele jr fahlman gabriel moon weinreb 
common lisp language 
digital press burlington ma 
sunderam 
pvm framework parallel distributed computing 
concurrency practice experience dec 
thinking machines cambridge ma 
cm fortran manual july 
thinking machines cambridge ma 
paris manual feb 
turner 
overview miranda 
sigplan notices dec 
steele jr connection machine lisp dialect common lisp data parallel programming 
proceedings second international conference supercomputing may 
