image denoising sparse code shrinkage hyv rinen patrik hoyer erkki oja neural networks research centre helsinki university technology box fin hut finland email www cis hut fi projects ica november sparse coding method nding representation data components representation rarely active 
representation closely related independent component analysis ica neurophysiological plausibility 
show sparse coding image denoising 
model noise free image data independent component analysis denoise noisy image maximum likelihood estimation noisy version ica model 
leads application soft thresholding shrinkage operator components sparse coding 
method closely related method wavelet shrinkage coring methods important representation determined solely statistical properties data 
fact method seen simple re derivation wavelet shrinkage method image data just basic principle maximum likelihood estimation 
hand allows method adapt dioeerent kinds data sets 
sparse coding method nding neural network representation multidimensional data small number neurons activated time :10.1.1.134.6077
equivalently means neuron activated rarely 
assume representation linear 
denote xn observed dimensional random vector input neural network vector transformed component variables linear outputs network 
denoting weight vectors neurons wn weight matrix rows weight vectors linear relationship wx assume number sparse components number neurons equals number observed variables need case general :10.1.1.134.6077
sparse coding formulated search weight matrix components possible 
zero mean random variable called sparse probability density function peak zero heavy tails practical purposes sparsity equivalent positive kurtosis 
sparse coding closely related independent component analysis ica 
data model ica postulates linear transform independent components 
inverting relation obtains pseudo inverse proven estimation ica data model reduced search uncorrelated directions components nongaussian possible 
independent components sparse precisely supergaussian amounts search uncorrelated projections sparse distributions possible 
estimation ica model sparse data roughly equivalent sparse coding components constrained uncorrelated 
connection ica shows clearly sparse coding may considered method redundancy reduction primary objectives sparse coding rst place 
sparse coding sensory data shown advantages physiological information processing viewpoints 
analyze denoising method sparse coding increasing evidence favor coding strategy 
signal corrupted additive gaussian noise attempt reduce gaussian noise soft thresholding sparse components 
intuitively neurons active non zero simultaneously sparse code may assume activities neurons small absolute values purely noise set zero retaining just components large activities 
method shown closely connected wavelet shrinkage method bayesian wavelet coring :10.1.1.29.5390
fact sparse coding may viewed principled adaptive way determining orthogonal wavelet basis data 
advantage method shrinkage nonlinearities adapted data 
organized follows 
section basic problem formulated maximum likelihood estimation nongaussian variable corrupted gaussian noise 
section optimal sparse coding transformation derived maximum likelihood estimation linear generative model ica 
section discusses alternative approach minimum mean square estimation 
resulting algorithm sparse code shrinkage summarized section connections methods discussed section 
extensions basic theory discussed section 
section contains experimental results drawn section 
maximum likelihood denoising nongaussian random variables maximum likelihood denoising starting point rigorous derivation denoising method fact distributions sparse components nongaussian 
shall developing general theory shows remove gaussian noise scalar nongaussian variables making minimal assumptions data 
method maximum likelihood ml estimation nongaussian variables corrupted gaussian noise 
denote original scalar nongaussian random variable gaussian noise zero mean variance oe assume observe random variable want estimate original denoting probability density gamma log negative log density maximum likelihood method gives estimator arg min oe gamma assuming strictly convex minimization equivalent solving equation oe gamma gives inverse function gamma oe ml estimator obtained inverting certain function involving score function density nongaussian variables score function nonlinear general case inverted rst order approximation ml estimator respect noise level possible gamma oe assuming convex 
estimator derived simply replacing observed observed quantity equal rst order 
problem estimator sign dioeerent sign symmetrical zero mean densities 
counterintuitive estimates possible discontinuous singular implies rst order approximation quite inaccurate near 
alleviate problem may sign max jyj gamma oe jf obtained exact maximum likelihood estimator nongaussian random variable corrupted gaussian noise approximations 
modeling sparse densities estimator dened practice densities need modelled parameterization rich 
developed parameterizations describe densities encountered image denoising 
parameters easy estimate inversion performed analytically 
models parameters able model dioeerent degrees addition dioeerent scales variances 
densities assumed symmetric zero mean 
called maximum posteriori estimator 
laplace density review classical laplace double exponential density classical sparse density 
density laplace distribution unit variance exp gamma jsj laplace density plotted fig 

density ml denoising nonlinearity takes form sign max jyj gamma oe function shrinkage function reduces absolute value argument xed amount depicted fig 
intuitively utility function seen follows 
density supergaussian random variable laplace random variable sharp peak zero assumed small values correspond pure noise 
thresholding values zero reduce noise shrinkage function considered soft thresholding operator 
mildly sparse densities rst density model suitable supergaussian densities sparser laplace distribution family densities exp gamma parameters estimated irrelevant scaling constant 
classical laplace density obtained gaussian densities correspond 
score function gaussian distribution linear function score function typical supergaussian distribution laplace density sign function reasonable approximate score function symmetric mildly supergaussian density zero mean linear combination functions 
shows typical density family 
simple method estimating derived 
projecting suitable metric score function observed data subspace spanned functions linear sign 
obtain efs gamma efs gamma efs gamma value density function zero 
corresponding estimators obtained replacing expectations sample averages estimated single kernel 
densities family nonlinearity takes form oe sign max juj gamma boe oe noise variance 
function shrinkage additional scaling depicted fig 

rigorously speaking function invertible case approximating sequence invertible functions obtained limit plots shrinkage functions 
eoeect functions reduce absolute value argument certain amount depends noise level 
small arguments set zero 
reduces gaussian noise sparse random variables 
solid line shrinkage corresponding laplace density 
dashed line typical shrinkage function obtained 
dash dotted line typical shrinkage function obtained 
comparison line dotted line 
densities normalized unit variance illustration purposes densities normalized unit variance parameterizations allow variance choosen freely 
noise variance xed 
plots densities corresponding dioeerent models sparse components 
solid line laplace density 
dashed line typical moderately supergaussian density 
dash dotted line typical strongly supergaussian density 
comparison gaussian density dotted line 
strongly sparse densities fact densities encountered image denoising sparser laplace density 
developed second model describes sparse densities ff ff ff ff ff ff ff scale parameter ff sparsity parameter 
ff laplace density obtained limit 
strong sparsity densities model seen fact kurtosis densities larger kurtosis laplace density reaches ff 
similarly reaches ff goes zero 
shows typical density family 
simple consistent method estimating parameters ff obtained relations efs ff gamma gamma resulting shrinkage function obtained sign max juj gamma ad juj ad gamma oe ff ff ff set zero case square root imaginary 
shrinkage function certain hard thresholding depicted fig 

choice model sparse density model models 
choosing model moments distributions 
suggest efs gp rst model second model 
limit case efs gp corresponds laplace density contained models 
models sake completeness give classical models sparse densities 
models suited method 
rst alternative generalized laplacian density exp gamma js dj ff strictly speaking negative log density convex minimum obtained point case point true minimum :10.1.1.29.5390
nd true minimum value likelihood compared value lead additional thresholding operation 
thresholding changes estimate little reasonable values parameters ff omit simpler accurate approximation minimization 
adjusts scale ff sparsity parameter normalizing constant 
ff density sparser sparser gives gaussian density 
estimation parameters ff second fourth moments data :10.1.1.29.5390
noted fourth moment quite sensitive outliers estimate robust 
closed form solution shrinkage function available wants parametrization estimators calculated numerically 
second alternative model sparse density obtained gaussian mixture model 
gaussian mixture model parametrization gaussian kernel exp gamma gamma represent sparse densities choose mixture gaussians zero mean dioeerent variance 
choosing right parameters mixture sparse signicant amount data near zero remaining data far zero 
estimation parameters performed expectation maximization em algorithm 
parameterization describe image components purposes 
finding sparse coding transformation previous section shown reduce additive gaussian noise scalar nongaussian random variables means ml estimation 
denoise random vectors apply shrinkage operation separately component 
shrinkage transform linearly data component wise denoising eoecient possible 
shall restrict class linear orthogonal transformations reasons explained 
restriction partly relaxed section 
consider estimation generative data model independent component analysis ica presence noise 
noisy version conventional ica model latent variables assumed independent nongaussian usually supergaussian constant square matrix gaussian noise vector 
noise free ica model shown describe important aspects basic higher order structure image data :10.1.1.134.6077
modeling image data ica model intuitively simple method denoising vector obtained follows nd estimates noise free independent components reconstruct proposed 
unfortunately estimating way general computationally demanding 
proven covariance matrix noise mixing matrix certain relation estimate obtained simply applying shrinkage nonlinearity components gamma relation orthogonal noise covariance proportional identity 
restricting orthogonal nd optimal transformation estimating matrix model constraint orthogonality transformation gamma adopt method rest 
estimate constraint orthogonality approximative procedure 
nd estimate conventional ica method transform inverse gamma gamma obtain orthogonal transformation matrix 
utility approximative method resides fact exist algorithms ica computationally highly eoecient 
procedure enables estimate basis data sets high dimensions 
empirically approximation deteriorate statistical properties obtained sparse coding transformation 
practice simplify estimation assuming access random variable statistical properties observed noise 
estimate ordinary noise free ica algorithms 
assumption unrealistic applications example image denoising simply means observe noise free images somewhat similar noisy image treated belong environment context 
assumption estimating matrix noisy data problematic noisy ica proven inherently dioecult problem 
principle estimate noisy data directly method noisy ica estimation discussed section 
mean square error approach minimum mean square estimator scalar case developed denoising method maximum likelihood estimation generative model 
interesting derive corresponding result criterion minimum mean square error 
consider basic scalar denoising problem section 
mean square error estimator minimized dene conditional expectation unfortunately minimum mean square error mmse estimator obtained closed form interesting nongaussian distributions 
obtained closed form distribution modelled mixture gaussians models suitable 
rst order approximation obtained 
fact equal estimator proven appendix 
tractable rst order approximation obtain estimator ml estimation 
analysis mean square error subsection analyze denoising capability scalar mmse estimator 
show roughly nongaussian variable better gaussian noise reduced 
measured fisher information 
due intractability general problem consider limit noise results rst order approximations respect noise level 
due rst order equality ml mmse estimators analysis valid ml estimator 
recall denition fisher information random variable density ef fisher information random variable strictly speaking density equals conventional fisher information respect hypothetical location parameter 
fisher information considered measure 
known set probability densities unit variance fisher information minimized gaussian density minimum equals 
fisher information invariant scaling constant main result performance ml estimator theorem proven 
theorem dene alternatively estimator 
small oe mean square error estimator ef gamma oe gamma oe oe oe variance gaussian noise get insight theorem useful compare noise reduction ml estimator best linear estimator minimum mean square error sense 
unit variance best linear estimator lin oe estimator mean square error ef gamma lin oe oe consider ratio errors obtaining index gives percentage additional noise reduction due nonlinear estimator gamma ef gamma ef lin gamma corollary follows immediately corollary relative improvement noise reduction obtained nonlinear ml estimator best linear estimator measured gamma oe oe small noise variance oe unit variance 
please note notation error proof denoted denoted considering mentioned properties fisher information theorem means nongaussian better reduce noise 
particular sparse variables sparser better denoising works 
gaussian follows fact ml estimator equal linear estimator lin shows gaussian variables allowing nonlinearity estimation improve performance nongaussian sparse variables lead signicant improvement minimum mean squares approach basis estimation mean square criterion choosing basis 
assume observe multivariate random vector noisy version nongaussian random vector noise gaussian covariance oe nd orthogonal transformation data shrinkage method reduces noise possible 
orthogonal weight matrix transformed vector equals wx covariance matrix equals covariance matrix means noise remains essentially unchanged 
noise reduction obtained scalar mmse method theorem proportional sum fisher informations components optimal orthogonal transformation opt obtained opt arg max constrained orthogonal rows estimate method approximation fisher information derived 
estimate optimal orthogonal transform opt need assume access random variable statistical properties observed noise 
image denoising result needs slightly modied 
necessary known fact ordinary mean square error inadequate measure errors images 
perceptually adequate measures obtained weighting mean square error components corresponding lower frequencies weight 
variance sparse principal components larger lower frequencies perceptually motivated weighting approximated simply objective function ef gi expressed ef multivariate gaussian variables improvement obtained stein estimators 
normalized fisher information scale invariant measure 
fact maximizing closely related estimation ica model 
maximizing sum intuitive method estimating ica data model 
seen perceptually weighted mean square error rediscover basis estimation method section 
sparse code shrinkage summarize algorithm sparse code shrinkage developed preceding sections 
method ml noise reduction applied sparse components rst choosing orthogonal transformation maximize sparseness components 
restriction sparse variables fact applications image processing distributions encountered sparse 
algorithm follows 
representative noise free set data statistical properties dimensional data want denoise estimate sparse coding transformation opt rst estimating ica transform matrix 
see section 

estimate density model models described section 
choose model estimate relevant parameters respectively 
denote corresponding shrinkage function respectively 

observing samples noisy version compute projections basis 
apply shrinkage operator corresponding density model component obtaining oe noise variance see estimating oe 

transform back original variables obtain estimates noise free data noise variance oe known estimate multiplying mean absolute deviation corresponding sparsest comparison wavelet coring methods resulting algorithm sparse code shrinkage closely related wavelet shrinkage dioeerences 
method assumes rst estimates orthogonal basis noise free training data similar statistical properties 
method considered principled method choosing wavelet basis class data limited bases certain mathematical properties self similarity basis determined data sole constraint orthogonality 

sparse code shrinkage shrinkage nonlinearities estimated separately component training data basis 
wavelet shrinkage form shrinkage nonlinearity xed shrinkage constant components set zero certain components constant resolution level 
complex methods crossvalidation possible 
dioeerence stems fact wavelet shrinkage uses minimax estimation theory method uses ordinary ml estimation 
note point conceptually independent point shows adaptive nature sparse code shrinkage 

method primarily intended sparse data directly modied kinds nongaussian data 

advantage wavelet methods fast algorithms developed perform transformation avoiding multiplication data matrix transpose 

course wavelet methods avoid computational overhead especially need additional noise free data required estimating matrix rst place 
requirement noise free training data essential part method shown section 
connection especially clear assumes steps sparse code shrinkage section omitted wavelet basis shrinkage function equal zero 
method essentially equivalent wavelet shrinkage 
related method bayesian wavelet coring introduced :10.1.1.29.5390
bayesian wavelet coring shrinkage nonlinearity estimated data minimize mean square error 
method adaptive wavelet shrinkage uses predetermined transformation 
extensions basic theory section extensions basic theory 
nongaussian noise fact derivation noise considered noise sparse components transformation 
noise weakly nongaussian laplace noise noise sum independent noise components due central limit theorem gaussian original noise 
means noise components may considered approximately gaussian basic method expected satisfactorily weakly nongaussian noise 
method necessary case strongly nongaussian impulsive noise 
noise may useful methods reconstructing missing pixels 
estimation parameters noisy data seen ml principle basis estimation section possible estimate sparse code transformation directly noisy data 
emphasize important point missing earlier 
basis estimation noisy data accomplished method estimate noisy ica model 
practice may problematic estimation noisy ica model open research problem probable performance obtained necessarily considerably lower obtained noise free data 
practical applications may useful estimate basis noisy data 
reasons estimation optimal nonlinearities noisy data may problematic research problem 
nonlinearities xed prior knowledge may give satisfactory results 
non orthogonal bases restriction orthogonal bases necessary basis estimated ml principle section 
fact transformation taken simply inverse mixing matrix gamma estimated ordinary ica estimation method 
hand unpleasant side eoeect noise transformed sparse components correlated 
able estimate noise free components computationally complex operations approximate noise structure diagonal matrix 
shrinkage functions simply adapted case nonorthogonal transformations replacing constant noise covariance estimate noise variance direction sparse components 
obtain component wise shrinkage ignoring correlations noise components 
advantage gained relaxing restriction orthogonality diminished need approximate noise covariance 
experiments generation image data interesting test sparse code shrinkage method real data data available free noise 
evaluating results dioecult decided test performance images corrupted noise 
chose separate datasets able compare performance see dioeerences results dioeerent datasets 
rst set images consisted natural scenes previously ica applied image data 
images hoped truly natural images images void human imposed structure 
example images shown 
images may obtained web pages 
second set consisted demo images kodak system 
royalty free may downloaded internet ftp 
intended represent images human built world quite dioeerent statistics natural scenes rst set 
displays example images call man scenes 
sets images picked random estimation transforms densities separate image sets picked actual denoising experiments 
www cis hut projects ica data images ftp ipl rpi edu pub image representative image rst set image data natural scenes 
representative image second set image data man scenes 
remarks image data windowing far considered problem denoising random vector arbitrary dimension 
applying framework images certain problems arise 
simplest way apply method images simply divide image theta windows denoise window separately 
approach couple drawbacks statistical dependencies synthetic edges ignored resulting blocking artifact resulting procedure translation invariant algorithm sensitive precise position image respect windowing grid 
solved problem sliding window approach 
means divide image distinct windows denoise possible theta window image 
dioeerent suggested values pixel select nal result mean values 
originally chosen heuristic grounds sliding window method interpretations 
rst interpretation spin cycling 
basic version introduced wavelet shrinkage method translation invariant property wavelet decomposition general 
coifman donoho suggested performing wavelet shrinkage translated wavelet decompositions data mean results nal denoised signal calling obtained method 
easily seen sliding window method precisely spin cycling distinct window algorithm 
second interpretation sliding windows due method frames 
consider case decomposing data vector linear combination set vectors number vectors larger dimensionality data matrix nd vector innite number solutions 
classical way select solution minimum norm pseudo inverse solution referred method frames solution 
consider basis window possible window position image overcomplete basis image 
transform orthogonal sliding window algorithm equivalent calculating method frames decomposition shrinking component reconstructing image 
local mean ica applied image data usually gives component representing local mean image intensity dc component 
component normally distribution sparse 
treated separately supergaussian components 
estimate suitable density model component denoise just 
component generally large variance relatively noise simply leave 
approach chosen take 
means experiments rst subtract local mean drop dimension pca estimate suitable sparse coding basis rest components 
restructuring image denoising add local means 
normalizing local variance image processing methods normalize local variance image 
considered variant method 
incorporate normalization method dividing image window norm 
done estimation parameters denoising procedure 
transform estimation methods estimating ica transform patches natural image data previously shown give transform mainly consisting local lters resembling somewhat called gabor lters wavelets nd 
image rst linearly normalized pixels zero mean unit variance 
set image patches windows taken random locations images 
patch local mean subtracted explained 
resulted linear dependency pixels patch reduced dimension standard pca 
preprocessed dataset input fastica algorithm hyperbolic tangent nonlinearity 
results shows results rst data set patches natural scenes theta window size 
pca transform consists global features resembles strongly fourier transform 
transform ica nds features resemble local edges bars 
said representative image data 
interesting dioeerence ica lters comprising separating matrix basis vectors making mixing matrix 
basis vectors constitute building blocks data thought generated 
type features seeing images 
looking closely easily seen lters similar basis vectors lter position orientation corresponding basis vector 
separating lters clearly spiky 
essence consequence whitening whitening amplify high frequencies smallest variance 
basic method ica transform orthogonalized obtained transform simply ica transform zero phase whitened data 
transform depicted seen iin separating matrix mixing matrix features transform essentially changed orthogonalization 
second data set image patches gathered man scenes 
results 
dioeerence ica transforms natural scenes quite clear 
man scenes stronger continuous lines edges shows ica decomposition basis vectors edge basis vectors natural scenes 
experimented larger window size pixels 
ica basis shown qualitatively similar estimated smaller windows consisting lines edges various locations various orientations 
comparison show daubechies wavelet basis lter length fig 

component statistics denoising procedure property individual components transform domain sparse distributions obvious tested requirement holds 
method considering noisy windows estimate norm noise added divide quantity 
estimation done reasonably size image window large theta 
transforms estimated natural scene data patches 
windows ordered mean frequency visualization purposes 
ordering irrelevant denoising algorithm 
top left pca transform orthogonal basis lters 
top right ica basis 
bottom left ica separating lters 
bottom right orthogonalized ica transform 
transforms estimated man scene data 
windows ordered mean frequency 
top left pca transform 
top right ica basis 
bottom left ica separating lters 
bottom right orthogonalized ica transform 
wavelet basis compared transforms preceding gures 
requires selecting suitable parametrizations estimating parameters experiments evaluate proposed parametrizations section approximate underlying densities 
measuring sparseness distributions done non gaussianity measure 
chosen widely measure normalized kurtosis 
recall normalized kurtosis dened efs efs gamma average sparseness transform sets estimated previous section calculated way sampled image patches dataset estimation transform 
transformed samples estimated pca ica orthogonalized ica transforms calculated normalized kurtosis component basis separately 
mean component displayed transform dataset 
sparse structure images transforms show supergaussian distributions individual pixel values show mildly supergaussian distribution local mean subtracted 
graph seen ica transform clearly nds sparser representation pca 
note orthogonalized ica representation quite sparse standard ica far supergaussian pca average 
attempted compare various parametrizations task tting observed densities 
picked component random orthogonal theta sparse coding transform natural scenes 
non histogram technique estimated density component representation derived log density shrinkage nonlinearity shown 
tted parametrized densities discussed section observed density 
note case densities sparser laplace density sparse parametrization 
seen density shrinkage nonlinearity derived density model match quite nonparametric estimation 
corresponding experiments window size theta shown pixels pca ica orth 
ica mean normalized kurtosis components dioeerent datasets dioeerent transforms dataset 
natural scenes patch bases 
man scenes bases 
man scenes bases 
analysis randomly selected component orthogonalized ica transforms natural scenes window size theta 
top non parametrically estimated log densities solid curve vs best parametrization dashed curve 
bottom non parametric shrinkage nonlinearity solid curve vs parametrization dashed curve 
analysis randomly selected component orthogonalized ica transforms natural scenes time window size theta 
top non parametrically estimated log densities solid curve vs best parametrization dashed curve 
bottom non parametric shrinkage nonlinearity solid curve vs parametrization dashed curve 
results dioeerent case 
results man scenes shown similar 
gaussian mixture model shown gave poor matches observed densities 
generalized laplace density give matches observed drawback allow analytical form shrinkage nonlinearity prefer parameterization 
mentioned possible variant method normalize local image variance dividing input window norm 
experiment denoising method estimate density parameters data 
proceeded experiments normalized observed image patch unit length transforming orthogonalized ica transform 
densities nearly sparse mildly sparse density model appropriate parametrization seen gures 
seen components sparse coding bases highly supergaussian natural image data basic case 
true images depict nature man scenes 
denoising results actual denoising experiments random image natural scene collection chosen denoising gaussian noise standard deviation added compared standard deviation original image 
noisy version subsequently denoised wiener lter give baseline comparison 
sparse code shrinkage method applied estimated orthogonalized ica transform theta windows component nonlinearities appropriate estimated parametrization 
shows results rst experiments 
visually sparse code shrinkage gives best noise reduction retaining features image 
wiener lter really eliminate noise 
method performing feature detector retains features clearly visible noisy data cuts probably result noise 
reduces noise due nonlinear nature shrinkage operation 
exactly parameters exception noise level raised 
results qualitatively similar lower noise level 
experiments performed data consisting man scenes 
show results noise levels fig 

data suit method better data natural scenes 
image consists mainly areas sharp edges 
type features basis density parameters adapted results quite 
see eoeect window size experiments larger window size 
results shown really change 
theory derived strict assumption noise gaussian interesting see method performs noise slightly non gaussian laplacian 
results experiment 
bit surprisingly method performed quite 
possible explanation see section 
note noise increasingly supergaussian impulsive assumptions method break better methods discussed section 
experimented variant method normalization local variance performed see section 
results displayed 
results compared figures 
variant cuts noise areas leaves noise area edges 
experiments median lter shown gave results qualitatively similar wiener lter results 
results obtained standard ica transform shown orthogonalized clearly inferior obtained orthogonalized method 
conclude visual inspection results shows method performs 
denoising result qualitatively quite dioeerent traditional ltering methods lines wavelet shrinkage coring results 
method sparse code shrinkage maximum likelihood estimation nongaussian random variables corrupted gaussian noise 
method rst determine orthogonal basis components multivariate data sparsest distributions possible 
sparseness components utilized ml estimation noise free components estimates reconstruct large number dioeerent variants wavelet shrinkage method choice wavelet basis choice shrinkage function 
choice fair comparison chose explicitly compare method wavelet shrinkage 
denoising natural scene noise level 
top left original image 
top right noise added 
bottom left wiener ltering 
bottom right results sparse code shrinkage 
denoising natural scene noise level 
top left original image 
top right noise added 
bottom left wiener ltering 
bottom right results sparse code shrinkage 
denoising man scene noise level 
top left original image 
top right noise added 
bottom wiener ltering 
bottom right results sparse code shrinkage 
denoising man scene laplacian noise noise level 
top left original image 
top right noise added 
bottom left wiener ltering 
bottom right results sparse code shrinkage 
left denoising local variance normalization gaussian noise level compare 
right denoising local variance normalization noise level raised compare 
original noise free data inverting transformation 
approximation estimation noisy ica model 
resulting method sparse code shrinkage closely connected wavelet shrinkage coring methods fact considered principled way choosing orthogonal wavelet basis data alternative way choosing shrinkage nonlinearities 

amari cichocki yang 
new learning algorithm blind source separation 
advances neural information processing systems pages 
mit press cambridge ma 
barlow 
computational goal neocortex koch davis editors large scale neuronal theories brain 
mit press cambridge ma 
bell sejnowski 
information maximization approach blind separation blind deconvolution 
neural computation 
bell sejnowski 
independent components natural scenes edge lters 
vision research 
coifman donoho 
translation invariant de noising 
technical report department statistics stanford university 
comon 
independent component analysis new concept 
signal processing 
cover thomas 
elements information theory 
john wiley sons 
ingrid daubechies 
lectures wavelets 
society industrial applied math philadelphia 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
royal statistical society ser 

donoho johnstone kerkyacharian picard 
wavelet shrinkage 
journal royal statistical society ser 

douglas cichocki amari 
bias removal technique blind source separation noisy measurements 
electronics letters 
efron morris 
data analysis stein estimator generalizations 
american statistical association 
field 
goal sensory coding 
neural computation 
huber 
projection pursuit 
annals statistics 
hyv rinen oja 
wavelets natural image statistics 
proc 
scandinavian conf 
image analysis finland 
hyv rinen 
independent component analysis presence gaussian noise maximizing joint likelihood 
neurocomputing 
hyv rinen 
fast robust xed point algorithms independent component analysis 
ieee trans 
neural networks 
hyv rinen 
gaussian moments noisy independent component analysis 
ieee signal processing letters 
hyv rinen 
sparse code shrinkage denoising nongaussian data maximum likelihood estimation 
neural computation 
hyv rinen oja 
fast algorithm estimating overcomplete ica bases image windows 
proc 
int 
joint conf 
neural networks washington 
hyv rinen oja 
fast xed point algorithm independent component analysis 
neural computation 
jutten herault 
blind separation sources part adaptive algorithm neuromimetic architecture 
signal processing 
karhunen oja wang rio 
class neural networks independent component analysis 
ieee trans 
neural networks 
kendall stuart 
advanced theory statistics 
charles 
lewicki olshausen 
probabilistic framework adaptation comparison image codes 
opt 
soc 
am 
optics image science vision 
luenberger 
optimization vector space methods 
john wiley sons 
mallat 
theory multiresolution signal decomposition wavelet representation 
ieee trans 
pami 
nason 
wavelet shrinkage cross validation 
journal royal statistical society series 
oja 
nonlinear pca learning rule independent component analysis 
neurocomputing 
olshausen field 
emergence simple cell receptive eld properties learning sparse code natural images 
nature 
olshausen field 
sparse coding overcomplete basis set strategy employed 
vision research 
fitzgerald 
numerical bayesian methods applied signal processing 
springer 
schervish 
theory statistics 
springer 
simoncelli adelson :10.1.1.29.5390
noise removal bayesian wavelet coring 
proc 
third ieee int 
conf 
image processing pages lausanne switzerland 
yu donoho 
translation direction invariant denoising images experience algorithms 
proceedings spie wavelet applications signal image processing iv pages 
mmse estimator derive rst order approximation mmse estimator showing identical 
denote gamma clearly denote terms lower order ef ef gamma ef gamma gamma gamma oe ef gamma gamma gamma oe ef ef goe ef goe oe second line seen approximation error occurred taylor expansion order oe 
means written ef gamma oe ef ef goe gamma oe oe variational calculus nd minimizes mean square error 
neglecting terms smaller order equating variational derivative right hand side respect zero obtain gamma oe gives gamma oe 

