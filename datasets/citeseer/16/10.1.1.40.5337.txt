markovian models sequential data bengio dept informatique recherche op universit de montr eal montreal qc canada hidden markov models hmms statistical models sequential data successfully machine learning applications especially speech recognition 
furthermore years new promising probabilistic models related hmms proposed 
summarize basics hmms review related learning algorithms extensions hmms including particular hybrids hmms artificial neural networks input output hmms conditional hmms neural networks compute probabilities weighted transducers variable length markov models markov switching state space models 
discuss challenges research active area 
hidden markov models hmms statistical models sequential data successfully applications artificial intelligence pattern recognition speech recognition modeling biological sequences 
focus learning algorithms developed hmms related models hybrids hmms artificial neural networks input output hmms weighted transducers variable length markov models markov switching models switching state space models :10.1.1.29.8423:10.1.1.126.6810:10.1.1.56.7115:10.1.1.53.917:10.1.1.30.5334:10.1.1.30.5334:10.1.1.33.1845
course lot hmms applications covered survey wants representative issues addressed mainly concerning learning algorithms extensions hmms related models 
note call learning called parameter estimation statistics system identification control engineering 
models probability distributions talk assumed represent necessarily true relations variables interest 
viewed tools decisions data particular new data 
important distinguish model classes correspond assumptions set solutions training algorithms em expectation maximization algorithm pick solution set optimize data dependent criterion 
models discussed call markovian models applied sequential data certain property described 
remind reader joint probability distribution sequence observations fy factored jy gamma supported national science engineering research council canada institute robotics intelligent systems 
performed bengio laboratories updates corrections comments sent bengio iro umontreal ca 
neural computing surveys www icsi berkeley edu jagota ncs notation mean probability random variable takes value context notation ambiguous 
neural computing surveys www icsi berkeley edu jagota ncs intractable general model sequential data conditional distribution jy gamma observed variable time depends details previous values gamma models discussed share property assume past sequence summarized concisely unobserved random variable called state variable carries information gamma useful describe distribution observation common models hmms best known contribution advances automatic speech recognition decades 
tutorial hmms context speech recognition :10.1.1.131.2084
see book focusses applications bioinformatics 
algorithms estimating parameters hmms developed 
application hmms speech independently proposed popularized :10.1.1.131.2084
early review alternative methods hmms related hmms speech recognition collection papers 
hmms applied variety applications outside speech recognition handwriting recognition pattern recognition molecular biology fault detection 
variants extensions hmms discussed include language models econometrics time series signal processing :10.1.1.17.9958
analysis sample computational complexity approximating distribution hmm probabilistic automaton done tools pac learning :10.1.1.116.3824
see analysis case hidden markov chains deterministic emissions shows classes markovian learning problems hard polynomial number samples required 
case probabilistic finite state automata show algorithm polynomial time complexity sample size learning subclass acyclic probabilistic finite state automata 
learning problem type algorithms discussed framed follows 
training set fd dn sequences data criterion quality model set data mapping model real valued scalar choose model certain set models way maximize minimize expected value criterion new data assumed sampled unknown distribution training data sampled 
general mathematical analysis learning theory learning algorithms discussed see example 
applications sequence observations fy new data simply continuation training data time series prediction 
applications large number training sequences different lengths thousands tens thousands sequences speech recognition databases 
applications objective model distribution sequence variables 
applications data consists sequences output variables input variables objective model conditional distribution jx 
applications input output sequences length 
example speech recognition interested distribution word sequences acoustic sequence 
sections review basic elements traditional hmms section application speech recognition section 
remaining sections describe extensions hmms markovian models related hmms hybrids artificial neural networks section input output hmms section including markov switching models section asynchronous input output hmms section generalizations hmms called weighted transducers section useful combine markovian models state space models markovian models continuous state section 
hidden markov models section remind reader basic definition hmm tutorial way 
formalize assumptions describe basic elements algorithms hmms 
algorithms estimate parameters hmms discussed details section generalized hmms input output conditional hmms 
markov model order probability distribution sequence variables fq neural computing surveys www icsi berkeley edu jagota ncs 
bayesian network representing independence assumptions markov model order jq gamma jq gamma gamma gamma fq gamma conditional independence property jq gamma jq gamma gammak gamma gammak summarizes relevant past information generally called state variable 
conditional independence property joint distribution sequence decomposed product jq gamma gammak special case markov model order models described 
case distribution simpler jq gamma completely specified called initial state probabilities transition probabilities jq gamma 
illustrate concept markov model order consider simple example 
robot discrete time step discrete positions grid different states values state variable time time step jumps position position stays position probabilities depend positions 
example probability distribution position depends current position 
plausible dependency may high probability jumping nearby position grid stay low probability jumping remote position 
model completely characterized conditional probabilities estimated simply observing long trajectory robot counting times jumps position 
bayesian network graphical representation conditional independencies random variables 
bayesian network markov model order shown 
shows directed acyclic graph dag node corresponds random variable 
edge variable variable implies causal direct influence absence edge implies conditional independence variables may exist path conditioning intermediate variables paths independent 
specifically joint probability distribution set random variables fa represented graph arbitrary connectivity product parents fb edge ag 
see formal definitions pointers related literature graphical probabilistic models inference algorithms :10.1.1.52.696:10.1.1.112.8434
see neural computing surveys www icsi berkeley edu jagota ncs relations hmms graphical models markov random fields kalman filters see application ideas turbo decoding 
formulations hmms graphical models proposed applied successfully speech recognition :10.1.1.56.8347
exploited review graphical model formulation lends naturally extensions context variables factoring state variable :10.1.1.56.8347
note probabilistic models described cast framework bayesian networks 
markovian models transition probabilities assumed homogeneous time steps 
example markov models order jq gamma jq 
homogeneous models number parameters reduced model trained sequences certain lengths generalize sequences different lengths 
sense models sequential data shows temporal translation invariance 
models input output conditional hmms section inhomogeneous different transition probabilities different time steps 
transition probabilities directly parameters model obtained parameterized function previous state conditioning variables advantages stated apply ability deal changes dynamics observed different parts sequences 
applications state variable discrete conditional distribution state variable time multinomial distribution 
exception approach briefly discussed section continuous state hmms generally called state space models 
hidden state problem markov models order quickly intractable large example multinomial state variable ng number required parameters representing transition probabilities 
necessarily restricts small value observed sequential data interest satisfy markov assumption small 
stated may sequential data modeled warrants hypothesis time past data sequence summarized concisely state variable 
precisely hidden markov models embed assume observed data sequence markov property low order unobserved related variable state variable assumed exist markov property low order typically 
hmms generally taken order hmm order emulate hmm higher order increasing number values state variable take 
return robot example suppose robot particular position different actions different probabilities 
problem observe trajectory robot actions performs time step 
try infer transition probabilities position position emission probabilities characterize distribution actions position 
relation observed sequence fy hidden state sequence shown graphically bayesian network conditional independence assumptions case order jq gamma jq jq jq simple terms state variable summarizes relevant past values observed hidden variables tries predict value observed variable state independence assumptions joint distribution hidden observed vari neural computing surveys www icsi berkeley edu jagota ncs bayesian network representing graphically independence assumptions hidden markov model order 
state sequence output observation sequence 
ables simplified follows gamma jq jq joint distribution completely specified terms 
initial state probabilities 
transition probabilities jq gamma 
emission probabilities jq 
applications state variable discrete variable state sequences forced start common initial state value state values common final state transition probabilities forced value prior knowledge structure model 
speech recognition literature talks states mean different values state random variable transition states transition probability non zero 
represent structure imposed choice zero non zero transition probabilities existence transitions talks topology hmm 
topology represented graph nodes represent values state variable states arcs represent transitions non zero probability 
graph confused graph bayesian networks introduced earlier node represents random variable 
common variant model emissions dependent current state previous state transitions jq jq gamma jq gamma computation straightforward done time 
observed really interested representing distribution 
simply marginalizing joint distribution yields exponential number terms discrete case discrete states fortunately efficient recursive way compute sum factorization probabilities takes advantage markov property order 
recursion neural computing surveys www icsi berkeley edu jagota ncs example left right topology hmm may speech recognition system represent distribution acoustic sequences associated unit speech phoneme word 
node represents value state variable arc represents transition non zero probability values state variable 
oval picture corresponds symbolic meaning word associated group states oval 
probability observing certain subsequence state takes particular value subsequence jy gamma gamma jq gamma gamma gamma jq gamma jq gamma gamma gamma gamma jq gamma jq gamma gamma gamma markov assumptions observed variable state respectively obtain second equation 
recursion initialized jq initial state probabilities 
recursion true model homogeneous probabilities conditioned variables 
recursion central algorithms hmms called forward phase 
note tp th sequence training data set length recursion allows compute likelihood function tp parameters model tuned order maximize likelihood training sequences tp 
computational cost recursion tm length sequence number non zero transition probabilities time step number values state variable take 
note applications prior knowledge imposes structure hmm form zero probability transitions 
obtained readily compute likelihood sequence follows qt note drop conditioning probabilities parameters context notation ambiguous 
neural computing surveys www icsi berkeley edu jagota ncs choice distributions transition probabilities hmms conventionally taken discrete valued hidden state multinomial distribution previous values state 
shortcut papers hmms speech recognition talk different states different values state random variable 
discrete state case model homogeneous transition parameters markov models order represented matrix transition probabilities gamma 
section discuss continuous state models called state space models state probability distribution usually gaussian mean linear function previous state 
discrete emissions types emission probabilities discrete discrete hmms continuous continuous hmms 
case discrete variable jq generally taken multinomial 
model homogeneous output distributions parameters matrix elements 
applications interest multivariate continuous 
obtain discrete distribution approaches common 
perform vector quantization order map vector valued discrete value quantize quantize jq emission probability generally :10.1.1.116.3824

multiple codebooks split vector variable sub vectors ti assumed approximately independent quantize separately maps quantize ti quantize ti jq emission probability 
example speech recognition systems represents spectral information time represents changes spectrum represents local average signal energy time time derivative 
continuous emissions continuous hmms commonly emission distributions gaussian distribution gaussian mixture jq ji ij sigma ij ji ji sigma probability observing vector gaussian distribution mean vector covariance matrix sigma 
variant continuous hmm gaussian mixtures called semi continuous hmm gaussians shared parameters specific state mixture weights jq ji sigma mixture weights play role similar multinomial coefficients discrete emission hmms described 
modeling approaches discrete features hmm selected modeler prior knowledge data number values state variable topology hmm forcing transition probabilities zero type distribution emissions includes choices number gaussians mixture 
basically address model selection question restrict discussion general prior knowledge topology speech recognition hmms numerical free parameters chosen numerically learning parameter estimation algorithm 
neural computing surveys www icsi berkeley edu jagota ncs parameter estimation point discussed hmm model 
briefly discuss issue learning parameters model 
details learning algorithms sections 
distributions em expectation maximization algorithm estimate parameters hmm order maximize likelihood function dj tp set training sequences tp length letter 
em algorithm introduced discussed formally section 
em algorithm graphical models bayesian networks 
training em algorithm addition emission distributions described previous subsection emission distributions exponential family mixtures thereof 
speech sequence recognition applications algorithm hmm conditioned sequence correct labels fw wl chooses maximizes product class conditional likelihoods jw training sequences 
noted criteria maximum likelihood criterion train hmms example incorporate prior parameters training discriminant focus doing classification correctly 
complex distributions described learning criteria maximizing likelihood data numerical optimization methods em algorithm usually derivative learning criterion respect parameters 
possible em algorithm generally preferred faster convergence properties 
section discuss em formally general framework includes hmms input output hmms 
basic principle em algorithm 
return robot example jumps position position time step takes observed action time 
knew trajectory robot addition sequence actions easy estimate transition probabilities counting relative frequencies jumps times goes position position divided times position probabilities similarly counting frequencies different actions position 
estimated parameters normalized frequencies 
know robot trajectory 
way understand em algorithm consider possible trajectories weight trajectory posterior probability trajectory observed actions current values parameters 
time step weights give different posterior probability positions states 
obtaining normalized frequency counts re estimated parameters obtain weighted frequency counts weights posterior probability particular state going particular state state 
re estimation formulae incorrect parameter values posterior weights new parameters may exactly maximize likelihood shown bring likelihood closer maximum 
iterating procedure rapidly converge maximum likelihood 
viterbi algorithm model class defined parameters model learned things done model 
subsection discuss infer state sequence observation sequence robot example infer trajectory robot sequence actions 
applications hmms speech recognition molecular biology applications example hidden state variable associated particular meaning phonemes words speech recognition :10.1.1.131.2084
state corresponds classification label states grouped label 
state sequence corresponds sequence classification labels words characters phonemes 
useful observed sequence infer state sequence corresponding 
achieved algorithms perform neural computing surveys www icsi berkeley edu jagota ncs 
dog cat 
shows part topology hmm may recognizing connected words groups state values represented nodes associated meaning word label 
word hmm represented oval groups corresponding set states 
state sequence corresponds sequence words 
transition probabilities word models language model 
maximization argmax jy argmax viterbi algorithm finds maximum relatively efficient recursive solution computational cost proportional number non zero transitions probabilities times sequence length 
fact application bellman dynamic programming algorithm 
define max gamma gamma computed follows markov conditional independence assumptions equations jq max gamma gamma jq argmax gamma gamma initialization max jq recording best previous state state time obtain sequence max max 
optimal obtained backward recursion starting argmax gamma 
forward phase computation cost viterbi algorithm tm number non zero transition probabilities time step 
number non zero transition probabilities large graph search algorithms may order look optimal state sequence 
optimal search approximate faster beam search 
large hmms speech recognition tens thousands words methods efficient 
methods employed large hmms progressive search performing multiple passes 
see details 
neural computing surveys www icsi berkeley edu jagota ncs speech recognition hmms speech recognition common application hmms discuss issues involves discussion relevant applications 
see survey statistical methods speech recognition 
basic speech recognition problem stated follows sequence acoustic descriptors obtained pre processing speech signal spectral information represented vector approximately numbers obtained rate millisecond time step find sequence words intended speaker pronounced words 
isolated speech recognition consider case isolated word recognition simpler connected speech recognition 
isolated word recognition single hmm built word preset vocabulary 
models compute jw word vocabulary acoustic sequence 
priori distribution words word acoustic sequence obtained picking model maximizes acoustic probability prior argmax argmax jw computational cost recognition simply number words times computation acoustic probability word forward computation equation 
recognition time significantly reduced search techniques mentioned previous section 
application hmms typical situation applications observation sequences single class label correct word model classify new sequences 
simple extension usual machine learning classification framework case sequential inputs fixed size vectors 
section look case class labels sequential structure take advantage model 
connected speech recognition interesting task recognizing connected speech users pause words 
case generalize isolated speech recognition system considering mapping acoustic sequence sequence words argmax jy argmax jw high level task occur applications speech recognition learn map sequences symbols vectors acoustic observations shorter sequences symbols phonemes 
equation introduced language model characterizes structural dependencies sequences symbols phonemes words 
see collection review papers subject 
language model crucial element modern speech recognition systems speech understanding systems translate speech actions word sequences particular language particular semantic context 
quality language model humans may important factors superiority speech recognition humans machines 
clearly computationally practical directly enumerate word sequences 
solutions generally adopted representing language model graphical form search techniques combine constraints acoustic model jw language model 
common type language model restricting context word bigrams jw gamma trigrams jw gamma gamma 
language models simple markovian interpretation combined acoustic hmms build large hmm transition probabilities hmms representing word possibly context words obtained neural computing surveys www icsi berkeley edu jagota ncs noun verb cat dog sentences words phonemes sub phonemic left right model example hierarchical organization speech knowledge topology hmm 
level represented different weighted transducer acceptor 
arcs represent transitions hmm states certain level groups states lower level 
low level models phoneme models shared places hmm 
neural computing surveys www icsi berkeley edu jagota ncs time viterbi search algorithms give segmentation observation sequence consecutive temporal segments associated classification label 
example temporal segments associated speech units phonemes part word dog 
language model 
example jw gamma may transition probability final state hmm word gamma initial state hmm word illustrated 
context different instantiations word may exist corresponding different contexts making hmm representing joint probability large 
large hmms represented explicitly computer particular instances word hmm created needed search algorithm traverses large virtual hmm 
transducers see section elegant way represent complicated data structures uniform mathematically grounded framework 
viterbi search algorithms may look optimal state sequence 
turn state sequence corresponds sequence words 
note sequence words may different obtained state sequence computationally expensive compute 
practice results obtained approximation 
state sequence gives segmentation speech partition observed sequence consecutive temporal segments illustrated 
sequence speech units words phonemes segmentation gives alignment observed sequence template sequence speech units represents 
hmm topology priori knowledge priori knowledge application speech language may impose structure hmm meaning values state variable 
seen state may associated certain label 
furthermore topology hmm strongly constrained transition probabilities forced zero 
number free parameters amount computation directly dependent number non zero transition probabilities imposing structure useful 
furthermore imposing structure completely answer difficult questions constructing hmm including parameters structure hidden state represent 
basic structure imposed speech hmms left right structure states word hmm ordered sequentially transitions go left right state figures 
set states generally partitioned subsets particular linguistic meaning attached phoneme word particular context 
example topology part hmm speech recognition shown 
reduce number free parameters help generalize designers speech recognition hmms notion speech unit representing particular linguistic meaning associated distribution acoustic subsequences re shared different places hmm 
simplest set speech unit think simply phoneme 
example speech unit phoneme may higher level units words contain linguistic definition 
complex speech units context dependent represent acoustic realization linguistic unit context left right linguistic units 
illustrated word pronounced sequence phonemes word hmm built concatenation corresponding speech units 
multiple pronunciations word word hmm concatenations parallel 
constraints topology sharing parameters states associated different instances speech unit represent strong assumptions relation speech signal neural computing surveys www icsi berkeley edu jagota ncs dog example constrained hmm representing conditional distribution jw 
acoustic sequence sequence words dog 
word sequence assumptions known strong 
focus current research field build faithful models keeping tractable sure imperfections model hurt final decision 
assumptions useful practice order build current state art speech recognition systems applications bioinformatics :10.1.1.131.2084
see prior topology discover sequential clusters data :10.1.1.44.3648
generally research learning language models addresses issue learning structure model addition parameters 
see example polynomial time algorithms constructively learn probabilistic structure language merging states learnability acyclic probabilistic finite automata 
learning criteria learning criterion choose parameters hmm 
subsection discuss types criteria aimed fitting observed sequences aimed discriminating observation sequences associated different classes word sequences 
applications hmms speech recognition sequences interest observation acoustic sequence classification correct word sequence traditional approach hmm speech recognition consider independently different hmm word speech unit learn distribution acoustic subsequences associated word speech unit 
concatenating speech units associated correct word sequence represent conditional acoustic probability jw 
hmm constrained knowledge correct word sequence called constrained model illustrated 
hand speech recognition correct word sequence unknown word sequences allowed language model taken account 
hmm allows word sequences called recognition model generally larger constrained model 
represents joint distribution summing possible state paths unconditional probability 
maximum likelihood criterion applied estimate parameters speech units maximize constrained acoustic likelihood tp jw training sequences 
purpose em gem algorithms described section 
approach called non discriminant learning acoustic distribution associated speech unit class conditional density functions learning various linguistic classes differ acoustically 
trying discriminate different interpretations different classes sufficient know differences jy directly describing decision surface space acoustic sequences 
non discriminant models contain additional information learn jw jy 
includes jy jy sufficient perform classification non discriminant approaches learn 
furthermore non discriminant approaches strongly rely neural computing surveys www icsi berkeley edu jagota ncs assumptions form probability density acoustic data 
models chosen represent data generally imperfect better classification results obtained objective learning closer reduction number classification errors 
approaches proposed train hmms discriminant criterion 
common maximum posteriori criterion maximize jy maximum mutual information criterion maximize log jw maximum mutual information criterion obtained comparing log probability constrained model log jw log probability unconstrained recognition model log log allowing possible interpretations word sequences 
maximizing criterion attempts increase likelihood correct constrained model decreasing likelihood models 
criteria proposed approximate minimization number classification errors 
gradient numerical optimization method generally criteria em algorithm general exception synchronous asynchronous input output hmm discrete observations described section 
performance performance speech recognition systems hmms varies lot depending difficulty task 
benchmarks compare speech recognition systems set arpa 
difficulty increases size vocabulary variability speech speakers factors 
example atis benchmark task provide airline information users vocabulary words laboratory experiments yielded incorrectly answered queries 
task involves recognition understanding 
large vocabulary task set arpa words understanding recognition word error rates reported 
performance speech recognition systems worse field laboratories 
speech recognition commercial applications telephone network 
system looks keywords 
error calls processes calls year 
imbalance emission transition probabilities problem faced classical applications hmms logarithmic scale range values emission probabilities jq take larger transition probabilities jq typically allowed transitions state state space observations large continuous 
consequence path chosen viterbi search algorithm argmax gamma jq jq influenced emission probabilities 
comparing paths equation difference emissions modeled sequence states compatible topology hmm choice existence non existence transitions obtained forcing transitions zero probability 
extreme case numerical value non zero transition probabilities completely ignored viterbi algorithm dynamic time warping match observation sequence sequence probabilistic prototypes associated emission distributions sequence state values hmm 
operational speech recognition models ignore transition probabilities altogether problem 
introduce called factor adjust weighing acoustic language model probabilities 
usually achieved linear combination log probabilities acoustic model emissions neural computing surveys www icsi berkeley edu jagota ncs language model transitions 
example 
heuristic solutions point weakness current probabilistic framework modeling speech 
notion duration speech models refers segmentation acoustic sequence intervals associated different speech units phonemes 
phonemes tend longer short duration vowels vary lot 
distribution durations associated speech unit principle represented multiple states left right structure appropriate transition probabilities 
imbalance problem constraint durations really effective obtained topology hmm forcing transition probabilities zero 
note learning algorithms parametric models em algorithm learn discrete structure topology hmm decided priori 
ignoring value non zero transition probability corresponds assigning uniform probability duration certain intervals zero outside intervals 
section discuss proposed asynchronous input output hmms significantly alleviate problem 
integrating artificial neural networks hmms artificial neural networks anns connectionist models successfully pattern recognition sequential data processing problems 
multi layered anns represent non linear regression classification model 
different researchers proposed different ways combine anns hmms particular automatic speech recognition 
proposed hybrids common neural networks capture temporally local possibly complex nonlinear dependencies hmm handle temporal structure elastic time alignment strong useful priori speech hmms 
put simply details differ neural networks generally part model represents emissions type observations associated state 
hybrids mainly differ 
mathematical formulations proposed integrate hmm output neural network takes input local speech observations 
cases neural network hmm trained separately cases training respect single criterion 
normally case ann part markovian probabilistic model compute emission probabilities 
approaches ann seen preprocessor hmm transform raw observations form easier model case possible train respect single discriminant criterion 
approaches ann post processing phase refine scores hmm associates segment speech 
proposed advantages systems include discriminant training ability represent data richer non linear models comparison gaussian discrete models improved incorporation context input multiple lagged values input variable 
models input output hmms described section designed learn long term dependencies better eliminate problem imbalance emission transition probabilities yielding effective models duration 
new variants hmms ann hmm hybrids attempt address modeling weaknesses hmms speech recognition incorrectness markov assumptions respect interpretation state values models poor modeling phoneme duration discussed section poor contextual information including short term acoustic context long term context prosody 
left right hmm seen flexible template viterbi algorithm sophisticated way align template observed speech 
anns successful classifying individual phonemes initial research focused dynamic programming tools hmms order go recognition individual phonemes local classification recognition sequences 
cases ann outputs interpreted probabilities scores generally combined dynamic programming algorithm akin neural computing surveys www icsi berkeley edu jagota ncs viterbi algorithm perform alignment segmentation 
cases dynamic programming algorithm embedded ann 
alternatively ann re score best hypotheses phoneme segmentation produced hmm assigning posterior probabilities phonemes phonetic segments hypothesized hmm 
hmm viewed particular kind recurrent ann 
ann hmm trained separately researchers proposed schemes trained ann trained way depends hmm 
models proposed bourlard rely probabilistic interpretation ann outputs 
ann trained estimate posterior probabilities hmm states context observation vectors jy gammak gamma centered current time step 
normalizing posteriors state priors obtains scaled emission probabilities jq jy gamma scaled emission probabilities usual viterbi algorithm recognition 
training ann optimal state sequence obtained constrained hmm knowledge correct word sequence 
time step ann supervised target value correct state target value states 
procedure converge yield speech recognition performance level state art systems 
bourlard draw links procedure em algorithm procedure optimize defined criterion training training local targets provided constrained viterbi alignment algorithm 
approach uses ann transform observation sequence form easier model hmm simple continuous emission models gaussian gaussian mixture 
ann non linear trainable pre processor feature extractor hmm 
case objective learning combined ann hmm system single criterion defined level sequence level individual observations segments phonemes characters 
applications idea ann viewed object spotter phoneme character speech handwriting recognition hmm post processor align sequence outputs ann higher level linguistic lexical model temporal structure observed sequences 
model introduced phoneme recognition 
described extended character recognition 
ann transforms input sequence intermediate observation sequence parameterized function 
example function may capture contextual influences transform input way invariant respect classifications interest 
basic idea implementation model optimization criterion train hmm continuous differentiable function intermediate observations gradients train parameters ann gradient descent chain rule derivatives called back propagation yields parameter gradients single sequence 
criteria considered maximum likelihood criterion maximum mutual information criterion 
cases derivatives obtained state posteriors jy computed em algorithm 
cases ann hmm hybrids possible priori idea ann accomplish train ann hmm separately 
shown experimentally ann hmm hybrid training ann jointly hmm improves performance speech recognition problem bringing error rate recognition task 
shown joint training respect discriminant criterion handwriting recognition problem reduced character error rate dictionary word dictionary 
idea training set modules separately respect global criterion gradient algorithms proposed years ago 
way integrate anns hmms mathematically clear way idea neural computing surveys www icsi berkeley edu jagota ncs bayesian network representing graphically independence assumptions synchronous inputoutput hidden markov model 
state sequence output sequence input sequence 
input output hmms described section 
input output hmms input output hidden markov models iohmms conditional hmms simply hmms emission transition distributions conditional sequence called input sequence noted case observations modeled emission distributions called outputs model represents distribution sequences conditional distribution jx :10.1.1.29.8423
simpler models input output sequences length extension section allows input output sequences different lengths 
transducers section seen generalizations conditional distributions allow input output sequences different lengths 
conditional independence assumption synchronous iohmm represented bayesian network 
say hmms homogeneous sense transition emission probability distributions depend iohmms inhomogeneous sense transition emission probability distribution change input change time 
simpler case input output sequences synchronous mathematics iohmms similar hmms general 
reason explain em algorithm train hmms iohmms section section 
ordinary hmms emission distributions homogeneous model jq iohmms timevarying conditional model jq 
similarly time invariant transition probabilities jq gamma iohmms jq gamma 
generally values inputs gammak different time steps condition distributions 
hmms pattern recognition trained choosing parameters maximize likelihood observations correct classification sequence jw iohmms may trained maximize likelihood jx decision variables observed variables literature learning algorithms iohmms proposed sequence processing tasks complex emission transition models anns :10.1.1.29.8423:10.1.1.53.917
control reinforcement learning literature similar models called partially observable markov decision processes :10.1.1.56.7115
neural computing surveys www icsi berkeley edu jagota ncs case objective model output sequence input sequence find strategy sequence actions seen inputs model minimizes cost function defined sequence outputs observed 
case iohmm represents probabilistic relationship actions observations hidden state variable 
hmms described section speech recognition represent conditional probability jw observation sequence classification sequence achieved deterministically attaching symbolic meaning different values state variable 
iohmms state variable stochastically related output variable classification problems view output sequence classification sequence input observed sequence conditioning sequence 
related iohmms research modeling tree shaped data structures sequences similar probabilistic framework 
applications conceivable hmms iohmms 
example speech recognition hmms model asynchronous iohmms model 
econometrics hmms model univariate multivariate time series model 
consider dependent variable predicted past value independent variables iohmm jx 
believe relevant hmm model joint sequence 
potential advantages iohmms hmms ffl output sequence discrete training criterion discriminant maximum posteriori criterion 
furthermore input sequence discrete em algorithm training criterion discriminant 
ffl local models emission transitions represented complex models anns flexible non linear models powerful parsimonious gaussian mixtures hmms 
furthermore anns take account wide context just observations time neighboring observations sequence violating markov assumptions independence assumptions conditioning input variable 
ann recurrent take account arbitrarily distant past contexts 
ffl compare iohmm jy hmm representing jw 
case output variable iohmm discrete takes relatively values phonemes output variable hmm acoustic vector take values 
explained section having output values reduces problem imbalance transition probabilities emission probabilities effect transition probabilities significant 
ffl expect long term dependencies easily learned iohmms hmms transition probabilities ergodic state variable mix forget past contexts quickly 
see development argument analysis difficulty learning represent long term dependencies markovian models general 
ffl obvious input output relation model modeling time series hmms appropriate iohmms offer particular 
iohmm prior knowledge kind transformations past series useful summarizing informations condition distribution value summarizing functions added ordinary state variable variables input iohmm emission distribution form jq playing role input variable iohmm 
case need values state variable provides information past sequence 
section describe particular kinds iohmms proposed econometrics literature 
section em algorithm training hmms synchronous hmms 
neural computing surveys www icsi berkeley edu jagota ncs markov switching models markov switching models introduced econometrics literature modeling non due abrupt changes regime economy 
point view taken extend time series regression models addition discrete hidden state variable allows changing parameters regression models state variable changes value 
consider example time series regression model fi observation output variable time random variable zero mean gaussian distribution vector input variables past values past values observed variables :10.1.1.30.5334
different sets parameters fi different discrete values hidden state variable basically specifies particular form emission distribution jq iohmm gaussian distribution mean linear function different parameters different values conditional mean non linear function neural network applications iohmms modeling distribution returns described shows non linear models yield improved modeling distribution stock market indices returns 
obtain complete picture joint distribution past observed values needs specify distribution state variable 
cases described econometrics literature distribution assumed time invariant specified matrix transition probabilities ordinary hmms complicated specifications suggested 
representation variance equation complex single constant parameter variance function state variable input variables 
see example markov switching arch models applied analyzing respectively changes variance stock returns interest rates 
parameters markov switching models generally estimated em algorithm maximize likelihood see section 
inference algorithms econometrics applications filtering smoothing prediction 
filtering algorithm compute estimate current distribution jy state past inputs outputs 
smoothing algorithm compute posteriori estimate distribution jy state path sequence inputs outputs 
prediction algorithm allows compute distribution states outputs past input output observations 
section consider state space models hidden state variable continuous hybrids discrete continuous state variables similar time series modeling applications 
em hmms iohmms section sketch application em expectation maximization algorithm hmms iohmms 
papers baum special case em algorithm applied discrete emissions hmms written general version em algorithm described 
basic idea em algorithm hidden variable joint distribution observed variable simpler marginal distribution observed variable 
hmms iohmms hidden variable state path seen simpler compute represent 
hidden variable em algorithm looks expectation values hidden variable log probability joint distribution 
neural computing surveys www icsi berkeley edu jagota ncs expectation called auxiliary function conditioned previous values parameters training observations 
step algorithm consists forming conditional expectation eq log qjx eq expectation fy tn set output sequences similarly respectively sets input state sequences 
em algorithm iterative algorithm successively applying step step 
step consists finding parameters maximize auxiliary function 
th iteration argmax shown increase brings increase likelihood algorithm converges local maximum likelihood yjx 
maximization done exactly increases iteration gem generalized em algorithm 
maximization general done solving system equations hmms iohmms state space models simple emission transition distributions done analytically 
discuss case discrete states expectation equation corresponds sum values state variable solution equation obtained efficiently recursive algorithms 
see rewrite joint probability states observations introducing indicator variables value log jx log jq gamma log gamma joint log probability training set sum training sequences sum 
moving expectation equation inside sums ignoring indices sequences training set notation heavy eq jx log jq eq gamma jx log gamma note expression maximization respect parameters emission transition probabilities completely decoupled separate sums 
simplify notation ignored practice forcing state sequences start state ignored initial state probabilities 
step problem simple likelihood maximization different types distributions weights probabilities sums 
weights state posterior probabilities ijx eq jx transition posterior probabilities gamma jjx eq gamma jx see posterior probabilities note jx gamma jx notation computed baum welch forward backward recursions 
neural computing surveys www icsi berkeley edu jagota ncs introduced forward recursion equation yields jx recursively 
note jx normalized perform filtering operation obtain jy 
markov assumptions equivalent equations conditioned input sequence baum welch backward recursion obtained jq jq jq jq multiplying results forward backward recursion normalizing output sequence probability obtain state posteriors smoothed estimates state distribution jx jx jq similarly transition posteriors obtained recursions emission transition probabilities follows gamma jx jq gamma gamma jx jq jq gamma care taken performing forward backward recursions order avoid numerical flow usually accomplished performing computation logarithmic scale small base 
details parameter update algorithm depends particular form emission transition distributions 
discrete exponential family mixture thereof exact simple solutions step exist weighted form maximum likelihood solutions distributions 
distributions incorporating artificial neural network compute conditional discrete probabilities conditional mean gaussian gem algorithm maximization observations likelihood numerical methods gradient ascent 
note maximizing gradient ascent equivalent maximizing likelihood gradient ascent 
shown noting quantities computed backward pass fact gradients likelihood respect quantities computed forward pass jq jx representation state variable complicated ordinary hmms multiple state variables performing step exactly difficult 
see example models discussed section 
asynchronous iohmms asynchronous hmms shown extend iohmm formalism case output sequences shorter input sequences normally case speech recognition output sequence typically phoneme sequence input sequence sequence acoustic vectors 
purpose states emit emit output time step certain probability conditioned current input 
conceived generative model output input asynchronous iohmm works follows 
time initial state chosen distribution length output sequence initialized 
time steps state picked transition distribution jq gamma state previous time step gamma current input decision taken output produced time emit distribution 
neural computing surveys www icsi berkeley edu jagota ncs positive case output produced emission distribution jq 
length output sequence increased gamma parameters model initial state probabilities parameters emit emission transition conditional distribution models emit gamma gamma jq jq gamma 
application em algorithm model similar outlined hmms synchronous iohmms forward backward recurrences require amounts storage computation proportional product input output lengths times number non zero transition probabilities ordinary hmms synchronous iohmms require resources proportional product input length times number transitions 
recognition algorithm looks output state sequence derived similarly viterbi algorithm hmms 
recognition algorithm computational complexity recognition algorithm ordinary hmms number transitions times length input sequence 
asynchronous iohmms proposed speech recognition applications map input sequences output sequences different length 
represent particular type probabilistic transducers discussed section 
acceptors transducers way view hmm way weigh various hypotheses 
example speech recognition hmms different sequences speech units corresponding subset possible state sequences associated different weights fact joint probability state sequences acoustic sequence 
generally weighted acceptors transducers assign weight sequence pair input output sequences :10.1.1.126.6810
weighted acceptors transducers attractive applications speech recognition language processing conveniently uniformly represent integrate different types knowledge sequence processing task 
advantage framework deals easily sequences different lengths 
furthermore algorithms transducers acceptors applied weight structures include limited probabilities useful sequence processing task involves processing numbers necessarily probabilistic interpretation 
weighted acceptor maps sequence scalar may probability example 
weighted transducer maps pair sequences scalar may interpreted conditional probability sequence 
weighted acceptors transducers represented labeled weighted directed graphs 
label arcs acceptor graph element set output values special null symbol 
labels associated arcs transducer graph input label output label take special null symbol value 
output sequence associated path graph associated acceptor transducer obtained sequence non null output values path 
null symbol input output sequences need length 
speech recognition hmm labels associated subsets state values speech units fact transducer weights probabilities 
represents joint distribution speech unit label sequences acoustic observations sequences 
transducers convenient represent hierarchical structure linguistic units designers speech recognition systems usually embed hmms 
language model speech recognition fact acceptor assigns probability possible sequence labels certain language 
acoustic transducer ju assigns probability speech unit phoneme particular context subsequence acoustic data intermediate transducers represent mapping sequences speech units sequences words jw 
generic composition operation allows combine cascade transducers acceptors joint distribution acoustics phonetic speech units words conditional independence neural computing surveys www icsi berkeley edu jagota ncs different levels ju jw integrates different levels knowledge data hierarchical representation speech shown :10.1.1.126.6810
search algorithms viterbi algorithm beam search 
look sequence values intermediate variables states hmms speech units words 
generalized transducers way generalize transducers proposed allows kind data structure labels discrete symbols sequences processed generalizes transducers parameterized operations weighted graphs allows cascade generalized transducers acceptors jointly trained respect global criterion 
framework data processing viewed transformation directed acyclic weighted graphs directed acyclic weighted graphs call hypotheses graphs 
graphs different graphs may represent transducers acceptors 
start node node typically represent set hypotheses input data path initial node final node corresponds distinct hypothesis weight sum product weights individual arcs 
example consider special case iohmms input sequence able compute iohmm hypothesis graph contains possible sequences weight jx path 
case graph simply regular lattice theta nodes mg 
generally hypotheses graph may different structure structure may data dependent 
feature profitably document recognition applications described 
normalized paths path weights formally interpreted probabilities different hypotheses conditional assumption correct hypothesis represented graph 
note weights formally interpreted probabilities viewed tools decision actual true probabilities certain events take place 
object maps graph called transformer viewed generalization transducer 
example iohmm represents model jx transformer 
equation transformers stacked top processing style resembles multi layer neural networks intermediate variables simple numeric vectors graphs representing set sequential interpretations data arbitrary data structures attached arcs graph 
training algorithms iohmms hmms require input output sequences observed iohmm composition trained separately framework different transformers trained jointly respect single performance criterion usually computed stage transformers cascade 
multi layer neural networks parameters transformer learned propagating gradients respect criterion reverse direction 
approach successfully part document analysis system reads amounts check images 
customers ncr process millions checks day 
transducers cascade incorporates sequence processing stages generating field location hypotheses segmentation hypotheses isolated character recognition hypotheses language model 
variable length markov models section briefly mention constructive learning algorithms acceptors transducers learn process discrete sequences language modeling tasks 
variable length markov model probability model strings state variable hidden value deterministic function past observation sequence 
function neural computing surveys www icsi berkeley edu jagota ncs uses past sequence different contexts name variable length markov model 
subsequences frequent deeper context maintained 
probability sequence form jy gamma gammad gamma gamma depth context preceding symbols gamma output distribution unconditional 
tree suffixes past contexts efficiently represent distribution node representing particular context gamma gammad children node representing contexts deeper time step 
constructive line pass learning algorithm proposed adaptively grow tree 
node tree depth represents particular value gamma gammad context depth may associated distribution symbol basic idea add child node deeper context certain values context measures sufficiently large kullback liebler divergence relative entropy output distribution child parent node 
potential branching factor tree equal size alphabet nodes may fewer children 
extension idea probabilistic synchronous transducers proposed 
conditional distribution output sequence input sequence form jx jx gammad gamma represented similar tree node represents particular input context associated distribution output input context root associated unconditional distribution output 
line pass constructive learning algorithm suffix tree transducers proposed adaptively grows tree new contexts encountered possibly maximum depth 
simple pruning algorithm discard deep nodes low posterior probability normalized product probability emitting right data times prior depends exponentially depth 
posteriors mixture large family trees formed generalization performance tracks best tree family 
algorithms language modeling handwritten character recognition :10.1.1.17.9958
state space models section draw connections hmms traditionally discrete hidden state state space models seen hmms continuous vector state variable 
keep mathematics tractable state space models restricted transition model gaussian mean vector linear function previous state possibly current inputs input output models jq gamma aq gamma bx sigma gamma sigma probability observing vector gaussian distribution mean covariance matrix sigma 
matrices parameters model 
various models covariance sigma gamma proposed may constant may depend previous state current input 
markov switching models introduced earlier state space models generally expressed functionally aq gamma bx zero mean gaussian random variable 
similarly gaussian emission model expressed equation 
kalman filter fact model associated algorithms allow compute jx forward recursion solving filtering problem 
similarly markov switching neural computing surveys www icsi berkeley edu jagota ncs models backward recursion rauch equations allows compute posterior probabilities jx solving smoothing problem 
context real time control applications learning line numerical maximization likelihood performed recursively second order method requires gradients 
line applications em algorithm backward pass equivalent rauch equations :10.1.1.131.8274
hybrids discrete continuous state disadvantage discrete representation state inefficient representation comparison distributed representation multiple state variables 
state variable take values logn bits information past observation sequence carried value 
example binary variables exponentially bits available 
types algorithms require exponentially computation called factorial hmms proposed properties approximations em algorithm cost grow exponentially :10.1.1.16.2929
models factorial distributed state appealing expressive power lot research trying computationally efficient learning algorithms see example 
hand models continuous valued state typically restricted model reasons computational tractability learning algorithm 
case problem em algorithm requires computing integrals sums done analytically gaussian case done numerically general 
model abrupt gradual changes time series researchers fact proposed hybrids state space models discrete state hmms iohmms known state space models switching jump linear systems 
see review models :10.1.1.30.5334
early models assume parameters distribution known priori approximate em algorithm heuristic step require exponential computations :10.1.1.131.2084
expensive monte carlo simulations address problem 
function lower bound log likelihood maximized tractable algorithm :10.1.1.30.5334
uses idea variational approximation proposed intractable models 
simpler version idea physics mean field approximation statistical mechanics systems 
note kind hybrid model underlines trade may occur choice model class fits data distribution efficiency training model 
similar trade offs generality model intractability learning algorithm described variants hmms finite state learning algorithms 
challenges research hidden markov models powerful models sequential data successfully applications notably speech recognition 
applied domains 
extensions related models proposed years making models applicable wider range learning tasks 
interesting questions remain unanswered research suggests promising directions 
ffl research focuses designing models better reflect data example trying remedy discrepancy markov assumptions simplify mathematics algorithms interpretations forced state variable speech recognition 
context hybrids hmms anns models asynchronous input output hmms promising clear superiority performance respect ordinary hmms remains shown 
ffl important issue directly discussed learning appropriate representation hidden state markovian models 
current applications neural computing surveys www icsi berkeley edu jagota ncs speech recognition econometric applications iohmms state space models lot prior knowledge applied definition hidden state represents order successfully learn remains learned 
happens try learn hidden state represent 
state variable keeps informations past sequence discards 
captures temporal dependencies 
shown markovian models including hmms iohmms markov switching models partially observable markov decision processes learning long term dependencies sequential data exponentially difficult span dependencies increases 
problem bad conditional models iohmms conditional markov switching models partially observable markov decision processes state state transformation conditioned extra information generally deterministic 
promising direction proposed manage problem split state variable multiple sub state variables may operate different time scales slow variables easily represent longer term context :10.1.1.16.2929:10.1.1.16.2929
see related bayesian networks factor state variable represent different types contexts 
ffl models raise general problem intractability computation likelihood step em algorithm 
address problems introduced promising methodology variational approximation tractable substructures bayesian network 
idea applied hybrids continuous discrete state variables :10.1.1.30.5334
ffl transducers offer generalization markovian models applied wide range learning tasks complex priori structural knowledge task smoothly integrated learning examples 
local probabilistic assumptions interpretations numbers processed learning algorithm may wrong inconsistent data normalization imposed probabilities may correspond strong assumptions correct solution 
difficulties inherent making probabilistic assumptions interpretations avoided removing local probabilistic assumptions delaying probabilistic interpretation final level decision 
ffl problem non stationary time series addressed certain extent iohmms markov switching models long new regimes time series resemble seen regimes 
models constructively add new states new distributions extent amount information data permits better reflect time series studied 
vein briefly mentioned variable length markov models section add context state variable training data encountered 
constructive algorithms parametric models careful balance fitting data allowing capacity representing course avoid overfitting 
ffl interesting direction research particular speech language processing applications concerns higher level tasks understanding man machine dialogue 
advocate complete integration recognition task understanding decision modules drive learning effect actions taken machine example methodologies developed reinforcement learning community 
author eon bottou patrick haffner yoram singer fernando pereira chris burges craig helpful discussions useful comments 
neural computing surveys www icsi berkeley edu jagota ncs bourlard morgan connectionist speech recognition 
hybrid approach vol 
kluwer international series engineering computer science 
boston kluwer academic publishers 
bengio neural networks speech sequence processing 
international thomson computer press 
baldi brunak bioinformatics machine learning approach 
mit press 
chrisman reinforcement learning perceptual aliasing perceptual distinctions approach proceedings th national conference artificial intelligence pp 

cacciatore nowlan mixtures controllers jump linear non linear plants advances neural information processing systems cowan tesauro alspector eds san mateo ca morgan kaufmann :10.1.1.29.8423
bengio frasconi input output hmm architecture advances neural information processing systems tesauro touretzky leen eds pp :10.1.1.29.8423
mit press cambridge ma 
meila jordan learning fine motion markov mixtures experts advances neural information processing systems mozer touretzky perrone eds mit press cambridge ma 
riley pereira weighted finite automata tools applications speech language processing tech 
rep technical memorandum tm bell laboratories 
pereira riley sproat weighted rational transductions application human language processing arpa natural language processing workshop 
mohri finite state transducers language speech processing computational linguistics vol :10.1.1.47.5713
pp 

pereira riley speech recognition composition weighted finite automata finitestate language processing roche schabes eds pp 
mit press cambridge massachussetts 
ron singer tishby power amnesia learning probabilistic automata variable memory length machine learning vol 

singer adaptive mixtures probabilistic transducers neural computation vol 

hamilton new approach economic analysis non stationary time series business cycle econometrica vol :10.1.1.30.5334
pp 
march 
shumway stoffer dynamic linear models switching amer :10.1.1.131.2084
stat 
assoc vol 
pp 

ghahramani hinton switching state space models tech :10.1.1.30.5334
rep technical report university toronto 
rabiner tutorial hidden markov models selected applications speech recognition proceedings ieee vol :10.1.1.131.2084
pp 

baum inequality applications statistical prediction functions markov processes model ecology bull 
amer 
math 
soc vol 
pp 

neural computing surveys www icsi berkeley edu jagota ncs baum petrie soules weiss maximization technique occuring statistical analysis probabilistic functions markov chains ann 
math 
statistic vol 
pp 

baum inequality associated maximization technique statistical estimation probabilistic functions markov process inequalities vol 
pp 

baker stochastic modeling automatic speech understanding speech recognition reddy ed pp 
new york academic press 
jelinek continuous speech recognition statistical methods proceedings ieee vol 
pp 

levinson rabiner sondhi application theory probabilistic functions markov process automatic speech recognition bell system technical journal vol 
pp 

rabiner juang hidden markov models ieee assp magazine pp 
january 
waibel lee readings speech recognition 
morgan kaufmann 
nag wong fallside script recognition hidden markov models international conference acoustics speech signal processing tokyo pp 

kundu bahl recognition handwritten script hidden markov model approach international conference acoustics speech signal processing new york ny pp 

burges lecun denker multi digit recognition space displacement neural network advances neural information processing systems moody hanson eds san mateo ca pp 
morgan kaufmann 
ha oh kim kwon unconstrained handwritten word recognition interconnected hidden markov models third international workshop frontiers handwriting recognition buffalo pp 
iapr may 
weissman guyon henderson recognition segmentation line hand printed words advances neural information processing systems hanson cowan giles eds denver pp 

guyon henderson line cursive script recognition time delay neural networks hidden markov models machine vision applications pp 

bengio lecun burges nn hmm hybrid line handwriting recognition neural computation vol 
pp 

krogh brown mian haussler hidden markov models computational biology applications protein modeling journal molecular biology vol 
pp 

baldi chauvin mcclure hidden markov models biological primary sequence information proc 
nat 
acad 
sci 
usa vol 
pp 

chauvin baldi hidden markov models protein coupled receptor family journal computational biology vol 
appear 
karplus barrett cline haussler hughey holm sander predicting protein structure hidden markov models proteins structure function genetics vol 
supp 
pp 

neural computing surveys www icsi berkeley edu jagota ncs smyth hidden markov models fault detection dynamic systems pattern recognition vol 
pp 

guyon pereira design linguistic postprocessor variable memory length markov models international conference document analysis recognition montreal canada pp 
ieee computer society press august 
guyon denker overview synthesis line cursive handwriting recognition techniques handbook optical character recognition document image analysis wang bunke eds world scientific 
garcia perron analysis real interest rate regime shift review economics statistics 
bengio 
experiments application iohmms model financial returns series tech 
rep epartement informatique recherche op universit de montr eal 
abe warmuth computational complexity approximating distributions probabilistic automata machine learning vol 
july 
valiant theory learnable communications acm vol :10.1.1.116.3824
pp 

sipser inference minimization hidden marko chains proceedings th annual acm conference computational learning theory colt pp 
acm 
dana ron yoram singer learnability usage acyclic probabilistic finite automata journal computer system sciences vol 
pp 

vapnik nature statistical learning theory 
new york springer 
markov example statistical investigation text eugene illustrating coupling tests chains proceedings academy science st petersburg vol 
pp 

pearl probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
spiegelhalter dawid lauritzen cowell bayesian analysis expert systems statistical science vol 
pp 

buntine operations learning graphical models journal artificial intelligence research vol 
pp 

heckerman tutorial learning bayesian networks tech 
rep tr microsoft research ftp ftp research microsoft com pub tech reports winter tr ps january 
smyth heckerman jordan probabilistic independence networks hidden markov probability models neural computation vol 
pp 

smyth belief networks hidden markov models markov random fields unifying view pattern recognition letters 
mackay mceliece 

press turbo decoding instance pearl belief propagation algorithm ieee journal selected areas communications 
zweig russel speech recognition dynamic bayesian networks proceedings aaai conference madison wisconsin aaai press 
neural computing surveys www icsi berkeley edu jagota ncs zweig russel probabilistic modeling bayesian networks asr proceedings international conference statistical language processing sidney australia 
gray vector quantization ieee assp magazine pp :10.1.1.116.3824
april 

lee automatic speech recognition development sphinx system 
kluwer academic publ 
huang 
lee 
hon semi continuous hidden markov modeling international conference acoustics speech signal processing pp 

huang jack hidden markov models speech recognition 
edinburgh university press 
dempster laird rubin maximum likelihood incomplete data em algorithm journal royal statistical society vol 
pp 

lauritzen em algorithm graphical association models missing data computational statistics data analysis vol 
pp 

viterbi error bounds convolutional codes asymptotically optimum decoding algorithm ieee transactions information theory pp 

bellman dynamic programming 
nj princeton university press 
nilsson problem solving methods artificial intelligence 
new york mcgraw hill 
ney noll data driven search organization continuous speech recognition ieee transactions signal processing vol 
pp 
february 
huang hwang improved search algorithm incremental knowledge continuous speech recognition international conference acoustics speech signal processing minneapolis minnesota pp 

weintraub large vocabulary dictation sri decipher speech recognition system progressive search techniques knowledge continuous speech recognition international conference acoustics speech signal processing minneapolis minnesota pp 

aubert ney steinbiss large vocabulary continuous speech recognition wall street journal data international conference acoustics speech signal processing adelaide australia pp 

kubala makhoul nguyen schwartz comparative experiments large vocabulary speech recognition international conference acoustics speech signal processing adelaide australia pp 

jelinek statistical methods speech recognition 
cambridge massachussetts mit press 
cole mariani zaenen zue survey state art human language technology www cse ogi edu html cambridge university press 
smyth clustering sequences hidden markov models advances neural information processing mozer jordan petsche eds mit press :10.1.1.44.3648
neural computing surveys www icsi berkeley edu jagota ncs stolcke omohundro hidden markov model induction bayesian model merging advances neural information processing systems hanson cowan giles eds san mateo ca pp 
morgan kaufmann 
stolcke omohundro best model merging hidden markov model induction tech 
rep tr international computer science institute berkeley ca january 
carrasco oncina learning regular grammars means state merging method grammatical inference applications proc 
nd international colloquium grammatical inference spain lecture notes artificial intelligence september 
brown acoustic modeling problem automatic speech recognition 
phd thesis dept computer science carnegie mellon university 
bahl brown de souza mercer speech recognition continuous parameter hidden markov models computer speech language vol 
pp 

nadas nahamoo model robust training method speech recognition ieee transactions acoustics speech signal processing vol 
assp pp 

juang discriminative learning minimum error classification ieee transactions signal processing vol 
pp 

haffner discriminant learning minimum memory loss improved rejection eurospeech madrid spain 
advanced research projects agency proceedings arpa human language technology workshop princeton new jersey march 
morgan kaufmann 
levinson statistical modeling classification survey state art human language technology cole mariani zaenen zue eds pp 
www cse ogi edu html cambridge university press 
sakoe chiba dynamic programming algorithm optimization spoken word recognition ieee transactions acoustics speech signal processing vol 
assp pp 
february 
rumelhart hinton williams learning internal representations error propagation parallel distributed processing rumelhart mcclelland eds vol 
ch 
pp 
cambridge mit press 
lippmann gold neural classifiers useful speech recognition ieee proc 
intl 
conf 
neural networks vol 
iv san diego ca pp 

bourlard links hidden markov models multilayer perceptrons ieee transactions pattern analysis machine intelligence vol 
pp 

morgan bourlard continuous speech recognition multilayer perceptrons hidden markov models international conference acoustics speech signal processing albuquerque nm pp 

bourlard speech pattern discrimination multi layered perceptrons computer speech language vol 
pp 

neural computing surveys www icsi berkeley edu jagota ncs lee waibel connectionist viterbi training new hybrid method continuous speech recognition international conference acoustics speech signal processing albuquerque nm pp 

robinson fallside recurrent error propagation network speech recognition system computer speech language vol 
pp 
july 
bottou gallinari learning vector quantization multi layer perceptron dynamic programming comparison cooperation international joint conference neural networks vol 
pp 

haffner waibel integrating time alignment neural networks high performance continuous speech recognition international conference acoustics speech signal processing toronto pp 

waibel continuous speech recognition linked predictive networks advances neural information processing systems lippman moody touretzky eds denver pp 
morgan kaufmann san mateo 
bottou fogelman blanchet speaker independent isolated digit recognition multilayer perceptrons vs dynamic time warping neural networks vol 
pp 

levin pieraccini time warping network hybrid framework speech recognition advances neural information processing systems moody hanson eds denver pp 

austin makhoul schwartz hybrid continuous speech recognition system segmental neural nets hidden markov models int 
journal pattern recognition artificial intelligence pp 

special issue applications neural networks pattern recognition guyon ed 
bridle recurrent neural network architecture hidden markov model interpretation speech communication vol 
pp 

bridle training stochastic model recognition algorithms networks lead maximum mutual information estimation parameters advances neural information processing systems touretzky ed pp 
morgan kaufmann 
morgan konig wu bourlard transition statistical training asr proceedings ieee automatic speech recognition workshop snowbird pp 

bengio de mori global optimization neural network hidden markov model hybrid ieee transactions neural networks vol 
pp 

bottou une approche th de apprentissage applications la reconnaissance de la parole 
phd thesis universit de paris xi 
bengio artificial neural networks application sequence recognition 
phd thesis mcgill university computer science montreal qc canada 
sondik optimal control partially observable markov processes finite horizon operations research vol 
pp 

sondik optimal control partially observable markov processes infinite horizon discounted case operations research vol 
pp 

neural computing surveys www icsi berkeley edu jagota ncs frasconi gori sperduti efficient classification data structures neural networks proc 
int 
joint conf 
artificial intelligence 
bengio frasconi diffusion context credit information markovian models journal artificial intelligence research vol 
pp 

markov model switching regressions journal econometrics vol :10.1.1.131.8274
pp 

shumway stoffer approach time series smoothing forecasting em algorithm journal time series analysis vol :10.1.1.16.2929
pp 


lee serial correlation discrete variable models journal econometrics vol 
pp 

hamilton analysis time series subject changes regime journal econometrics vol 
pp 

garcia fitted equilibrium asset pricing model produce mean reversion journal applied econometrics vol 
pp 

testing term structure interest rates stationary vector autoregression regime switching journal economic dynamics control vol 
pp 

hamilton autoregressive conditional changes regime journal econometrics vol 
pp 

cai markov model unconditional variance arch journal business economic statistics 
business cycle duration dependence parametric approach review economics statistics vol 
pp 

diebold evidence business cycle duration dependence business cycles indicators forecasting stock watson eds chicago university chicago press :10.1.1.16.2929
time series model periodic stochastic regime switching tech 
rep discussion universite de montreal montreal quebec canada 
garcia effects monetary policy asymmetric tech 
rep montreal quebec canada 
diebold lee regime switching time varying transition probabilities nonstationary time series analysis cointegration ed oxford oxford university press 
kiefer note switching regressions logistic discrimination econometrica vol 
pp 

kim dynamical linear models markov switching journal econometrics vol 
pp 

hamilton state space models handbook econometrics engle mcfadden eds north holland new york 
neural computing surveys www icsi berkeley edu jagota ncs bengio bengio em algorithm asynchronous input output hidden markov models international conference neural information processing xu ed hong kong pp 

bottou bengio le cun document analysis generalized transduction tech 
rep ha tm laboratories holmdel new jersey july 
lecun bottou bengio haffner gradient learning applied document recognition proceedings ieee vol 
pp 
november 
kalman bucy new results linear filtering prediction journal basic engineering asme vol 
pp 

rauch solutions linear smoothing problem ieee transactions automatic control vol 
pp 

ljung theory practice recursive identification 
mit press 
ghahramani hinton parameter estimation linear dynamical systems tech :10.1.1.131.8274
rep technical report crg tr university toronto 
ghahramani jordan factorial hidden markov models advances neural information processing systems mozer touretzky perrone eds mit press cambridge ma :10.1.1.16.2929
neal connectionist learning belief networks artificial intelligence vol 
pp 

hinton dayan frey neal wake sleep algorithm unsupervised neural networks science vol 
pp 

dayan hinton neal zemel helmholtz machine neural computation vol 
pp 

saul jordan mean field theory sigmoid belief networks journal artificial intelligence research vol 
pp 

saul jordan exploiting tractable substructures intractable networks advances neural information processing systems mozer touretzky perrone eds mit press cambridge ma 
bar shalom li estimation tracking 
boston ma artech house 
carter kohn gibbs sampling state space models biometrika vol 
pp 

likelihood estimation state estimation nonlinear state space models 
phd thesis graduate group managerial science applied economics university pennsylvania philadelphia pa 
parisi statistical field theory 
redwood city ca addison wesley 
bengio hierarchical recurrent neural networks long term dependencies advances neural information processing systems mozer touretzky perrone eds pp 
mit press cambridge ma 
