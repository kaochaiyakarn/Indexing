appear machine learning bayesian landmark learning mobile robot localization sebastian thrun www cs cmu edu thrun computer science department robotics institute carnegie mellon university pittsburgh pa editor pat langley received june revised april 
operate successfully indoor environments mobile robots able localize 
current localization algorithms lack flexibility autonomy optimality rely human determine aspects sensor data localization landmarks 
describes learning algorithm called ball enables mobile robots learn features landmarks best suited localization train artificial neural networks extracting sensor data 
rigorous bayesian analysis probabilistic localization produces rational argument evaluating features selecting optimally training networks approximate optimal solution 
systematic experimental study ball outperforms approaches mobile robot localization 
keywords artificial neural networks bayesian analysis feature extraction landmarks localization mobile robots positioning 
operate autonomously mobile robots know 
mobile robot localization process determining tracking position location mobile robot relative environment received considerable attention past years 
accurate localization key prerequisite successful navigation large scale environments particularly global models maps drawings topological descriptions cad models kortenkamp murphy press 
demonstrated survey localization methods borenstein everett feng number existing approaches diverse 
cox noted sensory information locate robot environment fundamental problem providing mobile robot autonomous capabilities 
virtually existing localization algorithms extract small set features robot sensor measurements 
landmark approaches popular years scan sensor readings presence absence landmarks infer robot position 
techniques model matching approaches extract certain geometric features walls obstacle configurations sensor readings matched models robot environment 
range features different approaches mobile robot localization quite broad 
range artificial markers barcodes natural objects ceiling lights doors geometric features straight wall segments corners 
raises question features best ones extract sense produce best localization results 
assuming features correspond landmarks robot environment questions addressed landmarks best suited mobile robot localization 
robot learn sets features define landmarks localization learn optimal features 
problem learning right landmarks recognized significant scientific problem robotics borenstein everett feng artificial intelligence greiner cognitive science chown kaplan kortenkamp 
localization algorithms enable robot learn features define landmarks 
rely static hand coded sets features localization principle disadvantages 
lack flexibility 
usefulness specific feature depends particular environment robot operates hinges availability particular type sensors 
example landmark ceiling light successfully mobile robot applications useless environment possess ceiling lights robot equipped appropriate sensor camera 
features static pre determined robot localize environments features meaningful sensors carry information extracting 

lack optimality 
feature generally applicable usually unclear optimal landmark 
course goodness features depends things environment robot operates type uncertainty faces 
existing approaches usually strive optimality lead brittle behavior 

lack autonomy 
human expert select appropriate features knowledgeable characteristics robot sensors environment 
consequently straightforward adjust existing localization approach new sensors new environments 
additionally humans fooled introspection 
human sensory apparatus differs mobile robots features appear appropriate human orientation necessarily appropriate robots 
principal deficiencies shared existing localization approaches borenstein everett feng 
presents algorithm called ball short bayesian landmark learning lets robot learn features routines extracting sensory data 
features computed artificial neural networks map sensor data feature space 
rigorous bayesian analysis probabilistic mobile robot localization quantifies average posterior error robot expected depends features extracted sensor data 
training networks minimize error robot learns features directly minimize quantity interest mobile robot localization see greiner 
conjecture learning approach proposed flexible static approaches mobile robot localization ball automatically adapt particular environment robot sensors 
conjecture ball yield better results static approaches directly chooses features optimizing utility localization 
ball increases autonomy robot requires human choose appropriate features robot 
third conjecture follow generality learning approach 
second conjecture backed experimental results illustrate ball yields significantly better results approaches localization 
section introduces basic probabilistic localization algorithm large parts adopted various successful mobile robot control systems 
section formally derives posterior error localization section derives neural network learning algorithm minimizing 
empirical evaluation comparison approaches described section followed general discussion related section 
section discusses implications points interesting directions research 

probabilistic model mobile robot localization section lays groundwork learning approach section providing rigorous probabilistic account mobile robot localization 
nutshell probabilistic localization alternates steps 
sensing 
regular intervals robot queries sensors 
results queries refine robot internal belief world located 
sensing usually decreases robot uncertainty 

acting 
robot executes action command internal belief updated accordingly 
robot motion inaccurate due slippage drift increases robot uncertainty 
derivation probabilistic model relies assumption robot operates partially observable markov environment chung state location robot 
words markov assumption states noise perception control independent noise previous points time 
various researchers demonstrated empirically probabilistic approach works dynamic populated environments due robustness underlying probabilistic representation burgard kaelbling cassandra kurien leonard durrant whyte cox koenig simmons kortenkamp weymouth nourbakhsh powers birchfield simmons koenig smith cheeseman smith self cheeseman thrun :10.1.1.31.7646

robot motion ball employs probabilistic model robot motion 
denote location robot global frame 
term location refer variables robot coordinates heading direction 
physically robot unique location point time internally belief located 
ball describes belief probability density locations denoted bel denotes space locations 
occasionally distinguish belief sensor snapshot denoted bel prior belief incorporating sensor information denoted bel posterior 
problem localization approximate closely possible true distribution robot location single peak robot location zero 
motion command translation rotation changes location robot 
expressed probabilistic terms effect motion command space motion commands described transition density specifies probability robot location previously just executed action practice usually suffices know pessimistic approximation easily derived robot kinematics dynamics 
robot sensors gradually lose information due slippage drift entropy bel increase 
incorporating sensor readings counteracts effect sensor measurements convey information robot location 

sensing denote space sensor measurements sensations denote single sensation sensations depend location robot 
denote probability observed location practice computing meaningful estimates sj difficult robotic applications 
example robot sensors include camera sj high dimensional density capable determining probability possible camera image potentially taken location full blown model environment available computing sj complex real time problem computer graphics 
current assume model environment robot sj estimated data 
overcome problem common practice extract filter lower dimensional feature vector sensor measurements 
example landmark approaches scan sensor input presence absence landmarks neglecting information contained 
model matching approaches extract partial models geometric maps sensor measurements compared existing model environment 
result comparison typically single value considered 
formally model extraction features sensor data assume sensor data projected smaller space robot function oe gamma 
maps sensations features borrowing terms signal processing literature oe called filter result filtering sensor reading oe called feature vector 
having know sj suffices know relates sensory features oe different locations environment reason called map environment 
majority localization approaches described literature assumes map borenstein everett feng 
probability learned examples 
represented piecewise constant function buhmann burgard burgard kaelbling cassandra kurien koenig simmons moravec martin nourbakhsh powers birchfield simmons koenig parameterized density gaussian mixture gaussians gelb smith cheeseman smith self cheeseman 
experimental comparison nearest neighbor algorithm represent 
landmark localization example oe filters recording presence absence individual landmarks models likelihood observing landmark various locations estimated data 
mathematically inclined reader may notice oe mathematically justified oe sufficient statistic vapnik estimating location approaches filter sensor data may yield sub optimal results ignoring important sensor information 
practice sub optimality tolerated approximate version usually easier obtain sj approximation probability 

robot localization reasons simplicity assume point time robot queries sensors executes action command terminates time 
response sensor query robot receives sensor reading extracts feature vector oe oe denote sequence feature vectors denote sequence actions 
furthermore denote sequence robot locations 
occasionally locations annotated distinguish variables integration 
initially time robot prior belief location prior belief denoted bel prior reflects robot initial uncertainty 
robot knows initial location goal localization compensate slippage drift bel pri point centered distribution peak correct location 
corresponding localization problem called position tracking 
conversely robot initial knowledge position bel prior uniform distribution 
corresponding localization problem called self localization global localization kidnapped robot problem engelson task significantly difficult position tracking 
sensor queries actions change robot internal belief 
expressed probabilistically robot belief executing gamma th action bel prior jf gamma gamma th sensor measurement bel posterior jf gamma treat cases separately starting second 

sensing bayes rule bel posterior jf gamma gamma jf gamma jf gamma markov assumption states sensor readings previous sensor readings actions knowledge exact location gamma oe follows gamma important notice markov assumption specify independence different sensor readings robot location unknown assumptions extent known localization 
mobile robot localization location usually unknown localization problem subsequent sensor readings actions usually depend 
see chung howard mine pearl thorough treatments conditional independence markov chains 
markov assumption simplifies leads important formula moravec pearl bel posterior jf gamma jf gamma bel prior jf gamma denominator right hand side normalizer ensures belief bel posterior integrates 
calculated jf gamma jf gamma bel prior summarize posterior belief bel posterior observing th feature vector proportional prior belief bel prior multiplied likelihood observing 
acting actions change location robot belief 
recall belief executing th action bel prior jf table 
incremental localization algorithm 

initialization bel gamma bel prior 
observed feature vector oe bel gamma bel bel gamma bel bel gamma normalization 
action command bel gamma bel rewritten theorem total probability jf depend action executed equivalent jf virtue markov assumption known renders bel pri expressed jf bel posterior put verbally probability time result multiplying probability previously having probability action carries robot location integrated potential locations transition probability defined section 

incremental localization algorithm beliefs updated incrementally 
follows fact belief bel posterior obtained belief bel prior just sensing belief bel prior computed belief bel posterior just executing action command 
incremental nature lets state compact algorithm probabilistic localization shown table 
seen table update bel probabilities known bel prior initial estimate uncertainty transition probability describes effect robot actions map environment 
provides graphical example illustrates localization algorithm 
initially location robot unknown orientation 
bel uniformly bel bel bel bel 
probabilistic localization illustrative example 
initially robot know bel uniformly distributed 
robot observes door changes belief accordingly 
robot moves meter forward result belief shifted flattened 
repeated observation door prompts robot modify belief approximates true location 
distributed locations shown 
robot queries sensors finds door 
information suffice determine position uniquely partially existence multiple doors environment partially feature extractor err 
result bel large door locations small shown 
robot moves forward response density bel shifted slightly flattened reflecting uncertainty introduced robot motion 
robot queries sensors finds door 
resulting density single peak fairly accurate 
robot knows high accuracy notice algorithm derived general instance updating algorithm partially observable markov chain 
example subsumes kalman filters kalman applied mobile robot localization smith self cheeseman leonard durrant whyte cox 
subsumes hidden markov models rabiner robot location state environment assumed 
due generality algorithm subsumes various probabilistic algorithms published literature mobile robot localization navigation see burgard kaelbling cassandra kurien koenig simmons kortenkamp weymouth nourbakhsh powers birchfield simmons koenig smith self cheeseman :10.1.1.31.7646

bayesian localization error section ball method learning oe 
input ball algorithm set sensor snapshots labeled location taken fhs kg denotes number training examples 
localization specific form state estimation 
common practice statistical literature state estimation vapnik casella berger effectiveness estimator judged measuring expected deviation estimated true locations 
ball learns oe minimizing deviation 

posterior error posterior key learning oe minimize localization error 
analyze error examine update rule table 
update rule transforms prior belief refined posterior belief usually accurate 
obviously posterior belief error depend oe determines information extracted sensor data denote true location robot derivation omit time index simplify notation denote error function measuring error true position arbitrary position concrete nature inessential basic algorithm example kullback leibler divergence metric distance 
bayesian localization error denoted obtained integrating error belief positions weighted likelihood bel robot assigns giving bel error computed prior sensor snapshot bel bel prior called prior bayesian error respect sensor reading denoted prior prior localization error function bel pri 
ready derive bayesian error sensor snapshot 
recall denotes true location robot 
definition robot sense feature vector probability 
response update belief equation 
posterior bayesian error error robot expected sensing obtained applying update rule error giving posterior bel posterior bel prior df posterior averaged possible sensor feature vectors weighted likelihood 
normalizer computed just equations 
far posterior error posterior corresponds single position 
averaging possible positions weighted likelihood occurrence obtain average posterior error posterior posterior bel prior df oe expression rewritten posterior oe bel prior oe oe ds oe oe bel prior error posterior exact localization error sensing 

approximating posterior posterior measures true bayesian localization error computed trivial situations solving various integrals usually mathematically impossible 
posterior approximated data 
recall learn oe robot set examples fhs kg consists sensor measurements labeled location taken 
approximate posterior expression posterior si oe bel prior oe oe oe si oe bel prior equation follows directly equation 
integration variables independent collapsed single summation training patterns si 
posterior stochastic approximation posterior data converges uniformly posterior size data set goes infinity 
leaving problems small sample sizes aside posterior lets robot compare different oe smaller posterior better oe purpose localization 
important result lets compare filters 
error posterior function prior uncertainty bel pri 
result specific oe optimal prior uncertainty perform poorly 
observation matches intuition robot globally uncertain usually advantageous consider different features knows location small margin uncertainty 

ball algorithm ball learns filter oe minimizing posterior search space filters oe computing oe argmin oe posterior oe class functions oe chosen 
section presents specific search space derives gradient descent algorithm 

neural network filters ball realizes oe collection backpropagation style feed forward artificial neural networks rumelhart hinton williams 
network denoted maps sensor data feature value 
formally oe gn gamma 
realized artificial neural network 
th network corresponds th feature dimension feature vector neural networks approximate large class functions hornik stinchcombe white 
features neural network potentially extract 
extent neural networks capable recognizing landmarks approach lets robot automatically select learn routines recognition 

stochastic filters glance appropriate define gn making feature vector concatenated dimensional output neural networks 
unfortunately definition imply contains infinite number feature vectors neural networks produce real valued outputs 
sensor readings noisy distributed continuously case sensors today robots chance zero different sensations taken location generate feature vector words gn large robot recognize previous location problem specifically occurs real valued function approximators feature detectors 
fortunately exists alternative representation nice properties 
ball algorithm jf finite 
neural network interpreted stochastic feature extractor generates value probability value probability gamma giving js js gamma assume joint probability js product marginal probabilities js js js stochastic setting lets oe express confidence result assigning probabilities different generally desirable property filter 
stochastic representation advantage important efficiency learning algorithm 
show posterior differentiable output function approximator weights biases neural networks 
differentiability necessary property training neural networks gradient descent 

neural network learning algorithm new stochastic interpretation oe requires post approximation posterior modified reflect fact oe generates probability distribution single theorem total probability starting point post posterior js sj bel prior ds js sj ds js sj bel prior ds approximation term governed posterior si js bel prior js si bel prior js js si bel prior mathematically inclined reader notice special cases 
equivalent assumes js deterministic js centered single armed appropriate definition posterior ready derive gradient descent learning algorithm training neural network feature recognizers minimize posterior done iteratively adjusting weights biases th neural network denoted direction negative gradients posterior gamma gamma posterior learning rate commonly gradient descent control magnitude updates 
computing gradient right hand side technical matter posterior neural networks differentiable posterior si posterior second gradient right hand side regular output weight gradient backpropagation algorithm derivation omit see hertz krogh palmer rumelhart hinton williams wasserman 
gradient computed posterior si bel prior fn js js gamma si bel prior fn js js delta ffi js ffi js si gamma js js bel prior si ffi gamma ffi denotes kronecker symbol js computed equation 
table describes ball algorithm summarizes main formulas derived previous section 
ball input data set specific prior belief bel pri 
train networks different prior beliefs characterized different entropies degrees uncertainty 
gradient descent update repeated reaches termination criterion early stopping cross validation set posterior regular backpropagation hertz krogh palmer 
table 
ball algorithm learning neural network filters oe 
input data set kg prior belief bel prior 
output optimized parameters weights biases wi networks gn algorithm 
initialize parameters wi network small random values 

iterate convergence criterion fulfilled si compute conditional probabilities js ae gamma output th network input cf 

compute error posterior cf 
posterior si bel prior delta fn js js si bel prior gamma network parameters compute posterior wi si wi si bel prior delta fn js js ffi gamma delta ffi js ffi js si gamma js js bel prior si gradients wi obtained backpropagation cf 

network parameters update cf 
wi gamma wi gamma posterior wi ball differs conventional backpropagation supervised learning target values generated outputs neural networks 
quantity interest posterior minimized directly 
output characteristics individual networks features extract emerge side effect minimizing post output ball algorithm set filters specified set weights biases different networks 
noted posterior resulting filter oe depend uncertainty bel prior 
presenting experimental results show cases uncertainty small entropy bel prior low quite different features extracted uncertainty large 
networks trained particular bel prior estimate location arbitrary uncertainties bel pri degraded performance 
helpful necessary train different networks different prior uncertainties 

algorithmic complexity complexity learning performance methods analyzed separately 
localization algorithm described table executed real time robot operation learning algorithm described table run offline 
primary concern analysis time complexity 

localization complexity probabilistic localization table depends representation bel 
worst case processing single sensor reading requires kn nw time training set size number networks number weights biases neural network 
processing action requires time 
various researchers implemented versions probabilistic localization algorithm real time burgard burgard fox thrun kaelbling cassandra kurien koenig simmons nourbakhsh powers birchfield simmons koenig thrun thrun :10.1.1.31.7646
relatively small computational overhead existing implementations scaling larger environments problematic 

learning ball requires time number gradient descent iterations 
number training patterns greater number inputs number hidden units network reasonable assumption number free parameters exceeds number training patterns huge margin dominates 
normal conditions training networks requires time 
constant factor small cf 
table 
existing localization algorithms features landmarks indicating small values practice 
ways reduce complexity learning 
training networks parallel trained similar way units trained cascade correlation algorithm fahlman lebiere 
sequential training reduce worst case exponential linear complexity networks trained requires time 

compact representations bel reduce complexity significantly 
example burgard 
koenig simmons simmons koenig number grid cells represent bel independent training set size 
representations learning algorithm scale quadratically size environment linearly size training set 
addition coarse grained representations reported koenig simmons simmons koenig reduce constant factor 

learning algorithm table interleaves computation posterior derivatives update weights biases 
bulk processing time spent computing posterior derivatives complexity reduced modifying training algorithm multiple updates networks parameters interleaved single computation posterior derivatives 
necessary steps include 
network outputs computed training example hs 
gradients posterior respect network outputs computed cf 


training example hs pseudo patterns generated current network output conjunction corresponding gradients giving gamma posterior ae 
patterns fitted multiple epochs regular backpropagation 
algorithm approximates gradient descent reduces complexity constant factor 
addition modifications online learning stochastic gradient descent higherorder methods momentum conjugate gradient methods hertz krogh palmer yield speedup 
little currently known principal complexity bounds apply 
noted learning oe done offline done 
modifications proposed complexity training low order polynomial linear light modifications discussed scaling approach larger environments larger training sets neural networks appear problematic 
implementation see training networks required minutes hours mhz pentium pro 

real world interface robot research 
testing environment 

empirical evaluation comparison section presents empirical results obtained ball data obtained mobile robot equipped color camera array sonar sensors shown 
compare approach state art methods reimplemented previously published approaches 

doors 
team researchers university developed similar probabilistic localization method uses doors primary landmark see koenig simmons simmons koenig 
group interested reliable long term mobile robot operation reason operated autonomous mobile robot daily basis years robot moved km hours 
located building group unique opportunity conduct comparisons environment sensor configuration 

localization ceiling lights 
various research teams successfully ceiling lights landmarks including robotics built landmark commercial service robot application deployed hospitals world wide king 
navigation system extremely reliable 
building ceiling lights easy recognize stationary rarely blocked obstacles making prime candidate landmarks mobile robot localization 
previously best localization algorithm thrun press thrun model matching distributed commercially mobile robot manufacturer real world interface included comparison approach incapable localizing robot global uncertainty 
fact approaches literature restricted position tracking localization assumption initial position known 
approaches global localization require single sensor snapshot suffices disambiguate position assumption rarely holds true practice 

testbed implementation section describes robot environment data specific implementation experiments 

environment shows hand drawn map testing environment meter long corridor segment 
environment contains windows corners various doors elevator trash bins hallway 
environment dynamic 
data recorded corridors populated status doors changed natural daylight strong effect camera images taken close windows 
strictly speaking dynamics violate markov assumption cf 
section documented burgard burgard kaelbling cassandra kurien leonard durrant whyte cox koenig simmons nourbakhsh powers birchfield smith self cheeseman probabilistic approach fairly robust dynamics :10.1.1.31.7646

data collection data collection robot moved autonomously approximately cm sec controlled local obstacle avoidance navigation routines fox burgard thrun 
separate runs total sensor snapshots collected mb raw data 
data recorded different pointing directions robot camera 
data set snapshots camera pointed outer side corridor doors clearly visible robot passed 

data set snapshots camera pointed interior building 
total number doors smaller doors wider 

data set data points camera pointed ceiling 
data set compare landmark localization ceiling lights 
illumination different runs varied slightly data recorded different times day 
individual run approximately quarters data training quarter testing different partitionings data different runs 
partitioning data items collected run part partition 
noted robot started specific location run moved autonomously meter long segment corridor 
principal heading directions data avoid collisions humans obstacle avoidance routines substantially changed heading robot 
data collected close center corridor 
consequently networks oe map specialized navigation algorithms thrun 
similar kuipers byun kuipers byun mataric definition landmark requires robot particular navigation algorithm stay certain proximity obstacles 
robot travels corridor directions everyday operation felt purpose scientific evaluation data obtained single travel direction sufficient 
location modeled dimensional variable hx measuring exact locations robot hand feasible large number positions robot odometry position tracking algorithm described thrun press derive position labels 
error automatically derived position labels significantly lower tolerance threshold existing navigation software thrun 

preprocessing runs images preprocessed eliminate daytime variations reduce dimensionality data 
pixel mean variance normalized image 
subsequently image subdivided equally sized rows independently equally sized columns 
row column characteristic image features computed ffl average brightness ffl average color color channels ffl texture information average absolute difference rgb values adjacent pixels subsampled image size computed separately color channel 
addition sonar measurements collected resulting total theta sensory values sensor snapshot 
course research tried variety different image encodings appeared significant impact quality results 
features somewhat specific domains possess brightness color texture cues believe applicable wide range environments 
basic learning algorithm depend specific choice features require preprocessing reasons computational efficiency 

neural networks experiments multi layer perceptrons sigmoidal activation functions rumelhart hinton williams filter preprocessed sensor measurements 
networks contained input units hidden units output unit 
runs different network structures hidden layers gave similar results long number hidden units layer smaller 
decrease training time pseudo pattern training method described item section interleaving steps backpropagation training computation posterior derivatives 
networks trained learning rate momentum version conjugate gradient descent hertz krogh palmer 
modifications basic algorithm exclusively adopted reduce training time initial comparison unmodified algorithm see table gave statistically indistinguishable results 
noted learning required minutes hours mhz pentium pro 

error function implementation error measures distance robot travel move robot travel erroneously believing larger error 

map nearest neighbor franke stanfill waltz compute 
specifically experiments entire training set memorized 
query location set values computed data points nearest nearness calculated euclidean distance 
desired probability assumed average gamma 
approach reasonably practice 
issue best approximate finite sample sizes orthogonal research described investigated depth example see burgard 
gelb nourbakhsh powers birchfield simmons koenig smith self cheeseman literature topic 

testing conditions particularly interested measuring performance different uncertainties local global 
comparisons motivated observation utility feature depends crucially uncertainty cf 
section 
runs uncertainty bel prior uniformly distributed centered true unknown position 
particular distributions widths gamma gamma gamma gamma gamma gamma 
range uncertainties captures situations global uncertainty situations robot knows position small margin 
uncertainty gamma corresponded global uncertainty environment long 
refer uncertainties spectrum gamma gamma local 
uncertainty gamma generally sufficient autonomous navigation routines smaller uncertainty 
bel prior training refer training uncertainty 
testing call testing uncertainty 
evaluating ball different prior uncertainties training testing investigate robustness approach 

dependent measures absolute error posterior depends prior uncertainty bel prior difficult compare different prior uncertainties 
chose measure error reduction defined gamma posterior prior posterior error posterior prior error prior denotes approximation prior data example prior error prior times large posterior error posterior sensor snapshot error reduction 
larger error reduction useful information extracted sensor measurement localization 
experiments initial error globally uncertain robot lower error reduce error 
advantage plotting error reduction absolute error results scale regardless prior uncertainty facilitates comparison 

results central hypothesis underlying research filters learned ball outperform human selected landmarks 
primary purpose experiments compare performance ball approaches 
performance measured terms localization error 
secondary purpose experiments understand features ball uses localization 
features ball similar chosen humans radically different 
set experiments evaluated error reduction ball compared approaches different experimental conditions 
error reduction directly measures usefulness filter oe particular type landmark localization 
lets judge empirically efficient approach estimating robot location 
different experiments conducted different uncertainties local global different numbers networks 
neural network uncertainty training testing experiment ball train single neural network compared approaches 
different approaches evaluated different uncertainties experiment testing uncertainty training uncertainty 
consequently results obtained represent best case ball oe trained specific uncertainty testing 
shows error reduction obtained different data sets 
diagrams solid line indicates error reduction obtained ball dashed line depicts error reduction approaches localization doors figures ceiling lights 
graphs confidence intervals shown 
seen diagrams ball significantly outperforms approaches 
example results obtained data set indicate doors appear best suited sigma uncertainty 
robot knows location sigma doors landmarks reduce uncertainty average 
ball identifies global uncertainty error reduction global uncertainty error reduction global uncertainty error reduction 
average results 
dashed line indicates error reduction obtained doors solid line indicates error reduction filters learned ball 
filter reduces error average 
comparison demonstrates ball extracts useful features sensor data 
advantage approach larger increasing uncertainties 
example robot uncertainty sigma ball reduces error data set doors reduce error little 
similar results occur data sets 
example data set camera pointed inside wall door approach reduces error maximum average 
environment doors appear best suited sigma uncertainty sigma basically doors interior side testing corridor wider fewer 
ball outperforms door approach 
successfully identifies feature reduces uncertainty equal conditions 
largest relative advantage approach door approach occurs robot globally uncertain position 
ball reduces error door approach yields noticeable reduction 
results obtained data set camera pointed upward generally similar 
prior uncertainty sigma ceiling lights manage reduce error indicating significantly better suited type uncertainty doors 
attribute finding fact ceiling lights spaced regular intervals meters 
ball outperforms localization ceiling lights cases seen 
example reduces error sigma prior uncertainty sigma uncertainty 
results significant confidence level 

multiple neural networks uncertainty training testing second experiment held uncertainty constant varied number networks dimensionality feature vector 
described section ball simultaneously train multiple networks 
primary result study number networks increases ball advantage alternative approaches increases 
table shows average error reduction confidence interval obtained sigma uncertainty different data sets 
difference ball approaches huge 
approach finds features reduce error average data set approaches reduce error respectively 
numbers ball times data efficient alternatives 
understand performance improvement single network case important notice multiple networks tend extract different features 
networks recognized features output redundant result 
outputs differ networks generally extract information sensors usually lead improved results demonstrated performance results 
observation different networks tend select different features result minimizing posterior error posterior shown earlier version table 
comparison ball approaches sigma uncertainty 
specifies dimension feature vector number networks numbers table give average error reduction confidence interval 
results rows sigma 
data set doors sigma sigma gamma ceiling lights gamma gamma sigma ball sigma sigma sigma ball sigma sigma sigma ball sigma sigma sigma ball sigma sigma sigma article thrun output networks largely uncorrelated demonstrating different non redundant features extracted different networks 

multiple neural networks different uncertainty training testing third experiment investigated ball performance trained particular uncertainty tested 
experiments practical importance current implementation slowness learning procedure prohibits training new networks time uncertainty changes 
conducted series runs networks trained sigma uncertainty tested various different uncertainties 
extreme case testing uncertainty sigma training uncertainty sigma 
shows results obtained networks different data sets 
expected networks perform best training uncertainty equals testing uncertainty 
primary results performance degrades gracefully training uncertainty differs drastically testing uncertainty networks extract useful information localization 
ball outperforms alternative approaches produces statistically indistinguishable results 
results suggest environment single set filters sufficient localization results multiple sets filters 

global localization final experiment ball applied problem global localization robot know initial location 
model assuming initial uncertainty uniformly distributed 
robot moves internal belief refined sensor readings 
environment single sensor snapshot usually insufficient determine position robot uniquely 
multiple sensor readings integrated time 
conducted global localization runs comparing door localization approach ball 
run robot started random position corridor 
sensor global testing uncertainty error reduction global testing uncertainty error reduction global testing uncertainty error reduction 
different uncertainties learned filters reduce error significantly recognize doors 
shows results obtained networks trained sigma uncertainty 
ball error reduction plotted solid line dashed line depicts error reduction doors landmarks 
shows results data set 

example global localization ball 
curve represents belief bel robot different time 
initially top curve robot position center rectangle 
initial belief bel uniformly distributed 
robot senses moves forward bel refined 
sensor measurements bel centered correct position 
numbers right side depict error posterior different stages experiment 
snapshots taken meter incorporated internal belief probabilistic algorithm described table 
approaches door approach ball data performance difference exclusively due different information extracted sensor readings 
ball networks trained sigma uncertainty prior robot operation held constant robot localized 
depicts example run ball 
shows belief bel error different stages localization 
row gives belief bel different time time progressing top bottom 
row true position robot marked square 
seen features extracted ball reduce uncertainty fairly effectively sensor readings meters robot motion robot knows summarizes result comparison ball door approach dashed line 
shows average error cm function distance traveled averaged different runs randomly chosen starting positions confidence intervals 
results demonstrate relative advantage learning features 
robot travel average error door approach 
contrast ball attains accuracy making approximately times data efficient 
ball yields average error smaller 
differences statistically significant level 
noted applied larger environments bidirectional case number sensor readings required global localization increases 
quantify increase useful consider problem localization information theoretic viewpoint 
reducing uncertainty log independent bits information 
takes bits code symbol alphabet symbols 
course consecutive sensor readings independent reducing amount information convey 
empirically ball requires average sensor readings reduce error 
numbers suggest average needs meters traveled 
absolute error cm function meters traveled averaged different runs different starting points 
meters robot takes sensor snapshot 
dashed line indicates error doors landmarks solid line corresponds ball gives superior results 
sensor readings bit independent position information 
consequently corridor twice size considered bidirectional case sensor readings sufficient obtain average localization error 

features neural networks extract 
analyzing trained neural networks led interesting findings 
general networks mixture different features localization doors dark spots wall color hallways blackboards 
runs networks sensitive spot corridor physical appearance differ places 
closer investigation spot revealed due irregular pattern ceiling lights wall slightly darker rest corridor 
illumination difference barely visible human eyes compensate total level illumination 
camera sensitive total level illumination explains robot repeatedly selected spot localization 
investigated effect different training uncertainties filter oe 
results suggest different filters learned different uncertainties question arises type features different conditions 
data set example depicts example outputs trained networks sigma sigma sigma training uncertainty 
curve plots output value corresponding network evaluated long corridor 
obviously features extracted different networks differ substantially 
network trained small uncertainty sensitive local features doors hallways dark spots network trained large uncertainty exclusively sensitive color walls 
roughly quarter corridor orange quarters light brown 
robot globally ignorant position wall color vastly superior feature illustrated performance results described previous section 
robot slightly uncertain wall color poor feature 
uncertainty gamma uncertainty gamma uncertainty gamma 
example output characteristics filter plotted data obtained runs 
filters trained sigma sigma sigma global uncertainty 
depicts output networks simultaneously trained local uncertainty sigma 
discussed different networks specialize different perceptual features 
examples detailed discussion different features learned various experimental conditions thrun 

related mobile robot localization frequently recognized key problem robotics significant practical importance 
cox considers localization fundamental problem providing mobile robot autonomous capabilities 
book borenstein everett feng provides excellent overview state art localization 
localization particular localization landmarks plays key role various successful mobile robot architectures 
localization approaches horswill koenig simmons kortenkamp weymouth mataric simmons koenig localize robot relative landmarks topological map ball localizes robot metric space approach burgard 

localization approaches localize robot globally mainly track position robot 
authors proposed probabilistic representations localization 
kalman filters gelb smith cheeseman smith self cheeseman represent location robot gaussian distribution represent unimodal distributions usually unable localize robot globally 
feasible represent densities mixtures kalman filters approach map building reported cox remedy limitation conventional kalman filters 
probabilistic approaches described burgard 
kaelbling cassandra kurien koenig simmons nourbakhsh powers birchfield simmons koenig employ mixture models discrete approximations densities represent multi modal distributions 
approaches capable localizing robot globally 
probabilistic localization algorithm described section 
example output characteristics filters optimized sigma uncertainty 
borrows literature generalizes approaches 
smoothly blends position tracking global optimization single update equation 
existing approaches mobile robot localization extract static features sensor readings usually hand crafted filter routines 
popular class approaches mobile robot localization localization landmarks scan sensor readings presence absence landmarks 
diverse variety objects spatial configurations landmarks 
example landmark approaches reviewed borenstein everett feng require artificial landmarks bar code reflectors everett reflecting tape ultrasonic beacons visual patterns easy recognize black rectangles white dots borenstein 
approaches natural landmarks require modifications environment 
example approaches kortenkamp weymouth mataric certain gateways doors walls vertical objects determine robot position robot uses ceiling lights position king 
approaches reported cartwright fisher walker dark bright regions vertical edges landmarks 
just representative examples different landmarks localization 
map matching comprises second quite popular family approaches localization converts sensor data metric maps surrounding environment occupancy grids moravec maps geometric features lines 
sensor map matched global map learned provided hand cox schiele crowley thrun thrun press yamauchi beer yamauchi langley 
map matching approaches filter sensor data obtain map map localization 
approaches common fact features extracted sensor readings predetermined 
hand selecting particular set landmarks robot ignores information sensor readings experiments demonstrate carry additional information 
mapping sensor readings metric maps fixed way robot ignores potentially relevant information sensor readings 
current lets robot determine features extract localization utility localization 
shown empirical comparison enabling robot extract features learning landmarks noticeable impact quality results 
probably related research greiner 
approach select set landmarks larger predefined set landmarks error measure bears close resemblance ball 
selection driven localization error sensing determined empirically training data 
regard approaches exploit basic objective function learning filters 
greiner approach differs ball primarily respects 
assumes exact location landmark known learning 
robot define landmarks starts set human defined landmarks rules suited localization 
second tailored correcting small localization errors perform global localization 
third class landmarks ball learn broader 
filters greiner functions parameters distance angle type landmark contrast ball employs neural networks degrees freedom 

bayesian approach mobile robot localization 
approach relies probabilistic representation uses bayes rule incorporate sensor data internal beliefs model robot motion 
key novelty method called ball training neural networks extract low dimensional feature representation highdimensional sensor data 
rigorous bayesian analysis probabilistic localization provided rational objective training neural networks directly minimize quantity interest mobile robot localization localization error 
result features extracted sensor data emerge side effect optimization 
empirical comparison localization algorithms various conditions demonstrated advantages ball 
compared ball existing method koenig simmons doors landmark localization king method uses ceiling lights 
experiment approach identified features led superior localization results 
cases performance differences large particularly localization global uncertainty 
ball ability learn features discover landmarks important consequences mobile robot navigation 
particular probabilistic paradigm principal advantages compared conventional approaches problem 
autonomy 
ball obviates need manually determining features extract sensor data 
previous approaches localization human designer manually specify features extract 
example landmark approaches rely predetermined landmarks chosen human expert requires expertise robot sensors environment 
ball replaces need selecting right features automated learning 

optimality 
ball filters learned attempting optimize accuracy localization routines employs 
asymptotically minimizing bayesian error yield optimal filters 
course ball fail find optimal set filters reasons ball trained finite sample sets backpropagation networks sufficient represent optimal filters gradient descent training procedure converge local minimum 

environmental flexibility 
method customize different environments 
routine relies static built landmarks fail environments possess landmarks 
example ceiling lights appropriate landmarks environments inappropriate environments possess 
providing method supports automatic customization robots environments hope achieve level flexibility facilitates design service robot applications service robots operated private homes design varies greatly home home 

sensor flexibility 
current approach hinge specific sensor technology 
contrast existing localization approaches closely tied particular type sensor 
example routines rely visual cues require robot equipped camera 
ball flexible automatically adapts particular type sensor 
conjecture scale better high dimensional sensor spaces arise large number sensors simultaneously 
exploiting information high dimensional sensor spaces proven extremely difficult human engineers reason believe data driven learning approaches proposed ultimately lasting contribution field robotics 
key limitation current approach learn location landmarks coordinates 
learns associate sensor readings robot locations 
robot knew location landmarks apply projective geometry predict landmark appearance different nearby locations 
current approach possible suspect limitation causes increased need training data 
second limitation arises fact initial training phase learning discontinued 
environment changes desirable localization routines adapt changes appears feasible extend ball accordingly 
robot knows roughly ball position estimates label sensor data automatically self labeled data training 
effectiveness extension practice remains seen 
applied ball specific problem mobile robot localization mathematical framework general applied range decision problems arising context mobile robotics 
include 
active sensing 
shown thrun mathematical framework yields rational incentive pointing sensors best localize robot 
empirical results demonstrated actively controlling pointing direction robot camera minimize expected localization error efficiency robustness localization improved 

sensor selection 
approach determine sensors include mobile robots 
previously robot designers lacked formal method determining sensors best suited localization 
comparing bayesian localization error different type sensors analysis provides rational criterion determining sensors best mount 

navigation localization 
approach determine move best localize robot problem previously studied kaelbling cassandra kurien 
mathematical details discussed thrun similar approach empirical results burgard fox thrun 
bayesian method instance general approach estimation hidden state integration high dimensional sensor data time 
put light limiting assumption current approach requirement training hidden state accessible 
obviously reasonable assumption situations studied unreasonable situations 
open research issue extension current methods estimation hidden state accessible 
preliminary results carried lab context intelligent building control led extension situations low dimensional projection hidden state accessible training 
suspect general paradigm bayesian analysis potential new class capable learning algorithms current just initial example 
acknowledgments author wishes rhino xavier mobile robot groups particularly wolfram burgard dieter fox tom mitchell reid simmons stimulating discussions feedback course research 
editor pat langley anonymous reviewers suggestions improved quality manuscript particular pat langley iterations improvements 
research sponsored part daimler benz research frieder defense advanced research projects agency darpa missile system command contract number 
views contained document author interpreted necessarily representing official policies endorsements expressed implied daimler benz research missile system command united states government 
notes 
noted mataric intuitively clear concept landmark difficult define 
adopt mataric definition define landmark element object feature serve point 
reader may notice authors chown kaplan kortenkamp kuipers levitt propose specific definitions landmarks unique correspond physical objects visible 

error measures measures related efficiency robot motion considered focuses problem state estimation current approach address robot control 

experiments observe overfitting effect experiments reported thrun demonstrate 
simply trained networks large number iterations 

see chatila laumond schiele crowley yamauchi beer 
far tracking position robot concerned results obtained directly transfer bidirectional case robot turns unnoticed ffi global localization bidirectional case generally difficult approach due increased number possible locations 
main point provide ways learning features landmarks localization reason believe qualitative results obtained transfer bidirectional case change goes corridor 
section quantifies effect larger environments general bidirectional case 

examples cox cox horswill fukuda koenig simmons kortenkamp weymouth leonard durrant whyte leonard durrant whyte cox mataric neven schoner nourbakhsh powers birchfield peters schiele crowley simmons koenig thrun wei von various chapters kortenkamp murphy press 


mobile robot localization tech 
rep scr tr 
princeton siemens corporate research 
borenstein 

nursing robot system 
doctoral dissertation technion haifa israel 
borenstein everett feng 

navigating mobile robots systems techniques 
wellesley ma peters buhmann 

data clustering learning 
arbib ed handbook brain theory neural networks pp 

cambridge ma books mit press 
buhmann burgard cremers fox hofmann schneider thrun 

mobile robot rhino 
ai magazine 
burgard fox schmidt 

estimating absolute position mobile robot position probability grids 
proceedings thirteenth national conference artificial intelligence pp 

menlo park ca aaai press mit press 
burgard fox schmidt 

position tracking position probability grids 
proceedings euromicro workshop advanced mobile robots pp 

ieee computer society press 
burgard fox thrun 

active mobile robot localization 
proceedings fifteenth international joint conference artificial intelligence pp 

san francisco ca 
casella berger 

statistical inference 
pacific grove ca wadsworth brooks 
chatila laumond 

position referencing consistent world modeling mobile robots 
proceedings ieee international conference robotics automation pp 

chown kaplan kortenkamp 

prototypes location associative networks plan unified theory cognitive mapping 
cognitive science 
chung 

markov chains stationary transition probabilities 
berlin germany springer publisher 
cartwright 

landmark learning bees 
journal comparative physiology 
cox 

experiment guidance navigation autonomous robot vehicle 
ieee transactions robotics automation 
cox 

modeling dynamic environment bayesian multiple hypothesis approach 
artificial intelligence 
engelson 

passive map learning visual place recognition 
doctoral dissertation department computer science yale university 
everett gage laird 

real world issues warehouse navigation 
proceedings spie conference mobile robots ix 
volume 
boston ma 
fahlman lebiere 

cascade correlation learning architecture tech 
rep cmu cs 
pittsburgh pa carnegie mellon university computer science department 
fox burgard thrun 

dynamic window approach collision avoidance 
ieee robotics automation magazine 
franke 

scattered data interpolation tests methods 
mathematics computation 
fukuda ito arai abe tanaka 

navigation system ceiling landmark recognition autonomous mobile robot 
proceedings international conference industrial electronics control instrumentation pp 

gelb 

applied optimal estimation 
cambridge ma mit press 
greiner 

learning select useful landmarks 
proceedings national conference artificial intelligence pp 

menlo park ca aaai press mit press 
hertz krogh palmer 

theory neural computation 
redwood city ca addison wesley 


environment perception laser radar fast moving robot 
proceedings symposium robot control pp 

karlsruhe germany 
hornik stinchcombe white 

multilayer feed forward networks universal approximators 
neural networks 
horswill 

specialization perceptual processes tech 
rep ai tr 
cambridge ma mit artificial intelligence laboratory 
howard 

dynamic programming markov processes 
cambridge ma mit press wiley 
kaelbling cassandra kurien 

acting uncertainty discrete bayesian models navigation 
proceedings ieee rsj international conference intelligent robots systems pp 

kalman 

new approach linear filtering prediction problems 
transactions asme journal basic engineering 
king 

autonomous mobile robot navigation system 
proceedings spie conference mobile robots pp 

volume 
boston ma 
koenig simmons 

passive distance learning robot navigation 
proceedings thirteenth international conference machine learning pp 

san mateo ca morgan kaufmann 
kortenkamp murphy 
eds 
press 
ai mobile robots case studies successful robot systems 
cambridge ma mit press 
kortenkamp weymouth 

topological mapping mobile robots combination sonar vision sensing 
proceedings twelfth national conference artificial intelligence pp 

menlo park ca aaai press mit press 
kuipers byun 

robust qualitative method spatial learning unknown environments 
proceeding eighth national conference artificial intelligence pp 

menlo park ca aaai press mit press 
kuipers byun 

robot exploration mapping strategy semantic hierarchy spatial representations 
journal robotics autonomous systems 
kuipers levitt 

navigation mapping large scale space 
ai magazine 
leonard durrant whyte 

directed sonar sensing mobile robot navigation 
boston ma kluwer academic publishers 
leonard durrant whyte cox 

dynamic map building autonomous mobile robot 
international journal robotics research 
mataric 

distributed model mobile robot environment learning navigation 
master thesis mit artificial intelligence laboratory cambridge ma 
mine 

markovian decision processes 
new york ny american elsevier 
moravec 

sensor fusion certainty grids mobile robots 
ai magazine 
moravec martin 

robot navigation spatial evidence grids internal report 
pittsburgh pa carnegie mellon university robotics institute 
neven schoner 

dynamics parametrically controlled image correlations organize robot navigation 
biological cybernetics 
nourbakhsh powers birchfield 

office navigating robot 
ai magazine 
pearl 

probabilistic reasoning intelligent systems networks plausible inference 
san mateo ca morgan kaufmann publishers 
peters guo beck 

fuzzy logik internal report 
sankt augustin germany german national research center information technology gmd 


points spatial cognition elusive landmark 
british journal developmental psychology 
rabiner 

tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
ieee log number 


concurrent localisation map building mobile robots ultrasonic sensors 
proceedings ieee rsj international conference intelligent robots systems pp 

yokohama japan 


navigation indoor environments 
graefe ed intelligent robots systems 
amsterdam elsevier 
rumelhart hinton williams 

learning internal representations error propagation 
rumelhart mcclelland eds parallel distributed processing 
cambridge ma mit press 
schiele crowley 

comparison position estimation techniques occupancy grids 
proceedings ieee international conference robotics automation pp 

san diego ca 
simmons koenig 

probabilistic robot navigation partially 
proceedings fourteenth international joint conference artificial intelligence pp 

san mateo ca morgan kaufmann 
smith self cheeseman 

estimating uncertain spatial relationships robotics 
cox wilfong eds autonomous robot 
berlin germany springer verlag 
smith cheeseman 

representation estimation spatial uncertainty tech 
rep tr 
menlo park ca sri international 
stanfill waltz 

memory reasoning 
communications acm 
thrun 

exploration model building mobile robot domains 
proceedings international conference neural networks pp 

san francisco ca ieee neural network council 
thrun 

bayesian approach landmark discovery active perception mobile robot navigation tech 
rep cmu cs 
pittsburgh pa carnegie mellon university department computer science 
thrun 

know know utility models mobile robotics 
ai magazine 
thrun 
press 
learning maps indoor mobile robot navigation 
artificial intelligence 
thrun burgard fox hofmann 
press 
map learning high speed navigation rhino 
kortenkamp bonasso murphy eds ai mobile robots case studies successful robot systems 
cambridge ma mit press 
thrun fox burgard 
press 
probabilistic approach concurrent mapping localization mobile robots 
machine learning autonomous robots joint issue 
vapnik 

estimations dependences statistical data 
berlin germany springer publisher 
wasserman 

neural computing theory practice 
new york ny von nostrand reinhold 
wei von 

keeping track position orientation moving indoor systems correlation range finder scans 
proceedings international conference intelligent robots systems pp 

fisher walker 

position refinement navigating robot motion information honey bee strategies 
international symposium robotic systems sir 
pisa italy 
yamauchi beer 

spatial learning navigation dynamic environments 
ieee transactions systems man cybernetics part cybernetics special issue learning autonomous robots 
available www aic nrl navy mil yamauchi 
yamauchi langley 

place recognition dynamic environments 
journal robotic systems special issue mobile robots 
