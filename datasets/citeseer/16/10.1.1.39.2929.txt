proceedings hpca february copyright ieee 
impact instruction level parallelism multiprocessor performance simulation methodology vijay pai parthasarathy ranganathan sarita adve department electrical computer engineering rice university houston texas rice edu current microprocessors exploit high levels instruction level parallelism ilp techniques multiple issue dynamic scheduling nonblocking reads 
presents detailed analysis impact processors sharedmemory multiprocessors detailed simulator 
analysis examine validity common direct execution simulation techniques employ previous generation processor models approximate ilp multiprocessors 
find ilp techniques substantially reduce cpu time multiprocessors effective reducing memory stall time 
consequently despite presence inherent latency tolerating techniques ilp processors memory stall time larger component execution time parallel efficiencies generally poorer ilp multiprocessors previous generation multiprocessors 
examining validity direct execution simulators previous generation processor models find appropriate approximations simulators reasonably characterize behavior applications poor overlap read misses 
highly inaccurate applications high overlap read misses 
applications errors execution time simulators range commonly model accurate model 
supported part national science foundation 
ccr ccr cda texas advanced technology program 
vijay pai supported fannie john hertz foundation fellowship 
copyright ieee 
published proceedings third international symposium high performance computer architecture february san antonio texas usa 
personal material permitted 
permission reprint republish material advertising promotional purposes creating new collective works resale redistribution servers lists reuse copyrighted component works obtained ieee 
contact manager copyrights permissions ieee service center lane box piscataway nj usa 
telephone intl 

shared memory multiprocessors built commodity microprocessors expected provide high performance variety scientific commercial applications 
current commodity microprocessors improve performance aggressive techniques exploit high levels instruction level parallelism ilp 
example hp pa intel pentium pro mips processors multiple instruction issue dynamic order scheduling multiple nonblocking reads speculative execution 
architecture studies shared memory systems direct execution simulators typically assume processor model single issue static order scheduling blocking reads 
researchers shown benefits aggressive ilp techniques uniprocessors detailed realistic analysis impact ilp techniques performance shared memory multiprocessors 
analysis required fully exploit advances uniprocessor technology multiprocessors 
analysis required assess validity continued direct execution simulation simple processor models study generation shared memory architectures 
contributions 
detailed study effectiveness state art ilp processors sharedmemory multiprocessor detailed simulator driven real applications 
study validity current direct execution simulation techniques model shared memory multiprocessors built ilp processors 
experiments assessing impact ilp shared memory multiprocessor performance show applications see performance improvements current ilp techniques multiprocessors 
improvements achieved vary proceedings hpca february copyright ieee 
widely 
particular ilp techniques successfully consistently reduce cpu component execution time impact memory read stall component lower application dependent 
deficiency arises primarily insufficient potential applications overlap multiple read misses system contention frequent memory accesses 
discrepancy impact ilp techniques cpu read stall components leads key effects applications 
read stall time larger component execution time previous generation systems 
second parallel efficiencies ilp multiprocessors lower previous generation multiprocessors application 
despite inherent mechanisms ilp processors multiprocessors built ilp processors exhibit greater potential need additional latency reducing hiding techniques previous generation multiprocessors 
results validity current simulation techniques approximate ilp multiprocessors follows 
applications ilp multiprocessor fails significantly overlap read latency direct execution simulation simple previous generation processor model higher clock speed processor cache provides reasonable approximation 
ilp techniques effectively overlap read latency direct execution simulation models show significant errors important metrics 
total execution time commonly technique gave error accurate direct execution technique gave error 
rest organized follows 
section describes experimental methodology 
sections describe analyze results 
section discusses related 
section concludes 
experimental methodology sections describe metrics evaluation architectures simulated simulation environment applications 
measuring impact ilp determine impact ilp techniques multiprocessors compare multiprocessor systems ilp simple equivalent respect processor 
ilp system uses state theart high performance microprocessors multiple issue dynamic scheduling non blocking reads 
refer processors ilp processors 
simple system uses previous generation microprocessors single issue static scheduling blocking reads matching processor model current direct execution simulators 
refer processors simple processors 
compare ilp simple systems determine multiprocessors benefit ilp techniques propose architectural tradeoff ilp simple architectures 
systems clock rate feature identical aggressive memory system interconnect suitable ilp systems 
section provides detail systems 
key metric evaluate impact ilp speedup execution time achieved ilp system simple system call ilp speedup 
study factors affecting ilp speedup study components execution time busy functional unit stall synchronization stall data memory stall 
components difficult distinguish ilp processors instruction potentially overlap execution previous instructions 
adopt convention studies 
cycle processor retires maximum allowable number instructions count cycle part busy time 
charge cycle stall time component corresponding instruction retired 
stall time class instructions represents number cycles instructions class spend head instruction window known reorder buffer active list retiring 
analyze effect component execution time examining ilp speedup component ratio times spent component simple ilp systems 
simulated architectures model processor numa shared memory systems system nodes connected twodimensional mesh 
systems invalidation coherence protocol release consistent 
details processors memory hierarchy modeled 
summarizes system parameters 
extended version includes results processor systems sensitivity analysis parameters 
processor models 
ilp processor resembles mips processor way issue dynamic scheduling non blocking reads register renaming speculative execution 
mips processor implements release proceedings hpca february copyright ieee 
ilp processor processor speed mhz maximum fetch retire rate instructions cycle instruction issue window entries functional units integer arithmetic floating point address generation branch speculation depth memory unit size entries network parameters network speed mhz network width bits flit delay hop network cycles cache parameters cache line size bytes cache chip direct mapped request ports hit time cycle number cache chip way associative request ports hit time cycles pipelined number write buffer entries cache lines memory parameters memory access time cycles ns memory transfer bandwidth bytes cycle memory interleaving way system parameters consistency 
simple processor uses single issue static scheduling blocking reads clock speed ilp processor 
direct execution simulation studies assume single cycle latencies processor functional units 
choose continue approximation simple model represent currently simulation models 
minimize sources difference simple ilp models single cycle functional unit latencies ilp processors 
investigate impact approximation simulated applications processor ilp system functional unit latencies similar ultrasparc processor 
approximation negligible effect applications water water results continue hold 
approximation little impact multiprocessors memory time dominates ilp processors easily overlap functional unit latency 
experiments related validity simulators investigate variants simple model reflect approximations ilp multiprocessors literature 
described section 
memory hierarchy 
ilp simple systems identical memory hierarchy identical parameters 
system node includes processor levels caching merging write buffer caches portion distributed memory directory 
split transaction system bus connects memory network interface rest system node 
cache request ports allowing serve data requests cycle write allocate policy 
cache request port fully pipelined write back cache write allocate policy 
cache additional port incoming coherence messages replies 
caches status holding registers reserve space outstanding cache misses cache allocates read misses write allocate 
support coalescing multiple misses line initiate multiple requests lower levels memory hierarchy 
include coalesced requests calculating counts analysis 
choose cache sizes commensurate input sizes applications methodology woo 
primary working sets applications fit cache secondary working sets applications fit cache 
simulation environment rice simulator ilp multiprocessors simulate ilp simple architectures described section 
models processors memory system network detail including contention resources 
driven application executables traces allowing interactions processors affect course simulation 
code processor cache subsystem performs cycle cycle simulation interfaces event driven simulator network memory system 
derived rice parallel processing testbed rppt 
simulate processor detail simulation times times higher equivalent direct execution simulator 
speed simulation assume instructions hit instruction cache cycle hit time accesses private data hit data cache 
assumptions previous multiprocessor studies direct execution 
model contention processor resources cache ports due private data accesses 
applications compiled version sparc gcc compiler modified eliminate branch delay slots restricted bit code op proceedings hpca february copyright ieee 
application input size cycles lu matrix block theta fft points theta radix radix keys max theta mp particles theta water molecules theta cube block theta application characteristics tions 
applications applications study lu fft radix splash suite mp water splash suite rice parallel compiler group 
modified lu slightly flags barriers better load balance 
gives input sizes applications execution times simple uniprocessor 
study versions lu fft include ilp specific optimizations implemented compiler 
specifically function inlining loop interchange schedule read misses closer overlapped ilp processor 
refer optimized applications lu opt fft opt 
impact ilp multiprocessor section describes impact ilp multiprocessors comparing processor simple ilp systems described section 
results figures illustrate key results 
application shows total ilp speedup ilp speedup different components execution time 
execution time components include cpu time data memory stalls synchronization stalls 
indicates relative importance ilp speedups different components showing time spent component normalized total time simple system 
busy stall times calculated explained section 
applications exhibit speedup ilp processors specific speedup seen varies greatly radix lu opt 
applications achieve similar significant cpu ilp speedup 
contrast data memory ilp speedup lower varies greatly applications slowdown 
radix lu opt 
chose combine busy time functional unit fu stalls cpu time computing ilp speedups simple processor see fu stalls 
key effect high cpu ilp speedups low data memory ilp speedups data memory time dominant ilp multiprocessors simple multiprocessors 
cpu ilp speedups fairly consistent applications data memory time dominant component execution time data memory ilp speedup primarily shapes ilp speedups applications 
analyze factors influence data memory ilp speedup greater detail section 
synchronization ilp speedup low varies widely applications 
synchronization account large portion execution time greatly influence ilp speedup 
section discusses factors affecting synchronization ilp speedup applications 
data memory ilp speedup discuss various factors contribute data memory ilp speedup section show factors interact applications section 
contributing factors shows memory time dominated read time applications 
focus factors influencing read ilp speedup 
factors summarized 
read ilp speedup ratio total stall time due read misses simple ilp systems 
total stall time due read misses system simply product average number misses average exposed cache latency 
equation uses terms express read ilp speedup isolates contributing factors factor factor 
factor 
factor isolated equation 
specifies ratio counts simple ilp systems 
counts differ reordering speculation ilp processor alter cache behavior 
factor greater contributes positively read ilp speedup ilp system sees fewer misses simple system 
factor 
second factor isolated equation 
specifies ratio exposed latency ilp simple systems 
lower factor higher read ilp speedup 
simple system entire latency 
understand factors contributing latency ilp system equa proceedings hpca february copyright ieee 
ilp speedup lu lu opt mp fft fft opt water radix ilp speedup cpu ilp speedup data mem ilp speedup synch ilp speedup ilp speedup components normalized execution time simple ilp lu simple ilp lu opt simple ilp simple ilp mp simple ilp fft simple ilp fft opt simple ilp water simple ilp radix sync write read hit read fu stall busy execution time components normalized latency simple ilp lu simple ilp lu opt simple ilp simple ilp mp simple ilp fft simple ilp fft opt simple ilp water simple ilp radix overlapped factor effect ilp average latency mp lu opt fft opt mshr utilization fft lu water radix mshr utilization mp lu opt fft opt mshr utilization fft lu water radix mshr utilization mshr occupancy mshr occupancy effectiveness ilp multiprocessor system tion expresses ilp latency difference total ilp latency overlapped latency 
total ilp latency expanded equation sum latency incurred simple system extra latency component added ilp system example due increased contention 
equation performs algebraic simplification express factor terms factors overlapped factor extra factor respectively ilp overlapped extra latencies expressed fraction simple latency 
read ilp speedup higher higher overlapped factor lower extra factor 
overlapped factor increases increased overlap misses useful 
number instructions read overlap limited instruction window size 
read misses longer latencies operations occupy instruction window 
read latency normally completely hidden read misses 
high overlapped factor high read ilp speedup applications exhibit read misses appear clustered instruction window 
hand extra factor low high read ilp speedup 
extra latencies arise contention system resources ilp techniques allow ilp processors issue memory frequently simple processors 
extra latency arise change behavior pattern ilp processors proceedings hpca february copyright ieee 
simple latency factor latency ilp overlapped latency ilp ilp latency simple latency ilp overlapped latency ilp extra latency simple latency latency simple extra factor overlapped factor ilp misses latency ilp misses simple factor factor simple latency extra latency read ilp speedup overlapped latency ilp factors affecting read ilp speedup forces misses resolved remote levels memory hierarchy 
summary factor contributes positively read ilp speedup ilp latency simple latency 
factor depends potential read overlap exploited overlap factor lost due contention extra factor 
positive contribution results latency overlapped ilp exceeds extra latency added ilp 
analysis applications read ilp speedup shown separately low applications lu lu opt fft opt radix exhibits slowdown 
show factors discussed section contribute read ilp speedup applications 
factor 
applications factors close implying negligible contribution factor read ilp speedup 
lu lu opt high factors respectively contribute significantly read ilp speedup 
high factors arise ilp system reorders certain accesses induce repeated conflict misses simple system 
ilp system conflicting requests overlap subsequent requests conflicting lines coalesce earlier pending misses reducing number misses seen system 
factor 
graphically represents overlapped extra latencies factors described section 
bars application show average read latency simple ilp systems normalized simple system latency 
light part ilp bar shows average overlapped latency dark part shows latency 
normalization dark light parts ilp bar represent overlapped factors percentages respectively 
difference full ilp simple bars represents extra factor 
ilp bar show factor read ilp speedup factor divided factor 
measure latency read time address generated time value arrives processor extra overlapped factors incorporate time spent read processor memory unit overlap seen time 
figures provide additional data indicate overlapped extra latency read issued memory system 
figures illustrate mshr occupancy distributions caches respectively 
give fraction total time vertical axis occupied misses number horizontal axis 
recall read misses reserve cache write allocate 
mshr occupancy graph indicates read overlap system 
mshr occupancy graph includes read write misses mshr occupancy greater corresponding mshr occupancy indicates resource contention seen reads due interference writes 
data understand reasons factor seen application 
lu opt fft opt mp moderate high overlapped factors due moderate high mshr occupancies 
clustering optimizations lu opt fft opt responsible higher overlap relative lu fft respectively 
increased frequency reads due high read overlap applications leads extra latency due contention effects primarily main memory system 
write traffic additionally increases extra factor significantly 
shown applications positive effects overlapped factor outweigh negative effects extra factor subsequently leading low factor higher read ilp speedups 
radix hand illustrates opposite extreme 
shows radix negative effects extra latency due increased contention significantly outweigh positive effects due overlap leading high factor 
proceedings hpca february copyright ieee 
parallel efficiency simple ilp lu simple ilp lu opt simple ilp simple ilp mp simple ilp fft simple ilp fft opt simple ilp water simple ilp radix simple ilp parallel efficiency simple ilp systems high extra factor primarily due write traffic 
shows radix saturated execution 
misses stall cache preventing accesses issuing cache eventually backup reaches primary cache ports processor memory units causing misses experience high extra latency 
backup causes radix see large read hit component 
low mshr occupancy seen shows radix little potential overlap multiple read misses 
fft application see overlap effects contention effects indicated low mshr occupancies 
leads factor close consequently read ilp speedup close 
discuss applications lu water show relatively high overlapped extra factors despite low mshr occupancies 
lu lu opt lesser extent ilp processor coalesces accesses cause cache misses simple case 
detailed statistics show misses primarily cache hits simple case 
simple latency includes cache hits remote misses ilp latency includes remaining remote misses 
change pattern leads higher average latency ilp system simple system leading high extra factor 
extra factor increases greater frequency memory accesses leads increased network memory contention ilp system 
lu overlap portion extra latency leading factor greater 
lu achieves read ilp speedup factor 
water stands apart applications synchronization characteristics 
extra latency arises reads wait pending acquire operation complete issuing 
latency contribution caused waiting overlapped lock acquire 
result water large apparent overlap 
water poor mshr occupancy prevents getting low factor read ilp speedup close 
summary key reasons low read ilp speedup applications lack opportunity applications overlapping read misses increased contention system 
synchronization ilp speedup general ilp processors affect synchronization time ways 
ilp reduces synchronization waiting times reduced computation time overlapped data read misses 
second acquire latency overlapped previous operations processor allowed release consistency 
third factor negative effect increased contention memory system due higher frequency accesses increase synchronization latency 
factors combine produce variety synchronization speedups applications ranging mp radix 
synchronization accounts small fraction total execution time applications synchronization ilp speedup contribute ilp speedup applications system 
impact ilp parallel efficiency shows parallel efficiency achieved processor ilp simple systems applications expressed percentage 
fft opt parallel efficiency ilp configurations considerably simple configurations 
extended version show trend continues processor systems 
understand reasons difference parallel efficiencies simple ilp multiprocessors presents data illustrate impact ilp uniprocessors analogous data multiprocessors 
multiprocessors uniprocessor cpu ilp speedups high memory ilp parallel efficiency application processor ilp multiprocessor execution time ilp uniprocessor execution time ilp multiprocessor theta parallel efficiency application processor simple multiprocessor defined analogously 
proceedings hpca february copyright ieee 
ilp speedup lu lu opt mp fft fft opt water radix ilp speedup cpu ilp speedup data mem ilp speedup ilp speedup components normalized execution time simple ilp lu simple ilp lu opt simple ilp simple ilp mp simple ilp fft simple ilp fft opt simple ilp water simple ilp radix sync write read hit read fu stall busy execution time components normalized latency simple ilp lu simple ilp lu opt simple ilp simple ilp mp simple ilp fft simple ilp fft opt simple ilp water simple ilp radix overlapped factor effect ilp average latency mp lu opt fft opt mshr utilization fft lu water radix mshr utilization mp lu opt fft opt mshr utilization fft lu water radix mshr utilization mshr occupancy mshr occupancy effectiveness ilp uniprocessor system speedups generally low 
comparing shows applications fft opt ilp speedup multiprocessor uniprocessor 
degradation directly implies lower parallel efficiency ilp multiprocessor simple multiprocessor 
describe reasons lower ilp speedup multiprocessor describe fft opt follow trend 
comparing shows applications read component execution time significant multiprocessor applications see large number remote misses 
consequently read ilp speedup plays larger role determining ilp speedup multiprocessor uniprocessor 
read ilp speedup lower cpu ilp speedup read ilp speedup higher multiprocessor uniprocessor larger role read misses results ilp speedup degradation multiprocessor applications 
second applications ilp multiprocessor may see read overlap dichotomy local remote misses multiprocessor configurations multiprocessors need clustering misses effective overlap require remote misses clustered remote misses order fully hide latencies 
applications fft opt achieve significant overlap uniprocessor see overlap consequently read ilp speedup mul proceedings hpca february copyright ieee 
data layouts provide similar latencies misses overlapped instruction window 
third read ilp speedups applications degrade increased contention multiprocessor 
radix extreme example mshr saturation occurs multiprocessor case uniprocessor 
mshr saturation arises extensive false sharing multiprocessor causes writes take longer complete writes occupy longer increasing mshr contention seen reads 
synchronization presents additional overhead multiprocessor systems cases sees ilp speedup application 
fft opt stands apart applications key reasons 
fft opt avoids reduction read overlap reads cluster instruction window blocked transpose phase algorithm usually block home node sharing pattern 
reads suffer effects dichotomy local remote misses described 
second remote misses causes blocked transpose phase algorithm contribute total execution time section communication 
phase sees significant read ilp speedup total read ilp speedup increases preventing degradation ilp speedup 
impact ilp simulation methodology previous sections detailed cycle cycle simulator understand impact ilp multiprocessor 
explore validity modeling ilp systems direct execution simulators simple processor model variants 
models metrics experiments section study variants simple model approximate ilp model literature 
simple xp simple xp model simple processor sped factors respectively 
simple xp seeks set peak ipc equal ipc achieved target ilp system ilp system generally obtains ipc approximately applications 
simple xp seeks achieve instruction issue rate equal ilp system 
memory hierarchy interconnect models latencies terms absolute time ilp system 
latencies terms processor cycles need appropriately scaled models 
final approximation simple xp cl speeds processor factor recognizes cache hits stall processor 
model additionally speeds cache write buffer access time processor cycle model 
rest memory hierarchy interconnect remain unchanged 
total execution time relative importance various components execution time primary metrics describe effectiveness simulation models 
extended version examines metrics 
due lack space results fft lu 
execution time components shows total execution time components application simulation model normalized execution time ilp model specific application 
section compares simple ilp models 
ilp speedup application indicates factor total execution time simple model deviates actual time ilp 
result error predicted total execution time increases applications better exploiting ilp 
error occurs time spent component execution time proportion ilp speedup component 
errors total execution time simple model range applications 
simple xp simple xp reduce errors total execution time reducing busy time compared simple 
busy time falls factors roughly respectively models resembles ilp busy time case simple xp 
absolute read time stays nearly unchanged compared simple increases cases due added contention 
synchronization time remains unchanged 
models add extraneous read hit stall components cache access takes processor cycle cycles considered busy remaining processor cycles considered stall time blocking reads 
similarly models incurs unwanted write component 
result errors total execution time range simple ilp processor stall completion writes simple processor wait write access write buffer retiring write ilp retire write issued memory system long slot memory unit available 
proceedings hpca february copyright ieee 
normalized execution time ilp simple xp xp xp cl lu opt sync write read hit read fu stall busy normalized execution time ilp simple xp xp xp cl sync write read hit read fu stall busy normalized execution time ilp simple xp xp xp cl mp sync write read hit read fu stall busy normalized execution time ilp simple xp xp xp cl fft opt sync write read hit read fu stall busy normalized execution time ilp simple xp xp xp cl water sync write read hit read fu stall busy normalized execution time ilp simple xp xp xp cl radix sync write read hit read fu stall busy predicting execution time components simple simulation models simple xp simple xp applications 
simple xp cl removes extraneous read hit write components simple xp 
model accurate simple models predicting total execution time giving approximately error applications 
presence high read ilp speedup inaccuracies predicting read time persist giving error predicting execution time lu opt 
simple xp cl significantly overestimates read time fft opt component respectively 
fft opt see corresponding errors total execution time simple xp cl account functional unit stall component ilp 
underestimate cpu time offsets overestimate read time prediction solve simple models fundamental inability account effects high moderate read ilp speedup 
simple models errors seen model highly application dependent ranging depending application exploits ilp 
error component weights certain studies accurate prediction relative weights various components execution may important accurate prediction total execution time 
examine simple models predict relative importance various components execution time 
specifically focus simple lu 
mp fft water radix opt opt ilp simple xp cl relative importance memory component simple xp cl models widely accurate predicting total execution time applications 
focus percentage execution time spent memory component simulation models data 
similar information components models derived graphs 
shown section memory component greater portion execution time ilp systems simple systems 
simple underestimates importance memory time applications 
contrast simple xp cl tends overestimate relative weight memory component model fails account read overlap generally underestimates cpu time 
errors highest applications moderate high memory ilp speedup overestimates lu opt fft opt respectively 
summary alternative models simple simple xp simple xp models see large range errors applications 
contrast simple xp cl model provides reasonable approximation ilp applications 
model predicts behavior busy cache hit compo proceedings hpca february copyright ieee 
nents execution time reasonably model possibility read speedup 
consequently model reasonably approximates ilp behavior applications low read ilp speedup show high inaccuracies predicting performance ilp applications high read ilp speedup 
key insight simple xp cl ilp processors hide nearly cache hit latency 
detailed statistics shown show ilp overlaps cache hit latency 
reasonable extension simple xp cl speed cache hit time single processor cycle 
model remain inadequate predicting performance applications overlap portions local remote memory accesses 
extending model account local remote memory accesses impractical overlap components memory highly application specific hardware dependent known priori 
related multiprocessor studies model effects ilp 
koren provide mean value analysis model bus ilp multiprocessors offers high degree parametric flexibility 
ilp parameters experiments overlapped latency percentage requests coalesced derived specific workload system 
simulation study shows parameters vary significantly application hardware factors provides insight impact behavior parameters 
furthermore model assumes uniform distribution misses properly account read clustering shown key factor providing read overlap 
considered design choices multiprocessor simulation results ilp multiprocessor 
olukotun compared complex ilp uniprocessor chip multiprocessor composed complex ilp processors 
studies consistency models ilp multiprocessors 
details benefits achieved ilp multiprocessor 
variants simple processor model section works heinrich holt 
studies aim model ilp processor behavior faster simple processors validates approximations 
wisconsin wind tunnel ii uses detailed analysis basic block level accounts pipeline latencies functional unit resource constraints model superscalar processor 
model account memory overlap results show important factor determining behavior aggressive ilp processors 
exists large body impact ilp uniprocessor systems 
studies identify investigate factors study determine read ilp speedup presence read clustering coalescing contention 
analyzes impact state ofthe art ilp processors performance sharedmemory multiprocessors 
examines validity evaluating systems commonly employed direct execution simulation techniques previous generation processors 
determine effectiveness ilp techniques compare execution times multiprocessor built state art processors multiprocessor built previous generation processors 
comparison suggest architectural tradeoff understand current multiprocessors succeeded exploiting ilp need improvement 
find applications ilp techniques effectively address cpu component execution time successful improving data read stall component execution time multiprocessors 
primary reasons lower read speedups ilp techniques insufficient potential applications multiple read misses outstanding simultaneously system contention frequent memory accesses 
disparity impact ilp cpu time read time implications 
read stall time larger component execution time ilp multiprocessors multiprocessors 
second applications show lower parallel efficiency ilp multiprocessor previous generation multiprocessor 
key reasons reduced parallel efficiency applications greater impact read stall time multiprocessor uniprocessor increased contention multiprocessor reduced overlap due dichotomy local remote memory accesses 
appear fundamental problems applications exploits overlap ilp proceedings hpca february copyright ieee 
multiprocessor see increase parallel efficiency 
results indicate despite latency tolerating techniques integrated ilp processors multiprocessors built ilp processors greater need additional memory latency reducing hiding techniques multiprocessors 
techniques include conventional hardware software techniques aggressive compiler techniques enhance read overlap applications accounting dichotomy local remote memory accesses 
addressing validity current direct execution simulation models approximate ilp multiprocessor find model increases speed cpu cache reasonable approximation applications low read ilp speedup 
model show significant inaccuracy cases high moderate read ilp speedup properly account effects overlapping read misses 
unfortunately full ilp simulation invariably take simulation time direct execution simulation models 
absence alternative expect direct execution simulators continue particularly large applications large data sets 
study provides insights inaccuracies generated suggests results simulations interpreted care 
accurate analysis large applications parallelization may serve enabling technology high performance ilp simulations 
acknowledgments vikram adve mukherjee vivek pai ram valuable feedback earlier drafts 
adve integrated compilation performance analysis environment data parallel programs 
supercomputing 
koren 
analytical model high performance superscalar multiprocessors 
pact 
burger quantifying memory bandwidth limitations current microprocessors 
isca 
butler patt 
effect real data cache behavior performance microarchitecture supports dynamic scheduling 
micro 
covington efficient simulation parallel computer systems 
intl 
journal computer simulation 
internal organization alpha mhz bit quad issue cmos risc microprocessor 
digital technical journal 
gharachorloo memory consistency event ordering scalable shared memory multiprocessors 
isca 
gharachorloo hiding memory latency dynamic scheduling shared memory multiprocessors 
isca 
heinrich performance impact flexibility stanford flash multiprocessor 
asplos vi 
holt application architectural bottlenecks large scale distributed shared memory machines 
isca 

lockup free instruction fetch prefetch cache organization 
isca 
mips technologies microprocessor user manual version january 
evaluation design alternatives multiprocessor microprocessor 
isca may 
olukotun case single chip multiprocessor 
asplos vii october 
oner dubois 
effects memory latencies non blocking processor cache architectures 
supercomputing 
pai impact instruction level parallelism multiprocessor performance simulation 
ece tr july revised december 
pai evaluation memory consistency models shared memory systems ilp processors 
asplos vii october 
rajagopalan 
effects interconnection networks performance shared memory multiprocessors 
master thesis rice university 
reinhardt decoupled hardware support distributed shared memory 
isca 
rosenblum impact architectural trends operating system performance 
sosp 
singh splash stanford parallel applications shared memory 
computer architecture news 
woo splash programs characterization methodological considerations 
isca 
zucker 
baer 
performance study memory consistency models 
isca 

