complexity issues markov decision processes judy goldsmith martin mundhenk dept computer science fachbereich iv theoretische informatik university kentucky universitat trier lexington ky trier germany survey complexity computational problems markov decision processes evaluating policies finding best policies approximating best policies related decision problems 
partially observable markov decision processes pomdps model sequential decision making outcomes uncertain state system completely observed 
consist decision epochs states observations actions transition probabilities rewards 
decision epoch process state signal sent observed outside 
note different states may send equal signals 
choosing action state generates reward possibly cost negative reward determines state decision epoch transition probability function 
policies prescriptions action take eventuality sequence observations previous decision epochs 
decision makers seek policies maximize expected sum rewards certain number decision epochs 
control theory mathematics operations research literature mdps concentrates primarily algorithms building models finding policies 
partially observable mdps developed drake dra 
sondik thesis son subsequent papers ss address resolve computational difficulties associated pomdps 
subsequent years different algorithmic methods designed see lov overview 
lower bounds complexity partially observable mdps obtained papadimitriou tsitsiklis pt pt 
results apply pomdps nonpositive rewards costs concern existence zero cost policies 
maximization problem investigated papadimitriou tsitsiklis binary search determine maximal performance pomdp general 
search depends positive negative rewards 
fact case existence problem policies exact value harder problem exists policy value greater equal example question exists history dependent policy pomdp nonnegative rewards nl complete compared pspace completeness pt theorem 
bbs considered various optimality criteria mdps 
mga investigation complexity markov decision process decision problems various parameters continued initiated pt 
methods mga applied lgm show complexity results planners described ai literature 
ai planning community analyzed complexity variations mdps wide variety related models 
ai papers especially bac cha ens consider complexity deterministic control processes complexity evaluation existence problems systems generally lower stochastic processes glm lit theorem 
complexity results coming ai community succinctly represented systems described phase temporal bayes nets bdh bdg sequential effects tree representation st lit probabilistic state space operators psos related representations 
problems consider complexity finding policies size polynomial size representation note stochastic systems hot jacm added editor decisions uncertainty computation number states leading different complexities cases 
instance littman showed general plan existence problem plans limited size polynomial size planning domain problem pspace complete lit proved independently glm 
stochasticity system add computational complexity 
instance bylander showed time dependent policy existence problem unobservable succinct mdps nonnegative rewards pspace complete transitions restricted deterministic 
pspace completeness stochastic version problem shown 
algorithms known yield approximations optimal policies pomdps see lov survey question approximable problems general begun receive attention deserves 
particular cases unobservable mdp shown non approximable pomdps bound number different observations shown approximable 
lgm showed problems approximable major collapse occurs 
markov decision processes markov decision processes roots operations research 
excellent put 
partially observable markov decision process pomdp describes controlled stochastic system states consequences actions system 
pomdp example ss 
consider hypothetical manufacturing operation produces finished product hour hour 
machine consists identical internal components operate product finished 
unfortunately component fail spontaneously 
component failed probability cause product defective 
machine order examine status internal components internal state directly observable 
control options hour production interval 
simplest manufacturing process continued examination finished product 
second alternative observe quality product defective machine stopped failed components replaced 
alternative replaces components hours 
alternatives incurs gains losses wish know optimal 
formally pomdp denoted tuple ffl finite sets states actions observations ffl initial state ffl theta theta state transition function probability reach state state action sigma ffl observation function observation state ffl theta reward function reward gained action state distinguish special types observability pomdp 
states observations identical identity function bijection pomdp called fully observable mdp 
called unobservable set observations contains element observation function constant state observation 
reward function maps nonnegative integers say nonnegative rewards 
policies performances policies describe act depending observations 
history dependent policy selects action dependent full history observations process started initial state 
seen function mapping sequences observations actions 
pomdp 
trajectory finite sequence states oe oe oe oe 
probability prob trajectory oe oe oe policy product probabilities state oei reached state oe policy appropriate observations 
reward rew trajectory sum rewards rew sigma gamma oe delta 
performance policy finite horizon initial state oe expected sum rewards received steps policy theta oe prob delta rew theta oe set length trajectories making observations probabilistically add power pomdps 
probabilistically observable mdp turned deterministic observations polynomial increase size 
state oe 
value val horizon maximal performance policy horizon note value may different different types policies 
functional problems calculating performance set problems considers complexity calculating performance pomdp policy jm steps 
performances rational numbers extend notion resp 
deal rational numbers 
say function gamman functions fl function intuitively complexity properties coincide originally defined ffk 
calculating performance policy reduces matrix powering 
theorem follows mga evaluating pomdp stationary time dependent history dependent policy complete 
include proof gives insight class 
technical lemma matrix powering show entry mth power nonnegative integer square matrix computed power polynomial dimension matrix 
lemma cf 
vin theta matrix nonnegative binary integers length function mapping lemma evaluating stationary policy pomdp 
proof pomdp stationary policy mapping show perf jsj performance run initial state jsj steps stationary policy computed 
transform unobservable mdp states rewards having performance policy stationary policy achieved hard wiring actions chosen mdp 
fag 
action policy consider constant function mapping observation action clear policy performance constant policy perf jsj perf jsj 
performance calculated recursive definition perf perf delta perf gamma perf 
state transition probabilities binary fractions length jsj 
order get integer function performance define function hm gamma delta 
show perf delta show function 
define weighted sum positive rewards hm gamma delta maxf gamma defined analogously weighted sum negative rewards 
gamma 
show gammap gamma functions 
matrix obtained transition matrix multiplying entries delta recursion definition resolved get gamma deltar delta gammak deltah logspace computable input lemma get gamma reward function part input 
closed multiplication polynomial summation follows analogous arguments show gamma functions differences functions ao follows 
unobservable fully observable mdps special case pomdps get corollary 
corollary stationary policies unobservable fully observable mdps evaluated 
lemma evaluating stationary policies unobservable mdps hard 
proof consider 
exists probabilistic logspace turing machine corresponding polynomial computation uses jxj random decisions jun 
fix input construct unobservable mdp action models behavior state pair consisting configuration polynomially integer counter number random moves reach configuration jxj 
add final trap state reached states containing halting configuration 
state transition function defined state transition function halting computation corresponds length jxj trajectory vice versa 
state transition function defined halting computation corresponds length jxj trajectory vice versa 
reward function chosen rew delta prob equals trajectories corresponding accepting computations independent length gamma rejecting computations 
number accepting computations minus number rejecting computations calculate calculating number trajectories length jsj rew delta prob minus number trajectories rew delta prob gamma 
exactly perf jsj 
reduction function constructs mdp having action hardness proof applies time dependent policies partially fully observable mdps 
history dependent policies issues 
logarithmic space bound depends essential ways fact input history dependent policy store respective sequence observations 
policy explicitly possible sequence observations listed 
consider encodings policies instance circuits size bounded size input pomdp called tractable policies complexity evaluation jumps break limits logarithmic space polynomial time 
theorem mun evaluating pomdp tractable history dependent policy complete 
pomdps nonnegative rewards obtain respective completeness results calculating value value pomdp maximum performance policy 
important results fully observable mdps value history dependent policy calculated efficiently dynamic programming bel der 
values time dependent policies equal 
turns calculation value easiest fully observable mdps hardest partially observable mdps 
results follow pt 
theorem calculating time dependent history dependent value fully observable mdp fp complete 
stationary policies respective problem fp hard fp np completeness result known 
unobservable mdps stationary value easier calculate case policy consists exactly action 
reflect pattern types policies value unobservable mdps harder calculate fully observable mdps 
theorem calculating history dependent value unobservable mdp fp np complete 
immediately implies lower bound partial observability 
theorem calculating stationary timedependent value partially observable mdp fp np complete 
history dependent policies harder 
theorem calculating history dependent value partially observable mdp complete 
fully observable mdps time dependent history dependent values equal equality holds pomdps fp np 
approximating value seen cases computing optimal policy mdp feasible 
obvious question optimal policies approximated polynomial time 
cases shown approximation optimal policy surprising collapse holds 
distinguish kinds approximability 
algorithm said approximate value pomdp factor polynomial time bounded input outputs gamma delta val val val value horizon jsj 
theorem lgm 
stationary value partially observable mdp approximable factor iff np 

time dependent value unobservable partially observable mdp approximable factor iff np 
considered class pomdps bound number states corresponding observation rewards corresponded probability reaching fixed set goal states bounded 
showed fixed optimal policies pomdps class approximated additive constant means exists polynomial time algorithm input pomdp value outputs number gamma call additive approximability shown class pomdps optimal history dependent policies approximated additive constant polynomial time approximation schemes lgm long priori bounds rewards 
notice theorem give information classes pomdps considered restrictions associated parameter hardness results contradict result 
theorem lgm history dependent value pomdps additively approximable pspace 
multiply expected reward mdp constant leverage additive approximation multiplicative approximation vice versa applying approximation mdp suitably modified reward 
theorem corollary theorem 
theorem lgm optimal value pomdps approximable pspace 
decision problems decision problems appear interested bit function values highest bit 
policy evaluation problem asks pomdp positive performance policy 
policy existence problem asks pomdp positive performance policy specified type 
complexity problems depends type observability type policy type encoding 
policy evaluation policy evaluation problem strongly related performance calculation 
essentially complexities equal compare section 
case mdps nonnegative rewards problem degenerates finding trajectory probability greater reward greater obtained 
theorem 
mga stationary time dependent policy evaluation problems partially observable mdps nonnegative rewards nl complete 

mga stationary time dependent policy evaluation problems partially observable mdps pl complete 

mun tractable history dependent policy evaluation problems fully observable partially observable mdps pp complete 
policy existence policy existence problem strongly related value performance calculation 
general guessing policy checking value gives upper bound complexity policy existence terms nondeterministic machine oracle policy evaluation 
case history dependent policies straightforward approach yields nondeterministic machine description length history dependent policy exponential description length respective pomdp 
turns history dependent policies useful structures yield better upper bound straightforward 
theorem history dependent policy existence problem 
pt complete fully observable mdps 
mga np complete unobservable mdps 
mun np pp complete tractable policies partially observable mdps 
pt pspace complete mdps 
guess check approach give tight bounds history dependent policies fully observable partially observable mdps 
stationary policies guess check optimal partially observable unobservable mdps fully observable mdps know exact complexity policy existence problem 
time dependent policies case check optimal fully observable mdps 
theorem stationary policy existence problem 
mga pl complete unobservable mdps 
pt hard fully observable mdps 
mga np complete partially observable mdps 
time dependent policies fit best guess check approach 
theorem time dependent policy existence problem 
pt complete fully observable mdps 
pt np complete unobservable partially observable mdps 
note results reflect type policy type observability influence complexity different ways 
case fully observable mdps policy existence problem easier history dependent policies stationary policies reversed unobservable mdps 
mdps non negative rewards problem reduces simple reachability problem 
theorem mga policy existence problem partially observable mdps nonnegative rewards nl complete 
succinctness mdps arise practice huge highly structured 
instance state may represented sequence values set parameters transitions may local parameter dependent small number possible values parameters 
ai practitioners call representation structured propositional 
consider variety representations systems equivalent 
see bdg lit glm lit lgm discussion 
consider mdps transition reward functions represented circuits describe succinct mga keeping complexity theory literature gw wag 
particular circuit transition function takes input outputs th bit probability 
similarly reward function 
horizon bounded number states exponential size description mdp 
unsurprisingly complexities succinctly represented problems exponentially higher called flat representations 
see blt conditions increases necessary 
example policy existence problems pomdps nonnegative rewards nl complete straightforward descriptions 
succinct descriptions completeness increases pspace 
techniques proofs familiar complexity theorists won go details 
influence parameters succinct mdps considered practice frequently arise context planning interested reward number steps polynomial size mdp description time run policy exponential number steps 
motivates restricting horizon logarithmic size state space 
circuits represent functions straightforward manner 
consider succinct representations circuit transition function takes input outputs entire probability 
necessarily number bits representation probability polynomial linear sublinear size input logarithmic number states jsj 
call representations compressed 
issue bit counts transition probabilities arises instance bbs tse 
note probabilities specified single bit strings rationals specified bit strings 
similarly reward function represented compact circuits 
turns number bits needed represent actions rewards contributes complexity 
factors problem descriptions involved flat succinct cases 
focus rest section decision problems policy evaluation policy existence problems 
allow focus classes problems interest complexity theoretic standpoint 
notation 
ff pol icy evaluation problem partially observable compressed mdp horizon set pairs consisting partially observable succinctly encoded mdp transition probability reward functions compressed represented jaj jsj ff policy perf jsj 
policy existence problems expressed similarly 
interesting results class np pp class sits polynomial hierarchy pspace 
surprising number problems arise study succinctly represented planning problems uncertainty turn np pp complete 
include theorems proofs related class np pp particular succinct unobservable mdps 
complexity policy existence problems unobservable mdps number actions mdp important 
proved stationary policy existence problem fact policy existence problem disjunctively reduces policy evaluation problem mdp computed set pairs action set actions large set states reduction longer performed time polynomial log theorem stationary policy existence problem unobservable compressed mdp log log horizon pp complete 
proof hardness pp straightforward turing machine simulation proof theorem 
show containment pp arguments similar proof lemma 
remember pp exists function see ffk 
show function constructed proof lemma circumstances respective matrix powering closed multiplication summation 
pp closed polynomial time disjunctive reducibility brs completes proof 
omitting restriction number actions complexity problem rises np pp theorem stationary policy existence problem unobservable compressed mdps log horizon np pp complete 
proof see problem np pp note corresponding policy evaluation problem 
existence question guess polynomial sized policy verify expected reward consulting pp oracle 
show np pp hardness needs np pp equals np closure pp tor seen closure pp polynomial time disjunctive reducibility exponential number queries queries computable polynomial time index list queries 
np pp pp relation iff means polynomial jyj jxj 
pp machine 
model computation log horizon policy evaluation problem unobservable compressed mdp 
compressed mdp depends input supplies policy 
fixed specifies policy 
policy specified expected value 
policy existence problem np pp complete 
horizon increases size state space may exponential size mdp representation get completeness pspace 
note pspace hardness depend existence negative positive rewards probabilistic transitions deterministic case nonnegative rewards pspace hard 
theorem stationary policy existence problem unobservable compressed mdps pspace complete 
proof hardness pspace follows fact set configurations polynomially space bounded turing machine chosen mdp state space local deterministic transition function easily defined 
mdp action configuration transition positive reward obtained accepting configuration reached 
containment pspace follows simulation similar proof theorem fact pspace lad 
consequence obtain policy existence problem flat mdps exponential horizon pspace 
originally proved lit 
complexity gap stationary time dependent policy existence problems compressed mdp log big flat mdps difference longer depends number actions 
theorem time dependent policy existence problems unobservable compressed mdps log horizon unobservable compressed mdp log log horizon np pp complete 
discussion open questions general lesson drawn results simple relationship policy existence problems stationary timedependent history dependent policies 
trivially true stationary policy exists time history dependent policies exist case problems easier 
problems categorized completeness 
stationary policy existence problem fully observable mdps candidate low set np 
major open questions form 
proved problems difficult compute heuristics needed order manage 
particular np pp complete problems attempt done techniques boolean satisfiability dynamic programming ml 
ao allender ogihara 
relationships pl determinant 
rairo theoretical informatics applications 
bac backstrom 
expressive equivalence planning formalisms 
artificial intelligence 
bbs 
complexity finite memory policies markov decision processes 
math 
foundations computer science pages 
lecture notes computer science springer verlag 
bdg boutilier dearden goldszmidt 
exploiting structure policy construction 
th international conference ai 
bdh boutilier dean hanks 
planning uncertainty structural assumptions computational leverage 
proceedings second european workshop planning 
de 
complexity partially observed markov decision processes 
theoretical computer science 
bel bellman 
dynamic programming 
princeton university press 
blt azar lozano tor 
complexity algorithmic problems succinct instances 
baeza yates manber editors computer science pages 
plenum press 
brs beigel reingold spielman 
pp closed intersection 
journal computer system sciences 
bylander 
computational complexity propositional strips planning 
artificial intelligence 
cha chapman 
planning conjunctive goals 
artificial intelligence 
der 
finite state markovian decision processes 
academic press 
dra drake 
observation markov process noisy channel 
phd thesis dept electrical engineering massachussets institute technology 
erol hendler nau 
complexity results hierarchical planning 
annals mathematics artificial intelligence 
ens erol nau subrahmanian 
complexity decidability undecidability results domain independent planning 
artificial intelligence 
ffk fenner fortnow kurtz 
gap definable counting classes 
journal computer system sciences 
glm goldsmith littman mundhenk 
complexity plan existence evaluation probabilistic domains 
proc 
th conf 
uncertainty ai 
morgan kaufmann publishers 
gw wigderson 
succinct representation graphs 
information control 
jun jung 
probabilistic time space 
proceedings th icalp pages 
lecture notes computer science springer verlag 
kushmerick hanks weld 
algorithm probabilistic planning 
artificial intelligence 
lad ladner 
polynomial space counting problems 
siam journal computing 
lgm littman goldsmith mundhenk 
computational complexity probabilistic planning 
lgm goldsmith mundhenk 
results markov decision processes 
technical report university kentucky department computer science 
lit littman 
probabilistic strips planning exptime complete 
technical report cs duke university department computer science november 
lit littman 
probabilistic propositional planning representations complexity 
proc 
th national conference artificial intelligence 
aaai press mit press 
lit littman 
probabilistic propositional planning representations complexity 
proc 
th national conference ai 
aaai press mit press 
lov lovejoy 
survey algorithmic methods partially observed markov decision processes 
annals operations research 
mga mundhenk goldsmith allender 
complexity policy existence problem partially observable finite horizon markov decision processes 
proc 
th mathematical foundations computer sciences pages 
lecture notes computer science springer verlag 
mundhenk goldsmith allender 
encyclopaedia complexity results finite horizon markov decision process 
ml littman 
new approach probabilistic planning 
technical report duke university department computer science 
mun mundhenk 
complexity pomdps tractable history dependent policies 
pt papadimitriou tsitsiklis 
intractable problems control theory 
siam journal control optimization pages 
pt papadimitriou tsitsiklis 
complexity markov decision processes 
mathematics operations research 
put puterman 
markov decision processes 
john wiley sons new york 
son sondik 
optimal control partially observable markov processes 
phd thesis stanford university 
ss smallwood sondik 
optimal control partially observed markov processes finite horizon 
operations research 
tor tor 
complexity classes defined counting quantifiers 
journal acm 
tse tseng 
solving horizon stationary markov decision problems time proportional log operations research letters pages september 
vin vinay 
counting auxiliary pushdown automata semi unbounded arithmetic circuits 
proc 
th structure complexity theory conference pages 
ieee 
wag wagner 
complexity combinatorial problems succinct input representation 
acta informatica 
