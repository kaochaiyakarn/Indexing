solving pomdps searching policy space eric hansen computer science department university massachusetts amherst ma hansen cs umass edu algorithms solving pomdps iteratively improve value function implicitly represents policy said search value function space 
presents approach solving pomdps represents policy explicitly finite state controller iteratively improves controller search policy space 
related algorithms illustrate approach 
policy iteration algorithm outperform value iteration solving pomdps 
provides foundation new heuristic search algorithm promises speedup focusing computational effort regions problem space reachable reached start state 
partially observable markov decision process pomdp provides elegant mathematical model planning control problems uncertainty effects actions current state 
known state probability distribution updated bayesian reasoning sufficient statistic summarizes information history process necessary optimal action selection 
standard approach solving pomdp recast completely observable mdp state space consists possible state probability distributions 
form solved dynamic programming related techniques rely markov assumption 
algorithms solving pomdps way rely value function maps state probability distributions expected values 
value function defined possible state probability distributions represented different ways example set vectors max operator smallwood sondik grid point values interpolation rule hauskrecht 
explicit representation value function policy represented implicitly value function step lookahead 
algorithms solving pomdps represent policy implicitly way improve policy gradually improving value function typically repeated backups value iteration reinforcement learning 
policy represented implicitly value function algorithms said search value function space 
presents approach solving pomdps represents policy explicitly relies search policy space 
approach choice represent policy critical way algorithms search value function space 
possible represent policy explicitly mapping state probability distributions actions partitioning probability space finite set regions mapping region action 
sondik describes policy iteration algorithm represents policy way 
algorithm complex difficult implement result practice 
consider alternative representation policy finite state controller related algorithms solving pomdps searching policy space finite state controllers 
policy iteration algorithm described hansen simplifies policy iteration pomdps representing policy finite state controller 
provides foundation related heuristic search algorithm time focus computational effort regions search space reachable reached start state 
background consider discrete time pomdp finite set states finite set actions finite set observations time period system state agent chooses action receives immediate reward expected value system transition state probability js agent observes probability zjs 
state system directly observed probability state calculated 
denote vector state probabilities called belief state denotes probability system state action taken observation follows successor belief state denoted determined revising state probability follows zjs js denominator normalizing factor zjs js 
pomdp solved finding rule selecting actions called policy optimizes performance objective comes acceptably close doing 
assume objective maximize expected total discounted reward infinite horizon fi discount factor 
recasting pomdp completely observable mdp continuous jsj dimensional state space consists possible belief states problem solved iteration dynamic programming update performs step backup belief state max fi words says value belief state set equal immediate reward best action plus discounted expected value resulting belief state iteration update called value iteration converges optimal value function limit 
number belief states backed iteration uncountably infinite obvious 
key computing dynamic programming update smallwood sondik proof preserves piecewise linearity convexity value function 
piecewise linear convex value function represented finite set jsj dimensional vectors real numbers fv value belief state defined follows max ik dynamic programming update transforms value function represented way improved value function represented finite set vectors algorithms performing dynamic programming update developed 
rely heavily linear programming computationally intensive algorithm presently fastest described cassandra littman zhang 
describe compute dynamic programming update refer kaelbling 
cassandra 

algorithms search value function space value iteration able extract policy value function iteratively improve 
possible ways correspond possible representations policy 
possibility view policy mapping belief states actions 
representation value function mapping belief states values policy ffi extracted step lookahead ffi arg max ae fi ae expected immediate reward action belief state second possibility represent policy finitestate controller 
correspondence vectors step policy choices plays important role interpretation policy 
vector corresponds choice action possible observation choice vector possible step policy choices vectors correspond optimize value belief state 
describe correspondence vectors step policy choices introduce notation 
vector denote choice action possible observation denote index successor vector correspondence vectors step policy choices kaelbling 
point optimal policy finite horizon pomdp represented acyclic finite state controller machine state corresponds vector nonstationary value function 
value iteration solve pomdps 
optimal value function infinite horizon pomdp necessarily piecewise linear convex 
example simple finite state controller corresponding value function pomdp states actions observations 
machine state labeled unique number action take state state transitions labeled observations 
vector value function labeled number machine state corresponds 
state pomdp belief state represented single real number represents probability states horizontal axis right represents belief state way 
vertical axes represent expected value belief state 
value function upper surface vectors reflects rule controller started machine state optimizes value starting belief state 
approximated arbitrarily closely piecewise linear convex function 
sondik cassandra 
point reliably value iteration converges optimal piecewise linear convex value function equivalent cyclic finite state controller 
finite state controller extracted value function correspondence vectors step policy choices noted earlier 
finite state controller reliably extracted suboptimal value function policy infinite horizon pomdp generally viewed mapping belief states actions represented implicitly value function extracted equation 
algorithms search value function space important able extract policy value function 
algorithms search policy space equally important able compute value function policy called policy evaluation 
conclude review pointing policy represented finite state controller policy evaluation straightforward 
piecewise linear convex value function computed solving system linear equations equation pair machine state system state fi js zjs value function linear facet jsj vector machine state finite state controller 
policy finite state controller value function defined belief space controller started machine state corresponds vector optimizes value starting belief state 
see 
policy iteration algorithm consider solves pomdp searching policy space policy iteration 
includes policy evaluation step computes value function policy represent policy explicitly independently value function 
sondik describes policy iteration algorithm pomdps represents policy mapping finite number polyhedral regions belief space actions 
region belief space represented set linear inequalities define boundaries 
known method computing value function policy represented way policy evaluation step sondik algorithm converts policy representation equivalent approximately equivalent finite state controller seen value function finite state controller computed straightforward way 
conversion representations extremely complicated difficult implement 
result sondik algorithm practice 
show policy iteration pomdps simplified representing policy finite state controller 
obvious simplification policy evaluation difficult step sondik algorithm straightforward 
approach show dynamic programming update interpreted transformation finite state controller ffi improved finite state controller ffi show perform policy improvement finite state controllers 
showing simple comparison vectors ffi provides basis transformation ffi set vectors represents value function current finite state controller ffi output dynamic programming update ffi input 
recall vector ffi associated action denoted possible observation transition vector ffi index 
follows fact ffi computed evaluating finite state controller 
similarly vector dynamic programming update associated action possible observation transition vector ffi denotes index vector 

specify initial finite state controller ffi select ffl detecting convergence ffl optimal policy 

policy evaluation compute value function ffi solving system equations equation 

policy improvement perform dynamic programming update transforms set vectors ffi set vectors 
vector action successor links associated machine state ffi keep machine state unchanged ffi ii 
vector pointwise dominates vector associated machine state ffi change action successor links machine state correspond 
pointwise dominates vectors machine state combined single machine state 
iii 
add machine state ffi action successor links associated 
prune machine state ffi corresponding vector long reachable machine state vector correspond 

termination test 
calculate bellman residual equal ffl gamma fi fi exit ffl optimal policy 
set ffi ffi node changed step goto step goto step 
policy iteration algorithm pomdps 
vectors duplicates vectors ffi action successor links case vector values pointwise equal 
duplicates indicate finitestate controller changed improve value function changing machine state changing corresponding action successor links adding machine state 
may machine states corresponding vector pruned reachable machine state corresponds vector 
point important example finite state controller improved step policy iteration 
dynamic programming update returns vectors corresponding potential machine states shown left panel dashed circles 
node duplicate machine state causes change 
node new machine state 
vector node pointwise dominates vector machine state action observation links machine state changed accordingly 
improved finite state controller value function shown right panel 
preserves integrity finite state controller 
finite state controller iteratively improved combination transformations changing machine states adding machine states pruning machine states 
outlines policy iteration algorithm policy improvement step uses simple transformations improve finite state controller 
illustrates policy improvement step simple example 
transformation finite state controller performing dynamic programming update adds little overhead policy improvement step simply compares vectors vectors ffi modifies finite state controller accordingly 
machine state changed policy evaluation step invoked compute value function transformed finite state controller 
prove generalization policy improvement theorem hansen 
theorem finite state controller optimal policy improvement transforms finite state controller value function better belief state better belief state 
policy improvement step change finite state controller vectors duplicates vectors ffi bellman optimality equation satisfied finite state controller optimal 
policy iteration detect convergence optimal finite state controller 
pomdp optimal finite state controller policy iteration may simply table comparison value iteration policy iteration test problems cassandra 

columns show number cpu seconds convergence ffl optimality values ffl 
test problem timing results value iteration shown timing results policy iteration 
algorithm terminated reaching ffl 
test problem maze cheese part painting network shuttle aircraft id find succession finite state controllers increasingly close approximations optimal policy 
stopping condition sondik uses detect ffl optimality finite state controller ffl optimal bellman residual equal ffl gamma fi fi fi discount factor prove convergence result hansen 
theorem policy iteration converges ffl optimal finite state controller finite number iterations 
completely observable mdps policy iteration converge ffl optimality optimality fewer iterations value iteration interleaving policy evaluation step update accelerates improvement value function 
completely observable mdps clear advantage policy evaluation step computationally expensive dynamic programming update 
pomdps policy evaluation low order polynomial complexity compared worst case exponential complexity dynamic programming update littman 
policy iteration appears clearer advantage value iteration pomdps 
table compares performance value iteration policy iteration test problems cassandra 

problems average number states average number actions average number observations incremental pruning algorithm perform dynamic programming updates value iteration policy iteration experiments performed alphastation mhz processor ram 
results show policy iteration consistently outperforms value iteration increased rate convergence dramatic 
test problems small optimal finitestate controllers maze cheese part painting 
policy iteration converges quickly reduces error bound zero single iteration 
problems finite state controllers couple machine states generated converging optimality 
illustrates difficulty solving pomdp primarily function size controller needed achieve performance simply function number states actions observations 
heuristic search policy iteration converges quickly value iteration limited solving small pomdps 
shared bottleneck update 
fastest algorithm performing prohibitively slow problems fifteen states actions observations 
policy iteration faster value iteration takes fewer iterations update converge 
single iteration computationally prohibitive policy iteration impractical value iteration 
section introduce new approach solving pomdps closely related policy iteration algorithm described previous section differs important respect dynamic programming update improve policy 
uses heuristic search 
heuristic search solve pomdps approximately 
lave describe branch bound algorithm solving infinite horizon pomdps initial belief state larsen washington best heuristic search algorithm ao similar way 
infinite horizon problems possible search finite depth algorithms find solution takes form tree grows depth search 
search tree repre sented tree nodes tree correspond belief states root tree initial belief state 
node represents choice action node represent set possible observations 
value node value best action belief state corresponds 
value node sum values belief states follow observation multiplied probability observation 
upper lower bounds computed belief states fringe search tree backed tree starting belief state root 
expanding search tree improves bounds interior nodes tree 
error bound difference upper lower bounds value starting belief state arbitrarily small expanding search tree far discounted pomdps ffl optimal policy belief state root tree finite search lave 
possible upper bound functions evaluating fringe nodes search tree discussed hauskrecht brafman add discussion 
lower bound function piecewise linear convex value function finite state controller improve lower bound search iteratively improving finite state controller policy iteration 
principal innovation heuristic search algorithm 
recall node search tree corresponds belief state 
expanding node child nodes backing lower bound equivalent performing step backup corresponding belief state equation 
backup may improve lower bound belief state know machine state added finite state controller improves value belief state 
expanding search node performs similar function dynamic programming update interpreted similar way potential modification finite state controller difference node expansion corresponds step backup single belief state dynamic programming update performs step backup possible belief states 
lower bound belief state search tree improved backed search tree possibly improves lower bound starting belief state root 
search algorithm way improve value starting belief state modifying finite state controller 
finite state controller modified 
specify initial finite state controller ffi select ffl detecting convergence ffl optimal policy 

policy evaluation compute value ffi solving system equations equation 

policy improvement perform forward search starting belief state back lower upper bounds leaves search tree 
continue lower bound starting belief state improved error bound value starting belief state equal ffl 
error bound equal ffl exit ffl optimal policy 
continue 
forward search change policy improves lower bound starting belief state consider reachable node search tree lower bound improved 
node said reachable reached starting root node selecting actions optimize lower bound 
nodes order leaves root action successor links machine state ffi keep machine state unchanged ffi ii 
compute vector node pointwise dominates vector machine state ffi change action successor links machine state node 
pointwise dominates vectors machine state combined single machine state 
iii 
add machine state ffi action successor links node 
prune machine state ffi reachable machine state optimizes value starting belief state 

set ffi ffi machine state controller changed goto step goto step 
heuristic search algorithm pomdps 
start example finite state controller improved heuristic search 
nodes left panel potential new machine states correspond path search tree starting belief state lower bound value belief state improved 
node new machine state 
vector corresponding node pointwise dominates vector machine state action observation links machine state changed accordingly 
node duplicate machine state causes change 
improved finite state controller value function shown right panel 
follows 
root search tree selecting action optimizes lower bound node reachable node search tree lower bound improved identified 
nodes backwards order fringe search tree corresponding vector computed current value function finite state controller modified transformations policy iteration algorithm 
algorithm summarized illustrated simple example 
heuristic search recognize change machine states detecting pointwise dominance add 
machine state changed policy evaluation invoked recompute value function policy iteration algorithm interleaves policy improvement step policy evaluation step 
machine state reachable machine state optimizes value starting belief state pruned affecting value starting belief state error bound simply difference upper lower bounds starting belief state 
implementation ao perform heuristic search 
expands search tree best order upper bound function identify promising solution tree 
select fringe nodes expansion heuristic select expansion node corresponding belief state value ub gamma fi depth greatest ub denotes upper bound function lower bound function value function current finite state controller denotes probability reaching belief state starting belief state root tree depth denotes depth belief state search tree measured number actions taken path root 
selection heuristic focuses computational effort improve bounds starting belief state 
advantages heuristic search dynamic programming update improve finite state controller 
importantly adds machine states finite state controller improve value starting belief state 
contrast policy iteration finds finite state controller optimizes value possible belief state usually larger controller 
test problems listed table example optimal finite state controller cheese grid problem machine states starting belief state uniform probability distribution policy iteration converges finite state controller fourteen machine states optimizes possible starting belief states 
related advantage heuristic search focus computational effort regions belief space reached starting belief state example focus search probable search tree 
avoids linear programming computationally intensive part dynamic programming update 
theoretical properties algorithm similar policy iteration specialized starting belief state hansen 
theorem finite state controller optimize value starting belief state heuristic search transforms finite state controller improved value starting belief state 
theorem heuristic search algorithm converges finite number steps finite state controller ffl optimal starting belief state 
test problems table small pomdps heuristic search algorithm improves value finite state controller starting belief state faster policy iteration considerably faster 
results mixed improvement error bound 
problems error bound converges quickly zero close converges slowly ao search algorithm policy improvement step runs memory trying reduce 
quickly error bound converges depends primarily closely upper bound function estimates optimal value function particular problem 
upper bound function estimate error bound improved deep expansion search tree 
quality upper bound determines aggressively search tree pruned poor upper bound function cause size search tree quickly exceed available memory 
sophisticated methods computing upper bound functions developed implemented hauskrecht brafman expect improve performance heuristic search algorithm accelerate convergence error bound 
plan implement memory bounded version ao search deeply tree chakrabarti washington 
promising aspect heuristic search algorithm potential solving problems dynamic programming update computationally prohibitive 
consider simple maze problem described hauskrecht states actions observations 
small problem range dynamic programming 
tested policy iteration problem initial finite state controller single machine state 
iteration policy iteration took fraction second resulted improved finitestate controller machine states 
second iteration took minutes resulted improved finite state controller machine states 
third iteration dynamic programming update ran hours finishing point policy iteration terminated 
clearly problem dynamic programming computationally prohibitive 
finite state controller iterations minutes error bound value starting belief state uniform state probability distribution 
maze problem heuristic search algorithm finite state controller machine states value starting belief state significant improvement performance achieved smaller controller 
minutes expanding search tree algorithm ran memory reducing error bound 
course single example results preliminary 
suggest heuristic search algorithm may extend range problems policy space approach described applied 
may reasons eliminates need perform update improves finite state controller incremental fashion allows finegrained control problem solving focuses computation improve value starting belief state 
testing wider range examples improved implementation algorithm planned determine far approach may extend range pomdps solved algorithms exact piecewise linear convex representation value function 
related algorithms improved policy iteration algorithm new heuristic search algorithm solve infinite horizon pomdps searching policy space finite state controllers 
representation policy finite state controller number advantages 
optimal policy pomdp equivalent finite state controller approximated arbitrarily closely finite state controller 
evaluation finite state controller straightforward value function piecewise linear convex 
finite state controller easier understand policy represented explicitly implicitly mapping regions belief space actions executed maintaining belief state run time 
bottleneck value iteration policy iteration pomdps dynamic programming update littman 
prove worst case complexity exponential number actions observations vectors current value function 
policy iteration faster value iteration requires fewer iterations dynamic programming update converge ffl optimality 
heuristic search improve finite state controller performing dynamic programming update reason believe may outperform policy iteration 
finite state controller finds optimizes value starting belief state value possible belief state usually smaller controller dynamic programming 
heuristic search focuses computational effort regions search space belief space improvement finite state controller 
heuristic search algorithm described combines areas research pomdps developed 
hand draws exact algorithms pomdps dynamic programming piecewise linear convex representation value function smallwood sondik sondik cassandra cassandra 
draws approximation algorithms pomdps perform forward search starting belief state including computing bounds fringe nodes search tree lave larsen washington hauskrecht 
past heuristic search find solution takes form tree grows depth search increases 
contribution show heuristic search find compact finite state controller containing cycles describes infinite horizon behavior 
acknowledgments 
shlomo zilberstein anonymous reviewer helpful comments tony cassandra hauskrecht making available test problems 
support provided part national science foundation iri iri int 
brafman 

heuristic variable grid solution method pomdps 
proceedings fifteenth national conference artificial intelligence 
aaai press mit press 
cassandra kaelbling littman 
acting optimally partially observable stochastic domains 
proceedings national conference artificial intelligence 
aaai press mit press 
cassandra littman zhang 
incremental pruning simple fast exact algorithm partially observable markov decision processes 
proceedings thirteenth annual conference uncertainty artificial intelligence 
morgan kaufmann publishers 
chakrabarti acharya 
heuristic search restricted memory 
artificial intelligence 
hansen 
improved policy iteration algorithm partially observable mdps 
advances neural information processing systems 
press 
hansen 
finite memory control partially observable systems 
ph diss department computer science university massachusetts amherst 
hauskrecht 
incremental methods computing bounds partially observable markov decision processes 
proceedings fifteenth national conference artificial intelligence 
aaai press mit press 
kaelbling littman cassandra 
planning acting partially observable stochastic domains 
computer science technical report cs brown university 
larsen 
decision tree approach maintaining deteriorating physical system 
phd thesis university texas austin 
littman cassandra 
efficient dynamic programming updates partially observable markov decision processes 
computer science technical report cs brown university 
lave 
markovian decision processes probabilistic observation states 
management science 
smallwood sondik 
optimal control partially observable markov processes finite horizon 
operations research 
sondik 
optimal control partially observable markov processes infinite horizon discounted costs 
operations research 
washington 
incremental markov model planning 
proceedings tai eighth ieee international conference tools artificial intelligence 
washington 
bi pomdp bounded incremental partially observable markov model planning 
proceedings fourth european conference planning 
