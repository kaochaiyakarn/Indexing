technical report department statistics university toronto monte carlo implementation gaussian process models bayesian regression classification radford neal department statistics department computer science university toronto toronto ontario canada radford stat utoronto ca january 
gaussian processes natural way defining prior distributions functions input variables 
simple nonparametric regression problem function gives mean gaussian distribution observed response gaussian process model easily implemented matrix computations feasible datasets cases 
hyperparameters define covariance function gaussian process sampled markov chain methods 
regression models noise distribution logistic probit models classification applications implemented sampling latent values underlying observations 
software available implements methods covariance functions hierarchical parameterizations 
models defined way discover high level properties data inputs relevant predicting response 
nonparametric bayesian regression model prior distribution infinite dimensional space possible regression functions 
known years priors functions defined gaussian processes hagan essentially model long spatial statistics name kriging 
gaussian processes largely ignored general purpose regression models apart special case smoothing splines wahba applications modeling noise free data computer experiments sack welch mitchell wynn 
shown bayesian regression models neural networks converge gaussian processes limit infinite network neal 
motivated examination gaussian process models high dimensional applications neural networks typically applied williams rasmussen :10.1.1.17.729
empirical rasmussen demonstrated gaussian process models better predictive performance nonparametric regression methods range tasks varying characteristics 
conceptual simplicity flexibility performance gaussian process models attractive wide range problems 
reason previous neglect gaussian process regression may straightforward implementation involves matrix operations time requirements grow cube number cases space requirements grow square number cases 
years ago may limited models datasets cases modern computers feasible apply gaussian process models datasets cases 
may possible reduce time requirements sophisticated algorithms gibbs mackay 
characteristics gaussian process model easily controlled writing covariance function terms hyperparameters 
approach adapting hyperparameters observed data estimate maximum likelihood maximum penalized likelihood long done context spatial statistics mardia marshall 
fully bayesian approach hyperparameters prior distributions 
predictions averaging posterior distribution hyperparameters done markov chain monte carlo methods 
approaches give similar results williams rasmussen rasmussen fully bayesian approach may robust models elaborate :10.1.1.17.729
applying gaussian process models classification problems presents new computational problems joint distribution quantities longer gaussian 
approximate methods bayesian inference models proposed barber williams gibbs mackay 
general approach exactly handling classification generalized models poisson response markov chain monte carlo scheme unobserved latent values associated case explicitly represented 
applies approach classification logistic probit models regression models noise follows distribution 
written software unix systems implements gaussian process methods regression classification framework bayesian neural network software 
software freely available research educational 
covariance functions supported may consist parts may specified terms hyperparameters described detail section 
covariance functions provide functionality similar neural network models 
software implements full bayesian inference hierarchical models matrix computations markov chain sampling methods described sections 
sections demonstrate software way classification problem model identify inputs relevant predicting class regression problem outliers 
conclude discussing areas research 
follow links home page www cs utoronto ca radford 
version described 
introduce detail idea bayesian modeling gaussian processes 
regression classification gaussian processes assume observed data cases vector inputs predictors case associated target response 
primary purpose predict target new case observed inputs 
interested interpretation point interpreting model failed capture regularities support predictive performance 
regression problem targets real valued classification problem targets finite set class labels take gamma 
convenient represent distributions targets terms unobserved latent values associated case 
bayesian regression classification models usually formulated terms prior distribution set unknown model parameters posterior distribution parameters derived generally exhibited explicitly 
focus prediction case final result predictive distribution new target value obtained integrating unknown parameters 
predictive distribution expressed directly terms inputs new case inputs targets observed cases mention model parameters 
expressing prior knowledge terms prior parameters integrate parameters obtain prior distribution targets set cases 
predictive distribution unknown target obtained conditioning known targets 
operations easily carried distributions gaussian 
fortunately gaussian processes flexible represent wide variety interesting regression models infinite number parameters formulated conventional fashion 
discussing nonparametric models may help see scheme works simple linear regression model written ff fi ffl ffl gaussian noise case assumed independent case case mean zero variance oe ffl moment assume oe ffl known ff fi unknown 
give ff fi independent gaussian priors means zero variances oe ff oe set cases fixed inputs prior distribution parameters implies prior distribution associated target values multivariate gaussian mean zero covariances cov ff fi ffl ff fi ffl oe ff oe ffi ij oe ffl ffi ij zero 
mean covariance function sufficient define gaussian process giving distribution possible relationships inputs target 
strictly speaking wish confine term gaussian process distributions functions inputs target 
relationship functional due noise may differ identical looser usage convenient 
suppose know inputs observed cases inputs case wish predict target 
equation compute covariance matrix associated targets assumption means zero covariances define gaussian joint distribution targets observed unobserved cases 
condition known targets obtain predictive distribution known results von mises section show predictive distribution gaussian mean variance fi fi fi gamma var fi fi fi gamma gamma covariance matrix targets observed cases equation delta delta delta vector known target values cases vector covariances known targets prior variance cov equation 
practice prior knowledge usually sufficient fix appropriate values hyperparameters define covariance oe ffl oe ff oe simple model equation 
give prior distributions hyperparameters base predictions sample values posterior distribution 
sampling posterior distribution requires computation log likelihood observed cases gamma log gamma log det gamma gamma derivatives computed sampling described sections 
procedure unnecessarily expensive simple regression model just discussed better handled standard computational procedures 
gaussian process procedure handle interesting models simply different covariance function equation 
example regression model arbitrary smooth functions obtained covariance function cov exp gamma ae gamma ffi ij oe ffl ae hyperparameters usually prior distribution fixed 
possibilities covariance function discussed section 
regression models non gaussian noise models classification problems targets set gamma defined terms gaussian process model latent values associated case 
latent values define distribution target case 
example logistic model binary targets defined terms latent values letting distribution target case exp gammay ji gamma latent values gaussian process prior cov exp gamma ae gamma covariance function gives model probability target varies smoothly function inputs 
classes analogous model defined latent values case gamma define class probabilities follows exp gammay gamma exp gammay latent values independent gaussian process priors 
representation redundant removing redundancy forcing latent values zero introduce arbitrary asymmetry prior 
computational reasons covariance function equation usually modified addition small amount jitter follows cov exp gamma ae gamma ffi ij gives amount jitter similar noise regression model 
including small amount jitter matrix computations better conditioned improves efficiency sampling having small effect model 
effect probit model produced larger amount jitter 
probit model binary targets defined directly terms latent values having covariance function jitter follows phi phi standard gaussian cumulative distribution function 
formulation probit model mimicked latent values covariance function includes jitter term equation 
regarded sums jitter free latent variables independent jitter variance 
probit model obtained theta theta 
integrating jitter gives effect equation 
scaling magnitude jitter non jitter parts covariance leave effect equation unchanged point threshold function replaced logistic function equation magnitude usually large value logistic function close zero 
covariance function allows latent values function inputs plus jitter class probabilities representable logistic probit model 
sorts models differ exact prior class probability functions embody 
clear models better typical situations 
possible amount jitter hyperparameter allowing data determine models appropriate select intermediate model 
latent values define regression models non gaussian noise latent value noise free value regression function 
practice usually necessary include small amount jitter covariance function latent values effect introducing minimum amount gaussian noise 
distribution noise particularly convenient expressed terms gaussian noise model noise variances cases independently drawn inverse gamma distribution 
implementation model case noise variances explicitly represented sampled 
latent values needed sample noise variances discarded purpose 
possible software allow option phi theta functions logistic allowing probit model implemented exactly done moment 
possible allow logistic replaced function produces exact logistic model finite amount jitter investigated detail 
covariance functions hyperparameters wide variety covariance functions gaussian process framework subject requirement valid covariance function result positive semidefinite covariance matrix targets set number cases inputs take possible values 
bayesian model covariance function usually depend various hyperparameters prior distributions 
hyperparameters control amount noise regression model scale variation regression function degree various input variables relevant magnitudes different additive components model 
posterior distribution hyperparameters concentrated values appropriate data observed 
characterizing set valid covariance functions trivial seen extensive discussions book 
way construct variety covariance functions adding multiplying covariance functions element element sum product symmetric positive semidefinite matrices symmetric positive semidefinite horn johnson 
sums covariance functions useful defining models additive structure covariance function sum independent gaussian processes simply sum separate covariance functions 
products covariance functions useful defining covariance function cases multidimensional inputs terms covariance functions single inputs 
current software supports covariance functions sum terms types constant part pair cases regardless inputs cases 
adds constant component regression function latent values classification model prior value constant component having variance constant term covariance function 
linear part covariance cases form oe produces linear function inputs seen section adds linear component function terms covariance 
jitter part zero different cases constant covariance case 
jitter improve conditioning matrix computations produce effect probit classification model 
noise regression model similar treated separately implementation jitter affects latent values targets noise affects targets 
number exponential parts covariance cases form exp gamma ae fi fi fix gamma fi fi fi exponential parts may different values ae covariance function positive definite range 
default value produces function additive component function infinitely differentiable constrained particular form 
parameters terms covariance function may fixed may treated hyperparameters prior distributions power exponential part currently fixed 
possible distributions functions obtained covariance functions form illustrated 
functions single input index dropped 
top left top right show functions drawn randomly gaussian process covariance function consisting single exponential part 
distance function varies amount comparable full range ae smaller top right top left 
bottom left shows functions generated covariance function sum constant linear exponential parts 
magnitude exponential part small functions depart slightly straight lines 
bottom right shows functions drawn prior covariance function sum exponential parts produce variation different scales different magnitudes 
software produce plots randomly drawn functions dimensions cholesky decomposition covariance matrix targets grid input points described section 
problems input variable oe ae parameters control degree input relevant predicting target 
ae close zero input little effect degree covariance cases little effect portion covariance due exponential part ae hyperparameter occurs 
cases high covariance greatly different values input input effectively ignored 
typical applications constant part jitter part covariance fixed values available prior information sufficient fix hyperparameters specify magnitudes linear exponential parts scales variation relevances various inputs 
standard deviation noise regression model typically unknown 
hyperparameters usually fairly vague prior distributions 
distributions proper improper prior produce improper posterior 
priors hyperparameters supported software take form 
hyperparameter take positive values value oe gamma gamma prior density oe ff ff gamma ff oe ff gamma exp 
cov exp gamma gamma cov exp gamma gamma cov cov exp gamma gamma exp gamma gamma exp gamma gamma functions drawn gaussian processes various covariance functions 
graphs shows functions independently drawn gaussian process mean zero covariance function graph 
ff positive shape parameter mean oe 
software accepts prior specifications terms ff gamma units correspond original hyperparameter 
large values ff produce priors concentrated near gamma small values ff produce vague priors 
single hyperparameters exponential part noise standard deviation simple regression model may prior described fixed value equivalent letting ff 
hyperparameters come groups ae exponential part oe linear part hierarchical priors expressed terms higher level hyperparameter associated group direct effect covariance function determines mean lower level hyperparameters 
example ae hyperparameters exponential part accompanied higher level hyperparameter ae top level oe ae gamma gamma prior form oe ff ff gamma ff oe ff gamma exp gammaoe ff 
value ae ae hyperparameters associated particular inputs independent oe ae gamma having gamma prior mean oe follows oe oe ff oe ff gamma ff oe ff gamma exp gammaoe ff oe note shape parameters levels ff ff different ff inputs 
top level hierarchy omitted effectively ff case ae independent 
lower level hierarchy omitted effectively ff case ae equal ae levels omitted ff ff case ae fixed values 
hierarchical priors link oe parameters linear part covariance noise standard deviations regression model target 
way linking hyperparameters different types linking hyperparameters pertaining different parts covariance function ae different exponential parts 
target latent value case hyperparameters currently independent gaussian processes model relationship value inputs 
exception different noise standard deviations possible regression models target 
contrast elaborate provisions different covariance functions software currently assumes mean function gaussian process zero 
appropriate problems prior knowledge vague 
note zero mean arbitrary aspect form prior specification ff controls diffuse prior shape 
fixed letting gamma prior oe specified power 
scheme analogous priors neural network models gamma results conjugate prior computational advantages 
gaussian process mean expect actual regression function take positive negative values equal parts range 
covariance function large constant term surprised actual function positive negative range interest 
mean function zero simply reflects lack prior knowledge sign turn 
practice usually desirable transform targets mean approximately zero order eliminate need large constant term covariance 
including large constant term undesirable increases round error matrix computations 
matrix computations inferences regarding gaussian process model particular values hyperparameters performed computations involving covariance matrix targets latent values associated observed cases 
appropriate hyperparameter values known priori matrix computations needed predictions targets new cases 
common situation hyperparameters unknown matrix computations support markov chain sampling methods described section predictions resulting sample hyperparameter values latent values required 
central object computations covariance matrix latent values underlying observed cases target values regression model 
covariance matrix denote depends observed inputs cases particular values hyperparameters considered fixed 
difficulty computations involving determined condition number ratio largest eigenvalue smallest eigenvalue 
condition number large round error matrix computations may cause fail highly inaccurate 
potential problem controlled covariance functions include jitter terms see section jitter contributes additively eigenvalue matrix reducing condition number 
covariance matrix targets regression model noise variance equivalent effect 
problems appears addition small amount jitter covariance seriously affect statistical properties model may desirable 
accordingly software attempt handle covariance matrices badly conditioned 
implementation finding cholesky decomposition lower triangular matrix ll cholesky decomposition simple algorithm see example section runs time proportional cholesky decomposition determinant easily computed square product diagonal elements 
practice log determinant sum logs diagonal elements 
cholesky decomposition generating latent target values prior 
standard methods randomly generate vector composed independent gaussian variates mean zero variance 
compute vector ln mean zero covariance matrix ll procedure produce plots covariance matrix targets grid input values addition unnoticeable amount jitter 
primary cholesky decomposition computing inverse arises predictive distribution new case equations log likelihood equation 
computations performed explicitly finding inverse gamma cholesky decomposition solving lv forward substitution solving backward substitution 
convenient explicitly compute inverse needed anyway order compute derivatives log likelihood 
computation gamma done applying procedure just described compute gamma vectors zero element value 
takes time proportional gamma computed prepare predictions regression model computing gamma vector targets training cases 
mean predictive distribution target test case time proportional compute vector covariances targets test case training cases 
compute predictive mean equation method implementation 
alternative solve lu solve lv compute predictive mean efficient time proportional test case gibbs mackay report accurate poorly conditioned 
require predictive variance mean compute gamma equation take time proportional predictions classification models involve similar operations focused latent value associated test case 
vector latent values associated training cases available 
vector covariances latent values latent value test case computed predictive mean variance latent value test case 
sample values gaussian predictive distribution easily obtained monte carlo estimates class probabilities test case computed simply averaging probabilities obtained substituting latent values sample equation 
may wish sample joint posterior distribution targets latent values set cases part computation order plot regression functions drawn posterior distribution 
conditional values hyperparameters latent variables associated training cases classification model case case noise variances regression model distributed noise distributions gaussian means covariances general equations 
regression model example vector latent values set test cases vector target values training cases gamma cov gamma gamma covariance matrix targets training cases covariance matrix latent values test cases matrix covariances targets training cases latent values test cases 
means covariances computed value generated cholesky decomposition covariance matrix described regard sampling prior 
markov chain methods sample posterior distribution hyperparameters regression model require computation log likelihood equation 
seen easily done cholesky decomposition markov chain sampling methods derivatives respect various hyperparameters required 
derivative log likelihood respect hyperparameter written follows mardia marshall gamma tr gamma gamma gamma trace product term computed time proportional assuming gamma computed 
second term computed time proportional computing gamma probably needed anyway compute multiplying left matrix derivatives multiplying result apart computation procedure repeated hyperparameter regression target 
markov chain methods classification models require similar computation vector targets replaced vector current latent values large data sets time required computations dominated required form cholesky decomposition compute gamma number operations required grows proportion machines memory caches time required computations may grow rate faster larger matrices fit fast cache 
software attempts reduce cache effects possible scanning matrices rows columns large matrices slowdown sgi machine substantial 
small data sets cases time required compute derivatives log likelihood respect hyperparameters dominate time grows proportion may occur example hyperparameters controlling relevance input variables computing matrix derivatives covariances takes lot time 
computations sped individual values exponential parts covariances saved appear expressions derivatives 
software small memory required large larger operations dominate anyway 
markov chain sampling covariance functions gaussian process models contain unknown hyperparameters integrated fully bayesian treatment 
number hyperparameters vary simple regression model dozen model inputs relevances individually controlled hyperparameters ae equation 
markov chain monte carlo methods see neal review feasible approach performing integrations complex models 
classification models latent values training case integrated regression models noise distribution integrate case case noise variances 
latent values variances included state markov chain sampled hyperparameters 
sampling posterior distribution hyperparameters facilitated representing logarithmic form sampling methods independent scale data 
widely method gibbs sampling easily applied problem difficult sample conditional distributions hyperparameter values latent values 
metropolis algorithm simple proposal distribution gaussian diagonal covariance matrix 
software supports option variety markov chain sampling methods 
simple methods explore region high probability inefficient random walk 
probably better models method suppress random walks neal 
appropriate way suppress random walks problem hybrid monte carlo method duane kennedy variant method due horowitz 
employed hybrid monte carlo method bayesian inference neural network models neal rasmussen gaussian process regression 
variants hybrid monte carlo method supported markov chain modules neural network gaussian process software 
give brief informal description method 
details neal rasmussen :10.1.1.17.729
hybrid monte carlo method suppresses random walks introducing momentum variables associated position variables focus interest 
gaussian process application position variables hyperparameters defining covariance function 
state simulation evolves way position momentum physical particle travelling region variable potential energy 
momentum causes particle continue consistent direction time region high energy low probability encountered 
motion randomized bit order ensure correct distribution sampled undesirable random walk behaviour results 
practice differential equations describe position momentum change time discretized bias due discretization error eliminated accepting rejecting new state metropolis style 
leapfrog discretization usually 
order perform leapfrog update derivatives log posterior probability respect hyperparameters computed 
decide accept update sequence updates log posterior probability normalizing constant 
log posterior probability computed log prior probabilities hyperparameters easily computed gamma form log likelihood equation 
derivatives adding derivative log prior easily computed derivative log likelihood computed equation 
original hybrid monte carlo method duane 
leapfrog updates done decision accept result 
momentum randomized time 
variation windows states neal increase acceptance probability 
variation due horowitz acceptance decision leapfrog update momentum partially randomized 
refer hybrid monte carlo persistence momentum 
hybrid monte carlo appropriate stepsizes leapfrog updates selected large stepsize acceptance rate low stepsize small progress needlessly slow 
different stepsizes different hyperparameters equivalent rescaling hyperparameters logarithmic form different scale factors 
software includes heuristic procedure automatically selects stepsize hyperparameter 
selections estimates second derivatives log posterior density respect hyperparameters indicate large change hyperparameter getting region low probability 
automatically selected stepsizes usually manually adjusted multiplying factor chosen basis preliminary runs 
accordingly real role heuristics set relative stepsizes different hyperparameters 
heuristics simple 
stepsizes high level hyperparameters scaled square root number low level hyperparameters control 
accord expect width posterior distribution scale 
similarly stepsize noise variance regression model scaled square root number training cases 
stepsizes hyperparameters ae hyperparameters exponential part covariance scaled basis number training cases 
right thing depends posterior distribution hyperparameters tightly concentrated number training cases increases 
conjecture posterior distributions typically concentrated prior concentrated number training cases increases ae parameters exponential part functions produced fractal 
mardia marshall consider problem spatial statistics context assumption range input variables increases number training cases presume typical situation regression classification problems 
additional training cases provide denser sampling fixed region provide limited amount information hyperparameters function modeled fractal nature information repeated scales 
classification model hybrid monte carlo updates hyperparameters likelihood current latent values associated training cases targets directly 
hyperparameter updates interleaved updates latent values gibbs sampling presently 
new latent values chosen case sequential scan 
values drawn conditional distribution latent value observed target training case current values hyperparameters latent values 
density conditional distribution proportional product likelihood target equation gaussian conditional density latent values 
conditional density latent values collected proportional exp gamma gamma time proportional gamma computed 
final conditional density efficiently sampled adaptive rejection method gilks wild 
gamma computed time proportional complete gibbs sampling scan takes time proportional sense perform quite gibbs sampling scans update hyperparameters adds little time requirements probably markov chain mix faster 
software supports regression models distributed noise expressed gaussian noise case case variances drawn inverse gamma distribution 
markov chain sample case case noise variances needed compute covariances targets 
approach case case latent values maintained updated gibbs sampling manner analogous classification models 
gibbs sampling easily done case case noise variances hyperparameters controlling noise level latent values targets 
software supports second approach latent values kept permanently 
latent values temporarily latent values case difference inner loop scan cases values case 
training cases way classification problem 
case plotted values plot symbol indicating class follows class filled square class plus sign class open triangle generated just noise variances updated equations discarded generate new values noise variances 
example way classification problem demonstrate software classification applied synthetic classification problem 
pairs data items generated randomly drawing quantities independently uniform distribution interval 
class item encoded selected follows dimensional euclidean distance point class set class set conditions held class set 
note effect class 
inputs available prediction target values plus independent gaussian noise standard deviation 
generated cases way training model testing resulting predictive performance 
training case shown 
data modeled gaussian process latent values covariance function consisted terms constant part fixed exponential part magnitude scales inputs ae variable hyperparameters jitter part fixed 
fairly large amount jitter produces effect close probit model discussed section 
ae vary separately control common higher level hyperparameter model ca discovering inputs fact irrelevant task predicting target 
hope posterior distribution ae irrelevant inputs concentrated near zero degrade predictive performance 
persistent form hybrid monte carlo sampling allows latent values resampled leapfrog update hyperparameters 
fairly low persistence leapfrog updates order allow energy dissipated rapidly replacement momentum consequent elimination kinetic energy 
larger persistence order suppress random walk behaviour 
update hyperparameters latent values associated training cases updated gibbs sampling scans 
sequence combined gibbs sampling leapfrog updates done sampling iteration hyperparameters latent values saved possible 
sampling continued iterations leapfrog updates took minutes sgi machine 
complete details regarding model sampling procedure may software documentation problem example 
convergence markov chain simulation assessed plotting values hyperparameters change course simulation 
shows progress ae hyperparameters exponential part covariance 
hoped see iteration apparent equilibrium reached hyperparameters ae ae associated irrelevant inputs values smaller ae ae associated inputs provide information target class 
markov chain simulation updates latent values associated training case define class probabilities equation 
latent values particular training case plotted course simulation 
gibbs sampling scans appear effective moving values equilibrium distribution fairly rapidly 
predictions test cases average predictive probabilities iterations equilibrium apparently reached 
reduce computation time fifth iteration starting iteration total iterations 
iteration covariance matrix latent values training cases inverted predictive mean variance latent values test cases latent values training cases saved iteration 
sample points predictive distribution produce monte carlo estimate predictive probabilities classes 
final predictive probabilities averaging predictions way iterations 
guess class test case largest predictive probability 
progress relevance hyperparameters course markov chain simulation 
values plotted log scale ae solid ae long dash ae short dash ae dotted 
latent values associated training case course markov chain simulation 
latent values shown class solid class dashed class dotted 
procedure took minutes sgi machine 
classification error rate test cases 
performance close analogous neural network model 
proper comparison predictive performance classification methods scope 
rasmussen done extensive comparisons gaussian process models methods regression problems 
expected time required problem varies considerably number training cases 
training cases time markov chain simulation minutes time required predictions test cases minute 
classification error rate training cases 
example regression problem outliers demonstrate software handle regression problems outliers applied gaussian process model non gaussian noise simple synthetic problem single input variable 
cases generated input variable drawn standard gaussian distribution corresponding target value came distribution mean sin cases distribution target mean gaussian standard deviation 
probability case outlier standard deviation 
data modeled gaussian process expected value target noise assumed come distribution degrees freedom 
particularly close actual noise distribution described heavy tails distribution may allow data modeled outliers having undue effect 
comparison data modeled assumption gaussian noise 
data predictions models shown 
models covariance function contained constant part fixed exponential part variable hyperparameters 
model non gaussian noise included small amount jitter order improve conditioning matrix computations sample latent values 
jitter equivalent small amount additional noise model amount noise variable hyperparameter real effect constrain total noise jitter 
markov chain sampling model distributed noise done alternating hybrid monte carlo updates hyperparameters consisting leapfrog updates updates case case noise variances 
latent values generated order allow gibbs sampling updates noise variances equations discarded 
markov chain simulated iterations 
regression problem outliers 
training cases shown dots input horizontal axis target vertical axis 
solid line gives mean predictive distribution model noise assumed come distribution degrees freedom 
dotted line gives mean predictive distribution model noise assumed gaussian 
predictions fifth iteration iteration 
time required simulation minutes sgi machine 
details model markov chain method obtained description example software documentation 
seen model distributed noise produces predictions reasonable produced model gaussian noise just looking scatterplot data 
predictions distributed noise closer true function 
discussion software described extends scope gaussian process models classification problems regression problems non gaussian noise markov chain monte carlo method latent values case represented 
models variety covariance functions defined terms hyperparameters hierarchical priors 
implementation allows wide variety markov chain sampling methods 
facilities usefulness gaussian process models variety problems explored 
examples show gaussian process models practically applied classification problems moderate size regression problems non gaussian noise examples regression classification models included software documentation 
major focus explore uses elaborate covariance functions real problems 
fairly simple model section illustrates hyperparameters defining covariance function adaptively determine relevant various inputs predicting target 
range covariance functions implemented permits hierarchical models elaborate 
example including exponential parts covariance function separate set relevance hyperparameters possible define prior distribution puts considerable prior weight models nearly additive form function decomposed sum functions depends small subset inputs 
model automatically determine appropriate additive decomposition additive model fact appropriate 
mirrors similar idea neural network models neal section 
implementation described straightforward 
operations performed simplest way gives acceptable results 
number modifications contemplated 
faster convergence probably obtained updating latent variables hybrid monte carlo gibbs sampling 
computation time matrix operations reduced conjugate gradient approach gibbs mackay 
direction look ways reducing eliminating need jitter covariance function appears usually acceptable solution problem poorly conditioned matrices may circumstances undesirable gaussian process model noise free data computer experiments sack welch mitchell wynn 
algorithmic improvements gaussian process models feasible datasets cases fairly run mill computers provided willing wait hours results larger datasets 
models feasible option regression classification problems 
despite fairly unfavourable growth memory requirements growth computation time gaussian process algorithms improvements computer technology years allow models applied problems encountered practice 
ease flexible hierarchical models defined gaussian processes believe prove useful techniques nonparametric regression classification 
carl rasmussen helpful discussions opportunity learn implementation gaussian process regression thesis 
david mackay chris williams comments manuscript 
research supported natural sciences engineering research council canada 
barber williams 
gaussian processes bayesian classification hybrid monte carlo appear advances neural information processing systems 
duane kennedy 
hybrid monte carlo physics letters vol 
pp 

gibbs mackay 
efficient implementation gaussian processes draft manuscript 
gibbs mackay 
variational gaussian process classifiers draft manuscript 
gilks wild 
adaptive rejection sampling gibbs sampling applied statistics vol 
pp 

horn johnson 
matrix analysis cambridge university press 
horowitz 
generalized guided monte carlo algorithm physics letters vol 
pp 

mardia marshall 
maximum likelihood estimation models residual covariance spatial regression biometrika vol 
pp 

neal 
probabilistic inference markov chain monte carlo methods technical report crg tr department computer science university toronto 
available postscript url www cs utoronto ca radford 
neal 
improved acceptance procedure hybrid monte carlo algorithm journal computational physics vol 
pp 

neal 
bayesian learning neural networks new york springer verlag 
hagan 
curve fitting optimal design prediction discussion journal royal statistical society vol 
pp 

hagan 
bayesian inference volume kendall advanced theory statistics rasmussen 
evaluation gaussian processes methods non linear regression ph thesis university toronto department computer science 
available postscript url www cs utoronto ca carl 
sacks welch mitchell wynn 
design analysis computer experiments discussion statistical science vol 
pp 


elements statistical computing new york chapman hall 
von mises 
mathematical theory probability statistics new york academic press 
wahba 
improper priors spline smoothing problem guarding model errors regression journal royal statistical society vol 
pp 

williams rasmussen 
gaussian processes regression touretzky mozer hasselmo editors advances neural information processing systems mit press 

correlation theory stationary related random functions volume basic results new york springer verlag 

