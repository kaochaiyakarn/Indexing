smooth line learning algorithms hidden markov models pierre baldi jet propulsion laboratory california institute technology yves chauvin net id simple learning algorithm hidden markov models hmms number variations 
classical algorithms baum welch algorithm algorithms described smooth line example presentation batch mode usual viterbi path approximation 
algorithms simple expressions result normalized exponential representation hmm parameters 
algorithms proved exact approximate gradient optimization algorithms respect likelihood log likelihood cross entropy functions usually convergent 
algorithms casted general em expectation maximization framework viewed exact approximate gem generalized expectation maximization algorithms 
mathematical properties algorithms derived appendix 
division biology california institute technology correspondence addressed 
department psychology stanford university 
hidden markov models hmms particular class adaptive systems extensively speech recognition problems see rabiner review interesting applications single channel kinetic modeling ball rice 
hmms related general statistical techniques em expectation maximization algorithm dempster 
applied modeling analysis dna protein sequences biology baldi 
stormo haussler 
krogh 
optical character recognition levin pieraccini 
order characterized set states alphabet symbols probability transition matrix ij probability emission matrix ij 
parameter ij resp 
ij represents probability transition state state resp 
emission symbol state 
hmms viewed adaptive systems training sequence symbols parameters hmm iteratively adjusted order optimize fit model data measured criterion 
commonly goal maximize likelihood lo logarithm sequence model 
likelihood computed exactly simple dynamic programming scheme known forward procedure takes account possible paths capable producing data 
certain situations preferable approximate likelihood lower bound corresponding likelihood probable path 
probable path easily computed slightly different recursive procedure known viterbi algorithm 
training sequences available usually treated independent goal maximize likelihood model lo logarithm log lo 
different objective functions posterior distribution model data possible instance stolcke omohundro 
learning examples hmms typically accomplished algorithm 
iteration baum welch algorithm expected number ij resp 
ij transitions resp 
emissions letter state induced model fixed set parameters training sequence calculated forward backward procedure see rabiner appendix formal definitions 
transition emission probabilities reset ij ij lo lo ij ij lo lo ij ij 
case single training sequence iteration baum welch algorithm resets transition emission parameter expected frequency current model training data 
case multiple training sequences contribution sequence weighted inverse likelihood 
clear baum welch algorithm lead abrupt jumps parameter space procedure suitable line learning training example 
order save computations viterbi approximation estimate likelihoods transition emission statistics computing path associated production sequence see juang rabiner merhav opposed forward backward procedure possible paths examined 
single path transition emission counts provided loops reasonably line version 
problem baum welch algorithm probabilities absorbing transition emission probability set remains equals 
course undesirable usually prevented artificially enforcing parameter fixed small threshold 
different algorithm hmm learning smooth overcomes previous obstacles line batch mode viterbi approximation defined follows 
normalized exponential representation parameters model introduced 
ij resp 
ij new parameter ij resp 
ij defined ij ij ik ij ij ik temperature parameter 
whichever changes applied learning algorithm normalisation constraints original parameters automatically enforced 
additional advantage representation parameters reach absorbing value 
representation general sense finite probability distribution written form provided probabilities allow infinite negative exponents 
property hmms general probabilities need avoided 
shall describe line version learning algorithm batch version obtained immediately summing training sequences 
estimating statistics ij ij instance forward backward procedure update equations new algorithm particularly simple ij lo ij ij ij lo ij ij learning rate incorporates temperature effects 
shall prove appendix equation represents line gradient descent negative log likelihood data remarkable simplicity escaped attention previous investigators partly role played baum welch algorithm 
notice line version gradient descent negative likelihood ij lo ij ij ij lo ij ij suitable learning rates excluding probably rare possibility convergence saddle points converge possibly local maximum likelihood 
back propagation gradient algorithm convergence exact batch mode stochastic line version 
general dynamic range reasons preferable 
furthermore case multiple training sequences requires global factor available 
certain situations may possible non scaled version form ij ij ij ij ij ij particularly true bulk training sequences tend likelihoods roughly range 
distribution likelihoods training sequences approximately gaussian requires standard deviation relatively small 
cases reasonable expect standard deviation decrease learning progresses 
likelihoods training sequences range viewed approximate rescaling corresponding vectors aligned certainly half space 
proper choice learning rate general different rate lead increase likelihood 
previous algorithms rely local discrepancy transitions emission counts induced data predicted value parameters model 
variations constructed discrepancy corresponding frequencies form ij lo ij ij ij lo ij ij ij lo ij ij ij lo ij ij ij ij ij ij ij ij appendix shown reached different heuristic line reasoning approximating gradient descent objective function constructed sum locally defined cross entropy terms 
variations obtained multiplying positive coefficients may depend sequence state case single training sequence vectors associated half plane rules increase likelihood provided learning rate sufficiently small 
case multiple training sequences reasonable expect average situations tend increase likelihood logarithm line steepest ascent 
accordingly convergence expected slower 
case line viterbi approximation optimal path associated current training sequence computed associated likelihood 
previous algorithms approximated replacing expected transition counts ij corresponding counts ij obtained optimal viterbi path similarly emissions 
definition ij appendix easy see ij ij ij number times transition appears path instance gradient descent equations approximated ij ij ij similarly emissions 
case architecture loops viterbi approximations particularly simple ij 
specific application baldi 
results obtained viterbi approximation rewritten form ij ij ij ij ij ij fixed state ij ij target transition emission values ij time transition part viterbi path corresponding training sequence similarly ij particular result loop viterbi path visits state times repeated visit line respect different paths associated different training sequences path 
shown appendix derived approximate gradient descent sum local cross entropy terms 
important difference time objective function discontinuous result evolution viterbi paths 
applications likelihoods lo small machine precision 
problem dealt scaling approach rabiner 
scaling equations derived baum welch algorithm readily extended algorithms 
possibility obvious reasons conjunction viterbi approximation calculate logarithm likelihoods logarithm likelihood optimal paths 
accordingly may possible situations replace previous algorithms factors lo log lo 
algorithms previously described seen general em framework dempster 

general em framework considers dependent random variables observed random variable 
value unique value general different values lead value observable addition parametrised family densities xj depending set parameters 
hmm terminology corresponds paths output sequences transition emission parameters 
usual problem try find maximum likelihood estimate set parameters observations em algorithm recursive procedure defined arg max arg max log xj shown general converges monotonically possibly local maximum likelihood estimator 
applied hmm context em algorithm baum welch algorithm 
interesting slightly different view em algorithm leads incremental variants neal hinton interpreted representing states statistical mechanical system 
energy state measured negative log likelihood seen double minimization step corresponding free energy function 
algorithm increases function result increases likelihood observation parameters necessarily maximizing called gem algorithm 
shown general proof appendix hmm context gradient gradient log likelihood observations parameters identical 
small gradient ascent step log likelihood lead increase sufficiently small learning rates gradient descent negative log likelihood related algorithms seen special cases gem algorithms approximations gem algorithms 
specific application natural ask previous learning rules 
believe answer question depends application considered architecture hmm possibly implementation constraints available precision 
clear play hmms role back propagation plays neural networks 
known hmms viewed particular kind linear neural networks 
back propagation applied equivalent networks parameter representation leads immediately 
hand easy see simple examples instance behave differently 
applications area dna protein sequence modeling viterbi paths play particular role viterbi learning approximation may desirable 
context extensive simulation results smooth algorithms baldi 

gradient method choice learning rate crucial may require experimentation 
obvious case neural networks modeling techniques ideas easily extended complex objective functions including instance regularizer terms reflecting prior knowledge hmm parameters 
true higher order hmms 
general simple algorithms introduced useful situations smoothness line learning important 
include large models parameters relatively scarce data may prone overfitting local minima trapping pathological behaviors trained discontinuous batch rules ii analog physical implementations continuous learning rules realized iii situations storage examples batch learning desirable 
mathematical appendix show correspond gradient ascent log likelihood likelihood respectively 
give different derivation gradient descent algorithms different objective function 
examine relation algorithms baum welch algorithm general em approach 
mathematical appendix brevity transition parameters considered analysis emission parameters analogous 
follows need partial derivatives ij ij ij ij ij ik ij ik derivation global gradient descent algorithms path model fixed sequence ff lo ff summation possible paths architecture finite sequence finite number paths needs considered 
training sequences likelihood model lo probability ff product corresponding transition emission probabilities path easy see ff ij ij ij ff ij number times transition path consistent production lo ij ij ij ij ij ff expected number times transition produce model combining leads ij ij ij lo chain rule simplifications get proposition ij ik ik ij lo ij ij log likelihood get proposition log ij ij lo ij ij clearly line version line version temperature parameter absorbed learning rate 
key conclusive point corresponds gradient descent negative log likelihood sensible algorithm 
notice contribution sequence weighted inverse likelihood 
may little paradoxical sight easily understood model assigns small likelihood sequence model performing properly sequence corresponding signal high 
heuristic derivation approximate gradient descent algorithms sum local cross entropy terms numbers ij calculated local distance current distribution ij transitions model expected distribution induced data measured cross entropy function io ij log ij ij equivalently consider local likelihood associated distribution transitions state logarithm likelihood yields term similar 
parameters updated gradient descent io order reduce distance ij io ij io ik ik ij ij ij ik ij approximation computes explicit derivative respect ij higher order contributions fact change ik affects quantities jl jo ik usually non zero neglected 
simplifications get proposition ij ij ij clearly line descent version 
learning rule derived approximating gradient descent sum local cross entropy terms distribution transitions model state expected value distribution data taken account 
similar complex learning rules derived weighting terms io sum instance lo 
temperature coefficient merged learning rate 
reasoning viterbi approximation yields notice instantaneous distribution transitions state viterbi path multinomial distribution form ij ij 
relations baum welch em gem algorithms examine relation previous algorithms baum welch algorithm useful examine proof convergence baum welch algorithm baum 
consider fixed architecture models different transitions emission parameters 
thought improvement seeking 
path model define probabilities ff fi assuming usual independence sequences likelihood model lo lo lo ff lo fi summations possible paths architecture 
sequence define probability distributions ff ff fi fi induced models sequence paths 
measure distance distributions cross entropy ho ff fi ff ff log ff ff fi fi cross entropy positive yields simple manipulations relation log lo lo lo ff log fi ff log ff gives log lo ff log fi ff log ff right hand side equal 
order find model higher likelihood model increase term ff log fi lo general em context term corresponds function defined 
baum welch algorithm directly maximizes term 
hand algorithm introduced just gradient descent step maximization term 
remarkably log likelihood log hand side gradient 
see observe probability fi product probabilities corresponding transitions associated path corresponding emissions associated sequence contribution sum ff log fi transitions additional similar term contributions resulting emissions rewritten lo ij ij log ij lo ij ij log ij ij number transitions contained path ij expected number transitions model data 
easy check expression probability distribution maximizes corresponds baum welch algorithm ij ij lo lo normalized exponential representation parameters gradient proposition ij ij lo ij ij ik ij lo ij ik ij lo ij ij line immediately yields algorithm 
batch version performs gradient ascent log leads small step sizes increase likelihood gradient zero 
leads increase viewed gem algorithm 
gradient cross entropy term maximized baum welch gradient log likelihood function 
proof hmms result proved similarly general em framework showing gradient likelihood gradient function maximized step em algorithm identical 
david haussler anders krogh esther levin useful discussions 
supported afosr onr 
baldi chauvin mcclure 
adaptive algorithms modeling analysis biological primary sequence information 
net id technical report submitted publication 
baldi chauvin mcclure 
hidden markov models molecular biology 
new algorithms applications 
advances neural information processing systems hanson cowan lee giles editors morgan kaufmann san mateo ca 
ball rice 
stochastic models ion channels bibliography 
mathematical bioscience 
press 
baum 
inequality associated maximization technique statistical estimation probabilistic functions markov processes 
inequalities 
blahut 
principles practice information theory 
addison wesley 
stormo 
expectation maximization algorithm identifying protein binding sites variable lengths unaligned dna fragments 
journal molecular biology 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
haussler krogh mian 
protein modeling hidden markov models 
computer information sciences technical report ucsc crl university california santa cruz 
juang rabiner 
segmental means algorithm estimating parameters hidden markov models 
ieee transactions acoustics speech signal processing 

krogh brown mian haussler 
hidden markov models computational biology applications protein modeling 
submitted publication 
levin pieraccini 
planar hidden markov modeling speech optical character recognition 
advances neural information processing systems hanson cowan lee giles editors morgan kaufmann san mateo ca 
merhav 
maximum likelihood hidden markov modeling dominant sequence states 
ieee transactions signal processing 
neal hinton 
new view em algorithm justifies incremental variants 
submitted biometrika 
rabiner 
tutorial hidden models selected applications speech recognition 
proceedings ieee 
stolcke omohundro 
hidden markov model induction bayesian model merging 
advances neural information processing systems hanson cowan lee giles editors morgan kaufmann san mateo ca 
