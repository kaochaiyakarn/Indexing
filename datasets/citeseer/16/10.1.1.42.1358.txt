enhanced representation time series allows fast accurate classification clustering relevance feedback introduce extended representation time series allows fast accurate classification clustering addition ability explore time series data relevance feedback framework 
representation consists piecewise linear segments represent shape weight vector contains relative importance individual linear segment 
classification context weights learned automatically part training cycle 
relevance feedback context weights determined interactive iterative process users rate various choices 
representation allows user define variety similarity measures tailored specific domains 
demonstrate approach space telemetry medical synthetic data 
time series account data stored business medical engineering social science databases 
innumerable statistical tests perform time series determining autocorrelation coefficients measuring linear trends utility collecting data comes ability humans visualize shape suitably plotted data classify 
example view diagnose 
chartists examine stock market data searching certain shapes thought indicative stock performance 
unfortunately sheer volume data collected means small faction data viewed 
attempts utilize classic machine learning clustering algorithms time series data met great success 
feel due typically high dimensionality time series data combined difficulty defining similarity measure appropriate domain 
example difficulties consider time series 
contains data points account database extracted 
addition consider problem clustering eamonn keogh michael pazzani department information computer science university california irvine california usa pazzani eamonn ics uci edu examples 
people group common distance measures raw data correlation absolute distance squared error group 
needed representation allows efficient computation data extracts higher order features 
representations proposed including fourier transformations faloutsos relational trees shaw envelope matching trees agrawal 
approaches met success shortcomings including sensitivity noise lack intuitiveness need fine tune parameters 
examples time series 
piece wise linear segmentation attempts model data sequences straight lines innumerable advantages representation 
pavlidis horowitz point provides useful form data compression noise filtering 
shatkay zdonik describe method fuzzy queries linear higher order polynomial segments 
keogh smyth demonstrate framework probabilistic pattern matching linear segments 
example time series piece wise linear representation 
pattern matching piece wise linear segments met success believe major shortcoming 
comparing time series see similar segments considered equal importance 
practice may wish assign different levels importance different parts time series 
example consider problem pattern matching 
cardiologist attempting diagnose myocardial mi pay close attention wave importance rest 
wish algorithm reproduce cardiologist ability need representation allows weight different parts time series differently 
propose representation 
piece wise linear segments represent shape time series weight vector contains relative importance individual linear segment 
show representation allows fast accurate clustering classification 
additionally show representation allows apply relevance feedback techniques information retrieval literature time series databases 
rest organized follows 
section introduces notation important operators 
section demonstrates techniques relevance feedback section describes classification algorithm takes advantage representation greatly boost accuracy 
representation time series numerous algorithms available segmenting time series pioneered pavlidis horowitz 
open question best choose optimal number segments represent particular time series 
problem involves trade accuracy compactness clearly general solution 
utilize segmentation algorithm proposed keogh 
method segments time series automatically selects best value emphasize algorithms regardless segmentation obtained 
notation clarity refer raw unprocessed temporal data time series piece wise representation time series sequence 
notation 
time series sampled points represented uppercase letter segmented version containing linear segments denoted bold uppercase letter tuple vectors length axr ayr aw th segment sequence represented line axr ayr aw represents segments weight 
illustrates notation 
axr ayr represent times series sequence straight segments sequence weights shown histogram contain relative importance segment 
time series segmented obtain sequence initialize weights 
weights changed weights renormalized sum products weight length corresponding segment equals length entire sequence true axr xr renormalization important gives property total weight associated sequence length constant regardless segments represent 
operation changes weight effect redistributing weights 
example weight single segment decreased segments weight slightly increased 
weights reflect relative absolute importance segments 
comparing time series advantage piece wise linear segment representation allows define variety distance measures represent distance time series 
important various domains different properties may required distance measure 
shows various distortions encounter time series briefly consider 
segmentation algorithm produces sequences acts noise filter need consider handling noise directly pavlidis horowitz keogh 
domains require distance measure insensitive offset translation 
example consider stocks values fluctuate respectively 
possible stock movements similar separated constant amount 
amplitude scaling sequences alike stretched compressed axis dealt easily 
simply requires normalizing sequences applying distance operator 
agrawal 
describe raw time series 
normalizing sequences similar accomplished times faster 
dealing longitudinal scaling stretching compressing time axis possible difficult 
refer interested reader keogh 
noise offset translation amplitude scaling longitudinal scaling linear drift discontinuities difficulties encountered defining distance measure time series 
linear drift occurs naturally domains 
example consider time series measure absolute sales ice cream cities similar populations 
expect time series similar city population remains constant experiences steady growth see linear drift 
possible remove linear trends box jenkins computationally expensive 
segmented representation possible define distance measure insensitive linear drift see 
datasets including shuttle dataset referred section contain discontinuities 
typically sensor calibration artifacts may causes 
attempt find remove approach simply define distance measure relatively insensitive possible define distance measures sequences invariant subset distortions 
comparing time series equivalent summing absolute difference pair dash lines weighted appropriate weighting factor segment 
experiments simple distance measure 
measure designed insensitive offset translation linear trends discontinuities 
convenient notational purposes assume endpoints sequences compared aligned 
general real data case 
process sequences breaking segments point corresponds endpoint sequence 
done 
aw bw ayr intuitively metric measures close corresponding segments parallel 
note desirable properties important property efficient compute 
particular comparing sequences approximately faster comparing corresponding time series especially important may need calculate distance frequently 
example suppose wishes hierarchically cluster data items group average method 
requires comparisons 
time series represented sequences shown contain data points sequence representations contain average segments 
results speedup 
example hierarchically clustered time series 
merging time series section define operation sequences call merge 
merge operator allows combine information sequences repeated application merge operator allows combine information multiple sequences 
basic idea merge operator takes sequences input returns single sequence shape compromise original sequences weight vector reflects corresponding segments sequence agree 
distance operator assume endpoints sequence aligned 
merging sequences may wish input sequences contribute final sequence 
accommodate associate term called influence input sequences 
influence term associated sequence scalar denoted si may informally considered mixing weight 
influence term comes depends application discussed detail sections 
merge sequences influence terms ai bi respectively algorithm creates sequence ai bi sign sign mag min ai bi max ai bi scale max max ayr min min ayr axr cyl ai bi ai bi ayr ai bi ai bi run axr rise ayr diff rise run scale cw aw bw sign mag diff cw normalize cw table merge algorithm 
shows sequences merged various values influence terms 
note resultant sequence sequence higher influence 
note weights differentiated closer influence terms 
sequence dominating maximum amount compromise place 
expect trivial operations merge ai merge ai ai result learning prototypes merge operator designed component sophisticated algorithms considered simple learning algorithm creates prototypical sequence 
creating prototype solely positive examples works merge merge merge examples merge operator various influence terms 
equal influence terms shape resultant sequence halfway influence term larger shape resulting sequence closer negative influence term shape resulting sequence looks differences exaggerated 
manner 
model sequence typical example class sequences 
merge sequence sequence member class resultant sequence considered general model class 
particular differences shape minimized averaging weights similar segments increased 
contrast creating prototype positive negative examples uses negative influence negative examples 
suppose sequence example class sequences 
suppose example different class 
merge negative influence term resultant sequence new prototype class differences shape exaggerated weights similar segments decreased 
maps neatly intuitions 
learning prototype class sequences set positive examples want shape learned average examples want increase weight segments similar segments predictors class 
trying learn negative example want exaggerate differences shape classes decrease weight similar segments segments similar classes discriminatory power 
shows illustration learning negative example 
negative example negative influence term 
magnitude influence term reflects corresponding sequence affects resultant sequence 
relevance feedback relevance feedback reformulation search query response feedback provided user results previous versions query 
extensive history text domain dating back rocchio classic 
attempts utilize relevance feedback techniques domains notably mars project rui 
best authors knowledge attempted explore time series databases relevance feedback framework spite fact relevance feedback shown significantly improve querying process text databases salton buckley 
section simple relevance feedback algorithm utilizes representation demonstrate synthetic dataset 
relevance feedback algorithm works manner 
initial query sequence rank sequences database query may hand drawn user 
best sequences shown user 
user assigns influences sequences 
positive influence sequences user approves 
negative influences sequences user finds irrelevant 
relative magnitude influence terms reflects strongly user feels sequences 
user likes si twice sj assign influences sequences merged produce new query process repeated desired 
merge new old old si si si new old experimental results test algorithm conducted experiment 
constructed type type time series defined follows type sin normalized zero plus gaussian noise type tan sin normalized zero plus gaussian noise time series originally sampled points segmented 
shows example type 
note superficially similar type somewhat sharper peak valley 
built initial query averaging time series segmenting result 
experimental runs 
run consisted steps 
coin toss decided type type target shape sin tan sin synthetic data created relevance feedback experiment 
original time series 
original time series noise added 
segmented version time series 
shape considered relevant particular experiential run 
initial query quality ranked sequences measured defined 
best sequences shown user rated assigning influences reflected closely thought resembled target shape 
new query built search rate process repeated twice 
evaluated effectiveness approach measuring average precision top sequences precision percent recall points 
precision defined proportion returned sequences deemed relevant recall defined proportion relevant items retrieved database 
results shown table 
order see ability assign negative influence terms helpful 
experimental run built queries contained just feedback sequences judged relevant 
results shown parentheses table 
initial query second query third query top table results relevance feedback experiments 
values recorded parentheses queries built just positive feedback 
expect initial query user input returns sequences essentially random order 
second query produces remarkable improvement third query produces near perfect ranking 
queries built just positive feedback produce improvement clearly inferior general method 
demonstrates utility learning positive negative instances 
classification done defining distance measures time series generally machine learning framework welldefined task building algorithm examining training set labeled examples accurately classify unlabeled instances 
section describe novel classification algorithm takes advantage representation evaluate classic machine learning methodology including cross validation comparison alternative approaches 
difficulty casting time series classification problems machine learning context machine learning problems typically mutually exclusive defined classes sick healthy 
time series problems common single defined class instances exhibit particular structure 
example consider 
attempting decide instance closer class class algorithm decide instance sufficiently close class classified 
naturally deciding close sufficiently close wish algorithm induce training set 
instances heart dataset 
note negative instances shown row exhibit particular structure 
positive instances shown rows fall types single peaked double peaked representation obvious approach classification merge positive instances training set resultant sequence template instances classified compared 
may circumstances domains may totally fail 
reason potential failure may distinct shapes typical class 
demonstrates problem exists heart dataset 
similar problem occur time series domains may distinct shapes prototypical single class merging single prototype result shape particularly resemble individual member class 
avoid problem algorithm needs able detect fact multiple prototypes class classify unlabeled instance class sufficiently similar prototype 
section describe algorithm call ctc cluster classify 
classification algorithm table shows outline learning algorithm 
input set sequences constitute training data 
output set sequences positive scalar 
unseen sequence classified member class distance member 
algorithm clusters positive examples group average agglomerative method follows 
algorithm begins finding distance negative instance closest positive example 
mean distances neg dis calculated 
distance positive instance similar positive example calculated 
mean distances pos dis calculated 
fraction pos dis neg dis calculated 
point closest positive examples replaced result merging sequences process repeated find fraction neg dis entire process repeated single sequence remains 
shows trace part algorithm dataset contains just positive instances 
set sequences returned set qi minimized 
returned pos disi neg disi 
algorithm described optimized simplicity requires comparisons 
storing distances calculated iteration recalculating distances necessary possible achieve speedup constant factors 
set positive instances cluster sequences pi neg dis mean distance negative instances closest match pi pos dis mean distance positive instances closest match pi pos dis neg dis closest pair sequences pi merge ai bi ci ai bi remove pi add pi best equal minimized return pos dis neg dis best best best table ctc learning algorithm 
trace ctc algorithm small dataset 
set prototypes specific generalize test set 
final set contains single sequence general trying model distinct shapes 
set best compromise 
experimental results test algorithm ran experiments datasets 
shuttle dataset consists output sensors hours space shuttle mission sts 
sensors measure variety phenomena 
inertia movement sensors measured degrees examples shown figures 
task distinguish sensors 
sensors sampled different frequencies 
problem algorithm great difficulty methods raw data 
created clean point versions sensor experiments 
heart dataset consists rr intervals obtained form ecg tapes sampled hz 
data long sequence contains ventricular events 
extracted events sections equal length chosen random 
shows examples ventricular events class data 
comparison purposes evaluated algorithms datasets described 
ctc algorithm described section 
algorithm weight feature disabled simply hardcoded weights equal 
nn simple nearest neighbor algorithm uses raw data representation time series 
unlabeled instance assigned class closest match training set 
absolute error distance measure having empirically determined superior obvious candidates squared error correlation datasets 
nns algorithm nn uses sequence representation distance measure defined section 
ran fold cross validation times 
algorithms trained tested exactly folds 
results table 
ctc nn nns default shuttle heart table experiment results classification experiments datasets ctc algorithm performs best 
ability outperform justification weighted representation 
shuttle dataset nn performs base rate 
surmise probably due sensitivity discontinuities trait dataset 
nns ability significantly better supports hypothesis 
related relevance feedback time series 
text domain active prolific research community 
salton buckley provide excellent overview comparison various approaches 
agrawal 
demonstrates distance measure time series works dividing sequences windows 
corresponding windows time series compared time series said similar windows similar 
individual windows said similar lies envelope specified width 
window data normalized remove effects amplitude scaling offset translation 
general pattern matching raw data feasible sheer volume data 
addition raw data may contain spikes noise confuse matching process 
variety higher level representations times series proposed notably discrete fourier transform 
approach involves performing discrete fourier transform original time series discarding informative coefficients mapping coefficients dimensional space 
original agrawal faloutsos swami allowed comparison time series equal length extended faloutsos ranganathan manolopoulos include subsequence matching 
including shatkay zdonik recognize piece wise linear higher order order representation greatly reduces required storage search space time series fail suggest robust distance measure 
introduced new enhanced representation time series empirically demonstrated utility clustering classification relevance feedback 
directions research include extensive evaluation algorithms incorporating query expansion salton buckley relevance feedback algorithm 
agrawal faloutsos swami 
efficient similarity search sequence databases 
proc 
th conference foundations data organization algorithms chicago october 
agrawal lin sawhney shim 
fast similarity search presence noise scaling translation times series databases 
vldb september 
box jenkins 
time series analysis forecasting control 
san francisco ca 
holden day cheng lu 

waveform correlation tree matching 
ieee conf 

faloutsos ranganathan manolopoulos 

fast subsequence matching time series databases 
sigmod proceedings annual conference minneapolis may zdonik 

approximate queries representations large data sequences 
proc 
th ieee international conference data engineering 
pp new orleans louisiana february 
keogh 

fast similarity search presence longitudinal scaling time series databases 
proceedings th international conference tools artificial intelligence 
pp 
ieee press 
keogh smyth 

probabilistic approach fast pattern matching time series databases 
proceedings rd international conference knowledge discovery data mining 
pp aaai press 
pavlidis horowitz 
segmentation plane curves 
ieee transactions computers vol 
august 
salton buckley 

improving retrieval performance relevance feedback 
jasis 
pp 

shaw 

structural processing waveforms trees 
ieee transactions acoustics speech signal processing 
vol 
february 
rocchio jr relevance feedback information retrieval smart system experiments automatic document processing ed 
salton prentice hall pp 

rui huang mehrotra ortega 

automatic matching tool selection relevance feedback mars 
proceedings nd int 
conf 
visual information systems 


www dresden mpg de data 
