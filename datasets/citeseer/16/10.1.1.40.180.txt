maximum entropy approach adaptive statistical language modeling ronald rosenfeld computer science department carnegie mellon university pittsburgh pa usa roni cs cmu edu may adaptive statistical described long information knowledge sources 
existing statistical language models exploit immediate history text 
extract information back document history propose trigger pairs basic information bearing elements 
allows model adapt expectations topic discourse 
statistical evidence multiple sources combined 
traditionally linear interpolation variants shown seriously deficient 
apply principle maximum entropy 
information source gives rise set constraints imposed combined estimate 
intersection constraints set probability functions consistent information sources 
function highest entropy set solution 
consistent statistical evidence unique solution guaranteed exist iterative algorithm exists guaranteed converge 
framework extremely general phenomenon described terms statistics text readily incorporated 
adaptive language model approach trained wall street journal corpus showed perplexity reduction baseline 
interfaced sphinx ii carnegie mellon speech recognizer reduced error rate 
illustrates feasibility incorporating diverse knowledge sources single unified statistical framework 
language modeling attempt characterize capture exploit regularities natural language 
statistical language modeling large amounts text automatically determine model parameters process known training 
language modeling useful automatic speech recognition machine translation application processes natural language incomplete knowledge 
view bayes law natural language viewed stochastic process 
sentence document contextual unit text treated random variable probability distribution 
example speech recognition acoustic signal goal find linguistic hypothesis rise 
seek maximizes pr 
bayes law arg max pr arg max pr ajl delta pr pr arg max pr ajl delta pr signal pr ajl estimated acoustic matcher compares stored models speech units 
providing estimate pr responsibility language model 
def 
words hypothesis 
way estimate pr chain rule pr pr jw gamma statistical language models try estimate expressions form pr jw gamma 
written pr wjh def gamma called history 
view information theory view statistical language modeling grounded information theory 
language considered information source abramson emits sequence symbols finite alphabet vocabulary 
distribution symbol highly dependent identity previous ones source high order markov chain 
information source certain inherent entropy amount non redundant information conveyed word average shannon theorem shannon encoding bits word average 
quality language model judged cross entropy regard distribution hitherto unseen text pm gamma delta log pm pm called logprob jelinek 
perplexity jelinek text regard model reported 
defined ppm ideal model capitalizes conceivable correlation language cross entropy equal true entropy practice models fall far short goal 
worse quantity directly measurable bounded see shannon cover king jelinek 
extreme correlations completely ignored cross entropy source pr prior log pr prior pr prior prior probability quantity typically greater language models fall range 
view goal statistical language modeling identify exploit sources information language stream bring cross entropy close possible true entropy 
view statistical language modeling dominant 
information sources document history potentially useful information sources history document 
important assess potential attempting incorporate model 
different methods doing including mutual information abramson training set perplexity perplexity training data see huang shannon style games shannon 
see rosenfeld details 
section describe information sources various indicators potential 
context free estimation unigram obvious information source predicting current word prior distribution words 
source entropy log vocabulary size 
priors estimated training data maximum likelihood model training set cross entropy gamma log 
information provided priors gamma log log short term history conventional gram gram bahl uses words history sole information source 
bigram predicts gamma trigram predicts gamma gamma 
gram family models easy implement easy interface application speech recognizer search component 
powerful surprisingly difficult improve jelinek 
capture short term dependencies 
reasons statistical language modeling 
unfortunately seriously deficient ffl completely blind phenomenon constraint outside limited scope 
result nonsensical ungrammatical utterances may receive high scores long don violate local constraints 
ffl predictors gram models defined ordinal place sentence linguistic role 
histories gold prices fell gold prices fell yesterday different trigram similar effect distribution word 
short term class history class gram parameter space spanned gram models significantly reduced reliability estimates consequently increased clustering words classes 
done different levels predictors may clustered may predicted word 
see bahl details 
decision components cluster nature extent clustering examples detail vs reliability tradeoff central modeling 
addition decide clustering 
general methods doing 
clustering linguistic knowledge jelinek merialdo 

clustering domain knowledge price 
smoothed unigram slightly higher cross entropy 
data driven clustering jelinek appendix jelinek appendix brown kneser ney suhm waibel 
see rosenfeld detailed exposition 
intermediate distance long distance grams attempt capture directly dependence predicted word grams distance back 
example distance trigram predicts gamma gamma 
special case distance grams familiar conventional grams 
huang attempted estimate amount information long distance bigrams 
long distance bigram constructed distance 
word brown corpus training data 
distance case control distance significant information expected 
bigram training set perplexity computed 
indication average mutual information word word gammad expected perplexity low increase significantly moved 

training set perplexity remained level see table 
concluded significant information exists words history 
distance pp table training set perplexity long distance bigrams various distances words brown corpus 
distance case included control 
long distance grams seriously deficient 
capture word sequence correlations sequences separated distance fail appropriately merge training instances different values unnecessarily fragment training data 
long distance triggers evidence long distance information evidence significant amount information longer distance history experiments long distance bigrams 
previous section discusses experiment long distance bigrams reported huang 
mentioned training set perplexity low conventional bigram increase significantly moved 

training set perplexity remained level 
interestingly level slightly consistently perplexity case see table 
concluded information exists distant past spread entire history 
shannon game ibm mercer roukos 
shannon game program implemented ibm person tries predict word document access entire history document 
performance humans compared trigram language model 
particular cases humans model examined 
cases predicted word word related occurred history document 
perplexity case 
see section 
concept trigger pair evidence chose trigger pair basic information bearing element extracting information long distance document history rosenfeld 
word sequence significantly correlated word sequence considered trigger pair trigger triggered sequence 
occurs document triggers causing probability estimate change 
trigger pairs selected inclusion model 
restrict attention trigger pairs single words number pairs large 
size vocabulary 
note bigram model number different consecutive word pairs number word pairs words occurred document significant fraction goal estimate probabilities form wjh 
interested correlations current word features history clarity exposition concentrate trigger relationships single words ideas carry longer sequences 
word 
define events ffi joint event space follows fw word 
ffi fw occurred document history considering particular trigger pair interested correlation event ffi event assess significance correlation ffi measuring cross product ratio 
significance extent correlation determining utility proposed trigger pair 
consider highly correlated trigger pair consisting rare words compare correlated common pair stock bond 
occurrence provides information occurrence stock bond 
occurrence test data expected benefit modeling occurrence stock 
stock common test data average utility may higher 
afford incorporate trigger pairs model stock bond may preferable 
measure expected benefit provided ffi predicting average mutual information see example abramson ffi ffi log bja ffi ffi log bja ffi ffi log bja ffi ffi log bja ffi related church hanks uses variant term equation automatically identify locational constraints 
detailed trigger relations trigger relations considered far trigger pair partitioned history classes trigger occurred occur call triggers binary 
wish model long distance relationships word sequences detail 
example wish consider far back history trigger occurred times occurred 
case example space possible histories partitioned 
classes corresponding particular number times trigger occurred 
equation modified measure amount information conveyed average way classification 
wsj corpus 
attempting design trigger model study long distance factors significant effects word probabilities 
obviously information gained simply knowing occurred 
significantly gained considering occurred times 
studied issues wall street journal corpus words 
index file created contained word record occurrences 
candidate pair words computed log cross product ratio average mutual information mi distance count occurrence statistics 
draw graphs depicting detailed trigger relations 
illustrations figs 

program manually browse shares shares st shares shares st probability shares function distance occurrence stock document 
middle horizontal line unconditional probability 
top bottom line probability shares stock occurred occur document 
hundreds trigger pairs able draw general 
different trigger pairs display different behavior modeled differently 
detailed modeling expected return higher 

self triggers triggers form particularly powerful robust 
fact thirds words highest mi trigger proved word 
words self trigger top triggers 

root triggers generally powerful depending frequency inflection 

potential triggers concentrated high frequency words 
stock bond useful 
winter winter summer winter summer probability winter function number times summer occurred document 
horizontal lines fig 

trigger triggered words different domains discourse trigger pair shows slight mutual information 
occurrence word stock signifies document probably concerned financial issues reducing probability words characteristic domains 
negative triggers principle exploited way regular positive triggers 
amount information provide typically small 
syntactic constraints syntactic constraints varied 
expressed decisions grammaticality cautiously scores low scores assigned ungrammatical utterances 
extraction syntactic information typically involve parser 
unfortunately parsing general english reasonable coverage currently attainable 
alternative phrase parsing 
possibility loose semantic parsing ward ward extracting syntactic semantic information 
information content syntactic constraints hard measure quantitatively 
beneficial 
knowledge source complementary statistical knowledge sources currently tame 
speech recognizer errors easily identified humans violate basic syntactic constraints 
combining information sources desired information sources identified phenomena modeled determined main issue needs addressed 
part document processed far word considered position different estimates wjh 
estimates derived different knowledge sources 
combine form optimal estimate 
discuss existing solutions section propose new 
linear interpolation models fp wjh combine linearly combined wjh def wjh 
method way combining knowledge sources way smoothing component models flat uniform distribution 
estimation maximization em type algorithm dempster typically determine weights 
result set weights provably optimal regard data optimization 
see jelinek mercer details rosenfeld exposition 
linear interpolation significant advantages method choice situations ffl linear interpolation extremely general 
language model component 
fact common set heldout data selected weight optimization component models need longer maintained explicitly 
represented terms probabilities assign heldout data 
model represented array probabilities 
em algorithm simply looks linear combination arrays minimize perplexity completely unaware origin 
ffl linear interpolation easy implement experiment analyze 
created interpolate program takes number probability streams optional bin partitioning stream runs em algorithm convergence see rosenfeld appendix 
program experiment different component models bin classification schemes 
general 
exact value weights significantly affect perplexity 
weights need specified accuracy 

little heldout data words weight arrive reasonable weights 
ffl linear interpolation hurt 
interpolated model guaranteed worse components 
components viewed special case interpolation weight component 
strictly speaking guaranteed heldout data new data 
heldout data set large result carry 
suspect new knowledge source contribute current model quickest way test build simple model uses source interpolate current 
new source useful simply assigned small weight em algorithm jelinek 
linear interpolation advantageous different information sources straightforward simple minded way 
simple source weaknesses ffl linearly interpolated models suboptimal components 
different consulted blindly regard strengths weaknesses particular contexts 
weights optimized globally locally bucketing scheme attempt remedy situation piece meal 
combined model optimal information disposal 
example section discussed huang reported significant amount information exists long distance bigrams distance 
tried incorporate information combining components linear interpolation 
combined model improved perplexity conventional distance bigram insignificant amount 
section see similar information source contribute significantly perplexity reduction provided better method combining evidence employed 
detailed example rosenfeld huang report early trigger models 
trigger utility measure closely related mutual information select triggers 
combined evidence multiple triggers variants linear interpolation interpolated result conventional backoff trigram 
example result table 
reduction perplexity true potential triggers demonstrated sections 
test set trigram pp trigram triggers pp improvement kw wsj table perplexity reduction linearly interpolating trigram trigger model 
see rosenfeld huang details 
ffl linearly interpolated models generally inconsistent components 
typically partitions event space provides estimates relative frequency training data class partition 
component models estimates consistent marginals training data 
reasonable measure consistency general violated interpolated model 
example bigram model partitions event space word history 
histories say bank associated estimate bigram wjh 
estimate consistent portion training data ends bank sense word training set ends bank bigram wjh bank bank training set count bigram bank 
bigram component linearly interpolated component different partitioning data combined model depends assigned weights 
weights turn optimized globally influenced marginals partitions 
result equation generally hold interpolated model 
backoff backoff method katz different information sources ranked order detail specificity 
runtime detailed model consulted 
contain information predicted word current context context exclusively generate estimate 
model line consulted 
previous case backoff way combining information sources way smoothing 
backoff method reconcile multiple models 
chooses 
problem approach exhibits discontinuity point backoff decision 
spite problem backing simple compact better linear interpolation 
problem common linear interpolation backoff give rise systematic overestimation events 
problem discussed solved rosenfeld huang solution speech recognition system chase 
maximum entropy principle section discuss alternative method combining knowledge sources maximum entropy approach proposed jaynes jaynes 
maximum entropy principle applied language modeling dellapietra 
methods described previous section knowledge source separately construct model models combined 
maximum entropy approach construct separate models 
builds single combined model attempts capture information provided various knowledge sources 
knowledge source gives rise set constraints imposed combined model 
constraints typically expressed terms marginal distributions example section 
solves inconsistency problem discussed section 
intersection constraints empty contains possibly infinite set probability functions consistent knowledge sources 
second step maximum entropy approach choose functions set function highest entropy function 
words desired knowledge sources incorporated features data assumed source 
worst remaining possibilities chosen 
illustrate ideas simple example 
example assume wish estimate bank jh probabilityof word bank document history 
estimate may provided conventional bigram 
bigram partition event space word history 
partition depicted graphically 
column equivalence class partition 
ends ends 
table event space partitioned bigram equivalence classes depicted columns 
class histories word 
consider equivalence class say history ends 
bigram assigns probability estimate events class bigram fthe estimate derived distribution training data class 
specifically derived fthe def bank estimate may provided particular trigger pair say loan bank 
assume want capture dependency bank loan occurred document 
different partition event space added 
rows equivalence class partition ends ends 
loan 
loan 
table event space independently partitioned binary trigger word loan set equivalence classes depicted rows 
similarly bigram case consider equivalence class say loan occur history 
trigger component assigns probability estimate events class loan bank loan hg estimate derived distribution training data class 
specifically derived loan hg def bank loan loan bigram component assigns estimate events column trigger component assigns estimate events row 
estimates clearly mutually inconsistent 
reconciled 
linear interpolation solves problem averaging answers 
backoff method solves choosing 
maximum entropy approach hand away inconsistency relaxing conditions imposed component sources 
consider bigram 
maximum entropy longer insist value fthe history ends 
acknowledge history may features affect probability bank 
require combined estimate equal fthe average training data 
equation replaced ends combined fthe stands expectation average 
note constraint expressed equation weaker expressed equation 
different functions combined satisfy 
degree freedom removed imposing new constraint remain 
equivalence classes depicted graphically rows columns clarity exposition 
reality need orthogonal 
similarly require combined equal loan hg average histories contain occurrences loan loan combined loan hg bigram case constraint weaker imposed equation 
tremendous number degrees freedom left model easy see intersection constraints non empty 
step maximum entropy approach find functions intersection highest entropy 
search carried implicitly described section 
information sources constraint functions generalizing example view information source defining subset subsets event space 
subset impose constraint combined estimate derived agree average certain statistic training data defined subset 
example subsets defined partition space statistic marginal distribution training data equivalence classes 
need case 
define subset event space desired expectation impose constraint subset specified index function called selector function def ae equation notation suggests generalization 
need restrict index functions 
real valued function 
call constraint function associated desired expectation 
equation hf pi generalized constraint suggests new interpretation hf pi expectation desired distribution 
require expectation functions ff match desired values fk 
respectively 
generalizations introduced extremely important mean correlation effect phenomenon described terms statistics readily incorporated maximum entropy model 
information sources described previous section fall category information sources described algorithm 
general description maximum entropy model solution 
maximum entropy generalized iterative scaling algorithm maximum entropy principle jaynes kullback stated follows 
reformulate different information sources constraints satisfied target combined estimate 

probability distributions satisfy constraints choose highest entropy 
general event space fxg derive combined probability function constraint associated constraint function desired expectation constraint written def consistent constraints unique solution guaranteed exist form unknown constants 
search exponential family defined satisfy constraints iterative algorithm generalized iterative scaling gis darroch ratcliff exists guaranteed converge solution 
gis starts arbitrary values define initial probability estimate def iteration creates new estimate improved sense matches constraints better predecessor 
iteration say consists steps 
compute expectations current estimate function 
compute def 

compare actual values desired values update formula delta 
define estimate function new def iterating continued convergence near convergence 
estimating conditional distributions generalized iterative scaling find estimate simple non conditional probability distribution event space 
language modeling need estimate conditional probabilities form wjh 
done 
simple way estimate joint conditional wjh readily derived 
tried moderate success lau 
reason event space size vocabulary size history length 
reasonable values huge space feasible amount training data sufficient train model 
better method proposed brown 
desired probability estimate empirical distribution training data 
constraint function desired expectation 
equation rewritten delta wjh delta modify constraint delta wjh delta possible interpretation modification follows 
constraining expectation regard constrain expectation regard different probability distribution say conditional wjh marginal better understand effect change define set possible histories define partition induced modification equivalent assuming constraint 
typically small set assumption reasonable 
significant benefits 
wjh wjh modeling feasible modeling minute fraction 

applying generalized iterative scaling algorithm longer need sum possible histories large space 
sum histories occur training data 

unique solution satisfies equations shown maximum likelihood ml solution function exponential family defined constraints maximum likelihood generating training data 
identity ml solutions apart aesthetically pleasing extremely useful estimating conditional wjh 
means hillclimbing methods conjunction generalized iterative scaling speed search 
likelihood objective function convex hillclimbing get stuck local minima 
maximum entropy minimum discrimination information principle maximum entropy viewed special case minimum discrimination information mdi principle 
prior probability function fq ff ff family probability functions ff varies set 
case maximum entropy fq ff ff defined intersection constraints 
wish find function family closest prior def arg min ff ff non symmetric distance measure kullback liebler distance known discrimination information asymmetric divergence kullback def log special case uniform distribution defined equation maximum entropy solution function highest entropy family fq ff ff see special case mdi distance measured uniform distribution 
precursor dellapietra history document construct unigram 
constrain marginals bigram 
static bigram prior mdi solution sought family defined constrained marginals 
assessing maximum entropy approach principle generalized iterative scaling algorithm important advantages 
principle simple intuitively appealing 
imposes constituent constraints assumes 
special case constraints derived marginal probabilities equivalent assuming lack higher order interactions 

extremely general 
probability estimate subset event space including estimates derived data inconsistent 
knowledge sources incorporated distance dependent correlations complicated higher order effects 
note constraints need independent uncorrelated 

information captured existing language models absorbed model 
document show done conventional gram model 

generalized iterative scaling lends incremental adaptation 
new constraints added time 
old constraints maintained allowed relax 

unique solution guaranteed exist consistent constraints 
generalized iterative scaling algorithm guaranteed converge 
approach weaknesses 
generalized iterative scaling computationally expensive problem methods coping see rosenfeld section 

algorithm guaranteed converge theoretical bound convergence rate systems tried convergence achieved iterations 

useful impose constraints satisfied training data 
example may choose turing discounting done constraints may derived data externally imposed 
circumstances equivalence maximum likelihood principle longer exists 
importantly constraints may longer consistent theoretical results guaranteeing existence uniqueness convergence may hold 
maximum entropy language modeling section describe maximum entropy framework create language model tightly integrates varied knowledge sources 
distance grams conventional formulation conventional formulation standard grams usual unigram bigram trigram maximum replaced unigram bigram trigram constraints information 
specifically constraint function unigram ae desired value set empirical expectation expectation training data def training associated constraint wjh denotes empirical distribution 
similarly constraint function bigram fw fw ae ends associated constraint wjh fw fw constraint function trigram fw fw ae ends associated constraint wjh fw fw complemented gram formulation constraint model induces subset event space modify gram constraints modifying respective subsets 
particular set subtraction operations performed 
modify bigram constraint exclude events part existing trigram constraint call complemented bigrams 

modify unigram constraint exclude events part existing bigram trigram constraint call complemented unigrams 
changes merely notational resulting model differs original significant ways 
applicable models 
fact applied conventional backoff model yielded modest reduction perplexity 
runtime backoff conditions better matched complemented events 
kneser ney similar observation motivate modification backoff scheme similar results 
purpose model important aspect complemented grams associated events overlap 
constraint active training datapoint 
turn results faster convergence generalized iterative scaling algorithm rosenfeld 
reason chosen complemented gram formulation 
triggers incorporating triggers formulate binary trigger pair constraint define constraint function ae set empirical expectation expectation training data 
impose desired probability estimate constraint wjh selecting trigger pairs section discussed mutual information measure utility trigger pair 
candidate trigger pair buenos aires proposed measure buenos ffi aires buenos ffi aires log ffi aires buenos ffi aires log ffi aires buenos ffi aires log ffi aires buenos ffi aires log ffi aires measure result high utility score case 
trigger pair really useful 
triggers addition grams 
trigger pairs useful extent information provide supplements information provided grams 
example aires predicted buenos bigram constraint 
possible fix modify mutual information measure factor triggering effects fall range grams 
gamma recall ffi def fa gamma context trigram constraints mi ffi mi ffi ffi def fa gamma designate measure mi 
wsj occurrence file described section possible ordered trigger pairs wsj word vocabulary filtered 
step word pairs occurred documents maintained 
resulted unordered pairs 
mi ffi computed pairs 
pairs bit average mutual information kept 
resulted ordered trigger pairs sorted mi separately random sample shown table 
larger sample provided rosenfeld appendix 
browsing complete list drawn harvest crop harvest corn soybean agriculture grain drought grains harvesting crop harvest forests farmers harvesting timber trees logging acres forest iran iranian iran lebanon hostages israeli hostage iraq persian terrorism arms israel terrorist hastings hastings judge trial district florida hate hate man love cuba castro fidel castro cuba communist miami revolution table best triggers words descending order measured mi ffi 

self triggers words trigger usually trigger pairs 
fact cases best predictor word word 
cases self trigger top predictors 

words stem predictors 

general great similarity stem words ffl strongest association nouns possessive triggers 
xyz 
xyz 
triggered words predictor sets xyz xyz similar 
ffl association nouns plurals 
ffl iran ian israel 

predictor sets similar preference self triggers predictor set biased predictor set biased predictor set biased 

preference frequent words expected mutual information measure 
mi measure optimal 
consider sentence district attorney office launched investigation loans connected banks 
mi measure may suggest attorney investigation pair 
model incorporating pair may attorney trigger investigation sentence raising probability default value rest document 
investigation occurs preceded launched allows trigram component predict higher probability 
raising probability investigation incurs cost justified example 
happens mi measures simple mutual information excess mutual information supplied grams 
similarly trigger pairs affect usefulness 
utility trigger pair diminished presence pair information provide overlap 
utility trigger pair depends way model 
mi fails consider factors 
optimal measure utility trigger pair procedure 
train model grams 

candidate trigger pair train special instance base model incorporates pair pair 

compute excess information provided pair comparing entropy predicting 

choose trigger pair maximizes excess information 

incorporate new trigger pairs vocabulary base model repeat step 
task large wsj words training data millions constraints approach clearly infeasible 
smaller tasks employed see example ratnaparkhi roukos 
simple system difficulty measuring true utility individual triggers means general directly compute information added system entropy reduced 
special circumstances may possible 
consider case unigram constraints single trigger provided word vocabulary 
crosstalk gram constraints trigger constraints trigger constraints possible calculate advance reduction perplexity due triggers 
verify theoretical arguments test code experiment conducted words wsj corpus language training data vocabulary see appendix 
model incorporating unigram constraints created 
training set perplexity pp exactly calculated simple maximum likelihood estimates 
word vocabulary best predictor measured standard mutual information chosen 
trigger pairs total mutual information bits 
argument training set perplexity model incorporating triggers delta gamma triggers added model generalized iterative scaling algorithm run 
produced output iteration training pp improvement complete agreement theoretical prediction 
model combining grams triggers major test applicability approach models constructed incorporated gram trigger constraints 
experiment run best triggers word judged mi criterion best triggers word 
gram trigger constraints constraints incorporated desired value constraint right hand side equations replaced turing discounted value better estimate true expectation constraint new data conventional backoff trigram model baseline 
maximum entropy models linearly interpolated conventional trigram weight model trigram 
words new data testing results summarized table 
vocabulary top words wsj corpus training set mw wsj test set kw wsj trigram perplexity baseline experiment top top constraints unigrams bigrams trigrams triggers perplexity perplexity reduction perplexity perplexity reduction table maximum entropy models incorporating gram trigger constraints 
interpolation trigram model done order test model fully retained information provided grams part lost trying incorporate trigger information 
interpolation reduced perplexity conclude gram information retained integrated model 
illustrates ability framework successfully accommodate multiple knowledge sources 
similarly little improvement triggers word vs triggers word 
little information left triggers exploited trigger pairs 
consequence suboptimal method selecting triggers see section 
triggers word highly correlated means information provide overlaps 
unfortunately mi measure discussed section fails account overlap 
baseline trigram model experiments reported compact backoff model trigrams occurring training set ignored 
modification standard arpa community results slight degradation perplexity case realizes significant savings memory requirements 
models described discarded information 
note modification invalidates maximumlikelihood principle discussed section 
furthermore constraints longer match marginals training data guaranteed consistent solution guaranteed exist 
intuition large number remaining degrees freedom practically guarantee solution proven case 
large test set ensure statistical significance results 
size perplexity half data set randomly selected perplexity set 
class triggers motivation section mentioned strong triggering relations exist different inflections stem similar triggering relation word 
reasonable hypothesize triggering relationship really stems inflections 
supported intuition observation triggers capture semantic correlations 
assume example stem loan triggers stem bank 
relationship hopefully capture unified way affect occurrence loan loans loan probability bank banks banking occurring 
noted class triggers merely notational shorthand 
wrote possible combinations word pairs lists result single class trigger 
class trigger training data word pairs clustered 
system better empirical question 
depends words behave similarly regard long distance prediction decided looking data 
constraints class trigger aa def fa 
subset vocabulary bb def fb 
subset 
constraint function class trigger aa bb aa bb ae aa bb set aa bb aa bb empirical expectation aa bb impose desired probability estimate constraint wjh aa bb aa bb clustering words class triggers writing constraints class triggers straightforward 
hard problem finding useful classes 
reminiscent case class grams 
general methods discussed section clustering linguistic knowledge clustering domain knowledge data driven clustering 
estimate potential class triggers chose methods 
choice strong conviction stem clustering certainly correct 
conviction supported observations section browsing best predictors list 
program developed carnegie mellon word vocabulary mapped stems 
mapping reversed create word clusters 
words formed clusters singletons 
words belonged cluster 
randomly selected sample shown table 
models trained 
included word self triggers word vocabulary 
second included class self triggers aa aa cluster aa 
threshold document occurrences types triggers 
models included unigram constraints threshold global occurrences 
unigram constraints facilitated quick estimation amount information triggers discussed section 
models trained words wsj text 
results summarized table 
grateful david evans steve henderson generosity providing tool accrual accrual accrue accrue accrued accruing accumulate accumulate accumulated accumulating accumulation accumulation accuracy accuracy accurate accurate accurately accusation accusation accusations accused accustomed ace ace achieve achieve achieved achieves achieving achievement achievement achievements acid acid table randomly selected set examples stem clustering morphological analysis provided program 
vocabulary top words wsj corpus training set kw wsj test set kw wsj unigram perplexity model word self triggers class self triggers constraints unigrams word self triggers class self triggers training set perplexity test set perplexity table word self triggers vs class self triggers presence unigram constraints 
stem clustering help 
surprisingly stem clustering resulted improvement test set perplexity context 
possible reason small amount training data may sufficient capture long distance correlations common members clusters 
experiment repeated time training words 
results summarized table disappointing 
class model slightly worse word difference appears insignificant 
stem clustering fail improve perplexity 
find satisfactory explanation 
possibility follows 
class triggers allegedly superior word triggers capture class cross word effects effect accused 
stem clusters consist common word frequent variants 
cases cluster cross word effects include rare words means impact small recall trigger pair utility depends frequency words 
vocabulary top words wsj corpus training set mw wsj test set kw wsj unigram perplexity model word self triggers class self triggers constraints unigrams word self triggers class self triggers training set perplexity test set perplexity table word self triggers vs class self triggers training data previous experiment table 
results disappointing 
long distance grams section showed quite bit information bigrams distance 
section reported unable benefit information linear interpolation 
maximum entropy approach possible better integrate knowledge 
long distance gram constraints long distance gram constraints incorporated formalism way conventional distance grams 
example constraint function distance bigram fw fw ae gamma gammaj associated constraint wjh fw fw fw expectation fw training data fw def training fw similarly trigram constraints similarly complemented grams section 
adding distance grams model model described section augmented include distance bigrams trigrams 
different systems trained different amounts training data words words words entire wsj corpus 
systems performance summarized table 
trigram model baseline described section 
training time reported alpha days amount computation done dec alpha workstation hours 
mw system different employed high thresholds cutoffs gram constraints distance bigrams trigrams included occurred times vocabulary top words wsj corpus test set kw training set mw mw mw trigram perplexity baseline constraints unigrams bigrams trigrams distance bigrams distance trigrams word triggers max word training time alpha days test set perplexity perplexity reduction table maximum entropy model incorporating gram distance gram trigger constraints 
mw system far fewer parameters baseline employed high gram thresholds reduce training time 
training data 
distance bigrams trigrams included occurred times training data 
done reduce computational load quite severe system size 
cutoffs conventional grams higher applied distance grams anticipated information lost knowledge source re introduced partially interpolation conventional trigram model 
actual values cutoffs chosen possible finish computation weeks 
observed maximum entropy model significantly better trigram model 
relative advantage greater training data 
large mw system practical consideration required imposing high cutoffs model perplexity significantly better baseline 
particularly notable model uses third number parameters trigram model vs 
assess relative contribution various information sources employed experiments maximum entropy models constructed various subsets sources mw system 
information source type number constraints table 
results summarized table 
vocabulary top words wsj corpus training set mw test set kw perplexity change trigram baseline models dist grams dist grams dist grams word triggers dist grams dist grams word triggers table perplexity maximum entropy models various subsets information sources table 
mw training data information provided distance grams largely overlapped provided triggers 
notable result mw system distance grams reduce perplexity added trigger constraints 
information distance grams appears largely overlapped provided triggers 
contrast distance grams resulted additional perplexity reduction mw system see tables 
maximum entropy knowledge integrator experiments reported clearly demonstrate ability significantly improve baseline trigram integrating conventional grams distance grams long distance triggers log linear model maximum entropy principle 
reduction perplexity due approach opposed arising alternative knowledge sources 
improvement achieved integrating knowledge sources different computationally intensive way 
section discussed earlier attempts 
linear interpolation combine conventional gram long distance grams distance 
gram component models trained data words interpolation weights optimized heldout data 
resulted consistently trained model 
perplexity reduced baseline compared reduction table 
second attempt rosenfeld huang combined evidence multiple triggers variants linear interpolation interpolated result conventional backoff trigram 
resulted reduction perplexity compared respective reduction framework 
admittedly comparison previous interactions various triggers consistently trained linear interpolation model triggers 
clear triggers interaction modeled consistently exponential growth number parameters 
case serves highlight biggest advantages method facilitates consistent straightforward incorporation diverse knowledge sources 
adaptation language modeling adaptation vs long distance modeling grew desire improve conventional trigram language model extracting information document history 
approach termed long distance modeling 
trigger pair chosen basic information bearing element purpose 
triggers viewed vehicles adaptation 
topic discourse known triggers capture convey semantic content document adjust language model better anticipates words domain 
models discussed far considered adaptive 
duality long distance modeling adaptive modeling quite strong 
clear distinction 
extreme trigger model history current document viewed static non adaptive probability function domain entire document history 
extreme trigram model viewed bigram adapted step penultimate word history 
fortunately type distinction important 
meaningful classification nature language source relationship training test data 
section propose classification study adaptive capabilities maximum entropy modeling techniques 
paradigms adaptation adaptation discussed far kind call domain adaptation 
paradigm heterogeneous language source wsj treated complex product multiple domains discourse sublanguages 
goal produce continuously modified model tracks sublanguage mixtures sublanguage shifts style shifts contrast cross domain adaptation paradigm test data comes source language model exposed 
salient aspect case large number vocabulary words test data high proportion new bigrams trigrams 
cross domain adaptation important cases data test domain available training system 
practice rarely happens 
limited amount training data obtained 
hybrid paradigm limited data domain adaptation important real world applications 
domain adaptation maximum entropy models naturally suited domain adaptation constraints typically derived training data 
model integrates constraints making assumption phenomena hold test data 
assumption limitation 
triggers selected mutual self triggers particularly prevalent strong see section 
true common moderately common words 
reasonable assume holds rare words 
unfortunately maximum entropy triggers described capture self correlations represented training data 
long amount training data finite self correlation rare words exceed threshold 
capture effects model supplemented rare words unigram cache described subsection 
source adaptive information self correlations word sequences 
principle captured appropriate constraint functions describing trigger relations word sequences 
implementation triggers limited single word triggers 
capture correlations conditional bigram trigram caches added described subsequently 
gram caches kuhn kupiec 
kuhn de mori kuhn de mori employed pos bigram cache improve performance static bigram 
jelinek incorporated trigram cache speech recognizer reported reduced error rates 
selective unigram cache conventional document unigram cache words occurred history document stored dynamically generate unigram turn combined language model components 
motivation unigram cache word occurs document probability re occurring typically greatly elevated 
extent phenomenon depends prior frequency word pronounced rare words 
occurrence common word provides little new information 
put way occurrence rare word surprising provides information occurrence common word deviates expectations static model requires smaller modification 
bayesian methods may optimally combine prior word new evidence provided occurrence 
rough approximation selective unigram cache implemented rare words stored cache 
word defined rare relative threshold static unigram frequency 
exact value threshold determined optimizing perplexity unseen data 
wsj corpus optimal threshold range gamma gamma significant differences range 
modified cross domain adaptation 
see subsection 
scheme proved useful perplexity reduction conventional cache 
especially true cache combined model captures self correlations common words see previous section 
conditional bigram trigram caches document bigram cache consecutive word pairs occurred history document stored dynamically generate bigram turn combined language model components 
trigram cache similar consecutive word triples 
alternative way viewing bigram cache set unigram caches word history 
unigram consulted time depending identity word history 
viewed way clear bigram cache contribute combined model word history non selective unigram cache hit 
cases uniform distribution bigram cache serve flatten degrade combined estimate 
chose conditional bigram cache non zero weight hit 
similar argument applied trigram cache 
cache consulted words history occurred trigram cache contribute immediately bigram cache hit 
experimentation trigram cache constructed similarly conditional bigram cache revealed contributed little perplexity reduction 
expected bigram cache hit unigram cache hit 
trigram cache refine distinctions provided bigram cache 
document history typically small words average wsj corpus 
modest cache refinement provided trigram small statistically unreliable 
way viewing selective bigram trigram caches regular non selective caches interpolated weights depend count context 
zero context counts force respective zero weights 
combining components maximize adaptive performance maximum entropy model supplemented unigram bigram caches described 
conventional trigram baseline added 
especially important mw system employed high cutoffs gram constraints 
cutoffs effectively model blind information gram events occurred fewer times 
conventional trigram reintroduced information 
combined model achieved consulting appropriate subset models 
time component models combined linearly 
weights fixed follow linear pattern time 
maximum entropy model incorporated information trigger pairs relative weight increased length history 
incorporated new information distance grams useful document weight start zero 
maximum entropy model started weight gradually increased words document 
conventional trigram started weight decreased concurrently 
conditional bigram cache non zero weight cache hit allowed relatively high weight 
selective unigram cache weight proportional size cache saturating 
formula min delta jhj trigram max gamma delta jhj unigram cache min delta unigram bigram cache ae word occurred earlier threshold words enter selective unigram cache static unigram probabilityof 
weights normalized sum 
general weighting scheme chosen considerations discussed specific values weights chosen minimizing perplexity unseen data 
results analysis table summarizes perplexity pp performance various maximum entropy model unigram bigram caches follows vocabulary top words wsj corpus test set kw training set mw mw mw pp change pp change pp change trigram baseline trigram caches maximum entropy trigram trigram caches table best domain adaptation perplexity pp results 
note adaptive model trained words baseline model trained words 
trigram static perplexity serves baseline 
trigram caches experiments represent best adaptation achievable maximum entropy formalism non selective unigram cache results slightly higher perplexity 
note improvement due caches greater data 
explained follows amount information provided caches independent amount training data fixed systems 
mw system higher perplexity relative improvement provided caches greater 
put way models data better harder improve 
maximum entropy numbers reproduced table 
relative advantage pure maximum entropy model greater training data mw system penalized high cutoffs 
uses constraint functions capture correlations training data 
data gram trigger correlations exist statistically reliable constraints employed 
true regard conventional grams baseline trigram model 
difference number distance grams trigger pairs 
trigram maximum entropy model interpolated conventional trigram significant perplexity reduction occurs mw system 
mw model employed high gram cutoffs blind low count gram events 
interpolation conventional trigram reintroduced information optimal form linear interpolation suboptimal distance grams 
trigram caches experiments represent best adaptive scheme achieved 
improvement due caches smaller data 
compared trigram caches experiment addition component improves perplexity relative mw system relative mw mw systems 
illustrate success domain adaptation scheme note best adaptive model trained words better baseline model trained words best adaptive model trained words baseline model trained words 
particularly noteworthy amount training data available various domains limited 
cases adaptation provides handy compensation 
cross domain adaptation need cross domain adaptation cross domain adaptation paradigm training test data assumed come different sources 
happens result significant degradation language modeling quality 
apart language sources bigger degradation 
effect quite strong sources supposedly similar 
consider example table 
training data consists articles wall street journal 
test data ap wire stories period 
sources considered similar especially relative sources technical literature fine literature broadcast 
perplexity ap data twice wsj data 
vocabulary top words wsj corpus training set wsj mw test set wsj kw ap kw oov rate trigram hit rate trigram perplexity table degradation quality language modeling test data different domain training data 
trigram hit ratio relative compact trigram related phenomenon cross domain modeling increased rate vocabulary words 
wsj ap example cross domain oov rate double domain rate 
similarly rate new bigrams trigrams increases reported complement measure trigram hit rate relative compact trigram training set singletons excluded 
phenomena follows relative importance caches greater cross domain adaptation 
rely correlations training data correlations assumed universal self correlations 
table shows improvement achieved model interpolated model cross domain paradigm 
predicted contribution component slightly smaller domain case contribution caches greater 
note triggers adaptation triggers generally suitable domain adaptation rely training set correlations 
class triggers cross domain adaptation 
possible correlations classes similar training testing domains 
membership classes modified better match test domain 
example 
may trigger pair data 
iraq may useful 

region adjusted appropriately 
construct gram constraints rudnicky 
automatically defining useful concepts region course difficult open problem 
limited data domain adaptation limited data domain adaptation paradigm moderate amounts training data available test domain 
larger amounts data may available outside domains 
situation vocabulary top words wsj corpus training data mw wsj test data kw ap trigram baseline perplexity maximum entropy perplexity perplexity reduction trigram caches perplexity perplexity reduction table perplexity improvement maximum entropy interpolated adaptive models adaptation paradigm 
compared domain adaptation experiment impact component slightly smaller caches greater 
encountered real world applications 
best integrate detailed knowledge outside domain detailed knowledge test domain open question 
form interpolation reasonable 
ideas pursued rudnicky 
establish baseline 
model information come outside domain wsj list triggers 
list models reported 
training including training triggers done words ap wire data 
table shows results 
compared domain case impact component somewhat diminished strong 
vocabulary top words wsj corpus trigger derivation data mw wsj training data mw ap test data kw ap trigram baseline perplexity maximum entropy perplexity perplexity reduction trigram caches perplexity perplexity reduction table perplexity improvement maximum entropy interpolated adaptive models domain adaptation paradigm 
compared domain case impact component somewhat diminished 
adaptive modeling speech recognition accuracy prominent language modeling automatic speech recognition 
section report effect improved models performance sphinx ii carnegie mellon speech recognition system 
detailed exposition including discussion lm interface issues rosenfeld chapter 
domain adaptation evaluate recognition error rate reduction domain adaptation paradigm arpa csr continuous speech recognition evaluation set november kubala pallet hwang 
consisted utterances produced context complete long documents male female speakers 
version sphinx ii huang experiment gender dependent acoustic models see huang 
addition words standard wsj lexicon vocabulary words correct phonetic transcriptions added order create closed vocabulary conditions 
forward backward passes sphinx ii run create word lattices independent best passes 
pass mw static trigram language model served baseline 
passes interpolated adaptive language model words training data 
adaptive runs unsupervised word word adaptation recognizer output update language model 
run supervised adaptation recognizer output sentence adaptation correct sentence transcription sentence adaptation 
results summarized table 
language model word error rate change trigram baseline unsupervised adaptation supervised adaptation table word error rate reduction adaptive language model conventional trigram model 
cross domain adaptation test error rate reduction cross domain adaptation paradigm cross domain system reported section 
sentences recorded male female speakers test data 
results reported table 
expected perplexity experiments relative improvement smaller achieved domain adaptation paradigm 
training data mw wsj test data sentences ap language model word error rate change trigram baseline supervised adaptation table word error rate reduction adaptive language model conventional trigram model cross domain adaptation paradigm 
detailed discussion recognition experiments see rosenfeld 
perplexity recognition error rate adaptive language model trained full wsj corpus words reduced perplexity baseline trigram 
associated reduction recognition word error rate favorable circumstances 
conform empirically observed square root law states improvement error rate approximately square root improvement perplexity 
impact error rate greater 
perplexity take account acoustic pay special attention outliers tails distribution recognition errors occur 
addition deficiencies factor blame 
language model affects recognition error rate discriminative power ability assign higher scores hypotheses lower scores 
perplexity affected scores assigned language model hypotheses part test set typically consists true sentences 
language model overestimates probabilities hypotheses directly penalized perplexity 
penalty indirect assigning high probabilities hypotheses means commensurate reduction total probability assigned hypotheses 
overestimation confined small portion probability space effect perplexity negligible 
model give rise significant recognition errors high scores assign hypotheses may cause selected recognizer 
am grateful peter brown stephen della pietra vincent della pietra bob mercer salim roukos introducing maximum entropy principle encouraging latest developments raymond lau salim roukos collaboration implementation training procedure david evans steve henderson providing program speech group carnegie mellon varied logistical help raj reddy huang appreciated support encouragement 
am grateful anonymous reviewers useful comments suggestions 
research supported department navy naval research laboratory 

views contained document author interpreted representing official policies expressed implied government 
arpa wsj language corpus arpa csr wall street journal corpus consists articles published wall street journal december november 
original data obtained conditioned processed linguistic research association computational linguistics data collection initiative acl dci 
corpus chosen arpa speech recognition community basis csr continuous speech recognition common evaluation project 
subsequently data processed doug paul mit lincoln labs paul baker conditioned speech recognition 
included transforming common text constructs way said read aloud transformed dollars cents quality filtering preparation various standard vocabularies 
refer data set wsj corpus 
version corpus experiments described punctuation marks assumed verbalized removed data 
known non verbalized punctuation condition 
form wsj corpus contained words 
experiments stated vocabulary derived frequent non vp words data 
includes words occurred times corpus occurred times 
words mapped unique symbol unk part vocabulary frequency 
pseudo word added vocabulary designate sentence 
pseudo word designate sentence part vocabulary 
top bottom vocabulary order descending frequency words count corpus said dollars 
arrow arduous annapolis anarchy alterations aggravate agendas adage acquainted accredited accelerator wimp waist fraction wsj corpus paragraph units set aside acoustic training system development evaluation 
rest data designated language model development arpa sites 
consisted words 
set set aside words language model testing taken separate time periods global time period july january february 
remaining data words large models 
smaller models trained appropriate subsets 
language training set statistics ffl article 
ffl paragraphs 
ffl sentences sentences paragraph average 
ffl words words article average 
data behaved extremes ffl maximum number paragraphs article 
ffl maximum number sentences paragraph 
ffl maximum number words sentence 
ffl maximum number words paragraph 
ffl maximum number words article 
bigrams occurred times corpus dollars nineteen frequent trigram training data occurred times 
abramson norman abramson 
information theory coding mcgraw hill new york 
bahl bahl fred jelinek robert mercer 
likelihood approach continuous speech recognition 
ieee transactions pattern analysis machine intelligence volume pami number pages march 
brown peter brown vincent della pietra peter desouza lai robert mercer 
class gram models natural language 
proceedings ibm natural language itl march 
paris france 
brown peter brown stephen dellapietra vincent dellapietra robert mercer arthur nadas salim roukos 
maximum penalized entropy construction conditional log linear language translation models learned features generalized csiszar algorithm 
unpublished ibm research report 
chase lin chase ron rosenfeld wayne ward error responsive modifications speech recognizers negative grams proc 
international conference spoken language processing yokohama japan september 
church hanks ken church patrick hanks 
word association norms mutual information lexicography 
computational linguistics volume number pages march 
cover king thomas cover roger king 
convergent gambling estimate entropy english 
ieee transactions volume number pages july 
darroch ratcliff darroch ratcliff 
generalized iterative scaling log linear models 
annals mathematical statistics volume pages 
dellapietra stephen della pietra vincent della pietra robert mercer salim roukos 
adaptive language modeling minimum discriminant estimation 
proceedings international conference acoustics speech signal processing pages san francisco march 
published proceedings darpa workshop speech natural language morgan kaufmann pages february 
dempster dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society volume number pages 
merialdo anne marie bernard merialdo 
natural language modeling phoneme text transcription 
ieee transactions pattern analysis machine translation volume pami number pages november 

population frequencies species estimation population parameters 
biometrika volume parts pages 

maximum entropy hypothesis formulation especially multidimensional contingency tables 
annals mathematical statistics volume pages 
huang fileno hsiao mei kai fu lee ronald rosenfeld 
sphinx ii speech recognition system overview 
computer speech language volume pages 
huang huang fileno mei hwang ronald rosenfeld 
overview sphinx ii speech recognition system 
proceedings arpa human language technology workshop published human language technology pages 
morgan kaufmann march 
hwang mei hwang ronald rosenfeld eric thayer ravi lin chase robert weide fil 
improved acoustic adaptive language models recognition 
proceedings arpa spoken language technologies workshop march 
jaynes jaynes 
statistical mechanics 
physics reviews pages 
jelinek fred jelinek 
self organized language modeling speech recognition 
readings speech recognition alex waibel kai fu lee editors 
morgan kaufmann 
jelinek fred jelinek 
trigrams 
eurospeech 
jelinek fred jelinek robert mercer bahl james baker 
perplexity measure difficulty speech recognition tasks 
th meeting acoustic society america miami beach florida december 
jelinek mercer fred jelinek robert mercer 
interpolated estimation markov source parameters sparse data 
pattern recognition practice gelsema kanal editors pages 
north holland amsterdam 
jelinek jelinek merialdo roukos strauss 
dynamic language model speech recognition 
proceedings darpa workshop speech natural language pages february 
katz slava katz 
estimation probabilities sparse data language model component speech recognizer 
ieee transactions acoustics speech signal processing volume assp pages march 
kneser ney reinhard kneser hermann ney 
forming word classes statistical clustering statistical language modeling 
proceedings st conference trier germany september 
kneser ney reinhard kneser hermann ney 
improved smoothing gram language modeling 
proceedings international conference acoustics speech signal processing detroit mi may 
kubala francis kubala members csr corpus coordinating committee cccc 
hub spoke paradigm csr evaluation 
proceedings arpa workshop human language technology pages march 
morgan kaufmann 
kuhn roland kuhn 
speech recognition frequency words modified markov model natural language 
th international conference coling pages budapest august 
kuhn de mori roland kuhn renato de mori 
cache natural language model speech recognition 
ieee transactions pattern analysis machine intelligence volume pami number pages june 
kuhn de mori renato de mori 
correction cache natural language model speech recognition 
ieee transactions pattern analysis machine intelligence volume pami number pages june 
kullback kullback 
information theory statistics 
wiley new york 
kupiec kupiec 
probabilistic models short long distance word dependencies running text 
proceedings darpa workshop speech natural language pages february 
lau raymond lau 
maximum likelihood maximum entropy trigger language model 
bachelor thesis massachusetts institute technology may 
lau raymond lau ronald rosenfeld salim roukos 
trigger language models maximum entropy approach 
proceedings international conference acoustics speech signal processing pages ii minneapolis mn april 
lau raymond lau ronald rosenfeld salim roukos 
adaptive language modeling maximum entropy principle 
proceedings arpa human language technology workshop published human language technology pages 
morgan kaufmann march 
mercer roukos robert mercer salim roukos 
personal communication 

pallet pallett fiscus fisher garofolo lund 
benchmark tests arpa spoken language program 
proceedings arpa workshop human language technology pages march 
morgan kaufmann 
paul baker doug paul janet baker 
design wall street journal csr corpus 
proceedings darpa sls workshop february 
price patti price 
evaluation spoken language systems atis domain 
proceedings third darpa speech natural language workshop richard stern editor morgan kaufmann june 
ratnaparkhi roukos ratnaparkhi roukos 
maximum entropy model prepositional phrase attachment 
proceedings arpa workshop human language technology pages march 
morgan kaufmann 
rosenfeld ronald rosenfeld adaptive statistical language modeling maximum entropy approach 
ph thesis proposal carnegie mellon university september 
rosenfeld ronald rosenfeld hybrid approach adaptive statistical language modeling 
proceedings arpa workshop human language technology pages march 
morgan kaufmann 
rosenfeld ronald rosenfeld adaptive statistical language modeling maximum entropy approach 
ph thesis carnegie mellon university april 
published technical report school computer science carnegie mellon university pittsburgh pa april 
rosenfeld huang ronald rosenfeld huang 
improvements stochastic language modeling 
proceedings darpa workshop speech natural language published morgan kaufmann pages february 
rudnicky alexander rudnicky 
personal communication 

shannon shannon 
mathematical theory communication 
bell systems technical journal volume pages part pages part ii 
shannon shannon 
prediction entropy printed english 
bell systems technical journal volume pages 
suhm waibel bernhard suhm alex waibel 
better language models spontaneous speech 
icslp yokohama vol 
pp 

ward wayne ward 
cmu air travel information service understanding spontaneous speech 
proceedings darpa speech natural language workshop pages june 
ward wayne ward 
evaluation cmu atis system 
proceedings darpa speech natural language workshop pages february 

