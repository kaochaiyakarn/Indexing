th int 
joint conf 
artificial intelligence nagoya japan august pp 
fl learning coordinate controllers reinforcement learning control basis manfred huber grupen department computer science university massachusetts amherst ma autonomous robot systems operating uncertain environment reactive adaptive order cope changing environment conditions task requirements 
achieve hybrid control architecture uses reinforcement learning top discrete event dynamic system deds framework learn supervise set basis controllers order achieve task 
system model automatically derived supervisor reduces complexity learning problem 
addition safety constraints may imposed priori system learns line single trial need outside teacher 
demonstrate applicability approach architecture learn turning gait legged robot platform 
autonomous robot systems operating uncertain environment able cope new situations task requirements 
important properties control architecture systems reactive allows flexible responses novel situations adapts longer lasting changes environment task requirements 
model control techniques successfully wide variety tasks sensitive imprecisions model robust respect unexpected situations 
better address reactivity requirements autonomous systems behavior architectures brooks developed 
paradigm system behavior constructed line combinations elemental reactive behaviors 
ad hoc character supported part nsf iri behaviors lead extremely complex organization behavioral elements 
addition resulting policy brittle respect relatively minor perturbations including introduced behaviors changes control context 
control basis approach attempts circumvent problem employing carefully designed declarative control primitives construct system behavior allows predictions outcome behavioral sequences 
approach successfully manipulation locomotion tasks grupen huber bottom approaches address issue reactivity composition cases designer specific task hand 
order render system adaptive allow adjust behavior changing task requirements learning techniques employed 
extreme case learning occur direct influence outside teacher order obtain autonomous behavior 
reinforcement learning techniques suited behavior composition tasks learn sequences behavior simple reinforcement signals 
applications techniques applied low level leading high complexity learning task 
complexity rendered approaches inadequate line learning complex systems 
address complexity problems provide base reactivity system done combine learning framework robustness behavior control approaches maes brooks mahadevan connell problem decomposed priori subproblems learned previously designed behaviors elemental actions reinforcement learning task 
dramatically reduces complexity state action spaces learning problem character behaviors restricts applicability limited domain potentially requiring design new components task changes 
addition approaches address safety considerations allowing exploration take random actions permitting occurrence catastrophic failures 
autonomous systems permissible system recover 
necessary systems learn single trial need outside supervision 
way address parametric controller inherently safe basis learning task singh learning component learns setting parameters controller assures baseline performance 
single controller increases designer effort limits scope system 
approach addresses complexity safety issues means hybrid continuous discrete control architecture 
behavior constructed line set stable convergent control elements 
stable character base controllers discrete event dynamic system deds framework construct supervisor provides structure reinforcement learning component 
dramatically reduces complexity learning problem reducing state action spaces supports techniques designed limit exploration safe relevant areas behavior space 
system inherits reactivity stability underlying control basis 
section briefly introduces control basis approach section describes discrete event architecture automatically synthesize admissible control policies reinforcement learning technique acquiring policy task 
section shows example architecture walking domain turning gait learned line single trial 
control basis approach control basis approach constructs behavior online combining feedback control elements drawn set carefully designed basis controllers 
individual control elements largely task device independent represent solutions generic robot control problems allowing small set elements span large range tasks wide variety platforms 
framework control derived line associating input resources oe sensors sensor abstractions output resources actuators feedback control laws phi drawn control basis 
resulting controllers phi oe activated concurrently task dependent composition policy subject delta constraint 
constraint restricts control actions subordinate controllers counteract objectives higher priority controllers 
resulting concurrent control policy inherits stability convergence properties elemental control elements 
complete control policy takes form sequence concurrent controller activations form shown 
different tasks framework achieved changing composition policies set controllers designing new control elements 
control basis composition policy 


controller binding control composition output resources input resources control composition approach successfully variety tasks manipulation locomotion domain grupen huber cases composition policies hand crafted requiring system designer anticipate exact behavior controllers 
achieve autonomous behavior robot system system able adapt novel situations task contingencies need outside supervision 
order achieve architecture learns optimal composition policy efficient way reinforcement learning paradigm 
composition architecture reinforcement learning barto offers flexible way acquire control strategies automatically adapt new contingencies task requirements 
reinforcement learning systems operate low level directly influencing actuator commands easily leading explosion complexity learning task 
renders approaches impractical line learning complex systems 
avoid problem able prevent catastrophic failures architecture attempts learn composition policy underlying controllers 
uses goal directed character control modules described section build description system influence basis controllers 
running controllers convergence possible behavior system modeled deds symbolic predicate space characterized individual goals control modules 
system model basis exploration learning system acquire optimal control strategy task 
deds formalism encodes safety constraints model limits exploration space admissible control policies 
architecture shown 
reinforcement learning state information transition probabilities control policy reinforcement symbolic events control activation physical sensors physical actuators continuous control modules event generators deds supervisor control architecture approach continuous sensor input actuator output handled elements control basis 
activation convergence controllers interpreted events deds model system behavior 
imposing safety domain constraints deds supervisor represents set admissible control policies 
exploration separate system identification phase additional transition probabilities model acquired improving quality largely task independent model 
model allows knowledge system behavior generalized tasks supports additional line learning estimated system model sutton addition acquiring intrinsic structure form transition probabilities hybrid architecture solves resource allocation problem learning assign resources controllers optimally current reinforcement structure 
deds supervisor feedback control primitives employed elemental actions approach act stable attractors form basins attraction continuous physical space 
system may described level means convergence predicates associated underlying controllers 
abstraction continuous state space discrete predicate space represents dramatic reduction complexity forms basis reinforcement learning task 
activation convergence determine progress plant space system modeled hybrid deds opening derivation possible supervisor large body formal techniques ramadge wonham particular constraints safety absence deadlock conditions imposed priori control policy 
predicate space description order allow abstraction step provide efficient way deriving model possible system behavior effects interactions control primitives described symbolically possible effects set predicates 
characterizations composite controllers generated automatically descriptions individual elements control basis limiting designer control elements 
opposed deds approaches designer provide complete system model simple descriptions allow automatic generation predicate space model possible system behavior details controller characterization construction system model see huber grupen 
model takes form nondeterministic finite state automaton forms basis supervisor synthesis reinforcement learning components proposed architecture 
supervisor synthesis deds framework control performed enabling disabling controllable events supervisor 
case architecture set events corresponds controller activations convergence events controllable 
allow safety chosen control policy ensure deadlock occurs deds formalism provides methods automatically impose constraints supervisory automaton 
doing system model pruned priori set policies obey set constraints ramadge wonham huber grupen fashion additional domain knowledge designer preferences incorporated control system reducing complexity reinforcement learning component 
learning system operation discrete supervisor limit exploration admissible parts behavior space activate controllers policy selected learning system 
learning component reinforcement learning provides mechanism adapting changing environments tasks external supervision mahadevan connell exploration paradigm system learns control policy maximizes amount reinforcement receives 
major drawback scheme inherent complexity need exploration order find better policies 
architecture problems addressed learning controller activations top predicate space model defined deds supervisor reducing size action state spaces considered learning task enforcing safety constraints exploration 
addition facilitates acquisition transition probabilities predicate states allows run time experience improve predictive power model 
learning component employs qlearning watkins widely temporal difference method learns value function state action pairs order represent quality action 
estimates discounted payoff action fl max computed immediate reinforcement obtained time step fl discount factor 
function control policy action highest value current state 
adapt finite state transition model underlying state space description learning value function represented distributed fashion probability controller state lead state represents value corresponding transition 
learning estimate updated gamma fi gamma fi fl max gamma fi learning rate 
time frequency counts determine transition probabilities nondeterministic supervisor 
allows architecture adapt efficiently changing task requirements line acquiring correct control policies 
locomotion example demonstrate applicability proposed architecture implemented legged twelve degree freedom walking robot 
objective acquire useful policies turning gaits 
shown hand crafted control gaits tasks derived huber example uses proposed architecture learn solutions autonomously minimal external supervision 
robot put initial stable configuration surface learning process initiated input outside teacher 
control basis deds supervisor control basis tasks successfully manipulation locomotion tasks grupen huber consists solutions generic robotics problems phi configuration space motion control phi contact configuration control phi kinematic conditioning 
basis control laws bound subsets system resources legs position orientation center mass shown 
details control basis resource bindings see huber input output resources contact controller posture controller path controller control basis controller resource notation example set possible controllers controller bindings limited order allow concise notation predicate space model 
set controller resource combinations available system consists instances contact configuration controller form phi legs robot instance kinematic conditioning controller phi set elemental candidate controllers delta constraint construct composite controllers allowing total composite controllers actions deds learning components 
addition choice candidate controllers limits predicate space predicates corresponding convergence controller input binding pair way phi phi phi phi phi wildcard indicates independence predicate evaluation output resource 
automatically constructing graph possible system behavior space safety constraint walking platform stable imposed supervisor 
terms predicates implies stance stable words evaluate true 
furthermore knowledge platform introduced form domain constraints 
legged platform employed example kinematic limitations allow simultaneous stability opposing support triangles 
adding knowledge reduces size supervisor shown deds supervisor rotation task numbers states represent values predicates 
noted purpose illustration complete supervisor built priori example 
general done incrementally course exploration violation constraints 
learning results supervisor derived control basis represents admissible control policies 
express task objectives optimal control policy 
order learn counterclockwise rotation task reinforcement schedule rewards system performs task correctly 
reinforcement example control action led counterclockwise rotation gamma led clockwise rotation 
robot system put flat surface learning process started 
experiments form performed order investigate performance control learning components 
trials system rapidly acquired correct gait pattern exploration slowly decreased 
minimum level random actions maintained introduce perturbations learn robust policy 
depicts typical learning profile task 
control steps learning curve counterclockwise task learning curve shows correct turning gait learned approximately learning steps 
entire learning process real robot took approximately minutes robot platform entered unsafe situations due limitations imposed deds supervisor 
final control policy shown 
graph shows possible transitions occur learned policy 
core policy cycle indicated bold transition arrows corresponds stable turning gait 
transition probabilities cycle making stable attractor policy 
core corresponding control actions indicated bottom 
learned control actions states attempt lead system stable cycle making policy robust respect perturbations 
learned control policy counterclockwise rotation task reactive adaptive control architectures autonomous systems pose challenges due complexity task limited amount supervision possible 
architecture employs hybrid control architecture reinforcement learning component order address issues 
continuous reactive control derived carefully designed control basis composition policy learned top predicate space deds framework 
allows imposition safety constraints permits new tasks learned online single trial need external teacher 
addition dramatically reduces complexity action state space making learning feasible complex tasks platforms 
barto barto bradtke singh 
learning act real time dynamic programming 
technical report dept univ mass amherst ma 
brooks brooks 
robust layered control system mobile robot 
ieee journal robotics automation march 
grupen grupen huber coelho jr 
distributed control representation manipulation tasks 
ieee expert april 
huber grupen huber grupen 
hybrid discrete event dynamic systems approach robot control 
technical report dept univ mass amherst ma oct 
huber huber macdonald grupen 
control basis walking 
proc 
int 
conf 
robotics automat pages vol minneapolis mn april 

application discrete event systems modeling controlling robotic agents 
proc 
int 
conf 
robotics automat pages san diego ca may 
maes brooks maes brooks 
learning coordinate behaviors 
proceedings aaai conference artificial intelligence 

mahadevan connell mahadevan connell 
automatic programming behavior robots reinforcement learning 
artificial intelligence 
ramadge wonham ramadge wonham 
control discrete event systems 
proceedings ieee jan 
singh singh connolly grupen barto 
robust reinforcement learning motion planning 
advances neural information processing systems nips 
owen 
subject indexed bibliography discrete event dynamic systems 
ieee robotics automation magazine 
antsaklis 
logical approach design hybrid systems 
mathematical computer modelling 
sutton sutton 
results dyna integrated architecture learning planning reacting 
thomas miller iii richard sutton paul werbos editors neural networks control pages 
mit press 
watkins watkins 
learning delayed rewards 
phd thesis cambridge university cambridge england 
