web prefetching partial match prediction alberto mendelzon cs toronto edu mendel cs toronto edu department computer science university toronto king college road toronto ontario canada december web traffic major components internet traffic 
main directions research area reduce time latencies users experience navigating web sites 
caching direction characteristics web cause caching medium poor performance 
prefetching studied web context 
study investigates partial match prediction technique taken data compression literature prefetching web 
main concern employing prefetching predict requests possible limiting false predictions minimum 
simulation results suggest high fraction predictions accurate predicts requests accuracy additional network traffic kept low 
furthermore simulations show prefetching substantially increase cache hit rates 
main directions research web reduce time latencies users experience navigating web sites 
area originated relevant research operating systems aim reduce file system latencies 
caching technique web domain 
caches reduce latencies allow fast retrievals potentially frequently accessed documents 
studies indicate benefits technique limited :10.1.1.34.4398
vast number documents available web quick rate change diverse needs users time main factors cause web caching perform poorly 
method applied operating systems prefetching studied web context 
prefetching employed web pages user access near transferred cache requested 
user request prefetched pages local cache reducing latency minimum 
prefetching complementary caching mechanism 
may take place system idle predictions accurate performance cache significantly improved 
investigates applicability multiple high order markov models doing prefetching web 
specifically algorithm employed compression context prediction partial match predict users requests particular web site 
trace driven simulation emulates network traffic additional prefetched pages workload behaviour clients browsers caches new scheme explores characteristics prediction algorithm new environment 
study evaluates costs benefits employing technique different parameters model vary 
experiments show requests predicted accuracy cache hits increase times 
results demonstrate algorithm efficiently tailored suit new environment deliver considerable benefits users achieving better performance approaches 
remainder document organized follows 
theoretical model implications employing web environment discussed section 
section describes simulation framework 
results simulations discussed section 
section presents related 
section summarizes suggests directions research 
prediction model order successful prefetching model describes users request patterns essential 
models may drive prefetching system compression literature 
previous indicates idea applying techniques compression algorithms making predictions fruitful 
compressors building model data operating order efficient 
general models generate probability distribution elements data subsequently achieve effective coding 
scheme data high probability occur encoded bits infrequently occurring data bits 
accurate model significantly improves performance coding 
likewise prefetching pages requested prefetched 
compressor effectively compress data sequence means model accurately describes data making predictions 
context modeling advocated earlier prediction model compressor uses crucial efficacy 
research compression community shown algorithms employing context modeling general purpose compression tend achieve superior performance 
context models comprise family techniques preceding characters order calculate probability 
simplest way predict character follow string selecting highest fixed probability 
case previous characters ignored 
sophisticated way computing letter probabilities take account preceding symbols 
evidently predictions accurate produced respect context 
length context order 
preceding symbols determine probability character order model 
prediction partial match ppm context models proposed literature :10.1.1.14.4305
ppm compressor uses multiple high order markov models store contexts 
algorithm uses high order contexts predictions contexts available 
predictions lower orders 
building model web context elements model access events particular web pages server case contexts correspond sequences events 
pages residing web server context denotes pages accessed specific order clients 
order prefetcher maintains markov predictors correspond contexts length way describing model trie 
contexts different lengths single structure 
algorithm constructing prediction model depicted 
example describing procedure give insight way model constructed 
example illustrated 
assume model order initially empty sequence accesses introduced 
step available context order context represented dotted square surrounding root node 
element child time new order context 
number label associated node counts occurrences context 
dotted squares show current context order 
element comes contexts updated turn 
process continues similar fashion entire sequence exhausted 
algorithm described single pass data require bulk processing need preprocessing 
operate real time 
model updated information new user requests comes time provide predictions concerning accesses 
order markov predictor scheme calculates conditional probability accessing page previous accesses pages fp pkg order 
formally predictor computes probability gamma 
objective build prediction model access patterns users 
input trie structure representing prediction model order constructed far set events deriving user 
output updated prediction model 
root node length null event length child node representing event node node construct child node representing event node node root node algorithm building prediction model 
interesting point case predictions different contexts merged subtle variations pattern sequences captured 
model example assume specific user seen page followed accesses pages pages prefetched predicted contexts fbg respectively 
consequently matter actual request sequence potentially predicted model 
simulation model order estimate performance algorithm simulation model 
simulation trace driven real web server log file employed emulate requests clients assess operational behaviour prefetching system 
overview architecture simulator depicted 
dispatcher module responsible coordination structure model inserting sequence 
components system 
prediction engine component constructs updates prediction model requests issued users offers predictions independently client 
traces pertaining different clients intermingled building model lead distorted view actual access patterns 
addition model differentiate discrete series accesses belong user 
model tries capture characteristics introducing notion browsing sessions 
browsing session oe set requests fr represents entry log carrying information client request time error codes 
request set constrained belong client oe host oe addition total ordering set time time time window group client prediction engine log dispatcher architecture simulation model 
requests sessions time gamma time max requests discrete clients correspond different browsing sessions session ends relevant user idle max time units typical value max order minutes 
model solve problem identifying different users proxy server 
prefetching beneficial 
important note building model log file entries taken account 
obviously requests non existent pages requests result error contribute model 
addition pages generated cgi programs prefetched usually involve user selections normally take place just request specific page 
requests inlined images ignored 
evident user issues request document subsequent requests embedded images follow 
prediction engine selects pages prefetch occurrence count associated node model 
addition occurrence count certain node divided count father node yields percentage times particular path followed possible ones 
fraction enables prediction engine set confidence threshold pages selected 
discussed previous section strategy combining predictions contexts multiple lengths may prove useful 
outline algorithm 
context superscript time window length stands server prediction engine located 
similar time window introduced simulation clients 
model produces predictions independently may associated different level confidence 
predictions merged single set duplicates removed 
objective predict request prediction model previous requests confidence threshold order model 
input trie structure representing prediction model order set requests confidence threshold order th output set predicted pages 
initialization separate phase current contexts updated new requests arrive length node depth representing access sequence null length child node parent cth remove duplicates algorithm making predictions multiple orders different confidence thresholds 
client module simulates client side system 
receives page requested additional pages sent prefetcher stores cache implemented lru replacement policy 
time simulation may exist instances client module running associated different client 
instances correspond active clients 
client oe termed active request issued web server window max time units simulation time 
formally client oe associated requests fr issued times time time 
client considered active time gamma time max client inactive cache emptied contents 
intuition scheme pages originating particular server infinite lifetime client cache 
behaviour simulated emptying cache specific time intervals 
results simulations driven access log files pertaining web server department computer science university toronto 
quantities expressed percentages evaluate performance prefetching system usefulness predictions number prefetched pages users requested divided total number requested pages 
accuracy predictions number prefetched pages users requested divided total number prefetched pages 
network traffic volume network traffic prefetching employed divided non prefetching case 
prefetching system aims maximizing metrics time minimizing 
obvious objectives conflicting 
pages prefetched probable accessed 
time return value lower accuracy decreasing increase network traffic high 
trade quantities prefetching algorithm take account 
trace characteristics log file experiments spans time period days contains user requests originating unique hosts 
mentioned earlier successful html text requests taken account leaving requests 
days training data order build prediction model rest simulate real system prefetching 
order navigation patterns users perceived prediction model useful know pages request web server leave 
information help fine tuning prefetching algorithm 
depicts average number maximum browsing session idle time sec pages distinct pages mean number page requests browsing session function maximum browsing session idle time 
pages requested single session maximum browsing session idle time varies 
curves represent results cases distinct pages session taken account 
standard deviation experiments reported table 
pages distinct pages mean std 
dev 
mean std 
dev 
table arithmetic mean standard deviation number page requests browsing session 
results reported maximum browsing session idle time set minutes 
results indicate users tend request pages single session exist sessions significantly higher numbers requested pages 
observation implies users really interested navigating site know exactly find information want 
prefetching beneficial avoided 
note results interpreted exact description users access behaviour 
represent way prediction model captures navigation patterns users 
experimental results subsequent paragraphs illustrate discuss outcome simulations experiment investigates influence single parameter time performance system 
best values obtained experiment combined 
varying number previous requests previous requests parameter increases number prefetched pages decreases 
previous requests order order order order previous requests order order order order previous requests order order order order varying number previous requests 
prefetcher making predictions clients exhibit interest web site leave requests 
results accurate predictions predicted requests fewer 
elaborate complete presentation experimental results interested reader refer 
varying confidence similar patterns observed confidence increases 
larger values parameter minimum confidence order order order order minimum confidence order order order order minimum confidence order order order order varying confidence 
cause fewer predictions reducing network traffic 
increment prediction accuracy significant highly probable pages prefetched 
varying number predictions way predict requests possible allowing maximum predictions parameter take large values represented 
number useful predictions increase note maximum predictions order order order order maximum predictions order order order order maximum predictions order order order order varying number predictions 
prefetching possible page result perfect metric return value minimal 
increase traffic dramatic accuracy degrades quickly 
varying client cache idle time results displayed correspond confidence setting 
maximum client cache idle time sec order order order order maximum client cache idle time sec order order order order maximum client cache idle time sec order order order order varying client cache idle time 
maximum client cache idle time increases network traffic number useful predictions diminish 
time accuracy remains nearly constant suggests aforementioned reduction due additional number requests serviced client cache service degradation 
client caches simulation left live longer probable page cache requested 
combining predictions representation prediction model stores multiple context orders structure offers opportunity combine predictions different orders 
compares constant confidence value weighted confidence model order 
note cases predictions independently order model combined single answer set 
weighted confidence employed higher order contexts trusted assigned smaller confidence 
graphs horizontal lines represent performance algorithm weighted confidence orders takes values reducing steps 
algorithm compared constant confidence values 
results suggest weighted confidence approach tries achieve performance minimum confidence constant conf 
weighted conf 
minimum confidence constant conf 
weighted conf 
minimum confidence constant conf 
weighted conf 
constant weighted confidence combining predictions different orders 
metrics 
performs average constant confidence algorithms categories 
weighted confidence algorithms different maximum order models compared 
prediction models orders ranging weighted confidence set order order order varying order model combining predictions different orders 
context order reduces subsequent order 
results show number useful predictions increase maximum order model cost accuracy 
efficiency algorithm evident performance algorithm specified metrics vary depending parameters 
different settings may limit algorithm predict highly probable pages relax constraints result page requests predicted 
cases table 
experiment predicts quarter total number requests issued web server accuracy 
second improves accuracy causes number predicted requests shrink fifth 
max order confidence prev 
requests network traffic useful pred 
accuracy table efficiency algorithm weighted confidence 
parameters common experiments maximum predictions pages client cache size pages browsing session idle time seconds client cache idle time seconds 
potential predictions order assess ability algorithm predictions useful know predictions done 
potential predictions restricted factors 
initial request browsing session possibly predicted 
second model predict sequence requests observed 
histogram shows percentage requests predicted log file confidence percentage requests potentially predicted 
study 
requests form page accessed page pattern occurred 
total number potential predictions sum bars histogram accounts requests 
experiments table indicate predictions anticipated figures cited histogram 
consideration confidence levels experiments predictions expected 
note estimate pessimistic predictions simulations start right access server 
outcome explained fact experiments predictions derived high order models usually exhibit greater confidence levels 
cache benefits simulations show prefetching complementary fashion caching technique 
reports cache hits various cache sizes prefetching 
case parameters algorithm experiment table 
prefetching employed cache experiences times cache hits non prefetching case 
shows relative benefits increase cache size gets smaller 
client cache size cache prefetching plain cache cache hits prefetching different cache sizes 
related follows overview cache prefetching techniques 
operating systems web communities cited observe similarities differences 
operating systems context griffioen appleton propose predictive cache prefetches file pages probability graph past accesses 
nodes graph represent accessed files directed weighted arc node node expresses metric probability requesting file requests file trace driven simulation revealed improvements time application wait read operations complete ranging compared non prefetching case 
claim data compression techniques successfully prefetching 
implementation idea prediction partial match ppm kroeger long 
lei duchamp employ access trees predictions :10.1.1.49.3592
access trees record files accessed execution program 
application re executed current activity compared saved access trees 
similarity detected files access tree prefetched 
trace driven simulation shows substantial reduction applications latency despite significant cpu overhead 
world wide web context bestavros proposes server initiated protocol sends client requested document server predictions 
predictions produced closure matrix element represents probability document requested certain time window request document previous operating systems basis padmanabhan mogul research prediction requests :10.1.1.43.33
trace driven simulation linear model network test algorithm 
results indicate benefits prefetching significant 
increase network traffic considerable order magnitude greater numbers reported 
accuracy predictions reported 
independently jacobson cao applied version partial match prediction technique prefetching proxies low bandwidth clients 
shows prefetching reduce latency predicting requests increasing traffic significant part reduction attributed caching effect prefetch buffer 
experiments indicate algorithm superior performance terms metrics discussed 
case prefetching process coupled server opposed proxy 
study attempts determine bounds performance proxy caching prefetching :10.1.1.34.4398
authors performed trace driven simulation infinite size cache complete knowledge requests 
external latency proxy cache server accounts total latency 
caching achieved reduction prefetching combination 
crovella barford discuss effects prefetching network 
trace driven simulation indicates straightforward approaches prefetching increase burstiness traffic 
authors propose transport rate control mechanism 
simulation denotes rate controlled prefetching significantly improves network performance compared straightforward approach non prefetching case delivering requested documents time 
commercial products promise enhance performance browsers 
algorithms sophisticated 
merely exploit idle time client network connection prefetch links current page user popular pages 
increase network traffic considerable 
investigated application multi order context modeling technique prefetching web domain 
special characteristics web context recognized algorithm tailored fit new environment 
despite heterogeneous nature users geographic origin reason visiting web site information looking imposes limitations performance prefetching system simulation suggests advantages employing prediction significant performance comparable better similar approaches 
considerable fraction users requests predicted high accuracy predicts requests accuracy reducing retrieval latencies boosting cache performance 
currently looking ways extending ffl perform time simulations order evaluate time savings users 
ffl introduce flexibility algorithm cooperate network layer order informed decisions 
ffl incorporate aging mechanism allow model automatically respond changes user access patterns similar technique restrict model memory requirements 
mary baker satoshi etienne john ousterhout margo seltzer 
non volatile memory fast reliable file systems 
international conference architectural support programming languages operating systems pages september 
bell cleary witten 
text compression 
prentice hall 
azer bestavros 
speculative data dissemination service reduce server load network traffic service time distributed information systems 
international conference data engineering pages new orleans lo february 
john cleary ian witten :10.1.1.14.4305
data compression adaptive coding partial string matching 
ieee transactions communications 
www com march 
mark crovella paul barford 
network effects prefetching 
ieee infocom san francisco ca usa 
kenneth krishnan jeffrey scott vitter 
practical prefetching data compression 
acm sigmod international conference pages washington dc usa june 
department computer science university toronto 
www cs toronto edu march 
fred douglis anja feldmann balachander krishnamurthy jeffrey mogul 
rate change metrics live study world wide web 
usenix symposium internet technology systems pages berkeley ca usa december 
james griffioen randy appleton 
design implementation evaluation predictive caching file system 
technical report cs department computer science university kentucky june 
quinn jacobson pei cao 
potential limits web prefetching low bandwidth clients proxies 
web caching workshop june 
donald knuth 
art computer programming sorting searching volume 
addisonwesley 
thomas kroeger long 
predicting file system actions prior events 
winter usenix conference 
thomas kroeger darrell long jeffrey mogul :10.1.1.34.4398
exploring bounds web latency reduction caching prefetching 
usenix symposium internet technologies systems pages san diego ca usa january 
hui lei dan duchamp :10.1.1.49.3592
analytical approach file prefetching 
usenix annual technical conference pages berkeley ca usa january 
venkata padmanabhan jeffrey mogul :10.1.1.43.33
predictive prefetching improve world wide web latency 
acm sigcomm computer communication review 

web prefetching partial match prediction 
technical report csrg department computer science university toronto march 
www peak com march 
web www web com march 
contents prediction model context modeling 
building model 
simulation model prediction engine 
client 
results trace characteristics 
experimental results 
varying number previous requests 
varying confidence 
varying number predictions 
varying client cache idle time 
combining predictions 
efficiency algorithm 
potential predictions 
cache benefits 
related operating systems context 
world wide web context 

