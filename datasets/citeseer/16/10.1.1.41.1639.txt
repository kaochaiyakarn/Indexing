probabilistic outputs support vector machines comparisons regularized likelihood methods john platt march john platt microsoft research microsoft way redmond wa microsoft com research microsoft com output classi er calibrated posterior probability enable post processing 
standard svms provide probabilities 
method create probabilities directly train kernel classi er logit link function regularized maximum likelihood score 
training maximum likelihood score produce non sparse kernel machines 
train svm train parameters additional sigmoid function map svm outputs probabilities 
chapter compares classi cation error rate likelihood scores svm plus sigmoid versus kernel method trained regularized likelihood error function 
methods tested data mining style data sets 
svm sigmoid yields probabilities comparable quality regularized maximum likelihood kernel method retaining sparseness svm 
constructing classi er produce posterior probability useful practical recognition situations 
example posterior probability allows decisions utility model 
posterior probabilities required classi er making small part decision classi cation outputs combined decision 
example combination viterbi search hmm combine recognition results phoneme recognizers word recognition 
simple case multi category classi er choosing category maximal posterior probability classes bayes optimal decision equal loss case 
support vector machines svms produce uncalibrated value probability 
output svm advances large margin classi ers alexander smola peter bartlett bernhard sch olkopf dale schuurmans eds mit press appear 
lies reproducing kernel hilbert space rkhs induced kernel 
training svm minimizes error function penalizes approximation training misclassi cation rate plus term penalizes norm rkhs minimizing error function minimize bound test misclassi cation rate desirable goal 
additional advantage error function minimizing produce sparse machine subset possible kernels nal machine 
method producing probabilistic outputs kernel machine proposed wahba 
wahba logistic link function jx exp de ned proposed minimizing negative log multinomial likelihood plus term penalizes norm rkhs log log 
output machine posterior probability 
minimizing error function directly produce sparse machine modi cation method produce sparse kernel machines 
chapter presents modi cations svms yield posterior probabilities maintaining sparseness 
chapter reviews modifying svms produce probabilities 
second describes method tting sigmoid maps svm outputs posterior probabilities 
svm plus sigmoid combination compared regularized likelihood kernel di erent data mining style data sets 
sec 
vapnik suggests method mapping output svms probabilities decomposing feature space direction orthogonal separating hyperplane dimensions feature space direction orthogonal separating hyperplane parameterized scaled version directions parameterized vector full generality posterior probability depends jt 
vapnik proposes tting probability sum cosine terms jt cos nt coecients cosine expansion minimize regularized functional eqn 
converted linear equation depends value current input evaluated 
preliminary results method shown promising 
limitations overcome method chapter 
example vapnik method requires solution linear system evaluation svm 
method chapter require linear system solver call evaluation averages jf price eciency dependencies jf modeled 
interesting feature vapnik method sum cosine terms constrained lie constrained monotonic see example 
strong prior considering probability jf monotonic svm trained separate positive examples negative examples 
method tting probabilities output svm gaussians class conditional densities jy jy 
rst proposed hastie tibshirani single tied variance estimated gaussians 
posterior probability rule jf sigmoid slope determined tied variance 
hastie tibshirani adjust bias sigmoid point jf occurs 
sigmoid monotonic single parameter derived variances may accurately model true posterior probability 
exible version gaussian jy 
mean variance gaussian determined data set 
bayes rule compute posterior probability jf jy jy prior probabilities computed training set formulation posterior analytic function form jf exp af bf issues model svm outputs 
posterior estimate derived gaussian approximation violates strong monotonic prior mentioned function non monotonic 
second assumption gaussian class conditional densities violated see 
fitting sigmoid svm motivation estimating class conditional densities jy parametric model posterior jf directly 
parameters model adapted give best probability outputs 
form parametric model inspired looking empirical data 
shows plot class conditional densities jy linear svm trained version uci adult data set see 
plot shows histograms densities bins wide derived threefold cross validation 
densities far away gaussian 
discontinuities derivatives densities positive margin negative margin 
discontinuities surprising considering cost function discontinuities margins 
theory explain form class conditional densities currently development 
class conditional densities margins apparently exponential 
bayes rule exponentials suggests parametric form sigmoid jf exp af sigmoid model equivalent assuming output svm proportional log odds positive example 
sigmoid model di erent proposed parameters trained discriminatively parameter estimated tied variance 
model svm output probabilities independently proposed speaker identi cation talk burges nips svm workshop 
histograms jy linear svm trained adult data set 
solid line jy dashed line jy notice histograms gaussian 
sigmoid works seen 
data points derived bayes rule histogram estimates class conditional densities 
linear svm trained adult data set sigmoid ts non parametric estimate extremely margins 
sets kernels described chapter sigmoid ts reasonably small amount bias margins 
non parametric model posterior probability handwritten digits shown close sigmoid 
sigmoid posterior model close true model 
view sigmoid function linearization log odds space posterior 
long monotonicity assured 
cases densities close gaussian sigmoid appropriate valuable 
fitting sigmoid parameters maximum likelihood estimation training set 
de ne new training set target probabilities de ned parameters minimizing negative log likelihood training data cross entropy error function min log log exp af minimization parameter minimization 
performed number optimization algorithms 
robustness experiments sigmoid data linear svm adult data set 
plus mark posterior probability computed examples falling bin width 
solid line best sigmoid posterior algorithm described chapter 
performed model trust minimization algorithm pseudo code shown appendix 
issues arise optimization choice sigmoid training set method avoid tting set 
easiest training set simply training examples svm 
ith training example 
training svm causes svm outputs biased estimate distribution sample 
examples margin forced absolute value exactly certainly common value test examples 
training examples fail margin subtly biased pushed margin corresponding margin substantially unbiased 
linear svms bias introduced training usually severe 
cases maximum support vectors lie margin input dimensionality usually small fraction training set 
real world problems linear svms optimal performance reached small causes bias margin failures small 
linear svms possible simply sigmoid training set 
non linear svms support vectors form substantial fraction entire data set especially bayes error rate problem high 
empirical experiments tting sigmoid training set non linear svms leads biased ts 
form unbiased training set output svm method forming unbiased training set approximate leave estimates described volume chapter weston 
requires solution linear system data point training set re run svm solver data point computationally expensive 
computationally inexpensive methods deriving unbiased training set generating hold set cross validation 
hold set fraction training set typically train svm train sigmoid 
hold set estimate parameters system kernel choice kernel parameters system parameters determined hold set main svm re trained entire training set 
svm training scales roughly quadratically training set size hold set times slower simply training entire data set 
determining system parameters unavoidable determining hold set may incur extra computation method 
cross validation better method hold set estimating parameters 
fold cross validation training set split parts 
svms trained permutations parts evaluated remaining third 
union sets form training set sigmoid adjust svm system parameters 
cross validation produces larger sigmoid training sets hold method gives lower variance estimate fold cross validation takes approximately times long training single svm entire training set 
results chapter fold cross validation 
cross validated unbiased training data sigmoid example reuters data set categories positive examples linearly separable negative examples :10.1.1.11.6124
fitting sigmoid svms maximum likelihood simply drive parameter large negative number positive examples reweighted 
nite number solutions nitely steep sigmoids validation set perfectly separable 
add regularization term prevent tting small number examples 
regularization requires prior model parameter space prior model distribution sample data 
imagine gaussian laplacian prior free parameter prior distribution variance 
free parameter set cross validation bayesian hyperparameter inference methods add complexity code 
simpler method create model sample data 
model assume sample data simply training data perturbed gaussian noise 
model parzen windows 
model free parameter 
sigmoid chapter uses di erent sample model sample data modelled empirical density sigmoid training data nite probability opposite label 
words positive example observed value assume nite chance opposite label sample data 
value similarly negative example target value non binary target require modi cation maximum likelihood optimization code 
simply kullback liebler divergence function behaved non binary probability correct label derived bayes rule 
choose uniform uninformative prior probabilities correct label 
observe positive examples 
map estimate target probability positive examples similarly negative examples map estimate target probability negative examples targets data sigmoid non binary targets value bayes motivated traditional non binary targets neural networks 
furthermore non binary targets converge training set size approaches nity recovers maximum likelihood sigmoid pseudo code appendix shows optimization bayesian targets 
empirical tests experiments determine real world performance svm sigmoid combination 
svm sigmoid compared plain svm misclassi cation rate 
assuming equal loss type type ii errors optimal threshold svm sigmoid jf optimal threshold svm 
rst experiment checks see threshold optimal svms 
second experiment compare svm sigmoid kernel machine trained explicitly maximize log multinomial likelihood 
linear kernel case equivalent comparing linear svm regularized logistic regression 
purpose second experiment check quality probability estimates svm sigmoid hybrid combination see error function causes fewer misclassi cations 
di erent classi cation task training testing number number set size set size inputs svms reuters linear adult linear adult quadratic web linear web quadratic table experimental parameters tasks 
rst task determining category reuters news article :10.1.1.11.6124
second task uci adult benchmark estimating income household census form data input vectors quantized 
third task determining category web page key words page 
reuters task solved linear svm adult web tasks solved linear quadratic svms 
parameters training shown table 
regularization terms set separately algorithm performance hold set 
value shown table svm sigmoid 
sigmoid parameters estimated fold cross validation 
quadratic kernel adult task 
kernel web task 
constants taken average data set dot product example 
normalization keeps kernel function reasonable range 
table shows results experiments 
table lists number errors raw svm svm sigmoid regularized likelihood kernel method 
lists negative log likelihood test set svm sigmoid regularized likelihood kernel method 
mcnemar test nd statistically signi cant di erences classi cation error rate signed rank test nd signi cant di erences log likelihood 
tests examine results pair algorithms example test set 
table underlined entries pairwise statistically signi cantly better non underlined entries statistically signi cantly better underlined entry 
signi cance threshold 
task raw svm svm regularized svm regularized sigmoid likelihood sigmoid likelihood number number number log log errors errors errors score score reuters linear adult linear adult quadratic web linear web quadratic table experimental results discussion interesting results observed experiments 
adding sigmoid improves error rate raw svm zero threshold necessarily bayes optimal 
reuters linear adult quadratic tasks sigmoid threshold signi cantly better standard zero threshold 
tasks ratio priors far tend push bayes optimal threshold away zero 
example adult quadratic task threshold jf corresponds threshold simply optimal threshold zero 
vc bounds generalization error guarantee zero threshold bayes optimal 
second interesting result adding sigmoid produces probabilities roughly comparable quality regularized likelihood kernel method 
tasks regularized likelihood yields signi cantly better probabilities 
web quadratic task svm sigmoid better log likelihood wilcoxon rank test prefers regularized likelihood kernel method data points accurate method 
third interesting result svm sigmoid regularized likelihood kernel machine completely dominant method error rate log likelihood 
svm sigmoid fewer errors regularized likelihood kernel method tasks regularized likelihood method fewer errors tasks 
result somewhat surprising svm kernel machine trained minimize error rate regularized likelihood trained maximize log likelihood 
experiments indicate factors kernel choice held constant di erence performance hard predict priori 
interesting note kernel methods produce sparse machines relying rkhs 
class methods penalize norm function rkhs norm see example volume chapter mangasarian 
fitting sigmoid tting sparse kernel machines may yield reasonable estimates probabilities 
chapter presents method extracting probabilities svm outputs useful classi cation post processing 
method leaves svm error function unchanged 
adds trainable post processing step trained regularized binomial maximum likelihood 
parameter sigmoid chosen post processing matches posterior empirically observed 
svm sigmoid combination compared raw svm kernel method entirely trained regularized maximum likelihood 
svm sigmoid combination preserves sparseness svm producing probabilities comparable quality regularized likelihood kernel method 
chris bishop valuable advice writing 
appendix pseudo code sigmoid training appendix shows pseudo code training shown 
algorithm model trust algorithm levenberg marquardt algorithm 
input parameters array svm outputs target array booleans ith example positive example 
prior number positive examples prior number negative examples outputs parameters sigmoid log prior prior prior prior prior lambda pp temp array store current estimate probability examples set pp array elements prior prior prior count compute hessian gradient error function respect len target pp pp pp gradient really tiny abs break err loop goodness fit increases det lambda lambda det determinant hessian zero increase stabilizer lambda continue lambda det lambda det compute goodness fit err len exp pp step sure log returns err log log err lambda break error decrease increase stabilizer factor try lambda lambda broken 
give break diff err scale err diff scale diff count count err count break bourlard morgan 
continuous speech recognition system embedding mlp hmm 
touretzky editor advances neural information processing systems volume pages 
morgan kaufmann san mateo ca 
chen donoho saunders 
atomic decomposition basis pursuit 
technical report department statistics stanford university may 
dietterich 
approximate statistical test comparing supervised classi cation learning algorithms 
neural computation 
duda hart 
pattern classi cation scene analysis 
john wiley sons 
dumais 
svms text categorization 
ieee intelligent systems 
hearst sch olkopf dumais osuna platt trends controversies support vector machines 
gill murray wright 
practical optimization 
academic press 
hastie tibshirani 
classi cation pairwise coupling 
technical report stanford university university toronto 

stanford edu trevor papers class ps 
joachims 
text categorization support vector machines 
european conference machine learning ecml 
joachims 
making large scale svm learning practical 
sch olkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 
kearns 
bound error cross validation approximation estimation rates consequences training test split 
neural computation 
mackay 
bayesian interpolation 
neural computation 

linear nonlinear separation patterns linear programming 
operations research 
merz murphy 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
irvine ca university california department information computer science 
mosteller rourke 
statistics 
addison wesley reading ma 
platt 
fast training support vector machines sequential minimal optimization 
sch olkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 
platt 
sequential minimal optimization fast algorithm training support vector machines 
technical report msr tr microsoft research 
press teukolsky vetterling flannery 
numerical recipes art scienti computing nd ed 
cambridge university press cambridge 
rumelhart hinton williams 
learning internal representations error propagation 
rumelhart mcclelland editors parallel distributed processing volume pages 
mit press cambridge ma 
vapnik 
statistical learning theory 
wiley new york 
forthcoming 
wahba 
multivariate function operator estimation smoothing splines reproducing kernels 
casdagli eubank editors nonlinear modeling forecasting sfi studies sciences complexity proc 
vol xii pages 
addison wesley 
wahba 
bias variance tradeo randomized 
cohn kearns solla editor advances neural information processing systems volume 
mit press cambridge ma 
appear 
wahba 
support vector machines reproducing kernel hilbert spaces randomized 
sch olkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 

