sentence maximum entropy language model rosenfeld school computer science carnegie mellon university pittsburgh pa introduce new kind language model models sentences utterances directly maximum entropy paradigm 
new model conceptually simpler naturally suited modeling sentence phenomena conditional models proposed date 
avoiding chain rule model treats sentence utterance bag features features arbitrary computable properties sentence 
model interfere training done sampling 
model computationally straightforward 
main computational cost training model generating sample sentences gibbs distribution 
interestingly cost different dependencies potentially lower comparable conditional model 
motivation conventional statistical language models estimate probability sentence chain rule decompose product conditional probabilities pr def pr wn pr jw gamma def pr jh def fw gamma history predicting word vast majority statistical language modeling devoted estimating terms form pr wjh 
application chain rule technically harmless uses exact equality approximation 
terms pr wjh may best way think estimating pr 
global sentence information grammaticality awkward encode conditional framework 

external influences sentence example effect preceding utterances dialog level variables equally hard encode 
furthermore influences factored prediction word current sentence causing small systematic biases estimate compounded 

pr wjh typically approximated pr jw gammak gamma markov assumption 
model improved including longer distance information implicit independence assumptions 
evidenced looking language data assumptions patently false significant global dependencies sentences 
simple example limitations chain rule approach consider aspect sentence length 
gram model effect number words utterance probability modeled directly 
implicit consequence gram prediction 
corrected word insertion penalty usefulness proves length important feature 
word insertion penalty model length geometric distribution fit empirical data especially short utterances 
alternative conventional conditional formulation propose maximum entropy language model directly models distribution entire sentence utterance faster train maximum entropy models date 
review maximum entropy framework maximum entropy models exponential distributions satisfy linear constraints 
years successfully applied language modeling approximate conditional probabilities form wjh model prepositional phrase attachment induce features word spelling :10.1.1.103.7637:10.1.1.40.180
conditional language modeling constraint feature functions respective desired values solution form wjh delta exp parameters derived generalized iterative scaling algorithm gis 
computational bottleneck gis computing expectations ep feature functions especially normalization constant exp training datapoint 
bottleneck quite severe training model incorporates gram trigger features words take hundreds workstation days 
bottleneck hinders widespread framework language modeling 
follows propose model training paradigm address computational problem modeling problem discussed section 
sentence models new formulation standard model replaced delta exp new model involve chain rule conceptually simpler 
addition normalization constant global single number model 
way think proposed model take old model map ffl null history 
new model normalization infeasible involves summation possible sentences similarly expectations ep computed explicitly 
problems overcome follows 
training feature functions estimate ratio independent ep derived 
testing knowing normalizing constant perplexity computed 
interfaced application speech recognizer value ignored hypotheses considered current sentence 
estimating expectations explicit summation sentences infeasible estimate expectations ep sampling 
gibbs sampling sample population character strings 
describe generate sentences unnormalized distribution exp alternative method applicable far efficient 
gibbs sampling sentences generate single sentence distribution characterized start arbitrary sentence iterate follows 
choose word position randomly sweeping sentence 

word vocabulary place position current word resulting sentence calculate 

choose word random distribution fq place word position sentence 
constitutes single step random walk underlying markov field 
iterations procedure resulting sentence guaranteed unbiased sample gibbs distribution 
corrective sampling possible estimate ep efficiently gibbs sampling 
exists distribution say computationally easier sample unbiased sample exists 
procedure 
generate sentences existing sample 

sentence sample calculate statistics expectations eq desired 
sampling function reasonably close 
approximates determine size sample needed reliable estimation 
choice conventional model incorporates linear interpolation knowledge sources target model 
choice exponential distribution previous iteration gis algorithm sample exists 
feature types straightforward feature constraint types implemented conventional gram models conditional models date ngrams distance ngrams class ngrams word triggers 
see implementation feature types conditional model 
hopefully just 
hope maximum entropy model type discussed break usability barrier currently hinders exploration integration multiple knowledge sources 
successful open experimentation researchers varied knowledge sources believe carry significant information 
sources may include ffl sentence length 
ffl number type verbs 
ffl various aspects grammaticality person agreement number agreement partial parser supplied information 
ffl dialog level information 
ffl prosodic time related information speaking rate pauses 
exact value theoretically known 
filler features useful define feature complement set existing features 
example set bigram features word may supplemented introducing feature features 
similar filler features defined feature types 
defined word position sentence length helps direct modeling 
filler features contribute numerical stability training procedure 
furthermore enable model assign reasonable probabilities sentences feature values defined known 
validation validate approach discussed built utterance model small kw training set broadcast news utterances unigram bigram trigram features appropriate filler features 
features introduced time 
unigram features introduced model allowed converge 
bigram features introduced model allowed converge 
trigram features introduced model allowed converge 
resulted faster convergence simultaneous feature types 
model unnormalized impossible compare perplexity comparable conventional model 
provide sample sentences generated gibbs sampling various stages training procedure 
table lists sample sentences generated initial model training took place 
initial set zero resulting model uniform model 
tables list sample sentences generated converged model unigram bigram trigram features respectively 
seen example sentences model successfully incorporated information provided respective features 
model described incorporates conventional features easy incorporate simple relative frequency model done purpose supporting comparisons 
model unaware nature features 
arbitrary features accommodated virtually change model code 
referring unit modeling sentence 
course method model word sequence utterance consistent linguistic boundaries 
naturally linguistically induced features may may applicable non sentences 

possible prove normalization constant feasibly computed 
evidence question ll getting care greg getting answer death right back month news wrote mean honor greg look today table sentences generated gibbs sampling initial untrained model 
initialized zero uniform model 
don live right angeles done know don go just point want table sentences generated gibbs sampling utterance model trained unigram features 
summary analysis introduced new kind language model incorporates arbitrary knowledge sources maximum entropy paradigm naturally suited modeling sentence phenomena conditional models proposed date 
avoiding chain rule model treats sentence utterance bag features features arbitrary computable properties sentence 
underlying equations considerably simpler conditional models 
computational considerations model straightforward computationally expensive 
main computational burden training model generating sample sentences 
efficient sampling crucial practical success 
want don los angeles ask news agenda way think table adding bigram features 
live los angeles business news tokyo says bill think way table adding trigram features 
interestingly computational requirements different type dependence comparable conditional models 
specifically computational cost determined combination rare features accurately want model 
experimental needs done improve efficiency sampling process 
particular heuristics explored ffl iteration consider subset vocabulary replacement 
trades computational cost iteration mixing rate underlying markov chain 
ffl rough estimation iterations need know direction rough magnitude correction increase sample size approaching convergence 
ffl determine sample size dynamically 
modeling considerations sentence model incorporating features conditional model fact identical 
training procedure conditional models restricts computation feature expectations histories observed training data see section 
biases solution interesting usually appropriate way 
example consider word trigger features gamma 
gamma 
correlated training data affect solution conditional model 
fact perfectly correlated occurring document resulting half value features 
beneficial model captures correlations recur new data 
sentence model incorporating features correlation explicitly instructed separate feature training data sentence training derive target values 
am grateful sanjeev khudanpur fred jelinek prakash narayan helpful discussions suggestions 
project sponsored national security agency number mda 
united states government authorized reproduce distribute reprints notwithstanding copyright notation 
jaynes information theory statistical mechanics physics reviews vol 
pp 

della pietra della pietra mercer roukos adaptive language modeling minimum discriminant estimation proceedings icassp pp 

lau rosenfeld roukos trigger language models maximum entropy approach proc 
icassp pp 
ii 
berger della pietra della pietra maximum entropy approach natural language processing computational linguistics vol 
march 
rosenfeld maximum entropy approach adaptive statistical language modeling computer speech language vol 
pp 

longer version carnegie mellon tech 
rep cmu cs 
ratnaparkhi roukos maximum entropy model prepositional phrase attachment proceedings arpa workshop human language technology morgan kaufmann 
della pietra della pietra lafferty inducing features random fields ieee transactions pattern analysis machine intelligence vol 
pp 
april 
darroch ratcliff generalized iterative scaling log linear models annals mathematical statistics vol 
pp 

geman geman stochastic relaxation gibbs distributions bayesian restoration images ieee transactions pattern analysis machine intelligence vol 
pp 

