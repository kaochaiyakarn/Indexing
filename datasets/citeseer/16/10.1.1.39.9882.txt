information bottleneck method naftali tishby hebrew university jerusalem israel tishby cs huji ac il fernando pereira labs research florham park nj research att com william bialek nec research institute princeton nj bialek research nj nec com de ne relevant information signal information signal provides signal examples include information face images provide names people portrayed information speech sounds provide words spoken 
understanding signal requires just predicting requires specifying features play role prediction 
formalize problem nding short code preserves maximum information squeeze information provides bottleneck formed limited set codewords constrained optimization problem seen generalization rate distortion theory distortion measure emerges joint statistics approach yields exact set self consistent equations coding rules solutions equations convergent re estimation method generalizes blahut algorithm 
variational principle provides surprisingly rich framework discussing variety problems signal processing learning described detail 
fundamental problem formalizing intuitive ideas information provide quantitative notion meaningful relevant information 
issues intentionally left information theory original formulation shannon focused attention problem transmitting information judging value recipient 
correspondingly information theory viewed strictly theory communication view accepted people consider statistical information theoretic principles irrelevant question meaning 
contrast argue information theory particular lossy source compression provides natural quantitative approach question relevant information 
speci cally formulate variational principle extraction ecient representation relevant information 
related argue single information theoretic principle contains special cases wide variety problems including prediction ltering learning various forms 
problem extracting relevant summary data compressed description captures relevant meaningful information posed suitable de nition relevance 
typical example speech compression 
consider lossless compression compression entropy speech components signal reconstructed 
hand transcript spoken words lower entropy orders magnitude acoustic waveform means possible compress losing information words meaning 
standard analysis lossy source compression rate distortion theory provides tradeo rate signal representation size average distortion reconstructed signal 
rate distortion theory determines level inevitable expected distortion desired information rate terms rate distortion function 
main problem rate distortion theory need specify distortion function rst turn determines relevant features signal 
features explicitly known arbitrary choice distortion function fact arbitrary feature selection 
speech example best partial knowledge precise components signal perceived neural speech recognition system 
relevant components depend complex structure auditory nervous system sounds utterances exposed early life 
virtually impossible come correct distortion function acoustic signals 
type diculty exists similar problems natural language processing bioinformatics example features protein sequences determine structure neural coding information encoded spike trains 
fundamental problem feature selection pattern recognition 
rate distortion theory provide full answer problem choice distortion function determines relevant features part theory 
step right direction 
possible solution comes fact interesting cases access additional variable determines relevant 
speech case transcription signal interested speech recognition problem speaker identity speaker identi cation goal 
natural language processing part speech labels words grammar checking dictionary senses ambiguous words information retrieval 
similarly protein folding problem joint database sequences dimensional structures neural coding simultaneous recording sensory stimuli neural responses de nes implicitly relevant variables domain 
problems formal underlying structure extract information variable relevant prediction 
choice additional variable determines relevant components features signal case 
short formalize intuitive idea information theoretic approach extends elements rate distortion theory 
derive self consistent equations iterative algorithm nding representations signal capture relevant structure prove convergence algorithm 
relevant quantization denote signal message space xed probability measure denote quantized codebook compressed representation 
ease exposition assume sets nite continuous space rst quantized 
value seek possibly stochastic mapping representative codeword codebook characterized conditional xjx 
mapping xjx induces soft partitioning block associated codebook elements probability xjx 
total probability codeword xjx average volume elements mapped codeword xj xj xjx log xjx conditional entropy determines quality quantization 
rst factor course rate average number bits message needed specify element codebook confusion 
number element bounded mutual information log xjx average cardinality partitioning ratio volume mean partition xj standard asymptotic arguments 
notice quantity di erent entropy codebook entropy normally want minimize 
information rate characterize quantization rate reduced throwing away details original signal need additional constraints 
relevance distortion rate distortion theory rate distortion theory constraint provided distortion function presumed small representations 
rate distortion function speci es implicitly relevant aspects values partitioning induced mapping xjx expected distortion hd monotonic tradeo rate quantization expected distortion larger rate smaller achievable distortion 
celebrated rate distortion theorem shannon kolmogorov see example ref 
characterizes tradeo rate distortion function de ned minimal achievable rate constraint expected distortion min fp xjx hd dg finding rate distortion function variational problem solved introducing lagrange multiplier constrained expected distortion 
needs minimize functional xjx hd normalized distributions xjx 
variational formulation known consequences theorem solution variational problem xjx normalized distributions xjx exponential form xjx exp normalization partition function 
lagrange multiplier determined value expected distortion positive satis es proof 
derivative xjx obtains xjx log xjx xjx marginal distribution satis es xjx 
normalization lagrange multipliers setting derivatives zero writing log obtain eq 

varying normalized xjx variations hd linked hd eq 
follows 
positivity consequence concavity rate distortion function see example chapter ref 

blahut algorithm important practical consequence variational formulation provides converging iterative algorithm self consistent determination distributions xjx 
equations satis ed simultaneously consistent probability assignment 
natural approach solve equations alternately iterate reaching convergence 
lemma due csisz ar assures global convergence case 
lemma yjx joint distribution 
distribution minimizes relative entropy dkl jp marginal distribution yjx 
dkl jp min dkl jp equivalently distribution minimizes expected relative entropy dkl yjx jq marginal distribution yjx 
proof follows directly non negativity relative entropy 
lemma guarantees marginal condition eq 
variational principle 
theorem equations satis ed simultaneously minimum functional hd minimization done independently convex sets normalized distributions fp fp xjx min min xjx xjx independent conditions correspond precisely alternating iterations eq 
eq 

denoting iteration step xjx xjx exp normalization function evaluated eq 

furthermore iterations converge unique minimum convex sets distributions 
proof see 
alternating iteration known ba algorithm calculation rate distortion function 
important notice ba algorithm deals optimal partitioning set set representatives optimal choice representation practice nite data important nd optimal representatives minimize expected distortion partitioning 
joint optimization similar em procedure statistical estimation general unique solution 
relevance variable information bottleneck right distortion measure rarely available problem relevant quantization addressed directly preserving relevant information variable 
relevance variable denoted independent original signal positive mutual information 
assumed access joint distribution part setup problem similarly rate distortion case 
new variational principle relevant quantization compress possible 
contrast rate distortion problem want quantization problem obtaining sample distribution interesting issue learning theory scope 
start problem see ref 

capture information possible 
amount information log obviously lossy compression convey information original data 
rate distortion tradeo compressing representation preserving meaningful information single right solution tradeo assignment looking keeps xed amount meaningful information relevant signal minimizing number bits original signal maximizing compression 
ect pass information provides bottleneck formed compact summaries nd optimal assignment minimizing functional xjx lagrange multiplier attached constrained meaningful information maintaining normalization mapping xjx quantization sketchy possible assigned single point pushed arbitrarily detailed quantization 
varying parameter explore tradeo preserved meaningful information compression various resolutions 
show interesting special cases exist sucient statistics possible preserve meaningful information nite signi cant compression original data :10.1.1.122.8863:10.1.1.122.8863
self consistent equations case rate distortion theory constraint meaningful information nonlinear desired mapping xjx harder variational problem 
surprisingly general problem extracting meaningful information minimizing functional xjx eq 
exact formal solution 
theorem optimal assignment minimizes eq 
satis es equation xjx exp yjx log yjx yj distribution yj exponent bayes rule markov chain condition yj yjx xjx solution number interesting features emphasize formal solution yj exponential de ned implicitly terms assignment mapping xjx 
just marginal distribution satisfy marginal condition eq 
consistency 
completely equivalent maximize meaningful information xed compression original variable 
proof 
note conditional distribution yj yjx xj follows markov chain condition variational variables scheme conditional distributions xjx unknown distributions determined bayes rule consistency 
xjx xjy xjx xjy equations imply derivatives xjx xjx xjy xjx xjy introducing lagrange multipliers information constraint normalization conditional distributions xjx lagrangian eq 
xjx xjx log xjx log xjy xjx derivatives respect xjx obtains xjx log xjx xjx log xjy xjx log xjy xjx log substituting derivatives eq eq rearranging xjx log xjx yjx log yj notice yjx log yjx function independent absorbed multiplier 
introducing yjx log yjx important notice modeling assumption quantization hidden variable model data 
markov condition di erent nally obtain variational condition xjx log xjx yjx log yjx yj equivalent equation xjx xjx exp dkl yjx jp yj exp exp dkl yjx jp yj normalization partition function 
comments 
notice kullback leibler divergence dkl yjx jp yj emerged relevant ective distortion measure variational principle assumed 
natural consider correct distortion dkl yjx jp yj quantization information bottleneck setting 

equation equations determine self consistently desired conditional distributions xjx 
crucial quantization performed conditional distributions yj self consistent equations include optimization representatives contrast theory selection representatives separate problem 
information bottleneck iterative algorithm ba algorithm self consistent equations suggest natural method nding unknown distributions value equations turned converging alternating iterations convex distribution sets fp xjx fp fp yj stated theorem 
theorem self consistent equations satis ed simultaneously minima functional xjx yj yjx jp yj minimization done independently convex sets normalized distributions fp fp xjx fp yj min yj min min xjx xjx yj minimization performed converging alternating iterations 
denoting iteration step xjx exp xjx yj yjx xj normalization partition function evaluated eq 

proof 
lack space outline proof 
show equations satis ed minima functional known physicists free energy 
follows lemma applied convex sets xjx ba algorithm 
second part lemma applied yjx jp yj expected relative entropy 
equation minimizes expected relative entropy variations convex set normalized fp yj denoting dkl yjx jp yj normalization lagrange multipliers obtain yjx log yj yj yjx yj yj expected relative entropy yjx xj yj yj gives eq 
yj independent equation interpretation weighted average data conditional distributions contribute representative prove convergence iterations verify iteration steps minimizes functional independently functional lower bounded sum non negative terms 
point notice yj xed back rate distortion case xed distortion matrix 
argument ba algorithm applies 
hand just showed third equation minimizes expected relative entropy ecting mutual information 
proves convergence alternating iterations 
situation similar em algorithm functional xjx yj convex distribution independently product space distributions 
convergence proof imply uniqueness solution 
structure solutions formal solution self consistent equations described requires speci cation structure cardinality rate distortion theory 
value lagrange multiplier corresponding values mutual information choice cardinality variational principle implies suggests deterministic annealing approach 
increasing value move convex curves information plane 
curves analogous rate distortion curves exists choice cardinality solutions self consistent equations correspond family annealing curves start trivial point information plane nite slope parameterized value interestingly curves family separate nite critical second order phase transition 
transitions form hierarchy relevant quantizations di erent cardinalities described :10.1.1.122.8863:10.1.1.122.8863
fascinating aspect information bottleneck principle provides uni ed framework di erent information processing problems including prediction ltering learning 
successful applications method various real problems semantic clustering english words document categorization neural coding spectral analysis 
insightful discussions rate distortion theory joachim buhmann shai fine greatly appreciated 
collaboration facilitated part israel binational science foundation bsf 
bialek tishby extracting relevant information preparation 
cover thomas elements information theory wiley new york 
csisz ar information geometry alternating minimization procedures statistics decisions suppl 

blahut computation channel capacity rate distortion function ieee trans 
inform 
theory 
slonim tishby agglomerative information bottleneck appear advances neural information processing systems nips 
pereira tishby lee distributional clustering english words th annual 
association computational linguistics pp 

