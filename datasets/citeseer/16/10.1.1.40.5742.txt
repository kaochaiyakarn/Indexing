parallel mining association rules rakesh agrawal john shafer ibm almaden research center harry rd san jose ca almaden ibm com tel fax consider problem mining association rules shared multiprocessor 
algorithms explore spectrum trade offs computation communication memory usage synchronization problem specific information 
best algorithm exhibits near perfect scaleup behavior requires minimal overhead compared current best serial algorithm 
keywords data mining association rules sequential patterns parallel algorithms availability inexpensive storage progress data capture technology organizations created ultra large databases business scientific data trend expected grow 
complementary technology trend progress networking memory processor technologies opened possibility accessing manipulating massive databases reasonable amount time 
data mining called knowledge discovery databases efficient discovery previously unknown patterns large databases 
promise data mining deliver technology enable development new breed decision support applications 
discovering association rules important data mining problem 
considerable research designing fast algorithms task 
exception far concentrated designing serial algorithms 
databases mined large measured gigabytes terabytes parallel algorithms required 
parallel algorithms mining association rules 
order determine best method mining rules parallel explore spectrum trade offs computation communication memory usage synchronization problem specific information parallel data mining 
specifically 
focus count distribution algorithm minimizing communication 
expense carrying redundant duplicate computations parallel 

data distribution algorithm attempts utilize aggregate main memory system effectively 
communication happy algorithm requires nodes broadcast local data nodes 
department computer science university wisconsin madison 

candidate distribution algorithm exploits semantics particular problem hand reduce synchronization processors segment database patterns different transactions support 
algorithm incorporates load balancing 
algorithms serial algorithm apriori 
chose apriori algorithm superior performance earlier algorithms shown 
preferred apriori apriorihybrid somewhat faster algorithm apriorihybrid harder parallelize performance apriorihybrid sensitive heuristically determined parameters 
furthermore performance apriori approximate apriorihybrid combining small workloads apriori cycles single workload requiring cycle algorithm quite similar apriori parallelization techniques directly apply algorithm 
algorithm perform apriori large datasets large number items 
algorithm attempts improve performance apriori hash filter 
see section optimization slows apriori algorithm 
concurrent algorithm parallelized simulation study 
suffers hash filter despite special communication operator build 
discuss section 
parallel algorithms implemented ibm system sp henceforth referred simply sp shared machine 
measurements implementation evaluate effectiveness design trade offs 
winning algorithm part ibm data mining product field 
organization rest follows 
section gives brief review problem mining association rules apriori algorithm proposed parallel algorithms 
section gives description parallel algorithms 
section presents results performance measurements algorithms 
section contains 
detailed version 
overview serial algorithm association rules basic problem finding association rules introduced follows 
fi set literals called items 
set transactions transaction itemset say transaction contains set items association rule implication form ae ae 
rule holds transaction set confidence transactions contain contain rule support transaction set transactions contain set transactions problem mining association rules generate association rules certain user specified minimum support called minsup confidence called minconf 
problem decomposition problem mining association rules decomposed subproblems 
find sets items itemsets support greater user specified 
itemsets minimum support called frequent itemsets 
itemset itemset having items 
lk set frequent itemsets minimum support 
member set fields itemset ii support count 
ck set candidate itemsets potentially frequent itemsets 
member set fields itemset ii support count 
processor id dataset local processor dr dataset local processor repartitioning candidate set maintained processor kth pass items candidate notation represents pass number lk gamma ck new candidates size generated lk gamma forall transactions increment count candidates ck contained lk candidates ck minimum support answer lk apriori algorithm 
frequent itemsets generate desired rules 
general idea say abcd ab frequent itemsets determine rule ab cd holds computing ratio conf support abcd support ab 
conf minimum confidence rule holds 
rule minimum support abcd frequent 
research focussed subproblem database accessed part computation algorithms proposed 
review section apriori algorithm parallel algorithms 
apriori algorithm gives overview apriori algorithm finding frequent itemsets notation 
pass algorithm simply counts item occurrences determine frequent itemsets 
subsequent pass say pass consists phases 
frequent itemsets gamma gamma th pass generate candidate itemsets apriori candidate generation procedure described 
database scanned support candidates counted 
fast counting need efficiently determine candidates contained transaction hash tree data structure purpose 
candidate generation gamma set frequent gamma itemsets want generate superset set frequent itemsets 
intuition apriori candidate generation procedure itemset minimum support subsets simplicity assume items itemset lexicographic order 
candidate generation takes steps 
join step join gamma gamma insert ck select item item gamma gamma lk gamma lk gamma item item gamma gamma gamma gamma prune step delete itemsets gamma subset gamma example ff gg 
join step ff prune step delete itemset itemset left parallel algorithms parallel algorithms subproblem problem finding frequent itemsets 
give parallel algorithm second subproblem problem generating rules frequent itemsets 
refer summary notation algorithm descriptions 
superscripts indicate processor id subscripts indicate pass number size itemset 
algorithms assume shared architecture processors private memory private disk 
processors connected communication network communicate passing messages 
communication primitives algorithms part mpi message passing interface communication library supported sp candidates message passing communication standard currently discussion 
data evenly distributed disks attached processors processor disk roughly equal number transactions 
require transactions placed disks special way 
algorithm count distribution algorithm uses simple principle allowing redundant computations parallel idle processors avoid communication 
pass special 
passes algorithm works follows 
processor generates complete complete frequent itemset gamma created pass gamma 
observe processor identical gamma generating identical 
processor pass data partition develops local support counts candidates 
processor exchanges local counts processors develop global counts 
processors forced synchronize step 

processor computes 
processor independently decision terminate continue pass 
decision identical processors identical pass processor dynamically generates local candidate itemset depending items local data partition candidates counted different processors may identical care taken exchanging local counts determine global pass processors scan local data asynchronously parallel 
synchronize pass develop global counts 
performance considerations steps similar serial algorithm 
non obvious step processors exchange local counts arrive global counts 
processor exact processor puts count values common order count array 
needed perform parallel vector sum arrays 
requires communicating count values done log communication steps 
avoids time consuming logic needed assure combine counts belong candidate 
full details process including mpi communication primitives described 
algorithm data distribution attractive feature count distribution algorithm data tuples exchanged processors counts exchanged 
processors operate independently asynchronously reading data 
disadvantage algorithm exploit aggregate memory system effectively 
suppose processor memory size jm number candidates counted pass determined jm increase number processors system theta jm total memory count number candidates pass processor counting identical candidates 
count distribution algorithm counts candidates pass serial algorithm 
data distribution algorithm designed exploit better total system memory number processors increased 
algorithm processor counts mutually exclusive candidates 
number processors increased larger number candidates counted pass 
processor configuration data able count single pass candidate set require passes count 
downside algorithm processor broadcast local data processors pass 
algorithm viable machine fast communication 
pass count distribution algorithm 
pass 
processor generates gamma retains nth itemsets forming candidates subset count 
itemsets retained determined processor id computed communicating processors 
implementation itemsets assigned round robin fashion 
sets disjoint union sets original 
processor develops support counts itemsets local candidate set local data pages data pages received processors 

pass data processor calculates local sets disjoint union sets 
processors exchange processor complete generating pass 
step requires processors synchronize 
having obtained complete processor independently identically decide terminate continue pass 
interesting step step processors develop support counts local candidates asynchronously 
step processors broadcasting local data receiving local data processors 
careful avoid network congestion asynchronous communication overlap communication time counting support 
see full details 
algorithm candidate distribution limitation count data distribution algorithms database transaction support candidate itemset transaction compared entire candidate set 
requires count duplicate candidate set processor data broadcast database transaction 
additionally count data distribution algorithms require processors synchronize pass exchange counts frequent itemsets respectively 
workload perfectly balanced cause processors wait whichever processor finishes pass 
problems due fact count data exploit problem specific knowledge data tuples candidate itemsets partitioned merely equally divide 
processors consulted information gathered proceed pass 
candidate distribution algorithm attempts away dependencies partitioning data candidates way processor may proceed independently 
pass heuristically determined algorithm divides frequent itemsets gamma processors way processor generate unique independent processors 
time data processor count candidates independent processors 
note depending quality itemset partitioning parts database may replicated processors 
itemset partitioning algorithm considers aspect identifying segments gamma supported different database transactions 
choice redistribution pass tradeoff decoupling processor dependence soon possible waiting itemsets easily partitionable 
partitioning algorithm exploits semantics apriori candidate generation procedure described section 
candidate distribution processor proceeds independently counting portion global candidate set local data 
communication counts data tuples required 
dependence processor processors pruning local candidate set prune step candidate generation 
information sent asynchronously processors wait complete pruning information arrive processors 
prune step candidate generation prunes candidate set possible information arrived opportunistically starts counting candidates 
late arriving pruning information subsequent passes 
algorithm described 
pass count data distribution algorithm 
pass 
partition gamma processors gamma sets balanced 
discuss partitioning done 
record frequent itemset gamma processor assigned itemset 
partitioning identically done parallel processor 

processor generates logically gamma partition assigned 
note access complete gamma standard pruning generating pass 

develops global counts candidates database dr time 

processed local data data received processors posts gamma asynchronous receive buffers receive processors 
needed pruning prune step candidate generation 

processor computes asynchronously broadcasts gamma processors gamma asynchronous sends 
pass 
processor collects frequent itemsets sent processors 
pruning step candidate generation join step 
itemsets received processor length gamma smaller gamma slower processor greater gamma faster processor 
keeps track processor largest size frequent itemsets sent 
receive buffers frequent itemsets processing 

generates local gamma happen received gamma processors needs careful time pruning 
needs distinguish itemset gamma long subset candidate itemset gamma itemset gamma set received processor probing gamma remember repartitioning took place pass gamma long prefix itemset question finding processor responsible checking gamma received processor 

pass dr counts computes asynchronously broadcasts processor gamma asynchronous sends 
data distribution algorithm step pass requires communicating local data support counts developed 
difference local data need broadcast processor candidate partitioning processors information transactions useful developing support counts processors 
allows processors send data network 
full details filtering described 
partitioning motivate algorithm partitioning example 
abd abe acd ace bcd bce bde 
abce 
consider abd members common prefix ab 
note candidates abcd abce abcde prefix ab 
apriori candidate generation procedure section generates candidates joining items assuming items itemsets lexicographically ordered partition itemsets common gamma long prefixes 
ensuring partition assigned processor ensured processor generate candidates independently ignoring prune step 
suppose repartition database way tuple supports itemset contained partitions assigned processor copied local disk processor 
processors proceed completely asynchronously 
actual algorithm involved reasons 
processor may obtain frequent itemsets computed processors prune step candidate generation 
example processor assigned set know bcde frequent able decide prune candidate abcde set prefix bc may assigned different processor 
problem need balance load processors 
details full partitioning algorithm 
parallel rule generation parallel implementation second subproblem problem generating rules frequent itemsets 
generating rules expensive discovering frequent itemsets require examination data 
frequent itemset rule generation examines non empty subset generates rule gammaa support support confidence support support 
computation efficiently done examining largest subsets proceeding smaller subsets generated rules required minimum confidence 
example frequent itemset abcd rule abc minimum confidence ab cd need consider 
generating rules parallel simply involves partitioning set frequent itemsets processors 
processor generates rules partition algorithm 
number rules generated itemset sensitive itemset size attempt equitable balancing partitioning itemsets length equally processors 
note calculation confidence rule processor may need examine support itemset responsible 
reason processor access frequent itemsets rule generation 
problem count data distribution algorithms pass processors frequent itemsets 
candidate distribution algorithm fast processors may need wait slower processors discovered transmitted frequent itemsets 
reason rule generation step relatively cheap may better candidate distribution algorithm simply discover frequent itemsets generate rules line possibly serial processor 
allow processors freed run jobs soon done finding frequent itemsets processors system working 
discussion tradeoffs initially clear algorithms win single winner 
count minimizes communication expense ignoring aggregate memory 
workstation cluster environment approach probably ideal may sp 
data distribution fully exploits aggregate memory cost heavy communication help explore issue 
data ability count single pass times candidates count algorithm strong contender 
third algorithm candidate distribution see incorporating detailed problem knowledge yield benefits count data distribution algorithms 
see beneficial removing processor dependence synchronous communication 
performance evaluation ran experiments node ibm sp model 
node multiprocessor thin node consisting power processor running mhz mb real memory 
attached node gb disk mb available tests 
processors run aix level communicate high performance switch hps adaptors 
combined communication hardware rated peak bandwidth megabytes second latency microseconds 
tests base communication routines actual point point bandwidth reached mb experiments run idle system 
see details sp architecture 
name average transaction length average size frequent itemsets average number transactions table data parameters synthetic datasets varying complexity generated procedure described 
characteristics datasets shown table 
datasets vary short transactions frequent itemsets fewer larger transactions frequent itemsets 
datasets mb processor size 
larger datasets due constraints amount storage available local disks candidate algorithm writes redistributed database local disks candidate partitioning run disk space larger datasets 
include results experiments mb processor count distribution algorithm show trends larger amounts data processor 
experiments repeated multiple times obtain stable values data point 
relative performance trade offs shows response times parallel algorithms datasets node configuration total database size approximately gb 
response time measured time elapsed initiation execution time processor finishing computation 
response times serial version run node worth data th total database 
run serial algorithm entire data disk space available 
obtained similar results node configurations dataset sizes 
experiments candidate distribution repartitioning done fourth pass 
tests choice yielded best performance 
results encouraging count candidate distribution algorithms response times close serial algorithm especially true count 
overhead count compared serial version run data 
third overhead spend waiting processors synchronize 
parallel algorithms data distribution fare 
expected data able better exploit aggregate memory multiprocessor fewer passes case datasets large average transaction frequent itemset lengths see table 
dataset candidate data count serial data relative performance algorithms dataset normal communication communication costs data distribution name serial count data candidate table number data passes required performance turned markedly lower reasons extra communication fact node system process single database transaction 
communication worst problems show machine sp fast communication 
points labeled normal correspond response times normal data distribution algorithm node configuration mb data replicated node 
points labeled communication correspond modified version data distribution algorithm receiving data nodes node simply processed local data times 
node exact data yielded exact results difference time spent communication management 
datasets discovered fully half time taken data distribution communication 
algorithm entirely cpu bound making savings due data making fewer passes practically negligible 
hoped better results candidate distribution algorithm considering exploits problem specific semantics 
candidate algorithm communicate entire dataset redistribution pass suffers problems data 
candidate performs redistribution 
data processors may selectively filter transactions sends processors depending dependency graph partitioned 
greatly reduce amount data traveling network 
unfortunately single pass filtered data redistribution costly 
question subsequent passes processor run completely independently smaller candidate sets compensate cost 
performance results show redistribution simply costs 
data distribution candidate algorithm unable capitalize optimal aggregate memory large candidate sets force count multiple occur candidate takes redistribution pass 
candidate just data passes count 
insufficient gains coupled high redistribution cost allow count small overhead emerge winner 
experiments show count overhead fairly small synchronization costs quite large data distributions skewed nodes equally capable different memory sizes processor speeds bandwidths capacities 
investigation issues broad topic plans 
think alternatives adding load balancing count distribution algorithm require redistribution complete database case candidate distribution algorithm 
extrapolating results study sense count distribution algorithm appropriate load balancing strategy continue dominate 
sensitivity analysis examine scaleup speedup characteristics count distribution algorithm 
report results data candidate distribution algorithms inferior performance 
scaleup see count distribution algorithm handles larger problem sets processors available performed scaleup experiments increased size database direct proportion number nodes system 
datasets previous experiments number transactions increased decreased depending multiprocessor size 
database sizes single node configurations shown table 
mb node datasets range mb single node case gb node case 
shows performance results datasets 
addition absolute response times number processors increased plotted scaleup response time normalized respect response time single processor 
clearly count algorithm scales able keep response time constant database multiprocessor sizes increase 
slight increases response times due entirely processors involved communication 
itemsets algorithm change database size increased number candidates support summed communication phase remains constant 
experiments fixed size multiprocessor nodes growing database mb node mb node 
plotted response times 
response time normalized respect response time mb node 
results show sublinear performance count algorithm program efficient database size increased 
results change database size increases amount cost communication 
increasing size database simply non communication portion code take time due transaction processing 
result reducing percentage time spent communication 
cpu processing scale perfectly get sublinear performance 
scaleup relative scaleup number processors scaleup number processors ideal relative amount data node mb amount data node mb ideal speedup relative speedup number processors speedup number processors ideal performance count distribution dataset hash filter count effect hash filtering speedup set experiments kept database constant varied number processors 
constraint available disk space size databases fixed mb 
shows results running count algorithm configurations processors 
run larger configurations amount data node small 
speedup response time normalized respect response time single processor 
graphs show count speedup performance 
performance fall short ideal processors 
artifact small amount data node processing 
mb node communication times significant percentage response time 
easily predicted experiments noticed data node processes significant communication time giving better performance 
simply seeing opposite effect 
larger datasets shown better speedup characteristics 
effect hash filtering park chen yu proposed hash filter reduce cost apriori particularly second pass reducing size basic idea build hash filter tuples read pass 
itemset tuple count incremented corresponding hash bucket 
pass upperbound support count itemset database 
generating candidate itemsets hashed candidate support count hash table minimum support deleted 
compares combined response times pass count algorithm hash filter algorithm 
times remaining passes identical 
count algorithm beats hash filter count explicitly forms uses specialized version hash tree done 
pruned apriori candidate generation algorithm equal theta represented simple dimensional count array drastically reducing memory requirements function call overhead 
savings hash filter prune lost due cost constructing hash filter regular hash tree storing counting parallel version hash filter algorithm called pdm performance results simulation study 
uses parallelization technique similar count entire candidate sets exchanged just candidate counts 
expensive communication cpu costs 
focus pdm efficient construction hash filter serial algorithm speed pass 
serial algorithm hash filter hurts performance resulting double performance hit pdm 
considered problem mining association rules shared multiprocessor data partitioned nodes 
parallel algorithms task apriori best serial algorithms mining association rules 
designs algorithms represent spectrum trade offs computation communication memory usage synchronization problem specific information 
count distribution algorithms attempts minimize communication replicating candidate sets processor memory 
processors local data communicate counts 
data distribution algorithm takes counter approach processor works entire dataset portion candidate set 
maximizes aggregate memory requires high communication broadcast data 
minimizing communication may best approach environment necessarily true sp 
lastly candidate algorithm incorporates domain knowledge partition data candidates allowing processor unique set candidates having repeatedly broadcast entire dataset 
maximizes aggregate memory limiting heavy communication single redistribution pass 
completely eliminates synchronization costs count data pay pass 
studied trade offs evaluated relative performance algorithms implementing node sp parallel machine 
count distribution emerged algorithm choice 
exhibited linear scaleup excellent speedup behavior 
processors overhead compared response time serial algorithm executing amount data 
data distribution algorithm lost cost broadcasting local data processor processor 
results show high bandwidth low latency system sp data redistribution costly 
candidate distribution algorithm similarly edged cost data redistribution gains having processor independently different subset problem single pass redistribution 
may learn carefully designed algorithm candidate beaten relatively simpler algorithm count illuminate fact problems require intricate parallelization 
exploring various possibilities shown true mining association rules 
acknowledgments maurice implemented parallel version association rule mining algorithm earlier version ibm system called sp nodes diskless data master node 
implementation changes architecture communication library basic algorithm benefited experience 
howard ho provided early prototype implementation mpi communication library get going 
ramakrishnan srikant patiently explained nuances serial apriori implementation 
discussions mike carey influential initial stages 
sp organization particularly bob sharon bob wonderful help arranging sp cycles tests 
rakesh agrawal tomasz imielinski arun swami 
mining association rules sets items large databases 
proc 
acm sigmod conference management data pages washington may 
rakesh agrawal john shafer 
parallel mining association rules design implementation experience 
research report rj ibm almaden research center san jose california february 
available www almaden ibm com cs quest 
rakesh agrawal ramakrishnan srikant 
fast algorithms mining association rules 
proc 
th int conference large databases santiago chile september 
message passing interface forum 
mpi message passing interface standard may 
han fu 
discovery multiple level association rules large databases 
proc 
st int conference large databases zurich switzerland september 
maurice arun swami 
set oriented mining association rules 
int conference data engineering taipei taiwan march 
int business machines 
scalable systems ga edition february 
heikki mannila hannu toivonen verkamo 
efficient algorithms discovering association rules 
kdd aaai workshop knowledge discovery databases pages seattle washington july 
jong soo park ming chen philip yu 
effective hash algorithm mining association rules 
proc 
acm sigmod conference management data san jose california may 
jong soo park ming chen philip yu 
efficient parallel data mining association rules 
fourth int conference information knowledge management baltimore maryland november 
savasere omiecinski navathe 
efficient algorithm mining association rules large databases 
proc 
vldb conference zurich switzerland september 
ramakrishnan srikant rakesh agrawal 
mining generalized association rules 
proc 
st int conference large databases zurich switzerland september 

