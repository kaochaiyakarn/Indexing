fast image retrieval algorithm automatically extracted discriminant features hwang john weng ming fang qian department computer science engineering michigan state university east lansing mi siemens corporate research college road east princeton nj mail cse msu edu qiang scr siemens com fisher discriminant analysis powerful classification perform number classes large number samples class small 
propose resolve problem dynamically grouping classes different levels tree 
recast problem classification regression problem classification class labels output regression numerical values output unified 
proposed hdr tree automatically forms clusters input space guided desired output produces discriminant spaces 
discriminant spaces organized coarse fine structure tree 
unified size dependent negative log likelihood proposed automatically handle sample situations number samples cluster smaller dimensionality discriminant space sample situations hdr tree reach near optimal performance 
fast computation hdr tree logarithmic retrieval time complexity 
proposed hdr tree tested synthetic data face image databases publicly available data sets manually selected features 
capability computers efficiently effectively retrieve information image databases databases gives significant impact progress digital library technology 
central task multimedia information system efficiently store fast correctly retrieve easily manage media rich information database 
essential issue image database representation image 
categorize existing approaches types model appearance 
model approaches manually defined features represent objects images 
lot efforts approach 
focus approach design efficient algorithm set manually selected features 
method appealing efficiency representing images 
model approach difficult generalize 
example face features useless car image database 
appearance approaches drawn lot attention machine vision 
appearance approach sensory input considered vector high dimensional space matter input image 
retrieval task problem finding best match training samples 
fewer features represent set images principal component analysis pca face recognition 
pca reconstruct images mean square errors 
pca features image reconstruction necessarily recognition 
features derived fisher linear discriminant analysis lda better recognition provided samples sufficiently rich cover typical variation class 
lda judiciously group similar classes single class coarse classification 
grouping equivalently increase number samples merged class allow better estimate class variation critical effectiveness lda 
cast classification problem regression problem class represented vector output space 
similarity classes measured distance output space guides class merging input space coarse classification 
case users assign distance preserved class vectors merged classes distance measures reflect classes merged 
rich literature area decision trees 
applications decision tree typically moderate dimensionality manually selected features 
designed cases number samples smaller input dimensionality common case appearance approaches 
deal wide range samples high dimensional space size dependent negative log likelihood node proposed hdr tree 
possible classes image database rich observations samples 
proposed hdr tree designed unbalanced samples classes 
method hierarchical discriminant regression discriminant analysis includes types class label output numerical output 
output class label analysis task called classification 
task called regression 
classification problem typically distance metric defined pair class labels 
possible map class labels dimensional output space th class label corresponds vector th component components zeros 
mapping distance different class labels 
map numeric output space set class labels losing numeric properties infinite number possible numerical vectors 
numerical output general class label output 
suppose want approximate mapping 
set training samples ng 
class label linear discriminant analysis class scatter class scatter matrices defined 
numerical output take value component perform discriminant analysis class information 
describe new hierarchical statistical modeling method address challenge 
consider mapping 
approximated regression tree high dimensional space called hierarchical discriminant regression hdr tree 
goal automatically generate discriminant features class label available numerical vectors space 
types clusters created node hdr tree clusters clusters 
clusters clusters output space clusters input space maximum clusters type node 
clusters determine virtual class label sample part 
virtual class label determine cluster input sample update part 
cluster approximates sample population space samples represented subtree rooted child node 
node finds nearest cluster euclidean distance updates pulling center cluster diagonal covariance matrix save space computation time neglecting diagonal elements 
cluster indicates corresponding cluster input belongs 
part update statistics cluster mean vector covariance matrix 
statistics cluster estimate closeness sample belong cluster 
total centers clusters give discriminant features span dimensional discriminant space 
negative log likelihood discussed section clusters determine cluster searched 
close sample search corresponding child may upper bound recursively corresponding terminal nodes 
computational efficiency clusters keep actual input samples 
orders statistics represent clusters 
example cluster keeps mean vector diagonal covariance matrix cluster keeps mean vector full covariance matrix efficient form 
summary algorithm recursively builds hdr tree set learning samples 
deeper node tree smaller variances clusters 
outline algorithm tree building retrieval 
procedure node subset training samples belong samples ng build subtree roots node recursively 
clusters allowed node 

number clusters node call clustering procedure obtain 
belongs th cluster belongs th cluster 

compute mean covariance matrix 

find nearest cluster probability distances 
sample belong cluster 
cluster portion samples belong cluster 
largest euclidean distance cluster larger number child node created cluster procedure called recursively input samples node number represents hdr tree space 
procedure clustering set output vectors yn return clusters 
represents maximum clusters allowed node 

mean cluster set 
find nearest cluster 
compute euclidean distance dist 
mean cluster set 
update cluster new member procedure create hdr tree just call procedure root training samples 
procedure retrieval testing sample described procedure 
procedure retrieval hdr tree sample estimate corresponding output vector parameter specifies upper bound width parallel tree search 

root tree compute probability distance cluster node 
select top clusters smallest probability distances clusters called active clusters 

active cluster received check points child node 
mark inactive explore child node computing probability distances clusters child node 
active clusters returned 

mark active clusters smallest probability distances 

steps recursively resulting active clusters terminal 

cluster shortest distance reached leaf nodes 
output corresponding mean cluster estimated output hierarchical version known gaussian distribution models deeper tree finer virtual class partition shallow levels sample distribution approximated mixture large gaussians large variances 
deep levels sample distribution approximated mixture small gaussians small variances 
multiple search paths allow sample falls gaussians lowest level research tree branches correspond immediate neighbors 
distance discriminant space subsection need estimate likelihood input vector belong cluster 
consider clusters 
cluster represented mean center covariance matrix 
dimensionality space typically high practical directly keep covariance matrix 
explained section internal node keeps clusters 
centers clusters denoted fc qg locations centers tell subspace centers lie 
computed method section discriminant space clusters formed clusters space detailed computational steps described follows 
suppose dimensionality space cluster centers eq procedure compute orthonormal basis vectors column unit basis vector matrix 
cluster center coordinate vector orthonormal basis cluster represented dimensional vector column vector represented discriminant subspace orthonormal basis vectors columns matrix representation original space mv 
linear manifold set vectors fv vn subset vector space want express subspace passes head tips vectors center vectors define set scatter vectors center set contains scatter vectors fs ng 
subspace spanned denoted span 
subspace passes head tips vectors represented linear manifold span 
orthonormal basis subspace span constructed gram schmidt orthogonalization procedure 
vector compute scatter part compute projection linear manifold 
th coordinates orthonormal basis 
call vector feature vector linear manifold size dependent negative log likelihood need system fully uses information available matter number samples observed 
terms measures likelihood mahalanobis distance euclidean distance 
likelihood uses covariance matrix individual cluster 
demanding richness observations 
mahalanobis distance uses average covariance matrices 
demanding just requires average covariance matrix reasonably rich observations necessarily cluster 
euclidean distance covariance matrix demanding 
samples available clusters euclidean distance suited distance 
consider negative log likelihood nll defined gaussian density ln call gaussian nll belong cluster call mahalanobis nll replaced scatter matrix average covariance matrices 
call euclidean nll replaced scale matrix predefined default noise estimate 
euclidean nll number samples node small 
gradually class scatter matrix clusters better estimated number samples increases 
mahalanobis nll 
cluster rich observations full gaussian nll 
realize smooth transition 
node compute class scatter sw total samples number samples cluster cluster node estimate sample covariance matrix start scale covariance matrix default virtual sample number predefined momentum sample number proportional gaussian nll expression measure change estimated covariance matrix number samples 
cluster estimated covariance matrix weighted sum sw estimated weight proportional respectively 
roughly measures average samples cluster counting sample 
weights sum 
words suppose nm nm weights wm nm covariance matrix compute gaussian likelihood matrix estimate size dependent negative log likelihood ln jw analysis necessary 
clusters increase samples speed wm equal equal exceed 
number sample cluster increases slower average number nm mainly class scatter larger large incorporated rich set samples 
appropriate 
limit growth nm nm minf proportional number reaches expected estimated reasonably weight class scatter sw equal eq 

putting upper bound nm allows take number samples sufficient 
important large sample cases reach best possible bayesian error rate distribution truly mixture gaussians 
experimental results experiments conducted test proposed hdr tree 
experimental results synthetic data 
show experimental results real images 
experiments synthetic data experiment data set clusters 
number dimension 
cluster modeled gaussian distribution 
centers clusters respectively 
covariance cluster identity matrix 
covariance matrices second third clusters respectively 
showed effects number samples 
fig 
samples class estimate decision boundaries 
number samples small gaussian nll suitable euclidean distance forms reasonable decision boundaries 
decision boundary estimated close euclidean distance 
samples class fig 

decision boundary euclidean distance mahalanobis distance 
fig 
samples class 
decision boundary close gaussian nll approximation model 

decision boundaries estimated different number samples different metrics 
lines mean decision boundaries bayesian decision rule 
lines euclidean distance 
lines measured gaussian nll 
lines mahalanobis distance 
lines 
experiments image data primary interest images algorithms perform face recognition tasks 
images came weizmann institute 
individual images data set 
framed face images contain different expressions taken different lighting conditions different orientations 
individuals data set 
example face images individual shown fig 
applied leave cross validation method test image data set 
images testing 
table compares different appearance methods 
eigenvalues principal component analysis pca 
pca outperformed nearest neighbor nn method accuracy speed 
eigenvalues result eigenvectors 
pca organized binary tree faster straight nn tested 
fastest algorithm methods tested worst performance 
lda shares best performance new hdr method test 
new hdr method faster lda compact representation 
proposed hdr tree automatically derives discriminant features high dimensional input space 
help decision tree retrieval time probe logarithmic 
output system class label numerical vectors depending system trainer gives training data 
experimental results showed algorithms deal large sample size small sample size various dimensionality 

face images weizmann institute 
table 
performance weizmann face data set method error rate avg 
testing time msec hdr pca pca tree lda nn pca pca oc breiman friedman olshen stone 
classification regression trees 
chapman hall new york 
kanade 
automatic generation object recognition programs 
proceedings ieee 
kriegman ponce 
recognizing positioning curved objects image contours 
ieee trans 
pattern analysis machine intelligence 
murase nayar 
visual learning recognition objects appearance 
int journal computer vision january 
murthy 
automatic construction decision trees data multidisciplinary survey 
data mining knowledge discovery 
tanaka 
design implementation video object database system 
ieee trans 
knowledge data engineering ff august 
landgrebe 
survey decision tree classifier methodology 
ieee trans 
systems man cybernetics may june 
swets weng 
discriminant eigenfeatures image retrieval 
ieee trans 
pattern analysis machine intelligence 
turk pentland 
eigenfaces recognition 
journal cognitive neuroscience 

