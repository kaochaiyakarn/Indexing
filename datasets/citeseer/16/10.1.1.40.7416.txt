accepted nips sv estimation distribution support bernhard sch olkopf 
robert williamson alex smola 
john shawe taylor gmd berlin germany department engineering australian national university canberra australia royal holloway college university london egham uk bs smola gmd de bob williamson anu edu au john dcs rhbnc ac uk suppose dataset drawn underlying probability distribution want estimate subset input space probability test point drawn lies outside bounded priori specified 
propose algorithm approaches problem trying estimate function positive negative complement 
functional form kernel expansion terms potentially small subset training data regularized controlling length weight vector associated feature space 
algorithm natural extension support vector algorithm case unlabelled data 
years new set kernel techniques supervised learning developed 
specifically support vector sv algorithms pattern recognition regression estimation solution inverse problems received considerable attention 
attempts transfer idea kernels compute inner products feature spaces domain unsupervised learning 
problems domain precisely specified 
generally characterized estimating functions data tell interesting underlying distributions 
instance kernel pca characterized computing functions training data produce unit variance outputs having minimum norm feature space 
kernel unsupervised learning technique regularized principal manifolds computes functions give mapping lower dimensional manifold minimizing regularized quantization error 
clustering algorithms examples unsupervised learning techniques kernelized 
extreme point view unsupervised learning estimating densities 
clearly knowledge density allow solve problem solved basis data 
addresses easier problem proposes algorithm computes binary function supposed capture regions input space probability density lives support function data live region function nonzero 
doing line vapnik principle solve problem general need solve 
applicable cases density data distribution defined singular components 
part motivation ben david lindenbaum 
turns considerable amount prior statistical literature short version fit details 
see brief overview full version 
algorithms introduce terminology notation conventions 
consider training data number observations set 
assume compact subset feature map map dot product space dot product image computed evaluating simple kernel 
gaussian kernel kx yk indices understood range compact notation 
bold face greek letters denote dimensional vectors comprising corresponding letters typeset normal face 
separate data set origin solve quadratic program min kwk subject 
parameter meaning clear 
nonzero slack variables penalized objective function expect solve problem decision function sgn 
positive examples contained training set sv type regularization term kwk small 
actual trade goals controlled 
multipliers introduce lagrangian kwk 
set derivatives primal variables equal zero yielding patterns called support vectors 
sv expansion transforms decision function kernel expansion sgn substituting obtain dual problem min ij subject extended version soon available svm gmd de 
show optimum inequality constraints equalities nonzero 
recover exploiting corresponding pattern satisfies 
conclude section note balls describe data feature space close spirit algorithms :10.1.1.42.1588
try put data small ball solving min 
subject ck leads dual min ij subject kernels depend constant 
case equality constraint implies linear term dual target function constant problem turns equivalent 
theory section analyse algorithm theoretically starting uniqueness hyperplane proposition 
describe connection pattern recognition proposition show parameter characterizes fractions svs outliers proposition 
give robustness result soft margin proposition briefly state error bounds theorem 
short version proofs omitted 
definition data set called separable exists 

proposition data set separable exists unique supporting hyperplane properties closer origin data distance origin maximal hyperplanes 
minw kwk subject 
proposition suppose supporting hyperplane data 
optimal separating hyperplane passing origin labelled data set ii suppose optimal separating hyperplane passing origin labelled data set suppose aligned 
positive kwk margin optimal hyperplane 
supporting hyperplane unlabelled data set fy note relationship holds true consider nonseparable problems 
case margin errors binary classification points wrong side separating hyperplane fall inside margin translate outliers classification points fall wrong side hyperplane 
proposition holds cum training sets margin errors outliers respectively removed 
utility proposition lies fact allows recycle certain results proven binary classification single class scenario :10.1.1.2.6040
explaining significance parameter case 
proposition assume solution satisfies 
statements hold upper bound fraction outliers 
ii lower bound fraction svs 
iii suppose data generated independently distribution contain discrete components 
suppose kernel analytic non constant 
probability asymptotically equals fraction svs fraction outliers 
parts ii follow directly proposition fact outliers dealt exactly way margin errors optimization problem binary classification case :10.1.1.2.6040
direct proof simply exploit constraints imposes fraction patterns upper bounding fraction outliers fraction patterns svs 
part iii proven uniform convergence argument showing covering numbers kernel expansions regularized norm feature space behaved fraction points lie exactly hyperplane asymptotically negligible cf 

proposition local movements outliers parallel change hyperplane 
proof suppose outlier kkt conditions 
transforming 
jj kwk leads slack nonzero 
feasible primal solution 

computed 
kkt conditions satisfied 
optimal solution 
move subject generalization 
goal bound probability novel point drawn underlying distribution lies outside estimated region certain margin 
start introducing common tool measuring capacity class functions map definition pseudo metric space subset 
set cover exists 
covering number minimal cardinality cover finite cover defined 
cover called proper idea finite approximate respect pseudometric distance finite sample pseudometric space functions dx max jf max dx 
logarithms base 
theorem consider distribution suppose generated probability sample find pfx log dlog basis proof lemma 
consider possibility small number points fails exceed corresponds having non zero slack variable algorithm take kwk class linear functions feature space application theorem 
known bounds log covering numbers class 
introduce notation size shortfall 
definition real valued function space fix denote quantity maxf similarly training sequence define pp theorem fix consider fixed unknown probability distribution input space class real valued functions range 
probability randomly drawn training sequences size fx xg log log log log proof similar proofs classification case theorem 
theorem bounds probability new training point falling region value complement estimate support distribution 
algorithm described hyperplane shifted origin define region 
note restriction placed class functions functions probability density functions 
choice gives trade size region bound holds increasing increases size region size probability increasing decreases size log covering numbers 
result shows bound probability points falling outside region estimated support quantity involving ratio log covering numbers bounded fat shattering dimension scale proportional number training examples plus factor involving norm slack variables 
similar bound terms norm slack variables form appears optimization performed algorithm 
result significantly stronger related results ben david lindenbaum bound involves square root ratio pollard dimension fat shattering dimension tends number training examples 
experiments discussion briefly illustrate approach artificial real world data 
displays toy examples shows parameter settings influence solution 
shows plot outputs 
training test set postal service database handwritten digits 
database contains digit images size test set 
fed algorithm gaussian kernel width typical value svms cf 
training instances digit 
testing done digit digits 
shown leads zero false positives learning machines seen non training correctly identifies non recognizing digits test set 
higher recognition rates achieved smaller values get correct recognition digits test set false positive rate fairly moderate 
results necessarily incomplete convey idea potential utility proposed algorithm 
width frac 
svs ols margin kwk pictures single class svm applied toy problems domain note cases fraction examples estimated region cf 
table 
large value causes additional data points upper left corner influence decision function 
smaller values third picture points ignored anymore 
alternatively force algorithm take outliers account changing kernel width fourth picture data effectively analyzed different length scale leads algorithm consider outliers meaningful points 
test train threshold test train threshold experiments postal service ocr dataset 
recognizer digit output histogram exemplars training test set test exemplars digits 
left get svs outliers consistent proposition right get respectively 
threshold marked graphs 
main inspiration approach stems earliest vapnik collaborators 
sixties proposed algorithm characterizing set unlabelled data points separating origin hyperplane 
quickly moved class classification problems terms algorithms terms theoretical development statistical learning theory originated days 
algorithmic point view identify weaknesses original approach may caused research direction decades 
firstly original algorithm limited linear decision rules input space secondly way dealing outliers 
conjunction restrictions severe generic dataset need separable origin hyperplane 
modifications incorporated dispose shortcomings 
kernel trick allows larger class functions nonlinearly mapping high dimensional feature space increases chances separation origin possible 
particular gaussian kernel separation exists see note dot products mapped patterns positive implying patterns lie inside orthant 
unit length 
separable 
second modification directly allows possibility outliers 
incorporated softness decision rule trick obtained direct handle fraction outliers :10.1.1.2.6040
believe approach proposing concrete algorithm behaved computational complexity convex quadratic programming problem far mainly studied theoretical point view abundant practical applications condition monitoring marketing information retrieval 
turn algorithm easy black box method questions selection kernel parameters width gaussian kernel tackled 
expectation theoretical results briefly outlined provide solid foundation formidable task 

supported arc dfg ja 
schnorr oliver helpful discussions 
ben david lindenbaum 
learning distributions density levels paradigm learning teacher 
journal computer system sciences 
bertsekas 
nonlinear programming 
athena scientific belmont ma 
cover thomas 
elements information theory 
wiley new york 
devroye wise 
detection abnormal behaviour nonparametric estimation support 
siam journal applied mathematics 

minimum volume sets generalized quantile processes 
stochastic processes applications 
scholkopf burges vapnik 
extracting support data task 
fayyad uthurusamy editors proceedings international conference knowledge discovery data mining 
aaai press menlo park ca 
scholkopf smola 
muller 
kernel principal component analysis 
scholkopf burges smola editors advances kernel methods support vector learning 
mit press cambridge ma 

scholkopf smola williamson bartlett :10.1.1.2.6040
new support vector algorithms 
appear neural computation 
neurocolt tr 
shawe taylor bartlett williamson anthony 
structural risk minimization data dependent hierarchies 
ieee trans 
inf 
theory 
shawe taylor cristianini 
margin distribution bounds generalization 
paul fischer hans ulrich simon editors computational learning theory th european conference eurocolt 
springer 
smola williamson mika scholkopf 
regularized principal manifolds 
computational learning theory th european conference volume lecture notes artificial intelligence pages 
springer 
tax duin 
data domain description support vectors 
verleysen editor proceedings esann pages brussels 
facto 
vapnik 
nature statistical learning theory 
springer verlag new york 
vapnik chervonenkis 
theory pattern recognition russian 
nauka moscow 
german translation theorie der akademie verlag berlin 
vapnik lerner 
pattern recognition generalized portraits 

