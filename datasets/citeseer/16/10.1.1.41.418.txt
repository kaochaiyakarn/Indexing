massachusetts institute technology artificial intelligence laboratory center biological computational learning department brain cognitive sciences memo march support vector machines training applications edgar osuna robert freund federico girosi publication retrieved anonymous ftp publications ai mit edu 
pathname publication ai publications aim ps support vector machine svm new promising classification technique developed vapnik group bell laboratories :10.1.1.15.9362:10.1.1.54.9934
new learning algorithm seen alternative training technique polynomial radial basis function multi layer perceptron classifiers 
main idea technique separate classes surface maximizes margin 
interesting property approach approximate implementation structural risk minimization srm induction principle 
derivation support vector machines relationship srm geometrical insight discussed 
structural risk minimization inductive principle aims minimizing bound generalization error model minimizing mean square error data set empirical risk minimization methods training svm obtain maximum margin classifier requires different objective function 
objective function optimized solving large scale quadratic programming problem linear box constraints 
problem considered challenging quadratic form completely dense memory needed store problem grows square number data points 
training problems arising real applications large data sets impossible load memory solved standard non linear constrained optimization algorithms 
decomposition algorithm train svm large data sets 
main idea decomposition iterative solution sub problems evaluation establish stopping criteria algorithm 
previous approaches results important details implementation algorithm second order variant reduced gradient method solver sub problems 
application svm preliminary results frontal human face detection images 
application opens interesting questions research opportunities context faster better optimization algorithms svm pattern classification recognition detection applications 
copyright fl massachusetts institute technology report describes research done center biological computational learning department brain cognitive sciences artificial intelligence laboratory massachusetts institute technology 
research sponsored muri onr arpa contract national science foundation contract asc award includes funds arpa provided hpcc program 
edgar osuna supported gran de daimler benz 
additional support provided daimler benz eastman kodak siemens corporate research contents support vector machines empirical risk minimization structural risk minimization support vector machines mathematical derivation linear classifier linearly separable problem soft margin hyperplane linearly non separable case nonlinear decision surfaces additional geometrical interpretation interesting extension weighted svm training support vector machine previous methods description computational results new approach large database training optimality conditions strategy improvement decomposition algorithm computational implementation results improving training svm directions svm application face detection images previous systems svm face detection system experimental results directions face detection svm applications report address problem implementing new pattern classification technique developed vladimir vapnik team bell laboratories known support vector machine svm :10.1.1.15.9362:10.1.1.54.9934
svm thought alternative training technique polynomial radial basis function multi layer perceptron classifiers weights network solving quadratic programming qp problem linear inequality equality constraints solving non convex unconstrained minimization problem standard neural network training techniques 
number variables qp problem equal number data points data set large optimization problem challenging quadratic form completely dense memory requirements grow square number data points 
decomposition algorithm guarantees global optimality train svm large data sets say data points 
main idea decomposition iterative solution sub problems evaluation optimality conditions generate improved iterative values establish stopping criteria algorithm 
demonstrate effectiveness approach applying svm problem detecting frontal faces images involves solution pattern classification problem face versus non face patterns large data base examples 
reason choice face detection problem application svm twofold problem important practical applications received lot attention years difficulty implementation svm arises data base large say larger problems involve large data base 
divided main parts 
part consisting sections describe svm approach pattern classification solution corresponding qp problem case large data bases 
second part section describe face detection system svm main components 
particular section reviews theory derivation svm extensions geometrical interpretations 
section starts reviewing done vapnik solving training problem svm 
section gives brief description initial approaches took order solve problem 
section contains main result presents new approach developed solve large database training problems support vector machines 
section presents frontal human face detection system developed application svm computer vision object detection problems 
support vector machines section introduce svm classification technique show leads formulation qp programming problem number variables equal number data points 
start reviewing classical empirical risk minimization approach showing naturally leads theory vc bounds idea structural risk minimization srm better induction principle srm implemented svm 
empirical risk minimization case class pattern recognition task learning examples formulated way set decision functions ff gamma set parameters set examples gamma drawn unknown distribution want find function provides smallest possible value expected risk jf gamma yj dxdy functions usually called hypothesis set ff called hypothesis space denoted expected risk measure hypothesis predicting correct label point set functions example set radial basis functions multi layer perceptron certain number hidden units 
case set set weights network 
probability distribution unknown unable compute minimize expected risk 
access sampling compute stochastic approximation called empirical risk emp jf gamma law large numbers guarantees empirical risk converges probability expected risk common approach consists minimizing empirical risk expected risk 
intuition underlying approach empirical risk minimization principle emp converges minimum emp may converge minimum convergence minimum emp minimum hold empirical risk minimization principle allow inference data set said consistent 
shown vapnik chervonenkis consistency takes place convergence probability emp replaced uniform convergence probability 
vapnik chervonenkis showed necessary sufficient condition consistency empirical risk minimization principle finiteness vc dimension hypothesis space vc dimension hypothesis space vc dimension classifier natural number possibly infinite speaking largest number data points separated possible ways set functions vc dimension measure complexity set necessarily proportional number free parameters classifier theory uniform convergence probability developed vapnik chervonenkis provides bounds deviation empirical risk expected risk 
typical uniform vapnik chervonenkis bound holds probability gamma form emp ln gamma ln vc dimension bound clear order achieve small expected risk generalization performances empirical risk ratio vc dimension number data points small 
empirical risk usually decreasing function turns number data points optimal value vc dimension 
choice appropriate value techniques controlled number free parameters model crucial order get performances especially number data points small 
multilayer perceptron radial basis functions network equivalent problem finding appropriate number hidden units 
problem known difficult usually solved sort cross validation technique 
bound suggests empirical risk minimization principle replaced better induction principle see section 
structural risk minimization technique structural risk minimization developed vapnik attempt overcome problem choosing appropriate vc dimension 
clear eq 
small value empirical risk necessarily imply small value expected risk 
different induction principle called structural risk minimization principle proposed vapnik 
principle observation order expected risk small sides equation small 
vc dimension empirical risk minimized time 
order implement srm principle needs nested structure hypothesis spaces ae ae ae ae property vc dimension set equation suggests disregarding logarithmic factors problem solved min hn emp srm principle clearly founded mathematically difficult implement reasons 
vc dimension difficult compute small number models know compute vc dimension 

assuming compute vc dimension easy solve minimization problem 
cases minimize empirical risk set choose minimizes eq 

implementation principle easy trivial control learning technique training phase 
svm algorithm achieves goal minimizing bound vc dimension number training errors time 
section discuss technique detail show implementation related quadratic programming 
support vector machines mathematical derivation section describe mathematical derivation support vector machine svm developed vapnik 
technique introduced steps consider simplest case linear classifier linearly separable problem linear classifier non separable problem non linear classifier non separable problem interesting useful case 
linear classifier linearly separable problem section consider case data set linearly separable wish find best hyperplane separates data 
purposes linearly separable means find pair delta class delta gamma class hypothesis space case set functions sign delta notice parameters scaled quantity decision surface unchanged 
order remove redundancy decision surface correspond unique pair constraint imposed min jw delta bj points data set 
set hyperplanes satisfy called canonical hyperplanes 
notice linear decision surfaces represented canonical hyperplanes constraint just normalization prove convenient calculations 
constraints imposed pair vc dimension canonical hyperplanes total number free parameters 
order able apply structural risk minimization principle need construct sets hyperplanes varying vc dimension minimize empirical risk training classification error vc dimension time 
structure set canonical hyperplanes defined constraining norm vector fact vapnik shows assume points lie unit dimensional sphere set ff sign delta kwk ag vc dimension satisfies bound ng data points lie inside sphere radius ng 
geometrical reason bounding norm constraints set canonical hyperplanes simple 
shown distance point hyperplane associated pair jx delta bj kwk normalization distance canonical hyperplane closest data points simply kwk kwk distance canonical hyperplane closest data point larger conclude constrained set canonical hyperplanes eq 
set hyperplanes distance data points equivalent placing spheres radius data point consider hyperplanes intersect spheres shown 
set examples linearly separable goal svm find canonical hyperplanes correctly classify data minimum norm equivalently minimum kwk keeping norm small keep vc dimension small 
interesting see minimizing kwk case linear separability equivalent finding separating hyperplane distance convex hulls classes training data measured line perpendicular hyperplane maximized 
rest distance referred margin 
gives geometrical interpretation better generalization expected separating hyperplane large margin 
construct maximum margin optimal separating hyperplane need correctly classify vectors training set different classes gamma smallest norm coefficients 
formulated follows bounding norm equivalent constraint hyperplanes remain outside spheres radius centered data points 
minimize phi kwk subject delta point problem solved standard quadratic programming qp optimization techniques complex dimensionality 
dimension input space problem tractable real applications 
order easily explain extension nonlinear decision surfaces described section look dual problem technique lagrange multipliers 
construct lagrangian kwk gamma delta gamma vector non negative lagrange multipliers corresponding constraints 
solution optimization problem determined saddle point lagrangian minimized respect maximized respect 
differentiating setting results equal zero obtain gamma separating hyperplane small margin 
separating hyperplane larger margin 
better generalization capability expected 
superscript denote optimal values cost function equation derive shows optimal hyperplane solution written linear combination training vectors 
notice training vectors contribute expansion 
substituting obtain gamma kw gamma delta writing matrix notation incorporating non negativity constraint get dual quadratic program maximize delta gamma delta subject delta symmetric theta matrix elements ij delta notice complementary slackness conditions form delta gamma imply constraint active 
vectors called support vectors 
equation follows computed gamma delta support vector linearity dot product equation decision function written sign delta soft margin hyperplane linearly non separable case consider case look linear surface separating hyperplane exist possible satisfy constraints problem 
order deal case introduces new set variables measure amount violation constraints 
margin maximized paying penalty proportional amount constraint violations 
formally solves problem minimize phi xi kwk xi subject delta gamma parameters determined define cost constraints violation 
monotonic convex functions errors defined see general case :10.1.1.15.9362
notice minimizing term amounts minimizing vc dimension learning machine minimizing second term bound 
hand minimizing second term controls empirical risk term bound 
approach constitutes practical implementation structural risk minimization set functions 
order solve problem construct lagrangian xi gamma kwk gamma delta gamma gamma fl non negative multipliers gamma fl fl associated constraints respectively 
solution problem determined saddle point lagrangian minimized respect xi maximized respect gamma 
differentiating setting results equal zero obtain xi gamma gamma xi gamma xi gamma xi kc gamma gamma gamma fl gamma gamma fl denoting ffi ck gamma rewrite equation ffi gamma gamma fl equation obtain substituting obtain ffi gamma delta gamma ffi gamma kc gamma gamma order obtain soft margin separating hyperplane solve maximize ffi delta gamma delta gamma ffi gamma kc gamma gamma subject delta ffi symmetric theta matrix elements ij delta penalizing linearly violations constraint set equations simplifies maximize delta gamma delta subject delta value assumed rest simplifies mathematical formulation shown results practical applications 
linearity dot product equation decision function written sign delta gamma delta support vector support vector correctly classified 
order verify notice complementary slackness conditions form delta gamma imply constraint active establishing need 
hand active due acceptable misclassified point 
equation fl gamma fl multiplier associated constraint fl implies establishing sufficiency notice sufficient condition fl equal zero 
note calculation threshold value assumes existence proof existence conditions exist 
think reasonable assumption equivalent assumption support vector correctly classified 
far computational results indicate assumption correct rest 
nonlinear decision surfaces previous sections treated linear decision surfaces definitely appropriate tasks 
extension complex decision surfaces conceptually quite simple done mapping input variable higher dimensional feature space working linear classification space 
precisely maps input variable possibly infinite vector feature variables oe oe oe oe fa real numbers foe real functions soft margin version svm applied substituting variable new feature vector oe 
mapping solution svm form sign oe delta sign oe delta oe key property sv machinery quantities needs compute scalar products form oe delta oe 
convenient introduce called kernel function oe delta oe oe oe quantity solution svm form sign quadratic programming problem numbers fang clearly unnecessary absorbed definition just formulation easier 
maximize delta gamma delta subject delta symmetric semi positive definite theta matrix elements ij 
notice decision surface nonlinear function linear superposition kernel functions support vector 
idea expanding input space feature space useful find solution problem starting feature space starting kernel 
problem find set coefficients fa set features foe 
scalar product oe delta oe defined example series converges uniformly 
scalar product oe delta oe easy compute function addition requirements require features oe scalar product defines class decision surfaces rich example includes known approximation schemes 
different approaches problem 
starting feature space approach consists choosing carefully set features properties 
example obvious choice take features oe monomials variable certain degree 
assuming simplicity dimensional space choose oe high coefficients equal 
case decision surface linear components oe polynomial degree choice unfortunate scalar product oe delta oe xy xy xy particularly simple compute high 
easy see careful choice parameters things simplify 
fact choosing easy see oe delta oe xy xy considerably reduces computation 
similar result complex structure coefficients true multivariable case dimensionality feature space grows quickly number variables 
example variables define oe case easy see oe delta oe delta straightforward extend example dimensional case 
example dimensions oe scalar product form eq 

dimension features monomials degree oe shown delta shown features monomials degree equal possible find numbers way scalar product delta provide examples choose features careful choice coefficients arrive analytical expression kernel infinite dimensional feature spaces consider dimensional examples 
multidimensional kernels built tensor products dimensional kernels 

consider feature space oe sin sin sin sin nx oe delta oe sin nx sin ny log fi fi fi fi fi sin sin gammay fi fi fi fi fi corresponds choice 
positive number consider feature space oe sin cos sin cos sin nx cos nx oe delta oe sin nx sin ny cos nx cos ny gamma gamma cos gamma corresponds choice 
examples infinite number features countable 
construct cases number features infinite uncountable 
consider map oe ix deltas oe fourier transform positive definite function simplicity complex features 
corresponds kernel oe delta oe ds gammay deltas gamma corresponds continuum coefficients 
starting kernel approach consists looking kernel known representation form set oe explicit analytic form may known 
order find solution problem need preliminary facts 
call positive definite kernel function omega theta omega gamma omega ae property omega assume omega kernel defines integral operator known complete system orthonormal eigenfunctions omega dy oe oe stewart reported theorem mercer statements equivalent 
function positive definite kernel 
dxdy 
eigenvalues eq 
positive 
series oe oe converges absolutely uniformly 
leads statement feature vector oe oe oe oe fa foe respectively eigenvalues eigenfunctions positive definite kernel solve problem scalar product oe delta oe simple expression oe delta oe number observations order ffl vapnik uses condition characterize kernels svm 
definition practical prove admissibility certain kernel 
ffl result similar mercer general 
young proves kernel positive definite omega dxdy omega gamma ffl kernels represent scalar product feature space closely related theory reproducing kernel hilbert spaces rkhs see appendix girosi 
fact moore considers general setting positive definite kernels replaces omega eq 
set calls functions positive hermitian matrices shows associate rkhs 
table report commonly kernels 
kernel function type classifier exp gamma yk gaussian rbf delta polynomial degree tanh delta gamma values multi layer perceptron table possible kernel functions type decision surface define 
additional geometrical interpretation just shows better generalization expected maximizing margin wonder support vectors geometrical common characteristic 
just scattered points linear combination 
turns 
order find optimal decision surface support vector training algorithm tries separate best possible clouds defined data points classes 
particularly expect points closer boundary classes important solution data points far away harder classify 
data points sense help shape define better decision surface points 
support vectors geometrical point view border points 
direct consequence preceding argument delivers important geometrical algorithmic property usually support vector 
ideas justified algebraically optimality conditions derived section 
shows examples preceding geometrical interpretations polynomial rbf classifiers 
interesting extension weighted svm original formulation svm existing literature extended handle frequent cases pattern classification recognition ffl unequal proportion data samples classes 
ffl need tilt balance weight class versus frequent classification error type expensive undesirable 
decision surfaces polynomial classifier rbf support vectors indicated dark fill 
notice reduce number position close boundary 
support vectors rbf centers 
way derive extension allow equation maximize delta gamma delta subject delta gamma gamma symmetric matrix elements ij gamma positive constants 
equation min phi xi kwk gamma gamma equations remain unchanged 
quadratic program interpreted penalizing higher penalty gamma undesirable type error equation important notice extension real impact complexity problem finding optimal vector multipliers bounding box constraints changed 
notice extension changed allow example higher values highly reliable valuable data points lower values data points confidence value 
training support vector machine solving quadratic program determines value desired decision surface equation optimization process referred training support vector machine 
section covers previous current possible approaches solving problem 
important characteristic quadratic form matrix appears objective function symmetric completely dense size square number data vectors 
fact implies due memory computational constraints problems large data sets samples solved kind data problem decomposition 
section deals approaches solving small problems constitute natural step decomposition algorithm described section iteratively solves small subproblems type 
previous training svm small data sets approached vapnik constrained conjugate gradient algorithm 
briefly described conjugate gradient ascent explore feasible region step move solution outside 
happened largest step conjugate direction taken maintaining feasibility 
time variable reached corresponding data point removed reducing approximating solution conjugate gradient process re started 
approach taken vapnik adapt problem algorithm bounded large scale quadratic programs due mor 
originally algorithm uses conjugate gradient explore face feasible region defined current iterate gradient projection move different face 
main modification consider binding frozen variables equal bounds movement gradient take outside feasible region 
process research training problem small data sets approached different algorithms computer packages method feasible directions cplex solve lp gams minos gams modeling language minos solver second order variant reduced gradient method algorithm implemented minos 
summary approaches computational results reported methods description method case linear constraints order solve nonlinear problem form maximize subject ax ex method follows skeletal approach 
find ex partitioning 

find optimal solution maximize rf delta subject ed gamma 
rf delta 
go 

find step size ffi solution maximize subject ffi ffi max ffi max min gamma 
ffi 
go step 
step involves solving linear program usually easy case training svm step maximize gamma delta subject delta gamma gamma step selects ffi min ffi opt ffi max ffi opt delta gamma delta gamma delta dd max min min gamma min gamma interesting modification done algorithm order help speed computer implementation solve problem times increasing upper bound starting value usually low scaled times reached original value 
solutions scaled starting point iteration 
computational point view method behaved lot better naive constrained conjugate gradient implementation terms speed graceful degradation increase hand implementation serious difficulties cases strictly bounds 
slow convergence allowed gams minos minos outperform orders magnitude 
gams minos gams modeling language allows fast description maintainability optimization problems 
language gams generates specified model calls user specified solver depending type problem hand 
case nonlinear programs minos solvers 
done gams minos important 
offered verification implementation method point comparison terms speed accuracy important pointed idea minos directly overhead gams represent 
reason considering important done gams minos improvement training speed due problem reformulation 
original problem rewritten maximize omega delta gamma delta omega omega subject delta omega strange sight transformation allows faster function gradient evaluation responsible important speedup steps solution model generation optimization 
reason formulation minos 
minos minos solves nonlinear problems linear constraints wolfe reduced gradient algorithm conjunction davidson quasi newton method 
details implementation described murtagh saunders minos user guide overview heuristics comparisons 
wolfe reduced gradient method depends reducing dimensionality problem representing variables terms independent set 
non degeneracy assumptions facilitate brief description program form minimize subject ax decomposed xn non singular xb xn 
denoting gradient rf rb rn direction vector db dn system ad holds choice dn letting db gammab gamma dn defining rf gamma rb gamma rn gamma rb gamma reduced gradient follows rf delta delta dn order feasible direction improving feasible direction feasibility rf delta vector dn chosen delta dn 
accomplished choosing db gammab gamma gammar gammax determining improving feasible direction line search procedure determine step size improved solution obtained 
reduced gradient methods allow components dn non zero 
opposite side example simplex method linear programming examines similar direction finding problem allow component dn non zero time 
interesting see second strategy looks restrictive result slow progress small step sizes possible due fact components changing simultaneously 
order reach compromise strategies mentioned set non basic variables xn partitioned xn corresponding decomposition dn dn 
variables called variables intended driving force iterates xn fixed xb adjusted maintain feasibility 
notice direction vector accordingly decomposed linear operator form db dn gammab gamma zd search direction surface active constraints characterized range matrix orthogonal matrix constraint normals az theta gammab gamma expressing directional derivative rf delta rf delta rf delta zd gamma rb gamma delta gamma rb gamma direction finding problem reduced minimize delta subject gammax jr jr direction finding problem described equation uses linear approximation objective function slow convergence happen contours flat thin directions 
expect faster convergence approach upgraded second order approximation formally goal minimize second order approximation direction finding problem rf delta delta linear manifold ad 
equations transforms minfr delta delta zd matrix called reduced hessian 
setting gradient equal zero results system equations zd gammar available line search direction zd performed new solution obtained 
minos implements certain computational highlights 
algorithm appears improvement current partition kr suitably chosen tolerance level non basic variables added set 
multiple pricing option minos allows user specify incorporate 

iteration basic variable reaches bounds variable non basic 

matrices computed implicitly 
reduced hessian matrix approximated dense upper triangular matrix 

sparse lu factorization basis matrix 
computational results order compare relative speed methods different problems small data sets solved computational environment 
training svm linear classifier ripley data set 
data set consists samples dimensions linearly separable 
table shows points comparison ffl difference gams minos original problem transformed version 
ffl performance degradation suffered conjugate gradient implementation increase upper bound opposite hand negligible effect gams minos modified minos 
ffl considerable advantage performance minos 
training svm third degree polynomial classifier sonar data set 
data set consists samples dimensions linearly separable polynomially separable 
results experiments shown table exhibit points comparison ffl difficulty experienced order methods method converge values strictly bounds 
ffl clear advantage solving problem directly minos removing overhead created gams incorporating knowledge problem solution process example fast exact gradient evaluation symmetry constraint matrix ffl negligible effect upper bound performance minos 
important computational result sub linear dependence training time dimensionality input data 
order show dependence table presents training time data points problems different dimensionality separability upper bound methods conjugate gradient gams minos gams minos modified minos sec sec sec sec sec sec sec sec sec sec sec sec sec sec sec table training time ripley data set different methods upper bound gams minos modified corresponds reformulated version problem 
methods gams minos modified minos sec sec sec sec sec sec sec table training time sonar dataset different methods upper bound new approach large database training mentioned training svm large data sets samples difficult problem approach kind data problem decomposition 
give idea memory requirements application described section involves training samples amounts quadratic form matrix delta entries need byte floating point representation megabytes gigabytes memory 
order solve training problem efficiently take explicit advantage geometric interpretation introduced section particular expectation number support vectors 
consider quadratic programming problem expectation translates having components equal zero 
order decompose original problem think solving iteratively system keeping fixed zero level components associated data points support vectors optimizing reduced set variables 
convert previous description algorithm need specify 
optimality conditions conditions allow decide computationally problem solved optimally particular global iteration original problem 
section states proves optimality conditions qp 

strategy improvement particular solution optimal strategy defines way improve cost function frequently associated variables violate optimality conditions 
strategy stated section 
presenting optimality conditions strategy improving cost function section introduces decomposition algorithm solve large database training problems section reports computational results obtained implementation 
optimality conditions order consistent common standard notation nonlinear optimization problems quadratic program rewritten minimization form minimize gamma delta delta subject delta gamma upsilon gamma pi upsilon ae ae pi associated kuhn tucker multipliers 
positive semi definite matrix see section constraints linear kuhn tucker kt conditions necessary sufficient optimality dimension separable non separable sec sec sec sec sec sec sec sec sec sec sec sec sec sec sec sec sec sec table training time randomly generated dataset different dimensionality upper bound rw upsilon gamma pi upsilon delta gamma pi delta upsilon pi delta gamma gamma order derive algebraic expressions optimality conditions assume existence see section consider possible values component 
case equations kt conditions gamma noticing sign obtain gamma gamma substituting obtain optimal solution value multiplier equal optimal threshold 
case equations kt conditions gamma ae defining noticing gamma equation written gamma gamma ae combining derived case requiring ae obtain 
case equations kt conditions gamma gamma applying similar algebraic manipulation described case obtain strategy improvement order incorporate optimality conditions expectation zero algorithm need derive way improve objective function value information 
decompose vectors partition index set optimality conditions hold subproblem defined variables sections set referred working set 
decomposition statements clearly true ffl replace changing cost function feasibility subproblem original problem 
ffl replacement new subproblem optimal 
follows equation assumption subproblem optimal replacement done 
previous statements suggest replacing variables zero levels subproblem variables violate optimality condition yields subproblem optimized improves cost function maintaining feasibility 
proposition states idea formally 
proposition optimal solution subproblem defined operation replacing satisfying generates new subproblem optimized yields strict improvement objective function 
proof assume existence assume loss generality proof analogous gammay 
ffl gamma ffi ffi ffl 
notice consider ffi gamma ffie jth pth unit vectors notice pivot operation handled implicitly letting ffi holding 
new cost function written gamma delta delta gamma delta delta delta ffie gamma ffie ffie gamma ffi delta ffie gamma ffi ffi gamma gamma ffi gamma ffi gamma ffi gamma choosing ffi small 
decomposition algorithm suppose define fixed size working set jbj big contain support vectors small computer handle optimize solver 
decomposition algorithm stated follows 
arbitrarily choose jbj points data set 

solve subproblem defined variables 
exists replace solve new subproblem 
notice algorithm strictly improve objective function iteration cycle 
objective function bounded convex quadratic feasible region bounded algorithm converge global optimal solution finite number iterations 
gives geometric interpretation way decomposition algorithm allows redefinition separating surface adding points violate optimality conditions 
computational implementation results implemented decomposition algorithm transformed problem defined equation minos solver 
notice decomposition algorithm flexible pivoting strategy way decides new points incorporate working set implementation uses parameters define desired strategy ffl lookahead parameter specifies maximum number data points pricing subroutine evaluate optimality conditions case 
lookahead data points examined finding violating subroutine continues finds data points examined 
case global optimality obtained 
ffl parameter limits number new points incorporated working set sub optimal solution non filled points violating optimality conditions inside sigma area 
decision surface gets redefined 
points inside sigma area solution optimal 
notice size margin decreased shape decision surface changed 
computational results section obtained real data face detection system described section 
shows training time number support vectors obtained training system data points 
emphasize data points collected phase bootstrapping face detection system training process harder correspond errors obtained system accurate 
shows relationship training time number support vectors number global iterations number times decomposition algorithm calls solver 
notice smooth relation number support vectors training time jump global iterations go samples 
increase responsible increase training time 
system working set variables able solve data points problem mb ram 
shows effect training time due parameter size working set data points 
notice clear improvement increased 
improvement suggests way faster new violating data points brought working set faster decision surface defined optimality reached 
notice working set small big compared number support vectors case samples training time increases 
case happens algorithm incorporate new points slowly second case happens solver takes longer solve subproblem size working set increases 
improving training svm directions algorithm described section suggests main areas improvements research 
areas number samples number samples training time sparcstation 
number support vectors obtained training 
solver second order variant reduced gradient method implemented minos results far terms accuracy robustness performance 
method general nonlinear optimization method designed particular quadratic programs case svm designed particular special characteristics problem 
having experience obtained minos new approaches tailored solver example projected newton interior point methods attempted 
point clear type algorithm appropriate stages solution process 
specific happen algorithm performs non zero variables early stages outperformed number non zero variables reaches threshold 
particular learned number non zero variables satisfy important effect performance solver 

pivoting strategy area offers great potential improvements ideas plan implement 
improvements qualitative characteristics training process observed ffl execution algorithm computational effort dedicated evaluation optimality conditions 
final stages common data points evaluated collect incorporate working set 
ffl small portion input data brought working set case face detection application 
ffl samples go working set enter exit set 
vectors responsible characteristic mentioned 
possible strategies exploit characteristics ffl keep list file part input vectors exited working set 
pricing stage algorithm computes optimality conditions evaluate data number support vectors number samples number support vectors versus training time sparcstation 
notice number support vectors better indicator increase training time number samples 
number global iterations performed algorithm 
notice increase experimented going samples 
increase number iterations responsible increase training time data points determine entering vectors 
strategy analogous revised simplex method algorithm keeps track basic variables non basic 
case training svm geometric interpretation heuristic think point support vector iteration close boundary classes boundary refined fine tuned possible switch active non active times 
heuristic combined main memory cache management policies computer systems 
ffl pricing stage bringing working set points violate optimality conditions try determine violating data points choose violating points 
done geometric idea violating points help defining decision surface faster save time iterations 
ffl approaches combined keeping track points exiting working set remaining violating data points 
far description implementation decomposition algorithm assumed memory available solve working set problem contains support vectors 
applications may require support vectors available memory manage 
possible approach taken approximate optimal solution best solution obtained current working set size 
algorithm implementation easily extended handle situation replacing support vectors new data points 
complex approaches pursued obtain optimal solution subject research 
working set size training time samples different values working set size lookahead 
training time samples different sizes working set size working set lookahead 
svm application face detection images section introduces support vector machine application detecting vertically oriented unoccluded frontal views human faces grey level images 
handles faces wide range scales works different lighting conditions moderately strong shadows 
face detection problem defined follows input arbitrary image digitized video signal scanned photograph determine human faces image return encoding location 
encoding system fit face bounding box defined image coordinates corners 
face detection computer vision task applications 
direct relevance face recognition problem important step fully automatic human face recognizer usually identifying locating faces unknown image 
face detection potential application human computer interfaces surveillance systems census systems standpoint face detection interesting example natural challenging problem demonstrating testing potentials support vector machines 
object classes phenomena real world share similar characteristics example tumor anomalies mri scans structural defects manufactured parts successful general methodology finding faces svm generalize spatially defined pattern feature detection problems 
important face detection object detection problems difficult task due significant pattern variations hard parameterize analytically 
common sources pattern variations facial appearance expression presence absence common structural features glasses light source distribution shadows system works testing candidate image locations local patterns appear faces classification procedure determines local image pattern face 
face detection problem approached classification problem examples classes faces non faces 
previous systems problem face detection approached different techniques years 
techniques include neural networks detection face features geometrical constraints density estimation training data labeled graphs clustering distribution modeling 
previous works results sung poggio rowley reflect systems high detection rates low false positive detection rates 
sung poggio clustering distance metrics model distribution face non face manifold neural network classify new pattern measurements 
key quality result clustering combined mahalanobis euclidean metrics measure distance new pattern clusters 
important features approach non face clusters bootstrapping technique collect important non face patterns 
drawback technique provide principled way choose important free parameters number clusters uses 
similarly rowley problem information design connected neural network trained classify faces non faces patterns 
approach relies training nn emphasizing subsets training data order obtain different sets weights 
different schemes arbitration order reach final answer 
approach face detection system svm uses prior information order obtain decision surface interesting property exploited approach detecting objects digital images 
svm face detection system system described detects faces exhaustively scanning image face patterns possible scales dividing original image overlapping sub images classifying svm determine appropriate class face non face 
multiple scales handled examining windows taken scaled versions original image 
clearly major svm classification step constitutes critical important part 
gives geometrical interpretation way svm context face detection 
specifically system works follows 
database face non face theta pixel patterns assigned classes respectively trained support vector algorithm 
nd degree polynomial kernel function upper bound process obtaining perfect training error 

order compensate certain sources image variation preprocessing data performed ffl masking binary pixel mask remove pixels close boundary window pattern allowing reduction dimensionality input space theta 
step important reduction background patterns introduce unnecessary noise training process 
ffl illumination gradient correction best fit brightness plane subtracted unmasked window pixel values allowing reduction light heavy shadows 
ffl histogram equalization histogram equalization performed patterns order compensate differences illumination brightness different cameras response curves 
decision surface obtained training run time system images contain faces misclassifications stored negative examples subsequent training phases 
images landscapes trees buildings rocks sources false positives due different textured patterns contain 
bootstrapping step successfully sung poggio important context face detector learns examples ffl negative examples abundant negative examples useful learning point view difficult characterize define 
ffl approaching problem object detection case face detection paradigm binary pattern classification classes object non object equally complex non object class broader richer needs examples order get accurate definition separates object class 
shows image bootstrapping misclassifications negative examples 

training svm incorporate classifier run time system similar sung poggio performs operations ffl re scale input image times 
ffl cut theta window patterns scaled image 
ffl preprocess window masking light correction equalization 
ffl classify pattern svm 
ffl class corresponds face draw face output image 
experimental results test run time system sets images 
set contained high quality images number faces 
set contained images mixed quality total faces 
sets tested system sung poggio 
order give true meaning number false positives obtained important state set involved pattern windows set 
table shows comparison systems 
test set test set detection rate false detections detection rate false detections ideal system svm sung poggio table performance svm face detection system figures output images system 
images training phase system 
directions face detection svm applications research svm application divided main categories topics 
simplification svm drawback svm real life applications large number arithmetic operations necessary classify new input vector 
usually number proportional dimension input vector number support vectors obtained 
case face detection example multiplications pattern 
reason overhead roots technique svm define decision surface explicitly data points 
situation causes lot redundancy cases solved relaxing constraint data points define decision surface 
topic current research conducted burges bell laboratories great interest order simplify set support vectors needs solve highly nonlinear constrained optimization problems 
closed form solution exists case kernel functions nd 
degree polynomials simplified svm current experimental face detection system gains acceleration factor degrading quality classifications 

detection objects interested svm detect objects digital images cars airplanes pedestrians notice objects different appearance depending viewpoint 
context face detection interesting extension lead better understanding approach problems detection tilted rotated faces 
clear point different view object treated single classifier treated separately 

multiple classifiers multiple classifiers offers possibilities faster accurate 
rowley successfully combined output different neural networks means different schemes arbitration face detection problem 
sung poggio classifier fast way quickly discard patterns clearly non faces 
just examples combination different classifiers produce better systems 
current experimental face detection system performs initial quick discarding step svm trained separate clearly non faces probable faces just averages taken different areas window pattern 
classification done times faster currently discarding input patterns 
done near area 
classifiers combined kind 
interesting type classifier consider discriminant adaptative nearest neighbor due hastie 
novel decomposition algorithm train support vector machines 
successfully implemented algorithm solved large dataset problems acceptable amounts computer time memory 
area continued currently studying new techniques improve performance quality training process 
support vector machine new technique far know presents second problem solving application svm vapnik character recognition problem 
face detection system performs state art systems opened interesting questions possible extensions 
object detection point view ultimate goal develop general methodology extends results obtained faces handle objects 
broader point view consider interesting function approximation regression extension vapnik done svm different areas neural networks currently 
important contribution application techniques domains statistical learning artificial intelligence 
believe tools duality theory interior point methods optimization techniques concepts useful obtaining better algorithms implementations solid mathematical background 
sherali shetty 
nonlinear programming theory algorithms 
wiley new york nd edition 
bertsekas 
projected newton methods optimization problems simple constraints 
siam control optimization march 
boser guyon vapnik 
training algorithm optimal margin classifier 
proc 
th acm workshop computational learning theory pages pittsburgh pa july 
carel 
detection localization faces digital images 
pattern recognition letters 
burges vapnik 
new method constructing artificial neural networks 
technical report bell laboratories corner road holmdel nj may 
burges 
simplified support vector decision rules 
carpenter shanno 
interior point method quadratic programs conjugate projected gradients 
computational optimization applications june 
cortes vapnik :10.1.1.15.9362
support vector networks 
machine learning 
girosi 
equivalence sparse approximation support vector machines 
memo mit artificial intelligence laboratory 
available url www ai mit edu people girosi svm html 
hastie buja tibshirani 
penalized discriminant analysis 
technical report statistics data analysis research department bell laboratories murray hill new jersey may 
hastie tibshirani 
adaptative nearest neighbor classification 
technical report department statistics division biostatistics stanford university december 
kruger malsburg 
determination face position pose learned representation graphs 
technical report ruhr universitat january 
mercer 
functions positive negative type connection theory integral equations 
philos 
trans 
roy 
soc 
london 
moghaddam pentland 
probabilistic visual learning object detection 
technical report mit media laboratory june 
moore 
properly positive hermitian matrices 
bull 
amer 
math 
soc 
mor 
solution large quadratic programming problems bound constraints 
siam optimization february 
murtagh saunders 
large scale linearly constrained optimization 
mathematical programming 
murtagh saunders 
minos user guide 
system optimization laboratory stanford university feb 
rowley baluja kanade 
human face detection visual scenes 
technical report cmu cs school computer science carnegie mellon university november 
stewart 
positive definite functions generalizations historical survey 
rocky mountain math 
sung 
learning example selection object pattern detection 
phd thesis massachusetts institute technology artificial intelligence laboratory center computational learning december 
sung poggio 
example learning view human face detection 
memo december 
vapnik 
estimation dependences empirical data 
springer verlag 
vapnik 
nature statistical learning theory 
springer verlag 
vapnik chervonenkis 
uniform convergence relative events probabilities 
th 
prob 
applications 
vapnik ya 
chervonenkis 
necessary sufficient conditions consistency empirical risk minimization method 
pattern recognition image analysis 
yang huang 
human face detection complex background 
pattern recognition 
young 
note class symmetric functions theorem required theory integral equations 
philos 
trans 
roy 
soc 
london 

methods feasible directions 
van princeton 
false detections obtained version system 
false positives negative examples class training process non faces faces geometrical interpretation svm separates face non face classes 
patterns real support vectors obtained training system 
notice small number total support vectors fact higher proportion correspond non faces 
faces faces faces faces faces faces 
