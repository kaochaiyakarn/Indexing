server directed collective panda seamons chen jones winslett computer science department university illinois urbana illinois ying cs uiuc edu keywords collective parallel architecture implementation results panda library input output multidimensional arrays parallel sequential platforms 
panda achieves remarkable performance levels ibm sp showing excellent scalability data size increases number nodes increases provides throughputs close full capacity aix file system sp 
argue performance traced panda server directed logical level version kotz perform array sequential disk reads writes high level interface collective requests built facilities arbitrary rearrangements arrays advantages panda approach ease easy application portability reliance commodity system software 
past years researchers hpcc community suggested approaches improve performance scientific applications massively parallel platforms 
described focus problem combining proposed techniques thought offered exceptional promise show fact fortuitous combination 
storage arrays subarray chunks main memory disk 
high level interfaces subsystems 
disk directed efficient disk bandwidth technique array chunking main memory decades means handling core computations way decomposing arrays computation parallel platforms 
array chunking memory improves performance increasing locality computation processor 
array chunking disk proposed number researchers way allowing fast strided access disk data parallel platforms researchers assume eventually array data placed traditional row major column major order consumption applications sequential platforms 
somewhat unusual see intrinsic value chunked arrangements schemas array data disk schemas general improve performance data consumers sequential platforms increase locality data multiple dimensions typically reducing number disk accesses application obtain working set data memory 
research supported arpa fellowship high performance computing administered institute advanced computer studies university maryland nsf pyi iri nasa ncc 
high level interfaces legacy group community origin database world 
high level interfaces give flexibility optimize beneath level interface 
high level interfaces give cleaner view situations just printout source code program meaningful human hexadecimal dump code 
cleaner view means higher level semantic understanding requests extra semantic information intelligent optimization decisions 
addition higher level interfaces easier users master give applications better portability 
disk directed kotz described sections uses information collective request issued group compute nodes 
description entire request digested nodes plan receive data compute nodes order minimize disk seeks 
nodes request pieces data order want compute nodes 
think illustration advantage having higher level semantic knowledge 
disk directed best knowledge implemented real systems results simulations look promising 
combined approaches discussed new version panda array library url bunny cs uiuc edu cadr panda html tested sequential unix workstations ibm sp 
panda panda runs top ordinary unix file systems uses mpi communication written compact 
panda combines high level interfaces interest disk directed panda implements disk directed call server directed details architecture discussed section 
academic interest panda little value offer significant performance advantages 
performance tests panda ibm sp panda provided excellent bandwidth wide variety conditions providing throughputs close full capacity underlying aix file system 
panda performance scaled increasing numbers nodes array data size 
performance results discussed section followed related section summary section 
panda internal architecture panda designed support synchronized collective spmd style application programs ordinary workstations distributed memory parallel architectures networks workstations 
philosophy panda maximize performance doing sequential reads writes possible 
high performance scientific applications typically spend time reading writing entire arrays theoretically sequential reads writes possible current systems support parallel offer sufficiently high level interface system realize random pattern read write requests arriving nodes viewed sequential pattern 
panda high level api described avoids pitfall 
second difficulty realizing sequential reads writes requests compute nodes may arrive nodes manner discourages sequential access pattern sequential access known compute nodes nodes 
panda avoids problem supporting collective operations server directed strategy nodes compute nodes direct flow requests 
nodes understand layouts array data files disk memory nodes sequentially read data files scattering data compute nodes arrives disk employing reverse strategy writes 
approach normal unix file system prefetching caching perfectly avoiding myriad buffering errors caused poor performance early parallel systems 
files laid sequentially disk generally case huge data files employed target applications sequential file reads translate inexpensive sequential disk reads cases 
server directed possible means achieving sequential show performs closely synchronized sets compute nodes removing reliance costly custom system software 
system architecture panda shown shows panda distributed compute nodes panda clients nodes panda servers 
server directed application program running compute nodes communicates client panda high level collective mpi file system mpi file system mpi file system panda collective interface compute nodes interconnect network nodes panda clients mpi mpi mpi mpi panda servers panda server directed architecture interface multidimensional arrays anticipate current architecture extensible additional data types 
panda currently supports applications distribute arrays compute nodes block array schemas 
default performance convenience panda uses disk schema identical memory schema 
refer natural chunking seamons 
users may override default declaring block schema disk useful users know data accessed wish optimize 
schemas disk memory may decompose arrays radically different ways illustrated example discussed 
panda assumes clients participate collective approximately time application need synchronize prior collective call 
panda high level interface array supports collective level requests checkpoint computation restart computation checkpoint files save output timestep computations requests write single array group arrays 
contains sample application code uses panda classes methods output timestep data note easy interface 
application collective request panda selected client master client sends selected server master server short high level description schemas arrays memory disk 
note clients servers play different role traditional client server architectures clients requests server 
initial request servers collective operation panda servers data requests clients 
master server informs servers schema information server plans request send chunks array data relevant clients 
case write operations servers know array data chunked disk described disk schema chunks implicitly assigned round robin fashion servers 
note high level interfaces led stripe data chunk level disk block level 
arrays striped nodes traditional row major column major order disk block size kotz kotz del rosario 
server plan assembles assigned chunk sending request logical sub chunk clients hold part assigned chunk 
depending memory schema disk schema client receive request sub chunk array data contiguous memory client may hold receive request 
client responsible reorganization required assemble requested sub chunk happens 
emphasis high level interfaces panda gives easy support strided requests clients servers send logical requests sub chunks 
allows clients servers choose local optimization strategies appropriate 
server responsible reorganizing received sub chunks form entire chunk traditional array order 
server sends assigned chunk disk begins assemble second assigned chunk 
reverse procedure holds reads 
panda strategy assumes clients servers sufficient memory buffer requested array sub chunk memory needed 
limit buffer space requirements maximize performance panda uses form sub chunking disk internal subdivision chunks smaller chunks break large disk chunks manageable units fly performing collective 
experimentation chose size mb experiments 
happens transparently user panda client change memory schema disk schema round robin assignment chunks way 
allow users explicitly request sub chunked schemas memory disk advantageous applications 
servers communicate plan formation array data gathered scattered clients clients 
completion collective request servers inform master server informs master client collective operation completed 
master client turn informs clients 
panda performance results tested performance panda server directed architecture ibm sp nasa ames research center 
table details characteristics ibm sp nas 
detailed information nas nasa gov parallel sp 
nas sp currently parallel facilities processor aix file system 
panda uses aix file system directly node storing array data 
partitions store array data approximately full time experiments 
experiments measure panda performance read write single array multiple arrays 
read write operations primitive operations panda underlie panda timestep checkpoint restart operations 
runs vary number compute nodes number nodes size arrays disk schemas 
runs fewer nodes 
preferred runs large numbers nodes able get large blocks machine time necessary larger runs 
flush file system buffer read operation writing large temporary file deleting file 
flush data disk fsync write operation 
performance data repeated test times average elapsed time 
elapsed time maximum time spent compute node collective request 
basis normalized throughput percentage measured underlying aix file system performance single nodes sp measuring throughput mb obtained reading writing file aix file system calls 
tests accessed files total size mb mb mb requests done panda array chunks larger mb 
peak throughputs obtained included table 
panda normalized throughput array reads writes calculated dividing panda throughput node peak aix throughput 
conducted tests read write single array size mb natural chunking block block block data schema memory disk 
configuration array chunks transferred panda clients servers little processing overhead panda 
peak message passing throughput mb mpi sp far exceeds peak measured aix file system throughput reads writes mb disk 
expect panda throughput node close maximum throughput aix file system 
panda scales reads writes varying size array number compute nodes number nodes tests conducted natural chunking 
tested combinations compute nodes nodes number compute nodes theta theta mesh theta theta mesh theta theta mesh number nodes 
tests total number nodes nodes node rs workstation processor mhz power multi chip risc node operating system aix operating system languages supported fortran fortran total memory gb total memory node mb total disk space gb total disk space node gb total memory bandwidth gb peak performance gflops high performance switch bandwidth hardware mb bidirectional disk peak transfer rate mb bus scsi bus peak transfer rate mb node file system block size kb measured peak throughput aix file system reads mb measured peak throughput aix file system writes mb nas measured message passing latency microseconds nas measured message passing bandwidth mb table system characteristics nas ibm sp 
amount data processor array chunk size compute nodes ranges mb processor mb processor depending array size number compute nodes 
figures show aggregate normalized throughput reads writes compute nodes 
throughputs compute nodes shown implied subsequent figures similar panda scales number compute nodes increases long array chunks remain large mpi latency dominate time transfer array chunk 
predicted throughputs high reads writes peak aix performance node 
array size increases sequential reads writes reach maximum mb size maximum chunk size panda transfers clients servers 
larger chunks broken mb reduce buffer space requirements nodes 
chunk size processor decreases throughput declines underlying aix file system throughput declines writing small file write size mb 
bottleneck nas ibm sp tests conducted natural chunking relatively slow mb peak disk transfer rate 
show panda able efficient available message passing bandwidth simulated infinitely fast disk commenting actual file system open close write read commands panda server code 
simulating fast disks potential system bottleneck peak mpi message passing bandwidth latency ibm sp 
repeated tests writing reading arrays natural chunking simulated disks 
case normalized throughput node throughput node divided peak message traffic bandwidth mb writes reads 
throughputs similar reads writes gathering scattering array data panda servers clients essentially identical respect total number messages message sizes 
figures show throughput compute nodes natural chunking simulating infinitely fast disk results similar compute nodes 
predicted throughputs reads writes similar near peak mpi performance cases 
array size decreases total elapsed time small due high throughput startup overhead panda measured approximately seconds begins dominate elapsed time causing normalized throughput decline startup costs included elapsed time 
panda server directed architecture array data automatically reorganized inmemory schema disk schema differ 
reorganization occurs operation array data transferred panda clients servers 
example suppose mb array size theta theta distributed block block block processors theta theta mesh 
declaring block disk schema place array traditional order disks data migrated sequential machine array single file traditional order simply concatenating files nodes 
traditional order particularly important today applications conducted experiments reading writing array data block disk schema block block block memory schema 
conducted experiments combinations compute nodes nodes number compute nodes theta theta mesh theta theta mesh theta theta mesh theta theta mesh number nodes 
logical node mesh block distribution disk thought theta theta number nodes 
figures show aggregate normalized throughput reading writing arrays traditional order 
throughputs high peak aix performance node slightly lower obtained natural chunking 
compared natural chunking extra messages extra mpi overhead required handle strided requests reorganize data memory disk schemas 
memory network bandwidth high relative disk bandwidth sp overheads reorganization compared natural chunking significant tests 
normalized tests conducted tests simulating infinitely fast disk observe panda utilizes network bandwidth 
shows aggregate normalized throughput writing arrays traditional order simulating infinitely fast disk reads similar 
compared natural chunking simulations cost reorganization visible throughput reads writes ranges peak mpi performance 
believe throughputs improved non blocking communication performing data rearrangement 
experiments described previously load balanced respect amount data processor amount data read written node 
natural chunking array chunks may unevenly distributed nodes number nodes evenly divide number compute nodes 
fortunately number compute nodes increases load imbalance significant fixed number nodes 
addition schema traditional order schemas just chosen distributes data evenly nodes 
reasons consider load imbalance significant issue panda 
conducted additional experiments reading writing multiple arrays due space limitations 
panda achieves high throughputs reading writing multiple arrays similar throughput single arrays size array chunks large mpi latency bottleneck 
related space permit proper discussion related panda limit discussion papers hpcc community appeared focus parallel implementations collective studies characterizing dynamic patterns scientific applications kotz pasquale show scientific applications regular patterns behavior physical periodicity strided access multidimensional arrays temporal periodicity checkpoint restart operations 
cases application compute nodes participate operations access array data 
observation behaviors led interest supporting efficient collective operations parallel file systems parallel libraries 
collective interface application compute nodes issue small number requests gather large amount data passing semantic information associated requests 
collective implementation opportunities disk optimization form large orderly contiguous disk requests servicing disk requests arrive random order 
parallel file systems multidimensional array libraries provided collective interface bennett corbett corbett del rosario seamons seligman shown collective provides high performance appropriate language compiler run time system support 
approaches optimization discussed kotz traditional caching phase disk directed traditional caching support collective interface requests served arrive 
node file cache prefetches optimizing sequential file accesses 
multiprocessor environment file accesses may sequential viewed level typically look strided viewed level individual compute node 
high level semantic view collective requests file system able predict sequential prefetching useful flush file cache 
intel cfs pierce uses traditional caching kotz shows cfs uses half raw disk bandwidth 
considers phase access strategy collective approach read operations compute nodes cooperate bring data memory way minimizes total number disk accesses having data layout memory conform data layout disk 
second phase compute nodes permute data memory node data needs 
disk directed collective operations proposed kotz 
approach compute nodes tell nodes collective request plan perform 
semantic information nodes determine compute nodes ask data send data 
nodes plan data accesses able form large contiguous disk request smaller requests 
simulation kotz promises performance 
kotz compares simulated performance disk directed traditional caching irregular distributions data shows disk directed slower test cases 
kotz uses approach implement core lu decomposition problem shows better traditional caching scheme 
approaches collective buffers requests issuing disk requests 
approach processor contributes requests buffer buffers gathered written read master processor 
bennett gives strategy minimize number requests coalescing large number requests compute nodes big request 
discussed approach compute nodes responsible figuring file get put data pass information nodes necessary disk accesses 
conceptually question read write file low level task properly belonging specialized routines normal scientific programmer wants deal relieve application code task panda shown 
implementation approaches described disk directed promise best performance 
disadvantages implementing disk directed physical level intended creators 
chief operating system file system dependencies implementations disk directed making ports difficult requirement custom system software making difficult architectures network workstations 
problems mind chose apply idea disk directed logical level incorporating panda top native file system 
performance study previous section showed logical level disk directed offers performance advantages 
described architecture performance panda offers high level interface array parallel sequential machines 
panda application program interface supports model level array traditional unix file system level 
physical details mapping array data files implementation algorithm perform mapping entirely invisible application programmer 
expect price encapsulating physical storage details drop performance demonstrates high performance array attainable high level interface fact combining portability high performance may possible high level interface 
expect high level interfaces panda interfaces choice scientific applications 
provide simplicity portability importantly allow efficient underlying implementation 
panda server directed architecture described prime example efficient underlying implementation array server directed architecture targeted spmd style applications distributed memory parallel machines network workstation environments read write memory arrays distributed processors style hpf 
intended applications closely synchronized referred collective 
high level interface instrumental success server directed provides panda global view upcoming collective operation panda plan perform operation sequential reads writes disk 
panda novel concepts disk directed logical level achieve sequential reads writes 
best knowledge panda implementation concepts disk directed important feature panda excellent performance ibm sp nas 
experiments panda achieves throughputs close full capacity underlying aix file system variety array sizes number nodes disk schemas 
results particularly exciting light panda small size lines code clean class hierarchy easy portability due mpi fact panda uses commodity file system aix sp requires custom system software 
panda excellent performance sp argues viability server directed removing complaint operating system file system dependence directed disk directed generally experience panda suggests may possible achieve performance massively parallel platforms costly custom operating system file system software able run panda network ordinary workstations changing code 
pleased performance panda sp see bright server directed way providing ease application program portability importantly high performance array near plan extensive performance study panda rearrangement facilities developing cost model predict panda performance memory disk schema 
look forward examining performance mixed workloads sp panda possible application sp dedicated set nodes curious impact node sharing intensive applications 
bennett bennett bryant sussman das saltz framework optimizing parallel proceedings scalable parallel libraries conference pages 
ieee computer society press october 
miguel del rosario design evaluation primitives parallel proceedings supercomputing pages 
language compiler parallel database support intensive applications proceedings high performance computing networking europe conference milano italy may springer verlag 
corbett corbett feitelson vesta file system programmer 
technical report research report rc ibm watson research center yorktown heights ny october 
version 
corbett corbett feitelson hsu snir nitzberg wong 
mpi io parallel file interface mpi technical report nas nasa ames research center january 
del rosario del rosario harry choudhary design vip fs virtual parallel file system high performance parallel distributed computing technical report sccs npac syracuse ny may 
gropp levine applications driven parallel proceedings supercomputing pages 
grimshaw french extensible file systems objectoriented approach high performance file proceedings international conference object oriented programming systems languages applications portland august 
kotz kotz throughput existing multiprocessor file systems informal study dartmouth pcs tr 
kotz kotz dynamic file access characteristics production parallel scientific workload 
proceedings supercomputing pages november 
kotz kotz disk directed mimd multiprocessors symposium operating systems design implementation november 
kotz kotz expanding potential disk directed dartmouth pcs tr submitted spdp 
kotz kotz disk directed core computation 
dartmouth tr pcs tr submitted hpdc 
pasquale pasquale polyzos dynamic characterization intensive scientific applications technical report 
cs university california san diego april 
pierce pierce concurrent file system highly parallel mass storage subsystem proceedings th conference hypercube computers applications monterey march 
pp 

ellis kotz best characterizing parallel file access patterns large scale multiprocessor duke university technical report cs october 
seamons seamons winslett physical schemas large multidimensional arrays scientific computing applications proceedings th international working conference scientific statistical database management charlottesville virginia september 
seamons seamons winslett efficient interface multidimensional array proceedings supercomputing washington november 
seligman seligman beguelin high level fault tolerance distributed programs school computer science carnegie mellon university usa 
technical report cmu cs december 
array schema information int int int int int sizeof int int sizeof double int distribute array processors int int store disk traditional order int distribution block block hpf block directives distribution block block schema object arrays memory memory new memory layout schema object arrays disk disk new disk layout array objects array temperature new array temperature memory disk array pressure new array pressure memory disk array density new array density memory disk object specify name file hold schema information simulation new sim simulation schema tell arrays part logical array group simulation include temperature simulation include pressure simulation include density run simulation timesteps int compute timestep collective timestep method may called repeatedly computation arrays output single collective request simulation timestep collective take checkpoint simulation checkpoint sample application code panda high level collective interface take checkpoints output data timestep computations number nodes aggregate throughput mb mb mb mb mb mb mb number nodes mb mb mb mb mb mb aggregate normalized throughput reading arrays mb mb compute nodes function number nodes natural chunking 
number nodes aggregate throughput mb mb mb mb mb mb mb number nodes mb mb mb mb mb mb aggregate normalized throughput writing arrays size mb mb compute nodes function number nodes natural chunking 
number nodes aggregate throughput mb mb mb mb mb mb mb number nodes mb mb mb mb mb mb aggregate normalized throughput reading arrays size mb mb compute nodes function number nodes natural chunking simulating infinitely fast disk 
number nodes aggregate throughput mb mb mb mb mb mb mb number nodes mb mb mb mb mb mb aggregate normalized throughput writing arrays size mb mb compute nodes function number nodes natural chunking simulating infinitely fast disk 
number nodes aggregate throughput mb mb mb mb mb mb mb number nodes mb mb mb mb mb mb aggregate normalized throughput reading arrays size mb mb compute nodes function number nodes traditional order disk 
number nodes aggregate throughput mb mb mb mb mb mb mb number nodes mb mb mb mb mb mb aggregate normalized throughput writing arrays size mb mb compute nodes function number nodes traditional order disk 
number nodes aggregate throughput mb mb mb mb mb mb mb number nodes mb mb mb mb mb mb aggregate normalized throughput writing arrays size mb mb compute nodes function number nodes traditional order disk simulating infinitely fast disk 

