cooperative behavior acquisition mobile robots dynamically changing real worlds vision reinforcement learning development minoru asada koh hosoda graduate school eng dept adaptive machine systems osaka university osaka japan rst discuss meaning physical embodiment complexity environment context multiagent learning 
propose vision reinforcement learning method acquires cooperative behaviors dynamic environment 
robot soccer game initiated robocup illustrate ectiveness method 
agent works team members achieve common goal opponents 
method estimates relationships learner behaviors agents environment interactions observations actions technique system identi cation 
order identify model agent akaike information criterion applied results canonical variate analysis clarify relationship observed data terms actions observations 
reinforcement learning estimated state vectors performed obtain optimal behavior policy 
proposed method applied soccer playing situation 
method successfully models rolling ball moving agents acquires learner behaviors 
computer simulations real experiments shown discussion 
partially supported japanese society promotion science project cooperative distributed vision dynamic dimensional scene understanding 
project id mail asada ams eng osaka ac jp preprint submitted elsevier science november building robot capability learning carrying task visual information acknowledged major challenges facing vision robotics ai 
vision action tightly coupled inseparable 
human beings see eye movements suggests actions signi cantly ect visual processes vice versa 
attempts build autonomous agent tight coupling vision modalities actions 
authors experiments contend vision isolated process component complicated system physical agent interacts environment 
view quite di erent conventional computer vision approaches paid little attention physical bodies 
typical example called segmentation di cult problems computer vision historical lack answers questions signi cance usefulness segmentation results 
issues di cult evaluate clear purpose 
issues task oriented 
straightforward design issues determined special purpose application 
concern nature physical agents capable sensing acting 
segmentation may correspond process building agent internal representation interactions environment 
standpoint control theory internal representation regarded set state vectors includes necessary su cient information accomplish task 
viewed state space representation robot learning reason 
especially true reinforcement learning receiving increased attention method requires little priori knowledge higher capability reactive adaptive behaviors 
works published reinforcement learning vision action 
whitehead ballard proposed active vision system involving computer simulation 
asada applied vision reinforcement learning real robot task 
methods environment include independently moving agents complexity environment great including agents 
case multi robot environment internal representation complex order accomplish tasks 
main reason learning robot share robot perception completely discriminate situations robots vice versa 
learner predict robot behavior correctly policy xed explicit communication available 
important learner understand strategies robots predict movements advance order learn successful behaviors 
littman proposed framework markov games learning robots try learn mixed strategy optimal worst possible opponent zero sum player game grid world :10.1.1.135.717
assumed opponent goal learner 
lin compared window current sensation sensations actions recurrent recurrent network 
showed superior recurrent network cope historical features appropriately 
di cult determine number neurons structures network advance 
furthermore methods utilize global information 
robotic soccer domain studying multi agent problems 
stone veloso proposed layered learning method consisting levels learned behaviors 
lower basic skills interception moving ball higher making decisions pass decision tree 
proposed method modular reinforcement learning coordinates multiple behaviors takes account tradeo learning time performance 
methods current sensor outputs states cope temporal changes object 
described existing learning methods multiagent environments need de ned state space de ned state vectors learning converge 
modeling architecture required reinforcement learning applicable 
rst discuss meaning physical embodiment complexity environment context multiagent learning 
propose vision reinforcement learning method acquiring cooperative behaviors dynamic environments 
method nds relationships behaviors learner agents interactions observations actions method system identi cation 
order construct local predictive model agents apply akaike information criterion aic results canonical variate analysis cva widely eld system identi cation 
local predictive model observation action learner observer 
apply proposed method simple soccer game 
task robot shoot ball passed back robot passer 
passer learns pass ball shooter 
environment consists stationary agent goal passive agent ball active agent opponent learner needs construct appropriate models agents 
learning robot identi es model reinforcement learning applied order acquire purposive behaviors 
proposed method cope moving ball state vector estimated way allows learning system predict motion image 
simulation results real experiments shown discussion 
physical embodiment meaning physical embodiment ultimate goal research design physical agents robots support emergence complex behaviors interactions 
order intelligent behavior occur physical bodies help bring system meaningful interaction physical environment 
interaction complex uncertain automatically consistent set natural constraints 
facilitates correct agent design learning environment rich meaningful agent interaction 
meaning having physical body summarized follows sensing acting capabilities separable tightly coupled 
ii order accomplish tasks sensor actuator spaces abstracted resource bounded conditions memory processing power controller 
iii abstraction depends agent embodied including internal workings experiences interactions environment 
iv consequences abstraction agent subjective representation environment 
evaluation conducted consequences behaviors 
real world inter agent agent environment interactions asynchronous parallel arbitrarily complex 
agent cope increasing complexity environment order accomplish task hand 
emphasize importance physical embodiment necessary show system performs coping new issues concrete task domain 
words need standard problem exposes various aspects intelligent behavior real environments 
task example adopt domain soccer playing robots robocup attempt foster ai robotics research providing standard problem wide range technologies integrated examined 
proposed methods related state action space construction reinforcement learning 
line learning method line 
related try explain environmental complexity relationships observations self motions subsection 
environmental complexity animal species regarded having kind intelligence di erence intelligence depend kind agent capabilities sensing acting cognition kind environment relationship 
agents bodies di erences intelligence occur complexity interactions environments 
case soccer playing robot vision complexity interactions may change presence agents eld teammates opponents judges 
view regarding levels complexity interactions especially viewpoint takes accounts existence agents 
body static environment body static environment de ned way notes changes image plane directly correlated self induced motor commands looking hand showing voluntary motion changing gaze observe environment 
theoretically discrimination body static environment di cult problem de nition static relative depends selection base coordinate system depends context task 
usually suppose orientation gravity provide ground coordinate system 
ii passive agents result actions self agents passive agents moving 
ball typical example 
long stationary categorized static environment 
simple correlation motor commands body static environment expected motion 
iii active agents active agents simple straightforward relationship self motions 
early stage treated noise disturbance lack direct visual correlation self induced motor commands 
complicated higher order correlations coordination competition 
complexity drastically increased 
complexity environment internal representation robot sophisticated complex order generate various intelligent behaviors 
real robot experiments show representation coping complexity agent environment interactions 
approach architecture learning successful necessary learning agent estimate appropriate state vectors 
agent obtain information necessary estimation owing limited sensing capability 
learning agent collect observed data nd relationship observed agents learner behaviors 
may identify suitable behavior optimal 
method system identi cation previously observed data motor commands input observation output system 
action observation 
state vector state vector state vector environment local predictive model reinforcement learning reward action action value function local predictive model local predictive model reinforcement learning reinforcement learning local interaction learner global interaction local predictive models fig 

proposed architecture shows learning architecture robot 
learning robot constructs local predictive models sequences sensor outputs action 
needs state vectors predict states dynamic environments 
learns cooperative behaviors state vectors estimated local predictive models 
reason phase learning follows strictly speaking robots fact interact learning robot construct local predictive model interactions account 
impossible collect adequate input output sequences estimate proper model dimension state vector increases drastically 
learning observing robot rst estimates local predictive model observed robots objects separate environment higher interactions robots obtained post reinforcement learning process 
learning schema order acquire cooperative behaviors multi robot environments schedule reinforcement learning 
actual learning methods categorized approaches learning policy real environment easy tasks simple environments di cult implement 
ii learning policy computer simulation transferring real environment gap simulation environment real need modi cations real experiment 
iii combination computer simulation real experiments simulation results learning real environment scheduled 
adopt third approach plan learning see 
robot constructs local predictive models learns behaviors real environment simulation results improve performance 
accelerates learning process 
local predictive models multi agent environment overview local predictive models shows overview local predictive model 
local predictive model estimates state vector sequences input output model obtain adequate precision increases historical length improve model 
reduces order estimated state vector information criterion size state space tractable 
reinforcement learning receives state vectors robot robot learning behaviors rl learning behaviors rl action fixed policy computer simulation learning behaviors rl learning behaviors rl action fixed policy policy transfer construction local predictive models construction local predictive models simulation results learned policy action learned policy action fig 

learning schedule multi robot environments local predictive models learns relationships 
canonical variate analysis cva number algorithms identify multi input multi output mimo combined deterministic stochastic systems proposed 
canonical variate analysis cva typical uses canonical correlation analysis construct state estimator 
input output generated unknown system ax bu historical length sequences sensor outputs motor commands state space full dimension estimator error aic estimated state vector fig 

local predictive model cx du ef 
denotes expected value operator kronecker delta 
unobserved gaussian distributed zero mean white noise vector sequences 
cva uses new vector linear combination previous input output sequences di cult determine dimension eq 
transformed follows ct 
follow simple explanation cva method 
fu 
construct new vectors 


ii compute estimated covariance matrices pp pf ff pp ff regular matrices 
iii compute singular value decomposition pp pf ff aux aux aux aux aux aux aux kq de ned aux pp iv dimensional new vector de ned estimate parameter matrix applying squares method eq 

mentioned learning observing agent applies cva method observed agent separately excessively high dimension state space 
denote estimated state vector sake reader understanding 
determine dimension agent important decide dimensionality state vector lag operator provides necessary historical information determining size state vector apply cva classi cation agents 
estimation improved larger larger historical information necessary 
desirable small possible respect memory size 
complex behaviors agents captured choosing order high 
order determine apply akaike information criterion aic widely eld time series analysis 
aic method balancing precision computation number parameters 
prediction error covariance matrix error aic calculated aic log rj number parameters 
optimal dimension de ned arg min aic parameter uence aic 
utilize log rj determine 
memorize dimensional vector agent dimensional vector motor command 
ii 
identify obtained data 
log rj procedure determine aic increment condition satis ed aic decrease 
reinforcement learning local predictive models local predictive model merely represents local interaction learner objects separately learning robot needs estimate global interaction models decide take actions accomplish tasks 
give brief explanation learning modular reinforcement learning accelerate learning time multiple goals 
learning learning method provides robots capability learning act optimally markovian environment 
simple version learning algorithm shown follows initialize combinations 
ii perceive current state 
iii choose action action value function 
iv carry action environment 
state immediate reward 
update action value function max learning rate parameter xed discounting factor 
vi return 
modular reinforcement learning time needed acquire optimal behavior mainly depends size state space di cult apply standard learning multiple tasks 
modular reinforcement learning method 
shows basic idea modular reinforcement learning number tasks ease illustration 
order reduce learning time state space classi ed categories maximum action values separately obtained learning area learned behaviors directly applicable learning area area learning necessary owing competition multiple behaviors re learning area 
states classi ed mahalanobis distance non kernel state kernel states kernel eventually composite state space classi ed learning area 
re learning area rl areas exclusive 
case states belonging learning area learning robot longer needs update action value function 
learning robot uses action value functions acquired previously 
learning robot re learning area robot estimates discounted value learn appropriate action value function 
result modular reinforcement learning take account tradeo learning time performance robot coordinates multiple behaviors 
re learning area learning area learning area state space task state space task action value maximum kernel state xx xx rl xx fig 

basic idea modular reinforcement learning experiments task assumptions apply proposed method simple soccer game includes mobile robots 
robot single color tv camera fig 

environment mobile robot know locations sizes weights ball agent 
know camera parameters focal length tilt angle kinematics dynamics 
move wheel steering system 
ects action environment conveyed agent visual information 
motor commands agent actions go straight turn right turn left go backward 
input de ned dimensional vector oe oe oe velocity motor angle steering respectively quantized 
output observed vectors shown 
result dimen goal robot ball area area area center position width height radius corners center position center position robot ball goal image features fig 

image features ball goal agent sions observed vector robot ball goal respectively 
simulated robotic experiments shooter passer construct local predictive models ball goal robot computer simulation 
passer begins learn behaviors conditions assume shooter stationary 
passer nished learning policy passer 
shooter starts learn shooting behaviors 
assign reward value shooter shoots ball goal passer passes ball shooter 
negative reward value robots collision robots occurs 
processes modular reinforcement learning applied shooter passer learn certain shooting passing behaviors avoiding 
transfer results computer simulation real environments 
order construct local predictive models real environment robot selects actions probabilities semi uniform undirected exploration 
words robot executes random action xed probability optimal action learned computer simulation 
perform trials robotic experiments 
local predictive models updated robots improve action value function obtained data 
local predictive model real environment increases estimated order state vector action value functions initialized action value functions computer simulation order accelerate learning 
perform trials check result learning real environment 
table shows result estimated state vectors computer simulation real experiments log jrj aic denote logarithm covariance matrix error local predictive model akaike information criterion respectively 
order predict situation follows su cient goal ball needs steps 
reasons may explain estimated orders state vectors di erent computer simulation real experiments noise prediction error real experiments larger computer simulation 
order collect sequences observation action robots select random action move result computer simulation 
experiences quite di erent 
result historical length real experiments larger computer simulation 
hand estimated order state vector robot real experiments smaller computer simulation components higher complicated interactions discriminated noise real environments 
table shows comparison performances computer simulation real experiments 
observed result replacing local predictive models passer shooter 
eventually large prediction errors sides noted 
local predictive models replaced physical agents 
shows sequence images shooter shoots ball kicked passer 
discussion kinds image features 
theoretically features considered 
necessary condition features provide table estimated dimension observer target log jrj aic computer simulation ball shooter goal passer passer ball shooter real experiments ball shooter goal passer passer ball shooter table performance result real experiments success success shooting passing learning learning sufficient information agent tasks hand 
redundant information ltered cva process eigen values redundant information lower dominant components 
experiment consider basic image features possible centroid area size radius side coordinates boundary rectangle experiment dominant features extracted linear combination constructed state vectors 
non linearity relationship objects 
cva state vector estimation linear approximation interactions learner agents separately 
call dynamics lower dynamics 
role reinforcement learning regarded absorb non linearity higher interactions agents 
interaction represents higher dynamics system 
may conclude long number agents large say reinforcement learning capable absorb non linearity passer shooter top view obtained images left shooter right passer fig 

acquired behavior number increase simple reinforcement application su cient represent higher complicated interactions 
di cult apply approach dynamically changing environments state vectors determined cva aic line 
think cva performed line way pca performed line neural networks 
may suggest natural extension approach line method line works 
aic vector size determination state vector estimation convincing 
ideally task performance better indicator aic trade state vectors slow convergence perceptual aliasing 
practical matter determining task performance requires iterations may computationally infeasible 
aic grounded information theory principled approaches 
approach suppose complexity environment correspond complexity interactions complexity internal representation 
may claim ant walking beach may walking complex environment behavior trail may complex doesn mean internal representation complex 
viewpoint classical ai geometrical complexities beach sand trial ant kinematics high interaction ant environment measured external observer complicated 
wrong 
ant behavior purely re exive complicated internal representation included 
process evolution walking skill developed embedded ant genes 
complexity intend claim geometry structure measured external observer interaction agent environment 
example case humanoid say dofs look mirror 
motions joints generate complicated image patterns mirror 
method identify single frame su cient predict change patterns image change simply correlated self induced motions 
concluding remarks proposes method behavior acquisition applies reinforcement learning multi robot environments 
method takes account tradeo precision prediction dimensionality state vector number steps needed 
mentioned believe agent interact dynamically changing world perception action separable tightly coupled physical body 
owing resource bounded constraints internal representation abstracted symbolized agent may symbols accomplish tasks shared homogeneous physical agents 
result observation active agents actions observation regarded communication 
observation role message receiving action role message sending 
conventional approaches provide communication protocol agents advance approach expects agents develop cooperative behaviors learning development internal representation 
partially supported japanese society promotion science project cooperative distributed vision dynamic dimensional scene understanding 
project id 
authors reviewers valuable comments improve 
agre 
computational research interaction agency 
arti cial intelligence pp 

akaike 
new look statistical model identi cation 
ieee trans 
ac pp 

aloimonos 
active vision revisited 
aloimonos ed active perception chapter pp 

lawrence erlbaum associate publishers 
aloimonos 
reply learned 
cvgip image understanding 
asada 
agent environment view having bodies case study behavior learning vision mobile robot 
proc 
iros workshop real autonomy pp 

asada noda hosoda 
purposive behavior acquisition real robot vision reinforcement learning 
machine learning 
asada noda hosoda 
action sensor space categorization robot learning 
proc 
ieee rsj international conference intelligent robots systems iros pp 

ballard pk rao deictic codes embodiment cognition 
behavioral brain sciences dec vol pp 

mahadevan 
robot learning 
kluwer academic publishers 
edelman 
reply representation reconstruction 
cvgip image understanding 
inaba 
remote robotics interfacing ai real world behaviors 
preprints pittsburgh 
kitano asada kuniyoshi noda osawa matsubara 
robocup challenge problem ai 
ai magazine 

canonical variate analysis identi cation ltering adaptive control 
proc 
th ieee conference decision control pp 
honolulu hawaii december 

lin mitchell 
reinforcement learning hidden states 
proc 
second international conference simulation adaptive behavior animals animats pp 

littman :10.1.1.135.717
markov games framework multi agent reinforcement learning 
proc 
th international conference machine learning pp 

matsuyama 
cooperative distributed vision 
proc 
international workshop cooperative distributed vision pp 


moore atkeson 
parti game algorithm variable resolution reinforcement learning multidimensional state spaces 
machine learning 
nakamura asada 
motion sketch acquisition visual motion guided behaviors 
th international joint conference arti cial intelligence pp 

morgan kaufmann 
nakamura asada 
stereo sketch stereo vision target reaching behavior acquisition occlusion detection avoidance 
proc 
ieee international conference robotics automation pp 

sandini 
vision action 
aloimonos ed active perception chapter pp 

lawrence erlbaum associate publishers 
sandini grosso 
reply purposive vision 
cvgip image understanding 
stone veloso 
machine learning soccer server 
proc 
iros workshop robocup 
takahashi asada hosoda 
reasonable performance learning time real robot incremental state space segmentation 
proc 
ieee rsj international conference intelligent robots systems iros pp 

asada hosoda 
behavior coordination mobile robot modular reinforcement learning 
proc 
ieee rsj international conference intelligent robots systems pp 

asada hosoda 
state space construction behavior acquisition multi agent environments vision action 
proc 
international conference computer vision appear 
watkins dayan 
technical note learning 
machine learning pp 

whitehead ballard 
active perception reinforcement learning 
proc 
workshop machine learning pp 

morgan kaufmann 

