integrating pomdp reinforcement learning layer simulated robot architecture larry adele howe computer science department colorado state university fort collins cs colostate edu www cs colostate edu layer control systems common robot architectures 
lower level designed provide fast fine grained control higher level plans longer term sequences actions achieve goal 
approach uses reinforcement learning rl low level partially observable markov decision process pomdp planning high level 
levels adapt behavior scope tasks combination expected robust degradations sensor actuator failures enhance system reliability 
implemented architecture khepera robot simulator 
set experiments show performance difficult achieve hand coded low level control performance rl pomdp system degrades slowly increasing sensor actuator failure 
layer control systems common robot architectures 
layer control pragmatic solution different time control granularities activity 
lower layer provides fast short horizon decision quickly executed actions upper layer adopts goal oriented view plans longer scope 
lower layer designed keep robot trouble running obstacles minor modifications course action sensory feedback course corrections upper layer ensures robot continually works target task goal paint corner go circles 
layer control architecture offers reliability 
example autonomous robots able deal failure sensors actuators robust way may operating direct human control space exploration hazardous environments 
situations hardware failure result failure mission control system sufficiently adaptive 
critical situations robust operation necessary 
office robot capable dealing sensor failures actuator failures 
wish develop robust control system autonomous robot operate office environment 
robust mean robot able deal sensor actuator malfunction caused hardware failure low battery conditions 
current system levels control 
lower level controls actuators move robot provides set behaviors higher level control planning system 
upper level plans sequence behaviors run order move robot current location goal 
architecture bottom level accomplished reinforcement learning rl top level partially observable markov decision process pomdp planner 
combination rl learning pomdp distinguishes approach previous 
believe rl choice implementing low level control able learn online adapt changes environment 
rl simultaneously learns value function policy reducing intervention programmer accommodating learning environment conditions change 
pomdp planning framework higher level control operates quickly policy generated provide reinforcement needed lower level behaviors 
testing architecture khepera robot simulator 
khepera small robot limited sensors defined environment 
simulation attractive option testing ideas 
simulator run faster real time require human intervention low battery conditions sensor failures physical robot 
research shows reinforcement learning low level pomdp planning higher level result system robust uses preprogrammed behaviors 
reinforcement learning bottom methods available implementing low level behaviors 
direct program reactions 
alternatively learned examples 
sammut uses approach referred behavioral cloning create rules airplane control system 
behavioral cloning extracts situation action rules logs actions taken human subject 
homogeneous multi layer architectures subsumption supported incremental development behaviors demonstrated robust performance 
general neural systems tend robust noise perturbations environment 
reason neural learning popular approach low level robot control 
neural control systems developed solve inverse kinematics robot arms visual robot positioning domains 
neural network robot hand control system human function 
uses adaptive neural network learn relationship object primitives set grasp modes picking cylindrical objects 
neural networks require long training periods large amounts data 
researchers examined rl techniques mobile robot navigation fine manipulation robot arms 
rl modules deal relatively high sensor data rates require sensor pre processing 
reinforcement learning modules learn continuously provide adaptation sensor drift changes actuators caused changes battery power 
extreme cases sensor actuator failure reinforcement learning modules adapt allow robot accomplish mission 
desirable property behavior learns value function independent self contained 
pomdp planning top lower level variety approaches explored higher level control 
common approach integrate deliberative symbolic planner reactive control levels ralph mea 
general planners produce complex plans longer duration appropriate application 
application assume limited sensor actuator capabilities environment known terrain offices 
khepera pulse encoders wheels vision determining exact state robot difficult 
effects actuators may deterministic worse effects behaviors implemented rl modules predictable especially training 
pomdp framework provides way deal uncertainty 
pomdp allows learning state transition observation probabilities line 
higher level adapt changes environment robot actuators sensors 
goals pomdp framework expressed reward functions 
rewards may associated goal state performing action certain state 
pomdp solution algorithms try maximize reward received 
define complex compound goals framework 
major drawback pomdp planning current pomdp solution methods scale size state space 
exact solutions feasible small pomdp planning problems 
pomdp planning requires robot map environment feasible 
small domain provide map robot 
gained integration combining pomdp planning rl construct system robust changes environment failure sensors actuators 
rl provides mechanism adapting changes malfunctions actuators sensors 
low level behaviors continuously learning maximize reward provided pomdp planner 
continuous learning allows reinforcement learning compensate gradual changes sensors actuators 
large changes actuators sensors cause reinforcement learning modules fail completely 
negative rewards accumulated rl modules searching better solution learn damaged sensors actuators possible 
goal low level behaviors maximize reward free search possible solution space 
instances existing policy require slight modification sensor actuator failure 
continuous learning may drawbacks especially backpropagation neural networks activation functions global support approximate value function 
drawbacks include training leading cyclical performance failure find stable solution randomly exploring state space 
current implementation uses lookup table storing value function suffer problems scale include sensors 
pomdp planner high level map environment learn relationship behaviors positions state space 
means adapt changes environment sensor actuator failures adjusting transition probabilities uses planning behavior 
reliability increased system adaption levels 
application domain khepera research uses khepera robot simulator 
khepera small mm diameter desktop robot limited sensors actuators 
sensors consist infrared transmitter receiver pairs arranged shown 
pulse encoders wheels 
robot arrangement khepera infrared sensors 
drive wheels side controlled independently providing ability control speed direction robot 
khepera robot simple adequately modeled simulator 
previous research shown simulation results successfully transferred real robot simulator reasonably accurate 
khepera simulator discrete time simulator run real time faster real time 
user supplied robot controller called simulator time tick 
sensor model includes stochastic modeling noise responds similarly real sensors 
simulation environment includes stochastic modeling wheel slippage acceleration 
michel reports transfer khepera simulator real khepera robot 
user writes set subroutines conform simple api 
subroutines simulator initialize run shutdown user control code 
simulator provides routines allow controller query sensors control effectors 
user create save restore simulated environment obstacles 
simulator include settings simulate failures 
added hooks simulator allow simulate sensor failures simulated effector failures code 
reinforcement learning behaviors rl pomdp implementation khepera implemented basic behaviors move forward turn right turn left 
robot moving performing behaviors 
turning number time steps turn behavior invoked determines far robot turns 
low level behaviors responsible dealing obstacles dealing problems arise adjusting sensor actuator malfunction 
move forward behavior adjust activity path blocked 
goal rl modules maximize reward pomdp planner 
reward function long took desired state transition state transition shortest amount time maximizes reward 
reward robot making efficient battery power possibility 
low level behaviors trained avoid obstacles learn necessary maximizing reward pomdp planner 
example suppose policy state perform action expected state entering state pomdp planner activates action waits change robot state 
drives robot wall receive reward 
way receive reward cause robot state change action receive zero reward robot leaves state happens receive positive reward new state behaviors implemented reinforcement learning module 
rl modules receive data sensors actuators reinforcement signal pomdp planner higher level 
rl modules active time 
active rl module gains complete control actuators full access sensor data 
rl modules implemented underlying code 
perform different behaviors due rewards receive 
rl modules may receive complete information sensors pomdp planner 
pre process sensory input pomdp reducing bits order pomdp solvable 
rl modules need restrictions deal data 
learning table lookup approximating value function 
able handle larger problems neural network reinforcement learning guaranteed converge performs poorly relatively simple problems 
neural networks tendency train portion state space visits forget value function portions state space visited 
leads cycle learns perform time begins perform poorly 
eventually re learn value function performance improves period 
number separate behaviors networks increased probability forget part learn forget cycle increases cause severe problems system tries recover multiple failures caused malfunctioning behaviors 
table lookup avoids problems adopted representation 
fortunately problem far small table lookup feasible 
pomdp planning markov decision processes previously examined approach planning robotic systems 
markov decision process model consists finite set states finite set actions set actions state executed state transition probabilities js 
transition probabilities just specify probability state previous state action taken 
mdp includes set observation probabilities ojs specify probability sensor detects observation state completely observable mdp assumed current state determined solely information immediately available sensors 
completely observable mdp methods real world robot planning tasks robots rarely determine state immediate sensor observations 
determination usually computationally expensive 
way deal uncertainty generalization mdp known partially observable markov decision processes 
robot unable determine current state solely current sensor observation necessary update state probability distribution states information transition observation probabilities 
state expressed probability density function possible states 
probability distribution time step calculated sjs theta normalization factor ensures probabilities sum 
necessary action may defined states 
sensor report comes sensor probability distribution updated theta ojs theta khepera robot sensors report distance values 
reduce amount data 
group sensors pairs pseudo sensors threshold output pseudo sensors 
results possible observations pomdp planner relatively robust single sensor failures 
solving pomdp planning performed selecting state goal finding policy reach state 
policy simply mapping specifies action take state 
completely observable mdps representation policy straightforward 
state mapped exactly action 
robot follow policy simply determines state looks state policy performs specified action 
pomdp robot tell exactly state 
state represented probability distribution interpreted unit length vector 
policy pomdp set vectors state space 
vector associated action 
policy select action robot calculates vector dot product current belief state vector policy 
whichever vector largest dot product wins action associated vector 
set vectors policy form upper surface piecewise linear function 
surface known value function 
algorithms compute optimal policy pomdp current algorithms computationally expensive 
witness algorithm probably best known algorithms converge quickly 
implemented incremental pruning algorithm underlying pomdp planner 
incremental pruning exact pomdp solution methods scale size state space 
necessary keep state space representation simple possible 
small area divided discrete positions discretized robot heading compass directions 
resulted possible states robot 
sensor environment state space experiments 
information reduced bits combining sensors pairs setting threshold combined sensor values 
environment state space shown 
simple state space generating policy required days computation sun ultra workstation 
major computational bottleneck solution large number linear programs 
order incremental pruning linear program solver high degree numerical stability 
great deal effort put creating linear program solver sufficient numerical stability speed 
linear program solver sparse vector implementation revised simplex method 
interface layers policy pomdp maintains probability distribution possible states 
probability distribution referred belief state 
pomdp uses current belief state select low level behavior activate 
implementation tracks state highest probability current state 
current state changes generate non zero reward behavior currently active 
time steps current state change generate zero reward 
state transition probabilities easy find expected state performing current action current state 
current state changes state want reward generated reward gamma generated 
note possibility incorrect reward assignment 
problem reinforcement learning converge incorrect reward assignment 
evaluation primary advantage rl pomdp combination adaptability 
expect system perform reliably robot hardware degrades 
hypothesize performance degrade gracefully roughly linearly shallow slope sensors actuators gradually fail 
test hypothesis degraded sensor actuator performance measured performance systems rl modules hand coded behaviors low level control 
hand coded behaviors provide control baseline performance demonstrate task difficulty 
rl modules programmed low level behaviors robot turn left turn right move forward 
hand coded behaviors hit obstacle turn try avoid 
hand coded behaviors obtain estimates state transition observation probabilities needed pomdp planner 
placed robot random position orientation possible states recorded effects performing actions starting position 
repeated trials states 
data collection process provided initial estimate state transition observation probabilities pomdp planner 
simplicity due computational limitations chose state robot location goal experiments 
iterative pruning algorithm find policy direct robot state initial state 
large pomdp problem high threshold value returned linear program solver 
probably resulted policy far optimal sufficient purposes 
policy experiments 
policy place ready train rl modules 
replaced hand coded behaviors rl modules trained placing robot random position orientation states turn generating transition observation probabilities 
repeated process times 
trial experiment degraded simulated sensor actuator performance set amount 
system configuration rl hand coded started simulated robot position orientation environment recorded performance 
failure robot re started position orientation try 
position orientation tried failures trial sensor impairment reinforcement learning hand coded average steps goal sensor impairment reinforcement learning hand coded response gradual failure sensors 
robot successfully achieves goal 
simulator setting collect data instances robot achieving goal 
case assessed performance metrics 
failures trial counts number times robot unable reach goal time steps configurations trial estimator reliability 
average steps goal assesses efficiency long take achieve goal 
obviously repeated failures trial causes number increase considerably 
gradual sensor failure sensors fail gradually battery power dust accumulates sensors sensors drift time 
assess effect gradual sensor failure system performance set scaling factor sensors restrict range 
range sensors normal sensors said impaired 
started sensors fully operational impairment 
unit impairment directed robot go goal state possible starting location 
average steps goal trial hand coded mild failure hand coded severe failure rl mild failure rl severe failure response intermittent failure actuator 
shows results experiment 
curves mean smoothed window results losing points 
hand coded behaviors expected time steps reach goal strongly correlated number failures 
hand coded behaviors bump reaction triggered threshold ir distance sensors exceeded 
examination data showed common failure caused robot running corner triggering bump reaction sensors generate signal threshold 
due threshold effect hand coded behaviors perform poorly small amounts sensor impairment 
rl module shows excellent reliability number failures trial increases gradually staying sensor impairment high 
cost rl reliability sharp increase cost decrease efficiency degradation rises slowly sensor impairment 
intermittent actuator failure actuators may partially intermittently fail due loose contacts shorted motor causes 
assess effect intermittent partial actuator failure simulated loss power right motor adding subroutine reduce value command signal fixed amount probability 
example robot control system issued command motor run speed failure simulation routine may send command right motor 
command signal negative added signal positive gamma added 
simulated mild failures sigma severe failures sigma added right motor command signal 
tested response hand coded learned behaviors mild severe failures 
test ran trial sensors actuators operational establish baseline ran trials simulating intermittent failure right motor 
shows results experiment average steps goal performance metric 
reinforcement learning behaviors mild failure performance initially degraded adapted quickly 
severe failure took longer recover trials performance approaching pre failure level 
expected hand coded behaviors recover 
severe failure robot able reach goal trial hand coded reinforcement learning behaviors failures trial data differentiate performance 
expected rl pomdp combination exhibits robust behavior presence sensor actuator degradation 
current test application inhibited known limitations methods specifically amount training required problem size 
project directed scaling size problem giving architecture autonomy 
problem size constrained part limitations table lookup representation 
reinforcement learning guaranteed converge value function table lookup method 
table lookup method scale size input space sensors 
overcome scaling problem neural networks approximation technique may represent value function 
value function approximation reinforcement learning backpropagation neural network serious flaw non local changes value function update 
property undesirable reinforcement learning training neural network area value function cause network forget areas learned 
lead poor performance learn forget cycle performance time degrades 
increase sensory input rl modules modifying standard neural network reinforcement learning decision tree representation value function 
table lookup decision trees subdivide input space intervals table lookup resolution intervals depends problem 
tree map input vector leaf nodes corresponds region search space 
learning associates value region 
new approach avoids cycling problem providing stable reliable convergence estimated value function large problems 
decision tree approach function approximation state aggregation averaging technique 
class function approximation proven converge 
increase size space pomdp investigating non optimal solution algorithms 
algorithms operate faster cost approximate solutions 
augment autonomy architecture enhanced add new behaviors dictated demands environment 
combination pomdp reinforcement learning supports learning new low level behaviors needed 
pomdp planner recognize situations existing behavior set reliable performing certain state transition instantiate new rl module state transition 
training specific behavior added set available behaviors similar states 
acknowledgments research supported part national science foundation career award iri 
united states government authorized reproduce distribute reprints governmental purposes notwithstanding copyright notation 
justin boyan andrew moore 
generalization reinforcement learning safely approximating value function 
tesauro leen editors advances neural information processing systems cambridge ma 
mit press 
rodney brooks 
build complete creatures isolated cognitive simulators 
kurt vanlehn editor architectures intelligence second carnegie mellon symposium cognition chapter pages 
lawrence erlbaum associates hillsdale new jersey 
james calvert william 
linear programming 
harcourt brace 
anthony cassandra michael littman nevin zhang 
incremental pruning simple fast exact algorithm partially observable markov decision processes 
proceedings thirteenth annual conference uncertainty artificial intelligence 
marco dorigo marco colombetti 
robot shaping developing autonomous agents learning 
artificial intelligence december 
james firby roger kahn peter michael swain 
architecture vision action 
proceedings th international joint conference artificial intelligence volume pages 
geoffrey gordon 
stable function approximation dynamic programming 
technical report cmu cs carnegie mellon university computer science department pittsburgh january 
henry johan guido stuart shapiro 
behavior ai cognitive processes emergent behaviors autonomous agents 
technical report state university new york buffalo department computer science bell hall buffalo new york april 
sven koenig richard goodwin reid simmons 
robot navigation markov models framework path planning learning limited computational resources 
lecture notes computer science 
sven koenig reid simmons 
risk sensitive planning probabilistic decision graphs 
proceedings fourth international conference principles knowledge representation reasoning pages 
michael littman 
witness algorithm solving partially observable markov decision processes 
technical report cs brown university department computer science providence ri december 
huan liu george 
neural network architecture robot hand control 
proceedings ieee international conference neural networks 
ieee july 
sridhar mahadevan jonathan connell 
automatic programming behaviour robots reinforcement learning 
artificial intelligence 
olivier michel 
khepera simulator 
available download epfl ch team michel sim 
gary 
ralph mea real time decision theoretic agent architecture 
phd thesis university california berkeley california 
larry adele howe 
decision tree function approximation reinforcement learning 
tech report tr cs colorado state university fort collins colorado october 
claude sammut 
automatic construction reactive control systems symbolic machine learning 
knowledge engineering review 
satinder pal singh 
efficient learning multiple task sequences 
advances neural information processing systems san mateo ca 
morgan kaufman 

neural learning robot control 
cohn editor proceedings eleventh european conference artificial intelligence pages chichester august 
ecai john wiley sons 
david wilkins karen myers john leonard wesley 
planning reacting uncertain dynamic environments 
journal experimental theoretical ai 
