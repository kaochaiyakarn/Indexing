software pipelining allan reese jones randall lee stephen allan utilizing parallelism instruction level important way improve performance 
time spent loop execution dominates total execution time large body optimizations focus decreasing time execute iteration 
software pipelining technique loop faster execution rate realized 
iterations executed overlapped fashion increase parallelism 
abc represent loop containing operations executed times 
operations single iteration parallelized parallelism may achievable entire loop considered single iteration 
software pipelining transformation utilizes fact loop abc equivalent bca gamma bc 
operations contained loop change operations different iterations original loop 
various algorithms software pipelining exist 
comparison alternative methods software pipelining 
relationships methods explored possibilities improvement highlighted 
categories programming techniques concurrent programming programming languages processors compilers optimization general terms algorithms key words instruction level parallelism software pipelining loop reconstruction optimization 
supported part nsf iri 
allan stephen allan department computer science utah state university logan utah 
mail cs usu edu 
reese jones evans sutherland slc ut mail es com randall lee ogden ut copyrighted acm computing surveys september 
permission digital hard copy part personal classroom granted fee provided copies distributed profit advantage copyright notice title publications date appear notice copying permission acm copy republish post servers redistribute lists requires prior specific permission fee 
acm contents background information modeling resource usage data dependence graph generating schedule initiation interval factors affecting initiation interval methods computing ii enumeration cycles shortest path algorithm iterative shortest path linear programming unrolling replication support software pipelining modulo scheduling modulo scheduling hierarchical reduction path algebra predicated modulo scheduling enhanced modulo scheduling kernel recognition perfect pipelining petri net model vegdahl technique enhanced pipeline scheduling instruction model global code motion renaming forward substitution pipelining loop reducing code expansion summary modulo scheduling algorithms perfect pipelining petri net model vegdahl enhanced pipeline scheduling acknowledgments software pipelining excellent method improving parallelism loops methods fail 
emerging architectures support software pipelining 
approaches improving execution time application program 
approach involves improving speed processor termed parallel processing involves multiple processing units 
techniques 
parallel processing takes various forms including processors physically distributed processors physically close asynchronous synchronous multiple processors multiple functional units 
fine grain instruction level parallelism deals utilization synchronous parallelism operation level 
presents established technique parallelizing loops 
variety approaches understood problem fascinating 
number techniques software pipelining compared contrasted ability deal necessary complications effectiveness producing quality results 
deals practical solutions real machines stimulating ideas may enhance current thinking problems 
examining practical impractical approaches hoped reader gains fuller understanding problem able short circuit new approaches may prone previously discarded techniques 
advent parallel computers methods creating code take advantage power parallel machines 
propose new languages cause user redesign algorithms expose parallelism 
languages may extensions existing languages completely new parallel languages 
theoretical perspective forcing user redesign algorithm superior choice 
need take sequential code parallelize 
regular loops loops lend parallelization techniques 
exciting results available parallelizing nested loops 
techniques loop distribution loop interchange skewing tiling loop reversal loop bumping readily available 
dependences loop permit vectorization simultaneous execution iterations techniques required 
software pipelining restructures loops code various iterations overlapped time 
type optimization massive amounts parallelism creates modest amount parallelism 
utilization fine grain parallelism important topic machines synchronous functional units 
machines horizontal multiple risc architectures vliw benefit utilization low level parallelism 
software pipelining algorithms generally fall major categories modulo scheduling kernel recognition techniques 
compares contrasts variety techniques perform software pipelining optimization 
section introduces terms common techniques 
section discusses modulo scheduling specific instances modulo scheduling 
section discusses lam iterative technique finding software pipeline section mathematical foundation effectively model problem 
section hardware support modulo scheduling discussed 
section extends benefits predicated execution machines hardware support 
sections discuss kernel recognition techniques 
section method aiken nicolau 
section demonstrates application petri nets problem section demonstrates construction exhaustive technique 
section introduces completely different technique accommodate conditionals 
final sections summarize various contributions algorithms suggest 
background information modeling resource usage operations conflict require resource 
example need floating point adder floating point adder operations execute simultaneously 
condition disallows concurrent execution operations modeled conflict 
fairly simple view resources 
general view uses categorize resource usage 
homogeneous heterogeneous 
resources termed homogeneous identical operation specify resource needed needs resources 
resources termed heterogeneous 

specific general 
resources heterogeneous duplicated say resource request specific operation requests specific resource class 
say resource request general 

persistent non persistent 
say resource request persistent resources required issue cycle 

regular irregular 
say resource request regular persistent resource conflicts issue cycle need considered 
say request irregular 
common model resource usage heterogeneous specific persistent regular indicates resources required operation 
resource reservation table proposed researchers models persistent irregular resources 
illustrated needed resources operation modeled table rows represent time relative instruction issue columns represent resources adapted 
reservation table series multiplies adds proceed add follow multiply cycles result bus shared 
low level model resource usage extremely versatile modeling variety machine conflicts 
data dependence graph deciding operations execute important know operations follow operations 
say conflict exists operations execute simultaneously matter executes 
dependence exists operations interchanging order changes results 
dependences operations constrain done parallel 
data dependence graph ddg illustrate follow relationships various operations 
data dependence graph represented ddg set nodes operations set arcs dependences 
directed arc represents follow relationship incident nodes 
software pipelining algorithms contain true dependences anti dependences output dependences 
operations precedes original code 
follow conditions hold data dependent reads data written anti dependent destroys data required output dependent writes variable term dependence refers data dependences anti dependences output dependences 
source stage stage stage stage stage stage result bus alu multiplier source source stage stage stage stage stage stage result bus alu multiplier source time time possible reservation tables pipelined add pipelined multiply 
reason operation wait operation 
control dependence exists execution statement determines statement executed 
able execute data available may execute known needed 
statement executes supposed execute change information computations 
control dependences similarity data dependences modeled way 
dependence information traditionally represented ddg 
copies operation representing operation various iterations choices 
different node represent copy operation node represent copies operation 
method 
suspected convention graph complicated read requires arcs annotated 
dependence arcs categorized follows 
loop independent arc represents follow relationship operations iteration 
loop carried arc shows relationships operations different iterations 
loop carried dependences may turn traditional cyclic graphs 
obviously dependences cyclic operations iteration represented distinctly 
cycles caused due representation operations 
generating schedule consider loop body 
operation iteration depends previous operation shown ddg dependence various iterations dependences loop independent 
type loop termed doall loop iteration appropriate loop control value may proceed parallel 
assignment operations particular time slot termed schedule 
schedule rectangular matrix sets operations pseudo code represent operations 
array accessing normally available machine operation level high level machine code represent dependences readable risc code appropriate choices 
implying target machine operations 
iterations 



loop body pseudo code data dependence graph schedule rows represent time columns represent iterations 
indicates possible schedule copies operation execute 
set operations executes concurrently termed instruction 
copies operation execute instruction 
similarly copies operation execute second instruction 
required iterations proceed lock step possible sufficient functional units 
doall loops represent massive parallelism relatively easy schedule 
doacross loop synchronization necessary operations various iterations 
loop example loop 
operation iteration precede iteration computed previous iteration 
doall loop possible parallelism achieved operations various iterations 
doacross loop allows parallelism operations various loops proper synchronization provided 
software pipelining enforces dependences iterations relaxes need iteration completely finish begins useful fine grain loop optimization technique architectures support synchronous parallel execution 
idea software pipelining body loop iteration loop start previous iterations finish executing potentially parallelism 
numerous systems completely unroll body loop scheduling take advantage parallelism iterations 
software pipelining achieves effect similar unlimited loop unrolling 
adjacent iterations overlapped time dependences various operations identified 
see effect dependences helpful unroll iterations 
shows ddg loop body 
example dependences true dependences 
arcs loop independent arc loop carried dependence 
difference iteration number source operation target operation denoted value pair associated arc shows similar example loop carried dependence iterations apart 
restrictive constraint iterations min iterations dif postlude loop body pseudo code iterations unrolled loop ddg representative code prelude new loop body postlude omitted pipeline prelude kernel postlude 
execution schedule iterations 
time min vertical displacement 
iteration dif horizontal displacement 
example min dif 
slope min dif schedule length kernel 
table dependence examples instruction ddg arc label instruction arc type dif true anti gamma true gamma anti anti true true anti output iterations overlapped 
common associate delay arc indicating specified number cycles elapse incident operations 
delay specify operations multi cycle floating point multiply 
arc annotated min time time elapse time operation executed time second operation executed 
identical operations separate iterations may associated distinct nodes 
alternative representation nodes lets node represent operation iterations 
operations loop behave similarly iterations reasonable notation 
dependence node iteration third iteration distinguished dependence iteration 
addition annotated min time dependence annotated dif difference iterations operations come 
characterize dependence dependence arc annotated dif min dependence pair 
dif value indicates number iterations dependence spans termed iteration difference 
convention version iteration dif min indicates dependence dif 
loop independent arcs dif zero 
minimum delay intuitively represents number instructions operation takes complete 
precisely value min placed instruction dif placed earlier min table shows examples code contain loop carried dependences 
code shown precedes loop 
loop control variable variable array expression 
example row indicates assigned value iterations 
true dependence dif 
iteration loop scheduled overlap instructions required iteration operations done parallel due dependences 
consider operations iterations dramatic improvement 
assume scheduling parallel environment called compaction schedule produced shorter sequential ff 
iterations min iterations dif loop body code iterations unrolled loop ddg execution schedule iterations 
iterations gamma loop body code ddg schedule operations execute concurrently allows operations different iterations execute concurrently operations iterations scheduled simultaneously operations scheduled early execution time possible 
store code iterations unrolled loop prohibitive copies operation 
seeks minimize code needed represent improved schedule locating repeating pattern newly formed schedule 
instructions repeating pattern called kernel pipeline 
indicate kernel enclosing operations box shown 
kernel loop body new loop 
iteration divided chunks executed parallel iterations termed pipeline 
numerous complications arise pipelining 
iteration executed indicated schedule 
note loop carried dependence various copies scheduling operations successive iterations early dependences allow termed greedy scheduling shown scheduled time step distance rest operations increases successive iterations 
cyclic pattern achievable examples forms 
example pattern emerge shown box 
notice contains copies operation 
double sized loop body serious problem execution time effected increase code size new loop body instructions length 
note delaying execution operation second cycle eliminate problem 
random nondeterministic scheduling algorithm prohibits pattern forming quickly 
seen care required choosing scheduling algorithm software pipelining 
version 
assumed maximum operations may performed simultaneously 
general resource constraints possible assume homogeneous functional units example simplicity presentation 
ffifl fflfi ffifl fflfi ffifl fflfi ffifl fflfi ffifl fflfi delta delta delta deltaff iterations iterations ddg schedule forms pattern schedule form pattern initiation interval schedule achieved iteration new loop started instruction 
delay initiation iterations new loop called initiation interval ii length delay slope schedule defined min dif 
new loop body contain operations original loop 
new loop body shortened execution time improved 
corresponds minimizing effective initiation interval average time iteration takes complete 
effective initiation interval ii iteration ct iteration ct number copies operation loop shows schedule iterations executed time cycle assuming functional units available support operations 
slope schedule min dif notice slope indicates time cycles takes perform iteration 
iterations executed cycle effective execution time iteration time cycle 
notice start finish exactly manner original loop instruction sequences ff omega required respectively fill empty pipeline 
ff consists instructions termed prelude omega consists instructions termed postlude 
earliest iteration represented new loop body iteration iteration represented new loop body iteration span pipeline gamma 
spans iterations prelude start gamma iterations preparing pipeline execute postlude finish gamma iterations point pipeline terminates 
ff omega number times executed number times executed gamma iteration ct ff omega execute gamma copies operation 
ddg reservation style resource constraints denoted boxes 
flat schedule final schedule stretched resource constraints 
factors affecting initiation interval resource constrained ii methods software pipelining require estimate initiation interval 
initiation interval determined data dependences number conflicts exist operations 
resource usage imposes lower bound initiation interval ii res 
resource compute schedule length necessary accommodate uses resource 
example requires resource cycle time issue resource cycle 
count resource requirements nodes clear resource required times resource required times resource required times 
cycles required kernel containing nodes 
relative scheduling operation original iteration termed flat schedule denoted shown 
schedule kernel size shown 
dependence constrained ii factor contributes estimate lower bound initiation interval cyclic dependences 
approaches estimating cycle length due dependences ii dep extend concept dif min path 
represent cyclic path node 
min sum min times arcs constitute cycle dif sum dif times constituent arcs 
see time execution node iterations case depends ii general time elapses execution copy dif iterations away ii dif ii large ii dif min iteration offset dif ii iterations ii dif time steps passed 
iterations ii dif ii min dif effect ii cyclic times arcs ddg follow transitive law path containing series arcs sum minimum delays min sum iteration differences dif functionally equivalent single arc source node path destination node dependence pair dif min 
cyclic dependences satisfied transitive arc representing cycle satisfy dependence constraint inequality function oe returns sequence number instruction operation sequence begins time represent actual time operation iteration executed 
formula results cycles time dif gamma time min words time difference cyclically dependent operations scheduled exceed minimum time 
time oe time dif oe ii dif formula cycles oe ii dif gamma oe min rewritten cycles min gamma ii dif lower bound initiation interval due dependence constraints ii dep solving minimum value ii cycles min gamma ii dep dif ii dep max cycles min dif example ii dep cycle min dif 
actual lower bound initiation interval ii max ii dep ii res example 
cycle having min dif equal ii termed critical cycle 
methods computing ii enumeration cycles method estimating ii dep simply enumerates simple cycles 
maximum min dif cycles ii dep 
shortest path algorithm method estimating ii dep uses transitive closure graph 
transitive closure graph reachability relationship 
dependence constraints expressed function ii single calculation compute transitive closure sufficient 
symbolic computation closure allows closure dependence constraints calculated independently particular value ii dependence constraint nodes closure represented set distances nodes separated order satisfy dependences 
flat schedule distance computed single dif min dependence pair arc min gamma ii dif compute minimum distance nodes separated information dependent ii variable simply take maximum distance paths shown see arc dif implies follow flat schedule min gamma ii time units 
general arc dif min equivalent arc min gamma dif ii provided ii known 
shown new minimum value negative 
example gamma indicates precede flat schedule time units 
computation gives earliest time placed respect paths having dif min having dif min evident represents stricter constraint 
case gamma ii second case gamma ii ii larger 
example ii gamma gamma 
ii dif min second arc represents larger distance 
example ii gamma gamma 
distances considered eliminated discovering ii sufficiently large 
previous example ii known due computation lower bounds satisfied constraint ignored 
computing closure dependence constraints equivalent finding longest path pair nodes strongly connected component 
equation see cycles distance referred cost transitive closure algorithms 
fa dif ming iterations iterations ii ii dif dif gammam min min min gamma ii dif effect ii minimum times nodes 
rectangle represents schedule iteration original loop 
positive value indicates precedes 
negative value indicates follows zero negative distance 
reversing sense inequalities floyd points shortest path algorithm calculate points longest path 
set nodes graph 
cost matrix defined follows entry ij contains set dependence information influences longest path dif min dependence pairs representing transitive closure dependence arcs node node initially ij set contains dif min dependence pairs arcs strongly connected component 
closure dependence constraint calculated follows ij ij ik kj function creates updated cost set adding dependence pair set dependence pair second set component wise 
function returns set union sets redundant dependence pairs removed 
cost redundant unnecessary provides additional constraints 
determining dif min dif min redundant involves separate tests 
min gamma ii dif min gamma ii dif distance associated dif min currently longer distance associated dif min 
dif dif larger values ii distance dif min remains longer due fact ii negative floyd original algorithm handles negative costs cycles positive costs 
sample graph table transitive closure graph source destination node node coefficient distance formula 
formally dependence pair dif min dependence pair dif min distance set dif min redundant dif dif min gamma min ii dif gamma dif previous example pair pair assume ii pair shown redundant gamma gamma 
initiation interval ii cost function cost ii gives number instructions node follow node flat schedule 
dependence constraint defined follows cost ii max dif min ij min gamma ii dif closure dependence constraints calculated dif min values cycles ddg available calculate ii dep minimum initiation interval due dependence constraints 
similar computing points longest path graph difference distance depends ii dif min values cycle containing node ii cost set entries main diagonal 
sample graph table shows result calculating closure dependence constraints graph 
simple path nodes represented closure table 
trying simple cases convince complex cycles having repeated nodes initial final node add redundant information computing maximum dif min cycle 
attempt throw redundant information example 
closure dependence constraints calculated lower bound initiation interval due dependence constraints equation 
dif min pairs diagonal table dependence pairs corresponding cycles graph 
substituting dependence pairs diagonal table equation yields ii dep max estimate lower bound initiation interval due dependence constraints 
iterative shortest path method computing ii dep simplified willing recompute transitive closure possible ii ii clear dif min pairs restrictive 
processing simpler cost table need contain dif min pair 
cost computing transitive closure grows square number values cost entry sizable savings 
path algebra attempt formulate software pipelining problem rigorous mathematical terms 
constructs matrix indicates entry min time nodes construction simple event dif value nodes zero 
assume arc dif min min 
shown see arc dif min implies follow min gamma ii time units 
general arc dif min represents distance min gamma dif ii computation gives earliest time placed respect flat schedule 
drawback able construct matrix estimate ii technique allows tell estimate ii large iteratively try larger ii appropriate ii 
gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma original matrix gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma closure closure computation consider graph 
estimate ii matrix shown 
note matrix restrictions due paths length execute 
min time iteration delay iterations ii 
distance required flat schedule 
defines type matrix multiply operation termed path composition omega represents minimum time difference nodes required satisfy paths length 
vectors omega omega phi omega phi omega phi omega notice similar inner product omega precedence phi omega addition phi maximum 
example get compose row matrix column follows gamma gamma gamma gamma gamma gamma omega gamma gamma gamma gamma gamma gamma max gamma gamma gamma gamma gamma gamma path composed edges indicated superscript follow time steps 
verify result noting path requires follow path dif min 
matrix representation graph dif values converted zero 
edges transitive closure formed adding min times edges compose path 
path composition just defined adds transitive closure edges 
technique section edges transitive closure added summing dif min values composing path 
thing simply adding minimum values arc path 
identical difs method zero 
multiple paths nodes store maximum distance nodes 
matrix shown 
formally perform regular matrix multiplication replace operations omega phi omega indicates min times added phi indicates need retain largest time difference required 
clearly need consider constraints placement dictated paths lengths 
closure gamma phi phi phi phi gamma number nodes indicates copies path multiplied 
paths length gamma need considered paths composed arcs contain cycles give additional information 
propose variant floyd algorithm shown closure efficient 
gamma represents maximum distance pair nodes considering paths lengths 
legal ii produce closure matrix entries main diagonal non positive 
example ii clearly minimal ii dep closure matrix contains non positive entries diagonal indicating ii sufficient 
ii matrix results 
positive values diagonal indicate ii small 
suppose repeat example ii shown 
diagonals negative closure table 
instance follow gamma time units 
words precede iteration time units 
values diagonal non positive ii adequate 
methods iterative technique find adequate ii try various values ii increasing order appropriate value 
may strange omega addition notation chosen show similarity path composition inner product 
variant floyd algorithm path closure gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma original matrix closure ii gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma original matrix closure ii loop code completely unrolled loop replicated loop linear programming method computing ii dep linear programming minimize ii restrictions imposed dif min pairs 
unrolling replication term unrolling various researchers mean different transformations 
loop completely unrolled iterations concatenated 
term replicated body loop copied number times loop count adjusted 
term unrolling represent concept 
term unrolling represent complete unrolling replication represent making fewer loop count copies loop body 
replicated copies exist newly formed schedule 
replication helpful different ways 
algorithms iteration differences greater handled replication eliminates occurrence non unit iteration differences 
algorithms replication allows fractional initiation intervals letting adjacent iterations scheduled differently 
time optimality possible new loop body include copy operation 
advantage achieved technique simple replication complicated fact replication increases complexity known replication helpful 
unrolling order find schedule see section methods 
iterations may examined find naturally occurring loop required copy operation new loop body 
support software pipelining software pipelining algorithms require loop limit run time constant 
pipeline stopped starts execute operations iteration executed 
speculative execution refers execution operations clear executed 
example consider loop controlled max 
suppose iterations execute greater max 
operations triangle executed 
executing operations iterations condition max iterations loop body code schedule part enclosed triangle executed false executed operations executed original loop 
operations change variables facility backing computations 
software pipelining applied general loops parallelism impressive support speculative execution 
speculative execution supported various mechanisms including variable renaming delaying speculative stores loop condition evaluated 
modulo scheduling historically early software pipelining attempts consisted scheduling operations iterations looking pattern develop 
modulo scheduling uses different approach operation placement done schedule legal cyclic interpretation 
words operation placed location ensure schedule overlapped iterations resource conflicts data dependence violations 
considering software pipeline schedule iteration shown offset repeated successive iterations 
schedule iteration length df iie different iterations represented kernel new loop body 
example span operations kernel come different iterations 
difficulty making sure placement operations legal successive iterations scheduled identically 
making determination clear offset just initiation interval known scheduling begins 
complications due resource conflicts guess achievable initiation interval 
problem difficult polynomial time algorithm determining optimal initiation interval 
problem shown np complete 
problem solved estimating ii repeating algorithm increasing values ii solution 
locations flat schedule relative schedule original iteration denoted flat schedule ii resulting regular pipeline gamma gamma gamma psi 
qs sw ddg schedule schedule renaming eliminate loop carried anti dependence pipelined loop formed overlapping copies offset ii illustrates flat schedule shows successive iterations offset initiation interval form pipelined loop body length 
termed modulo scheduling operations locations flat schedule value modulo ii executed simultaneously 
case operations mod execute mod execute 
type pipeline called regular pipeline iteration loop scheduled identically created new iteration started ii instructions resource conflicts dependences satisfied 
scheduling algorithms list scheduling priority select ready operations scheduled 
scheduling normally early possible schedule algorithms tried scheduling late possible alternating early late placement 
modulo scheduling operations placed time 
operations prioritized difficulty placement function number legal locations operation 
operations difficult place scheduled increase likelihood success 
conceptually place operation partially filled flat schedule think partial schedule repeated specified offset 
span copies schedule 
legal location violate dependences previously placed operations copies addition resource conflicts operations execute simultaneously schedule 
consider example dependence graph governing placement shown schedule 
suppose operation operation placed 
determine range locations flat schedule placed 
clearly operation placed earlier follow operation located 
latest scheduled 
consider iteration offset initiation interval operation iteration scheduled precede operation iteration 
legal location operation assuming operations scheduled 
loop carried dependences conflicts operations considered schedule built 
newly placed operation legal series offset schedules represented previously placed operations 
different scheduling techniques 
techniques schedule operation particular iteration previously scheduled operations specific iterations 
technique schedules operation iterations previously scheduled operations iterations 
words schedule operation iteration scheduling operation iterations 
different algorithms derived initial framework laid rau 
modulo scheduling hierarchical reduction important improvements basic modulo scheduling technique proposed lam 
modulo variable expansion variable expanded variable overlapped iteration motivation architectural support rotating register 
rau originally included idea adapted architectures part ideas published due proprietary considerations 
handling predicates sum resource requirements termed hierarchical reduction disjoint branches goal incorporated state art algorithms 
hsu stretch scheduling developed concurrently 
algorithm variant modulo scheduling strongly connected components scheduled strongly connected component digraph set nodes directed path node hj oe hhj ddg model node instruction group 
columns represent various resources rows represent time 
resource usage vectors shown square brackets instruction group 
separately 
lam uses traditional list scheduling algorithm modifications create flat schedule 
lam model allows multiple operations node dependence graph 
method breaks problem smaller problems scheduling separately needs way store schedule subproblem node 
strongly connected component reduced single node representing resulting schedule graph termed condensation 
condensed graph acyclic standard list scheduling algorithm finish scheduling loop body 
modifying ddg model node ddg schedule instructions subproblem single operation 
usage resource indicated placing mark th column table 
rows table indicate time instruction group 
shows example lam ddg model 
node consists operations scheduled time steps 
time step resources 
resource usage vector ae right node indicates times resource 
ae indicating resource times resources 
resource usage vector subscripted indicate resource usage point schedule 
node consisting time steps ae indicates resource usage vector utilized node th time step ae ae giving ae 
maximum number resource type available time step contained limit resource vector simplicity presentation single resource type assumed 
set node set 
strongly connected components tarjan algorithm 
scheduling connected components scheduling node strongly connected component transitive closure dependences paths node component node considered 
shortest path cost matrix discussed section utilized 
modulo scheduling uses closure dependence constraints form range locations flat schedule operation placed 
range depends placement nodes range nodes updated node scheduled 
initial dependence constraint range node defined follows oe low max cost ii oe care taken initial bounds 
lower bound negative 
oe low negative node scheduled instruction zero 
nodes scheduled topological order loop independent subgraph 
topological order graph sequential ordering nodes arc comes order 
loop independent subgraph graph strongly connected component loop carried dependence arcs 
node data ready predecessors scheduled 
nodes data ready lowest upper bound oe chosen 
selected scheduled placed instruction range oe low oe cause resource conflict 
node scheduled range scheduling algorithm fails current initiation interval 
dependence constraint range node may larger ii instructions 
case node placed ii consecutive instructions algorithm fails current initiation interval 
algorithm fails current initiation interval initiation interval increased algorithm retried 
node scheduled dependence constraint ranges remaining unscheduled node updated 
new lower bound larger current value placement location caused arc just placed oe known 
similarly new upper bound smaller current value placement location caused arc formulas update schedule range oe low max oe low oe cost ii oe min oe oe gamma cost ii consider strongly connected component 
sake simplicity assume operations loop execute concurrently resource conflicts 
loop single strongly connected component 
step scheduling component calculating closure dependence constraints 
table shows result calculating closure dependence constraints 
initial dependence constraint ranges computed oe low max cost ii oe ii initially follows gamma scheduling strategies may beneficial 
schedule cyclic cyclic locations considered looking ii consecutive locations flat schedule 
operation conflicts operations ii successive instructions cyclic instructions examined point continuing 
phi phi phi phi phi phi qs oe gamma gamma gamma gamma gamma gamma gamma gamma psi phi phi phi phi phi phi phi gamma gamma gamma gamma gamma gamma gamma gamma gamma psi example strongly connected component loop independent subgraph schedule table closure dependence constraints strongly connected component source destination node node example illustrates case initial dependence constraint range negative lower bound 
loop independent subgraph shown indicates initially data ready predecessors 
nodes upper bound arbitrarily chosen scheduled scheduled placement dependence constraint ranges updated scheduled data ready 
node smaller upper bound scheduled 
scheduled updated dependence constraint ranges nodes data ready 
node lower upper bound scheduled updated dependence constraint ranges nodes data ready 
node lower upper bound scheduled final dependence constraint range scheduled final schedule shown produces legal execution order flat schedule 
seen example dependence constraint ranges node shrink scheduling process proceeds 
narrowing dependence constraint ranges reflects increased constraints placed node nodes strongly connected component scheduled 
reducing ddg strongly connected component successfully scheduled condensed single node follows 
schedule strongly connected component contents new node 
resources represented reservation table shown resource requirements composite node easy represent 
minimum delay arcs entering leaving condensed node changed delay measured respect time step new node 
necessary model represent minimum time condensed node ff omega phi psi oe gamma gamma gamma gamma gamma psi qs gamma gamma gamma gamma gamma psi gamma gamma gamma gamma gamma psi omega omega omega omega omega omega omega omega ae pq example reducing strongly connected component 
arc specifying th instruction node precede th instruction node formulated node precede node gamma illustrates process reducing strongly connected component 
shows ddg strongly connected component 
assume schedule results scheduling strongly connected component oe oe 
shows ddg strongly connected component reduced single node 
node labeled represents node resulting reduction strongly connected component 
arcs removed contained satisfied condensed node 
arc changed 
scheduling acyclic ddg strongly connected components scheduled nodes condensed graph scheduled topological order ddg 
priority node resulting strongly connected component defined height ddg plus maximum height ddg 
due weighting insures nodes resulting graph reduction scheduled nodes possible 
composite nodes resource constraints difficult schedule need scheduled early 
node selected scheduling earliest instruction placed violating dependence constraints determined distance previously scheduled nodes 
height node ddg length longest path node sink 
height graph maximum height node 
lower limit placement formula oe low max dif min scheduled oe min gamma ii dif scheduled set nodes scheduled 
formula take account iteration difference arcs may loop carried dependences contained strongly connected component 
node placed instruction oe low oe low ii gamma cause resource conflict 
entire ddg scheduled result schedule single iteration forms regular pipeline flat schedule 
node placed instruction range scheduling algorithm fails current initiation interval 
ii incremented entire process repeated regular schedule achieved upper bound ii exceeded 
pipelining loop software pipelining algorithm summarized follows 
strongly connected components ddg 
nodes strongly connected component closure dependence constraints computed 
algorithm calculates absolute lower bound ii entire graph 
upper bound length loop body compacted pipelining constraints 
lower bound maximum lower bound estimate due resource constraints lower bound estimate due dependence constraints 
words lower bound simply means know schedule tighter smallest initiation interval possible largest various lower bounds 
scheduling process proceeds steps 
strongly connected component scheduled reduced single node 
acyclic ddg produced 
second step schedules acyclic ddg produce scheduling processes fail algorithm find regular pipeline current ii happens original ddg restored scheduling process repeated incrementing ii algorithm create regular pipeline initiation interval equal upper bound initiation interval algorithm fails software pipelining ineffective loop 
operations overlapped condition compete resources union requirements branches sum resource requirements feature adopted modulo scheduling algorithms 
important drawback fact schedules may respond appropriately larger ii ii increased causing operations spread apart operations tend remain clustered previous schedule smaller ii inefficiencies code introduced scheduling strongly connected components separately 
problems hierarchical scheduling originally proposed addressed enhanced modulo scheduling algorithm 
path algebra path algebra attempt formulate software pipelining problem rigorous mathematical terms 
section path algebra determine viable ii matrix matrix determine modulo schedule software pipelining 
nodes improvement early predicated execution methods 
note trade smaller ii smaller code size 
critical cycle having maximum min dif zero diagonal gamma indicating node exactly zero locations 
furthermore row matrix gamma indicates relative placement nodes respect 
row zero diagonal solution algebraic equation regarding distances termed eigenvector 
consider graph corresponding closure matrix shown 
case rows zero main diagonal nodes involved critical cycle graph 
row gives relative placement node respect element diagonal 
row indicates scheduled th location legal schedule formed st location th 
rows give relative placement 
row gamma gamma gamma indicates scheduled gamma st location th location gamma st 
notice elements row differ constant elements row relative placement operations indicated row identical 
understand resulting matrix reconsider alternative estimates ii ii original matrix closure shown 
tell ii sufficient guarantee correct schedule distances main diagonal 
example distance meaning follow instructions 
obviously happen ii 
shows schedule deduced line 
schedule incorrect instance arc obeyed 
ii shown diagonals negative closure graph 
instance follow time units 
words precede iteration time units 
row indicates different schedule legal 
solution elegant handle resource requirements 
theoretical tool practical 
predicated modulo scheduling predicated modulo scheduling advantages techniques discussed section represents improvement known defects 
excellent technique implemented commercial compilers 
researchers embraced modulo scheduling architectures hardware support modulo scheduling modified resulting code architectures hardware support 
described 
term predicated modulo scheduling represent general category algorithms 
precise method scheduling operations discussed probably complexity explaining process 
assume method similar employed lam hierarchical reduction schedules produced strongly connected components generates suboptimal results circumvented 
gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma original matrix closure derived schedule row ii gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma time row row row row row original matrix closure derived schedule various rows ii 
gamma gamma gamma psi 
qs sw ddg schedule schedule renaming eliminate loop carried anti dependence register renaming iterations overlapped reuse registers concern 
example suppose operation writes register call time operation iteration operation writes register call time operation 
data dependence graph lifetimes manifest dependence chain anti dependences shown dotted lines graph 
anti dependences annotation indicating loop carried dif operation writes register executed instruction min 
possible assume fetch value precedes write machine cycle 
dependence operation annotation indicating operation takes cycles complete min 
anti dependences force maximum min dif 
schedule shown length 
ii anti length longest cycle involved dependence cycle containing loop carried anti dependence 
ii initiation interval schedule anti dependences ignored 
replicate loop ii anti ii copies loop body different registers copy 
replicated loop scheduled shown 
operations write different registers indicated prime 
case copies operation versions register lifetime extends ii table shows schedule renamed versions register differing prime 
writes operation shown register name appearing left equal sign 
reads register shown register name appearing right equal sign 
simplicity registers shown 
operation writes iteration second iteration 
third little confusing obvious node exactly zero locations ii locations iteration 
point dependence constraints force distance techniques path algebra compute required schedule 
table modulo variable expansion time iterations iteration 
similarly operation writes third iterations writes second iteration 
uses writes problem fetches precede stores cycle 
registers required code space increased effective initiation interval halved versions original operation kernel 
termed modulo variable expansion 
note modulo variable expansion code expansion occurs scheduling increase complexity expansion occurs scheduling replication 
drawback scheme loops execute multiple span gamma times accommodated number copies loop span gamma number copies loop prelude postlude 
case prelude postlude execute iterations execution new loop body completes original iterations loops execute iterations handled 
situations new loop body contains multiple copies original iteration branch loop body avoids restriction 
case jump middle loop require special postlude registers current postlude appropriate 
solution execute iterations entering pipelined loop 
chosen remaining number iterations completed form span gamma termed preconditioning loop 
rau acceptable architectures little instruction level parallelism inadequate powerful processors 
hardware support modulo scheduling simplifies register renaming 
advent rotating register files loop carried anti dependences ignored code expansion 
variables redefined loop lifetime ii assigned static general purpose registers 
variables involved loop carried dependence cycles take advantage rotating register files 
rotating register file file file pointer rotates 
rotating register file register specifier refer physical register rotates set registers 
accomplished treating register specifier offset iteration control pointer icp points registers current iteration 
register computed sum register specifier icp modulo register file size 
icp decremented modulo register table rotating register file ii time register file size iteration execution 
way iteration accesses different registers code remains identical iteration 
provides hardware managed renaming 
rotating register file example schedule achieved 
operation writes register specifier case registers rotate problem 
similarly operation writes register specifier 
table rows table represent registers labeled 
time progresses horizontally 
superscripts represent original iteration defining variable 
asterisk indicates register contains value cycle value cycle 
number registers required rotating register files modulo variable expansion register assignment constantly changes opposed fixed loop 
register example stores iteration cycles iteration eighth cycle 
notice new ii location cycle decremented modulo 
icp initially decremented gamma mod second iteration register specifier assigned physical register 
predicated execution code contains conditionally executed code modulo scheduling complicated 
consider example 
suppose operation computes predicate boolean value determines operations operations executed 
predicate true operations executed 
predicate false operations executed 
clearly schedule illegal executed 
need versions code iteration replicated schedule 
code iterations overlapped combinations resulting complicated code expansion 
notation true false true correspond values predicate successive iterations true false true 
clearly combinations boolean values 
solution problem termed hierarchical reduction 
scheduling branch conditional separately code branches scheduled understanding branches executed run time 
words schedule created legal regardless branch taken needs branches considered 
resource conflicts adjusted point time union resources required different branches available requiring sum resources general register specifiers adjacent spaced reflect lifetime registers 
term union indicate operations execute due opposite values predicates compete resources 
available 
copy pipeline needed scheduling physically copies exists 
hardware implementation idea involves predicated execution 
predicated execution possible execute operation conditionally 
jumping operation executed hardware just ignore effects operation 
example floating point multiply specified executed predicate true 
false operation ignored treated op executed result register changed 
implementation saves effort implementation allows operation executed instruction predicate computed 
results predicate evaluation known target register written overlap possible 
hardware support may eliminate physical jump 
possible modulo schedule loops containing conditionals code expansion reduce code expansion caused distinct prelude postlude 
registers predicates stored rotating file 
predicates share regular rotating register file dedicated predicate register file 
process converting code predicate code termed conversion integral part enhanced modulo scheduling ems 
conditional branches removed control dependences data dependences conditionally executed operations data dependent operation generates predicate depend 
technique flexibility hierarchical reduction hierarchical reduction completely schedules conditional code attempts schedule operations 
arbitrary decision scheduling branch negatively impact placement operations considered pre scheduling phase 
predicated execution schemes operations instruction fetched true predicates complete execution 
ems hierarchical reduction limited number operations physically execute predicate value number operations predicate values 
cases operations execute disjoint predicate values scheduled time require non sharable resource 
proponents method enthusiastic recommend technique processors support predicated execution 
technique called reverse conversion simplifies process global scheduling 
ems advantage hierarchical reduction paths done 
increased flexibility produces superior code 
side effect predicated execution avoids having special instructions prelude postlude 
termed kernel code 
shows code requires distinct prelude postlude 
having distinct prelude postlude executes subset kernel operations execution various stages pipe controlled predicates 
notation 
indicates operation executes true 
stage pipeline associated different predicate 
predicates set operation jumps top loop 
predicate file rotates decrementing icp just regular rotating register file 
predicate register file shifted ii time steps 
code set predicates sets register specifier physical register changes predicates eventually effected 
logical predicate set true prelude set false postlude 
code example executed times predicates take values indicated 
logical predicate pointed icp physical register 
notice time step icp decremented old value new logical set 
true 
shows operations executed point time 
enhanced modulo scheduling modulo scheduling techniques basically differ handle predicates 
hierarchical reduction schedules operations branch conditional construct combining union requirements 
pre scheduling requirements creates complicated pseudo operations difficult schedule efficiently operations 
enhanced modulo scheduling uses conversion convert operations straight line predicated code 
form scheduling done noting disjoint operations conflict 
need preschedule various parts conditional construct 
modulo scheduling modulo variable expansion rename registers lifetimes distinct iterations 
predicated execution disadvantage operations taken branches executed results just thrown away 
resource requirements required sum requirements branch 
enhanced modulo scheduling branching structure termed reverse conversion inserting conditional branch instructions eliminate predicated execution 
words predicated execution allow operations scheduled independently branching structure branching structure reinserted 
method real benefits terms simplicity hampered fact technique prone code explosion 
predicated operation predicated happens scheduled early imperative operations predicated operation scheduled operation predicated cloned appear branch 
code predicated overlapped times code expansion order clearly unacceptable 
various techniques employed limit code explosion important restriction blocks scheduled 
term hyperblock denote set blocks scheduled 
initial simplicity scheduling regard predicates results complexity introducing conditionals back code 
insertion branches done code scheduling various code inefficiencies introduced common phases segmented 
enhanced modulo scheduling performs better hierarchical reduction increase code size 
increase code size undoubtedly results fact tighter code overlaps conditional constructs increase results unnecessary elongation predicated region 
kernel recognition modulo scheduling algorithms create kernel scheduling iteration legal overlapped ii cycles techniques schedule various iterations recognize kernel formed 
authors term type software pipelining algorithm unrolling algorithms term kernel recognition accurate may physical unrolling 
proponents modulo scheduling point need search kernel flaw proponents kernel recognition counter searching pattern done hashing efficient repeating scheduling various goal initiation intervals 
kernel recognition proponents argue general resource model hashing done encoding entire state scheduler point time 
oe 
kernel code predicate values time operations enabled predicates ability achieve fractional rates effortlessly algorithms superior 
modulo scheduling algorithm claim ii rarely incremented original minimum initiation interval 
obviously modulo scheduling achieve fractional rates replicating loop body scheduling 
increases complexity may result full iterations prelude postlude removed 
attempt kernel recognition techniques mimic attempt scheduling hand 
basic idea look iterations time try combine operations dependent 
steps follows 
unroll loop note dependences 

schedule various operations early data dependences allow 

look block consecutive instructions identical blocks 
block represents new loop body 
rewrite unrolled iterations new loop containing repetitive block loop body 
obvious question block repeating instructions surfaces 
unrolling pipelining software pipelining algorithm designed su solves problem ad hoc techniques force kernel moving duplicate operations section code containing operations 
moving operations scheduling introduces utilized instructions produces inferior results 
su developed extension basic algorithm loops containing multiple basic blocks abnormal entries conditional exits handled 
unfortunately problems carried complicated version 
interesting historical perspective performed terms target code execution time techniques 
sections explain independent kernel recognition type software pipelining algorithms 
perfect pipelining perfect pipelining combines code motion scheduling 
achieves fractional rates handles general dif min pairs 
techniques assist formation pattern somewhat ad hoc 
historically effectiveness local scheduling limited due small size basic blocks 
new architectural model multiple tests performed single instruction greatly enhances degree parallelism achieved 
aiken nicolau introduce perfect pipelining algorithm general machine model 
method important problem changing parameters 
answers question software pipelining effected architectures modified support better 
perfect pipelining somewhat similar method su loop pre scheduled unrolled overlapped sophisticated operations may move independently pre scheduling loops span multiple blocks easily accommodated 
perfect pipelining enhanced modulo scheduling powerful previous techniques take advantage architecture multi way branching 
multi way branching architecture allows instruction aiken consider perfect pipelining algorithm framework algorithms built 
perfect pipelining algorithm reader substitute possible algorithms perfect pipelining framework 
algorithm referenced aiken considers 
delta delta delta delta delta deltaff ae ae ae ae ae cc cc perfect pipelining instruction model branch target locations multiple boolean conditions 
illustrates basic instruction executable time unit model 
cycle executed control transfer depending values cc cc predicates set instruction 
instructions called tree instructions 
assignment operations executed destination selected simultaneously 
take advantage type architecture type global code motion called migration implemented 
migration improvement early trace scheduling copies merged code motion tied code correcting compensation motion directly cost benefit considered 
versions trace scheduling adopted improvements 
types code motion enumerated 
addition operation join multiple copies operation move past branch point termed unification important reduces code explosion 
importance unification multiple copies operation generated moving past branch points recombined 
aiken nicolau perform global code motion loop software pipelining simplify initial schedule 
global code motion performed loop unrolled unspecified number times result scheduled assuming infinite resources 
loop performing code motion unrolling significantly speeds pipelining 
pipelining algorithm need repeatedly perform similar code motions copy 
assumption infinite resources initial scheduling step algorithm requires motion 
constraint finite resources enforced sequentially ordered motion operation instructions limited fact operation may able temporarily reside instruction resource conflicts operations placed 
allowing free code motion algorithm ffifl fflfi ffifl fflfi ffifl fflfi ffifl fflfi oe iterations similar nodes different numbers intervening operations suffer race operation got node 
results somewhat history sensitive depending order operations moved best possible schedule 
perfect pipelining allows pattern form naturally 
successive instruction scheduled determine schedule begun repeat 
state schedule specific instruction represent set information controls operations may scheduled succeeding instructions 
general resource model state include resources committed previous scheduling operations persistent resource requirements 
operations latencies state include concept elapsed time dependent operations 
determine new instruction generated really necessary previous state schedule current state 
nodes reduced state said functionally equivalent 
instruction functionally equivalent earlier node replaced branch instruction creating loop 
nodes functionally equivalent 
clearly nodes look alike demonstrates sufficient 
identical state different succeeding instructions 
assume form loop operation appears proposed loop times operations appear twice 
obviously instructions form loop postlude static different depending number iterations executed 
instructions form loop evident fact keep repeating rest schedule 
illustrates case code equivalent nodes represents multiple iterations original loop exactly iteration seen algorithms 
code space increased advantage code scheduled tightly 
example advantage having copies loop performing iteration oe sw ddg result perfect pipelining results modulo scheduling result replicating times modulo scheduling instructions achieve iterations instructions 
case multiple iterations represent improvement 
ideal rate require single iteration loop execution time 
achieve ideal effective execution time modulo scheduling replicate see 
operation expanded operations indicated superscript due replication 
cases multiple iterations new loop body benefit achieved 
modified version shows case happens 
optimal result terms execution time modulo scheduling shown comparison 
ffifl fflfi ffifl fflfi ffifl fflfi ffifl fflfi ffifl fflfi ffifl fflfi ffifl fflfi oe ddg result perfect pipelining result modulo scheduling algorithm functionally equivalent nodes fx set operations iteration scheduled instruction 
referred set fb instruction set fd instructions similar contain operations superscripts operation differ constant 
similarity necessary functional equivalence sufficient 
functionally equivalent nodes nodes interchangeably schedule graph 
instructions similar functionally equivalent 
find similar instructions hashing compare state including resources committed persistent irregular operations ensure functional equivalence 
early versions algorithm limited fact true functional equivalence guaranteed 
example fc fc similar instructions scheduled different repeating pattern formed 
ddg see case lack similarity indicates nodes functionally equivalent 
cycle involving nodes initiated instruction cycle involving repeats instructions 
nodes containing operations similar pair operations instructions differ constant subscript 
procedure find functionally equivalent nodes include full state information 
dependence cycles capable different execution rates seen slow faster order get pattern 
accomplished forcing set operations considered scheduling contain operations fixed range iterations 
span limit indicates operations iteration scheduled operations iteration span scheduled 
stated way operation scheduled times operation scheduled times gamma 
span 
value span experimentally determined 
span small schedule form 
span large prelude postlude overly complicated 
schedule results span 
notice delayed scheduled 
perfect pipelining applied loop body consisting single basic block unlimited resources time optimal pipeline formed 
methods may form time optimal pipeline force contain single copy operation may delay start iteration 
hand greedy scheduling algorithm perfect pipelining waits pattern naturally develop 
consists single basic block resources unlimited produces time optimal pipeline 
example see power technique consider branches loop 
consider piece code shown taken builds list 
call append dependent previous call append serialized 
iterations loop scheduled assuming tests performed instruction execution graph results nodes represent parallel instructions hashing search technique function key determine storage location 
instructions contents hash location easily identified 
ffl ffi fi fl ffl ffi fi fl ffl ffi fi fl ffl ffi fi fl ffl ffi fi fl acceptable pattern may occur intervention 
ddg pipelining span 
key list append list sample code fft delta delta delta delta delta delta sw delta delta delta delta delta delta sw delta delta delta delta delta delta sw gamma gamma gamma gamma gamma gamma gamma gamma gamma psi oe oe oe oe hj hj hj bn omega omega omega omega omega omega omega omega omega omega omega omega omega omega ae gamma gamma gamma gamma gamma gamma gamma gamma psi delta delta delta delta delta delta sw delta delta delta delta delta delta sw hj fft fft ft fft fff fft fft fff fff fft fff ft ft ft ft ft ft ft ft fff loop unrolling times ft fff fft fff fft ft loop pipelining arcs represent flow control 
superscript operation represents iteration number operation 
notice branches leaving instruction 
arc labels correspond values tests 
label fft indicates tests false test true 
asterisk indicates don care condition 
reduce number different instruction formats append performed tests successive iterations executed performed 
notice ability perform multiple tests single instruction greatly reduces execution time loop cases append operation occur value equivalent nodes identified graph shows final result perfect pipelined scheduling 
petri net model petri net algorithm uses rich graph theoretic foundation petri nets solve problem kernel recognition 
achieves fractional rates works general dif min pairs extendible 
power perfect pipelining replaced ad hoc techniques mathematically sound approaches 
petri net model allan rajagopalan lee provides valuable solution problems associated formation pattern terms forcing pattern occur recognizing pattern formed 
able recognize pattern formed aiding efficient formation pattern essential kernel recognition type software pipelining 
techniques kernel development needs assisted manipulating final schedule look alike instructions may masquerade loop entry points repeating pattern achieved 
problems elegantly eliminated petri nets 
algorithm improvement gau algorithm suffers limitations 
dif values greater handled gao algorithm replicating code difs 
initiation intervals achieved replication acknowledgment arcs create cycles force initiation interval 
fq fq fq petri net concurrency transitions independent fire simultaneously conflict transitions fire simultaneously input place pass token successor 
gao method complicated addition superfluous arcs frequently achieves non optimal initiation intervals due fact cycles having min dif ii inadvertently created 
petri net bipartite graph having types nodes places transitions arcs transitions places 
shows petri net 
transitions represented horizontal bars places represented circles 
initial mapping associates place number tokens 
place said marked 
associated transition set input places set output places 
set consists places arc petri net 
similarly consists places arc petri net 
marking instant defines state petri net 
petri net changes state firing transitions 
transition ready fire belonging weight arc reader may see similarity transitions runners relay race 
runner run fire baton token 
case runner pass baton teammates simultaneously runner may receive baton teammates running 
transition fires number tokens input place decremented weight input arc number tokens output place incremented weight arc transition place 
transitions fire earliest firing rule fire soon input places sufficient tokens 
arcs transitions transitions independent fired concurrently 
fire fired placed token place dependent place contains token fire transitions represents conflict resolved suitable algorithm 
petri net models cyclic dependences loop 
data dependence graph shows relationship operations nodes 
show operations ready scheduled point time 
petri net ddg current scheduling status embedded 
operation represented transition 
places show current scheduling status 
pair arcs transitions represents dependence transitions 
place transitions contains token signal operation executed second 
firing transition thought passing result operation performed node nodes waiting result 
petri net cyclic state may reached series firings petri net take state passed 
state petri net represented token count place 
decisions deterministic identical state identical reservation table means behavior petri net repeats 
min values greater handled inserting dummy nodes min greater 
example arc dif replaced arcs dif dummy nodes 
implementation increases node count greatly simplifies recognition equivalent states marking contains delay information 
algorithm handles min values zero doing special firing check nodes connected predecessors min times zero 
compile time increased accommodating min times zero 
arcs added ddg graph strongly connected eliminating problem leading chain synchronization 
benefit rate firing controlled node allowed sustain rate faster slowest cycle dictates 
arcs added new cycles created 
new cycle larger min dif contained original graph schedule necessarily slowed ii increased 
simplicity number added arcs kept minimum 
formally done follows 
ddg represent acyclic condensation strongly connected component replaced single node 
node exists set nodes correspond single source node node 
arbitrarily selected source node create set nodes source node corresponding node 
dummy node added add arc ddg 
node sink arbitrarily select node add arc dif 
values dif selected cycle formed min dif ratio close possible lower bound ii exceeding 
example ddg strongly connected 
acyclic condensation consists nodes representing sets nodes original graph 
nodes acyclic condensation sources 
dummy node created arcs added 
nodes acyclic condensation sinks 
node acyclic condensation sink nodes connected node 
results adding arcs ddg 
dif values selected force newly formed cycles min dif 
ddg modified strongly connected corresponding petri net created 
basically nodes transitions petri net arc pair arcs petri net places inserted dependent transitions keep track may fire 
formal rules follows source node predecessors 
sink successors 
upsilon sigma xi pi upsilon sigma xi pi upsilon sigma xi pi upsilon sigma xi pi upsilon sigma xi pi upsilon sigma xi pi upsilon sigma xi pi upsilon sigma xi pi upsilon sigma xi pi upsilon sigma xi pi upsilon sigma xi pi hq qh ak oe oe 
xxxxx marked places tokens schedule ddg strongly connected petri net schedule assuming infinite resources 
dummy transition shown schedule 

node ddg transition created 

arc ddg node node place created arcs point time transitions fire form parallel instruction schedule 
marking repeats software pipeline exists 
schedule repeats petri net enters state defined placement tokens 
discussion properties cyclic petri nets see 
initial placement tokens specified 
initially node dependent previous iteration ready executed previous iteration deny execution 
guess places model arcs represent loop carried dependences need marked 
question comes deciding tokens placed 
number tokens necessary depends dif value arc dif zero corresponds tokens 
represent operation th iteration 
dif arc indicates get iterations ahead follow unconstrained 
number tokens place just dif corresponding arc dependence graph 
formal rule follows 

initial marking petri net arc ddg dif value corresponding place tokens assigned 
allows node fired times node fired 
modeling resources restriction dependence modeled resource constraint 
single adder operations needing adder scheduled 
resource usage regular handled introducing resource places 
rules governing resource places follows 
resource place created 
place assigned number tokens number instances particular resource 

resource usage controlled requiring nodes need resource cyclically connected place 
node uses particular resource loop transition resource place back 
node needs token input places fire arc resource place requires resource available scheduling node 
arc resource place allows node return resource 
modeling persistent irregular resources requires complicated model 
resource places insure resource available point time availability current reservation state kept 
reservation state rs union reservation tables offset reflect starting time operations begun execution completed 
consider operation scheduled 
corresponding reservation table length row intersected binary corresponding row reservation state 
resulting row operation required instance resource arcs weighted number tokens required source 
non zero resource conflict operation scheduled 
conflicts row unioned binary corresponding row reservation state reflect current resource needs 
operations current time cycle scheduled reservation state advanced reflect passage time rs rs rs gamma 
behavior table schedule shown termed behavior table 
row indicates initial marking place petri net number tokens place shown parentheses 
column marked schedule indicates transitions fire current marking 
simplicity presentation assumed resource conflicts 
state instruction state instruction identical kernel starts instruction ends instruction 
kernel initiation interval marked box 
order incorporate persistent irregular resource usage row behavior table attached resource state describing current resource commitments 
simplicity attached resource state shown 
algorithm enhanced addition regulates greediness algorithm 
cycle dummy nodes added petri net min dif equal estimated ii passes tokens rest petri net control rate firing 
attempts force estimated pace repeating section formed quickly 
example ensure nodes fire frequently cycles required schedule exhibits property 
larger examples reduce compile time obtain schedule span kernel achieved initiation interval 
general model software pipelining advantage easily extendible 
example predicates loop body handled passing token paths representing branches having merge node fire receives token branches 
control dependence treated form data dependence 
petri net pnp technique performs optimizations renaming forward substitution improve performance loops containing predicates 
petri net approach solves problem aiken faced having parts graph execute different rates 
making graph strongly connected forces transitions fire rate 
number tokens resource place reflects number resources type 
transition fires transitions competing resource prohibited firing token available 
maintain determinism selection algorithm consistent choosing candidates fire competing choices 
priorities scheduling similar scheduling algorithms 
transition augmented execution count incremented time transition fires 
transitions able fire selection algorithm able fire transition fired earliest iteration 
transitions prioritized iteration transitions part critical cycles selected 
petri net approach solves problem persistent irregular resources 
procedure identify identical state broken steps 
step uses hashing encoding tokens place discover tokens identical configuration 
state matching hash list step checks see current global reservation table agrees global reservation table previous step 
sufficiently large hash table number entries check small test identical state proceeds single element differs check identical state requires minimal effort 
comparison methods pnp similar spirit perfect pipelining 
problem perfect pipelining difficulty determining nodes functionally equivalent pattern repeating 
petri net uses combination places global reservation tables record state information repeating pattern reliably determined 
perfect pipelining creates kernels duplicate operations doing improve performance 
example loop length copies operation effective initiation interval generated loop length copy operation equivalent 
results problem scheduling node greedily 
node allowed execute fast possible long gaps occur schedule preventing pattern forming operations scheduled frequently effective initiation interval 
operations delayed satisfy dependence constraints 
regulates rate operations rate fluctuations increase code size 
thorough discussion problems see 
pnp algorithm compared lam algorithm loops low high resource conflicts 
low resource conflicts pnp algorithm marginally better 
high resource conflicts pnp algorithm shows significant improvement lam algorithm 
pnp finds lower ii fractional rates required 
lam algorithm schedules strongly connected component separately fixed ordering nodes belonging strongly connected component schedule 
fixed ordering resource conflicts nodes strongly connected component nodes schedule restricts overlap 
forces lam higher value ii cases optimal 
pnp algorithm compared vegdahl technique performs exhaustive search possible schedules look schedule shortest length 
pnp compares quite favorably exhaustive technique 
compile time pnp negligible compared vegdahl exhaustive technique 
vegdahl technique techniques discussed survey vegdahl represents exhaustive method possible solutions represented best selected 
software pipelining np complete algorithm impractical real code 
vegdahl uses exhaustive technique relies decision graph choices 
method uses dynamic programming scheduling algorithm schedule straight line code formed loop body unrolling 
vegdahl builds operation set graph nodes represent set operations scheduled far arcs represent series scheduling decisions 
essence nodes represent state information similar usage markings petri net algorithm difference operation set graph represents possible scheduling decisions point time single decision 
concept decision tree diagram graph nodes representing equivalent states merged 
considering loop carried anti dependences directly vegdahl defines duals dependences represent anti dependences 
appears data representation inherent part concept 
automatic interpretation dependence arcs disadvantage register renaming eliminates loop carried anti dependences 
consider partial operation set graph example 
span schedule obtained pnp reasonable apply vegdahl exhaustive technique loop body unrolled twice copies exist allows operations adjacent iterations scheduled 
initially operations scheduled represented initial node label 
choices scheduled scheduled scheduled 
choices represented arcs emanating initial node 
suppose scheduled choices time step scheduled operation iteration scheduled scheduled 
choices represented arcs coming node labeled 
note node labeled indicates operation scheduled arc labeled indicates scheduled time step 
scheduled time step scheduled second time step node reached 
notice node reached operations scheduled initially 
scheduling process follows 
create empty initial node mark open 

exists open node select open node curr 
observing resource conflicts dependence constraints consider possible operations scheduled operations curr executed 
exec possible set operations executed 
represent new state formed executing exec curr previous state 
contains operation iteration reduction applied removing set operations shifting remaining operations iteration 
example execute node results node existence node create arc curr node label exec 
create new node representing create arc curr label exec mark new node open 
repeat step possible set exec 

open nodes locate cycles graph 
cycles minimum length possibilities kernel software pipeline 
kernel recognition handled merging equivalent nodes followed finding minimal cycles 
example cycle shown bold consists instructions represents kernel 
possible schedule shown schedule nonoptimal schedule indicated numerous possibilities exist omitted operation set graph interest readability 
method generate prelude postlude code easily generated kernel 
order evaluate cycle minimal length best prelude postlude code considered 
algorithm exhaustive produces optimal results obviously inappropriate graphs size 
algorithm works best graph tightly constrained resource conflicts fewer state nodes operation set graph possible 
algorithm similarities petri net algorithm 
min values greater implemented introducing dummy nodes 
vegdahl states dif values greater handled fully tested 
dummy nodes addresses problem non unit latencies persistent resource usage modeled 
vegdahl mentions fractional rates method clearly potential achieving fractional initiation intervals 
vegdahl algorithm differs petri net partial operation set graph example 
graph large representative parts graph shown 
algorithm exponential time complexity fact physically loop sufficient number times algorithm begins 
upper bound number nodes operation set graph cardinality power set set containing operations unrolled loop 
size unrolled loop may negative effect complexity important able predict amount unrolling necessary achieve optimal schedule 
nodes operation set graph represent legal state firing sequence constrained dependences resource conflicts 
predicates implemented author possibility conditionals handled combining algorithm trace scheduling 
algorithm impractical complexity may useful small examples underscores reason researchers look heuristics 
enhanced pipeline scheduling enhanced pipeline scheduling integrates code transformation scheduling produce excellent results 
important benefit ability schedule various branches loop different ii ebcioglu nakatani propose algorithm modulo scheduling kernel recognition algorithms 
enhanced pipeline scheduling uses completely different approach software pipelining building code motion perfect pipelining retaining loop structure kernel recognition required 
algorithm quite complicated understand code produced 
benefits algorithm achieves combines code transformation software pipelining 
power set set set subsets set having elements power set members 
code motion pipelining described process shifting operations forward execution schedule 
operations shifted loop control predicate move ahead loop loop prelude move back bottom loop operations iteration 
degree shifting able compact loop depends resources available data dependence structure original loop 
algorithm similarity perfect pipelining uses modification perfect pipelining global code motion algorithm differs significantly enhanced pipeline scheduling manipulates original flow graph create require searching pattern unrolled code 
loop manipulated individual iterations enhanced pipeline scheduling encounter problems pattern identification inherent perfect pipelining 
software pipelining occurs operations moved back edge allowing enter loop body operations iterations 
pipelining algorithm iterates loop carried dependences considered 
pipeline prelude postlude generated automatic code duplication accompanies global code motion 
model handle min times greater inserting dummy nodes 
algorithm handle persistent irregular resources 
instruction model enhanced pipeline scheduling powerful algorithm utilize multi way branching architecture conditional execution 
conditional execution feature machine model powerful original perfect pipelining machine model includes multi way branching 
versions perfect pipelining included multi predicated execution 
order take advantage execution powerful software pipelining algorithm required 
shows typical control flow graph node termed tree instruction 
labels control flow graph nodes 
condition codes cc cc computed node entered statements path root leaf executed 
operations selected path executed concurrently old values operands 
example cc true left branch cc false executed operations executed simultaneously control transferred assignment effect types resource constraints placed machine model 
limited total number operations contained tree instruction 
limited fixed number different paths tree instruction 
conditional execution operations placed branch cfg node traditional machine model operations precede conditional branch operations 
enhanced pipeline scheduling utilize features scheduling technique require powerful architecture 
architectures represented tree instruction restricting form instruction 
example architectures support multi way branching conditional execution modeled tree instructions 
back edge data dependence graph edge head dominates tail flow graph locate natural loop 
node dominates node graph path source contain control flow graph cfg graph nodes represent computations edges represent flow control 
delta delta delta delta deltaff au cc cc gamma tree instruction enhanced pipeline scheduling global code motion renaming forward substitution nakatani ebcioglu renaming forward substitution move operations past predicates shorten dependence chains involving anti dependences 
renaming involves replacing operation op operations op op defines variable copy statement assignment free move outside predicate 
op moves predicate controls execution statement said speculative statement useful branch taken 
illustrates shortening data dependence chain involving anti dependence control dependence chain renaming shows original dependence graph shows graph renaming 
dependence eliminated forward substitution 
assignment statement var expr uses var subsequent instructions replaced expr termed forward substitution 
particularly useful result forward substitution operations executed simultaneously 
forward substitution change referenced value shows renamed 
notice case length dependence chain graph decreased 
see flexibility model node represent predicate transformations data dependences applied control dependences 
forward substitution may collapse true dependences prevent code motion 
true dependence operations may forward substitution copy operation operations involve constant immediate operand 
forward substitution performed true dependence collapsed 
shows example forward substitution renaming required move operation 
diagram shows part cfg operation moved instruction instruction note multiple tree instructions shown 
rectangular node authors term combining 
original dependence graph dependence graph renaming forward substitution dependence graph renaming ae ae ae delta delta delta deltaff ae ae ae delta delta delta deltaff cc cc cc cc cc cc example code motion renaming forward substitution fence fence fence fence iterations iterations iterations iterations enhanced pipeline scheduling data dependence graph loop control flow graph loop filling instruction filling second instruction filling third instruction final software pipeline entry point tree instruction 
dashed boxes represent branches tree instructions indicated 
true dependence involves immediate operand making dependence candidate forward substitution 
renamed giving forward substitution performed giving 
constant folding performed giving 
see advantage renaming moved eliminating motion requires forward substitution notice total execution time code reduced 
pipelining loop perform pipelining code moved backward control flow arcs 
algorithm consists phases repeat operations opportunity move 
phase code current loop body moved early dependences allow 
parallel instruction loop termed fence 
name derived fact code moved rest loop position early possible loop past fence 
fence bounds code motion 
phase fence instruction duplicated moved 
code loop body duplicated moves past top loop control predecessors 
code moved loop forms prelude 
code joins code bottom loop body represents originally performed different iteration 
process continues statements original loop body opportunity move 
example consider example 
statements loop connected true dependences 
fence instruction contains operation operations loop move shown 
fence instruction filled moved top loop part loop prelude 
fence considered filled resources consumed nearly consumed code motion fence exhausted 
early versions algorithm code migrated upwards known code executed early 
wasted motion caused race conditions eliminated versions 
filled fence duplicated follow loop back edge cfg rejoins loop bottom 
back edge shown represents fact executed motion back edge moves operations iterations 
software pipelining achieved 
non unit latencies handled inserting dummy nodes provision persistence resource constraints 
fence instruction consisting operation moved prelude back loop seen copies moves back loop second iteration dependent operations iteration 
operation moves way new fence instruction 
stage fence consisting moved prelude back loop 
second third iterations respectively depend operations iteration move top loop 
step fence moved prelude back loop operations iteration respectively 
case iteration dependent iteration operations move fence 
operations considered fence pipelining algorithm terminates 
general operations previous fence instructions move way current fence data dependences permit allowing operations iterations move fence moved back edge 
process continues instructions original loop body moved top chance fence data dependences loop seen 
note stage process valid loop maintained 
enhanced pipeline scheduling uses cfg dynamic data dependence information loop body 
node cfg represents tree instruction containing operations executed concurrently 
initially instructions contain single operation 
figures illustrate process sample loop 
operation shown separate instruction 
enhanced pipeline scheduling code motion top loop 
instruction predecessors loop independent subgraph termed fence shown dotted box 
shows loop possible operations moved false right branch cc conditional upward far possible 
convention move operations false branch considering possible code motion true branch 
note operation renamed moves past branch cc live true branch 
upward migration movement operations successors node attempted code motion allowed node 
example identical operation performing code motion successors allows duplicates recognized merged 
copies move create redundant code 
duplicate copies created code motion algorithm important 
merging termed unification identical operations branch points cfg key feature perfect pipelining enhanced pipeline scheduling order code motions figures follow strict order algorithm 
result order aids presentation 
strict sense algorithm possible migrations attempted migration allowed instructions located 
variable live path redefined 
delta delta delta deltaff omega omega omega omega au cw gamma gamma gamma gamma psi omega omega omega omega ae omega omega omega omega ae gamma gamma gamma gamma psi 
cc exit cc cc cc exit cc cc cc cc loop 
dotted box represents fence 
loop migrating false branch cc conditional oe gamma gamma gamma gamma delta delta delta delta delta delta delta 
oe 
bn omega omega omega omega ae delta delta delta delta deltaff exit cc cc cc exit cc cc cc cc cc cc cc loop filling fence loop moving filled fence bottom loop distinguishes global code motion global code motion algorithms shows loop fence instruction filled 
note operation moves past cc branch need renamed dead branch moves past cc branch renamed live loop exit 
upward code migration stops code movement possibilities encountered pass exhausted 
code motion stops movement exists 
empty instructions removed cfg eliminated 
note removing empty instructions causes problems adhering required min times 
necessary delays achieved insertion dummy nodes dummy nodes instruction prevent instruction eliminated 
persistent resources handled model 
insertion empty instructions removal empty instructions causes problems strict timing constraints consecutive instructions imposed persistent resources 
code motion stops current fence marked filled immediate successors current fence new fence instructions 
shows filled fence instruction migrating upward past join point loop program enters loop loop back edge returns top loop 
motion takes place instruction enters pipeline prelude enters features added version trace scheduling variable dead defined branches 
loop instruction iteration 
old fence instruction seen bottom loop operations contained moved note forward substitution uses operations old fence move false branch cc conditional 
analogous transformation required true branch empty 
moving filled fence instruction creates copies fence path entering loop loop 
example copies produced predecessors operations move software pipeline created 
new fence instructions filled migration process 
enhanced pipeline scheduling repeatedly fills fence instructions loop contains instruction loop prelude contain operations sufficient number iterations see loop carried dependences 
shows loop possible operations moved false branch cc conditional upward far possible 
operations resulted unification moved point 
note copies operations cc appeared branches condition cc merged single operation move past branch point 
note renamed moves past assigned shows loop fence instruction filled 
shows final pipelined loop 
example tree instruction 
enhanced pipeline scheduling starts large loop reduces size pipelining proceeds 
loop valid algorithm iterates loop shrink size 
occurs loop contains instruction loop carried dependences seen 
copy operations appear redundant 
copies needed live exit loop 
note operations originally assigned executed speculatively place results renamed copies copy operations insure correct value assigned loop termination 
resulting loop pipelined loop body prelude postlude generated automatically code duplication occurs operations moved cfg 
pipeline postlude exit branch moves past operations loop 
postlude generated conditional branch operation exit test moved upward past code loop 
branch moved operations moves past appear branches resulting loop postlude 
example shows part cfg branch operation moved note copied exit part postlude branch executed regardless outcome conditional branch original loop 
pipeline postlude enhanced pipeline scheduling typically small due renaming 
reducing code expansion enhanced pipeline scheduling places restriction movement operations filled fence reduce code expansion occurs copying operations predecessors node 
operations allowed move filled fence instruction operations move operations moved filled fence instruction free move independently 
requirement node predecessors possible code motion generate versions node execution time benefit 
restriction result accepting local minimum achieving global minimum 
enhanced pipeline scheduling theta theta theta theta theta oe cw 
oe cw gamma gamma gamma gamma psi cc exit cc cc cc cc exit cc cc cc cc cc loop migrating false branch cc conditional loop filling fence oe omega omega omega ae gamma gamma gamma psi cw qs cc exit exit cc cc exit cc cc cc cc cc cc cc cc final pipelined loop gamma gamma delta delta delta deltaff delta delta delta deltaff cc exit exit cc example copying operations branches moved branch operation optimal ii value trying achieve stops improvements seen 
shows example code duplication 
moving operations move forward substitution collapse true dependence move predecessor operation move predecessor code motion allowed operations allowed move independently copies needed containing containing filled fence type code duplication prevented move movement occur 
code duplication expensive enhanced pipeline scheduling tries reduce 
improvement enhanced pipeline scheduling ebcioglu introduce windowing technique designed reduce code expansion 
enhanced pipeline scheduling produces pipeline assuming infinite resources instructions extremely large renaming forward substitution repeatedly removing movement blocking dependences 
order reduce due finding parallelism utilized recommend movement operations allowed number instructions termed window size current fence instruction 
window size set value gives results particular application 
disadvantage technique limit number iterations executing concurrently window size small new operations iterations movement window fence instructions filled slowing rate migrate back current fence instruction 
summary table summarizes attributes various algorithms 
column path represents path algebra algorithm 
pred represents various predicated modulo scheduling algorithms 
perfect represents aiken nicolau perfect pipelining 
eps represents enhanced pipeline scheduling 
gamma gamma omega omega omega omega omega omega omega ae omega omega omega omega omega omega omega omega ae au example code duplication table comparison algorithms lam path pred ems perfect petri vegdahl eps fractional rates dif min dummy dummy dummy resource constraints np np complexity poly poly poly poly poly poly code expansion limited rate leading chain problem regular kernel recognition modulo table divided pieces 
upper section shows factors influence efficiency target code produced practicality algorithm 
middle section shows factors influence code size compile time 
bottom section lists attributes descriptive nature effect efficiency code produced generality algorithm may help classify algorithms 
table extremely useful determining best algorithms suggesting possible modifications enhance capabilities 
row labeled fractional rates indicates algorithm achieves fractional initiation intervals having replicate scheduling 
algorithms achieve fractional initiation intervals replication suffer inability easily decide appropriate replication factor 
algorithms address min times greater integral part technique introduce dummy nodes remove occurrences 
dif values greater handled algorithms eps replicate scheduling remove occurrences 
algorithms deal general resource constraints path algorithm 
serious limitation algorithm 
resource constraints expressed limit number functional units conflict graph resource vector heterogeneous resource requirements 
methods handle irregular persistent resources path ignores resource constraints marked np deal non persistent resources 
algorithms complexity 
vegdahl algorithm impractical moderate sized problems 
ems worst case complexity resulting exponential code explosion cases rare 
code expansion refers increase code size caused operation repeated multiple times schedule counting prelude postlude 
method achieves fractional rate incurring code expansion 
lam code expansion comes scheduling operations branches versions branch code modulo variable expansion eliminate loop carried anti dependences 
hardware support predicated modulo scheduling eliminates code expansion 
transformations eps uses reduce length cyclic dependences introduces code expansion 
limited rate indicates algorithm uses idea target ii making decisions 
modulo methods lower bound ii order save effort 
addition single copy operation exists kernel rate automatically limited ii kernel recognition algorithms lack ability control greediness initiating iterations possibly resulting greater compilation time longer initiation intervals greater execution time greater span increased length prelude postlude 
petri net algorithm achieves limited rate enhancement algorithm integral feature 
perfect rate indirectly limited setting maximum span single instruction new operations execute set span exceeded 
vegdahl exhaustive searches schedules altering algorithm limit rate execution greatly reduce compile time 
eps limited rate exist se rate limited number times operation moved past back edge window size 
leading chain problem refers problem algorithms experience nodes successors operations critical cycle 
leading chain creates problems algorithms measured rate unconstrained 
perfect solves leading chain problem delaying operations evident executing different rates 
possible methods handle predicates loop conversion 
eps capable achieving different ii branch predicate 
lam schedules subproblems combining 
eps perfect transformation 
petri path perform transformations prior scheduling improve pipelining results 
row labeled regular indicates operation prelude postlude executed interval executing kernel 
vegdahl method specify prelude postlude generated assume regular schedule produced 
regular schedule advantage disadvantage 
prelude postlude execute just kernel operations omitted predicated execution kernel code possible 
operations omitted separate scheduling prelude postlude may efficient 
minor point conversion regular schedule irregular schedule vice versa trivial transformation 
algorithms modulo kernel recognition eps 
modulo scheduling algorithms waiting pattern form modulo techniques analyze ddg create desirable schedule 
tight coupling scheduling pipelining constraints results pipeline near optimal 
ability adjust scheduling technique control register pressure prioritize various attributes gives great flexibility 
regular pipeline produced simplifies formation prelude kernel postlude 
replication suggested fractional rates achieved 
trial error approach finding achievable ii increases compile time software pipelining algorithm tight bound compile time 
uses technique produces results similar modulo scheduling techniques uses path algebra framework 
algorithm impractical handle resource constraints important elegant way formulates solves problem 
concise algorithm technique provides excellent conceptual model 
modulo scheduling techniques continued improve hardware support reduce code expansion allow tighter schedules rotating register files 
methods represent excellent choice software pipelining drawback fractional rates achieved replication scheduling 
perfect pipelining perfect pipelining important historical perspective due early exploration pipelining involving branches body loop 
unification important addition earlier global code motion techniques 
conceptually perfect pipelining advantage forced consider paths loop simultaneously 
words various paths loop able achieve different initiation interval 
frequency achieving optimal results hampered ad hoc nature scheduling 
main disadvantage perfect pipelining determining nodes functionally equivalent 
problems include generation loops contain copies original iteration needed need help pattern develop 
petri net model petri net model excellent choice software pipelining due strong mathematical orientation flexibility adapting wide variety constraints 
general reservation models pose problem adaptable method 
produces excellent schedules drawback need search pattern 
vegdahl vegdahl method interesting theoretical tool 
exponential complexity practical technique serves optimal small code size 
method adapted persistent resources significantly increase run time 
vegdahl method combination techniques 
example modulo scheduling determine span ii vegdahl exhaustive technique greatly restricted explore solutions span ii slightly better achieved 
parallel implementation greatly reduced search space algorithm reasonable broader class problems 
enhanced pipeline scheduling enhanced pipeline scheduling unique 
deal multi block loops maintains legal loop structure trying rebuild loop 
enhanced pipeline scheduling handles general loops 
renaming forward substitution utilized years degree successfully employed technique noteworthy 
enhanced pipeline scheduling advantages perfect pipelining 
algorithm increases speed convergence retaining loop construct reducing resulting code size 
techniques cause problems prematurely forcing operations remain restrict parallelism 
disadvantages lessened majority loop dependences removed automatic renaming combining 
serious drawback method persistent resources 
instructions inserted removed scheduling persistent resources require fixed set resources offset instruction initiation accommodated 
fractional rates achieved 
software pipelining effective technique extracting parallelism loops thwart attempts vectorize divide processors 
speedup modest noted software pipelining succeeds methods fail applied techniques extracted coarse grain parallelism 
variety architectures benefiting software pipelining underscores importance 
np complete scheduling problem software pipelining numerous effective heuristics developed 
resource conflicts cyclic dependences produce lower bound initiation interval 
lower bounds computed polynomial time 
considering algorithmic features low complexity ability deal conditionals accommodation resource conflicts achievement fractional initiation intervals current methods various degrees success 
researchers applying artificial intelligence techniques problem software pipelining 
uses genetic algorithms instruction scheduling 
neill allan genetic algorithms simulated annealing solve problem software pipelining 
expect see improvements application artificial intelligence techniques scheduling problems 
acknowledgments authors rau insight history modulo scheduling painstaking attention details 
anonymous referees 
quality tremendously improved comments 
supported part national science foundation cda iri 
aho sethi ullman 
compilers principles techniques tools 
addison wesley reading ma 
aiken 
compaction parallelization 
phd thesis department computer science cornell university ithaca ny 
aiken nicolau 
development environment horizontal microcode 
ieee trans software engineering may 
aiken nicolau 
optimal loop parallelization 
proceedings sigplan conference programming language design implementation pages atlanta ga june 
aiken nicolau 
perfect pipelining new loop optimization technique 
proceedings european symposium programming springer verlag lecture notes computer science pages atlanta ga march 
aiken nicolau 
realistic resource constrained software pipelining algorithm 
technical report rj ibm research division san jose ca october 
allan neill 
software pipelining genetic algorithm approach 
pact pages montreal canada august 
allan rajagopalan lee 
software pipelining petri net 
working conference architectures compilation techniques fine medium grain parallelism pages orlando fl january 
banerjee shen kuck 
time parallel processor bounds fortran loops 
ieee transactions computers september 

instruction scheduling genetic algorithms 
phd thesis colorado state university fort collins 
beck yen anderson 
architecture implementation 
journal supercomputing pages may 
mauricio jr architecture synthesis high performance application specific processors 
phd thesis carnegie mellon university pittsburgh pa 
chandy kesselman 
parallel programming 
ieee software november 
davidson 
design control pipelined function generators 
proc 
international ieee conference systems networks computers pages mexico january 

hsu bratt 
overlapped loop support 
proceedings third international conference architectural support programming languages operating systems pages boston ma april 

compiling 
journal supercomputing pages may 
ebcioglu 
compilation technique software pipelining loops conditional jumps 
proceedings th microprogramming workshop micro pages colorado springs december 
ebcioglu nakatani 
new compilation technique parallelizing loops unpredictable branches vliw architecture 
gelernter editor languages compilers parallel computing pages 
mit press cambridge ma 
ferrante ottenstein warren 
program dependence graph optimization 
acm transactions programming languages systems july 
fisher 
trace scheduling technique global microcode compaction 
ieee transactions computers july 
fisher 
trace scheduling technique global microcode compaction 
ieee transactions computers july 
freudenberger gross lowney 
avoidance suppression compensation code trace scheduler 
acm transactions programming languages systems july 
gao 
wong ning 
timed petri net model fine grain loop scheduling 
proceedings acm sigplan conference programming language design implementation pages june 
gao 
wong ning 
timed petri net model fine grain loop scheduling 
technical report acaps technical memo school computer science mcgill university montreal canada january 
altman gao 
minimizing register requirements resource constrained rate optimal software pipelining 
proceedings th annual international symposium microarchitecture 
appear 
hsu 
highly concurrent scalar processing 
phd thesis university illinois urbanachampaign urbana champaign ill 
hsu davidson 
highly concurrent scalar processing 
proceedings thirteenth annual international symposium computer architecture pages 
huff 
lifetime sensitive modulo scheduling 
conference record sigplan programming language design implementation pages albuquerque nm june 
acm press 
jones 
constrained software pipelining 
master thesis department computer science utah state university logan ut september 
jones allan 
software pipelining comparison improvement 
proceedings th international symposium workshop microprogramming microarchitecture micro pages orlando fl november 
kuck padua 
high speed multiprocessors compilers 
proceedings international conference parallel processing pages 
lam 
systolic array optimizing compiler 
phd thesis department computer science carnegie mellon university pittsburgh pa 
lam 
software pipelining effective scheduling technique vliw machines 
proceedings sigplan conference programming language design implementation pages atlanta ga june 
mahlke lin chen hank 
effective compiler support predicated execution hyperblock 
proceedings th annual international symposium microarchitecture micro pages portland december 
deo 
algorithms enumerating circuits graph 
siam journal computing 
nakatani ebcioglu 
combining compilation technique vliw architectures 
proceedings nd microprogramming workshop micro pages dublin ireland august 
acm press 
nakatani ebcioglu 
lookahead window compaction parallelizing compiler 
proceedings rd microprogramming workshop micro pages orlando fl november 
ieee computer society press 
nicolau 
environment development microcode pipelined architectures 
proceedings rd symposium workshop microprogramming microarchitecture pages orlando florida november 
neill 
software pipelining stochastic search algorithms 
master thesis department computer science utah state university logan ut 
rajagopalan allan 
specification software pipelining petri nets 
international journal parallel processing 
rau 
iterative modulo scheduling algorithm software pipelined loops 
proceedings micro th annual international symposium microarchitecture pages san jose ca november dec 
acm press 
rau fisher 
instruction level parallel processing history overview perspective 
journal supercomputing 
rau lee schlansker 
register allocation modulo scheduled loops strategies algorithms heuristics 
proceedings acm sigplan conference programming language design implementation pages san francisco ca june 
rau schlansker 
code generation schema modulo scheduled loops 
proceedings micro th annual international symposium microarchitecture pages 
ieee computer society press december 
rau yen yen 
departmental supercomputer design philosophies decisions trade offs 
ieee computer pages january 
rau 
scheduling techniques easily schedulable horizontal architecture high performance scientific computing 
fourteenth annual workshop microprogramming pages october 
rau 
architectural support efficient generation code horizontal architectures 
proceedings symposium architectural support programming languages operating systems pages march 
smith 
data structures form function 
harcourt brace jovanovich san diego ca 
su ding wang xia 
method global software pipelining 
proceedings th microprogramming workshop micro pages colorado springs december 
su ding xia 
extension software pipelining 
proceedings th microprogramming workshop micro pages new york ny october 
tarjan 
depth search linear graph algorithms 
siam journal computing june 

efficient search algorithm find elementary circuits graph 
communications acm 
lee schlansker 
parallelization loops exits pipelined architectures 
proceedings supercomputing pages november 
tokoro tamura 
approach optimization considering resource occupancy instruction formats 
proc 
th annual workshop microprogramming pages niagara falls new york november 
vegdahl 
dynamic programming technique compacting loops 
proceedings micro th annual international symposium microarchitecture pages 
ieee computer society press december 
vegdahl 
local code generation compaction optimizing microcode compilers 
phd thesis department computer science carnegie mellon university pittsburgh pa 
warter 
enhanced modulo scheduling loops conditional branches 
proceedings th annual international symposium microarchitecture micro pages portland december 
ieee computer society press 
warter mahlke 
hwu rau 
reverse conversion 
pldi pages albuquerque nm june 
acm press 
wolfe 
optimizing supercompilers supercomputers 
mit press 
wolfe 
tiny loop restructuring tool user manual 
oregon graduate institute science technology 
wood 
global optimization modular control constructs 
proceedings th microprogramming workshop micro pages hershey pa november 

efficient static scheduling loops synchronous multiprocessors 
phd thesis department computer information science ohio state university columbus oh 
zima chapman 
supercompilers parallel vector computers 
acm press cambridge ma 

