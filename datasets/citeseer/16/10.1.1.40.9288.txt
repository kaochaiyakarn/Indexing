hypergraph partitioning decomposition parallel sparse matrix vector multiplication member ieee computer engineering department bilkent university bilkent ankara turkey cs bilkent edu tr show standard graph partitioning decomposition sparse matrices reflect actual communication volume requirement parallel matrix vector multiplication 
propose computational hypergraph models avoid crucial deficiency graph model 
proposed models reduce decomposition problem known hypergraph partitioning problem 
proposed successful multilevel framework exploited develop multilevel hypergraph partitioning tool patoh experimental verification proposed hypergraph models 
experimental results wide range realistic sparse test matrices confirm validity proposed hypergraph models 
decomposition test matrices hypergraph models patoh hmetis result communication volume average graph model metis patoh times slower metis average 
index terms sparse matrices matrix multiplication parallel processing matrix decomposition computational graph model graph partitioning computational hypergraph model hypergraph partitioning 
partially supported commission european communities directorate general industry contract turkish science research council 
iterative solvers widely solution large sparse linear system equations multicomputers 
basic types operations repeatedly performed iteration 
linear operations dense vectors sparse matrix vector product form ax thetam square matrix sparsity structure coefficient matrix dense vectors 
goal parallelization computations iterative solvers rowwise columnwise decomposition matrix 

theta delta delta delta delta delta delta processor owns row stripe column stripe respectively parallel system processors 
order avoid communication vector components linear vector operations symmetric partitioning scheme adopted 
vectors solver divided conformally row partitioning column partitioning rowwise columnwise decomposition schemes respectively 
particular vectors divided xk yk respectively 
rowwise decomposition processor responsible computing linear operations th blocks vectors 
columnwise decomposition processor responsible computing linear operations th blocks vectors 
decomposition schemes linear vector operations easily efficiently parallelized inner product computations introduce global communication overhead volume scale increasing problem size 
parallel rowwise columnwise decomposition schemes require communication local computations considered pre post communication schemes respectively 
depending way rows columns partitioned processors entries entries may need communicated processors 
unfortunately communication volume scales increasing problem size 
goal find rowwise columnwise partition minimizes total volume communication maintaining computational load balance 
decomposition heuristics proposed computational load balancing may result extensive communication volume consider minimization communication volume decomposition 
dimensional decomposition worst case communication requirement gamma messages gamma words occurs nonzero column row rowwise columnwise decomposition 
approach checkerboard partitioning reduces worst case communication gamma messages gamma words 
approach worst case occurs row column submatrix nonzero 
computational graph model widely representation computational structures various scientific applications including repeated computations decompose computational domains parallelization :10.1.1.106.4101
model problem sparse matrix decomposition minimizing communication volume maintaining load balance formulated known way graph partitioning problem 
show deficiencies graph model decomposing sparse matrices parallel 
deficiency structurally symmetric square matrices 
order avoid deficiency propose generalized graph model section enables decomposition structurally nonsymmetric square matrices symmetric matrices 
second deficiency fact graph models standard proposed ones reflect actual communication requirement described section 
flaws mentioned concurrent 
propose computational hypergraph models avoid deficiencies graph model 
proposed models enable representation decomposition rectangular matrices symmetric nonsymmetric square matrices 
furthermore introduce exact representation communication volume requirement described section 
proposed hypergraph models reduce decomposition problem known way hypergraph partitioning problem widely encountered circuit partitioning vlsi layout design 
proposed models amenable advances circuit partitioning heuristics vlsi community 
decomposition preprocessing introduced sake efficient parallelization problem 
heuristics decomposition run low order polynomial time 
multilevel graph partitioning heuristics proposed leading fast successful graph partitioning tools chaco metis 
exploited multilevel partitioning methods experimental verification proposed hypergraph models approaches 
approach metis graph partitioning tool black box transforming hypergraphs graphs randomized clique net model section 
second approach lack multilevel hypergraph partitioning tool time carried led develop multilevel hypergraph partitioning tool patoh fair experimental comparison hypergraph models graph models 
objective patoh implementation investigate performance multilevel approach hypergraph partitioning described section 
released multilevel hypergraph partitioning tool hmetis second approach 
experimental results section confirm validity proposed hypergraph models appropriateness multilevel approach hypergraph partitioning 
hypergraph models patoh hmetis produce better decompositions graph models metis hypergraph models patoh slower graph models version version metis average 
graph models deficiencies graph partitioning problem undirected graph defined set vertices set edges edge ij connects pair distinct vertices degree vertex equal number edges incident weights costs assigned vertices edges graph respectively 
ij denote weight vertex cost edge ij respectively 
fp pk way partition conditions hold part nonempty subset parts pairwise disjoint union parts equal 
way partition called multiway partition bipartition 
partition said balanced part satisfies balance criterion avg weight part defined sum weights vertices part avg denotes weight part perfect load balance condition represents predetermined maximum imbalance ratio allowed 
partition edge said cut pair vertices belong different parts 
cut edges referred external internal edges respectively 
set external edges partition denoted ee cutsize definition representing cost partition ij ee ij cut edge ij contributes cost ij cutsize 
graph partitioning problem defined task dividing graph parts cutsize minimized balance criterion part weights maintained 
graph partitioning problem known np hard bipartitioning unweighted graphs 
standard graph model structurally symmetric matrices structurally symmetric sparse matrix represented undirected graph ga sparsity pattern corresponds adjacency matrix representation graph ga vertices ga correspond rows columns matrix exist edge ij diagonal entries ij ji matrix nonzeros 
rowwise decomposition vertex corresponds atomic task computing inner product row column vector columnwise decomposition vertex corresponds atomic task computing sparse saxpy daxpy operation denotes column matrix nonzero entry row column incurs multiply add operation local computations pre post communication schemes respectively 
computational load row column number nonzero entries row column graph theoretical notation ii ii 
note number nonzeros row column equal symmetric matrix 
graph model displays bidirectional computational interdependency view 
edge ij considered incurring computations ij ji edge represents bidirectional interaction respective pair vertices inner outer product computation schemes 
rows columns assigned processor rowwise columnwise decomposition edge ij incur communication 
pre communication scheme rows assigned different processors cut edge ij necessitates communication floating point words need exchange updated values atomic tasks just local computations 
post communication scheme columns assigned different processors cut edge ij necessitates communication floating point words need exchange partial values atomic tasks just local computations 
setting ij edge ij rowwise columnwise decompositions matrix reduce way partitioning associated graph ga cutsize definition 
minimizing cutsize effort minimizing total volume interprocessor communication 
maintaining balance criterion corresponds maintaining computational load balance local computations 
vertex effectively represents row column ga atomic task definition differs rowwise columnwise decompositions 
partition ga automatically achieves symmetric partitioning inducing partition vector vector components vertex corresponds assigning row column part rowwise columnwise decomposition 
matrix theoretical view symmetric partitioning induced partition ga considered inducing partial symmetric permutation rows columns partial permutation corresponds ordering rows columns assigned part rows columns assigned part gamma rows columns part ordered arbitrarily 
denote permuted version partial symmetric permutation induced internal edge ij part corresponds locating ij ji diagonal block kk external edge ij cost parts corresponds locating nonzero entry ij diagonal block ji diagonal block vice versa 
minimizing cutsize graph model considered permuting rows columns matrix minimize total number nonzeros diagonal blocks 
illustrates sample theta symmetric sparse matrix associated graph ga numbers inside circles indicate computational weights respective vertices rows columns 
illustrates rowwise decomposition symmetric matrix corresponding bipartitioning ga processor system 
seen fig 
cutsize graph bipartitioning equal total number nonzero entries diagonal blocks 
bipartition illustrated fig 
achieves perfect way rowwise decomposition sample structurally symmetric matrix corresponding bipartitioning associated graph ga load balance assigning nonzero entries row stripe 
number obtained adding weights vertices part 
generalized graph model structurally symmetric nonsymmetric square matrices standard graph model suitable partitioning nonsymmetric matrices 
proposed bipartite graph model enables partitioning rectangular structurally symmetric nonsymmetric square matrices 
model row column represented vertex sets vertices representing rows columns form bipartition vr exists edge row vertex vr column vertex respective entry ij matrix nonzero 
partitions pr vr respectively determine partition fp pk vr rowwise columnwise decomposition vertices vr weighted number nonzeros respective row column balance criterion imposed partitioning vr 
standard graph model minimizing number cut edges corresponds minimizing total number nonzeros diagonal blocks 
approach flexibility achieving nonsymmetric partitioning 
context parallel need symmetric partitioning square matrices achieved enforcing pr hendrickson propose bipartite graph partitioning algorithms adopted techniques standard graph model partitioning algorithm specific bipartite graphs 
propose simple effective graph model symmetric partitioning structurally nonsymmetric square matrices 
proposed model enables standard graph partitioning tools modification 
proposed model nonsymmetric square matrix represented undirected graph gr vr rowwise columnwise decomposition schemes respectively 
graphs gr differ vertex weight definitions 
vertex set corresponding atomic task definitions identical symmetric matrices 
weight vertex vr equal total number nonzeros row column gr 
edge set ij diagonal entries ij ji 
vertices adjacency list vertex denote union column indices diagonal nonzeros row row indices diagonal nonzeros column cost ij edge ij set ij ji set ij ji 
way rowwise decomposition sample structurally nonsymmetric matrix corresponding bipartitioning associated graph gr proposed scheme referred generalized model automatically produces standard graph representation structurally symmetric matrices computing cost edge 
illustrates sample theta nonsymmetric sparse matrix associated graph gr rowwise decomposition 
numbers inside circles indicate computational weights respective vertices rows 
illustrates rowwise decomposition matrix corresponding bipartitioning associated graph processor system 
seen fig 
cutsize graph bipartitioning equal total number nonzero entries diagonal blocks 
similar standard bipartite graph models minimizing cutsize proposed graph model corresponds minimizing total number nonzeros diagonal blocks 
seen fig 
bipartitioning achieves perfect load balance assigning nonzero entries row stripe 
mentioned earlier model matrix columnwise decomposition differs gr model vertex weights 
graph bipartitioning illustrated fig 
considered incurring slightly imbalanced versus nonzeros columnwise decomposition sample matrix shown vertical dash line identical communication requirement 
deficiencies graph models consider symmetric matrix decomposition fig 

assume parts mapped processors respectively 
cutsize bipartition shown equal theta estimating communication volume requirement words 
pre communication scheme entries assigned processor display need nonlocal vector component twice 
clear processor send processor similarly processor send processor block diagonal entries assigned processor post communication scheme graph model treats block diagonal nonzeros processor send multiplication results processor obvious processor compute partial result nonlocal vector component local phase send single value processor post communication phase 
similarly processor compute send single value processor actual communication volume fact words pre post communication schemes 
similar analysis rowwise decomposition nonsymmetric matrix fig 
reveals fact actual communication requirement words determined cutsize bipartition gr matrix theoretical view nonzero entries column diagonal block incur communication single value rowwise decomposition pre communication scheme 
similarly nonzero entries row diagonal block incur communication single value columnwise decomposition post communication scheme 
mentioned earlier graph models try minimize total number block diagonal nonzeros considering relative spatial locations nonzeros 
words graph models treat block diagonal nonzeros identical manner assuming block diagonal nonzero incur distinct communication single word 
graph theoretical view graph models treat cut edges equal cost identical manner computing cutsize 
cut edges cost stemming vertex part vertices part incur communications pre post communication schemes 
pre communication scheme processor sends processor sends post communication scheme processor sends processor sends similarly amount communication required cut edges cost stemming vertex part vertices part may vary words exactly words determined cutsize graph partitioning 
hypergraph models decomposition hypergraph partitioning problem hypergraph defined set vertices set nets hyperedges vertices 
net subset vertices vertices net called pins denoted pins 
size net equal number pins set nets connected vertex denoted nets 
degree vertex equal number nets connected graph special instance hypergraph net exactly pins 
similar graphs denote weight vertex cost net respectively 
definition way partition hypergraphs identical graphs 
partition net pin vertex part said connect part 
connectivity set net defined set parts connected connectivity jl net denotes number parts connected net said cut connects part 
cut nets referred external internal nets respectively 
set external nets partition denoted ne various cutsize definitions representing cost partition relevant definitions ne ne gamma cutsize equal sum costs cut nets 
cut net contributes gamma cutsize 
hypergraph partitioning problem defined task dividing hypergraph parts cutsize minimized balance criterion part weights maintained 
part weight definition identical graph model 
hypergraph partitioning problem known np hard 
hypergraph models decomposition propose computational hypergraph models decomposition sparse matrices 
models referred column net row net rowwise decomposition pre communication columnwise decomposition post communication schemes respectively 
column net model matrix represented hypergraph hr vr rowwise decomposition 
vertex net sets vr correspond rows columns matrix respectively 
exist vertex net row column respectively 
net vr contains vertices corresponding rows nonzero entry column ij 
vertex vr corresponds atomic task computing inner product row column vector computational weight vertex vr equal total number nonzeros row nets hr represent dependency relations atomic tasks vector components rowwise decomposition 
net considered incurring computation ij vertex row net denotes set atomic tasks vertices need note pin net corresponds unique nonzero ij enabling representation decomposition structurally nonsymmetric matrices symmetric matrices extra effort 
illustrates dependency relation view column net model 
seen net fv represents dependency atomic tasks computations hj ij kj illustrates column net representation sample theta nonsymmetric matrix fig 

fig 
pins net fv represent nonzeros net represents dependency atomic tasks computations row net model considered dual column net model 
model matrix represented hypergraph nr columnwise decomposition 
vertex net sets nr correspond columns rows matrix respectively 
exist vertex net column row respectively 
net contains vertices corresponding columns nonzero entry row ji 
vertex corresponds atomic task computing sparse saxpy daxpy operation computational weight vertex equal total number nonzeros column nets represent dependency relations computations vector components atomic tasks represented vertices columnwise decomposition 
net considered incurring computation ji vertex column net denotes set atomic task results needed accumulate note pin net corresponds unique nonzero ji enabling representation decomposition column net model hj ij kj row net model ji jh jk dependency relation views column net row net models 
structurally nonsymmetric matrices symmetric matrices extra effort 
illustrates dependency relation view row net model 
seen net fv represents dependency accumulating partial jh ji andy jk note row net column net models identical structurally symmetric matrices 
assigning unit costs nets net proposed column net row net models reduce decomposition problem way hypergraph partitioning problem cutsize definition pre post communication schemes respectively 
consistency proposed hypergraph models accurate representation communication volume requirement maintaining symmetric partitioning restriction depends condition net 
assume condition holds discussion paragraphs discuss appropriateness assumption paragraph section 
validity proposed hypergraph models discussed column net model 
dual discussion holds row net model 
consider partition hr column net model rowwise decomposition matrix loss generality assume part assigned processor defined partition vertex set hr induces complete part processor assignment rows matrix components vector 
vertex assigned part corresponds assigning row part partition induce part assignment nets hr consider partition inducing assignment internal nets hr respective vector components 
consider internal net part fp corresponds column pins net lie rows including row consistency condition need inner product computations assigned processor internal net contribute cutsize partition necessitate communication assigned processor assignment processor considered permuting column part respecting symmetric partitioning row assigned way decomposition fig 
internal nets part induce assignment columns part note part contains rows respecting symmetric partitioning consider external net connectivity set jl 
pins net lie parts connectivity set rows including row consistency condition need theta structurally nonsymmetric matrix 
column net matrix way partitioning hr 
way rowwise decomposition matrix obtained permuting symmetric partitioning induced inner product computations assigned parts processors contribution gamma external net cutsize accurately models amount communication volume incur parallel computations assigned processor map denote part processor assignment corresponding cut net column net model pre communication scheme cut net indicates processor map send local processors connectivity set net processors set 
processor map send local jl gamma gamma distinct processors 
consistency condition ensures row assigned part symmetric partitioning easily maintained assigning permuting column part contains row way decomposition shown fig 
external net fp incurs assignment permuting column part row assigned part contribution gamma net cutsize accurately models communication volume incur due processor send processors gamma fmap gamma fp fp essence column net model partition hr safely decoded assigning row processor rowwise decomposition 
similarly row net model partition safely decoded assigning column processor 
column net row net models minimizing cutsize corresponds minimizing actual volume interprocessor communication pre post communication phases respectively 
maintaining balance criterion corresponds maintaining computational load balance local computations 
displays permutation sample matrix fig 
symmetric partitioning induced way decomposition shown fig 

seen fig 
actual communication volume rowwise decomposition words processor send send send send seen fig 
external nets contribute cutsize respectively 
cutsize way decomposition fig 
leading accurate modeling communication requirement 
note graph model estimate total communication volume words way decomposition fig 
total number nonzeros diagonal blocks 
seen fig 
processor assigned nonzeros achieving perfect computational load balance 
matrix theoretical view denote permuted version matrix symmetric partitioning induced partition hr column net model 
cut net connectivity set map corresponds column containing nonzeros distinct blocks matrix connectivity set net guaranteed contain part map column contains nonzeros gamma distinct diagonal blocks note multiple nonzeros column particular diagonal block contributes connectivity net definition cutsize partition hr equal number nonzero column segments diagonal blocks matrix example external net fp map fig 
indicates column nonzeros diagonal blocks seen fig 

seen fig 
number nonzero column segments diagonal blocks matrix equal cutsize partition shown fig 

column net model tries achieve symmetric permutation minimizes total number nonzero column segments diagonal blocks pre communication scheme 
similarly row net model tries achieve symmetric permutation minimizes total number nonzero row segments diagonal blocks post communication scheme 
nonzero diagonal entries automatically satisfy condition net enabling accurate representation communication requirement symmetric partitioning nonzero diagonal entry jj implies net contains vertex pin 
diagonal entries matrix zeros consistency proposed column net model easily maintained simply adding rows contain diagonal entries pin lists respective column nets 
jj vertex row added pin list pins net net added net list nets vertex pin additions affect computational weight assignments vertices 
weight vertex hr equal gamma depending jj jj respectively 
consistency row net model preserved dual manner 
decomposition heuristics kernighan lin kl heuristics widely graph hypergraph partitioning short run times quality results 
kl algorithm iterative improvement heuristic originally proposed graph bipartitioning 
kl algorithm starting initial bipartition performs number passes finds locally minimum partition 
pass consists sequence vertex swaps 
swap strategy applied hypergraph bipartitioning problem kernighan 
fiduccia mattheyses fm introduced faster implementation kl algorithm hypergraph partitioning 
proposed vertex move concept vertex swap 
modification proper data structures bucket lists reduced time complexity single pass kl algorithm linear size graph hypergraph 
size refers number edges pins graph hypergraph respectively 
performance fm algorithm deteriorates large sparse graphs hypergraphs 
sparsity graphs hypergraphs refer average vertex degrees 
furthermore solution quality fm stable predictable average fm solution significantly worse best fm solution common weakness move iterative improvement approaches 
random multi start approach vlsi layout design alleviate problem running fm algorithm times starting random initial partitions return best solution 
approach viable parallel computing decomposition preprocessing overhead introduced increase efficiency underlying parallel algorithm program 
users rely run decomposition heuristic quality decomposition tool depends equally worst average decompositions just best decomposition 
considerations motivated phase application move algorithms hypergraph partitioning 
approach clustering performed original hypergraph induce coarser hypergraph clustering corresponds coalescing highly interacting vertices supernodes preprocessing fm 
fm run find bipartition bipartition projected back bipartition fm re run initial solution 
phase approach extended multilevel approaches leading successful graph partitioning tools chaco metis 
multilevel heuristics consist phases coarsening initial partitioning uncoarsening 
phase multilevel clustering applied starting original graph adopting various matching heuristics number vertices coarsened graph reduces predetermined threshold value 
second phase coarsest graph partitioned various heuristics including fm 
third phase partition second phase successively projected back original graph refining projected partitions intermediate level graphs various heuristics including fm 
exploit multilevel partitioning schemes experimental verification proposed hypergraph models approaches 
approach multilevel graph partitioning tool metis black box transforming hypergraphs graphs randomized clique net model proposed 
second approach implemented multilevel hypergraph partitioning tool patoh tested patoh multilevel hypergraph partitioning tool hmetis released 
randomized clique net model graph representation hypergraphs clique net transformation model vertex set target graph equal vertex set hypergraph vertex weights 
net hypergraph represented clique vertices corresponding pins 
net induces edge pair pins 
multiple edges connecting pair vertices graph contracted single edge cost equal sum costs edges represents 
standard clique net model uniform cost gamma assigned clique edge net size various edge weighting functions proposed literature 
edge cut set graph partitioning nets represented edge cut set hypergraph partitioning vice versa 
ideally matter vertices net partitioned contribution cut net cutsize bipartition 
deficiency clique net model impossible achieve perfect clique net model 
furthermore transformation may result large graphs number clique edges induced nets increase quadratically sizes 
randomized clique net model implementation proposed yields promising results graph partitioning tool metis 
model nets size larger removed transformation 
furthermore net size theta random pairs pins vertices selected edge cost added graph selected pair vertices 
multiple edges pair vertices resulting graph contracted single edge mentioned earlier 
scheme nets size smaller small nets induce larger number edges standard clique net model nets size larger large nets induce smaller number edges standard clique net model 
considering fact metis accepts integer edge costs input graph scheme nice features simulates uniform edge weighting scheme standard clique net model small nets random manner clique edge induced net size assigned integer cost close gamma average 
second prevents quadratic increase number clique edges induced large nets standard model number clique edges induced net scheme linear size net 
implementation parameters accordance recommendations 
patoh multilevel hypergraph partitioning tool exploit successful multilevel methodology proposed implemented graph partitioning develop new multilevel hypergraph partitioning tool called patoh patoh partitioning tools hypergraphs 
data structures store hypergraphs patoh mainly consist arrays 
array stores net lists vertices 
array stores pin lists nets 
size arrays equal total number pins hypergraph 
auxiliary index arrays nets sizes jv jn hold starting indices net lists pin lists vertices nets arrays respectively 
sparse matrix storage terminology scheme corresponds storing matrix compressed sparse row csr compressed sparse column csc formats storing numerical data 
column net model proposed rowwise decomposition arrays correspond csr storage scheme nets arrays correspond csc storage scheme 
private communication alpert 
split cut net splitting recursive bisection 
correspondence dual row net model proposed columnwise decomposition 
way graph hypergraph partitioning problem usually solved recursive bisection 
scheme way partition obtained bipartition partitioned recursive manner 
lg phases graph partitioned parts 
patoh achieves way hypergraph partitioning recursive bisection value restricted power 
connectivity cutsize metric needs special attention way hypergraph partitioning recursive bisection 
note cutsize metrics equivalent hypergraph bisection 
consider bipartition va obtained bisection step 
clear va internal nets parts vertex net sets ha respectively recursive bisection steps 
note cut net bipartition contributes total cutsize final way partition obtained recursive bisections 
recursive bisections va may increase connectivity cut nets 
parallel view cut net incurs communication single word nets may induce additional communication recursive bisection steps 
hypergraph bisection step cut net split pin wise disjoint nets pins va pins nets added net lists ha jn jn respectively 
note single pin nets discarded split operation nets contribute cutsize recursive bisection steps 
total cutsize equal sum number cut nets bisection step cut net split method 
illustrates cut nets bipartition splits nets respectively 
note net single pin net discarded 
similar multilevel graph hypergraph partitioning tools chaco metis hmetis multilevel hypergraph bisection algorithm patoh consists phases coarsening initial partitioning uncoarsening 
sections briefly summarize multilevel bisection algorithm 
patoh works weighted nets assume unit cost nets sake simplicity presentation fact nets assigned unit cost hypergraph representation sparse matrices 
coarsening phase phase hypergraph coarsened sequence smaller hypergraphs hm nm satisfying jv jv jv jvm coarsening achieved coalescing disjoint subsets vertices hypergraph forms single vertex weight vertex equal sum constituent vertices respective net set vertex equal union net sets constituent vertices respective multiple pins net cluster contracted single pin respective net furthermore single pin nets obtained contraction discarded 
note single pin nets correspond internal nets clustering performed coarsening phase terminates number vertices coarsened hypergraph reduces jv 
clustering approaches classified agglomerative hierarchical agglomerative clustering new clusters formed time hierarchical clustering new clusters may formed simultaneously 
patoh implemented randomized matching hierarchical clustering randomized hierarchic agglomerative clustering 
approaches abbreviated matching clustering agglomerative clustering respectively 
matching clustering works follows 
vertices visited random order 
vertex matched unmatched adjacent vertices selected criterion 
vertex exists merge matched pair cluster 
unmatched adjacent vertex vertex remains unmatched remains singleton cluster 
vertices said adjacent share net nets nets 
selection criterion patoh matching chooses vertex highest connectivity value uv connectivity uv nets refers number shared nets matching scheme referred heavy connectivity matching hcm 
matching clustering allows clustering pairs vertices level 
order enable clustering vertices level implemented randomized agglomerative clustering approach 
scheme vertex assumed constitute singleton cluster fug coarsening level 
vertices visited random order 
vertex clustered jc considered source new clustering 
unclustered vertex choose join cluster singleton cluster 
adjacent vertices unclustered vertex considered selection criterion 
selection vertex adjacent corresponds including vertex cluster grow new cluster fug 
note singleton cluster remains process far exists isolated vertex 
selection criterion patoh agglomerative clustering chooses singleton cluster highest cv cv value cv cv nets cv weight cluster candidate hcm hcc matching clustering hcm agglomerative clustering hcc rows matrix fug division cv cv effort avoiding polarization large clusters 
agglomerative clustering scheme referred heavy connectivity clustering hcc 
objective hcm hcc find highly connected vertex clusters 
connectivity values uv cv selection serve objective 
note uv cv denotes lower bound amount decrease number pins pin contractions performed joins 
recall additional decrease number pins single pin nets may occur clustering 
connectivity metric effort minimizing complexity coarsening levels partitioning phase refinement phase size hypergraph equal number pins 
rowwise matrix decomposition context column net model connectivity metric corresponds number common column indices rows row groups 
hcm hcc try combine rows row groups similar sparsity patterns 
turn corresponds combining rows row groups need similar sets vector components pre communication scheme 
dual discussion holds row net model 
illustrates single level coarsening theta sample matrix column net model hcm hcc 
original decimal ordering rows assumed random vertex visit order 
seen fig 
hcm matches row pairs connectivity values respectively 
note total number nonzeros reduces hcm clustering 
difference equal sum connectivity values matched row vertex pairs pin contractions lead single pin nets 
seen fig 
hcc constructs clusters clustering sequence connectivity values respectively 
note pin contractions lead single pin nets columns removed 
seen fig 
rows remain unmatched hcm row involved clustering hcc 
hcm hcc necessitate scanning pin lists nets net list source vertex find adjacent vertices matching clustering 
column net row net model total cost scan operations expensive total number multiply add operations lead nonzero entries computation aa 
hcm key point efficient implementation move matched vertices encountered scan pin list net pin list simple swap operation 
scheme avoids re visits matched vertices matching operations level 
scheme requires additional index array maintain temporary tail indices pin lists achieves substantial decrease run time coarsening phase 
unfortunately simple effective scheme fully hcc 
singleton vertex select cluster re visits clustered vertices partially avoided maintaining single vertex represent cluster pin list net connected cluster simple swap operations 
efficient implementation schemes total cost scan operations column net row net model low total number nonzeros aa 
order maintain cost reasonable limits nets size greater avg considered bipartitioning step avg denotes average net size hypergraph partitioned step 
note nets reconsidered levels recursion net splitting 
cluster growing operation hcc requires disjoint set operations maintaining representatives clusters union operations restricted union singleton source cluster singleton target cluster 
restriction exploited choosing representative target cluster representative new cluster 
sufficient update representative pointer singleton source cluster joining target cluster 
disjoint set operation required scheme performed time 
initial partitioning phase goal phase find bipartition coarsest hypergraph hm patoh greedy hypergraph growing algorithm bisecting hm algorithm considered extension gggp algorithm metis hypergraphs 
grow cluster randomly selected vertex 
coarse algorithm selected unselected vertices induce bipartition hm unselected vertices connected growing cluster inserted priority queue fm gains 
gain unselected vertex corresponds decrease cutsize current bipartition vertex moves growing cluster 
vertex highest gain selected priority queue 
vertex moves growing cluster gains unselected adjacent vertices currently priority queue updated priority queue inserted 
cluster growing operation continues predetermined bipartition balance criterion reached 
mentioned metis quality algorithm sensitive choice initial random vertex 
coarsest hypergraph hm small run times starting different random vertices select best bipartition refinement uncoarsening phase 
uncoarsening phase level gamma bipartition projected back bipartition gamma gamma constituent vertices gamma assigned part respective vertex obviously gamma gamma cutsize refine bipartition running boundary fm hypergraph bipartitioning algorithm gamma starting initial bipartition gamma moves boundary vertices overloaded part loaded part vertex said boundary vertex connected cut net 
requires maintaining pin connectivity net initial gain computations gain updates 
pin connectivity oe jn net part denotes number pins net lie part 
order avoid scan pin lists nets adopt efficient scheme initialize oe values pass level 
clear initial bipartition gamma gamma cut net set scan pin lists cut nets gamma initialize oe values 
net oe oe values easily initialized oe oe net internal part oe oe 
initializing gain value vertex gammad exploit oe values follows 
re scan pin list external net update gain value vertex pins depending net critical part containing respectively 
external net said critical part oe moving single vertex net lies part part removes net cut 
note pin cut nets critical parts 
vertices visited scanning pin lists external nets identified boundary vertices vertices inserted priority queue computed gains 
pass algorithm sequence vertices highest gains selected move part 
original fm algorithm vertex move necessitates gain updates adjacent vertices 
algorithm adjacent vertices moved vertex may priority queue may boundary vertices move 
vertices boundary vertices move inserted priority queue updated gain values 
refinement process pass terminates feasible move remains sequence maxf jv jg moves yield decrease total cutsize 
move said feasible disturb load balance criterion 
pass sequence tentative vertex moves respective gains 
construct sequence maximum prefix subsequence moves maximum prefix sum incurs maximum decrease cutsize 
permanent realization moves maximum prefix subsequence efficiently achieved rolling back remaining moves sequence 
initial gain computations pass level achieved rollback 
refinement process level terminates maximum prefix sum pass positive 
current implementation patoh passes allowed level uncoarsening phase 
experimental results tested validity proposed hypergraph models running metis graphs obtained randomized clique net transformation running patoh hmetis directly hypergraphs decompositions various realistic sparse test matrices arising different application domains 
decomposition results compared decompositions obtained running metis standard proposed graph models symmetric nonsymmetric test matrices respectively 
version version metis experiments 
hmetis patoh achieve way partitioning recursive bisection recursive metis pmetis sake fair comparison 
reason pmetis direct way partitioning version metis produces worse partitions pmetis decomposition nonsymmetric test matrices times faster average 
pmetis run default parameters sorted heavy edge matching region growing early exit boundary fm refinement coarsening initial partitioning uncoarsening phases respectively 
current version version hmetis run parameters greedy choice scheme early exit fm refinement ee fm coarsening uncoarsening phases respectively 
cycle refinement scheme experimentations achieved average better decompositions expense approximately times slower execution time average decomposition test matrices 
scheme faster clustering schemes producing slightly better decompositions average 
ee fm scheme observed faster refinement schemes difference decomposition quality average 
table illustrates properties test matrices listed order increasing number nonzeros 
table description column displays nature source test matrix 
sparsity patterns linear programming matrices symmetric test matrices obtained multiplying respective rectangular constraint matrices transposes 
table total number nonzeros matrix denotes total number pins column net row net models 
minimum maximum number nonzeros row column matrix correspond minimum maximum vertex degree net size column net model respectively 
similarly standard deviation std coefficient variation cov values nonzeros row column matrix correspond std cov values vertex degree net size column net model respectively 
dual correspondences hold row net model 
experiments carried workstation equipped processor kbyte external cache mbytes memory 
tested way decompositions test matrix 
specific value way decomposition test matrix constitutes decomposition instance 
pmetis hmetis patoh run times starting different random seeds decomposition instance 
average performance results displayed tables ii iv figs 
decomposition instance 
percent load imbalance values decomposition results displayed figures percent imbalance ratio defined theta max gamma avg avg table ii displays decomposition performance proposed hypergraph models standard graph model rowwise columnwise decomposition symmetric test matrices 
note rowwise columnwise decomposition problems equivalent symmetric matrices 
tables iii iv display decomposition performance proposed column net row net hypergraph models proposed graph models rowwise columnwise decompositions nonsymmetric test matrices respectively 
due lack space decomposition performance results clique net approach displayed tables ii iv summarized table main objective minimization total communication volume results performance metrics maximum volume average number maximum number messages handled single processor displayed tables ii iv 
note maximum volume maximum number messages determine concurrent communication volume concurrent number messages respectively assumption congestion occurs network 
seen tables ii iv proposed hypergraph models produce substantially better partitions graph model decomposition instance terms total cost 
symmetric test matrices hypergraph model produces better partitions graph model see table ii 
nonsymmetric test matrices hypergraph models produce better partitions graph models rowwise see table iii columnwise see table iv decompositions respectively 
seen tables ii iv clear winner hmetis patoh terms decomposition quality 
matrices hmetis produces slightly better partitions patoh situation way round matrices 
seen tables ii iii clear winner clustering schemes hcm hcc patoh 
seen table iv patoh hcc produces slightly better partitions patoh hcm columnwise decomposition instances nonsymmetric test matrices 
tables ii iv show performance gap graph hypergraph models terms total communication volume costs preserved amounts terms concurrent communication volume costs 
example decomposition symmetric test matrices hypergraph model patoh hcm incurs total communication volume graph model incurring concurrent communication volume average 
columnwise decomposition nonsymmetric test matrices patoh hcm incurs total communication volume graph model incurring concurrent communication volume average 
hypergraph models perform better graph models terms number messages performance gap large communication volume metrics 
performance gap increases increasing seen table ii way decomposition symmetric test matrices hypergraph model patoh hcc incurs total concurrent number messages graph model respectively 
seen table iii rowwise decomposition nonsymmetric test matrices patoh hcc incurs total concurrent number messages graph model respectively 
performance comparison graph hypergraph partitioning decomposition schemes conventional algorithms decomposition schemes follows 
mentioned earlier way decompositions thetam matrices conventional schemes incur total communication volume gamma gamma words respectively 
example way decompositions conventional schemes incur total communication volumes words respectively 
seen bottom tables ii iii patoh hcc reduces total communication volume words way decomposition symmetric nonsymmetric test matrices respectively average 
way decompositions conventional schemes incur concurrent communication volumes approximately words respectively 
seen tables ii iii patoh hcc reduces concurrent communication volume words way decomposition symmetric nonsymmetric test matrices respectively average 
illustrates relative run time performance proposed hypergraph model compared standard graph model rowwise columnwise decomposition symmetric test matrices 
figures display relative run time performance column net row net hypergraph models compared proposed graph models rowwise columnwise decompositions nonsymmetric test matrices respectively 
figs 
decomposition instance plot ratios average execution times tools respective hypergraph model pmetis respective graph model 
results displayed figs 
obtained assuming test matrix csr csc form commonly computations 
standard graph model necessitate preprocessing csr csc forms equivalent symmetric matrices correspond adjacency list representation standard graph model 
nonsymmetric matrices construction proposed graph model requires amount preprocessing time implemented efficient construction code totally avoids index search 
execution time averages graph models nonsymmetric test matrices include preprocessing time 
preprocessing time constitutes approximately total execution time average 
clique net model transforming hypergraph representation matrices graphs randomized clique net model introduces considerable amount preprocessing time despite efficient implementation scheme adopted 
execution time averages clique net model include transformation time 
transformation time constitutes approximately total execution time average 
mentioned earlier patoh hmetis tools csr csc forms construction form performed respective tool 
seen figs 
tools hypergraph models run slower pmetis graph models instances 
comparison fig 
figs 
shows gap run time performances graph hypergraph models decomposition nonsymmetric test matrices symmetric test matrices 
experimental findings expected execution times graph partitioning tool pmetis hypergraph partitioning tools hmetis patoh proportional sizes graph hypergraph respectively 
representation thetam square matrix diagonal nonzeros graph models contain je je edges symmetric nonsymmetric matrices respectively 
hypergraph models contain pins symmetric nonsymmetric matrices 
size hypergraph representation matrix greater size graph representation gap sizes decreases favor hypergraph models nonsymmetric matrices 
displays interesting behavior pmetis clique net model runs faster pmetis graph model columnwise decomposition nonsymmetric test matrices 
test matrices edge contractions hypergraph graph transformation randomized clique net approach lead number edges graph model 
seen figs 
patoh hcm patoh hcc run considerably faster hmetis decomposition instance 
situation probably due design considerations hmetis 
hmetis mainly aims partitioning vlsi circuits hypergraph representations sparse hypergraph representations test matrices 
comparison hcm hcc clustering schemes patoh patoh hcm runs slightly faster patoh hcc decomposition test matrices decomposition symmetric matrices ken ken nonsymmetric matrices 
seen fig 
patoh hcm hypergraph model runs times slower pmetis graph model decomposition symmetric test matrices 
seen figs 
patoh hcm runs times times slower pmetis graph model rowwise columnwise decomposition nonsymmetric test matrices respectively 
note patoh hcm runs faster pmetis graph model way way way columnwise decompositions nonsymmetric matrix lhr respectively 
patoh hcm achieves way rowwise decomposition largest test matrix bcsstk containing rows columns nonzeros seconds equal sequential execution time multiplying matrix bcsstk dense vector times 
relative performance results hypergraph models respect graph models summarized table terms total communication volume execution time averaging different values 
table displays averages best worst performance results tools hypergraph models 
table performance results hypergraph models normalized respect pmetis graph models 
symmetric test matrices direct approaches patoh hmetis produce better partitions pmetis graph model clique net approach produces better partitions average 
nonsymmetric test matrices direct approaches achieve better decomposition quality pmetis graph model clique net approach achieves better decomposition quality 
seen table clique net approach faster direct approaches decomposition symmetric test matrices 
patoh hcm achieves nearly equal run time performance pmetis clique net approach decomposition nonsymmetric test matrices 
interesting note execution time clique net approach relative graph model decreases increasing number processors fact percent preprocessing overhead due hypergraph graph transformation total execution time pmetis clique net approach decreases increasing seen table hmetis produces slightly better partitions expense considerably larger execution time decomposition symmetric test matrices 
patoh hcm achieves decomposition quality hmetis nonsymmetric test matrices patoh hcc achieves slightly better decomposition quality 
decomposition nonsymmetric test matrices performs slightly better patoh hcm terms decomposition quality slower 
symmetric test matrices proposed hypergraph model graph model achieves decrease communication volume requirement single parallel computation expense increase decomposition time patoh hcm hypergraph partitioning 
nonsymmetric test matrices proposed hypergraph models graph model achieves decrease communication volume requirement single parallel computation expense increase decomposition time patoh hcm 
computational hypergraph models proposed decompose sparse matrices minimizing communication volume maintaining load balance repeated parallel matrix vector product computations 
proposed models enable representation decomposition structurally nonsymmetric matrices structurally symmetric matrices 
furthermore introduce accurate representation communication requirement standard computational graph model widely literature parallelization various scientific applications 
proposed models reduce decomposition problem known hypergraph partitioning problem enabling circuit partitioning heuristics widely vlsi design 
successful multilevel graph partitioning tool metis experimental evaluation validity proposed hypergraph models hypergraph graph transformation randomized clique net model 
successful multilevel hypergraph partitioning tool patoh implemented patoh released multilevel hypergraph partitioning tool hmetis testing validity proposed hypergraph models 
experimental results carried wide range sparse test matrices arising different application domains confirmed validity proposed hypergraph models 
decomposition test matrices proposed hypergraph models graph models achieved decrease communication volume requirement single parallel matrix vector multiplication expense increase decomposition time patoh average 
effort showing computational hypergraph model powerful standard computational graph model provides versatile representation interactions atomic tasks computational domains 
alpert kahng directions netlist partitioning survey vlsi journal vol 
pp 

alpert hagen kahng hybrid multilevel genetic approach circuit partitioning tech 
rep ucla computer science department 
sadayappan iterative algorithms solution large sparse systems linear equations hypercubes ieee transactions computers vol 
pp 
dec 
bui jones heuristic reducing fill sparse matrix factorization proc 
th siam conf 
parallel processing scientific computing pp 

new mapping heuristic mean field annealing parallel distributed computing vol 
pp 

camp hendrickson leland massively parallel methods engineering science problems communication acm vol 
pp 
april 
hill empirical evaluation algorithms military airlift applications operations research vol 
pp 

decomposing irregularly sparse matrices parallel matrix vector multiplications proc 
rd int 
workshop parallel algorithms irregularly structured problems irregular pp 

duff grimes lewis sparse matrix test problems acm transactions mathematical software vol 
pp 
march 
fiduccia mattheyses linear time heuristic improving network partitions proceedings th acm ieee design automation conference pp 

garey johnson stockmeyer simplified np complete graph problems theoretical computer science vol 
pp 

goldberg burstein heuristic improvement techniques bisection vlsi networks proc 
ieee intl 
conf 
computer design pp 

hendrickson leland multilevel algorithm partitioning graphs tech 
rep sandia national laboratories 
hendrickson leland chaco user guide version tech 
rep sand sandia national laboratories nm 
hendrickson leland efficient parallel algorithm matrix vector multiplication int 
high speed computing vol 
pp 

hendrickson graph partitioning parallel solvers emperor clothes lecture notes computer science vol 
pp 

hendrickson partitioning rectangular structurally nonsymmetric sparse matrices parallel processing submitted siam journal scientific computing 
wagner wagner modeling hypergraphs graphs mincut properties information processing letters vol 
pp 
march 
iowa optimization center linear programming problems ftp col biz uiowa edu pub lp 
qu ranka partitioning unstructured computational graphs nonuniform adaptive environments ieee parallel distributed technology pp 

karypis kumar fast high quality multilevel scheme partitioning irregular graphs siam journal scientific computing appear 
karypis kumar metis software package partitioning unstructured graphs partitioning meshes computing fill reducing orderings sparse matrices version 
university minnesota department comp 
sci 
eng army hpc research center minneapolis 
karypis kumar aggarwal shekhar hypergraph partitioning multilevel approach applications vlsi domain ieee transactions vlsi systems appear 
karypis kumar aggarwal shekhar hmetis hypergraph partitioning package version 
university minnesota department comp 
sci 
eng army hpc research center minneapolis 
kernighan lin efficient heuristic procedure partitioning graphs bell system technical journal vol 
pp 
feb 
partitioning sparse rectangular matrices parallel processing lecture notes computer science vol 
pp 

kumar gupta karypis parallel computing design analysis algorithms 
redwood city ca benjamin cummings publishing 
mapping molecular dynamics computations hypercubes parallel computing vol 
pp 

lengauer combinatorial algorithms integrated circuit layout 
chichester wiley 
lewis van de distributed memory matrix vector multiplication conjugate gradient algorithms proc 
supercomputing pp 

martin otto partitioning unstructured meshes load balancing concurrency practice experience vol 
pp 

frieder el load balanced sparse matrix vector multiplication parallel computers parallel distributed computing vol 
pp 

sparse matrix computations parallel processor arrays siam scientific comput 
decomposing linear programs parallel solution lecture notes computer science vol 
pp 

set new mapping coloring heuristics parallel processors siam scientific statistical computing vol 
pp 
jan 

qu ranka parallel incremental graph partitioning ieee trans 
parallel distributed systems vol 
pp 

saad wu sparse matrix computations cm proc 
th siam conf 
parallel processing computing 
kernighan proper model partitioning electrical circuits proceedings th acm ieee design automation conference pp 

davis university florida sparse matrix collection www cise ufl edu davis sparse na digest vol 

table properties test matrices 
number number nonzeros matrix name description total avg 
column row rows cols row col min max std cov min max std cov structurally symmetric matrices sherman finite difference grid ken linear programming nl linear programming ken linear programming cq linear programming linear programming cre linear programming cre linear programming stochastic programming structurally nonsymmetric matrices optimal power flow lhr light hydrocarbon recovery nonlinear analog circuit lhr light hydrocarbon recovery nonlinear analog circuit lhr light hydrocarbon recovery lhr light hydrocarbon recovery bcsstk stiffness matrix bcsstk stiffness matrix table ii average communication requirements rowwise columnwise decomposition structurally symmetric test matrices 
graph model hypergraph model column net model row net model pmetis hmetis patoh hcm patoh hcc name comm 
comm 
comm 
comm 
proc 
volume proc 
volume proc 
volume proc 
volume avg max tot max avg max tot max avg max tot max avg max tot max sherman ken nl ken cq cre cre averages column avg max denote average maximum number messages respectively handled single processor 
comm 
volume column tot denotes total communication volume max denotes maximum communication volume handled single processor 
communication volume values terms number words transmitted scaled number rows columns respective test matrices 
table iii average communication requirement rowwise decomposition structurally nonsymmetric test matrices 
graph model hypergraph model column net model pmetis hmetis patoh hcm patoh hcc name comm 
comm 
comm 
comm 
proc 
volume proc 
volume proc 
volume proc 
volume avg max tot max avg max tot max avg max tot max avg max tot max lhr lhr lhr lhr bcsstk bcsstk averages column avg max denote average maximum number messages respectively handled single processor 
comm 
volume column tot denotes total communication volume max denotes maximum communication volume handled single processor 
communication volume values terms number words transmitted scaled number rows columns respective test matrices 
table iv average communication requirements columnwise decomposition structurally nonsymmetric test matrices 
graph model hypergraph model row net model pmetis hmetis patoh hcm patoh hcc name comm 
comm 
comm 
comm 
proc 
volume proc 
volume proc 
volume proc 
volume avg max tot max avg max tot max avg max tot max avg max tot max lhr lhr lhr lhr bcsstk bcsstk averages column avg max denote average maximum number messages respectively handled single processor 
comm 
volume column tot denotes total communication volume max denotes maximum communication volume handled single processor 
communication volume values terms number words transmitted scaled number rows columns respective test matrices 
iru zd hw iru zd hw iru zd hw iru zd hw relative run time performance proposed column net row net hypergraph model clique net hmetis patoh hcm patoh hcc graph model pmetis rowwise columnwise decomposition symmetric test matrices 
bars indicate hypergraph model leads slower decomposition time graph model 
iru zd hw iru zd hw iru zd hw relative run times way decompositions hw relative run time performance proposed column net hypergraph model clique net hmetis patoh hcm patoh hcc graph model pmetis rowwise decomposition symmetric test matrices 
bars indicate hypergraph model leads slower decomposition time graph model 
iru zd hw iru zd hw iru zd hw iru zd hw relative run time performance proposed row net hypergraph model clique net hmetis patoh hcc graph model pmetis columnwise decomposition symmetric test matrices 
bars indicate hypergraph model leads slower decomposition time graph model 
table performance averages proposed hypergraph models normalized respect graph models pmetis 
pmetis clique net model hmetis patoh hcm patoh hcc tot 
comm 
volume time tot 
comm 
volume time tot 
comm 
volume time tot 
comm 
volume time best worst avg best worst avg best worst avg best worst avg symmetric matrices column net model row net model avg nonsymmetric matrices column net model avg nonsymmetric matrices row net model avg total communication volume ratio smaller indicates hypergraph model produces better decompositions graph model 
execution time ratio greater indicates hypergraph model leads slower decomposition time graph model 
received degrees computer engineering information science bilkent university ankara turkey respectively 
currently working ph degree department computer engineering information science bilkent university ankara turkey 
current research interests parallel computing graph hypergraph partitioning 
received degrees middle east technical university ankara turkey respectively ph degree ohio state university columbus electrical engineering 
fulbright scholar ph studies 
worked intel supercomputer systems division beaverton research associate 
october department computer engineering information science bilkent university ankara turkey currently associate professor 
research interests include parallel computer architectures parallel algorithms applied parallel computing neural network algorithms graph hypergraph partitioning 
member acm ieee ieee computer society 

