word space hinrich schutze center study language information ventura hall stanford ca representations semantic information words necessary applications neural networks natural language processing 
describes efficient corpus method inducing distributed semantic representations large number words lexical statistics means large scale linear regression 
representations successfully applied word sense disambiguation nearest neighbor method 
tasks natural language processing require access semantic information lexical items text segments 
example system processing sound sequence ts needs know topic discourse order decide plausible hypotheses analysis right nice beach recognize speech 
similarly mail filtering program know topical significance words job properly 
traditional semantic representations ill suited artificial neural networks presume varying number elements representations different words incompatible fixed input window 
localist nature poses problems semantic similarity example dog cat may hidden inheritance hierarchies complicated feature structures 
neural networks perform best similarity targets corresponds similarity inputs traditional symbolic representations property 
microfeatures widely overcome problems 
microfeature representa tions encoded hand don scale large vocabularies 
presents efficient method deriving vector representations words lexical cooccurrence counts large text corpus 
proximity vectors space measured normalized correlation coefficient corresponds semantic similarity 
lexical easily measured 
vocabulary words possible counts keep track 
zero number non zero counts huge 
hand document collection counts small unreliable 
letter fourgrams bootstrap representations 
cooccurrence statistics collected selected fourgrams 
fourgrams frequent counts reliable cooccurrence counts rare words 
matrix purpose manageable 
vector lexical item computed sum vectors occur close text 
process confusion yields representations words fine grained reflect semantic differences various case inflectional forms word may corpus 
organized follows 
section discusses related 
section describes derivation vector representations 
section performs evaluation 
final section concludes 
related kinds semantic representations commonly connectionism microfeatures waltz pollack mcclelland kawamoto localist schemes separate node word cottrell 
approach scales original form applicable large vocabularies wide variety topics 
gallant gallant 
labor intensive method microfeatures features core stems encoded hand new document collection 
derivation word space fully automatic 
uses feature vectors represent words features interpreted 
vector similarity information word space semantically related words close unrelated words distant 
emphasis semantic similarity decomposition interpretable features similar kawamoto 
scholtes uses dimensional kohonen map represent semantic similarity 
kohonen map deal non linearities contrast singular value decomposition space higher dimensionality capture complexity semantic relatedness natural language 
scholtes idea grams reduce number initial features semantic representations extended looking gram statistics occurrence documents cf 
grams information retrieval 
important goal schemes semantic representation find limited number semantic classes classical thesauri roget crouch brown 
multidimensional space constructed word individual representation 
clustering classes introduces artificial boundaries cut words part semantic neighbor governor quits knights columbus bishop abortion gag rule gove qui shop vern quit hts hop rule ule line new york times selected fourgrams 
hood 
large classes members opposite sides class distantly related 
class size problematic words separated close neighbors lumped distant terms 
conversely multidimensional space arbitrary classification necessary 
derivation vector representations selection 
possible fourgrams empty space numbers non alphanumeric characters included special letters 
occurred months new york times 
reduced deleting rare ones frequency redundant uninformative fourgrams described 
group fourgrams occurs word deleted 
instance fourgrams tend occur deleted 
rationale move cooccurrence information fourgrams fully derived index matrix wasted included 
relative frequency occurring calculated 
instance relative frequency frequency divided frequency 
fourgrams occur predominantly stems words 
uninformative fourgrams sequences tion part different words residents resisted abortion construction knowledge carries semantic information 
fourgrams useless deleted 
identify fourgrams occurred frequently stems 
set fourgrams remained deletions 
reduce required size frequent frequent deleted 
shows line new york times selected fourgrams occurred 
computation vectors 
computation word vectors described depends vectors accurately reflect semantic similarity sense describe contents 
consequently needs able compare sets contexts fourgrams occur 
purpose collocation matrix fourgrams collected entry counts number times occurs fourgrams left columns matrix similar contexts corresponding fourgrams similar 
counts determined months new york times june october 
resulting collocation matrix dense entries zeros fourgrams cooccur 
entries smaller culling small counts increase sparseness matrix 
consequently computation employs vectors directly inefficient 
reason singular value decomposition performed singular values extracted cf 
deerwester algorithm berry 
represented vector real values 
singular value decomposition finds best square approximation original space dimensions vectors similar original vectors collocation matrix similar 
reduced vectors efficiently confusion described section 
computation word vectors 
think fourgrams highly ambiguous terms 
inadequate directly input neural net 
get back fourgrams words 
experiment reported cooccurrence information second time achieve goal case target word fourgrams 
selected words see context vector computed position occurred text 
context vector defined sum defined vectors window fourgrams centered target word 
context vectors normalized summed 
sum vectors vector representation target word 
confusion uses corpus 
formally set positions corpus occurs vector representation vector representation defined dot stands normalization ffl close treatment words case sensitive 
terminology surface form string characters occurs text lemma lower case upper case letters lower case possible exception word case insensitive term 
word exactly lemmas 
lemma length surface forms 
lower case lemma realized upper case surface form 
upper case lemmas hardly realized lower case surface forms 
confusion vectors computed lemmas occurred times months new york times news service may october words 
table lists percentage lower case upper case lemmas distribution lemmas respect words 
lemmas number percent lower case upper case total words number percent lower case lemma upper case lemma lemmas total table distribution lower upper case words lemmas 
word nearest neighbors burglar thief rob stray lookout chase thieves disable deter intercept repel halting surveillance shield maneuvers sentiment mindful domestic auto ed threefold inventories drastically cars melodies dic synthesizers soul funk tunes heap goose neatly pulls rake odd rough kid dad mom ok buddies mom oh hey hey jill julie biography judith novak lois learned ste 
dry oyster hot filling rolls lean float bottle ice workforce jobs employ ed ing attrition workers clerical labor hourly keeping hoping bring rest table random selected word nearest neighbors 
evaluation table shows random sample words nearest neighbors word space depending fit table 
neighbors listed order proximity head word 
burglar kid workforce closely related nearest neighbors 
true disable regard goal come characterization semantic similarity corpus opposed language general 
new york times military disable dominates iraq military oil pipelines ships disabled 
similarly domestic usually refers domestic market person named occurs newspaper jazz musician 
cases counted successes 
topic content moderately characterized objects goose rake expect farm 
little useful information extracted ste 
mainly occurs articles literally semantics don come 
neighbors ste part words associated water name river ste quebec popular salmon fishing frequent context ste 
significance ste depends heavily name occurs usefulness contributor semantic information limited poor characterization probably seen problematic 
word keeping added table show vector representations words wide variety contexts interesting 
table shows important words distinction word nearest neighbors pinch outs pitch cone hitting cary teufel mound pinch pepper coarsely combine kappa protein synthesize recombinant enzymes amino dna kappa phi graduate cum dean graduating nyu amherst college yale roe cod squid fish salmon flounder crab roe wade ing ing abortion reproductive overrule completion complete ing complex phase uncompleted incomplete completions interception td tds ok wouldn crazy ain anymore approve ing senate waxman bill omnibus triad ballistic missile ss triads triads organized cosa table words case inflection matter 
word senses correct sum capital goods seat government interest special attention financial motion movement proposal plant factory living ruling decision exert control space area volume outer space suit legal action tank combat vehicle receptacle train railroad cars teach vessel ship blood vessel hollow utensil table disambiguation experiments vector representations 
lower case upper case different inflections 
normalized correlation coefficient case inflectional forms word indicated example 
word sense disambiguation 
word sense disambiguation task semantic phenomena bear suited evaluate quality semantic representations 
vector representations disambiguation way 
context vector occurrence ambiguous word defined sum word vectors window 
set context vectors word training set clustered 
clustering programs autoclass cheeseman buckshot cutting 
clusters assigned senses inspecting members 
occurrence ambiguous word test set disambiguated assigning sense training cluster closest context vector 
note method unsupervised structure sense space analyzed automatically clustering 
see schutze detailed description 
table lists results disambiguation experiments performed algorithm 
line shows ambiguous words major senses success rate disambiguation individual senses major senses 
training test sets taken new york times newswire disjoint word 
disambiguation results best reported literature yarowsky 
apparently vector representations respect fine sense distinctions 
interesting question degree vector representations distributed 
algorithm disambiguation described set contexts suit clustered applied test text 
dimensions clustering training set error rate test set 
odd dimensions error 
dimensions occurrences test set misclassified 
graceful degradation indicates vector representations distributed 
discussion linear dimensionality reduction performed useful preprocessing step applications 
features carries small amount information 
neglecting individual features degrades performance directly input neural network 
word sense disambiguation results suggest information lost axes variations extracted singular value decomposition considered original dimensional vectors 
schutze forthcoming uses methodology derivation syntactic representations words verbs nouns occupy different regions syntactic word space 
problems pattern recognition characteristics uniform distribution information input features pixels high dimensional input space causes problems training features directly 
singular value decomposition useful preprocessing step data nature neural nets applicable high dimensional problems training slow possible 
presents word space new approach representing semantic information words derived lexical cooccurrence statistics 
contrast microfeature representations semantic representations summed context compute representation topic text segment 
shown semantically related words close word space vector representations word sense disambiguation 
word space promising input representation applications neural nets natural language processing information filtering language modeling speech recognition 
indebted mike berry nasa riacs autoclass san diego supercomputer center computing resources 
martin kay julian kupiec jan pedersen martin andreas weigend help discussions 
berry 
large scale sparse singular value computations 
international journal supercomputer applications 
brown pietra desouza lai mercer 

class gram models natural language 
manuscript ibm 
cheeseman kelly self stutz taylor freeman 

autoclass bayesian classification system 
proceedings fifth international conference machine learning 
cottrell 
connectionist approach word sense disambiguation 
london pitman 
crouch 
approach automatic construction global thesauri 
information processing management 
cutting karger pedersen tukey 

scatter gather approach browsing large document collections 
proceedings sigir 
deerwester dumais furnas landauer harshman 

indexing latent semantic analysis 
journal american society information science 
gallant 
practical approach representing context performing word sense disambiguation neural networks 
neural computation 
gallant caid carleton hecht nielsen qing 

hnc system 
proceedings trec 
kawamoto 
distributed representations ambiguous words resolution connectionist network 
small cottrell tanenhaus eds lexical ambiguity resolution perspectives psycholinguistics neuropsychology artificial intelligence 
san mateo ca morgan kaufmann 

searching text 
send gram 
byte magazine may 
mcclelland kawamoto 

mechanisms sentence processing assigning roles constituents sentences 
mcclelland rumelhart pdp research group eds parallel distributed processing 
explorations microstructure cognition 
volume psychological biological models 
cambridge ma mit press 
scholtes 
unsupervised learning information retrieval problem 
proceedings international joint conference neural networks 
schutze 
dimensions meaning 
proceedings supercomputing 
schutze forthcoming 
sublexical tagging 
proceedings ieee international conference neural networks 
waltz pollack 

strongly interactive model natural language interpretation 
cognitive science 
yarowsky 
word sense disambiguation statistical models roget categories trained large corpora 
proceedings coling 
