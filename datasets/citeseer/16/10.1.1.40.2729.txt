evolution successful learning backgammon strategy jordan pollack alan blair computer science department center complex systems brandeis university ma pollack blair cs brandeis edu td gammon parameter feed forward neural network develop competitive backgammon evaluation function 
play proceeds oll dice application network legal moves choosing move highest evaluation 
back propagation temporal dif ference learning methods wer employed 
apply simple hill climbing fitness 
start initial champion zer weights pr simply playing current champion network slightly mutated challenger changing weights challenger wins 
surprisingly worked 
investigate peculiar dynamics domain enabled pr discarded weak method succeed pr suboptimal equilibria meta game self learning 
keywords coevolution backgammon temporal dif ference learning self learning running head evolutionary learning 
current address dept computer science electrical engineering university queensland australia blair csee uq edu au machine learning 
took gr eat gerald start wasting computer cycles temporal difference learning game backgammon 
letting machine learning pr play hopes expert 
dream computers mastering domain self play intr early days ai forming part samuel checker player samuel donald michie tic tac toe learner michie self conditioning systems generally abandoned field due problems scale weak non existent internal 
mor learners usually develop brittle strategies appear clever fare poorly expert human computer players 
showed self play appr powerful millions iterations self play td gammon pr best backgammon players world 
derived weights ar viewed significant intellectual pr keep trade leverage sales minority operating system international business machines 
td backgammon esearch purposes boyan commercial purposes 
learning limited success ar eas zhang dietterich crites barto goal self organizing learning machine starts fr om minimal specification rises great sophistication td gammon stands :10.1.1.17.5519
success understood explained domains 
hypothesis success td gammon principally due back propagation temporal difference technologies inher ent bias fr om dynamics game backgammon evolutionary setup training task dynamically changes learning pr 
test hypothesis simpler evolutionary learning method backgammon hill climbing 

implementation details standard feedforward neural network layers sigmoid function set fashion units number player pieces points plus units indicate bar board 
addition added mor unit reports game endgame race situation making total input units 
ar fully connected hidden units ar connected output unit judges position 
including bias hidden units total weights 
game played generating legal moves converting pr oper network input picking position judged best network 
started weights set zer initial algorithm hillclimbing add gaussian noise weights play network mutant number games mutant wins mor half games select generation 
noise set step rms distance euclidean distance divided 
surprisingly worked 
networks evolved impr rapidly pr perceived comparing close backgammon players tossing biased coin may take dozens hundreds games find sur better replacing tested champion danger ous information pr ove challenger better player just lucky novice 
bur den system computation intr modifications algorithm avoid douglas ef fect firstly games ar played pairs der play random seed generate dice games 
unfairness due dice networks ar close particular identical win admittedly different moves early game dice oll particular move game may turn bad oll corr move parallel game 
secondly challenger wins contest just champion challenger small adjustment dir ection champion champion challenger idea similar inertia term back pr rumelhart introduced assumption small changes weights lead small changes decision making evaluation function 
just biting ear challenger adding champion curr ent decisions ar preserved champion lucky novice challenger 
initial stages evolution pairs parallel games played challenger win games 
liked rank players players tesauro wer available 
shows players rated moderately public domain player trained human expert pr 
things note wins increases fr om generations fr successful incr eases time player impr ther epochs starting performance pubeval begins 
fact shows simple self 
douglas world heavyweight boxing champion months 
playing hill climber capable learning 
second fact quite counter intuitive expected player impr har der challenge 
tr ue respect uniform sampling dimensional weight space tr ue sampling neighborhood player player part weight space small changes weights lead similar strategies ones moves situations 
games determine fitness incr eased rate change allows system drift may account subsequent degrading counteract drift decided change ules engagement evolution pr annealing schedule generations number games challenger win incr eased fr om generations incr eased course bout abandoned soon champion won mor game making average number games generation considerably 
numbers chosen ad hoc basis fr om observing fr successful challenges douglas ef fect particular un experiments showed determine annealing schedule mor principled manner see section 
games simple hill climb developed surprising player capable winning games networks wer sampled generations der test performance 
networks generation wer extracted benchmarks 
shows percentage wins sampled players thr ee benchmark networks 
note thr ee curves cr oss line show general improvement time 
game backgammon called bear pr learning 
bear occurs player pieces ar home board points dice pieces percentage wins generation players pubeval 
match consisted games 
generation win board 
test network ability game set racing pieces player thr point piece point 
graph shows average number bear network playing fixed set random dice str 
note pubeval stronger discuss str section 
percentage wins benchmark networks upper middle lower 
shows noisy nearly monotonic increase player skill evolution proceeds 
win generation rolls generation average number rolls generation sampled dice streams 
pubeval averaged rolls task 

analysis 
learnability learnability formally defined time constraint sear ch space 
hard randomly pick floating point weights backgammon evaluator 
simply impossible 
har find weights better curr ent set 
initially weights ar random quite easy playing impr expect get har der harder similar pr tornado constructing sear ch current weights find similar players moves capitalize slightly dif ferent choices exposed weaknesses tournament 
note dif ferent point originally feedforward neural network exploit similarity positions 
setting parameters initial uns involved guesswork lar ge set players examine try understand phenomenon 
champion networks generation fr om un sampled random players neighbor dif ferent rms distances find find winning challenger random neighbors different rms distances played games corr champion plots fraction games won function rms distance 
graph shows players impr ove time pr finding neighbor hood increases accounts fr successful challenges goes 
successive challenger 
number neighbor hood go algorithm 
ther factors study 
may due general growth weights variability strategy players ability simply tell expert players apart games 
distance versus probability random challenger winning champions generation 
rms distance fr om champion wins challenger take small step changing moves champion der beat 
hope evolution learnable convert fr om single question continuous str questions dependent pr answer 

replication experiments successful un tried evolve mor players parameters annealing schedule players competitive 
closer examination suggested uns failing wer annealed early fr successful challenges appropriate level 
pr annealing task har der challenger success rate fell lower abandoned fixed annealing schedule annealed challenger success rate exceeded averaged generations 
players evolved competitive quite original player apparently benefitted fr om extra inductive bias due having tailor annealing schedule 
refining heuristics schedules lead superior players goal 

relative versus absolute expertise backgammon allow expertise ther absolutely optimal strategy 
theoretically ther exists perfect policy backgammon deliver minimax optimal move position perfect policy exactly rate player linear scale practice especially games verify ther cycles help pr event early convergence 
cellular studies iterated prisoner dilemma od stable population tit tat invaded cooperate allows exploitation defect 
kind expertise dynamics seen clearly simple game scissors littman initially bad self play learning looks advance lead cycle 
small gr oup champions dominance cir cle arise hold temporal oligopoly pr advance 
hand may basic form instability pr events formation sub optimal allows learning pr 
pr specific non zero sum games zer sum games appropriate self play shown conver ge optimal play parties littman 
discussion believe evidence success learning backgammon simple hillclimbing fitness indicates temporal difference methodology led td gammon providing advantage essential success 
major contribution came fr om evolutionary learning dynamics backgammon 
similar bias mitchell evolution cellular automata edge chaos mitchell etal 
obviously ar suggesting hillclimbing advanced machine learning technique bring tasks 
internal cognition opponent behavior evolution usually population 
ther domain helpful permitted td learning hill climbing succeed thr self play clearly fail pr solving tasks scale 
section discuss issues evolutionary learning dynamics backgammon may critical learning success 

evolution versus evolution td gammon major milestone kind evolutionary machine learning initial specification model far simpler expected learning environment specified implicitly emerges evolution learning system training learner embedded environment impr hopefully spiral elusive goal achieve practice 
evolutionary effect seen population models completely unexpected hillclimbing evolution 
evolution ed sorting network pr hillis tic tac toe strategy games angeline pollack rosin belew schraudolph pr prey games cliff miller reynolds classification pr intertwined spirals pr pollack 
td gammon date viewed instance evolutionary learning sims artificial robot game sims domain complex backgammon substantial success 
weak player defeat str ong theory possible network learn backgammon static evolutionary playing fixed opponent evolutionary playing 
course inter learning expert hand td gammon simply learned fr om wouldn startling 
der isolate contribution evolutionary learning modify training setup original algorithm appr self play 
new setup curr ent champion mutant play number games opponent called foil dice streams weights ar adjusted champion loses games mutant wins 
number pairs games initially set incr challenger success rate exceeded averaged generations 
lower plots track performance algorithm thr ee benchmark networks fr om original experiments acting foil show learning rate pr winning 
weak foil learning fast initially pr winning tapers pr incr eases 
str ong foil learning slow initially pr winning small speeds increases 
evolutionary uns outperformed evolutionary version foil algorithm ev champion network plays ole foil 
evolution maintain high learning rate thr un automatically pr new generation player opponent appropriate skill level keep pr winning near 
mor weaknesses foil ar bias learning pr automatically corrected evolution pr see section 

dynamics backgammon general pr learning thr self play discovered early ai ml learner keep playing kinds games exploring ow region strategy space missing critical areas game vulnerable pr human experts 
pr particularly pr deterministic games chess tic tac toe 
pointed es backgammon suitable appr involving self play random initial conditions 
chess draw impossible game played untrained network making random moves eventually terminate may take longer game competent players 
mor randomness dice leads self play larger part sear ch space deterministic game 
worked population get ar ound limitations self play angeline pollack 
schraudolph added non determinism game go choosing moves ding boltzmann distribution statistical mechanics 
fogel expanded exploration cing initial moves 
epstein studied mix training self play random testing playing expert der better understand aspects game learning 
win generation performance pubeval players evolved playing benchmark networks original run generation compared evolutionary variant algorithm 
plots average runs 
performance original algorithm included comparison 
original ev believe add randomness game ce exploration alternative training paradigms 
ther critical dynamics backgammon sets apart fr om games random elements monopoly outcome game continues uncertain contact br side clear advantage 
monopoly early advantage purchasing pr leads accumulating 
observers find exciting backgammon helps novice come expert number situations dice oll impr sequence dramatically reverse player expected win 
order quantify ef fect collected statistics fr om games played th generation network 
collected dif ferent games ther contact move forn games racing stage move move number deviation move number contact racing game pr standard deviation probability winning contact positions racing positions 
contact racing probability game contact racing stage move smoothed distributions probability winning function move number contact positions left racing positions right 
density density pr move number move pr progress 
estimated pr winning fr om positions playing dif ferent dice streams 
shows standard deviation probability assuming mean function pr game contact racing stage move shows distribution pr winning function move number symmetrized smoothed convolution gaussian function 
data indicate pr winning tends hover near early stages game gradually moving play pr typically range long ther contact allowing chance 
numbers dif ferent players str players weaker ones believe effect integral part game dynamics expertise 
conjecture dynamics facilitate learning pr pr situation nontrivial chance winning nontrivial chance losing ther potential learn consequences current move 
deep contrast domains early lead hopeless situation fr om learning virtually impossible ef unattainable 
backgammon may shar ed tasks td learning successful zhang dietterich crites barto 

avoiding suboptimal equilibria meta game learning learning system viewed interaction teacher student teacher goal expose student weaknesses corr ect student goal teacher avoid corr ection 
build model teacher student interaction formal game call meta game learning avoid confusion game learned 
meta game teacher pr student sequence prompting student 
backgammon domain questions legal positions moves 
payoffs pr attempt maximize thr choices questions answers limited abilities self modification 
generally assume goal learning pr student interaction complex environment pr objective measure performance 
play similar oles ar assumed identical 
question find matrix enable performance continually impr ove measured 
ar closely corr may tempted ask questions ar easy 
ar anti correlated example questions dif 
case har learn see section 

general theory evolution self necessary 
attractive solution pr mor students play ole teacher single student act teacher pr questions ar appr level dif 
dynamics self teaching evolutionary situation hopefully lead continuing spiral impr may get bogged antagonistic collusive dynamics depending structure 
hillclimbing setup may think mutant teacher trying gain advantage adjustment weights exploiting weaknesses champion champion student trying avoid adjustment allowing weaknesses exploited 
student teacher ar approximately equal ability advantage student ow scope sear ch limiting domain teacher able look weakness 
games chess tic tac toe student achieve aiming draw win playing particular style game 
draws ar allowed teacher student may way collude example throwing alternate games angeline making suboptimal sequence early moves 
ef self learning systems may appear early conver gence evolutionary algorithms owing scope drawing collusion teacher student ar equilibria stable states 
hypothesis certain es backgammon operate formation mediocre stable states backgammon ergodic sense position fr om position sequence moves dice apparently cr randomness pr event player fr om strategy ows scope game appr 
early suboptimal moves pr opponent easy win see section collusion thr owing alternate games pr 
mediocre stable states arise human education systems example student gets answers right teacher positive teaching evaluations asking har der questions 
hope apply kind equilibrium analysis issues human education 

td gammon remains tr success machine learning causes success understood 
fundamental esearch basis td gammon beat sun time depending number hidden units achieved parity 
seminal incorporated number hand crafted expert knowledge features eventually engineering network achieved world 
mss follows maynard smith ess maynard smith exception racing situations positions pieces play master level play 
es included concepts existence prime pr hit pr escape fr om opponent barrier 
evaluation function impr multiple ply sear ch 
best players able evolve win time believed level networks 
compared networks heuristic endgame ratings level play achieved players somewhat testing pr play game network race algorithm move sides 
penalize td net having learned poorly racing phase game compare network performance noted network weak endgame substituting str expert system 
gerald commentary issue graciously cleared matter comparing dif somewhat 
phenomena fom ar performance position racing test set 
substantially worse racing specialists described pr section training times wer der training games networks hidden units games hidden unit net games hidden unit net 
achieve similar levels skills observe phenomena training endgame weakness conver gence believe achieved substantially similar advanced learning algorithms 
str players tuning learning parameters adding mor input features point 
claim th generation player near current enhanced versions td gammon challenge best humans surprisingly considering humble origins fr om hill climbing fitness measure 
parameters adding mor input features powerful players point study claim ther wr ong td learning hillclimbing just learning general 
course isn 
point environment machine learning method benchmarked weakest possible algorithm cr edit learning power pr distributed 
noticed weaknesses player stem fr om training punish double triple costs associated sever losses take account gambling pr doubling 
continuing develop player sensitive issues game 
inter players challenge th network web browser home page www demo cs brandeis edu td gammon success simpler learning paradigm find temporal dif ference methods ar primary cause success dynamics backgammon combined power evolutionary learning 
isolate features backgammon domain enable evolutionary learning may lead better understanding conditions necessary general complex self 
acknowledgments supported onr foundation postdoctoral fellowship 
gerry pr pubeval subsequent means calibrate jack ence pablo funes development www fr ont evolved player comments fr om brandeis demo gr oup anonymous justin boyan om dietterich leslie kaelbling br michael littman andr ew moore rich sutton ei zhang 
angeline pollack 

competitive evolve better solutions complex tasks 
forr est editor genetic algorithms proceedings fifth inter national conference 
angeline 

alternate interpr etation iterated prisoner dilemma evolution non mutual cooperation 
br maes editors proceedings th artificial life conference pages 
mit pr ess 
axelrod 
evolution cooperation 
basic books new ork 
boyan 

modular neural networks learning context dependent game strategies 
master thesis computer speech language pr cambridge university 
cliff miller 

ed queen measurements adaptive pr evolutionary simulations 
third european conference artificial life pages 
crites barto 

impr elevator performance learning 
touretzky mozer hasselmo editors advances neural information pr systems volume pages 
international business machines sept 
ibm family os warp hits shelves 
pollack 

massively parallel genetic pr 
angeline kinnear editors advances genetic programming ii 
mit press cambridge 
littman 

markov games framework multi agent learning 
machine learning proceedings eleventh international conference pages 
mor gan kaufmann 
littman 
algorithms sequential decision making 
ph dissertation pr br university computer science department 
maynard smith john evolution theory games cambridge cambridge university pr ess michie 

err 
survey part pages 
penguin 
mitchell cr 

revisiting edge chaos evolving cellular automata perform computations complex systems 
packard 

adaptation ds edge chaos 
kelso editors dynamic patterns complex systems pages 
scientific 
reynolds 

competition coevolution game tag 
proceedings th artificial life conference 
mit press 
rosin belew 

methods competitive evolution finding opponents worth beating 
inproceedings th international conference genetic algorithms pages 
mor gan kaufman 
rumelhart hinton 

learning back propagating err ors 

samuel 

studies machine learning game checkers 
ibm journal research development 
schraudolph dayan sejnowski 

difference learning position evaluation game go 
advances neural information pr systems volume pages 
morgan kauffman 
sims 

evolving morphology behavior competition 
br maes editors proceedings th artificial life conference 
mit press 
sutton 

learning pr methods temporal dif ferences machine learning 
tesauro 

connectionist learning expert pr comparison training 
editor advances neural information pr systems volume pages denver 
mor gan kaufmann san mateo 
tesauro 

practical issues temporal dif ference learning machine learning 
tesauro 

difference learning td gammon communications acm 
downs 

difference non determinism noise case study othello game 
conference artificial neural networks pages sorrento italy 
zhang dietterich 

high performance job shop scheduling time delay td lambda network 
mozer hasselmo editors advances neural information pr systems volume 
