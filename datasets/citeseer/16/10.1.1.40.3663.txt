novelty detection approach classification japkowicz department computer science rutgers university new brunswick new jersey nat paul rutgers edu catherine myers mark gluck research center rutgers university newark new jersey pavlov rutgers edu novelty detection techniques concept learning methods proceed instances concept differentiating positive negative instances 
novelty detection approaches consequently require negative training instances 
presents particular novelty detection approach classification uses redundancy compression differentiation technique gluck myers model hippocampus part brain critically involved learning memory 
particular approach consists training autoencoder reconstruct positive input instances output layer autoencoder recognize novel instances 
classification possible training positive instances expected reconstructed accurately negative instances 
purpose compare hippo system implements technique feedforward neural network classification applications 
practical applications supervised learning concept learning problems problems involve discriminating instances belong class 
class thought concept learned 
usually concept learning involves learning correct classification training set containing positive negative instances concept followed testing phase novel examples classified 
performance depends constructing training sets contain broad range positive negative examples 
classification methods designed conditions including quinlan backpropagation applied feedforward neu appears pp 
proceedings fourteenth international conference artificial intelligence held august montreal canada 
center molecular behavioral neuroscience ral network ff classification rumelhart hinton williams 
alternative learning concept broad range positive negative training examples concept learning techniques require mainly positive training examples introduced petsche hanson gluck 
techniques grouped novelty detection methods term stems fact negative inputs recognized novel compared positive inputs familiar belong class training 
novelty detection techniques proceed examining instances concept trying find commonalities generalizing 
techniques differ conventional classification approaches ff classification attempt recognize instances concept differentiate instances classes 
advantage novelty detection approaches problems easily addressed conventional approaches 
problems negative examples expensive difficult obtain 
machine fault diagnosis example positive examples plentiful typically involve recording machine normal operation 
negative examples involve causing machine break manner failure possible recording failure type 
monitoring tasks consist examining system readily available signals issuing alarm potential problem detected fall category problems 
need monitor system operation arises frequently development reliable novelty detection techniques provide important benefits critical military commercial systems helicopter fire pumps motors generators 
novelty detection methods need demonstrate certain level reliability 
purpose introduce particular novelty detection technique redundancy compression non redundancy differentiation demonstrate experimentally addition ing fewer negative examples technique able classify novel examples accurately conventional classification approaches 
particular novelty detection technique introduced uses autoencoder hinton 
autoencoder neural network learns map inputs narrow hidden layer output nodes attempt reconstruct input 
network narrow hidden layer forced compress redundancies input retaining differentiating non redundant information 
implement technique network trained reconstruct possible training set consisting positive examples 
having trained positive instances concept autoencoder able adequately reconstruct subsequent positive instances perform poorly task reconstructing subsequent negative instances concept different structural regularities 
identifying positive negative instances concept equivalent assessing instances reconstructed autoencoder 
processes previously proposed model computations occurring hippocampus gluck myers part brain involved learning memory 
presents hippo concept learning system idea assesses performance applying real world problems ch helicopters fault detection recognition promoter regions dna classification sonar targets hippo error rates applications compared produced ff classification 
section describes hippo including redundancy compression non redundancy differentiation component threshold determination component 
section discusses testing hippo results 
hippo implementing idea redundancy compression non redundancy differentiation results device able issue sorts signals recognized data novel data 
designing device sufficient building powerful stand concept learning system needs able discriminate recognized novel signals 
discrimination called threshold determination 
section describes hippo redundancy compression differentiation component discusses threshold determination component 
part overview functioning system 
redundancy compression non redundancy differentiation redundancy compression non redundancy differentiation process believed occur hippocampus gluck myers 
gluck myers connectionist models study function hippocampal region brain implications learning memory behaviors 
suggested hippocampus forms new stimulus representations learning representations specifically compress redundant information preserving differentiating non redundant information 
system built considerations able accurately predict range classical conditioning behaviors observed normal hippocampal damaged animals gluck myers 
compression differentiation constraints appear useful learning brain gluck myers suggest may useful machine learning tasks 
redundancy compression non redundancy differentiation technique uses autoencoder type proposed hinton 
autoencoder artificial neural network composed number input nodes number output nodes number hidden nodes type network learns reproduce inputs output layer multilayer learning algorithm backpropagation rumelhart hinton williams learning paradigm 
presents autoencoder 
past estimating learning algorithms reliability pomerleau solving catastrophic inference problem 
discuss novelty detection 
order novelty detection autoencoder trained positive instances concept backpropagation 
trained autoencoder fed new instances tries output layer 
quality reconstruction evaluated computing sum absolute error corresponding input output node 
error gamma inp corresponding input output nodes position size input 
error recorded various epochs 
autoencoder sufficiently trained error small instance labeled positive labeled negative 
part system responsible evaluating size error labeling new instances semi automated module called threshold determination component described section 
phenomena take place autoencoder understood follows narrow internal layer autoencoder forces generate output layer input layer hidden layer autoencoder 
internal representation compresses redundancies input pattern retaining differentiating non redundant information 
autoencoder specifically classification trained positive instances 
training consequently autoencoder learns reconstruct positive data learn reconstruct negative data 
negative data structural regularities different positive data inputs redundant positive data may redundant negative data vice versa reconstruction negative data done differently 
testing time reconstruction positive data succeed reconstruction negative data fail success failure criterion classifying new instances 
threshold determination threshold determination consists determining boundary discriminates reconstruction errors positive negative data 
threshold determination component semi automated component composed algorithms noiseless case requires positive negative data noisy case requires positive negative data 
application algorithms manually selected availability data expected quality separation positive negative reconstruction errors 
noiseless case noiseless case separation positive negative data clear stable reconstruction error positive instances lower negative instances sufficient training took place 
case positive negative instances necessary 
case negative training instances provided procedure built simply computes lower bound reconstruction error negative training instances epoch considered relaxes bound reducing certain percentage 
new instances subsequently classified checking reconstruction error new instance higher relaxed boundary certain acceptable proportion epochs considered 
case new instance negative positive 
case positive training instances provided treated similar fashion 
illustrates noiseless case shows boundary derived negative data 
particular case study uses algorithm ch helicopter relaxation ratio set epochs considered recorded epochs occur epoch acceptable proportion epochs considered set half 
noisy case noisy case separation positive negative data clear majority positive instances low reconstruction errors majority negative instances high reconstruction errors positive examples high reconstruction error negative examples low reconstruction error 
case threshold determination component needs process positive negative instances order establish boundary need decide data ignore exceptional possibly noisy 
procedure built case tries find epoch shows best separation reconstruction errors positive negative instances epochs considered time considers stable separation order find best separation procedure begins constructing boundaries absolute intermediate positive negative regions epoch versus reconstruction error space epoch considered 
instances belong class great certainty reconstruction errors fall absolute region class instances belong class certainty reconstruction errors fall intermediate region 
absolute negative region located intermediate negative region absolute positive region located positive intermediate region 
absolute intermediate regions differ actual regions span entire negative positive instance sets respectively 
absolute intermediate regions illustrated 
construct negative intermediate region partic boundary epoch reconstruction error positive region clear stable case boundary epoch reconstruction error intermediate negative positive regions absolute negative positive regions unclear case cases threshold determination 
ular procedure begins stating lower higher boundaries actual negative region proceeds repeatedly shrinking region manipulating boundaries believes accurate intermediate negative region 
region located upper boundary final intermediate negative region defines absolute negative region 
particular case studies algorithm promoter sonar targets density level final intermediate negative region lower half final intermediate negative region contains fifth negative data half negative data contained lower half contained lowest fifth 
intermediate positive region constructed similar fashion boundary classifying positive negative examples established midpoint lower boundary negative intermediate region upper boundary positive intermediate region 
order find epoch stable separation program calculates stable separation recorded epoch 
stability epoch defined slope line goes separation epoch epoch recorded 
separation selected classification best separation stability higher half greatest stability encountered 
functioning system training hippo carried phases 
phase redundancy compression non redundancy differentiation component trained positive instances concept 
phase results computation specialized autoencoder differentiate positive negative instance showing small reconstruction error positive case large 
second phase training consists training threshold determination component 
phase specialized autoencoder positive negative instances 
instance reconstruction error recorded fed threshold determination component analyzes reconstruction error instances issues discriminator 
discriminator interpreted boundary positive negative instances 
components trained hippo follows unlabeled instance input specialized autoencoder issue reconstruction error 
reconstruction error input discriminator issue classification 
illustrates functioning concept learner 
experiments hippo tested domains ch helicopter fault detection molecular biology promoter recognition sonar target classification 
results compared standard approaches classification decision tree learning system quinlan ff classification connectionist learning method rumelhart hinton williams 
introducing domains considered methodology evaluate approaches 
discuss results experiments 
case studies ch helicopter data obtained 
ch helicopter problem monitoring problem consists discriminating faulty non faulty ch helicopter sound emit operation 
sudden unexpected failure ch helicopter currently costly terms lives equipment 
development monitoring system identify imminent failures takeoff flight paramount importance 
data problem obtained pre processing vibration time signal various faulty non faulty helicopters 
complete data set composed non faulty instances faulty ones come form long vectors real numbers 
particular problem chose non faulty examples represent positive class 
specialized autoencoder redundancy compression differentiation positive instances positive negative instances discriminator threshold determination training phase specialized autoencoder discriminator reconstruction error new instance classification testing phase functioning system 
promoter problem takes input segments dna subset represent promoters 
promoter sequence signals chemical processes acting dna gene begins 
goal problem train classifier able recognize promoters taken positive class 
training set composed examples promoters negatives composed set nucleotides nucleotide take values fa tg 
promoter data obtained irvine repository machine learning modified response norton critique biological flaws underlying original formulation data norton 
addition usual problem run connectionist system example converted bit long vector nucleotide represented bits 
sonar target recognition problem takes input signals returned sonar system cases mines rocks targets 
sonar data obtained irvine repository machine learning subset instances positive negative data particular case study 
transmitted sonar signal frequency modulated chirp rising frequency 
data set contains signals obtained variety different aspect angles 
instance data represented bit long vector 
particular case study chose signals returned mine targets constitute positive class 
general methodology connectionist models hippo ff classification difficult train symbolic models 
connectionist models require tuned trained training requires phases concept learning threshold determination 
tuning necessary single phase sufficient symbolic models 
learning rate momentum bias hippo ff classification set respectively held constant case studies 
number hidden units recorded epochs determined experimentally case study random subsets entire data sets 
helicopter application hippo ff classification hidden units 
systems run epochs recorded epochs epochs epoch subsequently 
promoter problem systems hidden units sonar target recognition problem hidden units 
applications system run epochs recorded epochs 
threshold determination method section hippo ff classification 
tuned random subsets entire data sets 
case study thresholds established data sets systems 
helicopter problem noiseless method section selected applied negative instances 
promoter sonar target recognition problems noisy method section selected applied positive negative instances 
domains considered systems evaluated fold crossvalidation weiss kulikowski 
fold experiment training set divided training set concept learning threshold determination hippo ff classification 
hippo learns concept positive data negative data eliminated explains results reported section different reported previous experiments data gorman sejnowski ff classification input threshold determination component taken value output node output reversed positive instances supposed return larger signal negative ones 
concept learning training set system kept ff classification 
experiment testing sets systems corresponded 
note fold experiment hippo uses significantly fewer negative data training systems ch helicopter problem hippo uses negative data ff data 
case studies hippo uses negative instances ff classification instances 
results error rates hippo ff classification case studies considered listed table 
numbers sigma standard deviations fold averages 
case study hippo ff classif 
error error error helicopters sigma sigma sigma promoters sigma sigma sigma sonar targets sigma sigma sigma table results case studies 
table shows ch helicopter sonar target recognition case studies hippo performed better ff classification 
promoter case study hippo ff classification performed equally better 
comparisons statistically significant comparison sonar target recognition study 
altogether shows addition requiring smaller number negative training data systems hippo capable classifying novel instances accurately ff classification cases promoter data hippo performance matched ff classification 
believe limited performance hippo respect ff classification weakness representation version promoter problem hirsh noordewier norton 
new approach classification uses idea novelty detection particular redundancy compression non redundancy differentiation 
system introduced hippo attempts learn recognize concepts differentiate positive negative instances concept 
method works phases 
phase concept learned positive instances second phase system learns identify positive negative instances concept 
hippo tested real world applications compared conventional classification systems feedforward classification 
applications hippo performed better performed better feedforward classification third application hippo feedforward classification performed equally 
furthermore applications hippo significantly smaller number negative training data systems 
opens large number possible theoretical practical issues consider 
useful particular establish strength limitations approach precisely experimenting domains artificial real comparing hippo results methods feedforward classification 
attempt improve components hippo refined version autoencoder fully automating threshold determination component 
studies contribute exploration promising new approach concept learning accurate conventional methods requires fewer negative data training 
research supported office naval research young investigator program 
parts conducted centre neural networks king college london laboratoire universit pierre marie curie paris 
rick kaye brian davison helpful comments earlier drafts bob contributions helicopter analysis feature extraction 
gluck myers 
hippocampal mediation stimulus representation computational theory 
hippocampus 
gorman sejnowski 
analysis hidden units layered network trained classify sonar targets 
neural networks 
hinton 
connectionist learning procedures 
artificial intelligence 
hirsh noordewier 
background knowledge improve learning dna sequences 
proceedings tenth ieee conference artificial intelligence applications 


helicopter monitoring 
nips workshop novelty detection adaptive system monitoring 

episodic memory connectionist networks 
twelfth annual conference cognitive science society 
norton 
learning recognize promoter sequences coli modeling uncertainty training data 
proceedings twelfth national conference artificial intelligence 
petsche hanson gluck 
workshop novelty detection adaptive system monitoring 
neural information processing systems 
volume 
pomerleau 
input reconstruction reliability estimation 
proceedings fifth neural information processing systems conference 
quinlan 
programs machine learning 
san mateo ca morgan kaufmann 
rumelhart hinton williams 
learning internal representations error propagation 
rumelhart mcclelland eds parallel distributed processing 
cambridge ma mit press 

weiss kulikowski 
computer systems learn 
san mateo ca morgan kaufmann 
