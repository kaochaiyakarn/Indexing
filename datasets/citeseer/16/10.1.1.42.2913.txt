anfis adaptive network fuzzy inference system shing roger jang department electrical engineering computer science university california berkeley ca presents architecture learning procedure underlying anfis adaptive network fuzzy inference system fuzzy inference system implemented framework adaptive networks 
hybrid learning procedure proposed anfis construct input output mapping human knowledge form fuzzy rules stipulated input output data pairs 
simulation employ anfis architecture model nonlinear functions identify nonlinear components control system predict chaotic time series yielding remarkable results 
comparisons neural networks earlier fuzzy modeling listed discussed 
extensions proposed anfis promising applications automatic control signal processing suggested 
system modeling conventional mathematical tools differential equations suited dealing ill defined uncertain systems 
contrast fuzzy inference system employing fuzzy ifthen rules model qualitative aspects human knowledge reasoning processes employing precise quantitative analyses 
fuzzy modeling fuzzy identification explored systematically takagi sugeno numerous practical applications control prediction inference 
basic aspects approach need better understanding 
specifically 
standard methods exist transforming human knowledge experience rule base database fuzzy inference system 

need effective methods tuning membership functions mf minimize output error measure maximize performance index 
perspective aim suggest novel architecture called adaptive network fuzzy inference system simply anfis serve basis constructing set fuzzy rules appropriate membership functions generate stipulated input output pairs 
section introduces basics fuzzy rules fuzzy inference systems 
section describes structures learning rules adaptive networks 
embedding fuzzy inference system framework adaptive networks obtain anfis architecture backbone covered section 
application examples nonlinear function modeling chaotic time series prediction section 
section concludes giving important extensions directions 
ii 
fuzzy rules fuzzy inference systems fuzzy rules research supported part nasa ncc micro agreement rp program 
ieee trans 
systems man cybernetics vol 
pp 
may input output database rule base knowledge base decision making unit defuzzification interface interface fuzzy fuzzy crisp crisp fuzzy inference system 
fuzzy rules fuzzy conditional statements expressions form labels fuzzy sets characterized appropriate membership functions 
due concise form fuzzy rules employed capture imprecise modes reasoning play essential role human ability decisions environment uncertainty imprecision 
example describes simple fact pressure high volume small 
pressure volume linguistic variables high small linguistic values labels characterized membership functions 
form fuzzy rule proposed takagi sugeno fuzzy sets involved premise part 
takagi sugeno fuzzy rule describe resistant force moving object follows velocity high force velocity high premise part linguistic label characterized appropriate membership function 
consequent part described equation input variable velocity 
types fuzzy rules extensively modeling control 
linguistic labels membership functions fuzzy rule easily capture spirit rule thumb humans 
angle due qualifiers premise parts fuzzy rule viewed local description system consideration 
fuzzy rules form core part fuzzy inference system introduced 
fuzzy inference systems fuzzy inference systems known fuzzy rule systems fuzzy models fuzzy associative memories fam fuzzy controllers controllers 
basically fuzzy inference system composed functional blocks ffl rule base containing number fuzzy rules ffl database defines membership functions fuzzy sets fuzzy rules ffl decision making unit performs inference operations rules ffl interface transforms crisp inputs degrees match linguistic values ffl defuzzification interface transform fuzzy results inference crisp output 
usually rule base database jointly referred knowledge base 
steps fuzzy reasoning inference operations fuzzy rules performed fuzzy inference systems min consequent part premise part type type type average weighted average weighted px qy ax centroid area max multiplication commonly fuzzy rules fuzzy reasoning mechanisms 

compare input variables membership functions premise part obtain membership values compatibility measures linguistic label 
step called 

combine specific norm operator usually multiplication min 
membership values premise part get firing strength weight rule 

generate qualified consequent fuzzy crisp rule depending firing strength 

aggregate qualified consequents produce crisp output 
step called defuzzification 
types fuzzy reasoning proposed literature 
depending types fuzzy reasoning fuzzy rules employed fuzzy inference systems classified types type output weighted average rule crisp output induced rule firing strength product minimum degrees match premise part output membership functions 
output membership functions scheme monotonically non decreasing 
type fuzzy output derived applying max operation qualified fuzzy outputs equal minimum firing strength output membership function rule 
various schemes proposed choose final crisp output fuzzy output center area bisector area mean maxima maximum criterion 
type takagi sugeno fuzzy rules 
output rule linear combination input variables plus constant term final output weighted average rule output 
utilizes rule input fuzzy inference system show different types fuzzy rules fuzzy reasoning mentioned 
aware differences lie specification consequent part decreasing bell shaped membership functions crisp function defuzzification schemes weighted average centroid area different 
iii 
adaptive networks architectures learning algorithms vector vector input output adaptive network 
section introduces architecture learning procedure adaptive network fact superset kinds feedforward neural networks supervised learning capability 
adaptive network name implies network structure consisting nodes directional links nodes connected 
part nodes adaptive means output nodes depends parameter pertaining node learning rule specifies parameters changed minimize prescribed error measure 
basic learning rule adaptive networks gradient descent chain rule proposed werbos 
due state artificial neural network research time werbos early failed receive attention deserved 
presentation derivation author generalizes formulas 
basic learning rule gradient method notorious slowness tendency trapped local minima propose hybrid learning rule speed learning process substantially batch learning pattern learning proposed hybrid learning rule discussed 
architecture basic learning rule adaptive network multi layer feedforward network node performs particular function node function incoming signals set parameters pertaining node 
nature node functions may vary node node choice node function depends input adaptive network required carry 
note links adaptive network indicate flow direction signals nodes weights associated links 
reflect different adaptive capabilities circle square nodes adaptive network 
square node adaptive node parameters circle node fixed node 
parameter set adaptive network union parameter sets adaptive node 
order achieve desired input output mapping parameters updated training data gradient learning procedure described 
suppose adaptive network layers th layer nodes 
denote node th position th layer node function node output node output depends incoming signals parameter set gamma gamma gamma parameters pertaining node 
note node output node function 
assuming training data set entries define error measure energy function th entry training data entry sum squared errors gamma th component th target output vector th component actual output vector produced presentation th input vector 
error measure order develop learning procedure implements gradient descent parameter space calculate error rate ep th training data node output error rate output node calculated readily equation gamma gamma internal node error rate derived chain rule gamma 
error rate internal node expressed linear combination error rates nodes layer 
find ep equation 
ff parameter adaptive network ff ff set nodes outputs depend ff 
derivative error measure respect ff ff ff accordingly update formula generic parameter ff ff gammaj ff learning rate expressed ff ff step size length gradient transition parameter space 
usually change value vary speed convergence 
heuristic rules changing discussed section report simulation results 
learning paradigms adaptive networks 
batch learning line learning update formula parameter ff equation update action takes place training data set epoch sweep 
hand want parameters updated immediately input output pair update formula equation referred pattern learning line learning 
derive faster hybrid learning rule learning paradigms 
hybrid learning rule batch line learning apply gradient method identify parameters adaptive network method generally slow trapped local minima 
propose hybrid learning rule combines gradient method squares estimate lse identify parameters 
simplicity assume adaptive network consideration output output set input variables set parameters 
exists function composite function ffi linear elements elements identified squares method 
formally parameter set decomposed sets phi phi represents direct sum ffi linear elements applying equation output ffi linear elements values elements plug training data equation obtain matrix equation ax unknown vector elements parameters js dimensions theta theta theta respectively 
number training data pairs usually greater number linear parameters overdetermined problem generally exact solution equation 
squares estimate lse sought minimize squared error kax gamma bk standard problem forms grounds linear regression adaptive filtering signal processing 
known formula uses pseudo inverse gamma transpose gamma pseudo inverse non singular 
equation concise notation expensive computation dealing matrix inverse ill defined singular 
result employ sequential formulas compute lse sequential method lse efficient especially small easily modified line version see systems changing characteristics 
specifically ith row vector matrix defined equation ith element calculated iteratively sequential formulas widely adopted literature gamma gamma delta delta delta gamma called covariance matrix squares estimate equal xp initial conditions bootstrap equation fli fl positive large number identity matrix dimension theta dealing multi output adaptive networks output equation column vector equation applies th rows matrix sequential squares estimate interpreted kalman filter process noise reason equation loosely referred kalman filter algorithm 
note simulations described chapters deterministic noise added simulation settings 
combine gradient method squares estimate update parameters adaptive network 
epoch hybrid learning procedure composed forward pass backward pass 
forward pass supply input data functional signals go forward calculate node output matrices equation obtained parameters identified sequential squares formulas equation 
identifying parameters functional signals keep going forward till error measure calculated 
backward pass error rates derivative error measure node output see equation propagate output input parameters updated gradient method equation 
fixed values parameters parameters guaranteed global optimum point parameter space due choice squared error measure 
hybrid learning rule decrease dimension search space gradient method general cut substantially convergence time 
take example hidden layer back propagation neural network sigmoid activation functions 
neural network output units output equation column vector 
delta inverse sigmoid function ln gamma equation linear vector function element output linear combination parameters weights thresholds pertaining layer 
words weights thresholds hidden layer weights thresholds output layer 
apply back propagation learning rule tune parameters hidden layer parameters output layer identified squares method 
keep mind squares method data transformed delta obtained parameters optimal terms transformed squared error measure original 
usually cause practical problem long delta monotonically increasing 
hybrid learning rule pattern line learning parameters updated data presentation pattern learning line learning paradigm 
learning paradigm vital line parameter identification systems changing characteristics 
modify batch learning rule line version obvious gradient descent see equation strictly speaking truly gradient search procedure minimize approximate learning rate small 
sequential squares formulas account time varying characteristics incoming data need decay effects old data pairs new data pairs available 
problem studied adaptive control system identification literature number solutions available 
simple method formulate squared error measure weighted version gives higher weighting factors data pairs 
amounts addition forgetting factor original sequential formula gamma gamma value 
smaller lambda faster effects old data decay 
small lambda causes numerical unstability avoided 
iv 
anfis adaptive network fuzzy inference system architecture learning rules adaptive networks described previous section 
functionally constraints node functions adaptive network piecewise differentiability 
structurally limitation network configuration feedforward type 
due minimal restrictions adaptive network applications immediate immense various areas 
section propose class adaptive networks functionally equivalent fuzzy inference systems 
proposed architecture referred anfis standing adaptive network fuzzy inference system 
describe decompose parameter set order apply hybrid learning rule 
demonstrate apply stone theorem anfis simplified fuzzy rules radial basis function network relate kind simplified anfis 
anfis architecture simplicity assume fuzzy inference system consideration inputs output suppose rule base contains fuzzy rules takagi sugeno type rule rule layer layer layer layer layer type fuzzy reasoning equivalent anfis type anfis 
type fuzzy reasoning illustrated corresponding equivalent anfis architecture type anfis shown 
node functions layer function family described layer node layer square node node function input node linguistic label small large associated node function 
words membership function specifies degree satisfies quantifier usually choose bell shaped maximum equal minimum equal generalized bell function gammac gaussian function exp gamma gamma fa fa case parameter set 
values parameters change bell shaped functions vary accordingly exhibiting various forms membership functions linguistic label fact continuous piecewise differentiable functions commonly trapezoidal triangular shaped membership functions qualified candidates node functions layer 
parameters layer referred premise parameters 
layer node layer circle node labeled multiplies incoming signals sends product 
instance theta node output represents firing strength rule 
fact norm operators perform generalized node function layer 
layer node layer circle node labeled th node calculates ratio th rule firing strength sum rules firing strengths type fuzzy reasoning equivalent anfis type anfis 
convenience outputs layer called called normalized firing strengths 
layer node layer square node node function output layer fp parameter set 
parameters layer referred consequent parameters 
layer single node layer circle node labeled computes output summation incoming signals output constructed adaptive network functionally equivalent type fuzzy inference system 
type fuzzy inference systems extension quite straightforward type anfis shown output rule induced jointly output membership firing strength 
type fuzzy inference systems replace centroid defuzzification operator discrete version calculates approximate centroid area type anfis constructed accordingly 
complicated type type versions worth efforts 
shows input type anfis rules 
membership functions associated input input space partitioned fuzzy subspaces governed fuzzy rules 
premise part rule defines fuzzy subspace consequent part specifies output fuzzy subspace 
hybrid learning algorithm proposed type anfis architecture observed values premise parameters output expressed linear combinations consequent parameters 
precisely output rewritten premise parameters consequent parameters input type anfis rules corresponding fuzzy subspaces linear consequent parameters 
result set total parameters set premise parameters set consequent parameters equation delta delta delta identity function function fuzzy inference system respectively 
hybrid learning algorithm developed previous chapter applied directly 
specifically forward pass hybrid learning algorithm functional signals go forward till layer consequent parameters identified squares estimate 
backward pass error rates propagate backward premise parameters updated gradient descent 
table summarizes activities pass 
forward pass backward pass premise parameters fixed gradient descent consequent parameters squares estimate fixed signals node outputs error rates table passes hybrid learning procedure anfis 
mentioned earlier consequent parameters identified optimal consequent parameter space condition premise parameters fixed 
accordingly hybrid approach faster strict gradient descent worthwhile look possibility decomposing parameter set manner equation 
type anfis achieved membership function consequent part rule replaced piecewise linear approximation consequent parameters 
case consequent parameters constitute set hybrid learning rule employed directly 
noted computation complexity squares estimate higher gradient descent 
fact methods update parameters listed computation complexities 
gradient descent parameters updated gradient descent 

gradient descent pass lse lse applied get initial values consequent parameters gradient descent takes update parameters 
output output piecewise linear approximation membership functions consequent part type anfis 

gradient descent lse proposed hybrid learning rule 

sequential approximate lse anfis linearized premise parameters extended kalman filter algorithm employed update parameters 
proposed neural network literature 
choice methods trade computation complexity resulting performance 
simulations section performed third method 
note consequent parameters updated widrow hoff lms algorithm reported 
widrow hoff algorithm requires computation favors parallel hardware implementation converges relatively slowly compared square estimate 
pointed reviewers learning mechanisms applied determination membership functions convey linguistic subjective description ill defined concepts 
think case case situation decision left users 
principle size available input output data set large fine tuning membership functions applicable necessary human determined membership functions subject differences person person time time rarely optimal terms reproducing desired outputs 
data set small probably contain information system consideration 
situation human determined membership functions represent important knowledge obtained human experts experiences reflected data set membership functions kept fixed learning process 
interestingly membership functions fixed consequent part adjusted anfis viewed functional link network enhanced representation input variables achieved membership functions 
enhanced representation takes advantage human knowledge apparently insight revealing functional expansion tensor models 
fine tuning membership functions enhanced representation adaptive 
update formulas premise consequent parameters decoupled hybrid learning rule see table speedup learning possible versions gradient method premise parameters conjugate gradient descent second order back propagation quick propagation nonlinear optimization 
fuzzy inference systems simplified fuzzy rules reasoning mechanisms introduced earlier commonly literature inherent drawbacks 
type reasoning membership functions consequence part restricted monotonically non decreasing functions compatible linguistic terms medium membership function bell shaped 
type reasoning defuzzification process time consuming systematic fine tuning parameters easy 
type reasoning just hard assign appropriate linguistic terms consequence part function input variables 
cope disadvantages simplified fuzzy rules form introduced big small crisp value 
due fact output described crisp value equivalently singular membership function class simplified fuzzy rules employ types reasoning mechanisms 
specifically consequent part simplified fuzzy rule represented step function centered type singular membership function type constant output function type respectively 
reasoning mechanisms unified simplified fuzzy rules 
simplified fuzzy rule possible prove certain circumstance resulting fuzzy inference system unlimited approximation power match nonlinear functions arbitrarily compact set 
proceed descriptive way applying stone theorem stated 
theorem domain compact space dimensions set continuous real valued functions satisfying criteria 
identity function constant 
separability points 

algebraic closure functions fg af bg real numbers dense set continuous real valued functions words ffl function function jg gamma ffl application fuzzy inference systems domain operate closed bounded compact 
second criteria trivial find simplified fuzzy inference systems satisfy 
need examine algebraic closure addition multiplication 
suppose fuzzy inference systems rules output system expressed constant output rule 
az calculated follows az af af af af form equation 
apparently anfis architectures compute az class class membership functions invariant multiplication 
loosely true class membership functions set bell shaped functions multiplication bell shaped function bell shaped 
tightly defined class membership functions satisfying criteria pointed wang scaled gaussian membership function exp gamma gamma choosing appropriate class membership functions conclude anfis simplified fuzzy rules satisfy criteria stone theorem 
consequently ffl real valued function fuzzy inference system jg gamma ffl underlying compact set 
simplified anfis proper subset types anfis input variables typical initial mf typical initial membership function setting simulation 
operating range assumed 
draw types anfis unlimited approximation power match data set 
caution taken accepting claim mention construct anfis data set 
learning plays role context 
interesting aspect simplified anfis architecture functional equivalence radial basis function network rbfn 
functional equivalence established gaussian membership function simplified anfis 
detailed treatment 
functional equivalence provides shortcut better understanding anfis rbfn advances literatures apply directly 
instance hybrid learning rule anfis apply rbfn directly vice versa approaches identify rbfn parameters clustering preprocess orthogonal squares learning generalization properties sequential adaptation applicable techniques anfis 
application examples section presents simulation results proposed type anfis batch line pattern line learning 
examples anfis model highly nonlinear functions results compared neural network approach earlier 
third example anfis identifier identify nonlinear component discrete control system 
lastly anfis predict chaotic time series compare results various statistical connectionist approaches 
practical considerations conventional fuzzy inference system number rules decided expert familiar system modeled 
simulation expert available number membership functions mf assigned input variable chosen empirically examining desired input trial error 
situation neural networks simple ways determine advance minimal number hidden nodes necessary achieve desired performance level 
number mf associated inputs fixed initial values premise parameters set way mf equally spaced operating range input variable 
satisfy ffl completeness ffl means value inputs operating range find linguistic label ffl 
manner fuzzy inference system provide smooth transition sufficient overlapping linguistic label 
attempt keep epsilon completeness learning simulation easily achieved constrained gradient method 
shows typical initial mf setting number mf operating range 
note simulation examples membership functions bell function defined equation gammac slope mf physical meanings parameters bell membership function gammac epochs error measure rule increase step size downs point rule decrease step size combinations point heuristic rules updating step size contains fitting parameters parameters physical meaning determines center corresponding membership function half width controls slopes crossover points mf value 
shows concepts 
mentioned step size equation may influence speed convergence 
observed small gradient method closely approximate gradient path convergence slow gradient calculated times 
hand large convergence initially fast algorithm oscillate optimum 
observations update heuristic rules see 
error measure undergoes consecutive reductions increase 

error measure undergoes consecutive combinations increase reduction decrease 
numbers chosen arbitrarily results shown simulation appear satisfactory 
furthermore due dynamical update strategy initial value usually critical long big 
simulation results example modeling input nonlinear function example consider anfis model nonlinear sinc equation sinc sin theta sin epochs root mean squared error quick prop neural network anfis rmse curves quick propagation neural networks anfis 
grid points range gamma theta gamma input space equation training data pairs obtained 
anfis contains rules membership functions assigned input variable total number fitting parameters composed premise parameters consequent parameters 
tried anfis rules rules obviously simple describe highly nonlinear sinc function 
shows rmse root mean squared error curves neural network anfis 
curve average runs neural network runs started different set initial random weights anfis different initial step size 
neural network containing fitting parameters connection weights thresholds trained quick propagation considered best learning algorithms connectionist models 
demonstrate effectively model highly nonlinear surface compared neural networks 
comparison taken universal attempt exhaustive search find optimal settings quick propagation learning rule neural networks 
training data reconstructed surfaces different epoch numbers shown 
error measure computed forward pass epoch numbers shown 
note reconstructed surface epoch due identification consequent parameters looks similar training data surface 
lists initial final membership functions 
interesting observe sharp changes training data surface origin accounted moving membership functions origin 
theoretically final mf symmetric respect origin 
symmetric due computer truncation errors approximate initial conditions bootstrapping calculation sequential squares estimate 
example modeling input nonlinear function training data example obtained output gamma gamma takagi sugeno kondo verify approaches 
anfis see contains rules membership functions assigned input variable 
training data checking data sampled uniformly input ranges theta theta theta theta respectively :10.1.1.125.6421
training data training anfis training data upper left reconstructed surfaces upper right lower left lower right epochs 
example 
initial mf initial mf final mf final mf initial final membership functions example 
output predicted anfis architecture example 
connections inputs layer shown 
initial mf final mf final mf final mf example membership functions learning membership functions learning 
checking data verifying identified anfis 
allow comparison performance index adopted ape average percentage error gamma number data pairs th desired output calculated output respectively 
illustrates membership functions training 
training error curves different initial step sizes shown demonstrates initial step size critical final performance long big 
training checking error curves initial step size equal 
epochs final results ape trn ape chk listed table earlier 
simulation cited performed different assumptions different training checking data sets conclusive comments 
example line identification control systems epochs average percentage error average percentage error epochs error curves example training error curves initial step size solid line training solid line checking dashed line error curves initial step size equal 
model ape trn ape chk parameter 
training set size checking set size anfis model fuzzy model fuzzy model table example comparisons earlier 
rows 
repeat simulation example neural network employed identify nonlinear component control system anfis replace neural network 
plant consideration governed difference equation gamma output input respectively time index unknown function delta form sin sin sin order identify plant series parallel model governed difference equation gamma delta function implemented anfis parameters updated time index 
anfis membership functions input rules fitting parameters pattern line learning paradigm adopted learning rate forgetting factor 
input plant model sinusoid sin adaptation started stopped 
shown output model follows output plant immediately adaptation stopped changed sin sin 
comparison neural network fails follow plant adaptation stopped identification procedure continue time steps random input 
table summarizes comparison 
mf number determined trial errors 
mf number model output follow plant output satisfactorily adaptations 
decrease parameter numbers batch learning supposed effective 
show results epochs batch learning mf numbers respectively 
seen anfis model mf small 
mf number getting smaller correlation rule output getting obvious sense harder sketch time index time index plant output model output time index example plant output model output 
method parameter number time steps adaptation nn anfis table example comparison nn identifier 
initial mf final mf rule output example batch learning mf 
rule consequent part 
words parameter number reduced mildly usually anfis job cost sacrificing semantics terms local description nature fuzzy rules structured knowledge representation black box model neural networks 
example predicting chaotic dynamics example show anfis model highly nonlinear functions effectively 
example demonstrate proposed anfis employed predict values chaotic time series 
performance obtained example compared results cascade correlation neural network approach reported simple conventional statistical approach auto regressive ar model 
time series simulation generated chaotic mackey glass differential delay equation defined gamma gamma gamma prediction values time series benchmark problem considered number connectionist researchers lapedes farber moody jones sanger 
goal task known values time series point predict value point standard method type prediction create mapping points time series spaced apart gamma gamma gamma predicted value 
allow comparison earlier lapedes farber moody values 
simulation settings example arranged close possible reported 
obtain time series value integer point applied fourth order runge kutta method find numerical solution equation 
time step method initial condition derived 
assume integration 
mackey glass time series extracted input output data pairs format gamma gamma gamma initial mf final mf rule output example batch learning mf 
initial mf final mf rule output example batch learning mf 
initial mf input variables 
input second input third input fourth input final mf input variable 
membership functions example learning learning 

pairs training data set training anfis remaining pairs checking data set validating identified model 
number membership functions assigned input anfis arbitrarily set rule number 
initial membership functions input variable 
anfis contains total fitting parameters premise parameters consequent parameters epochs rmse trn rmse chk better compared approaches explained 
resulting fuzzy rules listed appendix 
desired predicted values training data checking data essentially differences seen finer scale 
final membership functions shows rmse curves indicate learning done epochs 
quite unusual observe phenomenon rmse trn rmse chk training process 
considering rmse vary small conclude anfis captured essential components underlying dynamics training data contains effects initial conditions remember set integration easily accounted essential components identified anfis 
comparison performed prediction auto regressive ar model number parameters gamma gamma time mackey glass time series time prediction errors example mackey glass time series step ahead prediction indistinguishable time series prediction error 
fitting parameters 
extracted data pairs identify remaining checking 
results obtained standard squares estimate rmse trn rmse chk worse anfis 
shows predicted values prediction errors 
obviously ar model causes fitting training data large errors checking data 
search best ar model terms generalization capability tried different ar models parameter number varied shows results ar model best generalization capability obtained parameter number 
best ar model repeat generalization test shows results fitting price larger training errors 
goes saying nonlinear anfis outperforms linear ar model 
noted identification ar model took seconds anfis simulation took hours hp apollo series workstation 
pay special attention optimization codes 
table lists methods generalization capabilities measured method predict points immediately training set 
non dimensional error index defined root mean square error divided standard deviation target series 
note average relative variance equal square 
remarkable generalization capability anfis believe comes facts 
anfis achieve highly nonlinear mapping shown example superior common linear methods reproducing nonlinear time series 

anfis adjustable parameters cascade correlation nn median size back prop nn listed table 
priori knowledge initial parameter settings anfis intuitively reasonable leads fast learning captures underlying dynamics 
table lists results challenging generalization test rows rows 
results rows obtained iterating prediction till 
anfis outperforms statistical connectionist approaches substantially large epoch number root mean squares error training checking rmse curves anfis modeling 
time sec 
desired solid line predicted dashed line mg time series time sec 
prediction errors mackey glass time series solid line step ahead prediction dashed line ar model parameter prediction errors 
parameter number training solid line checking dashed line training solid line checking dashed line errors ar models different parameter numbers 
time sec 
desired solid line predicted dashed line mg time series time sec 
prediction errors example mackey glass time series solid line step ahead prediction dashed line best ar model parameter number prediction errors 
method training cases non dimensional error index anfis ar model cascaded correlation nn back prop nn th order polynomial linear predictive method table generalization result comparisons 
rows 
method training cases non dimensional error index anfis ar model cascaded correlation nn back prop nn th order polynomial linear predictive method table generalization result comparisons rows rows 
results methods generated iterating solution 
results localized receptive fields multi resolution hierarchies networks trained 
rows 
amount training data row table 
illustrates generalization test anfis points desired outputs predicted outputs 
vi 
concluding remarks summary extensions current described architecture adaptive network fuzzy inference systems anfis type type reasoning mechanisms 
employing hybrid learning procedure proposed architecture refine fuzzy rules obtained human experts describe input output behavior complex system 
human expertise available set intuitively reasonable initial membership functions start learning process generate set fuzzy rules approximate desired data set shown simulation examples nonlinear function modeling chaotic time series prediction 
due high flexibility adaptive networks anfis number variants proposed 
instance membership functions changed representation asymmetric 
furthermore replace nodes layer parameterized norm learning rule decide best norm operator specific application 
employing adaptive network common framework proposed adaptive fuzzy models tailored data classification feature extraction purposes 
important issue training anfis preserve human plausible features membership functions ffl completeness sufficient overlapping adjacent membership functions minimal uncertainty pursue direction achieved maintaining certain constraints modifying original error measure explained 
ffl keep bell shaped membership functions need membership functions bell shaped regardless parameter values 
particular equation side bell shaped easy way correct replace equations 
ffl ffl completeness maintained constrained gradient descent 
instance suppose ffl adjacent membership functions form equation parameter sets fa fa ffl completeness satisfied gamma ensured training constrained gradient descent employed 
ffl minimal uncertainty refers situation region input space dominant fuzzy rule account final output multiple rules similar firing time desired solid predicted dashed time series anfis time prediction errors generalization test anfis 
strengths 
minimizes uncertainty rule set informative 
way modified error measure fi gammaw theta ln original squared error fi weighting constant size training data set normalized firing strength th rule see equation gammaw theta ln information entropy 
modified error measure data fitting anfis trained potentially better generalization capability 
due new error measure training gradient descent 
improvement generalization error measure data fitting weight elimination reported neural network literature 
assume structure anfis fixed parameter identification solved hybrid learning rule 
approach complete structure identification concerns selection appropriate input space partition style number membership functions input equally important successful applications anfis 
effective partition input space decrease rule number increase speed learning application phases 
advances neural networks structure identification shed lights aspect :10.1.1.125.6421
applications automatic control signal processing fuzzy control far successful applications fuzzy set theory fuzzy inference systems 
due adaptive capability anfis applications adaptive control learning control immediate 
replace neural networks control systems serve purposes 
instance narendra pioneering neural networks adaptive control achieved similarly anfis 
generic designs supervised control direct inverse control neural adaptive control back propagation utility neural networks control proposed werbos directly applicable schemes anfis 
particularly employed similar method back propagation time unfolding time achieve self learning fuzzy controller rules balance inverted pendulum near optimal manner 
expected advances neural network techniques control promote anfis vice versa 
active role neural networks signal processing suggests similar applications anfis 
nonlinearity structured knowledge representation anfis primary advantages classical linear approaches adaptive filtering adaptive signal processing identification inverse modeling predictive coding adaptive channel equalization adaptive interference noise echo canceling author wish anonymous reviewers valuable comments 
guidance help professor zadeh members fuzzy group uc berkeley gratefully acknowledged 
appendix suggested reviewers give readers concrete idea resulting fuzzy inference systems better list fuzzy rules explicitly 
list final fuzzy rules example predicts mackey glass chaotic time series 
suppose th input variable assigned linguistic values small large fuzzy rules training expressed gamma small gamma small gamma small small delta gamma small gamma small gamma small large delta gamma small gamma small gamma large small delta gamma small gamma small gamma large large delta gamma small gamma large gamma small small delta gamma small gamma large gamma small large delta gamma small gamma large gamma large small delta gamma small gamma large gamma large large delta gamma large gamma small gamma small small delta gamma large gamma small gamma small large delta gamma large gamma small gamma large small delta gamma large gamma small gamma large large delta gamma large gamma large gamma small small delta gamma large gamma large gamma small large delta gamma large gamma large gamma large small delta gamma large gamma large gamma large large delta gamma gamma gamma th row consequent parameter matrix gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma linguistic labels small large defined bell membership function different parameters gammac membership functions shown 
table lists linguistic labels corresponding consequent parameters equation astrom wittenmark 
computer controller systems theory design 
prentice hall 
atkeson 
generalization properties radial basis functions 
touretzky editor advances neural information processing systems iii pages 
morgan kaufmann publishers san mateo ca 
chen cowan 
orthogonal squares learning algorithm radial basis function networks 
ieee trans 
neural networks march 
dubois prade 
fuzzy sets systems theory applications 
academic press new york 
fahlman 
faster learning variations back propagation empirical study 
touretzky hinton sejnowski editors proc 
connectionist models summer school pages mellon university 
fahlman lebiere 
cascade correlation learning architecture 
touretzky hinton sejnowski editors advances neural information processing systems ii 
morgan kaufmann 
goodwin sin 
adaptive filtering prediction control 
prentice hall englewood cliffs 
haykin 
adaptive filter theory 
prentice hall englewood cliffs nj nd edition 
miller iii sutton werbos editors 
neural networks control 
massachusetts institute technology 

roger jang 
fuzzy modeling generalized neural networks kalman filter algorithm 
proc 
ninth national conference artificial intelligence aaai pages july 

roger jang 
rule extraction generalized neural networks 
proc 
th world congress pages volume artificial intelligence july 

roger jang 
self learning fuzzy controller temporal back propagation 
ieee trans 
neural networks september 

roger jang 
structure determination fuzzy modeling fuzzy cart approach 
proc 
ieee international conference fuzzy systems orlando june 
submitted 

roger jang 
sun 
functional equivalence radial basis function networks fuzzy inference systems 
ieee trans 
neural networks january 
small large small large small large small large table table premise parameters example 
jones lee barnes flake lee lewis 
function approximation time series prediction neural networks 
proc 
ieee international joint conference neural networks pages 
niranjan fallside 
sequential adaptation radial basis function neural networks 
touretzky editor advances neural information processing systems iii pages 
morgan kaufmann publishers san mateo ca 
kalman 
new approach linear filtering prediction problems 
journal basic engineering pages march 
kandel 
fuzzy expert systems 
addison wesley 
kandel editor 
fuzzy expert systems 
crc press boca raton fl 
kantorovich 
functional analysis 
pergamon oxford nd edition 
klassen 
pao 
characteristics functional link net higher order delta rule net 
ieee proc 
international conference neural networks san diego june 
kondo 
revised algorithm estimating degree complete polynomial 
tran 
society instrument control engineers 
japanese 
kosko 
neural networks signal processing 
prentice hall englewood nj 
lapedes farber 
nonlinear signal processing neural networks prediction system modeling 
technical report la ur los alamos national laboratory los alamos new mexico 

lee 
fuzzy logic control systems fuzzy logic controller part 
ieee trans 
systems man cybernetics 

lee 
fuzzy logic control systems fuzzy logic controller part 
ieee trans 
systems man cybernetics 

lee 
structure level adaptation artificial neural networks 
kluwer academic publishers 
ljung 
system identification theory user 
prentice hall englewood cliffs 
mackey glass 
oscillation chaos physiological control systems 
science july 
moody 
fast learning multi resolution hierarchies 
touretzky editor advances neural information processing systems chapter pages 
morgan kaufmann san mateo ca 
moody darken 
learning localized receptive fields 
touretzky hinton sejnowski editors proc 
connectionist models summer school 
carnegie morgan kaufmann publishers 
moody darken 
fast learning networks locally tuned processing units 
neural computation 
ahmed chan 
training radial basis function classifiers 
neural networks 
narendra 
identification control dynamical systems neural networks 
ieee trans 
neural networks 
nguyen widrow 
neural networks self learning control systems 
ieee control systems magazine pages april 

pao 
adaptive pattern recognition neural networks chapter pages 
addison wesley publishing 
parker 
optimal algorithms adaptive networks second order back propagation second order direct propagation second order hebbian learning 
proc 
ieee international conference neural networks pages 

fuzzy control fuzzy systems 
wiley new york 
iii crowder predicting mackey glass timeseries cascade correlation learning 
touretzky hinton sejnowski editors proc 
summer school pages mellon university 

real analysis 
macmillan new york nd edition 
rumelhart hinton williams 
learning internal representations error propagation 
rumelhart james mcclelland editors parallel distributed processing explorations microstructure cognition chapter pages 
mit press 
sanger 
tree structured adaptive network function approximate high dimensional spaces 
ieee trans 
neural networks march 
shah datum 
optimal filtering algorithms fast learning feedforward neural networks 
neural networks 
shar 
fast local algorithm training feedforward neural networks 
proc 
international joint conference neural networks pages iii 
singhal wu 
training multilayer perceptrons extended kalman algorithm 
david touretzky editor advances neural information processing systems pages 
morgan kaufmann publishers 
smith comer 
automated calibration fuzzy logic controller cell state space algorithm 
ieee control systems magazine august 

linear prediction theory mathematical basis adaptive systems 
springer verlag 
sugeno editor 
industrial applications fuzzy control 
elsevier science pub 

sugeno kang 
structure identification fuzzy model 
fuzzy sets systems 
sun 
roger jang 
adaptive network fuzzy classification 
proc 
japan symposium flexible automation july 

sun 
roger jang 
fu 
neural network analysis plasma spectra 
proc 
international conference artificial neural networks amsterdam september 
takagi hayashi 
nn driven fuzzy reasoning 
international journal approximate reasoning 
takagi sugeno 
derivation fuzzy control rules human operator control actions 
proc 
ifac symp 
fuzzy information knowledge representation decision analysis pages july 
takagi sugeno 
fuzzy identification systems applications modeling control 
ieee trans 
systems man cybernetics 

approach fuzzy reasoning method 
madan gupta ronald yager editors advances fuzzy set theory applications pages 
north holland amsterdam 

wang 
fuzzy systems universal approximators 
proc 
ieee international conference fuzzy systems san diego march 

wang mendel 
fuzzy basis function universal approximation orthogonal squares learning 
ieee trans 
neural networks september 
watrous 
learning algorithms connectionist network applied gradient methods nonlinear optimization 
proc 
ieee international conference neural networks pages 
weigend rumelhart huberman 
back propagation weight elimination time series prediction 
touretzky hinton sejnowski editors proc 
connectionist models summer school pages mellon university 
weigend rumelhart huberman 
generalization weight application forecasting 
touretzky editor advances neural information processing systems iii pages 
morgan kaufmann san mateo ca 
werbos 
regression new tools prediction analysis behavioral sciences 
phd thesis harvard university 
werbos 
overview neural networks control 
ieee control systems magazine january 
widrow stearns 
adaptive signal processing 
prentice hall englewood cliffs 
widrow winter 
neural nets adaptive filtering adaptive pattern recognition 
ieee computer pages march 

nonlinear optimization problem solving approach chapter pages 
north holland publishing 
zadeh 
fuzzy sets 
information control 
zadeh 
outline new approach analysis complex systems decision processes 
ieee trans 
systems man cybernetics january 

