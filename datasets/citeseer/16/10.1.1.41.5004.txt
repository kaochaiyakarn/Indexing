international conference parallel processing volume ii pages tiling iteration spaces multicomputers ramanujam sadayappan department computer information science ohio state university columbus ohio deal compiler support parallelizing perfectly nested loops coarse grain distributed memory machines 
relatively high communication start costs machines renders frequent communication expensive 
study effect clustering communication ensuing loss parallelism performance propose method aggregating number loop iterations tiles tiles execute atomically synchronizations performed execution tile 
result important dividing loops tiles lead deadlock 
conditions deadlock free tiles method deriving legal tiles nested loops 
develop approach optimize shape size tiles assignment tiles processors load balanced execution reduced communication costs distributed memory machines communication setup transfer rates instruction execution rates 
distributed memory machines multicomputers offer high levels flexibility scalability performance leave programmer algorithm designer difficult task detailed planning computation orchestration entire parallel execution 
machines hypercubes programmer forced manually distribute code data addition managing communication tasks explicitly 
task addition error prone time consuming generally leads non portable code 
parallelizing compilers multicomputers received great deal attention 
deal compiler support parallelizing programs coarse grain distributed memory machines consider class programs expressible perfectly nested loops regular dependence structure 
relatively high communication start costs machines renders frequent communication expensive 
study effect clustering communication ensuing loss parallelism performance propose method aggregating number loop iterations tiles tiles execute atomically processor executing iteration belonging tile receives data needs executing iterations tile executes iterations tile sends data needed processors 
communication execution tile important dividing loops tiles lead deadlock 
computation expressed nested loops executed multicomputer execution communication costs tile shape size chosen optimize performance 
addition assignment tiles processors inter processor communication minimized computation time 
develop approach determine shape size tiles execution nested loops distributed memory machines 
section discuss data dependence analysis iteration space graphs corresponding nested loop programs hyperplane technique identifying parallel threads computation 
section develops approach iteration space clustering tiles discusses conditions validity tiles 
section discusses choice tile shape 
section discusses choice tile size allocation tiles processors distributed memory machine discuss effect tile size shape performance tiling nested loop execution shared memory machines loop execution involves synchronization different iterations tile size chosen reduce synchronization task collection iterations initiation overhead 
section compares loop blocking tiling hierarchical memory multi vector processors compiling programs multicomputers 
section summarizes concludes discussion progress 
data dependence parallelism automatic detection parallelism involves finding sets computations performed simultaneously 
potential parallelism computation determined data dependences 
presence dependence computations statements implies executed parallel 
dependences imply precedence constraints computations satisfied correct execution 
algorithms exhibit regular data dependences certain dependence patterns occur repeatedly duration computation 
international conference parallel processing volume ii pages data dependences consider loop structure form delta delta delta un delta delta delta sn integer valued linear expressions involving gamma 
sn assignment statements form xo xk xo output variable produced expression evaluated input variables xk input variables statement produced statements statement earlier execution 
nested loops define iteration space comprises finite discrete cartesian space dimensionality loop nesting level 
iteration associate integer vector index index set iteration space delta delta delta ung loop executed sequentially elements lexicographical ordering 
induced sequential ordering non essential exposing exploiting parallelism modifying ordering reduce parallel execution time preserving semantics loop 
flow dependence occurs value assigned variable array element execution instance statement read subsequent execution instance statement 
anti dependence occurs value read variable array element execution instance statement reassigned subsequent execution instance statement 
output dependence occurs value assigned variable array element execution instance statement reassigned subsequent execution instance statement 
nested loops instance particular statement index iteration associated statement denoted 
data dependence relations statements nested loops augmented indices related iterations 
dependences constant distance dimension iteration space 
suppose dependence vector gamma called iteration space distance vector simply distance vector 
dependence relations represented iteration space graphs 
directed edge exists iteration defined iteration defined dependence exists statements loop constituting iterations algorithm may number dependence vectors 
general dependence vectors functions index point dependence vectors algorithm written collectively dependence matrix dm 
addition types dependence type dependence known control dependence 
control dependence exists statement conditional jump statement conditional jump statement controls execution statement 
data dependences flow dependence inherent computation forms dependence arise due storage imperative programming model 
analysis consider distributed memory machines considering flow dependences sufficient 
data dependences constrain order execution statements program 
methods calculate data dependence vectors 
consider program loops exhibit uniform regular dependence structure index point dependences specify dependences exhibited algorithm needs specify dependence index point 
dependence distances constant functions indices 
computations variety systolic array synthesis techniques exist analyze dependence structure linear transformations affine distances linear function indices 
hyperplane parallelism section reiterate fundamental result due karp dealing parallel execution computations expressed uniform recurrence equations lamport dealing parallel execution nested loops fortran result moldovan miranker wolfe 
lamport considered partitioning indices lie family parallel hyperplanes indices lying hyperplane executed simultaneously 
vector tn 
delta delta delta various values defines family hyperplanes 
terms hyperplane family hyperplanes vector defining interchangeably context clear 
case lamport result theorem tn hyperplane dependence vector din indices satisfy hi hi independent executed simultaneously 
exists computation defined nested loop involves deadlocks 
event communication costs negligible systolic arrays optimal international conference parallel processing volume ii pages hyperplane minimizes number hyperplanes required partition indices gives optimal time parallel implementation algorithm mimd machines 
execution time parallel algorithm max gamma min normal hyperplane defines direction sequentiality 
case dimensional index spaces hyperplane referred wavefront 
communication extremely regular fast systolic arrays leaving exploitation parallelism designer 
case multicomputers communication data access important issue case shared memory machines fast local memory access data stored local memory order magnitude faster access global memory 
case distributed memory message passing machines intel ipsc message start times larger message start time intel ipsc packet byte transmission cost access local memory takes negligible time 
exploiting spatial locality important trade degree parallelism communication data access cost decides schedule techniques developed systolic arrays directly applicable context multiprocessors 
section outline scheme tiling iteration spaces nested loops 
iteration space tiling basic idea tiling aggregate number iterations loop reduce communication overheads exploiting available parallelism 
tile defines atomic unit computation comprised iterations 
necessary data available execution tile data needed execution tiles available execution tile complete 
synchronization communication necessary execution tile 
imposes constraint division iteration space tiles result deadlock 
intuitively means dependence vectors cross tile boundary tiles direction 
division tiles compatible dependences nodes isg 
keep code generation simple necessary tiles identical shape size near boundaries iteration space 
discuss relaxation uniform size constraint account parallelism profile execution tiles 
conditions tiling problem choose tile shape size minimizes execution time distributed memory machine specified communication set transfer costs instruction execution rate 
legal tiles tiles dimensional spaces defined families parallel hyperplanes planes dimensional hyperplane 
tiles defined near boundary iteration space tile subset iteration space 
shape tiles defined families planes size tiles defined distance separation adjacent pairs parallel planes families 
consider example example gamma gamma gamma gamma shows iteration space graph defined loop 
distance vectors gamma 
tiles isg defined families lines 
note dimensional plane line 
shows tiles isg theta collections iterations 
case arbitrary dependence graphs clustering tasks atomic collections lead deadlock 
sarkar formulated condition absence deadlock case convexity constraint 
irigoin conditions iteration space graphs 
tiles iteration spaces legal dependence vectors crossing boundary pair tiles cross tile source dependence vectors tile sink 
ensuing discussion assume dependence matrix nested loop rank number dependence vectors vector hi defines family hyperplanes dimensional space delta delta delta hi xk various values dimensions vectors hk define legal tiles delta set dependence vectors 
define equivalent condition delta zero vector space 
dimensional planes boundary normal condition 
result sufficient condition 
give example condition violated tiles certain fixed size lead deadlock 
consider example international conference parallel processing volume ii pages example gamma gamma gamma gamma shows isg tile boundaries spaced apart 
dependence vectors gamma cross boundary opposite directions deadlock head vector gamma tile tail vector 
tiles small avoid deadlocks 
henceforth assume tiles larger magnitude dependence vectors refer free size tiles fsts 
extreme vectors results theory integer numbers integer programming set distinct dependence vectors find set vectors say ek dependence vector expressed non negative linear combination vectors ek 
set vectors ek referred extreme vectors 
ej vector vectors solving system linear inequalities delta dimensional plane containing extreme vectors 
extreme vectors exactly planes generated 
system theta inequalities variables 
case distributed memory machines dependence vectors cross tile boundary imply potential inter processor communication reduced set extreme vectors subset dependence vectors check set linearly independent vectors inequalities hold normal dimensional plane containing vectors set 
gamma delta sets checked 
choice tile shape shape tiles defined collection extreme vectors extreme vectors ek delta measure communication incurred due dependence vectors crossing tile boundaries 
set valid collections extreme vectors choose collection minimizes communication cost 
focus selecting shape tiles motivated legality condition communication reduction irigoin discuss partitioning hyperplanes legal tiles facilitate vectorized execution iterations single tile 
say dimensional tiles induced collection linearly independent vectors 
point view communication reduction including dependence vectors possible extreme vector set 
show result theorem tiles dimensional subsets induced collection vectors non extreme dependence vector illegal tiles 
proof set distinct dependence vectors dm corresponding extreme vectors ek 
assume dependence matrix rank loss generality consider tiles induced dependence vector dn extreme vectors ek omitted 
note dn collinear dn extreme vectors 
need consider cases case coefficient cn dn case rank matrix consisting vectors ek dn 
derive tiles 
case coefficient consider tile boundary formed vectors gamma ek dn 
vector normal tile boundary consideration 
dn lies boundary delta dn 
equivalently delta gamma ek lie plane delta positive delta delta opposite signs results illegal tiles 
result 
international conference parallel processing volume ii pages condition sufficient condition generation free size tiles fsts tiles legal independent size 
consider loop example 
show tiles induced vectors 
tiles induced illegal case extreme vector non extreme vector 
example legal tiles loop shown 
scheduling tiles tiles defined section dimensional iteration space 
dependence vectors isg define tile space dependence graph tsg indicates dependences precedence constraints tiles 
assume tile size larger magnitude dependence vector tile size dimension larger largest corresponding components dependence vector 
assumption leads ffl source sink dependence vector crosses tile boundary neighboring tiles ffl legal tiles satisfy condition equation 
interested choosing size tile optimize execution time interested free size tiles fsts 
case dependence vectors tile space dependence graph tsg tuples component lexicographic sequential order execution loop valid 
gamma tiles tile depend 
scheduling tiles scheduling hyperplane satisfy conditions theorem lamport result 
scheduling hyperplane dimensional vector components equal satisfies said conditions 
discuss scheduling allocation selection tile sizes context dimensional iteration spaces 
results generalize higher dimensional iteration spaces 
space regular isg free size tile fst depend neighbors satisfy exactly conditions 
tile depends neighbors 

tiles independent focus condition 
implies dependences possible tile space tiles executed atomically automatically covered 
tiles tile space dependence graph tsg uniform dependence vectors tiles scheduled wavefront technique 
applying technique get result defines valid schedule tile space tile space vectors defines optimal scheduling tiles 
computations tile executed order defined schedule isg 
allocation tiles processors general may tiles number processors 
tiles allocated inter processor communication minimized computation load balanced time 
tile space dependences choosing allocations tiles direction communication direction 
addition valid scheduling hyperplane tiles tile row column executed simultaneously 
wrapped row column distribution communication dimension provides balance 
interplay load balance communication depends tile size discussed greater detail section 
choice tile size discuss choice tile size context iteration spaces 
tiles defined parallelograms sides simplicity assume computation defined isg length width number processors iteration space divided strips thetal 
choice improves load balance 
assume computations strip completed proceeding strip 
facilitates dealing degree parallelism varies iteration space 
set cost packet packet size cost data transmission 
costs normalized respect cost executing single iteration 
simplicity 
assume number data items may needed iteration point 
cost sending cl values processor theta theta theta theta nodes executed tile cost execution tile theta theta theta theta optimal tile size minimizes cost 
order derive expression optimal cost solving zeros derivative cost respect ignore ceiling function write cost cost execution computation wavefront schedule tiles space pl theta theta theta gamma get optimal tile size take derivative cost respect find zeros 
case get value optimal theta theta gamma international conference parallel processing volume ii pages tiling planes induce tile space dependence graph identified hyperplane dimensional vector components equal valid scheduling hyperplane 
case dependence matrices nested loops rank optimal schedule unlimited processors available loop execution 
related related compiling complex memory machines caltech working interactive parallelization tools multicomputers provide user feedback interplay data decomposition task partitioning performance programs 
chen describes crystal functional language addresses issue programmability performance parallel supercomputers 
gallivan discuss problems associated automatically restructuring data moved local memories case shared memory machines complex memory hierarchies 
series theorems enable describe structure disjoint sub lattices accessed different processors information correct copies data local memories write data back shared address space modifications complete 
gannon discuss program transformations effective complex memory management cedar architecture level memory 
koelbel address problem automatic process partitioning programs written functional language called blaze user specified data partition 
group led kennedy rice university studying similar techniques compiling version fortran intel ipsc includes annotations specifying data decomposition show existing transformations improve performance 
rogers pingali method sequential program data partition performs task partitions enhance locality 
zima discuss superb interactive system semi automatic transformation fortran programs parallel programs machine loosely coupled hierarchical multiprocessor 
mace proves problem finding optimal data storage patterns parallel processing shapes problem np complete limited dimensional arrays addition efficient algorithms derived shapes problem programs modeled directed acyclic graph dag derived series parallel combinations tree subgraphs 
related tiling context hierarchical shared memory systems irigoin method divides iteration space nodes goals vector computation node data re partition parallelism partitions 
king ni discuss grouping iterations execution multicomputers number conditions valid tile formation case dimensional iteration spaces 
wolfe discusses technique referred iteration space tiling divides iteration space loop computations tiles blocks size shape traversing tiles results covering space 
optimal tiling memory hierarchy find tiles data tile fit highest level memory hierarchy exhibit high data reuse reducing total memory traffic 
addition context loop unrolling partitioning iteration space graphs discussed nicolau callahan 
summary progress dealt compiler support parallelizing programs coarse grain distributed memory machines considered class programs expressible tightly nested loops regular dependence structure 
relatively high communication start costs machines renders frequent communication expensive 
studied effect clustering communication ensuing loss parallelism performance propose method aggregating number loop iterations tiles tiles execute atomically 
execution tiles atomic important dividing iteration spaces loops tiles lead deadlock 
approach determining legal tiles choosing shape size tiles efficient execution nested loops multicomputers 
progress characterize set legal tiles 
parallelism profile execution tiles conceivable increasing tile size dimension uniformly decreasing lead efficient execution 
progress estimating effect varying tile size processor allocation performance parallel execution tiles 
addition investigating impact tiling nested loop execution shared memory machines decrease synchronization task collection iterations initiation overhead 
allen kennedy automatic translation fortran programs vector form acm trans 
programming languages systems vol 
pp 

theorem minkowski polyhedral monoids aggregated linear diophantine systems optimization operations research international conference parallel processing volume ii pages lecture notes economics mathematical systems vol 
pp 
springer verlag 
personal communication 
banerjee dependence analysis supercomputing kluwer academic publishers boston ma 
callahan cocke kennedy estimating interlock improving balance pipelined architectures journal parallel distributed computing vol 
aug pp 

callahan kennedy compiling programs distributed memory multiprocessors journal supercomputing vol 
oct pp 

chen design methodology synthesizing parallel algorithms architectures journal parallel distributed computing vol 
dec pp 

chen choo li compiling parallel programs optimizing performance journal supercomputing pp 

fortes moldovan parallelism detection transformation techniques useful vlsi algorithms journal parallel distributed computing vol 
pp 

gallivan jalby gannon problem optimizing data transfers complex memory systems proc 
acm international conference supercomputing st malo france pp 

gannon jalby gallivan strategies cache local memory management global program transformations journal parallel distributed computing vol 
october pp 

irigoin supernode partitioning proc 
th annual acm symp 
principles programming languages san diego ca jan 
karp miller winograd organization computations uniform recurrence equations journal acm vol 
jul pp 

king ni grouping nested loops parallel execution multicomputers proc 
international conf 
parallel processing vol 
pp 
ii ii 
koelbel mehrotra van semiautomatic process partitioning parallel computation international journal parallel programming vol 
pp 

koelbel mehrotra semi automatic process decomposition non shared memory machines proc 
international conference supercomputing may 
kremer bast zima advanced tools techniques automatic parallelization parallel computing vol 
pp 

kuck kuhn padua wolfe dependence graphs compiler optimizations proc 
acm th annual symposium programming languages williamsburg va jan pp 

lamport parallel execution loops communications acm vol 
feb pp 

mace memory storage patterns parallel processing kluwer academic publishers boston ma 
miranker winkler spacetime representation computational structures computing pp 

nicolau loop quantization generalized loop unwinding technique journal parallel distributed computing vol 
oct pp 

padua kuck lawrie high speed multiprocessors compilation techniques ieee trans 
computers vol 
sep pp 

padua wolfe advanced compiler optimizations supercomputers communications acm vol 
dec pp 

rogers pingali process decomposition locality proc 
acm sigplan conference programming language design implementation portland jun pp 

ramanujam sadayappan methodology parallelizing programs multicomputers complex memory multiprocessors proc 
supercomputing reno nv nov pp 

sarkar partitioning programs macro dataflow proc 
acm conf 
lisp functional programming august pp 

schrijver theory linear integer programming wiley interscience series discrete mathematics optimization john wiley sons new york 
wolfe loop skewing wavefront method revisited international journal parallel programming vol 
pp 

wolfe iteration space tiling memory hierarchies parallel processing scientific computing ed 
siam philadelphia pa pp 

international conference parallel processing volume ii pages wolfe optimizing supercompilers supercomputers pitman publishing london mit press 
wolfe iteration space tiling proc 
supercomputing reno nv nov pp 

wolfe banerjee data dependence application parallel processing international journal parallel programming vol 
pp 

zima bast superb tool semi automatic mimd simd parallelization parallel computing vol 
pp 

