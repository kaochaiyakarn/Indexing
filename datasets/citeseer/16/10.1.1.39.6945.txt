cache profiling spec benchmarks case study alvin lebeck david wood computer sciences department university wisconsin madison west dayton street madison wi 
cs wisc edu appear ieee computer june vlsi technology improvements continue widen gap processor main memory cycle times cache performance increasingly important system performance 
cache memories help alleviate cycle time disparity programs exhibit sufficient spatial temporal locality 
programs access patterns spend time transferring data cache 
fully exploit performance potential fast processors programmers explicitly consider cache behavior restructuring codes increase locality 
fast processors proliferate techniques improving cache performance move supercomputer multiprocessor communities mainstream computing 
examine techniques programmers improve cache performance 
show cprof cache profiler identify cache performance bottlenecks gain insight origin 
insight helps programmers understand known program transformations improve cache performance 
cprof cookbook simple transformations show tune cache performance spec benchmarks 
restructuring source code greatly improve cache behavior achieve execution time speedups ranging 
keywords performance cache memories cache profile program behavior performance tuning cache memories help bridge cycle time gap fast microprocessors relatively slow main memories 
holding referenced regions memory caches reduce number cycles processor stall waiting data 
disparity processor main memory cycle times increases year cache performance critical 
unfortunately caches programs exhibit sufficient locality 
programs patterns caches exploit spend execution time transferring data main memory cache 
example spec benchmark tomcatv spends time waiting memory decstation 
fortunately programs small changes source code radically alter memory pattern greatly improving cache performance 
consider known example traversing dimensional fortran array 
fortran lays dimensional arrays column major order consecutive elements column stored consecutive memory locations 
traversing columns inner loop incrementing row index produces sequential pattern spatial locality caches exploit 
inner loop traverses rows inner loop iteration different region memory 
xa xa row major traversal xa xa column major traversal arrays larger cache column traversing version better cache behavior row traversing version 
decstation column traversing version runs times faster row traversing version array single precision floating point numbers 
call type analysis mental simulation cache behavior 
mentally applying program pattern underlying cache organization predict program cache performance 
mental simulation similar asymptotic analysis algorithms worst case behavior programmers commonly study number operations executed function input size 
analyzing cache behavior programmers perform similar analysis basic understanding cache operation see section 
asymptotic analysis effective certain algorithms analyzing large complex programs difficult 
programmers rely execution time profile isolate problematic code sections apply asymptotic analysis sections 
unfortunately traditional execution time profiling tools gprof generally insufficient identify cache performance problems 
example profile identify procedure source lines bottleneck programmer easily conclude floating point operations responsible 
programmers benefit profile focuses specifically program cache behavior identifying problematic code sections data structures 
cache profiles help provide insight cause cache misses help programmer determine appropriate program transformations improve performance 
purpose article introduce broad audience techniques cache performance profiling tuning 
techniques sporadically supercomputer multiprocessor communities broad applicability programs running fast uniprocessor workstations 
show cache profiling cprof cache profiling system effective means improving program performance focusing programmer attention problematic code sections providing insight type program transformation apply 
section review reason cache behavior show knowing cause cache helps provide insight eliminate 
cookbook simple program transformation techniques improving program cache behavior including array merging padding aligning structures structure array packing loop interchange loop fusion blocking 
briefly describe cprof cache profiling system windows user interface 
case study cprof tune cache performance programs spec benchmark suite compress dnasa eqntott spice tomcatv xlisp 
show cprof identified source lines data structures exhibit poor cache behavior cprof helped provide insight necessary select appropriate program transformation 
execution time speedups programs range depending machine memory system 
understanding cache behavior brief review reason program cache behavior programmers recall basic operation cache memories 
caches sit fast processor slow main memory holding regions referenced main memory 
satisfied cache called hits proceed processor speed unsatisfied called misses incur cache penalty fetch corresponding data main memory 
current proces sors wait stall data arrive 
caches programs exhibit significant locality 
temporal locality exists program memory location multiple times short period 
caches exploit temporal locality retaining referenced data 
spatial locality occurs program accesses memory locations close accessed 
caches exploit spatial locality fetching multiple contiguous words cache block occurs 
cache memory terminology cache hit memory satisfied cache 
cache memory satisfied cache 
penalty time required fetch data main memory cache cache capacity total number bytes cache may contain 
block size number contiguous bytes fetched cache associativity number unique places cache particular block may reside 
fully associative cache block reside place cache 
set associative cache block reside exactly places cache 
direct mapped cache block reside exactly place cache 
compulsory misses cache block 
capacity misses fully associative cache lru 
conflict hits fully associative cache misses away set associative cache 
caches characterized major parameters capacity block size associativity 
cache capacity simply defines total number bytes may contain 
block size determines contiguous bytes fetched cache cache may contain blocks time 
associativity refers number unique locations cache particular block may reside 
block reside location cache call fully associative cache reside exactly location call direct mapped reside exactly locations call way set associative 
smith survey provides detailed description cache design 
parameters programmer analyze order cache behavior simple algorithms 
consider simple example nested loops outer loop iterates times inner loop sequentially accesses array byte integers 
cache small array large array 

determining expected cache behavior sequentially accessing array fits cache produce cache misses number cache blocks required hold array 
accessing array larger cache result ml cache misses number passes array 
size array smaller cache capacity see expect number cache misses equal size array divided cache block size number cache blocks required hold entire array 
size array larger cache capacity see expected number cache misses approximately equal number cache blocks required contain array times number outer loop iterations nl 
someday compilers may automate analysis transform code reduce frequency research produced promising results restricted problem domains :10.1.1.133.892
general codes current commercial compilers programmer manually analyze programs perform transformations hand 
select appropriate program transformations programmer obtain insight cause poor cache behavior 
approach understanding cause cache misses classify disjoint types compulsory capacity conflict 
compulsory caused referencing previously unreferenced cache block 
small array example see misses compulsory 
eliminating compulsory requires prefetching data explicit prefetch operation placing data items single cache block 
example integers example require bytes cut misses half changing declaration 
compulsory misses usually constitute small fraction cache misses discuss article 
compulsory misses fully associative cache lru replacement classified capacity capacity misses caused referencing cache blocks fit cache 
large array example see expect see capacity misses 
programmers reduce capacity misses restructuring program re blocks cache 
example may possible modify loop structure perform outer loop iterations portion array fits cache move portion array 
technique discussed section called blocking similar techniques exploit vector registers supercomputers 
hits fully associative cache misses way set associative cache classified conflict conflict block indicates block referenced past contained fully associative cache cache blocks map cache set accessed block consider execution doubly nested loop machine direct mapped cache inner loop sequentially accesses arrays dot product 
combined size arrays smaller cache expect compulsory misses 
ideal case occurs arrays map different cache sets 
overlap partially entirely get conflict misses array elements compete space set 
eliminating conflict misses requires program transformation changes memory allocation arrays contemporaneous accesses compete sets changes manner arrays accessed 
discussed section solution change memory allocation merging arrays array structures 
hill defines compulsory capacity conflict misses terms ratios 
generalizing concept individual cache misses introduce anti conflict misses fully associative cache lru replacement hit way set associative cache 
anti conflict misses generally useful understanding rare cases set associative cache performs better cache capacity 
cache conflict conflicting mappings conflicting cache mappings presence conflict misses indicates mapping problem 
shows arrays fit cache mapping produce conflict misses shows mappings result conflict misses 
discussion far assumes cache indexed virtual addresses systems index caches real physical addresses making cache behavior strongly dependent page placement 
operating systems page coloring minimize effect reducing performance difference virtual indexed real indexed caches 
techniques improving cache behavior analysis techniques described previous section help programmer understand cause cache misses 
section cookbook simple program transformations help eliminate misses 
program transformations classified type cache misses eliminate 
conflict misses reduced array merging padding aligning structures structure array packing loop interchange 
techniques change allocation data structures loop interchange modifies order data structures referenced 
capacity misses eliminated program transformations reuse data displaced cache loop fusion blocking structure array packing loop interchange :10.1.1.133.892
sections examples techniques loop interchange dis cussed 
merging arrays programs contemporaneously arrays dimension indices 
merging multiple arrays single compound array programmer increases spatial locality potentially reduces conflict misses 
programming language accomplished declaring array structures arrays example 
simple transformation performed fortran provides structures 
fortran structures programmer obtain effect complex indexing example 
old declaration arrays int val size int key size new declaration array structures struct merge int val int key struct merge merged array size example 
merging arrays old declaration integer integer new declaration integer xy preprocessor macro definitions perform addressing define xy define xy example 
merging arrays fortran padding aligning structures referencing data structure spans cache blocks may incur misses structure smaller block size 
padding structures multiple block size aligning block boundary eliminate misalignment misses generally show conflict misses 
padding easily accomplished example declaring extra pad fields 
alignment little difficult address structure multiple cache block size 
statically declared structures generally require compiler support 
dynamically allocated structures aligned programmer simple pointer arithmetic example 
note dynamic memory allocators versions malloc return cache block aligned memory 
old declaration twelve byte structure struct ex struct int val val val new declaration structure padded byte block size struct ex struct int val val val char pad example 
padding structures original allocation guarantee alignment ar struct ex struct malloc sizeof struct ex struct size new code guarantee alignment structure 
ar struct ex struct malloc sizeof struct ex struct size ar int ar example 
aligning structures packing packing opposite padding packing array smallest space possible programmer increases spatial locality reduce conflict capacity misses 
example example programmer observes elements array value greater fit type unsigned char requires bits unsigned int typically requires bits 
machine byte cache blocks code example permits elements block reducing maximum number cache misses factor 
old declaration array unsigned integers 
unsigned int values loop sequencing values values example 
unpacked array new declaration array unsigned characters 
valid iff unsigned char values loop sequencing values values example 
packed array structures loop fusion numeric programs consist operations data coded multiple loops arrays 
combining loops programmer increases program temporal locality frequently reduces number capacity misses 
example combines doubly nested loops operations performed entire row moving 
example separate loops example fused loops note loop fusion exactly opposite loop fission program transformation splits independent portions loop body separate loops 
loop fission helps optimizing compiler detect loops exploit vector hardware supercomputers 
vector supercomputers employ caches relying high bandwidth interleaved main memories transformations described may counterproductive machines 
blocking blocking general technique restructuring program reuse chunks data fit cache reduce capacity misses 
spec matrix multiply part dnasa fortran program implements column blocked algorithm example achieves speedup naive implementation example decstation 
algorithm tries keep columns matrix cache duration outermost loop ideally getting hits matrix large columns fit cache dimensional row column blocked algorithm 
continue example 
naive matrix multiply continue example 
spec column blocked matrix multiply cprof cache profiling system analysis transformation techniques described help programmer develop algorithms minimize cache misses 
cache misses result complex interaction algorithm memory allocation cache configuration program executed programmer expectations may match reality 
developed cache profiling system cprof addresses problem identifying cache misses occur program classifying compulsory capacity conflict 
tool helps provide insight necessary programmers select program transformations improve cache behavior 
cache memory system profilers differ better known execution time profilers focussing specifically memory system performance 
memory system profilers obviate execution time profilers provide supplementary information necessary quickly identify memory system bottlenecks tune memory system performance 
number cache memory system profilers differ level detail programmer 
high level tools identify procedures basic blocks incur large memory overheads 
cache profilers pfc sim cprof identify cache misses source line level allowing detailed analysis 
course extra detail come free runs faster profilers requiring address tracing full cache simulation 
full simulation permits profiler identify data structures responsible cache misses determine type features provided cprof 
similar cprof difference granularity source code annotated type classification 
annotates source code procedure level provides types uniprocessors compulsory replacement 
determining replacement result referencing data fit cache capacity mapping problem conflict left user 
provides insight cause replacement misses identifying data structures competing space cache 
cprof unique provides fine grain source identification data structure support classifies misses compulsory capacity conflict 
cprof uses flexible windows interface see cache profile way helps programmer determine cache performance bottlenecks 
data window lists source lines data structures sorted descending order importance allowing quick identification poor cache behavior 
misses cross referenced programmer quickly determine data structures source line responsible cache misses 
cprof annotates static dynamic data structures 
dynamically allocated structures labeled concatenating procedure names call stack point allocation 
appended counter value allows unique identification dynamically allocated structures 
text window view individual source files line annotated corresponding number cache misses 
windows user interface allows user browse source file moving line higher lower number cache misses 
detail window displays number type currently selected source line data structure 
cprof effective identifying program exhibits poor cache behavior cache types help programmer select type program transformation apply 
section describe cprof tune program performance 
case study spec benchmarks section describe study cprof cookbook transformations tune cache performance programs spec benchmark suite compress dnasa eqntott spice tomcatv xlisp 
purpose section fold 
show obtain significant speedups cache profiling codes extensively tuned execution time profilers 
second show cprof gain insight cache behavior determine transformations improve performance 
performance results terms speedup user execution time models decstation 
machines separate kilobyte direct mapped instruction data caches byte blocks write buffer 
mhz mips processor chip 
major difference memory systems machines cache penalty processor cycles decstation cycles decstation helps illustrate importance cache profiling cache penalty increases 
uses mhz mips system time accounts little total execution time programs 
compress exception system time relatively high large amount case excluding system time eliminates bias introduced different systems 
cprof user interface cprof user interface divided sections data presentation section command buttons 
top section text window middle section data window bottom section detail window 
particular window depends selected command button 
source button opens pull menu entry source file additional entry allows display list source files sorted number cache misses 
selecting files displays source code text window 
source line labeled number cache misses generated line 
highlight line cache misses 
arrow arrow buttons allow movement source file line higher lower number misses respectively 
detail window refines cache misses highlighted line type 
selecting type causes window open displays data structures referenced source corresponding number cache misses type selected 
sort lines button displays list source lines data window sorted number cache misses 
entry contains file name line number number cache misses percent total misses 
sorted list data structures displayed sort vars button 
entry list contains variable name count number misses percentage total misses 
selecting type causes window open displays source lines data structure corresponding number cache misses type selected 
user selects types read write display set metrics button 
counts displayed data window written file dump counts button 
processor chip cycle penalty 
machines secondary differences significant performance impact 
example xx deep write buffers deep write buffer 
addition performs sequential prefetch cache misses reducing effective penalty long sequential accesses 
secondary factors significantly affect execution time necessary model factors cprof cache simulation 
reduce experimental error averaged execution time runs 
programs compiled optimization level mips version compilers 
spice exception compiled optimization level spec file 
note run times reported full optimization profiled programs optimization level full symbolic debugging 
cache profiling high optimization levels suffers difficulties debugging incorrect line numbers cprof uses symbol table information 
table shows applications benefited various restructuring techniques 
benchmark dnasa consists numerical kernels broke kernels poor cache performance analyzed restructuring technique merging loop loop padding program arrays fusion interchange aligning packing blocking btrix dnasa cholesky dnasa compress eqntott dnasa mxm dnasa spice tomcatv vpenta dnasa xlisp table 
program restructuring techniques table summarizes restructuring techniques improve cache behavior program studied 
separately 
table execution time results benchmarks 
full programs execute faster modified improve cache behavior 
breaking kernels dnasa shows machine program seconds speedup seconds speedup seconds speedup modification original merged key value arrays compress reduced hash table size original dnasa tuned kernels original btrix loop interchange loop fusion original cholesky loop interchange original loop interchange naive mxm spec column blocked original merged arrays loop interchange vpenta loop fusion original eqntott changed short char original spice merged pointer number original merged arrays tomcatv loop fusion original xlisp padded node bytes table execution time speedup original tuned times dnasa include spec version matrix multiply mxm 
striking results speedups vpenta decstation decstation decstation 
discuss experience cache profiling modifying program 
provide brief description program followed results initial cprof execution 
discuss modifications performed resulting speedups 
compress compress unix utility implements known lempel ziv data compression algorithm 
input character compress searches hash table prefix key 
key matches array accessed obtain appropriate value 
hash table quite large entries reduce probability collisions 
collision occur secondary probe initiated 
cprof indicates source lines data structure stores keys responsible cache misses 
source line initial probe hash table accounts cache misses 
source line performs secondary probe operation collision accounts misses 
cprof shows misses capacity misses 
recall eliminate capacity misses processing data portions fit cache 
applying insight compress reduced hash table size small fit data cache 
change results speedups decstation 
modification changes program output compression ratio original file size compressed file size related size hash table 
output compatible compressed file match standard spec output 
clear trade speed compression ratio 
unoptimized version compression ratio optimized version 
tried improve cache performance compress changing compression ratio 
compress large number capacity misses conflict misses account misses key array misses value array 
windows interface cprof allowed quickly notice array index arrays 
separate arrays reduce total space requirements key integer value short alignment restrictions require padding combined array structures price poor spatial locality 
referencing key compress speedup btrix cholesky compress dnasa eqntott mxm tomcatv spice vpenta xlisp speedups decstation obtained cache profiling cache set key key key key key key key key val val val val val val val val key val key val key val key val key val key val key val key val 
original cache mapping 
new cache mapping cache mappings compress initial allocation strategy key value arrays resulted cache misses successful hash table probe 
merging arrays array structures effectively interleaves elements arrays results cache successful probe 
corresponding value resides array different cache block see 
merging arrays single array structures places key value cache block see improving spatial locality 
modification accesses value hit cache assuming proper alignment reducing number conflict misses providing speedups decstation 
eqntott spec benchmark eqntott cad tool converts boolean equations equivalent truth tables 
execution time profiling shows eqntott spends time quick sort routine 
cprof reveals time spent moving sort keys memory cache misses generated comparison routine 
offending routine examines arrays generates capacity misses indicating need re blocks cache bring fewer blocks 
cprof indicates capacity misses due fetching bit structures dynamically allocated line bit data type bit integer type short inspection source code shows bit data types take values set 
changing type definition bit integer bit integer short char reduces number misses routine half 
speedup execution time 
prefetch capabilities exploit sequential accesses compare routine reducing benefit modification 
eqntott integer values represent symbolic values zero dash 
enumerated types compiler potentially allocate bits array element resulting eighth number cache misses 
trade fewer cache misses time unpack data implementation dependent 
xlisp spec benchmark xlisp small lisp interpreter solving queens problem 
reduce computation requirements profiling profiled xlisp solving queens problem speedup results table standard queens input 
programmers aware cache behavior sensitive input data programs may exhibit cache behavior smaller input sizes poor behavior larger inputs 
case results obtained smaller input data sufficient achieve reasonable speedups larger input 
cprof shows approximately cache misses occur mark sweep garbage collection conflict misses 
phase program traverses reachable nodes marks accessible sweeps sequentially memory segment placing unmarked nodes free list 
mark sweep garbage collection inherently poor locality alternate algorithm provide better cache behavior 
extensive modification outside scope study 
cprof shows cache misses generated single source line checks flag mark accessibility sweep 
conflict misses dominate improved spatial locality sweep routine separating flags rest node structure 
placing flags single array sequential sweep exhibited excellent spatial locality hit eliminating cache misses sweep routine 
unfortunately change increased number misses mark routine fetch node corresponding flag 
modification increased spatial locality sweep expense spatial locality mark resulting negligible change performance 
returning cprof see node structures allocated line incur large number conflict misses 
inspection source reveals node structure occupies bytes byte cache block 
consequently half nodes reside entirely single cache block see 
remaining half nodes reside contiguous cache blocks potentially causing cache misses referenced 
explicitly padding original node structure bytes cache block size ensuring alignment cache block boundaries obtained speedup decstation 
important realize padding data structures guaranteeing alignment worse padding 
example nodes generating misses cache half 
similarly memory allocators ultrix malloc routine return aligned memory xlisp pre allocates large chunks manages bypassing alignment performed allocator 
application specific memory managers certainly role programmers remember impact padding alignment cache performance 
cache set pad pad pad pad 
original node mapping 
new node mapping cache mappings xlisp node structures pattern corresponds different node structure pad indicates wasted storage 
initial allocation strategy resulted cache misses half nodes cache 
padding structures equal cache block size alignment cache block boundaries reduces cache node resident cache 
padding data structures wastes memory space xlisp node structures bytes information 
explicit padding increases allocated size bytes required language semantics bytes increase storage 
increase adversely affect virtual memory performance larger programs problem input 
tomcatv tomcatv fortran mesh generation program uses dimensional data arrays requires approximately byte 
algorithm see consists forward pass arrays read written loops backward pass loop arrays calculate errors forward pass loop add errors 
arrays larger cache arrays sequentially accessed expect see large number capacity misses 
cprof shows read accesses arrays loop initial forward pass generating large number conflict misses 
easily observed source code arrays referenced indices 
improve spatial locality merged placing elements cache block 
modification results speedups decstation 
running cprof modified tomcatv finds capacity misses rx ry arrays dominate 
shows forward pass composed loops loop initially arrays including writing rx ry followed loop computes maximum values rx ry arrays final pass loop rx ry arrays adjust values 
addition disjoint forward pass loops additional forward pass loop add errors arrays backward pass loop rx ry arrays 
rx ry arrays referenced order loop forward pass loops 
recall array bytes size larger byte data cache 
elements referenced start loop cache start loop 
solution improve temporal locality restructuring program allowable operations performed element resident cache 
transforming program loop fusion see merges loops program contains forward loop backward loop 
perform ll forward wave loop 
rx ry aa dd loop 
rx ry loop 
aa dd rx ry backward wave loop 
rx ry aa forward wave loop 
rx ry endfor ll forward wave loop 
rx ry rx ry aa dd rx ry aa dd rx ry backward wave loop 
rx ry aa endfor original tomcatv loop fused tomcatv tomcatv code original tomcatv algorithm contains loops forward wave 
arrays referenced consecutive loops data accessed loop displaced data referenced previous loop 
loop fused version tomcatv performs operations forward wave row arrays 
results speedups decstation respectively 
operations forward pass backward pass loop data dependencies 
notice folded addition error corrections forward pass 
loop fusion addition array merging produced speedup decstation 
speedups high expected increase number conflict misses slight increase number instructions executed 
spice spice spice analog circuit simulator written fortran 
primary data structure sparse matrix implemented arrays 
particular separate arrays row pointers row numbers column pointers column numbers values 
cprof shows source lines accessing row pointer row number arrays cause cache misses 
source lines accessing column pointer column number arrays contribute additional cache misses 
pair source lines contained small loop locates element sparse matrix 
cprof shows majority misses caused source lines conflict misses indicating mapping problem 
windows interface cprof allows quickly observe row column pointer row column number arrays nearly accessed index 
merging pointer number arrays improve spatial locality results speedup decstation 
dnasa nasa kernels dnasa collection floating point intensive kernels known nas kernels vpenta cholesky btrix fft mxm emit 
kernel initializes arrays copies working arrays calls application routine 
discuss kernels separately better describe cache optimizations 
study emit vortex generation code fft fast fourier transform code emit low ratio kbyte data cache shuffling ffts inherently poor cache performance 
speedup obtained entire collection kernels decstation 
vpenta vpenta kernel simultaneously inverts routine commonly solve systems partial differential equations 
cprof finds ratio startling due conflict misses 
cprof identify mapping problems discovered nested loops responsible cache misses 
loop accesses arrays accesses arrays 
recall eliminate conflict misses changing allocation data structures order accessed 
inspection source code reveals techniques applied 
discovered loops interchanged traverse arrays column order identified opportunities array merging 
modifications result speedups decstation 
interesting note original code runs slower despite faster processor cycle time 
apparently due higher penalty machines drams incurs approximately ns additional delay due asynchronous interface 
loop interchange increases spatial locality results sequential access pattern prefetch logic exploit 
speedup modified code 
tomcatv running cprof modified version vpenta shows capacity misses dominate 
fusing loops improve temporal locality eliminating multiple passes arrays results speedups original version respectively 
cholesky cholesky performs cholesky decomposition substitution 
cprof reveals large number capacity misses nested loops 
inspection source code identifies array traversed row major column major order 
statically transposing array effectively performing loop interchange simpler code modification results speedups decstation 
blocking applied cholesky chose apply simpler transformation 
btrix btrix tri diagonal solver 
cprof shows misses capacity misses occur nested loops 
checked array order immediately noticed array traversed row order 
observed statically transposing array allow fusion different loops 
notice able apply transformations single run cprof 
decstation obtain speedup 
kernel dominated gaussian elimination routine 
cprof finds misses capacity occur gaussian elimination loop inspection shows traversed row order 
interchanging loops trivial case results speedup decstation 
continue original gaussian elimination continue continue interchanged gaussian elimination mxm mxm matrix matrix multiply routine 
naive matrix multiply algorithm known cache little data re loop iterations 
spec mxm implementation simple algorithm column blocked implementation described cookbook earlier re uses columns inner loops 
interesting note improving cache performance original rationale blocking mxm intent improve opportunity vectorizing compilers reuse contents vector registers cray supercomputers 
case transformation improves performance vector registers caches 
standard spec column blocked algorithm achieves speedup naive algorithm decstation decstation 
larger matrices dimensional row column blocked algorithm perform better standard spec input size extra overhead decreases performance 
summary demonstrated cache profiling program transformation obtain significant speedups spec benchmarks 
speedups range depending machine memory system greater speedups obtained fortran programs 
fortran support structures programs exhibit poor spatial locality 
improving spatial locality interleaving elements disjoint arrays provided substantial improvements fortran programs 
fortran provides structures greatly simplify transformations 
cprof effective identifying merge arrays 
loop interchange improved spatial locality fortran programs programs loop interchange trivial transformation easily identified inspection 
temporal locality fortran programs improved loop fusion requires programmers perform allowable operations data cache versus performing operation turn data 
important remember transformations discussed may counter productive machines vector registers 
programs benefited padding alignment structures merging arrays array structures changing declaration variable pack elements single cache block 
notice padding packing opposite approaches dependent program profiled 
processor cycle times continue increase faster main memory cycle times memory hierarchy performance increasingly important 
programmers mentally simulate cache behavior help select algorithms cache performance 
unfortunately actual cache performance match programmer expectations programs complex fully analyze interactions memory patterns data allocation cache organization 
cases tool cprof important element programmer tool box 
cprof provides cache performance information source line data structure level allowing programmer identify hot spots 
insight cprof provides classifying cache misses compulsory capacity conflict helps programmers select appropriate program transformations improve program spatial temporal locality performance 
karen miller alain chris maguire earlier version cprof 
alain scott bugs current version 
james larus provided great deal support qpt generate input cprof suggested reducing hash table size compress 
lebeck james larus mark hill provided helpful comments suggestions early drafts 
supported part national science foundation presidential young investigator awards ccr mips national science foundation cda ccr mip donations bell laboratories digital equipment xerox graduate school university wisconsin 

callahan kennedy porterfield analyzing visualizing performance memory hierarchies instrumentation visualization acm press 

goldberg hennessy performance debugging shared memory multiprocessor programs proceedings supercomputing pp 
november 

graham kessler mckusick execution profiler modular programs software practice experience pp 


gupta martonosi anderson analyzing memory system bottlenecks programs performance evaluation review pp 
june 

hill smith evaluating associativity cpu caches ieee transactions computers pp 
december 

kessler mark hill page placement algorithms large real index caches acm trans 
computer systems pp 
november 

lam rothberg wolf cache performance optimizations blocked algorithms proceedings asplos iv pp 
april 

spec newsletter standard performance evaluation fairfax va december 
spec suite drive fairfax va 
hill cache performance integer spec benchmarks risc acm sigarch computer architecture news pp 
june 

porterfield software methods improvement cache performance supercomputer applications ph 
thesis dept computer science rice university 

smith cache memories acm computing surveys pp 
sept 

zorn hilfinger memory allocation profiler lisp proceedings summer usenix conference june 

