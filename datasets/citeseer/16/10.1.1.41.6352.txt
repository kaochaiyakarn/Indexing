number iterations convergence diagnostics generic metropolis algorithms adrian raftery university washington steven lewis university washington order markov chain monte carlo mcmc necessary determine long simulation needs run 
idea discard number initial simulations arbitrary starting point initial simulations came stationary distribution intended markov chain 
consecutive simulations markov chains dependent highly 
saving simulations require large amount storage researchers mcmc prefer saving third fifth tenth simulation especially chain highly dependent 
referred thinning chain 
burn thinning mandatory practices reduce amount data saved mcmc run 
chapter outline way determining advance number iterations needed level precision mcmc algorithm 
introduced section section describe software implements available free charge statlib 
section show output method diagnose lack convergence slow convergence due bad starting values high posterior correlations stickiness chain 
section describe methods combined ideas muller gelman yield automatic generic metropolis algorithm 
simplicity discussion context single long chain 
basic ideas determine number iterations diagnose slow convergence multiple sequences advocated gelman rubin see section 
determining number iterations practical application mcmc number important decisions 
include number iterations spacing iterations retained final analysis number initial burn iterations discarded 
simple way making decisions proposed raftery banfield raftery lewis 
mcmc bayesian inference sample generated run algorithm adequately represent posterior distribution interest 
interest focuses posterior quantiles functions parameters bayesian confidence intervals posterior medians main requirement mcmc estimates quantities approximately correct 
consider probability statements regarding quantiles posterior distribution function parameter 
quantile thing percentile expressed fractions percents 
suppose want estimate data probability function 
find approximate number iterations required actual quantile interest example corresponds requiring cumulative distribution function quantile estimated sigma probability 
reasonable requirement roughly speaking wanted reported intervals actual posterior probability 
run mcmc algorithm initial iterations discard iterations store th problem determine calculate iteration value th iteration form fz binary process derived markov chain markov chain 
reasonable suppose dependence fz falls fairly rapidly lag form new process gamma consisting th iteration original chain approximately markov chain sufficiently large 
determine form series 
compare order markov chain model second order markov chain model choose smallest value order model preferred 
compare models calculating likelihood ratio test statistic second order markov model order markov model bic criterion gamma log number iterations pilot sample see models better fits pilot sample 
bic introduced schwarz context generalized log linear models raftery provides approximation twice logarithm bayes factor second order model 
assuming markov chain determine mk number burn iterations discarded 
follows standard results state markov chains see cox miller 
gamma ff ff fi gamma fi transition matrix ff probability changing state second state fi probability changing second state state 
equilibrium distribution ff fi gamma fi ff data gamma step transition matrix ff fi ff gammaff gammafi fi gamma ff gamma fi 
suppose require 
requirement ff fi max ff fi holds log ff fi max ff fi log assuming usually case practice 
determine note estimate data large approximately normally distributed mean variance gamma ff gamma fi fffi ff fi requirement gamma satisfied gamma ff gamma fi fffi ff fi phi gamma phi delta standard normal cumulative distribution function 
thinned chain simulations perform final inference results precision thinning done 
value thinning iterations probability satisfying accuracy criterion greater required criterion conservative case 
table maximum percent error estimated quantile 
min percent error cauchy method requires mcmc algorithm run initial number iterations order get pilot sample parameter values 
rough guide size pilot sample note required minimized successive values fz independent implying ff gamma fi gamma case min phi gamma gamma user needs give required precision specified quantities 
result far sensitive gamma example min 
min general reasonable choice size pilot sample 
specifying required precision terms error cumulative distribution function quantile refers may natural specify required precision terms error estimate quantile 
order see relates accuracy scale shown table approximate maximum percentage error estimated quantile corresponding selected values defined max gamma sigma gamma gamma shown distributions normal light tailed moderate tails cauchy heavy tailed 
suppose regard relative error scale standardized variable score acceptable corresponding estimated quantile gamma gamma normal distribution compared true value gamma 
knew data light normal tails table suggests sufficiently small 
heavier tailed distribution required achieve accuracy heavy tailed cauchy required corresponding min 
software implementation method implemented fortran program obtained free charge sending mail message send general statlib stat cmu edu 
despite name mcmc just gibbs sampler 
program takes input pilot mcmc sample quantity interest values returns output estimated values recommend run second time iterations produced check iterations recommended basis pilot sample adequate 
value second call appreciably call mcmc algorithm continued total number iterations adequate 
practical setting quantities interest quantiles 
recommend called quantile primary interest maximum values 
typically tail quantiles harder estimate central quantiles medians reasonable routine practice apply quantity primary interest twice 
convergence diagnostics section assume program run 
output program values possible just take parameters final run mcmc recommended 
performing little output analysis values may indicate ways mcmc algorithm improved 
graphical examination initial min iterations suggest ways improve algorithm 
program outputs diagnostic purposes 
outputs combined calculate min raftery lewis 
statistic measures increase number iterations due dependence sequence 
values greater indicate high level dependence 
values greater indicate problems alleviated changing implementation 
dependence due bad starting value case starting values tried high posterior correlations remedied crude correlation removing transformations stickiness markov chain removable changing mcmc algorithm 
may surprising bad starting value lead high values happens progress away bad starting value tends slow gradual leading highly autocorrelated sequence high values entire pilot sequence including initial values estimate important examine iteration sequenced plots initial mcmc generated posterior sample key parameters model 
example hierarchical models important look plot random effects variance equivalent parameter 
starting value parameter close zero componentwise mcmc gibbs sampler get stuck long time close starting value 
plot simulated values parameter show algorithm remained stuck near starting value 
problem arises example follows 
example illustrate ideas example analysis longitudinal world fertility survey data raftery lewis kahn lewis 
data complete birth histories iranian women focus estimation unobserved heterogeneity 
probability woman child calendar year simplified version model log gamma ffi ffi iid sigma prior gaussian prior sigma inverted gamma shape parameter scale parameter lewis 
ffi random effects representing unobserved sources heterogeneity fertility frequency 
measured covariates model omitted ease exposition 
shows run mcmc algorithm starting value sigma close zero sigma gamma values ffi randomly generated gamma 
figures starting value omitted reasons scaling 
sigma series highly autocorrelated run program confirms 
obtain 
min 
high value trend appearance suggest starting value problem 
contrast values ffi run correlated diagnostics series mislead 
shows series sigma different starting values illustrating simple trial error approach choice adequate starting value 
starts sigma method raftery lewis yields 
starts sigma 
starts sigma 
results trajectories satisfactory suggesting results relatively insensitive starting value diagnostics indicate problem 
example bears main points 
important monitor mcmc run key parameters start different starting values diagnostics suggest doing 
variance starting random effect woman mcmc output model equation iranian world fertility survey data starting sigma gamma series sigma values series values ffi random effect woman survey 
generic metropolis algorithms section develop generic metropolis algorithm fully automated done 
combines methods sections ideas muller gelman 
basic algorithm parameter updated time proposal distribution normal centered current value 
user specify variance proposal distribution parameter outline strategy doing 
version metropolis algorithm referred metropolis gibbs parameter time updated 
prefer name form metropolis algorithm gibbs sampler 
name sampling full conditional distributions required generic algorithm 
strategy consists applications mcmc 
denote vector parameters denote variance proposal function th component oe values set user 
performance metropolis algorithm variance starting variance starting variance starting values sigma runs mcmc algorithm different starting values sigma sigma sigma 
sensitive user specified oe large chain move current state small chain move frequently slowly 
situation take long time get adequate sample posterior distribution 
values oe number values suggested literature 
general setting tierney suggested setting standard deviation proposal distribution fixed multiple times current estimated variance matrix 
suggested variable schedule way chain behaving determine value oe muller clifford 
gelman studied optimal variance case simulating univariate standard normal distribution metropolis algorithm normal distribution centered value proposal distribution 
considered different optimality criteria asymptotic efficiency markov chain geometric convergence rate markov chain 
case unit normal distribution optimal proposal standard deviation 
expect arbitrary normal distribution optimal proposal standard deviation roughly times standard deviation normal 
gelman result readily incorporated generic simulation strategy outlined 
generic simulation strategy consists steps simulation ffl assign large value oe times approximate marginal standard deviation ffl run mcmc min scans obtain pilot sample 
simulation ffl pilot sample calculate conditional standard deviations component sample estimates components 
done linear regression 
ffl assign oe sd gammaj gammaj denotes th component 
ffl run mcmc min scans obtain second sample 
second sample necessary obtain reasonable estimates conditional standard deviations 
simulation ffl calculate conditional standard deviations parameters second sample 
ffl reassign oe sd gammaj 
ffl run program get ffl run mcmc scans 
ffl rerun program check big 
ffl big program says number iterations required greater number iterations run inference final sample 
ffl return simulation 
example illustrate simulation strategy simulate trivariate normal distribution gamma gamma gamma gamma conditional variances kind posterior covariance matrix common regression parameters intercept independent variables corresponding non zero means 
challenging example conditional variances times smaller marginal variances high pairwise correlations 
start generating small pilot sample min draws 
order determine size pilot sample select values take corresponds requiring estimated cumulative distribution function quantile fall interval probability 
leads min 
proposal standard deviations getting pilot sample set oe oe oe 
know actual distribution find true quantile gamma gamma 
contains iteration sequenced plots shows iteration pilot sample clearly obtain sample posterior 
calculated sample covariance matrix pilot sample conditional variance parameter 
reported table summarizes results 
took simulations number iterations indicated program actual number iterations run current simulation 
sample covariance matrix pilot sample far truth 
input pilot sample program says equal set 
min pilot sample clearly large 
recalculated proposal standard deviations oe oe oe 
inputs second simulation 
obtained sample min draws posterior 
results shown second simulation column table 
second sample covariance matrix closer true covariance matrix pilot sample covariance matrix 
correct headed right direction 
conditional variances second sample recalculate proposal standard deviations time 
shown inputs third simulation 
second metropolis sample input program produced values simulation second simulation third simulation fourth simulation sequence plots intercept estimate trivariate normal example 
table simulation strategy results trivariate normal example 
input control parameters simulation reported double line 
resulting estimates simulation shown line 
variable true simulation value second third fourth oe oe oe gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma third run metropolis algorithm 
case indicated set set 
recalculated proposal standard deviations perform third metropolis run total iterations 
marginal conditional sample variances third sample reasonably close correct values 
ran program said 
new considerably greater number iterations third sample repeated third simulation 
accordingly obtained fourth sample containing iterations 
marginal conditional sample variances fourth sample closer correct values sequence looks reasonably stationary 
additional check calculated sample estimate cumulative distribution function true quantile finding proportion simulations equal gamma gamma fourth sample 
estimate satisfying original criterion 
fourth sample says number iterations fourth sample 
conclude fourth sample perform inference 
discussion described way determining number mcmc iterations needed software implements 
shown diagnose slow convergence help design fully automatic generic metropolis algorithm 
method designed common situation interest focuses posterior quantiles parameters interest applied estimation probabilities parameters arises image processing analysis pedigrees expert systems example raftery lewis 
simplicity described methods context single long run mcmc iterations 
gelman rubin argued independent sequences starting points sampled distribution 
method situation applied pilot sequences starting point iterations distributed different sequences 
various implementations possible research needed topic 
really necessary multiple sequences 
gelman rubin argue absence mcmc algorithms give misleading answers certainly true 
creation starting distribution major chore add substantially complexity mcmc algorithm gelman rubin section 
clearly trade extra cost required produce starting distribution penalty cost experienced rare occasions mcmc starting distribution arrives misleading parameter estimates answer ultimately depends application 
experience mcmc algorithms converge rapidly poorly chosen starting values simple diagnostics section usually reveal fact 
simple trial error new starting values leads rapidly satisfactory starting value illustrated example section 
cases diagnostics reveal lack convergence gelman rubin multiple sequences certainly final inferences stake 
diagnostics section multiple sequences 
bad starting value close local mode gelman rubin multiple sequence methods designed protect possible causes slow convergence mcmc 
include high posterior correlations removed approximate orthogonalization see hills smith stickiness chain 
arises hierarchical models algorithm enters parts parameter space random effects variance small require redesigning mcmc algorithm besag green 
acknowledgment research supported onr contract 
nih hd 
grateful andrew gelman stimulating discussions gilks helpful comments 
besag green 

spatial statistics bayesian computation 
journal royal statistical society 
clifford 

contribution discussion approximate bayesian inference weighted likelihood bootstrap 
journal royal statistical society 
cox miller 

theory stochastic processes 
london chapman hall 
gelman 

note efficient metropolis jumping rules 
unpublished manuscript department statistics university california berkeley 
gelman rubin 

single series gibbs sampler provides false sense security 
bayesian statistics bernardo eds oxford university press pp 

gelman rubin 

inference iterative simulation multiple sequences discussion 
statistical science 
hills smith 

parametrization issues bayesian inference discussion 
bayesian statistics bernardo eds oxford university press pp 

lewis 

contribution discussion papers gibbs sampling related markov chain monte carlo methods 
journal royal statistical society series 
lewis 

multilevel modeling discrete event history data markov chain monte carlo methods 
unpublished doctoral dissertation department statistics university washington seattle wa 
metropolis rosenbluth rosenbluth teller teller 

equations state calculations fast computing machines 
journal chemical physics 
muller 

generic approach posterior integration gibbs sampling 
technical report department statistics purdue university west lafayette 
raftery 

note bayes factors log linear contingency table models vague prior information 
journal royal statistical society series 
raftery banfield 

stopping gibbs sampler morphology issues spatial statistics 
annals institute statistical mathematics 
raftery lewis 

iterations gibbs sampler 
bayesian statistics bernardo eds oxford university press pp 

raftery lewis 

long run diagnostics implementation strategies markov chain monte carlo 
statistical science 
raftery lewis kahn 

event history modeling world fertility survey data 
working center studies ecology university washington 
schwarz 

estimating dimension model 
annals statistics 
tierney 

exploring posterior distributions markov chains 
proceedings rd interface computing statistics 

