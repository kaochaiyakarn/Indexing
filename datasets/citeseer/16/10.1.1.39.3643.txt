case compressed caching virtual memory systems paul wilson scott kaplan yannis smaragdakis dept computer sciences university texas austin austin texas cs utexas edu www cs utexas edu users oops compressed caching uses part available ram hold pages compressed form effectively adding new level virtual memory hierarchy 
level attempts bridge huge performance gap normal uncompressed ram disk 
unfortunately previous studies show consistent benefit compressed virtual memory 
study show technology trends favor compressed virtual memory attractive offering reduction paging costs tens percent increasingly attractive cpu speeds increase faster disk speeds 
elements approach innovative 
introduce novel compression algorithms suited compressing memory data representations 
algorithms competitive mature compressors complement 
second adaptively determine memory compressed keeping track program behavior 
solves problem different programs phases program performing best different amounts compressed memory 
decades cpu speeds continued double months years disk latencies improved slowly 
disk latencies orders magnitude greater main memory access latencies adjacent levels memory hierarchy typically differ order magnitude 
programs run entirely ram benefit improvements cpu speeds runtime programs page dominated disk seeks may run times slowly cpu bound programs 
wil wil proposed compressed caching virtual memory storing pages compressed form main memory compression cache reduce disk paging 
appel promoted idea evaluated empirically douglis dou russinovich rc 
unfortunately douglis experiments sprite showed speedups programs speedup slowdown 
russinovich data mixed pc workload showed slight potential benefit 
widespread belief compressed virtual memory attractive machines fast local disk diskless handheld computers network computers laptops slow disks 
douglis pointed compressed virtual memory attractive cpus continue get faster 
crucial point generally overlooked operating system designers adopted compressed caching 
case value compressed caching modern systems 
aim show discouraging results studies primarily due machines quite slow current standards 
current fast disk machines compressed virtual memory offers substantial performance improvements advantages increase processors get faster 
study trends memory disk bandwidths 
show compressed caching increasingly attractive regardless os improvements sophisticated prefetching policies reduce average cost disk seeks log structured file systems reduce cost writes disk 
show better compression algorithms provide significant improvement performance compressed caching 
better variants available introduce new family compression algorithms designed inmemory data representations file data 
concrete points analysis come simulations programs covering variety memory requirements locality characteristics 
stage experiments simulation chosen method evaluation allowed easily try ideas controlled environment 
noted simulation parameters relatively conservative perfectly realistic 
instance assume quite fast disk experiments 
time costs compressions simulations actual runtime costs exact pages compression decompression simulated time 
main value simulation results estimating exact benefit compressed caching clearly substantial 
demonstrate possible detect reliably memory compressed phase program execution 
result compressed virtual memory policy adapts program behavior 
exact amount compressed memory crucially affects program performance compressing memory needed detrimental compressing little memory slightly prevent memory faults 
fixed fraction compressed memory adaptive compressed caching scheme yields uniformly high benefits test programs wide range memory sizes 
compression algorithms wlm explained compressor knowledge programming language implementation exploit knowledge achieve high compression ratios data programs 
particular explained pointer data contain little information average pointers compressed single bit 
describe algorithms weaker assumptions primarily exploiting data regularities imposed hardware architectures common programming language implementation strategies 
algorithms fast fairly symmetrical compression slower decompression 
especially suitable compressed virtual memory applications pages compressed re decompressed 
variant algorithms successfully years virtual memory system apple newton personal digital assistant disk sw walter smith personal communication 
previously published algorithm sketched smith newton results achieved slightly compression explain results algorithms quite encouraging 
straightforward implementation competitive best ziv lempel compressor find superior lzrw algorithm written ross williams wil previous studies compressed virtual memory compressed file caching 
explain believe results significant algorithms competitive superior advanced ziv lempel algorithms different 
despite immaturity complement techniques 
suggest areas research significantly effective algorithms memory data 
algorithms interesting implemented small amount hardware including tiny amount space dictionaries providing extraordinarily fast cheap compression small amount hardware support 
background compression understand algorithms relationship algorithms necessary understand basic ideas data compression 
focus lossless compression allows exact reconstruction original data lossy compression generally disaster compressed vm 
data compression algorithms deep sense ad hoc exploit expected regularities data achieve compression 
compression algorithms embody expectations kinds regularities encountered data compressed 
depending kind data compressed expectations may appropriate inappropriate compression may better worse 
main key compression having right kinds expectations data hand 
compression thought consisting phases typically interleaved practice modeling encoding bcw nel 
modeling process detecting regularities allow concise representation information 
encoding construction concise representation 
ziv lempel compression 
compression algorithms including overwhelmingly popular family detection exact repetitions strings atomic tokens 
token size usually byte speed reasons data ziv lempel algorithm apple previously faster 
unfortunately detailed performance comparisons 
sense byte oriented characters text file multiple byte oriented kinds image data intel architecture machine code unicode 
ziv lempel compressor models reading input data token token constructing dictionary observed sequences looking repetitions goes 
encodes writing strings output time observed writing special codes repetition encountered number dictionary entry 
output consists appropriately labeled new data old data repetitions corresponding lz decompressor reads data interpreter reconstructing dictionary created compression 
sees new string adds dictionary just compressor sending uncompressed output 
sees code repetition dictionary item copies item output 
way dictionary matches dictionary compressor point data stream output replicates original input expanding repetition codes strings represent 
main assumption embodied kind compressor literal repetitions multi token strings occur input ll see bytes row exactly bytes order saw 
natural assumption text reasonable kinds data wrong memory data 
memory data representations commonly thought lz style compression general purpose memory data fairly arbitrary different programs operate different kinds data different ways hope better algorithm lz compressing memory data 
assumption basically false second hasty dubious 
different programs different things common regularities compression algorithm needs average 
consisting byte strings data memory worth stressing widespread confusion optimality compression algorithms 
general encoding scheme huffman coding arithmetic coding provably optimal small factor compressor regularities data known advance detail 
compression algorithms proven optimal simplifying assumption source stochastic randomized typically markov source real data sources programs generally stochastic wjnb proof hold real data 
best viewed records data structures array memory words typically store records fields words 
note fields records usually word aligned data words frequently numbers pointers 
pointers usefully viewed numbers integer indices array memory 
integer pointer data certain strong regularities 
integer values usually numerically small low order bytes significant information content similar integers nearby memory 
likewise pointers point objects nearby memory similar nearby pointers may point area memory pointers nearby may point area 
regularities quite common strong 
reason heap data clustered common memory allocators tend allocate small area memory time data structures constructed particular phase program execution clustered consist types similar objects wjnb 
kinds data show similar regularities 
examples include hidden headers allocators put heap objects virtual function table pointers objects booleans regularities strong largely inmemory data representations designed primarily speed space real programs usually random data random things data 
randomized data regular way consider array random integers zeroes upper bits 
exploiting memory data regularities goal section convey basic flavor algorithms call wk algorithms actual code available web site wish explore experiment 
note algorithms designed years ago cpu slower today stress simplicity speed achieving high compression 
believe better algorithms designed refining basic modeling technique combination traditional modeling sophisticated encoding strategies 
simplicity strikingly effective experiments 
compression algorithms exploit memory data regularities scanning input data bit word time looking data numerically similar specifically repetitions high order bit pattern word low order bits different 
perform partial matching bit patterns 
detect repetitions encoder maintains dictionary just seen words 
algorithms manages dictionary direct mapped cache set associative cache lru replacement algorithm set 
simple software caching schemes trivially implemented fast hardware 
due lack space exact algorithm matter compressed caching performance discuss direct mapped algorithm 
compression algorithms regularities strong 
typical lz style compressor uses dictionary kilobytes kb compressors bytes achieve similar compression ratios inmemory data 
compressor scans page reading word probing cache dictionary matching pattern emitting bit code classifying word 
word may ffl match dictionary entry ffl match upper bits ffl match bit pattern 
special case check see word zeroes matches full word zero case fourth bit pattern 
zeroes case bit tag written compressed output page 
cases additional information emitted 
match case entire bit pattern match written output 
full bit match dictionary index written indicating dictionary word repeated 
partial bit match case dictionary index differing low bits written 
corresponding decompressor reads compressed output examining bit tag time split arrived experimentally early data set partially overlaps study 
effectiveness algorithm sensitive parameter varying split bits difference high bits means matches encoded compactly somewhat fewer things match 
appropriate action 
conventional compression schemes tag indicating match directs read item word compressed input insert dictionary echo output 
tag indicating zeroes directs write word zeroes output 
tag indicating full word match directs copy dictionary item output full match case low bits replaced bits consumed input partial match 
encoding performed quickly 
writing result compressing word directly output algorithm writes kind information different intermediate array reads input data separate postprocessing pass packs information output page fast packing routine 
output page segmented segment containing kind data tags dictionary indices low bits full words 
example bit tags written bytes byte array special routine packs consecutive words holding tags single word output shifting xoring 
decompression segments main pass reconstructs original data 
adaptively adjusting compression cache size perform compressed caching system adapt working set sizes programs caches 
program working set fits comfortably ram pages pages kept compressed overwhelming majority pages kept uncompressed form accessed penalty 
program working set larger available ram compressing pages allow kept ram pages compressed working set captured 
case reduction disk faults may greatly outweigh increase compression cache accesses disk faults times expensive compression cache faults 
douglis observed experiments different programs needed compressed caches different sizes 
implemented adaptive cache sizing scheme varied split uncompressed compressed ram dynamically 
adaptive caching system results inconsistent programs ran faster ran slower 
believe douglis adaptive caching strategy may partly fault 
douglis fairly simple scheme caches competed ram basis pages accessed normal global replacement policy arbitrating needs multiple processes keeping touched pages ram 
uncompressed cache holds touched pages compressed cache scheme requires bias ensure compressed cache memory 
believe biased recency caching robust adaptive cache sizing policy solely lru ordering pages caches 
online cost benefit analysis adaptive cache sizing mechanism addresses issue adaptation performing online cost benefit analysis program behavior statistics 
assuming behavior relatively near resemble behavior relatively past mechanism keeps track aspects program behavior bear directly performance compressed caching different cache sizes compresses fewer pages improve performance 
system uses kind recency information kept normal replacement policies maintains approximate ordering pages touched 
system extends retaining information pages evicted 
information discarded replacement policies retained tell replacement policy working compared different replacement policy 
maintain lru recency ordering pages memory comparable number evicted pages 
ordering primarily model cache model program doing 
simplified example 
understand system works consider simple version manages pool page frames chooses compressed cache sizes frames frames 
compression cache frames compression ratio hold accessed pages uncompressed form uncompressed cache compressed form 
effectively increases size memory terms effect disk fault rate 
task adaptation mechanism decide doing preferable keeping pages uncompressed form zero compressed form 
generally assume pages compressed evicted disk compression cache significant size 
experiments show cost small 
memory size misses cost benefit computation rate histogram 
shows example rate histogram decorated significant data points 
real data scale actual curve typically high far left data points chosen reasonable 
benefit configuration reduction disk faults going memory size memory size 
measure benefit simply counting number times fault pages st th positions lru ordering multiplying count cost disk service 
cost configuration cost compressing decompressing pages outside uncompressed cache region 
exactly touches pages st position lru ordering touches 
example compressed caching beneficial compressing decompressing pages faster fetching pages disk 
general recency information allows estimate cost benefit compression cache size regardless current size compression cache pages currently memory 
analysis find current split memory caches better 
simply count number touches pages different regions lru ordering interpret hits misses relative different sizes uncompressed cache corresponding sizes compressed cache effective memory size 
multiple target sizes 
generalize scheme different target compression cache sizes interpreting touches different ranges lru ordering appropriately size 
adaptive component system computes costs benefits target sizes counts touches regions lru ordering chooses target size lowest cost 
compressed cache size adjusted demand driven way memory compressed uncompressed access compressed page compressed ram disk occurs 
system chooses target uncompressed cache size corresponding effective cache size computed number page frames left compressed cache multiplied estimate compression ratio compressed pages 
means statistics kept adaptivity mechanism exact past information may contain hits different recency region indicated current compressibility estimate 
matter simulations approximate statistics pages touched quite sufficient 
indicates system sensitive details replacement policy uncompressed cache normal lru approximation fine 
clock algorithm bits fifo lru segmented queue kernel page traps segmented queue tlb handlers 
overheads updating statistics performing cost benefit analyses adaptively choosing target split low just instructions uncompressed cache lru list implemented tree auxiliary table hash table sparse page table structure 
adapting behavior adapt program behavior statistics decayed exponentially time 
time defined number interesting events elapsed 
events system considers interesting page touches affect cost benefit analysis hits compressed memory target compression sizes currently suggests 
defining time way benefit touches pages ignored filtering high frequency events 
additionally decay factor inversely proportional size memory total number page frames time typically advances slowly larger memories small ones small memories usually shorter replacement cycle need decay statistics faster rate larger ones 
decay rate inversely proportional memory size time advance inappropriately slowly small memories inappropriately quickly large ones 
small cache wait long respond changes large jump brief changes program behavior persist long worth adapting 
extensive simulation results show strategy works intended adaptivity ensures memory size cache responds changes behavior program relatively quickly benefit relatively persistent program behavior quickly continually distracted short duration behaviors 
single setting decay factor relativized automatically memory size works variety programs wide ranges memory sizes 
detailed simulations section describe methodology results detailed simulations compressed caching 
captured page image traces recording pages touched contents varied unix programs simulate compressed caching detail 
code applications tracing filtering tools compressors simulator available web site detailed study research 
note traces contain executable code pages 
focus data pages main interest compressing memory data 
explain section compressing code equally extra complication certainly done 
techniques complementary proposed compressing code data rc indicate code pages exhibit locality properties data pages 
methodology test suite 
simulations traced programs intel architecture linux operating system page size kb study effect larger page sizes section 
behavior programs described detail wjnb 
brief description ffl gnuplot plotting program large input producing scatter plot 
ffl rscheme bytecode interpreter garbage collected language 
performance dominated runtime generational garbage collector 
ffl espresso circuit simulator 
ffl gcc component gnu compiler performs compilation 
ffl ghostscript postscript formatting engine 
ffl pascal translator 
programs constitute test selection locality experiments try test adaptivity compressed caching policy relative locality patterns various memory sizes 
data footprints vary widely gnuplot rscheme large programs pages respectively gcc ghostscript medium sized pages espresso small pages 
processors 
pentium pro mhz processor approximately represents average desktop computer time 
compressed caching fast machines 

ultrasparc mhz fastest processors available average processor years 
compressed caching works better faster processor 

ultrasparc mhz slower sparc machine provides interesting comparison pentium pro due different architecture faster memory subsystem 
different compression algorithms experiments 
wkdm recency compressor operates machine words uses direct mapped word dictionary fast encoding implementation 

lzo specifically lzo carefully coded lempel ziv implementation designed fast particularly decompression tasks 
suited compressing small blocks data small codes dictionary small 
compressors study written speed optimized implementation intel assembly pentium pro 

lzrw fast lempel ziv implementation 
algorithm douglis dou 
perform lzo wanted demonstrate algorithm allow effective compressed cache today hardware 
runtimes test suite 
results terms time spent paging helpful know processing time required execute program test suite 
shows time required execute programs processors paging occurs 
times added paging time information obtain total turnaround time architecture memory size virtual memory configuration 
program pro sparc sparc name mhz mhz mhz gnuplot rscheme espresso gcc ghostscript processing times program test suite processor study 
memory available paging occurs times turnaround times 
brief note compressor performance 
compression algorithms achieve roughly factor compression average programs 
compress decompress page half millisecond processors 
wkdm algorithm fastest compressing page milliseconds decompressing milliseconds pentium pro faster sparc mhz twice fast sparc mhz 
mb compressed uncompressed second bandwidth quite fast disk 
lzo slower lzrw slower 
tracing 
simulator takes input trace pages program touches augmented information compressibility cost compression touched page particular compression algorithm 
create trace keep trace size manageable steps tracing filtering tools 
traced program portable tracing tool 
added module emit complete copy page referenced 
refer traces page image traces 
creating compression traces 
record actual effectiveness time cost compressing page image created set compression traces 
combination compression algorithm cpu created trace recording expensive effective compression page image reduced page image trace 
test programs compression algorithms cpu resulted compression traces 
tool creates compression traces linked compressor decompressor consumes reduced page image trace 
trace record page image trace compresses decompresses page image outputs trace record 
record contains page number times compressing decompressing page contents moment resulting compressed size page 
page image compressed decompressed times median times reported 
timing precise solaris high resolution timer compression timings done solaris operating system 
avoid favorable hardware caching effects caches filled unrelated data compression 
conservative burstiness page faults usually mean relevant memory cached second level cache real system 
simulation parameters 
different target compression sizes values equal simulated memory size 
persistent phases program behavior system time adapt memory pages holding compressed data 
limiting number target compression sizes guarantees cost benefit analysis incurs low overhead 
decay factor th event size memory weight equal event 
results particularly sensitive exact value decay factor 
estimates 
simulation estimate costs reading page disk writing disk 
conservatively assumed writing dirty pages disk incurs cost compensate file systems keep low cost multiple writes log structured file systems 
additionally assumed disk uniform seek time ms 
admittedly complex model disk access yield accurate results affect validity simulations ms seek time disk fast modern standards 
section examine effect faster disk seek time ms 
results detailed simulations wide range results test programs chose wide range memory sizes simulate 
plots section show entire simulated range program 
subsequent sections concentrate interesting region memory sizes 
range usually begins size program spends time paging time executing cpu ends size program causes little paging 
shows log scale plots paging time programs function memory size 
line plot represents results simulating compressed cache particular algorithm sparc mhz machine 
paging time regular lru memory system compression shown comparison 
seen compressed caching yields benefits wide range memory sizes indicating adaptivity mechanism reliably detects locality patterns different sizes 
note compression algorithms exhibit benefits definite differences performance 
aims conveying general idea outcome experiments 
results analyzed detail subsequent sections isolate interesting memory regions algorithms architectures trends 
normalized benefits effect compression algorithms goal quantify benefits obtained compressed caching identify effect different compression algorithms system performance 
hard see effect indicate compression algorithms obtain similar results 
detailed plot reveals significant variations algorithm performance 
plots normalized paging times different algorithms interesting region 
recall usually begins size program spends time paging time executing cpu ends size program causes little paging 
normalized paging time mean ratio paging time compressed caching paging time regular lru memory size pages total paging time espresso sparc traditional vm lzo lzrw wkdm memory size pages total paging time gcc sparc traditional vm lzo lzrw wkdm memory size pages total paging time gnuplot sparc traditional vm lzo lzrw wkdm memory size pages total paging time ghostscript sparc traditional vm lzo lzrw wkdm memory size pages total paging time sparc traditional vm lzo lzrw wkdm memory size pages total paging time rscheme sparc traditional vm lzo lzrw wkdm compressed caching yields consistent benefits wide range memory sizes 
memory size pages normalized paging time espresso sparc traditional vm lzo lzrw wkdm memory size pages normalized paging time gcc sparc traditional vm lzo lzrw wkdm memory size pages normalized paging time gnuplot sparc traditional vm lzo lzrw wkdm memory size pages normalized paging time ghostscript sparc traditional vm lzo lzrw wkdm memory size pages normalized paging time sparc traditional vm lzo lzrw wkdm memory size pages normalized paging time rscheme sparc traditional vm lzo lzrw wkdm varying compression algorithms affect performance significantly 
algorithms yield benefits compared uncompressed virtual memory significantly better 
replacement policy 
seen algorithms obtain significant benefit uncompressed virtual memory interesting ranges memory sizes 
benefits common large parts plots 
time losses rare exhibited gnuplot small 
additionally losses diminish faster compression algorithms faster processors shown plot 
adaptivity perform optimally cost reduced having fast compression algorithm direct function performing unnecessary compressions 
gnuplot interesting program study closely 
program stores data highly compressible exhibiting ratio average 
way compressed vm policy look quite large memory sizes expecting compress pages required data remains memory 
gnuplot running time dominated large loop iterating twice lot data 
small memory sizes behavior compressed caching policy tries exploit ends benefits seen 
larger sizes benefit substantial reaching 
shown performance difference compressed caching different compression algorithms 
wkdm algorithm achieves best performance vast majority data points due speed comparable compression rates lzo 
lzrw algorithm douglis yields consistently worst results 
fact combined slow machine current standards partially responsible disappointing results douglis observed 
implementation architecture effects past sections showed results sparc mhz machine 
expected faster sparc mhz machine lower compression decompression overhead perform better 
pentium pro mhz machine usually slower sparc machines compressing pages unexpectedly older architecture see remarks memory bandwidth 
shows test programs simulated wkdm lzo architectures 
wkdm performance displayed agrees observations machine speeds 
performance lzo significantly better pentium pro mhz machine expect machine speed 
reason pointed earlier implementation lzo pentium pro hand optimized speed intel assembly language 
surprisingly effect optimization quite significant seen 
ghostscript instance pentium pro faster sparc mhz lzo 
technology trends memory bandwidth problem 
compressed caching benefits increases cpu speed relative disk latency 
different factor comes play disk memory bandwidths taken account 
observation moving data memory takes execution time wkdm compression algorithm 
ratio true pentium pro mhz machine slow memory subsystem sparc mhz fast processor 
significantly better sparc mhz machine 
memory bandwidth limiting factor near 
importantly faster memory architectures rambus soon widespread compression algorithms fully benefit need read contiguous data 
trend favorable 
memory bandwidths historically grown disk bandwidths latencies grown rates 
analysis technology trends dahlin technology trends web page www cs utexas edu users dahlin sensitivity analysis cost benefits compressed caching dependent relative costs compressing page vs fetching page disk 
compression insufficiently fast relative disk paging compressed virtual memory worthwhile 
hand cpu speeds continue increase far faster disk speeds years compressed virtual memory increasingly effective increasingly attractive 
decade cpu speeds increased year disk latency bandwidth increased year 
works increase cpu speeds relative disk speeds third year doubling half years years 
memory size pages normalized paging time gcc wkdm traditional vm pro sparc sparc memory size pages normalized paging time gcc lzo traditional vm pro sparc sparc memory size pages normalized paging time ghostscript wkdm traditional vm pro sparc sparc memory size pages normalized paging time ghostscript lzo traditional vm pro sparc sparc memory size pages normalized paging time rscheme wkdm traditional vm pro sparc sparc memory size pages normalized paging time rscheme lzo traditional vm pro sparc sparc sparc mhz usually better performance pentium pro mhz sparc mhz significantly better 
pentium pro mhz faster hand optimized version lzo algorithm surpassing sparc mhz 
memory size pages normalized paging time espresso wkdm sparc traditional vm ms ms ms ms ms memory size pages normalized paging time gcc wkdm sparc traditional vm ms ms ms ms ms memory size pages normalized paging time gnuplot wkdm sparc traditional vm ms ms ms ms ms memory size pages normalized paging time ghostscript wkdm sparc traditional vm ms ms ms ms ms memory size pages normalized paging time wkdm sparc traditional vm ms ms ms ms ms memory size pages normalized paging time rscheme wkdm sparc traditional vm ms ms ms ms ms sensitivity analysis studying disks various speeds 
conservatively covers cases slower cpus perfect prefetching larger page sizes 
shows plots simulated performance adaptive caching system page compression timings measured mhz ultrasparc 
line represents paging costs simulations disk fault cost 
costs normalized performance conventional lru memory disk page access time curve represents speedup slowdown comes compressed caching 
middle line plot regarded performance machine speed mhz ultrasparc average page fetch cost kb pages ms third average disk seek time fast disk 
note normalized performance terms assuming twice fast disk exactly equivalent assuming twice slow cpu 
time studying case fast disk conservatively covers case perfect prefetching multiple pages twice fast disk equivalent prefetching needed pages seek 
turn conservatively covers case larger page sizes 
sensitivity analysis account fast disks subsumes scenarios 
looking middle line plot see disk page access cost ms programs show reduction paging times percent averaged interesting range memory sizes 
compressed virtual memory clear win disk access cost ms kb page 
line middle taken represent system cpu speed disk costs factor lower ms kb page 
performance system significantly worse speedup obtained 
top line represents system disk page accesses cost ms kb page 
programs degrades performance point compressed caching worthwhile 
going direction technology trends look lower line see performance system twice fast processor relative disk 
programs doubling cpu speed offers significant additional speedup typically decreasing remaining paging costs percent 
related compression algorithms roughly similar known mtf move front algorithm maintains lru ordering unusual partial matching fixed bit word basic granularity operation 
general mtf scheme fairly obvious invented independently times bcw reinvented 
partial matching high bits viewed simple fast approximation delta coding technique purely numeric data sensor input data digitized audio nel 
delta coding form differential coding encodes numerical value numerical difference previous numerical value 
traditional delta coder algorithm encode value difference low bits values mtf dictionary unique previous value 
gooch jones compression algorithm designed memory data 
match algorithm designed hardware implementation similar small dictionary words 
rizzo devised compression algorithm specific memory data 
approach compress away large number zeros data 
rizzo asserts complex modeling costly 
shown possible find regularities great computational expense 
addressed compression machine code shown possible compress machine code factor specially tuned version conventional compressor yu factor compressor understands instruction set eef 
believe similar techniques fast achieve compression ratio similar ratios get data compression ratio code data generally achievable 
size reduction russinovich extremely fast simple untuned general purpose compression algorithm rc 
paging data support assumption full workloads exhibit kind locality needed compressed paging making focus data paging reasonable 
significant previous study compressed caching done douglis implemented compressed virtual memory sprite operating system evaluated decstation times order magnitude slower machines experiments 
douglis results mixed compressed virtual memory beneficial programs detrimental 
apparent dis delta coding misnomer really modeling technique obvious encoding strategy 
performance modeling believe primarily due slow hardware today standards 
supported sensitivity analysis showed times slower machine mhz ultrasparc yield mixed results better compression algorithms available douglis 
discussed earlier russinovich study rc showed simple compression cache achieve significant benefits pc application workload studied 
results accurately reflect trade offs involved 
hand reported compression overheads unrealistically low ms compression intel dx improbable memory bandwidth limitations account 
single factor responsible results high overhead handling page fault incurred ms overhead containing actual seek time 
overhead certainly result slow processor possibly artifact os windows implementation 
study compressed caching performed published done gooch jones 
simulations demonstrate efficacy compressed caching 
additionally addressed problem memory management variable size compressed pages 
experiments lzrw compression algorithm software showed programs kinds reduction paging costs observed 
benefits greater hardware implementation match algorithm 
gooch jones address issue adaptively resizing compressed cache response behavior 
assumed beneficial compress pages avoid disk faults 
clearly true pages compressed memory accesses may suffer decompression overhead disk faults may avoided 
purpose adaptive mechanism determine trade beneficial compression performed 
gooch jones acknowledge compressed cache sizes damage performance 
results strongly suggest need adaptivity test programs exhibit performance deterioration software compression memory sizes 
compressed virtual memory appears quite attractive current machines offering improvement tens percent virtual memory system performance 
improvement largely due increases cpu speeds relative disk speeds substantial additional gains come better compression algorithms successful adaptivity program behavior 
programs examined currently available hardware virtual memory system uses compressed caching incur significantly paging cost 
memory sizes running program suffers tolerable amounts paging compressed caching eliminates paging cost average savings approximately 
gap processor speed disk speed increases benefit continue improve 
recency approach adaptively resizing compression cache provides substantial benefit nearly memory size kinds programs 
tests adaptive resizing provided benefit wide range memory sizes program paging little 
adaptivity perfect small cost may incurred due failed attempts resize cache performs vast majority programs 
capable providing benefit small medium large footprint programs 
wk compression algorithms successfully take advantage regularities memory data providing reasonable compression high speeds 
decades development ziv lempel compression techniques wkdm compressor fared favorably fastest known lz compressors 
research memory data regularities promises provide tighter compression comparable speeds improving performance applicability compressed caching programs 
appears compressed caching idea time come 
hardware trends favor improvement compressed caching performance 
past experiments failed produce positive results improved components required compressed caching successfully applied today 
andrew appel kai li 
virtual memory primitives user programs 
fourth international conference architectural support programming languages operating systems asplos iv pages santa clara california april 
bcw timothy bell john cleary ian witten 
text compression 
prentice hall englewood cliffs new jersey 
dou fred douglis 
compression cache line compression extend physical memory 
proceedings winter usenix conference pages san diego california january 
eef jens ernst william evans christopher fraser steven lucco todd proebsting 
code compression 
proceedings sigplan conference programming language design implementation las vega nevada june 
acm press 
morten gooch jones 
main memory hardware data compression 
nd euromicro conference pages 
ieee computer society press september 
gooch jones 
performance evaluation computer architectures main memory data compression 
journal systems architecture pages 
elsevier science 
nel mark nelson 
data compression book nd ed 
books 
rc mark russinovich bryce 
ram compression analysis february 
reilly online publishing report available ftp 
de info windows win update model html 
luigi rizzo 
fast algorithm ram compression 
operating systems review pages 
sw walter smith robert 
model address oriented software hardware 
th hawaii international conference systems sciences january 
wil paul wilson 
issues strategies heap management memory hierarchies 
oopsla ecoop workshop garbage collection object oriented systems october 
appears sigplan notices march 
wil ross williams 
extremely fast compression algorithm 
data compression conference pages april 
wil paul wilson 
operating system support small objects 
international workshop object orientation operating systems pages palo alto california october 
ieee press 
wjnb paul wilson mark johnstone michael neely david boles 
dynamic storage allocation survey critical review 
international workshop memory management scotland uk 
springer verlag lncs 
paul wilson scott kaplan 
virtual memory tracing user level access protections 
preparation 
wlm paul wilson michael lam thomas moher 
effective static graph reorganization improve locality systems 
proceedings sigplan conference programming language design implementation pages toronto ontario june 
acm press 
published sigplan notices june 
yu tong lai yu 
data compression pc software distribution 
software practice experience november 
