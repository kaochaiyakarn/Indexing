unifying information theoretic framework independent component analysis te won lee mark girolami anthony bell terrence sejnowski salk edu ci paisley ac uk terry salk edu howard hughes medical institute computational neurobiology laboratory salk institute la jolla california usa department computing information systems university paisley pa scotland department biology university california san diego la jolla california usa institut fur technische universitat berlin berlin germany international journal computers mathematics applications press show different theories proposed independent component analysis ica lead iterative learning algorithm blind separation mixed independent sources 
review theories suggest information theory unify lines research 
pearlmutter parra cardoso showed infomax approach bell sejnowski maximum likelihood estimation approach equivalent 
show negentropy maximization equivalent properties approaches yield learning rule fixed nonlinearity 
girolami fyfe shown nonlinear principal component analysis pca algorithm karhunen oja viewed information theoretic principles minimizes sum squares fourth order marginal cumulants approximately minimizes mutual information comon 
lambert proposed different cost functions multichannel blind deconvolution 
show property relates infomax principle 
discuss convergence stability research issues blind source separation 
blind source separation independent component analysis ica received attention potential applications signal processing speech recognition systems telecommunications medical signal processing 
goal ica recover independent sources sensor observations unknown linear mixtures unobserved independent source signals 
contrast correlation transformations principal component analysis pca ica signals nd order statistics reduces higher order statistical dependencies attempting signals independent possible 
different research communities considered analysis independent components 
hand study separating mixed sources observed array sensors classical difficult signal processing problem 
seminal blind source separation herault jutten introduced adaptive algorithm simple feedback architecture able separate unknown independent sources 
approach developed jutten herault karhunen cichocki unbehauen 
comon elaborated concept independent component analysis proposed cost functions related approximate minimization mutual information sensors 
parallel blind source separation studies unsupervised learning rules proposed linsker 
goal maximize mutual information inputs outputs neural network 
approach related principle redundancy reduction suggested barlow coding strategy neurons 
neuron encode features statistically independent possible neurons natural ensemble inputs decorrelation strategy visual processing explored atick 
nadal parga showed low noise case maximum mutual information input output neural network implied output distribution factorial multivariate probability density function factorized product marginal roth baram bell sejnowski independently derived stochastic gradient learning rules maximization applied respectively forecasting time series analysis blind separation sources 
bell sejnowski put blind source separation problem information theoretic framework demonstrated separation deconvolution mixed sources 
adaptive methods plausible neural processing perspective cumulant cost functions proposed comon 
similar adaptive method source separation proposed cardoso laheld 
algorithms performing ica proposed different viewpoints 
maximum likelihood estimation mle approaches ica proposed elaborated pearlmutter parra 
girolami fyfe motivated information theoretic indices exploratory projection pursuit epp marginal negentropy projection index showed kurtosis seeking projection pursuit extract underlying sources linear mixture 
multiple output epp network developed allow full separation underlying sources girolami fyfe 
nonlinear pca algorithms ica developed karhunen xu oja viewed infomax principle approximately minimize sum squares fourth order marginal cumulants comon approximately minimize mutual information network outputs girolami fyfe 
bell sejnowski pointed similarity infomax algorithm algorithm signal processing lambert elucidated connection different cost functions 
show property relates infomax principle seemingly different approaches put unifying framework source separation problem information theoretic approach 
original infomax learning rule blind separation bell sejnowski suitable super gaussian sources 
girolami fyfe derive choosing negentropy projection pursuit index learning rule able blindly separate mixed sub super gaussian general term negentropy relative entropy cover thomas source distributions 
lee girolami sejnowski show learning rule extension infomax principle satisfying general stability criterion preserving simple architecture bell sejnowski 
optimized natural gradient amari equivalently relative gradient cardoso laheld learning rule gives superior convergence 
simulations results real world physiological data show power proposed methods lee girolami sejnowski 
organized follows section formulate problem assumptions usually ica 
section reviews infomax approach bell sejnowski 
section describe respectively relation infomax mle negentropy maximization nonlinear pca higher order statistics property 
section discuss convergence properties stability proposed algorithms 
potential applications research issues discussed section 
problem statement assumptions assume dimensional zero mean vector delta delta delta components mutually independent 
vector corresponds independent scalar valued source signals 
write multivariate vector product marginal independent distributions 
data vector delta delta delta xn observed time point theta scalar matrix 
mixing assumed instantaneous source mixing channel generalizations time delayed convolved sources considered discussion 
instantaneous mixtures occur difference time arrival sensors neglected 
components observed vectors longer independent multivariate satisfy product equality eq 
mutual information observed vector kullback leibler kl divergence multivariate density density written product form log dx mutual information positive equal zero components independent thomas cover 
goal ica find linear transformation dependent sensor signals outputs independent possible wx estimate sources 
sources exactly recovered inverse permutation scale change 
rs wa permutation matrix scaling matrix 
matrices define performance matrix normalized reordered perfect separation leads identity matrix 
linear mixing unmixing model adopt assumptions comon cardoso laheld 
number sensors greater equal number sources 
sources time instant mutually independent 

source normally distributed 

sensor noise low additive noise signals permitted 
assumption needed full rank matrix 
assumption basis ica expressed follows assumption unmixing gaussian sources ill posed sources white random processes 
non white gaussian processes may recovered time decorrelation methods different spectra schuster 
pure gaussian processes rare real data 
assumption necessary satisfy infomax condition mutual information outputs minimized low noise case linsker nadal parga 
imagine noise independent source sensor outputs available number sources noise signal segregated mixtures 
information maximization nadal parga showed low noise case maximum mutual information inputs outputs neural processor implied output distributions factorial 
words maximizing information transfer nonlinear neural network minimizes mutual information outputs factorial code optimization performed synaptic weights nonlinear transfer function 
roth baram bell sejnowski independently derived stochastic gradient learning rules maximization applied respectively forecasting time series analysis blind separation sources 
furthermore deco detailed study unsupervised information theoretic approach ica 
bell sejnowski proposed simple learning algorithm feedforward neural network blindly separates linear mixtures independent sources information maximization 
show maximizing joint entropy output neural processor approximately minimize mutual information output components invertible monotonic nonlinearity wx 
joint entropy outputs neural network delta delta delta yn delta delta delta yn gamma delta delta delta yn marginal entropies outputs delta delta delta yn mutual information 
maximizing delta delta delta yn consists maximizing marginal entropies minimizing mutual information 
outputs amplitude bounded random variables marginal entropies maximum uniform distribution maximizing joint entropy decrease delta delta delta yn mutual information positive 
delta delta delta yn joint entropy sum marginal entropies delta delta delta yn delta delta delta yn maximal value delta delta delta yn achieved mutual information bounded random variables delta delta delta yn zero marginal distribution uniform 
show implies nonlinearity form cumulative density function true source distribution bell sejnowski chose nonlinearity fixed logistic function 
equivalent assuming prior distribution sources super gaussian distribution heavy tails peak centered mean 
weights determined maximizing joint entropy respect rewrite derivative eq respect written terms kl divergence multivariate uniform distribution denoted multivariate uniform estimate 
gammad kp limit transfer function optimized joint entropy maximum 
invertible mapping kl divergence eq equal kl divergence estimate source distribution sources 
kp kp kl divergence invariant invertible transformation 
mutual information outputs zero delta delta delta yn mutual information nonlinearity delta delta delta un zero nonlinearity introduce dependencies 
relation papoulis uniform distribution follows means independent variable distribution approximately form derivative nonlinearity 
case logistic function appropriate shown bottom 
distributions music speech signals approximated bell sejnowski separated mixtures music speech signals infomax logistic activation function 
infomax minimize mutual information 
bell sejnowski answer question thought experiment illustrate mismatch source slope nonlinearity maximal joint entropy value achieved higher joint entropy due lower marginal entropies 
cases infomax minimize mutual information 
occurs excessive mismatch nonlinearity cumulative density function true source distribution 
simple architecture realize mapping single layer feedforward neural network nonlinear output activation function 
nonlinearity essential minimizing mutual information perform ica 
motivation choice sigmoid function provides combination higher order statistics taylor series expansion essential minimize higher order correlations 
learning rule derived maximizing output entropy neural processor proposed bell sejnowski 
relate determinant jacobian matrix papoulis det see bell sejnowski visual interpretation optimal information flow 
evaluating expected value logarithmic representation eq gives output entropy det jg gamma logistic function function input dy dx logistic function exp gammax derivative gamma 
maximized respect equivalent maximizing absolute value jacobian determinant transfer function log det log term eq log det gammat second term product splits sums log terms dependent particular ij log gamma gradient vector log likelihood called score function amari cardoso gamma gamma delta delta delta gamma un un un general learning rule theta gamma gamma efficient way maximize joint entropy follow natural gradient deltaw theta gamma proposed amari cichocki yang equivalently relative gradient cardoso laheld 
gradient simplifies learning rule eq speeds convergence considerably 
show general learning algorithm eq derived theoretical viewpoints mle pearlmutter parra infomax bell sejnowski negentropy maximization girolami fyfe 
elegant way parameterizing learning rule eq separate mixed sub proposed girolami girolami fyfe choosing negentropy projection pursuit index 
girolami parametric density model employed sub super gaussian sources resulting simple form ae tanh super gamma gaussian gamma tanh sub gamma gaussian giving deltaw theta gamma tanh gamma uu diagonal matrix elements sign kurtosis source estimate extended infomax algorithm yields learning rule eq function nonlinearity satisfying general stability criterion cardoso section 
negentropy maximization approach related minimizing mutual information maximizing negentropy girolami 
girolami fyfe motivated information theoretic indices exploratory projection pursuit epp marginal negentropy projection index 
epp statistical method allows structure high dimensional data identified friedman 
achieved projecting data low dimensional subspace searching structure projection 
projections identify non gaussian structure multiple modes interesting point view identifying potential higher order structure highdimensional data 
projections maximally non gaussian highly desirable pursuing informative views data friedman 
girolami showed observed data fits latent variable model everitt conforms deterministic ica mixing model kurtosis seeking projection pursuit extract underlying sources 
multiple output epp network developed allow full separation underlying sources girolami 
jones sibson noted approximately symmetrical gaussian low kurtosis clustered projections difficult identify indices third fourth order moments suggested indices information theoretic criteria 
girolami developed single multiple output algorithms epp negentropy maximization 
showed negentropy maximizing pursuit perform general ica sources may sub super gaussian 
negentropy output neurons stochastically maximized driving distributions maximally away gaussian distributions 
girolami showed maximizing output data negentropy identical minimizing mutual information output data shown equivalent ica observed data modeled sum independent latent variables 
brief derivation follows negentropy defined kl divergence gaussian distribution pg mean covariance cover thomas log pg du vector estimated sources parameters wx 
parametric form output factorable equality holding independent mutual information zero 
assume decorrelated factorable factorized 
kp log pg du delta delta delta un log un pg un dun log pg du log pg du log du log pg du kp gammai sum written sum kl divergences 
substitution pg pg eq follows assumption decorrelated 
term eq negative mutual information gammai 
second term expanded gammai gamma gamma log pg du gammai gamma gamma log det gamma log det terms need justified equality eq eq 
term substituted log det transformation equality eq wx 
second integral log pg du entropy gaussian distribution distribution pg yield covariance matrix cover thomas page 
assumed uncorrelated covariance matrix identity determinant 
follows gammai gamma gamma log negentropy maximized stochastic gradient ascent gammai gamma gamma log input data entropy nonlinear function input data covariance matrix functions weight parameters maximizing sum marginal respect equivalent minimizing mutual information gammai leads exactly learning rule section infomax 
maximizing respect eq gives log du log log det log note eq second terms eq depend gammat derivation learning rule eq depends assumption decorrelated girolami showed slightly different objective function related maximizing marginal leads learning algorithm eq making assumption decorrelated 
maximum likelihood estimation goal mle model observation generated latent variables linear mapping noiseless case parametric density estimator find parameter vector minimizes difference generative model observed distribution 
note considered basis vectors estimate observed vector 
difference estimate observation measured kl divergence log dx gamma log observation parametric estimate distribution 
divergence kp zero estimate matches observation 
pearlmutter parra cardoso showed infomax mle equivalent ica briefly described 
normalized log likelihood log number independent realizations log likelihood converges probability law large numbers expectation log dx note rewritten gamma log dx gamma log dx gamma kp dependent maximizing log likelihood minimizes kl divergence observed density estimated density gamma kp invertible matrix kl divergence invariant invertible transformation minimizing kl divergence eq minimizes kl divergence estimate sources true source distribution 
gamma kp eq eq equivalent ica 
higher order moments cumulants previous sections nonlinearity output approximated true source density 
examine cumulants study higher order correlations sources 
observed vector covariance matrix mutual information eq expressed comon gamma log hx det hx eq diagonal elements covariance matrix 
multivariate negentropy eq marginal log pg dx spatial whitening transformation diagonalization covariance matrix remove second order redundancy data vx denotes whitening transformation matrix det mutual information spatially white data reduces gamma transformation higher order correlations required reduce remaining redundancy vector non gaussian sources 
transformation seeks orthogonal matrix accounts correct rotation data 
comon minimized degree dependence outputs contrast functions approximated edgeworth expansion kl divergence 
determined orthogonal matrix higher order cumulants 
note cumulants describe characteristics non gaussian processes 
truncated edgeworth expansion stuart ord written terms th order cumulants hermite polynomials denoted kn hn respectively pg 










pg denotes gaussian density 
cumulants kn coefficients related form expressed terms moments 
terms orthogonal hermite polynomials defined stuart ord gamma pg computed recursively gamma gamma gamma gamma validity truncated series expansion approximation eq discussed stuart ord 
expansion terms higher fourth order lead excessive fluctuations tails distribution leading potentially negative values 
expansion eq truncated fourth order 
substituting expression marginal eq eq comon assumption signals consideration approximately symmetric third order cumulants negligible contribution eq 
mutual information eq transformed data approximated gamma invariant orthogonal transformation log pg du gamma log det log det gamma log det wh iw gamma log det gamma hg hg entropy normal density matrix determinant equalities employed det wh iw det det det det det result rotation negentropy equal approximation mutual information rewritten gamma orthogonal transformation mutual information data approximately minimized maximizing sum squares fourth order marginal cumulants 
maximizing contrast function approximately equivalent maximizing sum marginal 
corroborates claim maximizing marginal respect minimizes mutual information 
gamma comon proposed contrast function phi max higher order statistics approximated cumulants th order maximization requires intensive computation batch method 
simulation results lee sejnowski lee results experimental data mckeown 
indicate th order cumulants sufficient completely separate mixtures sources 
suggests correct rotation whitened data requires th order statistics increasing number sources 
consistent assumption taylor expansion nonlinear function infomax negentropy maximization mle provides statistics higher fourth order necessary data independent possible 
nonlinear pca nonlinear extension oja principle component analysis pca subspace network oja originally developed karhunen xu apparent connection infomax principle shown separate whitened linear mixtures sources karhunen oja karhunen karhunen wang 
major shortcoming algorithm restricted separation sub gaussian sources stability requirements 
property data 
characteristics led girolami fyfe relate nonlinear pca algorithm infomax principle showing approximate online adaptive equivalent batch algorithm proposed comon 
section summarize results girolami fyfe generalization cope sub super gaussian source distributions 
alternative form nonlinear pca rule satisfies dynamic asymptotic stability criteria algorithm girolami fyfe 
nonlinear pca input signals giving learning rule approximate stochastic gradient descent algorithm minimizes error incurred representing vector nonlinear projection basis reduced dimensionality wf nonlinear estimate denotes estimation error 
minimize cost function find linear transformation giving estimated sources constrained orthonormal ef gamma wf scalar resulting inner product row vector length ones elements 
rewriting eq transpose form gives ef gamma gamma wf wf observed data spatially white follows wg assume unit variance independent components rewrite cost function ef gammaf gamma polynomial gamma hyperbolic nonlinear function cubic dominating element write gamma gamma rightmost term neglected satisfied white standardized data girolami 
cost function rewritten gamma ef gamma gamma ef gamma number sources term gamma expression fourth order marginal cumulant unnormalized kurtosis 
spatially white standardized data cost function considered negative sum marginal fourth order cumulants linearly transformed data 
gamma minimizing cost function eq equivalent maximizing sum fourth order cumulants kurtosis estimated sources positive super gaussian 
optimization eq respect equivalent maximization sum squares marginal fourth order cumulants mixtures strictly super gaussian sources 
function phi max equivalent comon contrast function eq 
comon shown maximizing contrast function approximately minimizes mutual information 
consider case activation function gamma applying reduction cost function form minimizing cost function eq equivalent maximizing sum fourth order cumulants kurtosis estimated sources negative sub gaussian 
contrast function eq different nonlinear term 
negatively cubic term interpreted accounting different prior source distribution 
summarize differences learning rules eq eq formulate general cost function girolami fyfe 
sign sign function nonlinearity output neurons sigma note new form minimization signal representation error criterion valid observed data zero mean spatially white 
mse mean squared error cost function eq relates mutual information shown section assumption probability densities symmetric third order cumulant terms expansion removed fourth order approximation edgeworth expansion 
mutual information approximated follows see section gamma section maximizing marginal respect minimizes mutual information giving gamma corroborates maximizing sum marginal cumulants minimizing mse cost function derived nonlinear pca interpreted approximate information theoretic contrast ica 
algorithms algorithms introduced perform blind deconvolution 
lambert proposes different multichannel blind deconvolution separation deconvolution algorithms classes cost functions 
algorithms similar information theoretic learning algorithm bell sejnowski relationship infomax algorithm obvious 
intuitive explanations proposed bell sejnowski lambert bell girolami fyfe lee bell 
show algorithm information theoretic cost function 
white zero mean stochastic process property satisfies eff subscript denotes time points time shifted version nonlinearity monotone nonlinear function 
property eq states autocorrelation function equal crosscorrelation function process output nonlinearity correlation functions measured lag 
may rewrite property eq spatial processes follows eff subscripts denote independent white stochastic processes 
fact eq differs eq insofar subscripts refer spatial temporal samples allows relate property spatial ica formulation 
left side eq describes second order cross correlation estimated sources right side eq accounts higher order cross correlation estimates due nonlinearity thought combination higher order terms taylor series expansion 
common way derive learning rule blind deconvolution estimate mean squared error estimate true source true source available estimator needed 
valid estimator nonlinear estimate form function reflect information true signals define cost function minimizes mse source estimate nonlinear estimate 
simplicity consider source estimate ef gamma form nonlinearity derived maximum posteriori map model forming conditional log likelihood model observed data follows 
independent source estimated source modeled source plus independent noise source define error variable difference true source signal estimated source signal gamma assumed estimated nonlinear function giving gamma conditional density source variable described map model jz js assume js modeled white zero mean gaussian process giving js exp gamma gamma oe constant oe oe oe variance justification gaussian process conditional estimator js sum 
zero mean independent sources sum gaussian observation due central limit theorem 
substituting eq eq logarithm conditional estimate eq follows log jz log gamma gamma oe log derivative eq respect gives log jz gamma oe log estimation error minimized eq zero solving gives expression gamma oe log comparing eq eq assuming unit variance oe form nonlinear estimator satisfy proportional derivative log density true source distribution 
apply eq initial property rewriting eq matrix form eff gamma eff gamma eff left side eq identity matrix assume gamma multiplying eq gives theta gamma eff optimal nonlinearity applied ica equivalent gamma precisely score function eq 
theta gamma ef exactly convergence criterion infomax learning rule eq 
justification nonlinearity eq corroborates infomax principle application blind source separation blind deconvolution 
properties blind source separation algorithms review discuss important properties ica algorithms source separation problem 
convergence insights sections suggest estimation true source densities crucial extract sources 
researchers tried find separating matrix parametric estimate nonlinearity associated source density pearlmutter parra moulines cardoso xu 
pearlmutter parra proposed contextual ica cica assumed weighted sum parametric logistic functions model source density 
moulines cardoso xu 
model underlying mixtures gaussians showing separate sub super gaussian sources 
parametric modeling approaches general computationally expensive learn complex density parameters 
empirical results lee girolami sejnowski makeig personal communication electroencephalographic eeg data data event related potentials erp cica indicate fail find independent components number time samples small give reliable density estimate data points erps 
simulation results performed researchers show ica algorithms fixed nonlinearity converge separating solution nonlinearity crude approximation underlying sources 
bell sejnowski reported infomax algorithm separate super gaussian sources music speech logistic function imposes super gaussian prior 
lee girolami sejnowski report sound sources pearlmutter parra separated easily faster convergence cica logistic function parametric density estimator 
empirical results suggest simple density estimators may sufficient separate mixed sources 
detailed analysis needs done determine conditions source densities algorithm converges correct separation cardoso 
units learning algorithm left right side eq match rate convergence depends scales axes 
natural gradient amari relative gradient cardoso laheld greatly improves convergence ica making gradient invariant scale axes 
normal entropy gradient euclidean gradient assumes space orthonormal ij unit length points orthogonal direction 
case metric tensor identity matrix ij kl ffi ij kl amari showed riemannian space non orthonormal metric tensor 
fortunately case ica convergence proceed fast space orthonormal rescale euclidean gradient natural gradient follows denotes natural gradient denotes euclidean gradient 
units match convergence optimal 
detailed derivation intuitive explanation amari yang amari 
stability generic stability analysis separating solutions examined cardoso laheld pham amari chen cichocki cardoso 
analysis mean field updates approximated order perturbation parameters separating matrix 
linear approximation near stationary point gradient mean field stationary point 
real part eigenvalues derivative mean field negative parameters average return stationary point 
sufficient condition guaranteeing asymptotic stability derived cardoso ef gamma ef tanh substituting eq eq gives sech gamma ef tanh gamma gamma ef tanh delta ensure sign sign gamma ef tanh learning rule eq sign gamma gamma ef tanh delta discussion unified lines research ica information theoretic framework conjecture framework suited investigate ica different theoretical viewpoints 
applications real world problems extended infomax algorithm applied real world problems analyzing electroencephalographic eeg data makeig jung functional magnetic resonance imaging fmri data mckeown 
makeig 
showed bell sejnowski algorithm able linearly decompose eeg activity artifacts 
jung 
show extended infomax algorithm additionally extract sub gaussian artifacts line noise eye movements 
mckeown 
extended infomax algorithm investigate task related human brain activity fmri data 
assumption sources underlying fmri recordings spatially independent temporally independent case eeg consistently transiently brain activations 
difficult real world problem separation convolved time delayed sources 
blind separation experiments real recorded audio data addressed researchers 
voices recorded room microphones cocktail party problem separated account time delayed convolved sources lambert lee bell torkkola yellin weinstein girolami fyfe cichocki amari cao jutten 
application underwater communication considered li sejnowski 
limitations research ffl nonlinear mixing problem ica starts linear model mixing 
researchers tackled problem nonlinear mixing 
yang amari cichocki taleb jutten lee kohler pajunen karhunen proposed extensions linear mixing combined certain nonlinear mixing models 
approaches include self organizing feature maps identify nonlinear features data lin cowan 
methods generally applicable 
may necessary restrict model sufficiently allow solution 
ffl overcomplete ica overcomplete ica applicable sources sensors sensors sources recovered 
interesting discussion jutten cardoso 
preliminary results lewicki sejnowski suggest overcomplete representation data extent extract independent components priori knowledge source distribution 
ffl noisy ica papers studied ica presence noise nadal parga needs done determine effects noise performance 
noise treated bayesian framework overcomplete basis functions lewicki sejnowski generative models hinton ghahramani 
ffl non stationarity problem sources stationary sources may appear disappear move speaker moving room weight matrix change completely time point 
unsupervised methods needed handle abrupt changes occur real environments 
nadal parga proposed analytical method time dependent mixtures 
murata 
suggest adaptation learning rate cope changing environments 
potential applications ica mentioned 
ffl ica recordings olfactory system hopfield suggested olfactory system may factorial code representation 
currently applying ica data olfactory system white personal communication test hypothesis 
ffl ica communications complex valued signal mixing occurs radio channels 
problem occurs current mobile communication applications cdma code division multiple access systems 
torkkola incorporated prior knowledge source distributions nonlinear transfer function adaptively determine time varying mixing matrices 
simulations showed infomax successfully applied radio signals fading channels 
ffl ica data mining data mining extraction hidden predictive information large databases powerful new technology great potential helping companies focus important information data warehouses 
example moody explored ica financial data modeling 
girolami cichocki amari suggested ica projection pursuit networks data clustering data mining 
ffl biological evidence ica 
learning rules require local information biologically plausible learning rules eq eq single feedforward architecture non local neurons information synaptic weights neighboring neurons connected 
number local learning rules ica recurrent herault jutten architecture linsker network bell sejnowski ica learning rule 
extended exploratory projection pursuit network inhibitory lateral connections girolami fyfe local learning rule 
possible form ica learning rule brain 
field suggested factorial codes efficient coding strategy visual sensory processing ica applied natural images yields localized oriented filters similar visual cortex bell sejnowski 
ica extract local features face recognition systems sejnowski 
factorial coding principles brain areas cerebellum efficient coding schemes motor control prediction 
ffl ica conditional density estimator classification problems roth baram ica conditional density estimator classification time series prediction 
results indicate ica useful conditional density estimator classical pattern recognition issues 
ffl hardware implementation ica analog vlsi chip implementation herault jutten algorithm fabricated cohen 
implementation extended infomax algorithm vlsi challenging goal 
extension time delays convolved mixtures significance practical applications computationally demanding 
acknowledgments lee supported office naval research 
girolami supported ncr financial systems knowledge laboratory advanced technology development division dundee scotland 
bell sejnowski supported howard hughes medical institute office naval research 
amari cichocki yang 

new learning algorithm blind signal separation 
advances neural information processing systems 
amari 
natural gradient works efficiently learning 
neural computation 
press 
amari chen cichocki 
stability analysis adaptive blind source separation 
neural networks 
atick 
information theory provide ecological theory sensory processing 
network barlow 
possible principles underlying transformation sensory messages sensory communication 
ed mit press bartlett sejnowski 
viewpoint invariant face recognition independent component analysis attractor networks 
advances neural information processing systems 
mit press 
bell sejnowski 
information maximization approach blind separation blind deconvolution 
neural computation 
bell sejnowski 

independent components natural scenes edge filters 
vision research 

techniques blind deconvolution equalization blind deconvolution haykin editor prentice hall 

non linear neural algorithm 
neural networks 
cardoso 
laheld 
equivariant adaptive source separation 
ieee trans 
signal processing 
cardoso 

infomax maximum likelihood blind source separation 
ieee signal processing letters 

cardoso 

blind signal processing review 
appear proc 
ieee 
cardoso 
unsupervised adaptive filtering chapter entropic contrasts source separation 
simon haykin editor 
nips blind signal processing organized cichocki 
cohen 
current mode subthreshold mos implementation herault jutten network ieee solid state circuits cichocki unbehauen 
robust learning algorithm blind separation signals 
electronics letters 
cichocki amari cao 
neural network models blind separation time delayed convolved signals 
trans 
fundamentals 
comon 
independent component analysis new concept 
signal processing 
cover thomas 
elements information theory john wiley sons new york 
deco 
information theoretic approach neural computing springer verlag isbn 
everitt 
latent variable chapman hall london 
friedman 
exploratory projection pursuit journal american statistical association 

sources separation priori knowledge maximum likelihood solution eusipco 
girolami self organizing artificial neural networks signal separation ph thesis department computing information systems paisley university scotland 
girolami 
alternative perspective adaptive independent component analysis algorithms 
technical report computing information systems paisley university scotland issn 
girolami fyfe 
stochastic ica contrast maximization oja nonlinear pca algorithm 
international journal neural systems press 
girolami fyfe 
generalised independent component analysis unsupervised learning emergent properties 
proc 
international conference neural networks houston 
girolami fyfe 
extraction independent signal sources exploratory projection pursuit network lateral inhibition 
proceedings vision image signal processing journal 
vol 
girolami fyfe temporal model linear anti hebbian learning 
neural processing letters journal 
girolami cichocki amari 
common neural network model exploratory data analysis independent component analysis 
technical report bip brain information processing group riken japan 
haykin 
adaptive filter theory nd ed prentice hall herault jutten 
space time adaptive signal processing neural network models 
denker 
ed editor neural networks computing aip conference proceedings new york 
american institute physics 
hinton ghahramani 
generative models discovering sparse distributed representations 
philosophical transactions royal society 
hopfield 
olfactory computation object perception 
proc 
natl 
acad 
sci 
usa 
hyvarinen oja 
fast fixed point algorithm independent component analysis 
neural computation 
jones sibson 
projection pursuit 
royal statistical society 
jung humphries lee makeig mckeown sejnowski 
extended ica removes artifacts electroencephalographic recordings 
advances neural information processing systems press 
jutten cardoso 

source separation really blind proc 

jutten herault 
blind separation sources part adaptive algorithm neuromimetic architecture 
signal processing 
karhunen 
separation signals nonlinear pca type learning 
neural networks 
karhunen 
neural approaches independent component analysis source separation 
proc 
esann th european symposium artificial neural networks bruges belgium 
karhunen oja wang 
class neural networks independent component analysis 
ieee trans 
neural networks 
lambert 
multichannel blind deconvolution fir matrix algebra separation multipath mixtures 
thesis university southern california department electrical engineering 
lambert bell 
blind separation multiple speakers multipath environment 
icassp munich germany april 
lee 

independent component analysis theory applications kluwer academic publishers press 
lee bell 
blind source separation real world signals 
ieee proc 
icnn houston 
lee kohler 
blind source separation nonlinear mixing models 
ieee proc 
florida usa 
lee girolami sejnowski independent component analysis extended infomax algorithm mixed sub gaussian super gaussian sources 
neural computation submitted 
lee sejnowski 
independent component analysis sub gaussian super gaussian mixtures 
th joint symposium neural computation institute neural computation 
lewicki sejnowski 
learning nonlinear overcomplete representations efficient coding 
advances neural information processing systems press 
li sejnowski 
adaptive separation mixed broadband sound sources delays beamforming herault jutten network ieee journal oceanic engineering 
lin cowan 
faithful representation separable input distributions 
neural computation 
linsker local synaptic learning rules suffice maximize mutual information linear network 
neural computation 
linsker local learning rule enables information maximization arbitrary input distributions 
neural computation 
moody 
multi effect decompositions financial data modeling 
advances neural information processing systems 
mit press 
makeig bell jung sejnowski 
independent component analysis electroencephalographic data 
advances neural information processing systems mit press cambridge ma 
makeig jung bell sejnowski 
independent component analysis electroencephalographic data 
proceedings national academy sciences 
mckeown jung makeig brown kindermann lee sejnowski transiently time locked fmri activations revealed independent component analysis proceedings national academy sciences 
schuster 
separation independent signals time delayed correlations 
physical review letters 
moulines cardoso 
maximum likelihood blind separation deconvolution noisy signals mixture models 
proc 
icassp munich 
murata mueller amari 
adaptive line learning changing environments 
advances neural information processing systems 
mit press 
nguyen thi 
jutten 
blind source separation convolutive mixtures 
signal processing 
nadal parga non linear neurons low noise limit factorial code maximizes information transfer network 
nadal parga redundancy reduction independent component analysis conditions cumulants adaptive approaches neural computation 
oja 
nonlinear pca learning rule independent component analysis 
neurocomputing 
oja karhunen 
signal separation nonlinear hebbian learning proceedings ieee icnn 
pajunen 
blind source separation binary sources sensors sources 
proc 
icnn houston 
pajunen karhunen 
maximum likelihood approach nonlinear blind source separation 
proceedings int 
conf 
artificial neural networks icann lausanne 
papoulis 
probability random variables stochastic processes 
nd ed 
mcgrawhill new york 
pearlmutter parra 
context sensitive generalization ica 
iconip 

pham jutten 
separation mixture independent sources maximum likelihood approach proc 
eusipco 
pham blind separation instantaneous mixture sources independent component analysis 
ieee trans 
signal proc 
roth baram 
multidimensional density shaping sigmoids 
ieee trans 
neural networks 
stuart ord 
kendall advanced theory statistic distribution theory john wiley new york 
taleb jutten 
nonlinear source separation post nonlinear mixtures 
esann 
torkkola 
blind separation convolved sources information maximization 
ieee workshop neural networks signal processing kyoto japan 

torkkola 
blind separation radio signals fading channels 
advances neural information processing systems press mit press 
hyvarinen oja 
ica fixed point algorithm extraction artifacts eeg 
proc 
ieee nordic signal processing symposium espoo finland 
yang amari 
adaptive line learning algorithms blind separation maximum entropy minimum mutual information 
neural computation 
yang amari cichocki information back propagation blind separation sources non mor linear mixtures 
proc 
icnn houston 
yellin weinstein 
multichannel signal separation methods analysis 
ieee transactions signal processing 
xu 
mse reconstruction principle self organizing nets 
neural networks 
xu cheung yang amari 
maximum equalization entropy maximization mixture cumulative distribution functions 
ieee proc 
icnn houston 

