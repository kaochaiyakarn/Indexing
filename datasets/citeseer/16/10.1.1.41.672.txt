conservation generalization case study draft cullen schaffer department computer science cuny hunter college park avenue new york ny delta delta schaffer hunter cuny edu february results study applicability information loss attribute selection criterion decision tree induction 
results illustrate basic counter intuitive consequences conservation law generalization performance suggest new avenues research 
branches mathematics construction study counterexamples accepted productive mode research 
premise reported similar tack may useful study inductive concept learning 
essential property problem inductive generalization admits general solution 
algorithm learning certain sets concepts necessarily bad learning 
algorithm strictly dominates 
learners differ generalization performance problems superior 
consequence algorithm appropriate sphere application sense worthy study 
practice research inductive learning concentrated quite narrow range algorithms 
specialists care great deal distinctions various approaches decision tree pruning induction decision trees rule sets 
step back consider universe possible learners look alike different 
propose look detail learning method drawn deliberately far corner universe 
counter examples important means correcting intuitions 
learners alike different accepted certain basic ideas constructed 
ideas strong appeal nature inductive generalization guarantees correct universal sense 
learning method investigate designed illustrate point 
show uses value mainly violates clearly intuitions necessary elements successful induction 
fact approach contrary accepted notions applicable plausible cases suggests profitable look narrow range far considered venture explore middle ground 
section briefly review theoretical facts regarding zero sum nature generalization performance 
learners facts imply dominates 
sections consider learners standard highly nonstandard attempt understand superior 
progression sections simple problems slightly harder ones 
ask apply lessons investigations identify realistic cases nonstandard learner superior 
conservation generalization performance consider learner attempting classify test case positive negative suppose case true probability positive 
learner classify test case correctly certain percentage time learning situation situation specified roughly underlying concept kinds noise affecting data size training set 
note corresponding learning situation exactly opposite true 
conditions identical way test case probability gamma positive 
clearly learner classifications wrong situation exactly right assuming crucial point distinguish 
situations correct incorrect classifications effectively cancel stated assumption 
assumption justified roughly speaking attribute vector test case appear training set 
probability test case positive bearing likelihood training data contain bayes rule training set gives information class test case 
consequence restrict attention learning situations classification involves generalization new point attribute space learning situation may paired corresponding 
situations correct incorrect classifications cancel 
group learning situations concept normally find correct classifications outnumber incorrect ones concepts reverse true concepts 
normally case concepts paired way learning situations correct classifications corresponding incorrect classifications 
take concepts learning situations correct incorrect classifications cancel 
sense concepts generalization performance null 
formal version argument leads conservation law gener alization performance 
learner training set size distribution finite space attribute vectors define generalization accuracy ga concept average accuracy learner test cases attribute vectors seen training 
define generalization performance gp ga gamma normalizing percent accuracy achieved random guesser 
basis definitions may state versions conservation law sum noise free concepts gp integral noisy concepts gp dc note conservation law says expected performance 
concepts generalization performance positive negative expected performance may positive 
law say balance maintained concepts leading positive negative generalization performance 
consequence positive better chance performance concepts offset precisely negative worse chance performance 
learners strictly improved 
increase generalization performance concepts corresponding decrease imply increase violating conservation law restriction total remain zero 
put point way learner may dominate sense generalizing concepts better 
constitute strict improvement violate conservation law 
fact law implies superior generalization concepts offset precisely inferior generalization order maintain parity 
importance studying generalization accuracy separately accuracy stressed wolpert explored theoretical ramifications great depth 
wolpert noted conservation law related theoretical results may framed different closely related forms 
fact version stated apply directly experiments reported sections appropriate variant easily constructed basis argument 
experiments wide variety criteria considered selecting attributes decision tree induction information gain gini beta ort 
differences important criteria common 
experiments reported compare prototypical criterion information gain radical alternative information loss 
information gain chooses attribute encodes maximum information class membership cases training set information loss chooses attribute minimum information 
information gain intuitively attractive 
goal build tree gives information possible class membership natural construct informative attributes 
information loss correspondingly hard justify 
conservation law guarantees gain learners dominate loss alternatives 
gain learners superior concepts balanced concepts 
question 
particular know realistic concepts information loss superior 
initial experiments described section compare information loss information gain building level decision trees 
iba langley call decision stumps 
decision stumps proven surprisingly effective wide range real world problems naturally want consider loss gain sophisticated learners 
considering performance small problems concepts binary attributes 
small attribute spaces possible measure performance sizable portion possible concepts 
consider problems plausible attribute spaces 
binary attributes attribute vectors 
deterministic concept may specified binary string length gives class attribute vector 
string identification number concept 
example table shows identification number corresponds attribute concept ab 
attributes class gamma 
table concept id example class ab methodology run trials deterministic concepts 
trials run sample randomly selected concepts 
concept run trials 
trial randomly selected attribute vector set aside testing 
training set size sampled replacement attribute vectors equally remaining gamma attribute vectors 
decision stumps induced information gain information loss criteria select root attribute resulting trees classify test case 
concept accuracy scores range correct 
inducing decision stumps random procedures invoked break ties cases attributes equally ranked splitting criteria number positive negative cases leaf equal 
concept measure accuracy third learner split training data just classifies test case class seen training 
simplicity refer learners gain loss default 
attribute concepts results trials summarized 
point graph indicates accuracy loss gain possible concepts 

loss gain vs loss attribute concepts notable point highlighted graph performance loss gain centered 
expected 
theory requires null generalization performance concepts null performance defined random guesser 
experimental conditions random guessing yield average correct classifications concept 
second notable point graph highly clustered 
loss gain treat attributes attribute values class values symmetrically 
consequence concepts omega omega omega omega interchangeable point view learners yield performance results differ due sampling variation 
convenient discussion prototypical member group interchangeable concepts represent group average performance members group 
divided portions equal performance line 
line concepts gain outperforms loss loss superior 
conservation law requires kind equilibrium classes concepts 
graph goes step farther shows case necessary balance maintained relatively small number concepts gain far superior loss relatively large number loss moderately superior 
upper left corner graph interchangeable concepts represented prototype id 
gain induces perfect decision stump stumps produced loss average correct classifications percent worse chance percent worse gain 
lower left quadrant graph clusters concepts gain outperforms loss loss outperforms gain 
strong interest region learners performing worse random guesser 
purposes important part right half equal performance line 
loss outperforms gain chance 
notable concept groups fall region ffl simplest exemplified prototype ab id includes symmetry attribute disjunctions concepts loss averages correct classifications gain 
ffl second set called concepts 
prototype abc id 
concepts loss averages correct classifications gain 
ffl largest win loss majority concepts exemplified prototype ab ac bc id 
prototype class majority attributes 
loss averages correct classifications concepts gain averages just 
short reasonably realistic concepts loss outperforms gain generalization accuracy percentage points 
summarizes relative performance loss default 
notable point strong correlation performance learners 
correlation easily explained 
minimize information gain loss attempts keep class ratios leaves stump builds close ratio root 
class root loss normally ensure leaves 
successful class predicted leaf prevalent root models produced loss default coincide 
note passing loss gain highly correlated performance 
loss default vs loss attribute concepts graph shows loss default closely related definitely distinct learners 
clearest evidence concept group lower middle graph concepts exemplified prototype ab id loss averages correct classifications default averages 
concepts gain averages correct classifications loss winner generalization accuracy nearly percent 
highlights fact default induction strategy embodying induction bias just way sophisticated algorithms 
learners default performs better chance concepts worse chance 
shows experimental conditions default achieves generalization accuracy percent positive better chance nearly loss default 
goes show problems loss performs generally problems gain performs vice versa 
example learner performance correlates negatively gain complement builds decision stump information gain criterion labels leaves minority class 
learner achieve accuracy percent concepts lower left corner outperforming gain loss nearly percentage points 
performance balanced accuracy percent larger set concepts 
note cases prevalence majority class percent higher performance algorithm attempts determine class 
prevalence majority class lower bound performance acceptable learner 
summarize attribute concepts find ffl general pattern gain wins large margin concepts loss wins smaller margin larger number concepts 
ffl number simple realistic concepts loss superior gain 
ffl concepts loss superior gain default 
reasonable question findings hold increase size attribute space 
attribute concepts trials run possible concepts 
shows random sample 
see immediately general pattern attribute concepts 
gain far outperforms loss loss outperforms gain smaller margin larger number concepts 
course small wins involve accuracy differences percent 
investigating concepts right half equal performance line find number interesting groups loss outperforms gain ffl group composed majority concepts exemplified prototype abcd bcd ab cd abc abcd id 
prototype loss correct classifications compared gain 
note concepts group described including exemplar attribute vector keep id number small examples follow id different concept group 

loss gain vs loss attribute concepts vectors hamming distance exemplar 
prototype exemplar 
ffl second group consists conjunctions disjunctions literals exemplified prototype ab id 
prototype loss correct classifications compared gain 
ffl related group consists slightly complex concepts exemplified prototype ab id 
prototype loss correct classifications gain 
ffl fourth group exemplified concept classifies attribute vectors positive symmetric id 
prototypical concept loss correct classifications gain 
number plausible concepts highly unintuitive learner outperforms standard alternative 
find examples concept groups loss outperforms default gain 
attribute bits attribute bits table bit map concept class class gamma 
ffl group exemplified prototype ab cd id includes symmetry concepts form 
positive examples may described succinctly attributes 
prototype concept loss correct classifications gain default 
ffl second group exemplified prototype ab cd id 
prototype loss correct classifications gain default 
ffl prototype third group somewhat complex describe particularly interesting 
take bits attribute vector gray code dimension theta bit map remaining bits gray code dimension 
positive examples lying open theta box bit map 
table illustrates 
prototypical concept group id loss correct classifications gain default 
functions attributes number possible concepts begins unmanageable 
compromise running trials randomly selected set 
summarizes results 
clarity shows randomly selected points 
general pattern observed large wins gain cases balanced smaller 
loss gain vs loss attribute concepts 
wins loss large number cases 
pattern appear artifact number attributes experiments 
histogram gives precise picture pattern gain outperforms loss percentage points larger number concepts loss superior percentage points 
discover reasonably simple concepts loss outperforms gain chance simply sifting concepts lower right portion 
significant point design concepts basis experience smaller problems 
crucial hope identify similar examples concept space large explore exhaustively 
ffl family concepts designed discovered includes positive examples may described phrase hamming distance exemplars 
loose specification includes distinct concept groups depending relative location exemplars 
prototype id complementary exemplars loss correct classifications gain 
prototype id neighboring exemplars loss cor loss gain number concepts loss outperforms gain various levels 
rect classifications gain 
concept group exemplified second prototype includes members ab cd may specified succinctly boolean form 
family concepts suggested example section 
ffl concepts second loosely related designed family generated combining random rules form ab 
form rules suggested second example section 
prototype concept id generated way loss correct classifications gain 
ffl third example natural generalization attributes majority concept group described section 
prototype id loss correct classifications gain 
concept group provides particularly clear illustration fact loss default closely correlated performance distinct learners 
prototype default correct classifications 
experience smaller attribute spaces suggests general heuristic designing concepts loss outperforms gain 
concepts map positive examples form continuous path width segments length 
diagram define open box table map kind 
loss correct classifications prototype concept generated heuristic id gain 
concepts kind natural sense easily described 
open box simple visual interpretation 
case design principle important section 
experiments far considered experiments involving simple problems learners 
ask apply learned realistic conditions 
sophisticated learner larger attribute space find interesting concepts loss splitting superior gain splitting 
methodology experiments abandon decision stumps favor standard decision tree induction algorithm 
default settings uses information gain criterion select attributes splitting 
compare standard algorithm modified version attribute selection criterion replaced loss equivalent 
simplicity gain loss denote respectively default modified versions 
algorithm report results pruned unpruned trees 
case noted experiments conducted concepts defined vectors binary attributes 
training draw attribute vectors replacement vectors equally full space testing draw way vectors seen training 
noted results averaged trials 
involves line change procedure file info causing return minus usual value 
accuracy size unpruned pruned unpruned pruned gain loss table results majority 
attributes functions far sample thoroughly 
examples section designed discovered 
majority set experiments compares loss gain majority function attributes 
table summarizes results showing average accuracy average number nodes induced trees 
loss unpruned averages percentage points better nearest competitor 
paired test loss unpruned vs gain unpruned significant level 
contiguity second concept derives thin segmented map heuristic noted section 
positive examples concept attributes value contiguous 
table shows sample positive negative examples note attributes considered adjacent 
concepts investigated smaller attribute spaces natural visual context 
results contiguity concept shown table 
unpruned trees paired test shows loss superior significance 
pruned trees test valid learners produce trivial node tree trials 
remaining trials loss superior times binomial test shows significant level 
note small differences accuracy misleading 
produce different trees gain pruned percent errors loss pruned 
positive negative table sample classifications contiguity concept 
accuracy size unpruned pruned unpruned pruned gain loss table results contiguity concept 
exemplars consider concepts randomly selected exemplar attribute vectors 
positive instances hamming distance exemplars 
new set exemplars chosen trial 
concepts inspired exemplar concepts section 
results trials shown table 
unpruned trees paired test shows loss superiority significant level 
pruned trees trials pruned trees single nodes 
tree produced loss superior binomial test shows significant level 
small differences accuracy misleading 
produce different trees gain pruned percent errors loss pruned 
results show marked differences loss gain loss pruned ahead closest competitor percentage points 
pruned trees differ trials paired tests show loss superior gain significance pruned unpruned trees 
results summarized table 
accuracy size unpruned pruned unpruned pruned gain loss table results exemplar concepts 
accuracy size unpruned pruned unpruned pruned gain loss table results exemplar concepts 
class biased random functions feature small attribute space concepts splitting outperforms gain splitting positive negative instances equally prevalent 
leads investigate relative performance changes balance positive negative classes adjusted controlled fashion 
set experiments run trials different randomly selected concept 
generate concept class assigned attribute vectors probability class positive 
value increases increments trial 
summarizes results showing difference accuracy loss pruned gain pruned 
smooth line indicating general trend added system procedure default parameters 
shows relative performance depends strongly peak advantage loss roughly 
typical concepts percent attribute vectors classified positive loss pruned appears outperform gain pruned percentage points 
shows similar summary relative performance gain unpruned 
pattern similar peak 
difference accuracy pruned trees class biased random functions 
appears occur somewhat higher value class biased random concepts eliminate high variability results preceding section run trials fixed 
results summarized table 
pruned unpruned trees loss outperforms gain percentage points difference significant level paired test 
accuracy size unpruned pruned unpruned pruned gain loss table results class biased random concepts 

difference accuracy unpruned trees class biased random functions 
ratio concepts preceding results suggest loss expected outperform gain difficult concepts approximately ratio positive negative instances 
majority concept 
mod attribute vector classified positive value considered binary string evenly divisible 
table summarizes results trials concept 
pruned unpruned trees loss outperforms gain percentage points difference significant level paired test 
accuracy size unpruned pruned unpruned pruned gain loss table results mod concept 
accuracy size unpruned pruned unpruned pruned gain loss default table results majority 
note passing machine learning researchers considered unbiased noise empirical 
results suggest biased noise noise shift negative instance positive vice versa say affect algorithms different way 
return point 
majority examples section shown loss outperforming gain 
examples loss differ significantly default 
natural seen biases default loss closely related 
useful example loss clearly superior gain default order distinguish independent value learner 
majority concept provides example increase number attributes 
trials concept train test examples sampled manner attribute space 
results summarized table 
loss unpruned superior closest competitor percentage points 
difference significant level paired test 
noted experiments smaller attribute spaces majority provides clear example concept default performs worse chance 
sided test shows half percentage point difference default performance achievable random guessing significant level 
majority provides clear example phenomenon described prior consequence conservation law generalization performance 
law holds training sets size upward sloping generalization learning curves concepts balanced downward sloping curves concepts performance remain null 
get data learning curves run trials number training cases increasing increments trial 
number test cases remains fixed 
recall training test cases sampled replacement training sets size normally include examples possible attribute vectors 
crucial measuring generalization accuracy test cases sampled attribute vectors seen training 
summarizes results 
learning curve loss unpruned follows familiar upward sloping pattern curves gain learners strongly downward sloping curve default appears essentially flat 
examples uci data conclude presentation empirical results examples data taken uci repository 
letter recognition instances standard letter recognition data set attributes values set 
class values set fa 
zg 
approximate conditions previous experiments replace attribute binary indicator original attribute takes value median replace class variable binary indicator original class trials conducted single randomly selected sample cases 
trial set randomly divided training test cases 
table summarizes results trials 
unpruned trees paired test shows percent superiority loss significant level 
pruned trees loss gain perform identically trials 
loss superior 
binomial test shows significant level 
smooth trend lines added procedure statistical programming language default parameters 

gain unpruned 
gain pruned 
default 
loss unpruned 
loss pruned generalization learning curves majority 
accuracy size unpruned pruned unpruned pruned gain loss table results problem letter recognition data 
accuracy size unpruned pruned unpruned pruned gain loss table results problem kr kp data 
previous examples small difference accuracies loss pruned gain pruned mainly due fact agree 
cases produce trees classify test cases differently gain pruned percent errors 
king rook vs king pawn second example king rook vs king pawn chess endgame data 
original data set consists cases half classified won 
basis experiments class biased random functions attempt create conditions favorable learners adding biased class noise 
trials full data set randomly divided training set cases test set 
sets cases classified win randomly changed won probability 
table summarizes results 
loss learners superior gain counterparts percentage points case superiority significant paired test level 
gain pruned percent errors 
note performance loss pruned may attributed small trees induces loss unpruned performs significantly better gain pruned yields trees average twice large 
discussion reported set construct counter example prevailing intuitions 
intuitions suggest information gain related attribute selection criteria inherently 
far general ization concerned theory assures notion inherent value meaningless 
criterion bad respect certain concepts learning conditions 
realistic concepts learners results reported show information gain may inferior criterion runs squarely intuitions 
convenience relied mainly artificial data artificial concepts contiguity majority 
easy collect real data yielding similar results 
example take records cases heard supreme court classify case outcome votes individual attributes 
real world example majority concept ought expect generalize better data information loss information gain 
short intuitions suggest information gain superior information loss real problems intuitions proven faulty 
clear course view results supporting adoption information loss attribute selection criterion practical decision tree induction 
strong counter argument simply seen bias loss decision tree learners default learner 
adding loss decision tree algorithm collection tools contains default increase versatility substantially 
hand fact exhibit positive results radically unintuitive attribute selection criterion information loss suggest value investigating broader range criteria research previously concentrated 
example consider criteria ffl ignore class choose attributes far possible split data groups equal size 
tend eliminate short branches useful attributes play part determining class 
ffl depend class distribution smallest branch split 
favor identification small highly pure subgroups larger groups emphasized gain traditional criteria 
ffl measure accuracy split optimistic variant standard pessimistic laplacean metrics 
adjust estimated accuracy optimistic metric adjust outward 
theory guarantees nonstandard criteria useful wide range concepts 
analysis required determine portion range practical importance information loss appropriate concepts interest quite radical alternatives 
focusing attribute selection clear lesson reported general 
constructing counter example prevailing intuitions just looked pruning method designed minimize cross validated performance say decision tree algorithm labels leaves minority class 
learners nonsensical conservation law guarantees domains application 
general point relied intuition guide choices aspects algorithms induction cut step rich array unintuitive alternatives 
general point counter examples help keep perspective achieving 
normal research practice report successes failures 
result journals repositories filled cases methods roughly expected 
safe say research effort expended standard decision tree methods past decade concentrated loss learners repository filled examples appearing justify decision 
counter examples serve remind constitution existing repositories due simply nature real world arbitrary self choice 
richard becker john chambers allan wilks 
new language programming environment data analysis graphics 
wadsworth brooks cole pacific grove california 
leo breiman jerome friedman richard olshen charles stone 
classification regression trees 
wadsworth brooks pacific grove california 
wray buntine 
theory learning classification rules 
phd thesis university technology sydney 
wray buntine tim niblett 
comparison splitting rules decision tree induction 
machine learning 
usama fayyad irani 
attribute selection problem decision tree generation 
proceedings tenth national conference artificial intelligence pages 
robert holte 
simple classification rules perform commonly datasets 
machine learning 
wayne iba pat langley 
induction level decision trees 
machine learning proceedings ninth international conference pages san mateo ca 
morgan kaufmann 
liu white 
importance attribute selection measures decision tree induction 
machine learning 
john mingers 
empirical comparison selection measures decision tree induction 
machine learning 
murphy aha 
uci repository machine learning databases machine readable data repository 
maintained department information computer science university california irvine ca 
data sets available anonymous ftp ics uci edu directory pub machine learning databases 
ross quinlan 
programs machine learning 
morgan kaufmann san mateo ca 
cullen schaffer 
conservation law generalization performance 
william cohen haym hirsh editors machine learning proceedings eleventh international conference 
morgan kaufmann san francisco ca 
david wolpert 
connection sample testing generalization error 
complex systems 
david wolpert 
training set error priori distinctions learning algorithms 
technical report sfi tr santa fe institute 

