incremental eigenanalysis classification peter hall david marshall ralph martin department computer science cardiff university cardiff cf xf peter dave ralph cs cf ac uk eigenspace models convenient way represent sets observations widespread applications including classification 
describe new constructive method incrementally adding observations eigenspace model 
contribution explicitly account change origin change number eigenvectors needed basis set 
method seen considers change origin needed eigenspace model classification purposes 
empirically compare incremental method alternatives literature show method useful classification computes smaller eigenspace model representing observations 
contribution method incrementally computing eigenspace models context classification 
eigenspace models widely computer vision 
applications include face recognition observations images lie linear space formed lexicographically ordered pixels modelling variable geometry observations ordered points curve estimation motion parameters observations comprise matching points consecutive frames 
applications examples classification 
previous workers considered incremental eigenspace computation methods severe limitations classification consider shift origin 
important described 
continuing note notation order 
vectors columns denoted single underline 
matrices denoted double underline 
size vector matrix important denoted subscripts 
particular column vectors matrix denoted superscript superscript vector denotes particular observation set observations treat observations column vectors matrix 
example th column vector matrix 
denote column extension matrix square brackets 
matrix vector appended column 
eigenspace models computed eigenvalue decomposition evd covariance matrix set observations mean observations 
british machine vision conference technique referred principal component analysis karhunen loeve expansion 
models regarded dimensional dimensional space number variables observation number new variables represent observations desired degree accuracy 
centre mean observations axes eigenvectors columns matrix lengths axes square roots eigenvalues diagonal full eigensystem involved certain eigenvalues discarded appropriate criterion reduces system approximation practice low dimensional methods available solve eigenproblems larger number observations see murakami kumar sirovich kirby examples 
note rank covariance may number observations 
alternative approach computing eigenmodel singular value decomposition svd 
clear difference batch incremental methods computing eigenspace models 
batch method computes eigenmodel observations simultaneously 
incremental method computes eigenspace model successively updating earlier model new observations available 
case observations construct eigenspace model training observations instances class 
model decide observations belong class 
despite popularity eigenspace models little vision literature computing incrementally researchers fields addressed issue 
example numerical analysts bunch update evd svd 
built de roberts working signal processing examined error accumulation algorithms 
incremental methods important vision community opportunities offer 
give examples allow construction procedures storage render feasible previously inaccessible problems part learning system observations added eigenmodel 
example security system may need learn new people part classifier system 
applications requires observations reproducible eigenspace representation high fidelity 
define fidelity reciprocal size residue vector turn defined observation 
residue vector part observation perpendicular eigenspace 
second application requires observations classified 
eigenspace model describing training observations larger necessary observations correctly classified belong class 
large observations classified incorrectly belonging 
typically classification decided computing probability measure 
conventional multi dimensional gaussian distribution eigenspace models case surface considered contour standard deviation 
classification measure observation british machine vision conference note incremental method unknown training observations input 
reproduction observations experiments showed improved fidelity true mean observations assuming mean origin 
classification methods experiments showed wrong value generally leads large poor classification 
previous incremental method literature attempt estimate mean origin place 
methods useful reproduction applications appropriate classifiers 
rest give incremental method update mean observations added produce model useful classification 
course models reproduction 
section states problem precisely derives new incremental method 
section go compare alternative incremental methods literature 
experimental results section show method consistently best classifying loses little gains fidelity 
final drawn section 
problem method problem eigenspace model constructed observations comprises mean set eigenvectors eigenvalue associated eigenvector number observations 
mean eigenvectors columns matrix spread observations measured corre sponding eigenvalue diagonal matrix eigenvectors eigenvalues solutions eigenproblem covariance matrix defined previously 
typically eigenvectors eigenvalues kept criteria keeping eigenvectors eigenvalues vary application dependent 
give examples keep largest eigenvectors keep eigenvectors eigenvalue exceed absolute threshold keep largest eigenvectors specified fraction energy eigenvalue spectrum retained 
case remaining eigenvalues corresponding eigenvectors discarded 
eigenvectors eigenvalues remain eigenspace model eigenmodel denote instructive examine discarding eigenvectors eigenvalues spectral decomposition writing matrices block form british machine vision conference provided new observation projected eigenspace model give dimensional vector eigenvectors basis 
projected back dimensional space loss represented residue vector vector lies entirely subspace spanned eigenvectors 
residue vector orthogonal lies complementary space orthogonal vector eigenmodel 
large observation represented eigenmodel 
general incremental method effective able include new orthogonal directions new observations included model short allow number dimensions increase appropriate 
problem address eigenspace model original observations correlation matrix new observation estimate eigenspace model computed previous observations new 
importantly incremental method include additional eigenvector necessary 
equally importantly account change mean effective classification 
derivation method show new eigenspace model incrementally computed evd 
eigenproblem adding easy confirm new mean new covariance matrix 
assume initially turns additional eigenvalue small discard corresponding eigenvector stage 
new eigenvectors rotation current eigenvectors plus new orthogonal unit vector 
unit residue vector obvious choice additional vector 
form unit residue vector new observation lies exactly current eigenspace residue zero british machine vision conference setting substitution equations followed left multiplication gives eigenproblem solution eigenvectors solution eigenvalues note reduced dimensions discarding eigenvalues deemed negligible 
left hand side comprises additive terms 
scale factor terms dimensional vector zeros 
uses equation fact orthogonal vector second term scale factor equations set results compute updated eigenspace model solve intermediate eigenproblem size solution problem yields new eigenvalues directly new eigenvectors computed equation 
call matrix sought note properties solution 
case reduces effect simply rotate current eigenvectors 
happen new eigenvectors scaled shows method converges stable solution limit 
previous british machine vision conference fix attention solutions vision literature allow change dimension methods 
method vision literature allows change origin 
methods basic method solve intermediate eigenproblem form compute rotation matrices eigenvalues takes different form 
compare methods considered basis form intermediate matrix kind decomposition evd svd 
murakami kumar assume fixed mean origin 
compute evd vector zeros matrix zeros diagonal matrix entries square roots 
set zero different values method 
eigenvectors eigenvalues new model scaling term ensures new eigenvectors form orthonormal basis set 
note risk division zero practice means small eigenvectors removed system suffer numerical problems 
addition see lack term second matrix provides rotation scaling new observation lies eigenspace previous observations 
proposed stipulated number eigenvectors kept 
enhanced method providing way determine new observation lies eigenspace previous observations 
chandrasekaran fixed mean origin 
despite fact svd evd solve intermediate problem 
compute svd note set zero 
svd matrix yields left singular vectors singular values right singular vectors new left singular vectors computed exactly equation new singular values identically right singular vectors 
form arises chandrasekaran compute svd matrix column observation evd computed covariance matrix observations 
british machine vision conference authors svd 
example chaudhuri svd recursive estimation motion parameters set consequently allowance change rank covariance matrix 
interest include term 
experimental method results space prevents giving results experimental tests performed 
summarise results results relating fidelity classification little detail 
reader referred detailed report 
matter incremental methods consideration strategy new observation added size new eigenmodel computed decision keep reduce size 
examined effects different methods reduction kept eigenvectors eigenvalues exceeded absolute threshold recommended second kept stipulated number largest eigenvectors recommended third kept largest eigenvectors stipulated fraction energy retained 
measured general accuracy incremental compared batch ensuring evd models compared evd models svd models svd models 
accuracy measured variety ways average alignment closest eigenvectors average difference eigenvalues energy batch model accounted incremental model vice versa 
accurate svd method chandrasekaran 
example energy retained average angular deviation incremental eigenvectors batch eigenvectors conditions method gave eigenvectors average angular deviation measure murakami kumar svd method accurate terms eigenvalues 
method proved best terms energy measures 
desirable similarity batch model task hand represent incrementally observations 
high fidelity important general applications probability measures importance classification discussed section 
cases discarded eigenvectors stipulating number kept 
method kept vector comparative methods 
explicitly keep mean doing allows testing fair method keeps amount information 
fidelity measures observations construct eigenmodel represented model 
measure observation computed size residue vector respect final model 
mean value reside measure error fidelity reciprocal 
classification probability observation belongs eigenspace model british machine vision conference mahalanobis distance 
computed mahalanobis distance observation construct eigenspace model computed probability mean distance classification measure 
prediction regarding classification fixed mean methods observations classified cluster mean origin representing data grows volume 
varying mean methods classification value invariant centred data 
test generated observations dimensional space various values tried 
cluster initially generated mean origin unit standard deviation direction 
progressively shifted cluster away origin cluster standard deviations origin 
typical results shows observations dimensional space eigenvectors kept comparator methods method kept eigenvectors plus mean 
results bear prediction regarding classification 
shows mean residue function distance 
rises distance methods hardly 
eigenvectors kept method gives better performance 
log probability mean mahalanobis distance method chandrasekaran murakami kumar shift standard deviations mean residue error pixel method chandrasekaran murakami kumar shift standard deviations probability residue measures function distance log probability mean mahalanobis distance shift mean residue error shift 
second prediction fixed mean methods prone mis classification 
show synthetic data 
computed eigenmodel set training observations computed fidelity classification measures set test observations 
training set synthesised way shifted 
test set generated cluster centred origin 
clearly distance training set origin increases classification measure fall sets distinct 
typical results 
results bear prediction see probability mis classification method falls distance comparator methods remain constant 
shows test set increasingly distant training set residue error grows methods 
results experiments real image data tested british machine vision conference log probability mean mahalanobis distance observations method chandrasekaran murakami kumar shift standard deviations mean residue pixel method chandrasekaran murakami kumar shift standard deviations probability mis classification residue error observations set larger class log probability mean mahalanobis distance shift mean residue error shift effect discarding eigenvectors keeping fixed number synthetic data 
randomly selected images olivetti database faces computed eigenmodel incremental methods computed residue classification measures 
interest performance method varying numbers eigenvectors eigenvalues discarded 
results shown 
see method consistently classifies observations better fixed mean methods consistently competitive respect mean residue error 
log probability mean mahalanobis distance method chandrasekaran murakami kumar number representative vectors mean residue pixel method chandrasekaran murakami kumar number representative vectors variation classification residue number vectors retained real image data log probability mean mahalanobis distance vectors kept mean residue error vectors kept 
new method incremental eigenspace models updates mean 
experimental results show method better suited classification ap www cam orl uk html british machine vision conference plications methods tested 
additionally method competitive terms fidelity 
conclude method gives best general performance incremental eigenspace computation 
fixed mean methods relied classification 
svd methods generally regarded numerically stable evd 
intend investigate incremental svd shift mean 
early investigations suggest straightforward 
bunch nielsen sorenson 
rank modification eigenproblem 
numerische mathematik 
bunch nielsen 
updating singular value decomposition 
numerische mathematik 
chandrasekaran manjunath wang winkler zhang 
eigenspace update algorithm image analysis 
graphical models image processing 
chaudhuri sharma chatterjee 
recursive motion parameters 
computer vision image understanding 
cootes taylor cooper graham 
training models shape sets examples 
proc 
british machine vision conference 
roberts 
efficient numerically stabilized rank updating 
ieee trans 
acoustics speech signal processing 
hall marshall martin 
incrementally computing eigenspace models 
technical report dept comp 
sci cardiff university 
moghaddam pentland 
probabilistic visual learning object representation 
ieee pami 
murakami kumar 
efficient calculation primary images set images 
ieee pami 
sirovich kirby 
low dimensional procedure characterization human faces 
opt 
soc 
america 

karhunen loeve techniques optimal processing imagery 
optical engineering 
