memory weighted majority prediction recommender systems delgado ishii department intelligence nagoya institute technology cho ku nagoya japan ishii ics ac jp recommender systems learning systems data representing multi user preferences items vote user item matrix try predict preference new items products regarding particular user 
user preferences fact learning target functions 
main objective system filter items predicted preferences user options attractive probably 
study recommender systems viewed pool independent prediction algorithms user situations learner faces sequence trials prediction step 
goal mistakes possible 
interested case learner reasons believe exists target functions pool consistently behaves similar neutral opposite target function trying learn 
learner doesn know priori ones willing discover information benefit prediction 
simple effective method introduced constructing compound algorithm learner 
prediction transformations original preferences recorded votes items users similarity patterns updated weights combining memory individual prediction line weighted majority voting 
prove mistake bounds algorithm closely related total loss best predictor pool 
keywords recommender systems line learning memory algorithms collaborative filtering user modeling 

general task recommender systems predict votes particular user called active user subject item deciding recommendation 
prediction usually done database user votes sample population users 
memory collaborative filtering algorithms commonly recommender systems vote prediction active user indicated subscript done partial information regarding active user set weights calculated directly entire user vote database :10.1.1.21.4665
assumed predicted vote active user item weighted sum votes users ws mean vote user respectively 
number users database non zero weights voted item weights ws express similarity user active user normalizing factor absolute values weights sum unity 
combine statistical formulation vector space model similarity expressed terms cosine normalized inner dot product vectors va vi ws va total number subjects items rated user user components vector deviations mean votes vi vi vi equation statistical correlation coefficient collaborative filtering 
hand term frequency inverse document frequency tf idf coefficient components equation document similarity measure widely information retrieval ir 
case item considered term vote represents term frequency user analogous vector representing document 
founded theory supporting types algorithms variations empirical results shown useful collaborative filtering task :10.1.1.21.4665
study recommender systems viewed pool independent prediction algorithms user situations learner faces sequence trials prediction step goal mistakes possible 
interested case learner reasons believe exists target functions pool consistently behaves similar neutral opposite target function trying learn 
learner doesn know priori ones willing discover information benefit prediction 
simple effective method introduced constructing compound algorithm learner 
prediction transformations original preferences recorded votes items similarity patterns updated weights combining memory individual prediction line weighted majority voting :10.1.1.21.4665
prove mistake bounds algorithm closely related number total loss best predictor pool 

line learning algorithms line algorithms fall computational learning theory colt call mistake bound model 
called learning experts advice line learning model continuous interactive process pool algorithms considered expert predictor weight measure confidence prediction task 
trail valid instance algorithms predictor gives verdict set binary prediction 
weighted majority output ro arg max ro ai ai result prediction th algorithm algorithms voting correct label shown algorithm proceeds performing multiplicative update weights experts denoted wci strategy punishes weights algorithms mistakes rewards leaves unchanged weights correct 
algorithm goes back prediction phase loops 
natural extension notion line prediction collaborative filters binary votes define binary prediction active user item ro wc arg max wca vi ro note weights wca data database 
initialized non negative numbers updated trial 
votes predictions continuous interval weighted sum prediction formula wc wc case notion mistake replaced quantity measures far prediction correct label :10.1.1.21.4665
absolute loss 
algorithm predicts pa correct label va loss trial pa va definition applies algorithms pool master algorithm 
ways perform updates weights 
updates done trial just ones mistake occurs 
common standard weighted majority algorithm updates weights prediction wrong multiplying contributing wrong value fixed value 
winnow algorithm variants multiply weights contributing correct value factor 
note case update equivalent defining weight wa failed trial ga ba ga number times voted correct value ba number times voted wrong value predicting value 
mistake bounded model weighted majority algorithms built nice set theoretical empirical support literature 
unfortunately serious problems applied collaborative filtering 
originally designed pool algorithms experts predicting target function objective give confidence weight subpool algorithms mistakes punishing erratic algorithms bringing benefits majority voting 
said totally independent possibly unrelated opposite target functions prediction 

votes considered absolute values may case different levels similarity exists target functions pool target function predicted 
example binary case algorithm votes consistently target function totally opposite target function learned helpful consider inverse function votes prediction 
contrary line learning algorithms memory algorithms calculate weights solely data available consider votes 
example correlation coefficient weight ws expresses degree similarity users real number interval :10.1.1.21.4665
actual vote deviation mean express opinion user item positive value zero negative value 
easy verify sign contribution vote final equation eq determined table vi vi ws ws table 
voting schema memory algorithms 
interpreted words similar users tend dislike items dissimilar users usually opposite tastes intuitive 
summarize section say weight wc online algorithms represents confidence expert prediction weight ws memory algorithms represents similarity predictors active target function 
related semantics weights cases 
hand calculated predictions done past 
done updates done trial considering value weight previous trial actual prediction calculation historical data accumulated actual trail 
interesting idea explored combination approaches proposal theoretically grounded robust algorithm collaborative filtering 

similarity prediction section introduce main contribution 
version algorithm combines characteristics memory prediction line prediction 
call memory algorithm 
define individual prediction function original vote memory similarity measure users 
explain weights updated prove mistake bounds master algorithm active user closely related total lost best predictor pool 
assumptions predictions master algorithm values target functions pool labels associated instances assumed 
individual prediction lets recall vote vi fact value target function represents user preference prediction 
realizing real line prediction convert value individual prediction denoted xa relates active user user subject individual predictor defined ws formula essence equation prediction relies just user 
note captures users votes depicted table 
includes means active user user calculation memory algorithms 
benefit theoretical analysis users vote haven rated item default vote value mean 
refers total number users pool 
ready integrate concept prediction 
substituting vi xa equation obtain wc xa wc introduce new notation enable refer values occur trail update step occurs 
term update trail refer th trial update step occurs 
assume total trails 
assume master algorithm predicts unknown preference active user item subject applied pool users predictors 
trial corresponds different item subject 
allows drop index making trail denote prediction th user pool update trail accordingly wc denote weights prediction master algorithm label respectively 
assume initial weights wc positive 
sa wc wca init sa wca fin 
update step require updates done trail 
allows calculate exactly total loss master algorithm ma update step weight wc multiplied factor depends wc factor satisfies xa va xa va lemma implies factor exists particular upper lower bound chosen update factor 
lemma 
proof easy check inequality case 
inequality follows convexity function 
convexity implies way writing inequality 
lemma basic lemma derive bounds loss master algorithm 
lemma assume assume assume wca wca xa va wc 
wa fin ln ln init proof deal case trail forces update factors wc desired 
case inequality sa wca xa va sa wca xa va triangle inequality bounded sa wc logarithm sides gives desired result 
sa proving mistake bounds recall predictions labels allowed prediction trail corresponds item described referring item trail equation 
lemma gives upper bound total loss active user lemma conditions lemma satisfied wca ln wca proof lemma follows observation ln lemma 
theorem sequence instances labels 
ma total loss sequence applied active user pool prediction algorithms 
wca init ln wca fin ma proof theorem follows immediately lemma corollary assume pool prediction algorithms active user mi loss th algorithm pool sequence instances 
apply pool equal initial weights total loss ln mi ln sequence 
proof follows theorem fact wa init wa fin 
related works weighted majority binary prediction algorithms recommender systems introduced nakamura abe theory learning binary relations weighted majority voting 
see prediction task learning problem binary relations user related item just case prefers 
valued matrix rows represent users columns represent items instances represent binary relation init 
information matrix 
briefly explain model 
consists instance space target concept class representing class user preferences domain see information matrix represents target binary function entry represents value target function observation matrix time general satisfies entry observed 
prediction function observation matrix time column instance row user fin line incremental learning easily described terms observation matrix follows 
starting initially elements time arbitrary pair predicts value observation matrix learner actual value updated accordingly 
prediction weighted majority voting rows entry column observed weighted weight wi representing believed degree similarity rows users 
prediction algorithm wrong weights updated multiplying contributing correct value contributing wrong value 
process repeated matrix fully observed differ aspects 
order obtain acceptable theoretical analysis strictly followed original idea learning binary relations weighted majority voting 
case prediction task considers master algorithm entire matrix pool independent prediction algorithms 
principle denying intrinsically parallel nature prediction process 
restrict analysis binary domain converting binary scores predictions originally real numbers scale 
somewhat interesting empirical results artificial data comparing memory algorithms pearson algorithm variations fail give conclusive results real data 
fail realize binary relation different independent possibly opposite target functions learnt 
just simple weighted majority schema sufficient information capturing relative cases low vote low confidence high vote treat 
previous gave empirical support slightly different version algorithm 
opportunity experiments eachmovie database raw votes items averaged votes movie categories order avoid sparseness 
obtained encouraging results comparing memorybased algorithms line machine learning algorithms collaborative filtering ones 

summary weighted majority winnow line learning algorithms forte learning simple things really 
algorithms advantages fast incremental able quickly focus relevant predictions adapting target concepts change time having reasonable accuracy coverage tradeoffs founded theoretical base 
hand designed pool algorithms predicting target function 
somewhat inefficient information independent target functions case collaborative filtering recommender systems 
recommender systems incremental nature attempts shown information eachmovie database see www research digital src eachmovie line learning algorithms perform voting prediction task 
memory algorithms widely collaborative filtering recommender systems 
statistical nature need great amount data time order properly give interesting results 
slow response changes react accordingly changes target function strong theoretical background 
positive aspect memory algorithms cover relative voting independent target functions considering similarity users covariance votes correlation coefficient 
novel combination approaches looking forward merge benefits worlds 
memory weighted majority weighted majority algorithm specifically tailored recommender systems 
study recommender systems viewed pool independent prediction algorithms user database situations learner faces sequence trials prediction step 
defining algorithm individual prediction function original vote target function similarity measure users able combine memory line prediction 
weighted majority performed prediction master algorithm active user updating weights trail 
prove upper bounds total loss master algorithm particular active user order log mb proportional size pool total lost best predictor respect user pool 
experiments algorithm performed standard data sets eachmovie microsoft ms web order obtain empirical support ideas theory :10.1.1.21.4665
furthermore continue theoretical machine learning recommender systems promising field 

breese heckerman kadie :10.1.1.21.4665
empirical analysis predictive algorithms collaborative filtering 
proceedings fourteenth conference uncertainty artificial intelligence madison wi 
resnick iacovou bergstrom riedl 
grouplens open architecture collaborative filtering netnews proceedings cscw conference 
delgado ishii ura 
content collaborative information filtering actively learning classify recommend documents matthias gerhard weiss eds 
cooperative agents ii proceedings cia 
lnai series vol 
springer verlag 
salton buckley 
improving retrieval performance relevance feedback journal american society information science vol 
littlestone warmuth 
weighted majority algorithm 
information computation 
littlestone learning quickly irrelevant attributes abound new linear threshold algorithm 
machine learning 
blum line algorithms machine learning survey 
survey talk dagstuhl workshop line algorithms june blum mitchell empirical support winnow weighted majority algorithms results domain 
machine learning 
nakamura abe 
collaborative filtering weighted majority prediction algorithms proceedings icml 
morgan kaufman eds 
goldman warmuth 
learning binary relation weighted majority voting 
machine learning pp 
delgado ishii 
online learning user preferences recommender systems 
proceedings ijcai workshop machine learning information filtering 
billsus pazzani 
learning collaborative filters proceedings icml 
morgan kaufman eds 
