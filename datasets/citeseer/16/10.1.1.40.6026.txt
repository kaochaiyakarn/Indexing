comparison cellular encoding direct encoding genetic neural networks fr ed eric gruau darrell whitley larry cwi kruislaan computer science dept computer science dept sj amsterdam colorado state university colorado state university netherlands fort collins usa fort collins usa gruau cwi nl whitley cs colostate edu cs colostate edu compares efficiency encoding schemes artificial neural networks optimized evolutionary algorithms 
direct encoding encodes weights priori fixed neural network architecture 
cellular encoding encodes weights architecture neural network 
previous studies direct encoding cellular encoding create neural networks balancing poles attached cart fixed track 
poles balanced controller pushes cart left right 
cases velocity information pole cart provided input cases network learn balance single pole velocity information 
careful study behavior systems suggests possible balance single pole velocity information input learning compute velocity 
new fitness function introduced forces neural network compute velocity 
new fitness function tuning syntactic constraints cellular encoding achieve tenfold speedup previous study solve difficult problem balancing poles information velocity provided input 
reinforcement learning problems training set input output pairs exist 
behavior particular instantiation artificial neural network ann supply reinforcement signal evaluates performance providing positive negative feedback 
particularly difficult reinforcement learning problems require recurrent network 
genetic algorithms appear particularly suited training neural networks class reinforcement learning problems require feedback performance 
hand known problems attempting train neural networks evolutionary algorithms emphasize recombination 
wieland introduced interesting test problems involve balancing poles fixed cart moving finite track 
possible actions system include applying force cart push left right 
controller fixed force applied cart force may variable depending output controller 
note corresponds having discrete bang bang output action continuous variable force output action 
traditionally inputs system pole angle pole velocity cart position cart velocity 
wieland offered solution pole balancing problem pole angle cart position inputs 
wieland solution involves fully recurrent network potentially learn compute velocity information 
classic pole balancing problem velocity information supplied input pole angle restricted sigma degrees case linear solution known exist 
experimental studies suggest linear solutions exist problems involve balancing poles velocity information provided 
produced results similar wieland poles evolved cellular encodings neural networks solve problems 
recurrent network required velocity information provided neural networks recurrent connections able balance poles multiple initial states time steps 
networks able pole balancing implementing cyclic strategies keep system state oscillation 
new evaluation function introduced penalizes oscillation forces networks learn compute velocity information 
networks developed ways current study 
networks evolved cellular encoding 
method generates weights architectures neural networks 
second neural networks fixed architectures trained genetic algorithm find weights 
approach originally wieland weight encoded bits values distributed values gamma gamma gamma representation zero 
networks varying sizes tried best network solving problem selected 
fixed architecture networks included bias input wieland 
number neurons different reported wieland 
relatively small populations strings high mutation rate 
goal continue comparison direct encoding cellular encoding started 
new evaluation function new syntactic constraints allowed solve simpler version pole balancing problem times faster cellular encoding ce 
speedup terms number evaluations wall clock time 
solve difficult version pole balancing problem poles balanced information velocity supplied 
previous attempted problem problem addressed wieland 
general results indicate cellular encoding able optimize architecture weights time 
contrast direct encodings assumes priori architecture suboptimal general case 
achieving performance direct encoding requires testing architectures individually 
due number architectures parameters available take considerable effort 
result better performance obtained cellular encoding 
benefit cellular encoding structure resulting neural networks fits problem hand analyzed 
particular velocity information supplied input linear solutions pole pole problems produced 
linear solutions obvious represent somewhat unexpected result problem simultaneously balancing poles 
case velocity known solutions hidden units produced 
reproducing wieland results section describes experiments done direct encoding weights 
balancing pole input problem 
pole angle angular velocity cart position velocity 
variant pole balancing problem involves balancing pole velocity information case pole angle cart position inputs 
variant involves balancing poles different length mass length mass small pole percent big pole 
problem inputs angle velocity pole plus cart position velocity 
difficult problem combination balancing poles velocity information 
problem inputs cart position inputs angles large small pole 
big pole length mass meter 
length cart track meters 
inputs scaled input neural network 
cart position scaled sigma meters input range sigma 
pole angles ranging sigma degrees scaled sigma 
pole angles outside range produce failure signal 
cart velocity ranging sigma meters second pole angular velocity ranging sigma degrees second scaled sigma 
velocities outside ranges allowed resulting inputs scaled accordingly produce inputs outside sigma range 
simplest measure performance testing long neural controller balance pole allowing poles angles go outside range sigma degrees crashing ends tracks 
attempted reproduce wieland results poles 
population size experiments mutation rate 
exactly genetic algorithm wieland genitor software package steady state genetic algorithm linear ranking selection purposes 
fixed mutation rate adaptive mutation strategy 
results reported adaptive mutation strategy rate mutation increased parents similar 
genitor uses selection bias linear ranked gene pool select genes crossover 
selection bias indicating best individual population twice selected crossover mean individual population 
networks wieland fixed number fully connected hidden units inputs specified specific problem 
addition wieland inputs input unit fixed value added bias 
hidden units arbitrarily chosen output unit 
network fully connected matter unit selected output long unit trial 
classic pole input problem solved single hidden unit input units including bias unit 
pole input problem solved hidden units input units including bias input 
pole input problem solved hidden units input units including bias unit 
unable find architecture solve pole problem velocity information 
networks slightly different wieland bias input 
wieland neurons solve pole input problem neurons solve pole input problem 
wieland report solution pole problem velocity information 
network sizes genetic algorithm parameters chosen trials represent best performance obtain 
results experiments reported table compared obtained cellular encoding 
cellular encoding real weights cellular encoding language local graph transformations controls division cells grow artificial neural network 
cell input site output site linked cells directed ordered links 
cell possesses list internal registers represent local memory 
registers initialized default value duplicated cell division occurs 
registers contain neuron attributes weights threshold value 
graph transformations classified cell divisions modifications cell registers 
mother illustration cpo division mother cell divides child cells 
cell division replaces cell called parent cell cells called child cells 
cell division specify child cells linked 
practical purposes give name graph transformation names turn manipulated genetic algorithm 
sequential division denoted seq child cell inherits input links second child cell inherits output links child cell connected second child cell 
parallel division denoted par child cells inherit input output links parent cell 
link duplicated 
child cells connected 
examples types cell division gruau whitley 
general particular cell division specified indicating child cell link inherited mother cell 
division called cpo illustrated 
similar sequential division output links duplicated child cells 
describing instructions modify cell registers useful describe neural network unit performs computation 
default value weights bias 
default transfer function identity 
neuron computes weighted sum inputs applies transfer function multiplies result coefficient default 
see figures examples neural networks 
instruction mult sets coefficient worth mentioning multiplicative coefficient imposed weights scale gamma 
result genetic program directly produce anns hidden units 
produced network units network simplified ann hidden units neurons computing identity function see section analysis ann structure add constant output 
fact encoding generated excess neurons obtain weights greater 
multiplicative coefficient encoding produce directly anns hidden units assuming hidden units required 
ann computation performed integers activity coded bits corresponds activity 
instruction sets bias 
instruction step sets transfer function clipped linear function gamma 
weight instruction modify cell registers 
integer parameters specifying real number floating point notation real equal integer divided 
resulting weight distributions similar identical wieland 
parameters set weights input links 
neuron happens input links weights input links set default value 
cellular code grammar tree nodes labeled names graph transformations 
cell carries duplicate copy grammar tree internal register called reading head points particular position grammar tree 
step development cell executes graph transformation pointed reading head advances reading head left right subtree 
cells terminate development lose reading heads neurons 
order cells execute graph transformations determined follows cell executed graph transformation enters fifo queue 
cell execute head fifo queue 
cell divides child reads left subtree enters fifo queue 
order execution tries model happen cells active parallel 
ensures cell active twice cell active 
control program symbol progn 
program symbol progn arbitrary number subtrees subtrees executed starting subtree number 
consider control problem number control variables number sensors want solve control problem ann input units output units 
development initial graph cells consists input units connected reading cell connected output units 
input output units read code fixed development 
effective cells pointers place holders inputs outputs 
initial reading cell reads root grammar tree 
divide reads generate cells eventually generate final decoded ann 
syntactic constraints genetic operators bnf grammar general technique specify subset syntactically correct grammar trees underlying data structure 
default data structure tree 
data structure tree list set integer 
syntactic constraints trees produced bnf grammar recursive nonterminal type tree associated range specifies lower upper bound number recursive rewritings 
experiments set lower bound upper bound number neurons final neural network architecture 
list set data axiom 
nn 
nn par nn nn 
cpo nn nn 
seq nn nn 
attribute 
attribute 
progn set weight list integer integer integer step syntactic constraints restrict space possible solutions structure set range number elements structures 
integer data structure set lower bound upper bound random integer value 
list set data structures described set subtrees called elements 
list data structure store vector subtrees 
subtrees derived elements 
subtrees may derived element 
set data structure list data structure subtrees derived different element 
example rule list generates trees 
rule set generates trees 
shows syntactic constraints 
nonterminal nn 
recursive 
rewritten recursively times 
time rewritten recursively generates division adds new ann unit 
final number ann units 
note particular case size proportional size genome constraints grammar fig 
controls size genome result directly constraints growth controls size 
nonterminal attribute 
implement subset possible specializations ann units 
weights set values gamma 
multiplicative coefficient set value ranges gamma bias set value gamma 
transfer function set clipped linear function default transfer function 
lower bound set range specializations generated case ann unit compute sum inputs apply identity function 
upper bound set specializations generated 
case neuron weighted sum subtract bias apply clipped linear function multiply coefficient 
crossover 
crossover implemented cellular codes syntactically correct produce offspring syntactically correct parsed bnf grammar 
terminal grammar tree primary label 
primary label terminal name nonterminal generated 
crossover tree may occur root symbols subtrees exchanged primary label 
simple mechanism ensures closure crossover operator respect syntactic constraints 
crossover trees classic crossover genetic programming advocated koza subtrees exchanged 
crossover integers disabled 
crossover lists sets implemented crossover bit strings underlying arrangement data structures string 
mutation 
mutate node tree labeled terminal replace subtree node single node labeled nonterminal parent rewrite tree bnf grammar 
mutate list set array data structure randomly add suppress element 
mutate integer add random value uniformly distributed sigma 
time offspring created nodes mutated small probability 
tree list set nodes mutation rate integer node 
comparative experiments cellular encoding direct encoding parallel genetic programming algorithm subpopulations 
complete description algorithm gruau 
parallel genetic program combines advantages massively parallel model island model 
individuals distributed islands island grid forming torus 
islands arranged grid individuals migrate neighbor islands 
sites grid occupied 
density population kept 
mating site randomly chosen grid 
successive random walks performed best individuals random walks mated 
offspring placed site migration rate 
individual exchanged adjacent processors individuals created crossover 
mimd parallel machine paragon processors 
subpopulations individuals resides processor total population 
evaluation function defining fitness function critical impact quality results mainly fitness shapes solution 
initial experiments fitness number time steps pole stay balanced 
early experiments indicated learning control system single random start state easy achieve resulting generalization poor 
alleviate problem neural network balance pole starting initial positions time steps positions 
initial positions chosen frontier domain impossible recover 
enabled generate recurrent anns balance pole cart cart position pole angle input 
assumption recurrent connections possible remember previous position cart pole compute speed 
discovered hypothesis appear true 
setting described evolved networks see recurrent links important 
genetic program generate ann hidden units hidden units balance pole initial positions time steps 
obviously network compute speed 
able balance pole cart keeping swinging left right 
amplitude oscillation subjected small increase decrease time 
due round errors 
starting resting position time step sufficient amplitude oscillation reach fatal value system crashes system crash sooner 
force ann compute speed included fitness term penalizes oscillations 
new fitness function 
single initial position variables zero big pole angle value 
fitness complicated single fitness case suf ficient provide precision allow generalization 
degrees pole problem degrees pole problem 
cart poles simulated time steps 
fitness weighted sum fitnesses 
fitness number time steps system stay balanced divided 
second fitness pole balanced time steps constant multiplied inverse sum absolute values normalized variables cart position cart velocity big pole angle big pole velocity time steps 
constant chosen 
obviously pole swinging angle velocities going high absolute value inverse sum going low 
second fitness directly penalizes swinging 
penalizing oscillation fitness function biased finding neural networks return pole centered upright position 
second fitness greater set 
total fitness fitness plus second fitness 
runs generalization tests ffl balanced pole time steps 
ffl checked sure ann able bring cart pole back centered upright position started initial position training set 
note result fixed architecture reported old fitness function weakens comparison 
single pole problem experimented variants classic problem balancing single pole attached cart finite track 
common variant problem uses inputs pole angle velocity cart position velocity bang bang control strategy 
wieland tested variant inputs continuous control strategy force applied cart output unit activity multiplied 
magnitude force applied smoothly varies opposed bang bang control 
wieland experimented variant problem inputs pole angle cart position ann parallel dynamic compute relevant velocity information 
problem solved continuous control strategy 
parallel dynamic case initial activation neurons set 
computation network relaxed times parallel output considered 
ce code direct code problem learning gen learning gen pole inputs pole inputs poles inputs poles inputs table results experiments 
learning column reports average number evaluations needed find solution gen column report performance generalization ran generalization test whitley initial settings cart pole generated 
normalized input variables representing cart position cart velocity pole position pole velocity take values 
note values scale positions sigma input range 
results test cases 
generalization tested counting number test cases ann balance pole time steps 
whitley reported average successes test cases inputs bang bang control strategy 
results obtained cellular encoded networks fixed architectures direct encoding implementation wieland methods reported table 
result represents average experiments 
learning time increases generalization decreases neural network learn compute velocity information 
due lack precision speed computed neural network 
cellular encoding needs evaluations fixed architecture coding solve problems 
noted numbers reported fixed architecture best obtain trials different architectures 
took great deal human intervention get fixed architecture solve problems rapidly reliably 
fixed encoding needed evaluations solve pole problem inputs velocity pole problem inputs including velocity 
time spent finding parameters quick reliable convergence shown table 
unable find fixed architecture solution pole input problem velocity information 
cellular encoding find solution problem 
poles table reports results experiments involving poles 
anns input units respectively cart position cart velocity big pole position big pole velocity small pole position small pole velocity 
failure angle pole problem sigma degrees 
sampling full ranges input variables impractical testing generalization pole system controlled narrow range values 
defined new ranges variables training testing purposes 
actual input ranges change values training testing 
intervals cart variable ones learning set position varies sigma meters velocity varies sigma meters second 
interval big pole 
angle varies sigma degrees pole velocity sigma degrees second 
test cases settings intervals reduced ranges 
small pole set zero moves quickly 
poles balanced average initial settings reduced ranges 
ran experiment poles information pole cart velocities 
ann inputs cart position big pole angle small pole angle 
run single experiment larger number processors processors larger population processor total population 
solution generations 
solution big pole set degrees ann able bring poles cart back null position time steps 
able solve problem fixed architecture encoding 
analysis ann structures pole input problem velocity genetic program solutions input units 
automatically generated minimal networks 
shows example neural networks generated genetic program pole problem information velocity 
anns show features enable compute speed 
minimum path input output unit smaller 
reason ann relaxed times output considered 
minimum path greater current state cart pole affect control decision taken ann time step 
second intermediate neurons retain information previous state 
result possible paths input output units length strictly greater 
information flows neurons takes time steps go input layer output layer 
neurons register previous cart position pole position compute speed 
ann represented hidden units clipped linear transfer function 
transfer functions piecewise linear equal identity origin 
neurons saturated 
net input stays linear part transfer function 
replace transfer function identity function 
ann simplified hand merging ann units compute identity 
network mathematically equivalent ann shown relaxed times hidden units 
tested solution verify working way 
ann simple behavior fully understood 
pole far equilibrium variables cart position pole angle high hidden units saturated information velocities 
resulting control smooth see pole swinging 
variables smaller hidden units saturated compute velocities pole quickly brought rest 
modes computation clearly distinguished 
test case pole angle set degrees far equilibrium pole swings 
time steps changes mode suddenly pole smoothly brought back rest 
poles velocity 
neural networks generated genetic program control poles hidden units 
remaining hidden units simplified 
wieland suggested pole problem difficult wieland neurons solve problem 
designed hand coded solution problem running gp hidden units 
threshold output unit weights solution scaling gamma gamma gamma gamma 
solution genetic algorithm obvious 
begins negative weights 
third fourth negative weights indicate big pole right resp 
left push car left resp 
ann generated pole velocity information 
inputs shown top network diagram 
specify ann unit clipped linear transfer function schematization graph function 
inside circle represent multiplicative coefficient different 
bias represented 
represent weights different 
equivalent ann obtained simplification 
hidden units 
ann gp balancing poles 
hidden units 
ann mathematically equivalent ann hidden units 
right 
counterintuitive going big pole bend worse 
weight weight positive small pole angle bigger big pole absolute value cart pushed right resp 
left poles brought back null position 
small negative weights tend push cart back center 
poles velocity 
shows mathematical equivalent ann hidden unit relaxed time 
picked solution genetic algorithm hidden units computing identity 
simplified 
need keep hidden unit maintain delay possible compute speed 
compares new version cellular encoding direct encoding problem balancing poles cart moving fixed track 
new version cellular encoding allows data structures encoding weights neural network uses syntactic constraints bound certain properties neural networks number hidden units 
real valued weights conjunction cellular encoding addresses criticism previous cellular encoding restricted evolving boolean networks 
compared cellular encoding direct encoding method produces architectures fixed size 
cellular encoding required evaluations solve problems direct encoding method 
large amount human effort intervention experience required find proper architecture direct encoding attractive cellular encoding 
solutions cellular encoding smaller obtained methods architectures developed wieland problems 
advantage cellular encoding automatically find small architectures structure complexity fit specificity problem 
cases studies architectures hidden units architectures simplify hidden units 
provides new insight complexity problem solving 
particular results indicate competitive linear solutions exist problem balancing pole sigma degrees problem balancing poles 
problem balancing poles velocity information represents complex control task 
problem solved cellular encoding evolved networks hidden units 
problem solved fixed architecture coding 
supported part nsf iri tmr european fellowship fr ed eric gruau 
san diego super computer center providing access paragon 
bibliography collins jefferson 
selection massively parallel genetic algorithms 
proc 
th international conf 
genetic algorithms morgan kaufmann pp 
gruau 
neural network synthesis cellular encoding genetic algorithm phd thesis ecole normale sup erieure de lyon 
anonymous ftp lip ens lyon fr pub phd phd ps english phd ps french gruau whitley 
adding learning cellular developmental process comparative study evolutionary computation 
gruau 
automatic definition modular neural networks adaptive behavior koza 
genetic programming programming computers means natural selection ma mit press 
born 
parallel genetic algorithm function optimizer parallel computing 
whitley dominic das anderson 
genetic reinforcement learning problems learning 
whitley gruau 
cellular encoding applied proc 
th international conf 
genetic algorithms morgan kaufmann pp 
wieland 
evolving controls unstable systems 
connectionist models proc 
summer school morgan kaufmann pp 
