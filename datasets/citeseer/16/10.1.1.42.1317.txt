theoretical results nonlinear principal components analysis edward september nonlinear principal components analysis nlpca neural networks feedforward autoassociative networks layers 
third layer fewer nodes input output layers 
nlpca shown give better solutions feature extraction problems existing methods little know theoretical properties method estimates 
studies nlpca 
proposes geometric interpretation showing nlpca fits lower dimensional curve surface training data 
layers project observations curve surface giving scores 
layers define curve surface 
layers continuous function show implications nlpca projections suboptimal producing larger approximation error nlpca unable model curves surfaces intersect nlpca parameterize curves parameterizations having discontinuous jumps 
establish results identification score values discuss implications interpreting score values 
discuss relationship nlpca principal curves surfaces nonlinear feature extraction method 
keywords nonlinear principal components analysis feature extraction data compression principal curves principal surfaces 
nonlinear principal components analysis nlpca approach nonlinear feature extraction 
uses layer autoassociative neural networks bottleneck layer nodes fig 
reduce dimension input variables 
second fourth layers network sigmoidal activation functions layers layers model nonlinear functions 
third layer fewer nodes fifth 
values output nodes layer trained approximate inputs 
network trained bottleneck node activation values layer give lower dimensional representation inputs 
oe oe oe qs delta delta delta delta delta delta delta au qs sw oe oe oe sw delta delta delta delta delta delta delta qs qs au layer nlpca neural network architecture 
linear function denoted sigmoidal functions oe 
nlpca related nonlinear feature extraction methods successfully applied different problems 
example shows nlpca applied image compression problems 
show version nlpca data compression problem 
shows version nlpca construct control charts 
examples suggest nlpca promising method 
little known theoretical properties nlpca estimates 
purpose investigate properties nlpca estimates 
section proposes geometric interpretation nlpca 
nlpca reduces dimension inputs fitting curve surface data 
layers network project original data curve surface activation values bottleneck node called scores give location projection 
layers define curve surface 
approach similar principal curves surfaces different nonlinear feature extraction method 
layers model continuous functions section examines implications fact 
projections nonlinear curves surfaces suboptimal inputs mapped point curve surface closest increases training error leads incorrect results 
second nlpca model curves surfaces intersect example circles 
third nlpca parameterize curves surfaces parameterizations discontinuous jumps 
section examines identification score values 
score values depend curve surface parameterized nlpca determined starting weight values nonlinear optimization routine 
show score values identified homeomorphic transformation discuss implications result interpreting score values 
summarize give directions research section 
feature extraction methods section gives overview feature extraction problem approaches solving 
theta matrix contains observations subjects cases items variables process variables attributes 
denote th row vector theta th column vector theta 
loss generality assume columns mean centered gamma average average ij superficial dimension observations dependencies columns intrinsic dimension observations labeled smaller situations may want extract new set variables called features contain information smaller intrinsic dimension 
call values feature variables scores denote theta matrix observations scores hypothesized related follows ffl 
ffl 
ffl ip ffl theta vector noise smooth function 
feature extraction problem find estimates function called globally parameterized surface vector describes location point relative parameterization surface unidimensional surface called curve 
example unit circle example dimensional parameterized curve cos sin 
definition curve surface unique different functions define curve surface 
example unit circle defined cos sin constant 
curve defines curve original different parameterization 
curves parameterized arc length gamma length curve calculus arc length curve df ds ds gamma curve parameterized arc length unit speed property defined df ds differential geometry smooth curve parameterized arc length 
example vector unit length unit speed property 
unit circle unit speed reparameterized unit circle 
curve dimensional surface parameterized arc length parameterization unique choice origin sign flips defining parameterizations higher dimensional surfaces complicated 
unit speed property generalized surfaces terms areas volumes parameterizations unique 
see discussion 
principal components analysis principal components analysis established feature extraction method assumes linear 
spectral decomposition uu theta orthogonal matrix column vectors unit length eigenvectors diagonal matrix diagonal elements delta delta delta eigenvalues 
principal component transformation xu theta matrix score values 
pca transformation important properties see 

principal component transformation change basis basis unit vectors basis consisting column vectors column vector theta called th principal axis principal direction 

total variance mean centered columns ij tr tr gg variance th principal axis var xg xg gg 
standardized linear combination rows larger variance max var xa var xu likewise constrained orthogonal gamma variance maximum value 
subspace spanned eigenvectors smaller mean square deviations original matrix subspace dimension columns theta basis subspace min kx gamma proj xk kx gamma proj xk proj denotes projection row vectors subspace spanned columns nlpca generalizes objective function 
methods generalize including 
see section additional discussion approaches 
principal curves surfaces principal curves pc proposed 
pca finds vector satisfies minimum distance property 
pc extends pca fitting unit speed curve similar objective function 
lengths projections pca vector principal curves method generalizes expression defining projection index map observations point curve closest sup fs kx gamma inf kx gamma kg projection index evaluated gives arc length fixed origin point closest points convention selects largest score value 
points projected point curve called ambiguity points 
fig 
illustrates role curve parabolic shape example observation equally close branches parabola projections sides shown 
projection index selects note definition parabola unit speed property 
simplification clarify discussion 
projection right branch score value arc length origin greater assuming parameterized increases axis symmetry phi phi phi phi phi phi phi phi arc length graphical illustration projection index principal curves method estimates squares objective function min kx gamma composition functions gives dimensional coordinates projection curve objective function minimized principal curve algorithm alternates estimating fixed estimating fixed 
defined linear principal curves algorithm equivalent power method extracting dominant eigenvalue extracts principal component 
formally random vector defined continuous probability density 
defines principal curve set curves intersect self consistent js curve self consistent point call mean points support projected definition principal curves show curve principal curve satisfies objective function 
dimensional principal surfaces ps proposed extended higher dimensional surfaces 
principal surfaces extend pcs fitting surface satisfies minimum distance objective function similar 
projection index dimensional surface sup sup fs kx gamma inf kx gamma kg surface estimated objective function min kx gamma principal surface algorithm similar principal curve algorithm 
finding parameterization dimensional surface difficult finding curve 
curve assumes continuous result discussed shows parameterization determined choice origin scaling principal curves defined unit speed origin pc defined endpoints data 
indeterminacy problematic surfaces due factors rotations 
example consider parameterizing hemisphere theta theta 
usual spherical coordinates vector lying sphere angle axis projection plane latitude angle axis longitude 
relationship theta sin cos sin sin cos ways parameterizing hemisphere 
alternatively hemisphere parameterized projecting plane setting gamma gamma third parameterization projections 
projection point plane point intersection plane line passing north pole projection plane undefined 
uses second approach 
problem approach surfaces bend back full sphere parameterized way 
example sphere equation parameterized way parameterized spherical coordinate projection approach 
example uses projections parameterize full sphere 
revisit discussion section show spherical coordinates hemisphere defined parameterization projection index continuous 
principal curves defined unit speed 
address issue scaling principal surfaces suggests rescaling score vector ij min max 
nonlinear principal components analysis nonlinear principal components analysis nlpca proposed independently principal curves surfaces motivated pca implementation feedforward neural networks 
presenting nlpca method sketch neural network implementation pca 
feedforward neural networks extract principal components architecture shown fig 

network layers nodes input output layers node hidden layer 
weights determined minimize squares objective function min ij gamma ij min ij gamma network called autoassociative neural network trained reproduce inputs 
hidden layer autoassociative network called bottleneck layer dimensional inputs pass dimensional bottleneck layer reproducing inputs 
data compression occurs bottleneck layer 
note architecture estimates pca solution minimizes objective function pca equation 
early description application neural networks encoder decoder problem described pp 

develops theoretical properties method 
statement true partial sphere greater hemisphere equation 
sw sw layer neural network principal components analysis 
linear activation functions denoted 
nlpca direct generalization neural network implementation pca 
layer neural network nonlinear activation functions hidden layer represent continuous function weak assumptions 
nlpca adds hidden layers nonlinear activation functions input bottleneck layers bottleneck output layers giving network total layers 
network models composition functions 
fig 
shows example nlpca network 
nlpca network nodes input layer nodes third bottleneck layer nodes output layer 
nodes layers nonlinear activation functions layers layers represent arbitrary smooth functions 
nodes layers usually linear activation functions nonlinear 
direct connections allowed layers direct connections allowed cross bottleneck layer 
pca networks data compression takes place dimensional inputs pass dimensional bottleneck layer reproducing inputs 
network trained bottleneck node activation values give scores 
define notation facilitate discussion method relationship principal curves surfaces 
neural networks nonlinear activations functions continuous functions subnetworks consisting layers layers continuous functions 
denote function modeled layers denote function modeled layers 
notation note weights autoassociative nlpca network determined objective function min kx gamma principal curves nlpca bottleneck node features common 
give pca solution linear 

define function curve 
define function important difference nlpca defines continuous principal curves projection index discontinuous 
section discusses effects difference 

objective functions equations minimize expression 
nlpca minimizes expression functions principal curves performs minimization plugs optimal conjecture nlpca discontinuous nlpca solutions principal curves 
furthermore principal curve unique principal curve nlpca composition functions equal solutions differ parameterizations 
similar analogies principal surfaces nlpca multiple bottleneck nodes 
effects continuous projection index suboptimal projections defining continuous values causes suboptimal projections certain values curve nonlinear 
projection necessarily equal points set ambiguity points measure zero 
suboptimal mapped point curve point closest inf part equation 
illustrate reason example 
suppose estimating parabola shown figures 
set points fx axis symmetry ambiguity points curve point parabola closest 
approach ambiguity point direction normal curve optimal discontinuous cross ambiguity point 
plots fig 
show normals drawn points gamma intersect ambiguity point 
optimal project point normals gamma 
axis symmetry noise 
axis symmetry noise nlpca fit parabola nlpca model parabola problem fitted nn neural network simulator written author lines fig 
show model projects points normals 
fig 
shows fit precisely fx gamma gamma gamma 
software available mkt kellogg nwu edu 
noiseless data 
nlpca models curve perfectly training projections points lie parabola wrong 
mapping points normals nlpca maps point curve roughly coordinate 
approximation different noise added data fig 

training network 
projections points parabola points fairly close curve 
approach ambiguity point plot shows suboptimal behavior culminating ambiguity point projected 
reason behavior nlpca avoid discontinuous ambiguity point 
fig 
shows principal curve fit parabola noisy data function clearly discontinuous projections close gamma 
axis symmetry 
principal curve fit parabola show results fitting nlpca model circle equation 
circle important fraction variance unexplained kx gamma kx gamma xk principal curves estimated trevor hastie implementation available anonymous ftp research att com 
intersect parameterized linearizing curve parameterizing lengths projections line curve loops back 
parabola parameterized projections axis fig 
pca scores estimate order feature correctly noiseless situations 
center circle ambiguity point 
generated values 
plots fig 
show nlpca fits noiseless noisy data 
nlpca fit perfect noiseless circle training projections points near circle 
projections points away circle 
example point gamma gamma mapped near gamma mapped near gamma 
behavior seen plot ambiguity point marked projected 
consider projections vertical sequence points 
points near projected correctly points approach projections pulled 
pull obvious points 
plot shows nlpca fit noisy data 
nlpca appears extract parabolic shaped curve circle fitted curve curve back required circle 
fig 
shows principal curve fit circle noisy data 
projections principal curves method difficulty endpoints curve 
notes problem see fig 

projection index map center circle endpoints depending parameterization increases clockwise counter clockwise direction 
center mapped wrong point fit perfect points closer center ends 
examines related questions shows defined equation continuous point ambiguity point theorem 
propose corollary result continuous defined ambiguity points 
neural networks defined point suboptimal projections avoid having ambiguity points 
nlpca solutions principal curves layers nlpca network suboptimal projections 
principal curves suffer behavior 
examples section suggest nlpca model evaluated points outside region covered training data extrapolation points 
noise 
noise nlpca fit circle 
principal curve fit circle reduced class curves parameterizations projection index modeled continuous function curves surfaces intersect approximated certain possibly desirable parameterizations surfaces possible 
reason easily understood example 
circle intersects described polar coordinates equation 
defined continuous small change values result small change values 
clearly case point jumps 
nlpca principal curve nlpca principal curve fits full circle asking nlpca model curve intersects produce strange results illustrated fig 

generated equally spaced values interval corresponding points perimeter circle 
fitted nlpca model evaluated model cross validation grid independently noted nlpca problems approximating curves intersect 
points understand nlpca solution 
insure thorough training trained network iterations quasi newton bfgs nonlinear optimization algorithm resulting 
data noiseless expected 
curve fig 
nlpca approximation shows nlpca job extracting circle ends curve repel 
training results poor fit 
projections cross validation grid bad particularly quadrant highly nonlinear projections entire perimeter circle 
projections particularly bad positive part line projections extend plot 
optimal project points quadrant points circle quadrant projections clearly suboptimal 
fig 
shows principal curve fit 
estimate nearly perfect projections right place 
exception point mapped right 
similarly parameterizations discontinuous jumps projection index continuous 
example projection index continuous hemisphere discussed section parameterized spherical coordinates specified circle example jump 
spherical coordinates parameterize hemisphere exchanging coordinates sin cos cos sin sin theta 
score interpretations solutions nonlinear feature extraction methods obtained solving squares optimization problem 
unique minimum problem fitted values unique possibly ambiguity points 
score values unique different sets score values different parameterizations curve surface example recall parameterizations unit circle equations 
score indeterminacy raises important question applications score values interpreted decisions decision interpretation change curve surface parameterized differently 
question particularly important parameterizations resulting nlpca depend starting values choice nonlinear optimization method 
applications nonlinear feature extraction methods score values interpreted 
application call latent variables 
suppose measurements variable measurement known related continuous function subject error equation 
function latent variable values estimated suitable feature extraction method 
latent variable problems common physical social engineering sciences 
gives thorough account theory linear latent variable models continuous discrete data 
second application data visualization 
dimension predictor variables large usually difficult visualize data graphically 
plots data projected lower dimensional subspaces reveal important aspects data 
third application control charts 
variables commonly monitored fabrication product 
goal data determine fabrication process operating normally control wrong control 
difficult distinguish control control states large 
demonstrates score values nonlinear feature extraction method plotted control chart detect control conditions 
applications nonlinear feature extraction methods require interpreting scores 
application data image compression 
intrinsic dimension data substantially superficial dimension 
case score values provide compressed version original data 
note score values interpreted decisions 
example capture variation observed nonlinear components 
uses version nlpca image compression 
authors analyze theta pixel images dimensional representations images 
second application noise reduction 
observed variables noisy fitted values reduce noise 
distinction noise reduction latent variable applications fitted values scores 
third application curve estimation 
nonlinear feature extraction methods discontinuous projection index fit curve passing middle set data points 
uses principal curves determine path particles collider chamber 
result partially resolves parameterization question 
result shows parameterizations curve surface satisfying squares objection function related homeomorphism prove result appendix 
result projection index surface pair minimizing leastsquares objective function suppose homeomorphism 
second projection index surface pair homeomorphism 
exists homeomorphism result assumptions may restrictive 
assumption curve surface homeomorphism equivalent assuming curve surface intersect loops 
assumption implications trace curve respective projection indices map ambiguity points points curve 
result apply curves distinct satisfy squares objective function problems multiple minima objective function 
implications theorem different curves surfaces discussed separately 
case curves score values vector order elements vector preserved homeomorphic transformations score vector homeomorphic transformation implies 
result score values treated ordinal data treated continuous data magnitude difference score values interpretable order note level identification sufficient control chart applications mentioned 
surfaces problematic orientation set points greatly distorted homeomorphic transformation 
basic properties convexity preserved transformations 
function homeomorphism continuous continuous inverse 
consider hypothetical quality control example similar application discussed 
suppose large number process variable large monitored dimensional observations lie dimensional surface intrinsic dimension process variables 
plotting dimensional score vectors plane shown fig 
reveals points process control solid points lie circular region control point plotted pi lies outside region 
establish procedure process judged control score value falls circular region control 
problem procedure circular control region resulted choice starting values training method 
surface extracted different starting values different training method parameterization surface score values different possibly yielding control region shape circular 

circular control region 
heart shaped control region effect homeomorphic transformation control region show shape control region distorted 
suppose surface extracted simultaneous method param polar coordinates 
theorem guarantees score values solution related current score values homeomorphic continuous transformation 
consider homeomorphic transformation gamma exp gamma gamma exp gamma gamma transforms circular boundary heart shaped boundary shown fig 

expression exp gammax exp gammax derivative logistic function shape similar gaussian distribution 
points near moved closer origin locations points nearly unchanged 
transformation continuous toone 
transformation clear control point pi lies outside normal operating range 
nlpca estimates curve surface passing middle input data 
layers nlpca network projection index projects inputs layers define nlpca models projection index continuous function implications 
projections nonlinear curves surfaces suboptimal resulting larger training value error methods discontinuous projection indices principal curves 
nlpca approximate curves surfaces intersect parameterize curve surface parameterization discontinuous jumps 
parameterizations surface related homeomorphic transformation 
level identification order score variable values curves preserved score values surfaces change greatly possibly leading different interpretations 
important area research applications scores interpreted defining finding useful parameterizations surfaces 
question examined thoroughly case pca factor analysis 
deciding dimension subspace problem picking extracting set basis vectors spanning subspace researchers social sciences select alternative set basis vectors parameterization subspace certain properties interpretable original basis vectors resulting pca factor analysis estimation 
change basis called rotation social science literature 
see discussion 
surface score values interpreted similar done nonlinear feature extraction methods 
proof result denote range fs range fs inverse image gamma fx sg 
show homeomorphism show toone continuous 
show 
suppose 
function exists function exist 
assume 
function unique gamma 
exist gamma gamma 

contradiction 
statement true case 
assume 
function exist exist 

contradiction 

proving continuous straightforward follows definition continuous 
acknowledgments richard mah tom rick helpful discussions 
computing equipment empirical results partially funded nsf dms 
baldi hornik neural networks principal component analysis learning examples local minima neural networks vol 
pp 

latent variable models factor analysis charles griffin london 
cybenko approximation superpositions sigmoidal function math 
control signals systems vol 
pp 

david demers garrison cottrell nonlinear dimensionality reduction neural information processing systems vol 
pp 

dong nonlinear principal component analysis principal curves neural networks computers chemical engineering vol 
pp 

friedman exploratory projection pursuit jasa vol 
pp 

colin fyfe roland non linear data structure extraction simple hebbian networks tech 
rep department computer science university strathclyde 
hastie principal curves surfaces ph thesis stanford university november 
hastie stuetzle principal curves jasa vol 
pp 

juha karhunen representation separation nonlinear pca type learning neural networks pp 

kramer nonlinear principal component analysis autoassociative neural networks aiche journal vol 
pp 

leblanc tibshirani adaptive principal surfaces jasa vol 
pp 

mardia kent bibby multivariate analysis academic press london 
rumelhart hinton mcclelland learning internal representations error propagation parallel distributed processing volume foundations pp 

mit press 
terence sanger optimal hidden units layer nonlinear feedforward neural networks international journal pattern recognition artificial intelligence vol 
pp 

lectures classical differential geometry dover publication new york second edition 
tan michael reducing data dimensionality optimizing neural network inputs aiche journal press 
thorpe elementary topics differential geometry springerverlag new york 

