experiments sentence boundary detection mark stevenson robert gaizauskas department computer science regent court portobello street university sheffield sheffield dp uk dcs shef ac uk november explores problem identifying sentence boundaries transcriptions produced automatic speech recognition systems 
experiment determines level human performance task described memory computational approach problem 
problem addresses problem identifying sentence boundaries transcriptions produced automatic speech recognition asr systems 
unusual field text processing generally dealt punctuated text commonly texts nlp machine readable versions highly edited documents newspaper articles novels 
types text edited example concentrate output asr systems 
differ sort texts normally nlp number ways text generally single case usually upper may contain transcription errors 
compares short text format produced asr system fully punctuated version includes case information 
remainder error free texts newspaper articles novels shall referred standard text output speech recognition system asr text 
evening worlds leading fashion designers murdered miami police say planned killing carried execution schools inspections going tougher force bad teachers couples shared queens golden day evening 
world leading fashion designers murdered miami 
police say planned killing carried execution 
schools inspections going tougher force bad teachers 
couples shared queen golden day 
example text shown standard asr format speech recognition systems evaluated terms word error rate wer percentage tokens wrongly transcribed 
large vocabulary tasks speaker independent systems wer varies depending quality recording recognised 
see cole 
possible situations nlp system may required process asr text 
obvious examples nlp systems take speech input 
moore 
dictation software programs output information added asr text results far usable 
important pieces information available asr output sentence boundary information 
knowledge sentence boundaries required nlp technologies 
part speech taggers typically require input format single sentence line example brill tagger parsers generally aim produce tree spanning sentence :10.1.1.14.5427
trivial linguistic analysis carried text split sentences 
worth mentioning transcribed speech sensibly divided sentences 
argued gotoh renals main unit spoken language phrase sentence 
situations appropriate consider spoken language sentences 
example broadcast news radio television news programs 
darpa hub broadcast news evaluation focussed information extraction asr text news programs 
news programs scripted deviations script relied accurate transcriptions news program 
spoken portion british national corpus contains words manually marked sentence boundaries 
technology identifies sentence boundaries speed process creating corpus type 
important distinguish problem just mentioned problem called sentence splitting 
problem aims identify sentence boundaries standard text includes punctuation problem effectively reduced deciding symbols potentially denote sentence boundaries 

problem trivial punctuation symbols occur sentences 
example sentence dr jones lectures final full denotes sentence 
sake clarity shall refer process discovering sentence boundaries standard punctuated text punctuation disambiguation finding asr text sentence boundary detection 
related despite potential application technology carry sentence boundary detection task little research area 
related field punctuation disambiguation 
palmer hearst applied neural network problem 
brown corpus training evaluation noting full stops text indicate sentence boundaries 
part speech information words surrounding punctuation symbol input feed forward neural network 
mentioned part speech taggers require sentence boundaries pre determined potential circularity avoided prior probabilities token determined brown corpus markup 
network trained potential sentence marks wall street journal tested items corpus 
punctuation marks correctly disambiguated 
reynar ratnaparkhi applied maximum entropy approach problem 
system considered word left right potential sentence boundary claimed examining wider context help 
words prefix suffix presence particular characters prefix suffix candidate dr candidate corporate designator 
features considered 
system tested corpus palmer hearst system correctly identified sentence boundaries 
mikheev optimised approach evaluated test corpus 
accuracy reported knowledge highest quoted result test set 
systems achieve high results punctuation disambiguation task 
problem largely solved 
clear techniques successful asr text 
go describe system attempts task similar sentence boundary detection asr text 
beeferman produced system added intra sentence punctuation commas output asr system 
mention comma frequently punctuation symbol correct insertion text far legible 
operated augmenting standard trigram speech recognition model information commas accesses lexical information 
tested separating trigram model asr system applying sentences wall street journal 
system achieved precision recall compared original punctuation text 
evaluation carried randomly drawn output sentences system wall street journal 
human judges blindly marked sentence acceptable unacceptable 
penn treebank sentences correct system output correct 
interesting human judges agree completely acceptability sentences wall street journal 
section go describe experiments quantify level agreement expected humans carry sentence boundary detection 
section goes describe computational approach problem 
determining human ability beeferman experiments demonstrated humans agree acceptability comma insertion may useful determine agree placing sentence boundaries 
carried experiments transcriptions news programmes specifically transcriptions editions bbc television program clock news 
transcriptions consisted punctuated mixed case text sentences boundaries marked reserved character 
texts produced trained transcribers listening original program broadcast 
experimental subjects recruited 
subjects educated bachelor degree level native english speakers fluent second language speakers 
subject text sentence boundaries removed 
texts transcriptions editions news program containing sentences represented minutes broadcast news 
subjects randomly split groups 
subjects group subjects text stripped punctuation converted upper case 
text simulated asr text errors transcription 
remaining pair subjects subjects text punctuation removed case information retained mixed case text 
simulated standard text 
subjects asked add sentence boundaries text thought occurred 
process determining human ability linguistic task generally difficult lack appropriate 
compare person judgement 
example attempts determine level performance expected humans perform word sense disambiguation see fellbaum simply compared human judgements chosen expert 
seen section significant degree human disagreement acceptability intra sentential punctuation 
human transcribers clock news access original news story contains information just transcription 
conditions reasonable consider opinion expert 
table shows performance human subjects compared transcripts 
see worst performing subjects annotated mixed case text subject precision recall complementary evaluation metrics commonly information retrieval 
case precision percentage commas proposed system correct recall percentage commas occurring test corpus system identified 
minute long television news program broadcast united kingdom monday friday 
measure weighted harmonic combining precision recall formula pr accurate best performing annotating upper case text subject 
algorithm implemented provide baseline tagging text 
average length sentences text words baseline algorithm randomly assigns sentence break word boundary probability annotators labelled random show results algorithm applied 
method produced low result comparison expert annotation 
annotator text upper upper upper mixed mixed random upper random mixed table results human annotation experiment performance human annotators upper case text quite significantly lower reported performance algorithms performed punctuation disambiguation standard text described section 
suggests performance may obtained task may lower achieved standard text 
insight task gained determining degree subjects agreed 
carletta argues kappa statistic adopted judge annotator consistency classification tasks area discourse dialogue analysis :10.1.1.14.1751
worth noting problem sentence boundary detection far formulated classification task token boundary classified sentence boundary 
carletta argues incompatible measures annotator agreement discourse analysis making comparison impossible 
solution look field content analysis experienced problems adopt solution kappa statistic 
determines difference observed agreement linguistic task expected chance 
calculated formula pr proportion times annotators agree pr proportion expected chance 
detailed instructions calculating probabilities described siegel castellan 
pr gamma pr gamma pr value kappa statistic ranges perfect agreement level expected chance 
claimed content analysis researchers usually regard demonstrate reliability allows tentative drawn see carletta :10.1.1.14.1751
began analyse data computing kappa statistic sets annotators 
annotators marked mixed case subjects observed kappa value measure subjects annotated single case text 
values high suggest strong level agreement annotators 
manual analysis annotated texts suggested subjects agree cases 
added texts annotated random annotation algorithm calculated new values 
mixed case test produced kappa value upper case text 
values suggest high level agreement sentences produced random algorithm nonsensical 
problem word boundaries text sentence boundaries 
compare subjects annotations agreed sentence boundaries find agreed word boundaries sentence boundaries 
problem effect standard measures inter annotator agreement cramer phi kendall coefficients see siegel castellan 
carletta mentions problem asking difference kappa statistic computed clause boundaries transcribed word boundaries transcribed phoneme boundaries sentence boundaries suggested :10.1.1.14.1751:10.1.1.14.1751
meaningful values obtained restricted boundaries clauses token boundaries 
difficult imagine clauses identified parsing parsers require part speech tagged input text 
mentioned part speech taggers require input text split sentences 
consequently lack available systems splitting asr text grammatical clauses 
computational approach sentence boundary detection remainder describes implemented program attempts sentence boundary detection 
approach timbl memory learning algorithm previously successful applied word sense disambiguation problem 
memory learning known case lazy learning operates set training examples categorising new cases assigning class similar learned example 
apply methodology sentence boundary detection task presenting timbl examples word boundaries training text categorised sentence boundary boundary 
unseen examples compared categorised class similar example 
shall discuss method timbl determines similar training example described daelemans 
done punctuation disambiguation beeferman comma insertion section wall street journal text experiment 
texts reliably part speech tagged sentence boundaries easily derived corpus 
text initially altered remove punctuation map characters upper case 
corpus containing sentence breaks training corpus remainder contained sentence breaks held back unseen test data 
stage extract statistics training corpus 
examined training corpus computed word text probability started sentence probability ended sentence 
addition part speech tag computed probability assigned word sentence probability assigned word 
word boundary corpus translated feature vector representation consisting elements shown table 
vectors test corpus similar format difference classification feature included 
results obtained shown top row table 
precision recall quite promising conditions 
text different asr text important way text mixed case 
experimented repeated capitalisation information removed features removed feature vectors 
results form experiment shown bottom row table 
seen recorded performance far lower capitalisation information indicating important feature task 
experiments shown easier add sentence boundary information mixed case test essentially standard text punctuation removed asr text assuming zero word error rate 
result agreement results human annotation experiments described section 
far greater difference automatic system performance standard asr text human annotators 
reynar ratnaparkhi section argued context word side sufficient punctuation disambiguation problem 
results system suggest may attempted smooth probabilities turing frequency estimation see gale sampson effect final results 
position feature preceding word probability preceding word ends sentence part speech tag assigned preceding word probability part speech tag feature assigned word sentence flag indicating preceding word word flag indicating preceding word capitalised word probability word begins sentence part speech tag assigned word probability part speech feature assigned word sentence flag indicating word word flag indicating word capitalised word sentence boundary boundary table features timbl representation case information applied applied table results sentence boundary detection program insufficient sentence boundary detection problem 
introduced problem sentence boundary detection text produced asr system area application nlp technology 
attempt determine level human performance expected task 
noticeable difference observed performance mixed upper case text 
kappa statistic commonly method calculating inter annotator agreement meaningfully applied situation 
memory system identifying sentence boundaries asr text implemented 
noticeable difference system applied text included case information demonstrating important feature problem 
propose offer solution sentence boundary detection problem asr transcripts 
aim highlight problem worthy exploration field nlp 
authors steve renals gotoh providing data human annotation experiments useful conversations 
grateful people took part human annotation experiments michael george andrea lisa ferry 
beeferman berger lafferty 
lightweight punctuation annotation system speech 
proceedings ieee international conference acoustics speech signal processing pages seattle wa 
brill :10.1.1.14.5427
simple rule part speech tagger 
proceeding third conference applied natural language processing anlp pages trento italy 

users guide british national corpus 
oxford university computing services 
carletta :10.1.1.14.1751
assessing agreement classification tasks kappa statistic 
computational linguistics 
chinchor robinson brown 
hub named entity task definition version 
technical report saic 
www nist gov speech hub 
cole editor 
survey state art human language technology 

available cse ogi edu html 
site visited 
daelemans zavrel van der van den bosch 
timbl tilburg memory learner version guide 
technical report ilk technical report 
ilk report available ilk nl ilk papers ilk ps gz 
fellbaum grabowski baumann 
matching words senses wordnet naive vs expert differentiation senses 
fellbaum editor wordnet electronic lexical database applications 
mit press cambridge ma 
gale sampson 
turing frequency estimation tears 
journal linguistics 
gotoh renals 
information extraction broadcast news 
philosophical transactions royal society london series mathematical physical engineering sciences 
appear 
mikheev 
feature lattices maximum entropy modelling 
proceedings th meeting association computational linguistics coling acl pages montreal canada 
moore dowding bratt cheyer 
interface battlefield simulations 
proceedings fifth conference applied natural language processing pages washington dc 
palmer hearst 
adaptive sentence boundary disambiguation 
proceedings conference applied natural language processing pages germany 
reynar ratnaparkhi 
maximum entropy approach identifying sentence 
proceedings fifth conference applied natural language processing pages washington 
siegel castellan 
nonparametric statistics behavioural sciences 
mcgraw hill second edition 
stevenson wilks 
combining weak knowledge sources sense disambiguation 
ijcai stockholm sweden 
van rijsbergen 
information retrieval 
butterworths london 

