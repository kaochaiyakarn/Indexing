iterations gibbs sampler 
adrian raftery university washington steven lewis university washington april revised september gibbs sampler estimate posterior distributions gelfand smith question iterations required central implementation 
interest focuses quantiles functionals posterior distribution describe easily implemented method determining total number iterations required number initial iterations discarded allow burn 
method uses gibbs iterates example require external specification characteristics posterior density 
method described situation long run generated easily applied runs different starting points 
applies generally markov chain monte carlo schemes gibbs sampler 
quantiles estimated quantities interest probabilities full posterior distributions draws posterior distribution required approximately independent 
method applied different posterior distributions 
include multivariate normal posterior distribution independent parameters bimodal distribution cigar shaped multivariate normal distribution dimensions highly complex dimensional posterior distribution arising spatial statistics 
case method appears give satisfactory results 
results suggest reasonable accuracy may achieved iterations frequently reduced posterior tails known light 
frequent exceptions required number iterations higher 
important exception high posterior correlations parameters crude correlation removing research supported onr contract nih hd 
authors grateful jeremy york providing data examples helping analysis useful discussions suggestions julian besag anonymous referee helpful comments 
fortran program called implements methods described may obtained statlib sending mail message statlib temper stat cmu edu containing single line send general 
program maintained questions may addressed mail adrian raftery raftery stat washington edu 
greatly increase efficiency cases 
important exception arises hierarchical models gibbs sampler tends get stuck different markov chain monte carlo schemes may improve matters 
method proposed diagnose exceptions quite effectively 
gibbs sampler introduced geman geman way simulating high dimensional complex distributions arising image restoration 
method consists iteratively simulating conditional distribution component random vector simulated current values components 
complete cycle components vector constitutes step markov chain stationary distribution suitable conditions distribution simulated 
gelfand smith pointed algorithm may simulate posterior distributions may solve standard statistical problems 
gibbs sampler extremely computationally demanding relatively small scale statistical problems important know iterations required achieve desired level accuracy 
describe investigate simple method doing briefly mentioned raftery banfield 
focus situation single long run gibbs sampler practiced geman geman besag york example 
gelfand smith adopted choose starting point ii run gibbs sampler iterations store iterate iii return 
choice ways implementing algorithm settled subject considerable debate controversy workshop bayesian computation stochastic simulation columbus ohio february 
intuitive considerations suggest long run may efficient 
heuristic argument run follows 
consider ways obtaining values simulated posterior distribution 
way consists picking th value single long run length st second way gelfand smith 
way starting point subsequence length closer draw stationary distribution corresponding starting point second way chosen user 
way gives result worse second way 
may exploited way reducing value obtain result total iterations 
formal argument similar lines smith concluding discussion workshop bayesian computation stochastic simulation 
gelman rubin hand argued long run approach may efficient important different starting points 
essence argument know case individual problem single run converged combining results runs starting points gives honest conservative assessment underlying uncertainty 
illustrate argument showing ising model convergence quite slow 
example refers dimensional binary state space gamma parameter spaces arise typical statistical problems taken seriously 
suggest combining internal information partial run properties markov chains may provide alternative way solving problem sacrificing appealing simplicity single long run 
particular markov chain theory provides results just ergodicity geometric rate convergence stationary distribution distribution sample means 
method easily runs different starting points 
method iterations estimate posterior quantile 
consider specific problem calculating particular quantiles posterior distribution function parameter 
formulate problem follows 
suppose want estimate probability function 
find approximate number iterations required correct answer example corresponds requiring cumulative distribution function quantile estimated sigma probability 
reasonable requirement roughly speaking wanted reported intervals actual posterior probability 
run gibbs sampler initial iterations discard iterations store kth 
typical choices literature besag york 
problem determine note may store iterates solution conservative 
calculate iteration form ffi ffi delta indicator function 
fz binary process derived markov chain marginalization truncation markov chain 
reasonable suppose dependence fz falls fairly rapidly lag form new process fz gamma fz approximately markov chain sufficiently large 
formal proof intuitively plausible 
data method described assess assumption provides reasonable approximation case hand 
proof go follows 
process fz ergodic underlying markov chain oe mixing sense billingsley direct consequence construction fz oe mixing rate 
maximum difference gamma gamma gamma eventually declines geometrically function fz arbitrarily close order markov chain sense sufficiently large 
follows draw standard results state markov chains see example cox miller 
assuming fz markov chain determine mk number burn iterations discarded 
gamma ff ff fi gamma fi transition matrix fz equilibrium distribution ff fi gamma fi ff step transition matrix ff fi ff gammaff gammafi fi gamma ff gamma fi 
suppose require 
requirement ff fi max ff fi holds log ff fi max ff fi log determine note estimate large approximately normally distributed mean variance fffi gammaff gammafi ff fi requirement gamma satisfied fffi gammaff gammafi ff fi ae phi oe phi delta standard normal cumulative distribution function 
kn determine form series fz 
compare order markov chain model second order markov chain model choose smallest value order model preferred 
compare models recasting closed form log linear models table bishop fienberg holland bic criterion gamma log likelihood ratio test statistic 
introduced schwarz context generalized log linear models raftery provides approximation twice logarithm bayes factor second order model 
non bayesian test choice significance level presence large samples size arise routinely gibbs sampler 
implement method run sampler initial number iterations min run determine number additional runs required 
procedure iterated indicated number iterations run may apply method entire run ff fi determine precisely number iterations produced fact adequate 
determine min note required minimized successive values fz independent case min phi gamma gamma example min 
note user required kth iterate iterates method proposed conservative sense possibly overestimating number iterations required 
hand majority cases examined preferred value fact 
storage considerations point desirability storing portion iterates reasonable 
user needs give required precision specified quantities 
result far sensitive gamma may table maximum percent error estimated quantile min percent error cauchy natural specify required precision terms error estimate quantile error cumulative distribution function quantile refers 
order see relates accuracy scale shown table approximate maximum percentage error estimated quantile corresponding range values 
defined maxf gamma gamma gamma shown distributions normal light tailed moderate tails cauchy heavy tailed 
suppose regard error acceptable corresponding estimated quantile normal distribution compared true value 
knew light normal tails table suggests sufficiently small 
heavier tailed distribution required achieve accuracy heavy tailed cauchy required corresponding min 
suggests sure advance heavy posterior tail reasonably safe choice cauchy catastrophic 
suggests method refined initial set gibbs iterates estimate asymptotic rate decay posterior tail methods hall choosing light estimate referring distribution appropriate degrees freedom 
sight appear component wise tails remedy 
suspect real solution problem reappear results transformed back scale quantity actual interest measured 
extensions quantiles quantiles estimated possible run gibbs sampler min iterations apply method section quantile individually maximum values guarantee quantile marginally specified accuracy achieved equivalently expected number estimated quantiles lie true value sq 
different demanding accuracy requirement probability quantiles lie true value 
conservative solution uses bonferroni bound idea consists replacing proceeding 
estimating probabilities gibbs sampler estimate probabilities distributions 
examples include probability ground truth pixel particular color image reconstruction besag probability individual genotype genetic pedigree analysis probability individual specific disease expert system diagnosis lauritzen spiegelhalter 
method directly applicable case takes slightly simpler form 
fz process formed truncation 
specify just required probability 
simply specifies proceeds 
independent iterates expensive analyze gibbs iterate simulate may complex applications genetic pedigree analysis desirable gibbs iterates approximately independent 
achieved making big 
determine compare independence model order markov chain model fz choose smallest value independence model preferred 
models compared recasting independence saturated models theta table bic criterion 
calculated exactly kn min approximate independence iterates 
examples apply method examples simulated real 
case give results 
results shown table results examples example gamma 
indep 
normal pars 

bimodal 
cigar 
spatial 
spatial smoothness table examples 
value column headed gamma specification 
results quantiles accuracy requirements shown qualitatively similar 
example multivariate normal distribution independent parameters simulated example method gave small number burn iterations value slightly larger theoretical minimum 
result specified bounds 
expect reassuring check performance method 
example bimodal posterior distribution simulated gibbs sampler mixture bivariate normal distributions bv sigma sigma gamma sigma joint distribution quite strongly bimodal marginal distributions components 
simulated values second component shown 
result surprisingly similar example 
amount burn negligible larger theoretical minimum 
gibbs iterates slightly highly correlated example value regarded index 
result specified bounds 
example cigar dimensions order investigate effect high posterior correlations parameters gibbs sampler simulate dimensional multivariate normal posterior distribution component zero mean unit variance pairwise correlations equal 
highly correlated distribution principal component proportional mean parameters accounts variance posterior distribution concentrated thin cigar space 
note poor parameterization gibbs sampler 
simulated values parameter shown 
results applying method strikingly different saw 
amount burn longer negligible huge 
dependency structure binary sequence complicated leading level dependency high required large 
number iterations result accurate 
phenomenon due high level dependency sequence primarily sampler slow converge desired distribution 
interest consider situation iterations large number substantially prescribed 
point diagnostics changes cumulative estimates suggest gibbs sampler converged 
iterations gamma gamma compared true value outside prescribed tolerance empirical quantile 
method indicated clearly number iterations insufficient achieve desired accuracy 
example illustrates importance parameterization gibbs sampler see wakefield 
parameterization leads highly correlated posterior distribution considered example poor gibbs sampler leads considerable inefficiency 
simple linear reparameterization lead fold reduction required number iterations 
example dimensional posterior distribution spatial statistics besag york considered problem mapping risk disease incidence data 
denote unknown log relative risk zone corresponding observed number cases 
assumed poisson distribution mean expected number assuming constant risk 
substantial spatial structure represented joint density exp gamma ij gamma denotes fact zones contiguous spatial smoothness parameter 
assumed generated gaussian white noise parameter prior distribution expf gamma gamma gamma improper resulting posterior distribution proper 
main aim find posterior distribution features underlying mechanism may interest 
show result thyroid cancer deaths france results similar 
gibbs sampler involves parameters iterations shown 
result similar examples 
number column obtained running gibbs sampler total iterations treating value obtained complete run true value 
example spatial smoothness parameter consider separately spatial smoothness parameter example 
gibbs iterations shown 
results quite different somewhat similar example 
dependency structure induced binary sequence complex leading dependency high leading 
amount burn negligible fairly small 
feasible determine correct answer case 
difficulty example probably resolved appropriate reparameterization problem fundamental 
problem due fact gets stuck close zero iterations time 
having close high spatial smoothness small value small value forces close 
gibbs sampler gets caught periodically vicious circle escape requires rare event 
solution may different variation metropolis dynamics gibbs sampler involving simultaneous updating kind 
kind problem arise hierarchical models generally 
note method determining number iterations carry forms metropolis dynamics 
discussion proposed method determining iterations necessary gibbs sampler 
easy implement require initial run sampler 
appears give encouraging results examples 
thorough investigation required various kinds difficult posterior distributions 
nice posterior distributions examples suggest accuracy level specified illustration achieved running sampler iterations iterates 
posterior nice required number greater 
example suggests poor parameterization reason massive inefficiency gibbs sampler simple minded reparameterization may potential lead substantial savings 
problems may arise hierarchical models gibbs sampler tendency get stuck illustrated example 
experience suggests method diagnoses problems fairly 
prescribed number iterations larger min ways proceed 
simply run sampler specified number iterations best course iterates computationally inexpensive 
may worthwhile different markov chain monte carlo scheme 
common practice running gibbs sampler throw away substantial number initial iterations order 
results suggest may usually necessary quite wasteful 
surprising geometric rate convergence markov chains stationary distribution 
large numbers iterations required due high level dependence successive iterates failure gibbs sampler converge initially 
suspect typical statistical problems uncertainty due initial starting point gelman rubin capture methods relatively small part uncertainty number gibbs iterations realistically large 
course far having established conclusively diagnostic checks proposed gelman rubin remain important 
method theirs may regarded complementary method viewed determining total number iterations required typically little changed long run small number shorter runs different starting points 
specifically different runs different starting values run nr gamma iterations discarded 
methods synthesized approach determine total number required iterations method gelman rubin check convergence incorporate uncertainty starting point 
common practice th th iterate discard rest 
results suggest cases 
nice cases dependency successive iterates weak sense storage issue 
alternative approach determining number iterations starts viewing sequence gibbs iterates standard time series geyer geweke hills smith 
quantity interest mean function series variance mean equal spectrum corresponding series zero estimated standard spectral methods 
requires user specify spectral window window width estimate spectrum zero quite sensitive choices 
obtaining posterior quantiles defining bayesian confidence intervals key goal analysis 
case method exploits natural simplification arises implied 
avoids need specify quantities required precision spectral window widths yields simple estimate number burn estimations provides practical lower bound min number iterations known gibbs sampler starts running 
may argued required posterior mean standard deviation quantiles 
case really interest shape posterior distribution may little point running gibbs sampler cheaper methods frequently available posterior means standard deviations 
posterior mean standard deviation provide summary posterior distribution 
case robust measure location median may preferable posterior mean descriptive measure median quantile 
posterior standard deviation way obtaining approximate confidence interval say posterior mean plus minus posterior standard deviations 
sample posterior available worth calculating required interval directly defined quantiles 
single measure posterior dispersion required may better robust measure posterior standard deviation scaled version inter quartile range defined quantiles 
appropriate summaries posterior distribution defined terms quantiles sight mean quantity required 
important message required number iterations dramatically different different problems different quantities interest problem 
unwise rely single rule thumb important method proposed determine number iterations needed problem hand 
besag 
statistical analysis dirty pictures discussion 
statist 
soc ser 

besag york bayesian image restoration applications spatial statistics discussion 
ann 
inst 
statist 
math 
billingsley 
convergence probability measures 
new york wiley 
bishop fienberg holland 
discrete multivariate analysis 
cambridge mass mit press 
cox miller 
theory stochastic processes 
london chapman hall 
gelfand smith 
sampling approaches calculating marginal densities 
amer 
statist 
ass 
gelman rubin 
overview approach inference iterative simulation 
workshop bayesian computation stochastic simulation columbus ohio february 
geman geman 
stochastic relaxation gibbs distributions bayesian restoration images 
trans 
pattern anal 
machine intell 
geweke 
evaluating accuracy sampling approaches calculation posterior moments 
fourth valencia international meeting bayesian statistics valencia spain april 
geyer 
monte carlo maximum likelihood exponential families 
workshop bayesian computation stochastic simulation columbus ohio february 
hall 
simple estimates exponent regular variation 
roy 
statist 
soc ser 

hills smith 
parametrization issues bayesian inference 
fourth valencia international meeting bayesian statistics valencia spain april 
lauritzen spiegelhalter 
local computations probabilities graphical structures application expert systems discussion 
statist 
soc ser 

raftery 

note bayes factors log linear contingency tables vague prior information 
roy 
statist 
soc ser 

raftery banfield 
stopping gibbs sampler morphology issues spatial statistics 
ann 
inst 
statist 
math 
schwarz 
estimating dimension model 
ann 
statist 

image processing procedures applied estimation genotypes pedigrees 
technical report department statistics university washington 
wakefield 
parameterization issues gibbs sampling 
workshop bayesian computation stochastic simulation columbus ohio february 

