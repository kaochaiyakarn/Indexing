prior knowledge support vector kernels bernhard sch olkopf patrice simard alex smola vladimir vapnik max planck institut fur kybernetik tubingen germany gmd berlin germany research schulz drive red bank nj usa bs gmd de explore methods incorporating prior knowledge problem hand support vector learning machines 
show invariances group transformations prior knowledge locality images incorporated constructing appropriate kernel functions 
trying extract regularities data additional knowledge functions estimate 
instance image classification tasks exist transformations leave class membership invariant local translations usually case images local structure correlations image regions carry equal amounts information 
study investigates question sources knowledge designing appropriate support vector sv kernel functions 
start giving brief sv machines vapnik chervonenkis vapnik sec 

regarding prior knowledge invariances method design kernel functions invariant classification hyperplanes sec 

method applicable invariances action differentiable local parameter groups local transformations translational invariance pattern recognition 
sec 
describe kernels take account image locality localized receptive fields 
sec 
presents experimental results types kernels followed discussion sec 

optimal margin hyperplanes linear hyperplane decision functions sgn delta vc dimension controlled controlling norm weight vector training data sigma separating hyperplane generalizes minimizing kwk subject delta delta conditions separating training data margin 
nonseparable cases dealt introducing slack variables cortes vapnik shall omit modification simplify exposition :10.1.1.15.9362
applies nonseparable case 
solve convex optimization problem introduces lagrangian multipliers ff derives dual form optimization problem maximize ff gamma ff ff delta subject ff ff turns solution vector expansion terms training examples ff ff corresponding constraints met nonzero respective examples called support vectors 
substituting expansion yields decision function sgn ff delta shown minimizing corresponds minimizing upper bound vc dimension separating hyperplanes equivalently maximizing separation margin classes 
section shall depart modify dot product minimization corresponds enforcing transformation invariance time constraints hold 
invariant hyperplanes invariance self consistency argument 
face problem express condition invariance decision function need know coefficients optimization turn take account desired invariances 
way circle ansatz consider decision functions sgn ffi defined ff bx delta bx matrix determined 
follows vapnik suggested incorporate invariances modifying dot product 
nonsingular defines dot product equivalently written delta ax positive definite matrix clearly invariance local transformations sufficient condition local invariance aiming 
strictly speaking invariance necessary points support vectors lie region sgn ffi constant training hard predict examples turn svs 
virtual sv method scholkopf burges vapnik run standard sv algorithm carried obtain initial sv set similar heuristics applied case :10.1.1.42.5400
local invariance pattern transformations differentiable local parameter group local transformations fi fi fi approximately enforced minimizing regularizer fi fi fi note sum may run labelled unlabelled data principle require decision function invariant respect transformations elements test set 
different transformations different patterns 
local invariance term fi fi fi ff bl delta bx ff bl delta bx delta fi fi fi chain rule 
bl delta bx denotes gradient delta respect evaluated point delta bl delta bx 
substituting facts yields regularizer ff bx fi fi fi ff ff bx delta bx fi fi fi fi fi fi choose reduces standard sv target function kwk form obtained substituting expansion ff cf 
quadratic term utilizing dot product chosen bx delta bx bx delta bx 
assuming span space condition requiring nonsingular information get lost preprocessing satisfied preprocessing whitening matrix gamma modulo unitary matrix disregard nonnegative square root inverse nonnegative matrix defined 
practice matrix gamma nonnegative invertible 
recover standard sv optimal hyperplane algorithm values determine trade invariance model complexity control 
shown corresponds objective function phi gamma delta kwk choosing preprocessing matrix obtained formulation problem standard sv quadratic optimization technique effect minimize tangent regularizer maximum modified dot product coincides minimum subject separation conditions delta defined 
note preprocessing affect classification speed bx delta bx delta bx precompute bx svs obtain machine modified svs fast standard sv machine cf 

relationship principal component analysis pca 
provide interpretation 
tangent vectors sigma zero mean sample estimate covariance matrix random vector delta sigma random sign 
observation call tangent covariance matrix data set fx respect transformations positive definite diagonalized sds orthogonal matrix consisting eigenvectors diagonal matrix containing corresponding positive eigenvalues 
compute gamma sd gamma gamma diagonal matrix obtained inverse square roots diagonal elements 
dot product invariant orthogonal transformations may drop leading ff gamma delta gamma pattern transformed projecting eigenvectors tangent covariance matrix rows resulting feature vector rescaled dividing square roots eigenvalues 
words directions main variance random vector scaled back emphasis put features variant example image analysis represent translations emphasis put relative proportions ink image positions lines 
pca interpretation preprocessing matrix suggests possibility regularize reduce dimensionality discarding part features common usage doing pca 
ideas described section tested linear case 
generally sv machines nonlinear kernel function shown compute dot product high dimensional space nonlinearly related input space map phi phi delta phi 
case analysis leads tangent covariance matrix shown evaluated terms kernel function scholkopf 
techniques kernel pca scholkopf smola muller 
kernels local correlations kernel delta implicitly constructs decision boundary space possible products pixels 
may desirable natural images correlations short distances reliable features long range correlations 
take account define kernel follows cf 
fig 

compute third image defined pixel wise product 
sample pyramidal receptive fields diameter centered locations obtain values ij 
raise ij power take account local correlations range pyramid 
sum ij image raise result power allow range correlations order understood definite cf 

alternatively pseudoinverse 
aside note goal build invariant sv machines provided approach open problem sv learning scaling sv machines far way automatically assigning different weight different directions input space trained sv machine weights layer svs form subset training set 
choosing support vectors training set gives limited possibilities appropriately dealing different scales different directions input space 

kernel utilizing local correlations images corresponding dot product polynomial space spanned mainly local correlations pixels see text 
resulting kernel order delta contain possible correlations delta pixels 
experimental results experiments subset mnist data base handwritten characters bottou consisting training examples test examples resolution pixels entries gamma 
linear sv machine separating hyperplane obtain test error rate training binary classifiers maximum value cf 
class classification polynomial kernel degree drops 
experiments degree kernels various types 
number chosen written product integers compare results kernel 
considered classification task results higher polynomial degrees similar 
series experiments homogeneous polynomial kernel delta preprocessing gaussian smoothing kernels standard deviation obtained error rates gradually increased improvement performance possible simple smoothing operation 
applying virtual sv method retraining sv machine translated svs scholkopf burges vapnik problem results improved error rate :10.1.1.42.5400
training full pattern set virtual sv performance scholkopf 
invariant hyperplanes 
table reports results obtained preprocessing patterns cf 
choosing different values cf 

experiments patterns rescaled entries computed horizontal vertical translations preprocessing carried resulting patterns scaled back 
done ensure patterns derivatives lie comparable regions note pattern background level constant gamma derivative 
results show derived linear case lead improvements nonlinear case degree polynomial 
dimensionality reduction 
scaling operation affine linear argument leading hold case 
report results dimensionality reduction case data kept scaling table classification error rates modifying kernel delta invariant hyperplane preprocessing matrix gamma cf 

enforcing invariance leads improvements original performance 
error rate table dropping directions corresponding small eigenvalues cf 
leads substantial improvements 
results case cf 
table degree homogeneous polynomial kernel 
principal components discarded error rate 
dropping principal components important leads substantial improvements table cf 
explanation 
results table somewhat distorted fact polynomial kernel translation invariant performs poorly data evident case principal components discarded 
better results obtained translation invariant kernels gaussian rbfs scholkopf 
kernels local correlations 
exploit locality images pyramidal receptive field kernel diameter cf 
sec 

obtained improved error rate degree kernel local correlations led 
albeit significantly better degree homogeneous polynomial error rates element test set accuracy cf 
bottou worse virtual sv result 
methods exploit different types prior knowledge expected combining leads better performance yielded best performance 
purpose benchmarking ran system postal service database handwritten digits resolution theta 
case obtained test error rates sv degree polynomial kernel virtual sv kernel sv virtual sv 
compares favourably known results data base second memory nearest neighbour classifier simard lecun denker 
discussion general class admissible kernel functions sv algorithm provides ample possibilities constructing task specific kernels 
considered image classification task forms domain knowledge pattern classes required locally invariant second local correlations images assumed reliable long range correlations 
second requirement seen general form prior knowledge thought arising partially fact patterns possess variety transformations object recognition instance object rotations deformations 
typically transformations continuous implies local relationships image fairly stable global relationships reliable 
incorporated types domain knowledge sv algorithm constructing appropriate kernel functions leading substantial improvements considered pattern recognition tasks 
method constructing kernels transformation invariant sv machines put forward deal type domain knowledge far applied linear case partially explains led moderate improvements far translational invariance 
applicable differentiable transformations types mirror symmetry dealt techniques virtual svs scholkopf burges vapnik :10.1.1.42.5400
main advantages compared technique slow testing speed invariances leaves training time unchanged 
proposed kernels respecting locality images led large improvements applicable image classification cases relative importance subsets products features specified appropriately 
slow training testing constant factor depends specific kernel 
described techniques directly applicable kernel methods sv regression vapnik kernel pca scholkopf smola muller 
include nonlinear case cf 
remarks sec 
incorporation invariances translation construction kernels incorporating local feature extractors edge detectors different pyramids described sec 


chris burges leon bottou parts code helpful discussions tony bell remarks 
boser guyon vapnik 
training algorithm optimal margin classifiers 
haussler editor proceedings th annual acm workshop computational learning theory pages pittsburgh pa 
acm press 
bottou cortes denker drucker guyon jackel lecun muller sackinger simard vapnik 
comparison classifier methods case study handwritten digit recognition 
proceedings th international conference pattern recognition neural networks jerusalem pages 
ieee computer society press 
cortes vapnik 
support vector networks 
machine learning 
scholkopf 
support vector learning 
oldenbourg verlag munich 
isbn 
scholkopf burges vapnik 
incorporating invariances support vector learning machines 
von der malsburg von seelen sendhoff editors artificial neural networks icann pages berlin 
springer lecture notes computer science vol 

scholkopf smola 
muller 
nonlinear component analysis kernel eigenvalue problem 
technical report max planck institut fur kybernetik 
press neural computation 
simard lecun denker 
efficient pattern recognition new transformation distance 
hanson cowan giles editors advances neural information processing systems pages san mateo ca 
morgan kaufmann 
simard lecun denker 
tangent prop formalism specifying selected invariances adaptive network 
moody hanson lippmann editors advances neural information processing systems pages san mateo ca 
morgan kaufmann 
vapnik 
nature statistical learning theory 
springer verlag new york 
vapnik chervonenkis 
theory pattern recognition russian 
nauka moscow 
german translation theorie der akademie verlag berlin 
