improving algorithms boosting aslam department computer science dartmouth college sudikoff laboratory hanover nh cs dartmouth edu www cs dartmouth edu motivated results information theory describe modification popular boosting algorithm adaboost assess performance theoretically empirically 
provide theoretical empirical evidence proposed boosting scheme lower training testing error original non confidence rated version adaboost 
modified boosting algorithm analysis suggests explanation boosting confidence rated predictions markedly outperforms boosting confidence rated predictions 
motivations analyses provide impetus study boosting information theoretic opposed decision theoretic light 
boosting mechanism training sequence weak learners combining hypotheses generated weak learners obtain aggregate hypothesis highly accurate 
popular widely studied boosting algorithms adaboost proposed freund schapire :10.1.1.156.2440:10.1.1.32.8918
adaboost proceeds rounds round weak learner trained performance output hypothesis assessed obtain relative weight predictions output combination distribution modified boosting round weight 
adaboost essentially assesses performance weak hypotheses measuring error hypothesis 
argue predictive error assess performance learning algorithms necessarily best measure performance weak hypotheses boosting scenario 
argue true performance weak hypothesis captured joint distribution predictions correct labels performance best measured purposes boosting odds making correct prediction particular output label informationtheoretic measures mutual information conditional entropy 
partially supported nsf eia 
ideas propose modification adaboost algorithm analyze performance theoretically empirically 
derive bounds training error proposed boosting algorithm give evidence training error lower adaboost 
analysis yields criterion determining best weak hypothesis round boosting order minimize training error 
case non confidence rated hypotheses demonstrate adaboost generally best weak hypotheses dictated proposed modified 
case confidence rated hypotheses strong connection proposed modified boosting algorithm techniques suggested schapire singer :10.1.1.156.2440
analysis suggests information theoretic explanation boosting confidence rated predictions markedly outperforms boosting confidence rated predictions cases 
analyses example kivinen warmuth provides impetus study boosting information theoretic opposed decision theoretic light 
describe number experiments uci trec data sets 
motivation consider version adaboost proposed schapire singer shown :10.1.1.156.2440
sequence 
training examples element known instance space element known label space kg 
version adaboost concerned binary classification problem mathematical convenience uses label space 
boosting algorithms adaboost proceeds rounds 
round weak learner trained local distribution initially uniform training examples weak hypothesis returned 
note assumed range sign interpreted desired label magnitude interpreted confidence label assessment 
hypotheses referred having confidence rated predictions 
hypotheses restricted range version adaboost identical freund schapire xm ym initialize 
train weak learner distribution 
get weak hypothesis 
choose 
update exp normalization factor chosen distribution 
output final hypothesis sign schapire singer adaboost original version :10.1.1.156.2440
performance respect training distribution assessed performance determining parameter distribution update rule better performance larger value chosen 
update rule modifies distribution training round correctly classified examples decreased weight incorrectly classified examples increased weight 
larger better performance distribution skewed 
output hypothesis linear combination outputs respective weak hypotheses weight associated weak hypothesis performance assessment purposes motivating proposed algorithm boosting restrict moment hypotheses range 
consider drawing examples random training set distribution random variables corresponding labels predictions respectively 
may consider joint distribution pd pd probability drawing example denote follows pd pr note probabilities pd pd pd pd simply weight true negative false positive false negative true positive predictions 
note joint distributions pd pd 
rows correspond correct labels columns correspond predictions 
error hypothesis simply sum false positive false negative weights error pd pd schapire singer shown performance adaboost optimized set follows :10.1.1.156.2440
error ln quality predictor half log odds making correct prediction 
setting distribution update rule effect uniformly scaling correctly classified examples factor similarly incorrectly classified examples uniformly scaled factor 
net effect scaling ensures error words uncorrelated respect error distribution motivation proposed boosting algorithm fact qualitative quantitative performance predictor entirely captured error 
performance captured joint distribution pd various measures performance calculated joint distribution error measure 
consider example predictors respective joint distributions pd pd shown 
error quite different qualitatively 
hypothesis lower false negative rate expense higher false positive rate 
information theoretic perspective information correct label gained predicts predicts 
fact information gain quantified bits entropy minus conditional entropy jh 
fact expected information gained correct label prediction precisely mutual information strongly believe weak hypothesis optimized respect measure experiments proposed boosting algorithm bear belief 
consider odds correct prediction actual prediction 
predicts odds correct predicts odds correct 
odds corresponds odds correct hypotheses 
negative predictions trustworthy positive predictions assess relative trustworthiness joint distribution 
propose knowledge boosting algorithm adaboost assigns weight xm ym initialize 
train weak learner distribution 
get weak hypothesis 
choose 
update exp normalization factor chosen distribution 
output final hypothesis sign infoboost predictions weak hypothesis proposed boosting algorithm assigns individual weights possible labels predicted hypothesis predictions restricted range weight assigned adaboost proportional log odds correct prediction 
proposed modification adaboost derive optimal weights assigned individually positive negative predictions provide evidence theoretical empirical proposed algorithm lower training testing error 
hypothesis predictions allowed techniques provide alternate derivation weights schapire singer assign block hypotheses :10.1.1.156.2440
analysis suggests information theoretic explanation weights confidence rated hypotheses may perform non confidence rated hypotheses 
infoboost section describe analyze proposed modification adaboost motivated previous section 
descriptions analyses modeled closely schapire singer :10.1.1.156.2440
consider proposed boosting algorithm shown purposes distinction refer boosting algorithm infoboost 
infoboost mechanically quite similar adaboost important regards 
round boosting performance parameters calculated chosen 
motivated previous section function quality negative prediction function quality positive prediction 
second distribution update rule effectively recognizes classes examples false positive false negative true positive true negative update rule scales weight example class 
third final hypothesis weights outputs individual weak hypotheses differently depending actual prediction values 
analysis analyze performance infoboost derive optimal settings 
parameters analysis closely modeled schapire singer :10.1.1.156.2440
calculate final distribution examples unfolding distribution update recurrence dt dt exp exp note exponent numerator quantity quite similar final hypothesis 
letting sign dt exp indicator variable predicate true 
sequence implications 
exp exp exp exp cases exp 
bound weight misclassified examples need sum indicator variable examples divide initial distribution uniform 
applying fact equation 
exp result analagous adaboost 
theorem training error infoboost error optimizing order optimize performance infoboost ensure small possible round boosting 
round exp schapire singer fix set :10.1.1.156.2440
allow confidence rated predictions restrict range 


solving equations find minimized ln ln plugging optimal values 
equation prd 
prd 
prd prd theorem weak hypotheses range training error infoboost error prd 
prd 
confidence rated hypotheses range outside note optimal settings 
determined numerically manner similar suggested schapire singer :10.1.1.156.2440
comparison adaboost results compare favorably results obtained adaboost 
schapire singer show adaboost error consider fixing local distribution running boosting round adaboost infoboost weak learner hypothesis returned weak learner :10.1.1.156.2440
note prd 
prd 

furthermore concave function simple convexity argument prd 
prd 
inequality strict prd prd zero 
may conclude upper bound infoboost upper bound adaboost weak hypothesis optimized adaboost 
course users adaboost reasonably seek weak learners optimize users infoboost reasonably seek weak learners optimize expression righthand side equation 
case difference bounds greater 
note difficult fairly compare performance adaboost infoboost discussion comparing upper bounds general distributions round likelihood different 
effect seen equation particularly easy interpret hypotheses restricted range hypotheses confidence ratings 
case error weak hypothesis conditional error hypothesis prediction conditional error hypothesis prediction 
bound adaboost bound infoboost prd 
prd 
plot illustrates difference bounds 
information theoretic perspective motivated earlier believe weak learners maximize mutual information equivalently minimize conditional entropy learning boosting maximizing mutual information uncertainty entropy correct label reduced possible plot function 
axis error hypothesis respective conditional errors predictions respectively 
axis bound adaboost bound infoboost 
information theoretic sense 
section argue infoboost weak learners non confidence rated version adaboost general 
experimental results section bear 
consider hypotheses confidence rated 
discussed section effect adaboost update rule create new distribution predictions previous hypothesis uncorrelated correct labels error measure error 
believe better measures correlation error 
notation described section consider joint distribution pd 
propose probabilistic information theoretic measures correlation 
fact considering complete independence measures equivalent random variables probabilistically independent mutual information zero precisely joint distribution product marginal distributions 
adaboost update rule guarantee independence infoboost update rule guarantee probabilistic information theoretic independence respect argue example argue follows 
hypotheses confidence rated optimal settings follows ln ln half log odds correct predicting half log odds top bottom joint distributions pd pd adaboost update rule pd infoboost update rule 
rows correspond correct labels columns correspond predictions 
note joint distribution produced adaboost update rule product distribution independent respect joint distribution produced infoboost update rule product distribution 
correct predicting 
effect infoboost update rule ensure odds infoboost update rule ensures weights true negative false negative examples identical weights true positive false positive examples identical 
easily show joint distribution satisfying requirements product marginals error associated joint distribution 
infoboost update rule guarantees respect predictions uncorrelated correct labels probabilistic information theoretic sense 
effect facts practice follows 
possible weak hypothesis maximizes mutual information error rate 
situation adaboost cease improve adaboost update rule modify distribution examples weak hypothesis chosen subsequent rounds 
experiments detailed section see effect adaboost regularly quite early typically th boosting round experiments 
happen infoboost previous hypothesis mutual information zero respect current distribution 
fact infoboost performs best practice weak learner maximizes mutual information results section 
note confidence rated hypotheses allowed 
weights derived equations identical confidences assigned schapire singer analogous block domain partitioning hypothesis :10.1.1.156.2440
probable explanation confidence rated predictions markedly outperform non confidence rated predictions confidence ratings distribution update rule guarantee respect subsequent distribution current weak hypothesis uncorrelated correct labels strongest possible information theoretic sense 
note simply uncorrelated error sense weaker condition 
experiments initially motivated trec text retrieval conference batch filtering task conducted number experiments trec data section 
experiments comparing adaboost infoboost uci machine learning datasets currently conducted preliminary results monk data sets subsequent section 
trec experiments trec filtering task large collection text documents provided collection topics relevance judgments 
topic specification interest eastern europe topic relevance assessments subset documents provided 
ran experiments data trec 
competition document collection entire set ap newswire articles relevance judgments provided topics 
topics competition anomalous example number topics handful just judged relevant document 
eliminated topics fewer judged relevant documents conducted experiments remaining topics documents relevance assessments provided 
learning curves similar topics report average prediction errors topics 
created weak hypotheses words follows 
word collection weak hypothesis corresponding word predicts word appears document 
considered anti word predictors identical corresponding word predictors signs predictions 
devised weak learners 
error returns weak hypothesis minimizes usual prediction error respect distribution 
weak learner optimized adaboost sense described section 
opt returns weak hypothesis minimizes respect distribution expression right hand side equation 
weak learner optimized infoboost sense described section 
strictly speaking anti words unnecessary adaboost infoboost predictors 

mutual information returns weak hypothesis maximizes mutual information respect distribution 
trec competition training occurred ap collection testing occurred collections 
conducted boosting experiments topics 
adaboost error weak learner 

adaboost mutual information weak learner 

infoboost error weak learner 

infoboost opt weak learner 

infoboost mutual information weak learner 
results 
respect training error note rounds boosting adaboost mutual information weak learner ceases progress 
likelihood due reasons outlined section 
remaining combinations adaboost error weak learner performed worst followed infoboost error weak learner infoboost mutual information weak learner infoboost opt weak learner performed best 
respect testing error adaboost mutual information weak learner ceased improve roughly rounds boosting performance worse leveling error 
remaining combinations adaboost error weak learner performed worst followed infoboost error weak learner 
infoboost opt weak learner infoboost mutual information weak learner performed best performance virtually indistinguishable rounds boosting 
uci experiments monk data sets available university california irvine machine learning repository particularly simple experiment test sets provided attributes discrete labels binary 
preliminary results data sets figures 
note performance infoboost opt weak learner infoboost mutual information weak learner essentially identical learning curves quite superimposed 
particularly interesting results monk data set classification noise deliberately added training set 
noisy data set infoboost test error substantially lower adaboost 
note trivial hypothesis predicts average training testing error combination adaboost mutual information weak learner negligible performance 
www ics uci edu mlearn mlrepository html adaboost error adaboost mi infoboost error infoboost mi infoboost opt adaboost error adaboost mi infoboost error infoboost mi infoboost opt average training testing errors combination systems trec filtering data 
left plot training error right plot testing error 
axis corresponds boosting round axis corresponds average error topics 
adaboost error adaboost mi infoboost error infoboost mi infoboost opt adaboost error adaboost mi infoboost error infoboost mi infoboost opt average training testing errors combination systems monk data set 
left plot training error right plot testing error 
axis corresponds boosting round axis corresponds prediction error 
adaboost error adaboost mi infoboost error infoboost mi infoboost opt adaboost error adaboost mi infoboost error infoboost mi infoboost opt average training testing errors combination systems monk data set 
left plot training error right plot testing error 
axis corresponds boosting round axis corresponds prediction error 
adaboost error adaboost mi infoboost error infoboost mi infoboost opt adaboost error adaboost mi infoboost error infoboost mi infoboost opt average training testing errors combination systems monk data set 
left plot training error right plot testing error 
axis corresponds boosting round axis corresponds prediction error 
described modification adaboost assessed performance theoretically empirically 
provided theoretical empirical evidence proposed boosting scheme lower training testing error original non confidence rated version adaboost 
modified boosting algorithm analysis suggests information theoretic explanation boosting confidence rated predictions markedly outperforms boosting confidence rated predictions 
strongly feel weak hypotheses chosen information theoretic criteria boosting algorithms designed information theoretic principles 
results suggesting adaboost analyzed information theoretically hope provide impetus study boosting information theoretic opposed decision theoretic light 
mark montague dan conducting experiments section david latham implementing system experiments 
eric bauer ron kohavi 
empirical comparison voting classification algorithms bagging boosting variants 
unpublished manuscript 
leo breiman 
classifiers 
annals statistics appear 
harris drucker corinna cortes 
boosting decision trees 
advances neural information processing systems 
yoav freund robert schapire 
experiments new boosting algorithm 
proceedings thirteenth international conference machine learning 
yoav freund robert schapire 
decision theoretic generalization line learning application boosting 
journal computer system sciences 
kivinen manfred warmuth 
boosting entropy projection 
proceedings twelfth annual conference computational learning theory 
richard maclin david opitz 
empirical evaluation bagging boosting 
fourteenth national conference artificial intelligence 
quinlan 
bagging boosting 
thirteenth national conference artificial intelligence 
robert schapire 
output codes boost multiclass machine learning problems 
proceedings fourteenth international conference machine learning 
robert schapire yoav freund peter bartlett wee sun lee 
boosting margin new explanation effectiveness voting method 
annals statistics appear 
robert schapire yoram singer :10.1.1.156.2440
improved boosting algorithms confidence rated predictions 
proceedings eleventh annual conference computational learning theory 
