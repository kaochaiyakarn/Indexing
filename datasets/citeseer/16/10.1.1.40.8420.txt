feedforward recurrent networks systematic 
analysis implications connectionist cognitive architecture steven phillips electrotechnical laboratory running head networks systematic 
janet wiles simon dennis helpful discussions earlier version jeff elman mike humphreys michael johnson critical comments members information science division discussion analysis methods anonymous reviewers critical comments suggested improvements 
began department computer science university queensland support scholarship completed information science division electrotechnical laboratory support sta fellowship 
author address information science division electrotechnical laboratory tsukuba ibaraki japan 
email etl go jp 
tel 
fax 
human cognition said systematic cognitive ability generalizes structurally related behaviours 
connectionist approach cognitive theorizing strongly criticized failure explain systematicity 
demonstrations generalization notwithstanding show widely networks feedforward recurrent support systematicity condition local input output representations 
connectionist explanation systematicity results leave choices develop models capable systematicity local input output representations justify choice similarity nonlocal component representations sufficient systematicity 
years systematicity debate implications cognitive architecture little impact day day practices connectionists version cognitive science 
despite seminal papers fodor pylyshyn smolensky numerous rebuttals sides see fodor mclaughlin fodor smolensky architectures debate appear connectionists choice plunkett elman 
fact enthusiasm rethinking series see cover note elman bates johnson smith parisi plunkett suggests debate effect general program establishing principles connectionist cognitive architecture 
goal purpose analyze widely networks feedforward recurrent networks capacity support systematicity 
main result network systematic sense learning behaviours conveys appropriate generalization structurally related behaviours 
means support appropriate generalization defined section term systematicity significance cognitive architecture 
relative definition simulations analyses conducted feedforward recurrent networks capacity demonstrate systematicity 
implications results discussed terms principles connectionist cognitive architecture 
systematicity generalization basic observation human cognition cognitive ability organized groups collections behaviours 
distributed arbitrarily space possible behaviours cognitive abilities organized particular regularity 
just firing electrons sheet material reveals high degree atomic structure physics probing mental capability humans suggest degree structure cognition 
structure grouping cognitive ability referred systematicity implication cognitive architecture systematicity debate 
examples adapted fodor pylyshyn illustrate organized nature cognitive ability 
ability infer john went store john mary went store ability related inference mary went store mary john went store 
inferences share logical structure rephrased cognitive states inference second 
inference capabilities says structurally unrelated inference plus equals 
classical solution systematicity posit structured representations processes sensitive structure representations fodor pylyshyn 
example having representational state identifies components cognitive process maps state new state identifying process extends situation input output mapped states john mary mary john 
parser example sort architecture 
half fodor pylyshyn argument connectionism architectural components network activations nodes weighted connections enforce systematicity 
briefly localist case node represents complex object structure sensitive processing supported simply labeling angle electrons scattered random quantized dependent structure material fired 
half connectionist demonstrations systematicity implement classical architectures 
concerned identifying connectionist properties don enforce systematicity 
see fodor smolensky continued arguments implementation issue 
node corresponding representation 
node activity inputs network processes labels causal consequences support systematicity 
distributed case representations activations collections nodes activity vectors suffice general decomposition vectors structural components supposedly represent uniquely determinable 
may accept associative networks sufficient capture systematicity demonstrations generalization complex networks utilizing distributed representations processes answered fodor pylyshyn objections 
generalization results demonstrations systematicity need know relevant architectural properties replicated avoided connectionist systems designed incorporate systematicity 
learning additional connectionist property enforces systematic behaviour suitable criterion systematicity generalization specified 
rest criteria devised hadley considered requirement systematicity 
difficulties treating systematicity generalization problem 
difficult determine exactly level generalization achieved humans 
uncertainty exaggerated lack control subject background knowledge strategies employ task 
take bounds approach yield interesting results 
example safely identify lower bound degree generalization exhibited humans immediately rule class architectures meet lower bound 
results form 
strong systematicity linguistic evidence generalization hadley formulated criterion systematicity 
hadley definition model regarded strongly systematic demonstrates correct performance components novel syntactic positions assumed generalization performance chance level 
possible output responses takes activated output unit network response base level chance demonstrating generalization novel position 
qualify demonstration generalization 
assume human performance significantly chance level refer experimental support assumption discussion section 
example strong systematicity correctly infer john lover mary premise john loves mary having trained examples john appeared object position binary relation loves 
hadley models mcclelland kawamoto chalmers elman smolensky st john mcclelland demonstrate strong systematicity 
essentially statistical analysis training sets likelihood components appeared allowable positions 
result raises question lack strong systematicity due fundamental limitation connectionist models simply intention demonstrate strong systematicity phenomenon 
section attempt address question analysis simulation number connectionist architectures 
analysis point regarding subsequently hadley definition strong systematicity includes correct performance components novel positions novel levels embedding mary knew tom hit john 
simulations analyses focus definition negative results clearly apply extended definition 
see niklasson van gelder generalization criteria systematicity 
component representations 
component similarity niklasson claims demonstrated strong systematicity transformation logical expressions task 
component representations presupposed similarity position role complex expression 
example proposition symbols shared common feature vector representation 
furthermore called novel component network tested representation common feature features vector components active 
consequently generalization demonstrated due common feature network trained components considered demonstration strong systematicity explanation propositional symbols obtained similar representations 
surface similarity syntactically convenient way denoting role component 
sole basis structure sensitivity identical component representations fill different structural roles 
suppose subject infer mary statement john loves mary question john loves 
property strong systematicity infer dogs statement john loves dogs question 
may reasonable assume nouns similar internal representation surface external similarity basis inference 
suppose subject infer loves statement john loves mary question john mary 
property systematicity infer dogs statement john dogs mary question 
second case surface representation physical stimulus see hadley similar comment mcclelland kawamoto 
chases 
supposed identify different structural role verb 
course reason inferences possible surrounding context 
novel input may derive similar internal representations context relied possess inherently similar external physical characteristics 
implication connectionist modeling similar input output representations basis demonstrations strong systematicity explain representations arise 
similar component representations occurs chalmers words common class verbs nouns share common feature vector representations niklasson van gelder component representations objects belonging common category proposition conjunctive constructed vectors representing categories 
models assume similarity connectionist networks expected learn 
point systematicity property level compositional objects level component objects 
language systematicity property level sentences level words fodor pylyshyn 
ability represent process sentences particular structure relates sentences conforming structure 
control effect component similarity demonstrations systematicity orthogonal local representations represent components subsequent simulations analyses 
feedforward network tasks considered section simple recurrent network depend processing tuples ordered pairs atomic objects 
tuples simplest complex object demonstrating generalization position 
humans deal complex objects simplicity tuples attractive analysis 
tasks involve auto association tuples test capacity represent object task vector component rest 
querying second component tuple test inference capacity task 
generalization lack generalization position auto association inference task referred strong weak systematicity representation inference 
hadley definition includes generalization recursively embedded structures 
lower bounds approach taken means failure demonstrate systematicity task implies failure tasks involving complex structures 
architecture feedforward network examined consists layers input hidden output unit component position input output layers 
insert task auto association tuples demonstrate systematicity representation network capable constructing representations structurally related objects 
connectionist framework representations vectors activation space construction internal representations simply allocating vector internal hidden unit activation space 
useful representation representations component objects subsequently accessible 
auto association tasks useful require networks construct access representations structured objects 
simplest structured objects test capacity tuples binary relation loves subject object 
demonstrate strong systematicity network demonstrate correct processing instances components positions occur training set 
task example correctly auto associating tuple john mary having examples mary subject position 
analysis weak systematicity representation assuming local representation see output unit dedicated detecting particular component particular position network hidden unit internal representation complex object 
example output unit detects presence absence mary subject position 
correct performance task depends orienting mary subject hyperplane position points training set classes mary subject position mary subject position 
training points network correctly classify test cases 
output unit detects mary object position class points training set mary object position requirement demonstrating strong systematicity 
class points training set provides information regarding orientation hyperplane correctly classify points mary object position network expected generalize components appearing subject position components appearing object position 
case network said weakly systematic generalization novel combinations may demonstrated novel positions hadley 
course fortuitous set starting weights may permit network generalize new position require network demonstrate generalization position chance level 
number ways hyperplane oriented fortuitous arrangements 
concluded feedforward network demonstrate strong systematicity respect task case local input output representations 
simple recurrent network architecture lack strong systematicity feedforward network due independence weights accessing components different positions 
simple recurrent network elman inputs outputs represented set units transformed set weights 
components complex objects temporally spatially simple recurrent network possibility exhibiting strong systematicity auto association tuples task 
insert task auto association tuples task recurrent network essentially feedforward network described previous subsection presenting components simultaneously components time step 
presenting components time network constructed internal representation ordered pair network required output ordered pair order 
shows example network input output mapping required perform trial network consisted ffl random generation training examples distribution atomic simple recurrent network elman trained perform word prediction set sentences generated grammar 
objects appear position atomic objects appear second position 
weights updated sequence patterns 
context units reset zero sequence 
ffl random initialization weights uniform distribution 
ffl training network standard error backpropagation algorithm learning rate momentum term sum squares error function rumelhart hinton williams performance training set target output units patterns training set 
ffl testing remaining combinations atomic object position combined atomic object left second position training set 
ffl recording network response output patterns test set 
testing criteria 
network response considered correct maximally activated output unit target activation maximum criterion output units target criterion 
addition number training examples ordered pairs varied 
network initialized random set weights train test trial repeated times training set size 
result strong systematicity representation recurrent network showed perfect performance maximum testing criteria unseen object second position test set trained ordered pairs 
trained ordered pairs performance dropped mean maximum criterion criterion trials 
insert analysis due high dimensionality network internal representational space number hidden units general difficult determine nature internal representations constructed network solve particular task 
examination relationship hidden unit activation principal components analysis pca elman canonical discriminants analysis cda wiles reveal dimensions greatest importance 
internal representations learnt simple recurrent network analyzed case network demonstrated generalization position trials training set consisted training examples 
network trained training criterion training examples network resulting hidden unit activation patterns saved 
full details analysis provided phillips 
plots internal space guide nature internal representations suggested techniques 
second time step network construct representation pair maintains information regarding second components pair 
help identify information pca points second time step performed 
comparison variance hidden unit versus principal component dimensions shows number significant principal components number hidden units 
components account variance 
activations hidden units correlated suggesting multiple hidden units coding information 
insert hidden unit activations plotted second principal components 
plot shows levels spatial organization 
level points grouped clusters output mapping level appears cluster organization points labeled object 
consistency suggested rotational ordering points cluster 
insert consistency cluster organization suggests object information encoded dimensions independent virtue apparent regularity second object information 
suggestion correct may exist directions hidden unit activation space clusters superimposed 
words may projection points hidden unit activation space example point representing pair karen john point representing pair karen ann 
direction exists cda grouping points basis object 
cda identify dimensions hidden unit activation space points group karen ann karen bill karen karen karen john close points different groups far apart 
cda performed hidden unit activations second time step grouped object time step 
points plotted clusters object vivian left second position order test strong systematicity 
second canonical discriminants 
plot shows clusters superimposed supports suggestion object information encoded independent second object information 
case points belonging cluster differed location gamma insert similar form internal representation rd th time steps phillips suggesting network extracted components set dimensions 
encoding decoding component representations input output sets weights respectively network requires training unique object positions necessarily simulations demonstrated 
task querying tuples task simple recurrent network ordered pair query question requests second component ordered pair 
example pair john bill question requests component network respond john 
auto association tuples task possible atomic objects may appear second positions 
addition query objects request second component ordered pair 
simulation conditions querying tuples task follows ffl local encoding input output vectors 
possible input vectors task possible components plus possible queries input units encode input vectors output units encode possible components 
ffl generation training sequences sequence consisting patterns consisting components position combined possible components second position combined possible queries 
fifth component test strong systematicity 
test set consisted sequences components position combined fifth component second position combined queries 
ffl random initialization weights uniform distribution range gamma 
ffl training network standard error backpropagation algorithm learning rate momentum term sum squares error function rumelhart performance training set reached criterion 
training criteria output units outputs target output unit training pattern 
network reach criterion th epoch epoch presentation training pattern training terminated 
weights updated sequence patterns 
context units reset zero sequence 
ffl testing remaining tuples criteria correctness maximally activated output unit corresponds target activation maximum criterion output units target activation criterion 
ffl train test trial repeated times training criterion weights randomly initialized trial 
simple recurrent network examined hidden units 
case hidden unit network training criterion 
result weak systematicity inference case hidden units simple recurrent network learnt perfectly training set trials training criteria output units target outputs patterns training set 
trials performance test set zero maximum testing criteria 
network correctly respond trial sequences network second component target 
case hidden units training reached criterion trials 
trials regardless network reached training criterion performance test set zero testing criteria regard extraction second component 
hidden units trials converged training criterion 
trials performance zero maximum testing criteria extraction second component test set 
trials test set performance maximum testing criterion testing criterion 
possible components level performance better chance 
furthermore trials network acquire perfect performance training set epochs 
results summarized table 
insert table analysis querying tuples task network ordered pair question requesting second component pair 
suppose mary component network demonstrate strong systematicity 
solve task network implement function mary mary mary mary mary mary mary mary question vectors mary representation ordered pair mary position object second position mary representation ordered pair mary second position object position function implements mapping 
notation refer network internal representation object may different object external representation 
correctly discriminate mary component hyperplane partitioned satisfy inequalities wc mary wc mary wc mary wc mary wc input hidden context hidden weight vectors respectively bias weight unit threshold component mary considered correct response 
subtracting equation equation equation equation leaves gamma gamma exist weight satisfies inequalities network represent solution single hyperplane component 
consequently correct extraction mary component requires hyperplanes positioned input space hidden units hidden layer hyperplane positioned hidden space output unit detecting mary component correct orientation hyperplanes hidden layer depicted 
circles indicate mary target output squares indicate mary target output 
numerals indicate request second component indicates component object just difference vector 
insert assuming hidden unit hyperplanes correct position resulting representation hidden layer types points dependent high low side relative threshold hyperplanes shown 
hidden unit activation space point types partitioned single hyperplane groups mary circles mary squares 
clearly partitioning possible 
addition network required demonstrate strong systematicity mary component 
training set analysis essentially parallels explanation perceptrons solve exclusive task hertz krogh palmer minsky papert 
positions mary second position occur empty circle square 
consequently training set provide information regarding partitioning points vertical dimension network expected generalize cases 
insert critical difference inference task third time step network may required extract component second position 
case auto association task demanded position component extracted third time step second position component extracted fourth time step 
case components extracted set weights different times 
inference case different sets weights trained perform extraction 
network demonstrate strong systematicity respect task 
discussion simulations analyses previous sections demonstrated general architectures feedforward recurrent networks local input output representations necessary properties support systematicity 
fodor pylyshyn argued local internal representations support systematicity 
technical reasons limitation applies distributed representations local input outputs networks 
results general depend size training set number internal units connections 
lack generalization connections map component representations differ ent positions learned independently 
special case autoassociation tuples recurrent network degree generalization position demonstrated 
apparent solution utilized group weights retrieve component resembled buffer 
solution suffice task 
positive example said sufficient support systematicity 
way attempting defer implication results argue human cognition systematic fodor pylyshyn claim human cognition context sensitive properties systematicity ideals best 
connectionist models systematic may provide reasonable approximation human cognition 
van gelder niklasson argued position experimental results logical inferences 
subjects significantly better inference thematic examples 
lack systematicity may reflect familiarity domain 
psychological experiments bain submitted domains schema induction clearly demonstrated situations perfect symbol generalization subjects 
experiments 
submitted subjects series tasks task consists learning mapping string shape pairs strings 
task letter strings shapes kel triangle predicts goh goh square predicts dus 
string paired shape generate mappings 
strings shapes meaningless chosen randomly new task 
relations strings shapes task conform specific structure structure task new task uses new set strings shapes 
structure task instance visualized square string lies vertex shape consistently horizontal example raining clouds 
clouds 
raining instance inference modus vertical transition vertex identifying predicted string 
significance experiments fourth task subjects perfect predictions seeing possible mappings 
fact degree generalization goes strong systematicity generalization novel position strings appear mappings implications systematicity connectionist cognitive architecture easily dismissed grounds human cognition best quasi systematic phillips 
subjects highly experience particular problem knowledge spatial analogues greatly facilitate performance 
subjects call knowledge acquired course development alter point 
reiterates need develop techniques encode accessible information 
worth noting possible technique weight sharing sufficient demonstrate degree learning transfer see example hinton support degree transfer experiments phillips submitted 
connectionist networks lacking strong systematicity weak systematicity result simple recurrent network assumptions nature internal representations ordered pairs 
assumptions external representations components encoded locally requiring intermediate layer order units inputs outputs order units 
result applied architectures conforming restrictions 
see suppose strings labeled shapes triangle square 
presentations subjects see triangle predicts square predicts third presentation triangle subjects correctly predict appear previous mappings previous tasks 
weights shared subnetworks encode knowledge common various tasks subnetwork dedicated 
feedforward network context units simple recurrent network regarded group input units holding representation ordered pair 
sets input units map representations hidden units turn map representations output units 
specific assumptions regarding nature order pair representation layered feedforward networks weakly systematic inference 
jordan recurrent network jordan recurrent network see differs simple recurrent network context copied output unit activations hidden unit activations previous time step 
simple recurrent network network requires hidden units discriminate component training set provides information retrieving component missing position 
network weakly systematic inference 
insert pollack recursive auto associative memory pollack recursive auto associative memory see essentially simple recurrent network additional set output units targets current context values network auto associates current input current context 
units associated weights play part extraction components 
purpose encourage formation useful representations context layer 
generalization crucially dependent training weights units representing retrieved components architecture demonstrate strong systematicity inference task 
forced simple recurrent network forced simple recurrent network variation recursive auto associative memory targets output units previous context current input input 
additional information hints regarding target function improve learnability networks see example abu mostafa 
recursive auto associative memory additional output units play part extraction components forced simple recurrent network 
network demonstrate strong systematicity inference task 
recurrent networks higher order connections research generalization conducted networks higher order weighted connections weights multiply add incoming activations see example giles 
specifically addressed systematicity predictions comparing number free parameters higher order networks order networks analyzed 
number higher order connections layers units great number order connections general times additional number parameters generalization position completely connected networks number training patterns 
analysis simple recurrent network network hidden units sufficient represent task assuming possible components position 
input units output units total number weights additional units represent question inputs 

second order network require hidden layer decision boundaries curved surfaces hyperplanes case input context units connected output units 
total number second order connections unique pairs units layer connected units second layer theta number free parameters greater generalization position training set 
fully connected higher order network support strong systematicity 
simple recurrent network word prediction christiansen chater test strong systematicity simple recurrent network word prediction task 
claim limited supported strong systematicity experiments analysis shows support strong systematicity 
simple recurrent network trained predict words sentences conforming particular phrase structured grammar 
second experiment supposedly demonstrating strong systematicity network sentence fragment mary says john boy point plural verbs prepositions correctly predicted 
word boy appear position training set authors claim demonstration strong systematicity 
case prediction plural verb preposition require knowledge word boy 
fact requires knowledge noun 
network received input word correctly predicted singular plural noun follow 
activations boy boys minimal compared nouns emphasis added suggesting played role prediction 
hidden units contained information nouns john mary necessarily boy information copied context units time step time boy meaning complexity better worse input units 
information context units predict word learned examples training set include boy 
christiansen chater remarked suggestion hadley network achieved correct prediction novel word boy appeared sides conjunction 
demonstration change explanation intervening word predicts noun phase grammar 
best experiment ambiguous issue strong systematicity 
better test prediction novel words examined experiment 
experiment network received word mary correctly predicted singular plural noun follow 
christiansen chater point predicted nouns included novel nouns girl girls 
light results second experiment provide evidence strong systematicity 
networks demonstrating strong systematicity hadley definition strong systematicity extended included recursively embedded structures 
relative definition model said strongly systematic example having trained instances mary appears subject position generalizes instances john loves mary tom knows john loves mary 
complex networks satisfying definitions transformation network niklasson van gelder associative semantic network hadley hayward definition systematicity revised include capacity assign appropriate meanings words test sentences hadley hayward 
degree systematicity called strong semantic systematicity may explain prediction plural verbs test case see 
network satisfied stringent criterion demonstrating generalization components appear training set 
network satisfies strong semantic systematicity hadley hayward requires identifying roles agent patient components play novel positions novel levels embedding 
definition considered 
exhibited associative semantic network 
transformation network transformation network niklasson van gelder consists recursive autoassociative memory raam networks pollack feedforward network 
task transformation network learn transformations logical expressions 
raam trained encode vector representations logical expressions atomic constituents raam trained decode representations expressions back constituents 
feedforward network trained map representations logical expressions 
network demonstrated strong systematicity having trained expressions containing propositions network generalized expressions containing plot hidden unit activations trained network niklasson van gelder showed similar form level clustering simple recurrent network 
graph clearly shows clusters second proposition cluster positioning proposition 
organization suggests component objects decoded complex representations set dimensions weights 
interestingly transformation task variation autoassociation task task simple recurrent network examined 
propositions encoded retrieved manner addition intervening connectives 
light analysis conducted simple recurrent network solution sense 
network demonstrated generalization novel constituent appear training set 
result surprising weights input unit representing updated training activation patterns 
examination encoding constituents suggests generalization achieved 
propositions unit input vector set distinguish symbols 
novel constituent received training distinguish connectives 
addition minimum euclidean distance determine correct constituent works locally encoded constituents 
regardless input hidden weights component novel proposition projected direction propositions activations inputs representing constituents zero local representation 
closest point novel constituent 
interesting extension simulations inference task task network required extract specific value specific constituent complex representation 
light results simple recurrent network expect transformation network demonstrate strong systematicity tasks 
semantic network associative semantic network hadley hayward quite different structure functionality previously discussed networks 
sufficient space full explanation network 
observation suggests network achieves generalization position 
task consists representing semantic content sentence 
semantic concepts principal proposition agent action patient individual words represented individual nodes 
network learns association inputs nodes representing words target semantic representations 
point note architecture respect generalization position direct connections input nodes position role nodes agent patient 
formation input position nodes mitigated single proposition node 
weight vectors input layer position node effectively related scalar factor 
linking input position nodes single intermediate node introduces weight dependency position 
suggested property facilitates generalization position sentences associative semantic network 
implications connectionist cognitive architecture main implication results architecture higher cognition completely connected networks coupled learning function exemplified feedforward recurrent networks 
networks permit states counterpart higher cognition specifically states weak systematicity 
negative results imply connectionism unsuitable cognitive modeling tell echo fodor mclaughlin additional principles units weights organise network connectivity way enforce systematicity 
possibility tensor networks smolensky connectivity arranged way implement inner fi outer omega product operators 
tensor networks introduced smolensky connectionist method representing symbol structures 
important property tensors construction recovery component representations occurs set units inner product operator independent component value position complex object 
learning extract component position automatically extends positions 
example illustrates 
suppose john loves mary instance binary relation loves 
may domains humans weakly systematic 
fodor mclaughlin fodor rejected solution implementation issue issue realizing systematicity 
smolensky relational instances represented tensors jm omega omega vector representations components john mary respective roles 
assuming orthonormal person loved determined jm fi case automatically extends related instance mary loves john properties inner outer products fi omega omega fi fact extends instances relation fi fi similarly second component accessed role tensors representation scheme included larger learning network appropriate role vectors learned demonstrate generalization position phillips 
network address hadley strong systematicity criteria including generalization novel levels embedding subject study 
important point additional principles built connectionist networks demonstrate systematicity 
component similarity revisited argued rely priori similarity component input output representations basis systematicity 
restriction preclude network constructing similar internal component representations previous learning basis systematicity subsequent tasks 
suppose prior learning categorization previously dissimilar input stimuli assigned similar internal representations 
extreme case components tuples task evenly distributed single internal unit dimension ann bill john 
new internal unit serve input subnetwork purpose autoassociation task 
case auto association task requires learning element vector mapping auto association john bill mapping 

linear relationship new similarity input output representations tuples necessary generalization remaining pairs feedforward network input units completely connected output units 
components appeared positions training set 
components appeared training condition previous categorization task respect auto association network said demonstrated strong systematicity 
clearly choice component representation significant impact generalization 
permitting arbitrary input output representations potentially issue 
previous example demonstrated generalization may ensured making novel components sufficiently close trained components independent structure belong 
insists component similarity basis generalization crucial question justifies particular choice component representation 
hindsight may obvious generalization weights extract components different positions trained independently 
observation come directly inspection network connectivity 
comes analysis connectivity conjunction learning task requirements example simple recurrent network exhibited generalization position task generalization network task 
feedforward recurrent networks systematic 
case local component representation networks require additional properties constraints enforce generalization position 
focussed property dependency weights associated representation components different positions 
dependency enforced group weights position tensor semantic networks 
may enforced learning method appeared case transformation network analysis needed area 
cognitive architecture network units coupled learning algorithm configuring pattern connectivity attractive 
fewer commitments design specific mechanisms realize cognitive behaviours 
commitments grounded partly observation learning external environmental events 
accepts requirements systematicity requirements met just type architecture 
additional properties necessary explain networks configured particular way exhibit systematicity additional subnetworks required preprocess potential components similarity representations may possible demonstrate strong systematicity 
way standard approach suffice 
computer science saw revolution software design called procedural rd generation languages relational th generation languages sql standard purposes modeling information systems 
analogously answers question respect structure related properties systematicity attention approaches connectionist modeling higher cognition 
abu mostafa 

learning hints neural networks 
journal complexity 
chalmers 

syntactic transformations distributed representations 
connection science 
christiansen chater 

generalization connectionist language learning 
mind language 
elman 

representation structure connectionist models 
tech 
rep university california san diego ca 
elman 

finding structure time 
cognitive science 
elman 

incremental learning importance starting small 
tech 
rep university california san diego ca 
elman bates johnson smith parisi plunkett 

rethinking connectionist perspective development 
neural network modeling connectionism 
cambridge ma mit press 
fodor 

connectionism problem systematicity continued smolensky solution doesn 
cognition 
fodor mclaughlin 

connectionism problem systematicity smolensky solution doesn 
cognition 
fodor pylyshyn 

connectionism cognitive architecture critical analysis 
cognition 
hadley 

compositionality systematicity connectionist language learning 
tech 
rep css tr simon fraser university burnaby bc 
hadley 

systematicity connectionist language learning 
mind language 
hadley hayward 

strong semantic systematicity unsupervised connectionist learning 
moore lehman eds proceedings seventeenth annual conference cognitive science society pp 

bain 
submitted 
induction relational schemas common processes reasoning learning set acquisition 
www psy uq edu au people department gsh 
hertz krogh palmer 

theory neural computation 
redwood city ca addison wesley 
hinton 

mapping part hierarchies connectionist networks 
artificial intelligence 
jordan 

serial order parallel distributed processing approach 
tech 
rep mit cambridge ma 


forcing simple recurrent networks encode context 
tech 
rep new jersey institute technology nj 
proceedings long island conference artificial intelligence computer graphics 
mcclelland kawamoto 

mechanisms sentence processing assigning roles constituents sentences vol 
computational models cognition perception chap 

cambridge ma mit press 
minsky papert 

perceptrons th edition 
cambridge ma mit press 
niklasson van gelder 

connectionist models exhibit non classical structure sensitivity 
ram eiselt eds proceedings sixteenth annual conference cognitive science society pp 

lawrence erlbaum 
niklasson van gelder 

systematicity connectionist language learning 
mind language 
niklasson 

structure sensitivity connectionist models 
proceedings connectionist summer school pp 

hillsdale nj lawrence erlbaum 
giles 

training second order recurrent neural networks hints 
sleeman edwards eds machine learning proceedings ninth international conference 
san mateo ca morgan kaufmann 
phillips 

strong systematicity connectionist framework network 
ram eiselt eds proceedings sixteenth annual conference cognitive science society pp 

lawrence erlbaum 
phillips 

connectionism problem systematicity 
ph thesis university queensland department computer science brisbane australia 
phillips 

systematicity psychological evidence connectionist implications 
langley eds proceedings nineteenth annual conference cognitive science society pp 

phillips 
submitted 
analysis learning transfer neural networks relevance connectionism 
plunkett elman 

exercises rethinking handbook connectionist simulations 
neural network modeling connectionism 
cambridge ma mit press 
pollack 

recursive distributed representations 
artificial intelligence 
rumelhart hinton williams 

learning internal representations error propagation vol 
computational models cognition perception chap 

cambridge ma mit press 
smolensky 

proper treatment connectionism 
behavioral brain sciences 
smolensky 

tensor product variable binding representation symbolic structures connectionist systems 
artificial intelligence 
smolensky 

connectionism constituency language thought 
macdonald macdonald eds connectionism debates psychological explanation vol 
chap 
pp 

cambridge ma blackwell 
smolensky 

reply constituent structure explanation integrated connectionist cognitive architecture 
macdonald macdonald eds connectionism debates psychological explanation vol 
chap 
pp 

cambridge ma blackwell 
st john mcclelland 

learning applying contextual constraints sentence comprehension 
artificial intelligence 
van gelder niklasson 

cognitive architecture 
ram eiselt eds proceedings sixteenth annual conference cognitive science society pp 

lawrence erlbaum 
wiles 

operators curried functions training analysis simple recurrent networks 
hanson lippman eds advances neural information processing systems 
san mateo ca morgan kaufmann publishers 
captions 
feedforward network auto association tuples 

simple recurrent network temporal version auto association tuples task 
parenthesized values indicate number units dashes indicate zero input 
starred outputs considered testing 

generalization second position function number training patterns log scale 
number correct responses maximum solid line dashed line test response criteria averaged trials 
error bars indicate confidence levels 
horizontal dotted line indicates chance level performance 

variance hidden unit activation principal components ordered magnitude 

principal components analysis points hidden unit activation space generated training sequences second time step 

canonical discriminants analysis points hidden unit activation space generated training sequences second time step grouped basis input object 

orientation hidden unit hyperplanes extract mary component 

orientation output unit hyperplane hidden unit activation space 

alternative recurrent network architectures jordan recurrent network pollack recursive auto associative memory 
mary loves john john loves mary mary object mary subject john mary 

john 

mary mary john copy back output hidden context input number training patterns maximum criterion criterion chance level hidden unit principal component principal component hidden units ann karen vivian john ann ann vivian bill bill john bill bill bill john john vivian ann bill vivian bill bill john bill john karen vivian john john john vivian bill karen ann bill karen john vivian john vivian john john ann bill bill vivian john karen bill ann vivian ann ann karen karen john vivian karen john john john karen vivian bill karen karen ann bill ann ann john karen bill vivian vivian karen john ann karen ann bill vivian vivian vivian ann bill ann karen bill john bill ann karen karen ann john bill vivian karen ann vivian ann vivian ann karen ann vivian ann vivian karen karen bill ann bill ann john bill karen john bill vivian karen john karen bill ann karen ann ann karen karen john ann karen bill bill john ann bill bill ann bill john ann vivian bill karen john john john bill john bill vivian ann karen john ann vivian bill ann karen bill john john karen bill bill bill bill john ann bill vivian bill ann john bill john karen vivian vivian ann vivian principal component second principal component john karen bill ann ann karen vivian john vivian john bill john john vivian ann bill vivian john bill john karen vivian vivian bill karen ann bill karen john vivian john vivian ann vivian john karen bill ann vivian john vivian karen karen vivian bill ann bill john karen bill karen john ann karen ann bill ann bill ann karen bill john bill ann karen karen ann john bill vivian karen ann vivian ann vivian ann karen ann vivian ann vivian bill ann bill ann john bill karen john bill vivian karen john karen bill ann karen john ann karen john ann ann bill john ann vivian bill karen bill john bill vivian ann karen john ann vivian bill ann karen bill karen bill john ann bill vivian bill ann john bill john karen ann vivian canonical discriminant second canonical discriminant mary mary mary mary mary mary mary mary mary output hidden input context copy output input context hidden copy context table summary performance simple recurrent network systematicity inference task networks hidden units 
hidden units convergent trials trials correct test cases train test max train test max trials trials 
