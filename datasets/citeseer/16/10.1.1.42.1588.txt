extracting support data task bernhard scholkopf chris burges vladimir vapnik bell laboratories corner road holmdel nj usa big att com big att com big att com report novel possibility extracting small subset data base contains information necessary solve classification task support vector algorithm train different types handwritten digit classifiers observed types classifiers construct decision surface strongly overlapping small subsets data base 
finding opens possibility compressing data bases significantly data important solution task 
addition show theory allows predict classifier best generalization ability solely performance training set characteristics learning machines 
finding important cases amount available data limited 
fayyad uthurusamy eds proceedings international conference knowledge discovery data mining 
aaai press menlo park ca learning viewed inferring regularities set training examples 
research devoted study various learning algorithms allow extraction underlying regularities 
matter different outward appearance algorithms rely intrinsic regularities data 
learning successful intrinsic regularities captured values parameters learning machine polynomial classifier parameters coefficients polynomial neural net weights biases radial basis function classifier weights centers 
variety different representations intrinsic regularities conceals fact stem common root 
permanent address max planck institut fur kybernetik tubingen germany supported arpa onr contract number study explore support vector algorithm algorithm gives rise number different types pattern classifiers 
show algorithm allows construct different classifiers polynomial classifiers radial basis function classifiers neural networks exhibiting similar performance relying identical subsets training set support vector sets 
sense support vector set stable characteristic data 
case available training data limited important means achieving best possible generalization controlling characteristics learning machine 
bound statistical learning theory vapnik predict degree yields best generalization polynomial classifiers 
section follow vapnik boser guyon vapnik cortes vapnik briefly algorithm idea structural risk minimization 
experimental results obtained support vector machines 
support vector machine structural risk minimization case class pattern recognition task learning examples formulated way set functions ff ff ff ff gamma index set necessarily subset set examples gamma generated unknown probability distribution want find function ff provides smallest possible value risk ff jf ff gamma yj dp problem ff unknown unknown 
induction principle risk minimization necessary 
straightforward approach minimize empirical risk remp ff jf ff gamma turns guarantee small actual risk small error training set imply small error test set number training examples limited 
limited amount data novel statistical techniques developed years 
structural risk minimization principle technique 
fact learning problem ff probability gamma bound ff remp ff phi log holds phi defined phi log gamma log delta gamma log parameter called vc dimension set functions 
describes capacity set functions implementable learning machine 
binary classification points separated classes possible ways functions learning machine possible separation exists function takes value class gamma class 
fixed number training examples control risk controlling quantities remp ff ff ff ff denoting subset index set empirical risk depends function chosen learning machine ff controlled picking right ff 
vc dimension depends set functions ff ff ff learning machine implement 
control introduces structure nested subsets sn ff ff ff ff ff ff ae ae ae sn ae corresponding vc dimensions satisfying hn set observations structural risk minimization principle chooses function ff subset ff ff ff guaranteed risk bound right hand side minimal 
support vector algorithm structure set hyperplanes 
particular choice structure gives rise learning algorithm 
support vector algorithm structure set hyperplanes 
describe note dot product space set vectors hyperplane fx delta corresponds uniquely pair theta additionally require min delta bj hyperplane corresponds decision function constructed way find smallest ball bx fx kx gamma ak rg containing points define decision function fw fw bx sigma sgn delta possibility introducing structure set hyperplanes fact vapnik set kwk ag vc dimension satisfying note 
dropping condition kwk leads set functions vc dimension equals dimensionality due kwk get vc dimensions smaller enabling high dimensional spaces 
support vector algorithm 
suppose set examples gamma want find decision function fw property fw function exists implies delta practical applications separating hyperplane exist 
allow possibility examples violating cortes vapnik introduce slack variables get delta gamma support vector approach minimizing guaranteed risk bound consists minimize phi delta fl subject constraints 
minimizing term amounts minimizing vc dimension learning machine minimizing second term bound 
term hand upper bound number misclassifications training set controls empirical risk term 
suitable positive constant fl approach constitutes practical implementation structural risk minimization set functions 
introducing lagrange multipliers ff kuhn tucker theorem optimization theory show solution expansion ff nonzero coefficients ff cases corresponding example precisely meets constraint 
called support vectors support vector expansion 
remaining examples training set irrelevant constraint satisfied automatically appear support vector expansion 
solution unique coefficients ff 
solving quadratic programming problem maximize ff ff gamma ff ff delta subject ff fl ff linearity dot product decision function written sgn ff delta delta far described linear decision surfaces 
appropriate tasks 
allow general decision surfaces nonlinearly transform input vectors high dimensional feature space map oe linear separation 
maximizing requires computation dot products oe delta oe high dimensional space 
certain conditions boser guyon vapnik expensive calculations reduced significantly function oe delta oe get decision functions form sgn ff delta experimental results experiments support vector algorithm standard quadratic programming techniques construct different types classifiers 
done choosing different functions decision function function maximized constraint ff ff gamma ff ff ffl homogeneous polynomial classifiers delta degree ffl radial basis function rbf classifiers exp gamma gamma oe delta ffl neural networks tanh delta delta gamma theta results reported obtained fl cf 

postal database handwritten digits training testing cf 
lecun 
digit theta vector entries gamma 
preprocessing consisted smoothing gaussian kernel width oe 
get class classifiers digit recognition problem constructed class classifiers trained separate digit combined doing class classification maximal output applying sgn function class classifiers 
performance various types classifiers results different functions summarized table 
compared values achieved database layer neural net lecun layer neural net human performance bromley sackinger 
table performance different types classifiers constructed support vector algorithm choosing different functions 
raw errors rejections allowed test set 
normalization factor sigmoid case chosen delta tanh 
class classifiers show average number support vectors class classifiers 
polynomial delta degree degree raw error av 
svs rbf exp gamma gamma yk oe delta oe raw error av 
svs sigmoid tanh delta gamma theta theta raw error av 
svs similar performance different functions suggests choice set decision table performance classifiers degree predicted vc bound 
row describes class classifier separating digit stated column rest 
remaining columns contain degree degree best polynomial predicted described procedure parameters dimensionality high dimensional space maximum possible vc dimension linear classifiers space estim vc dimension estimate actual classifiers smaller number free parameters linear classifiers space numbers errors test set polynomial classifiers degrees 
table shows procedure chooses polynomial degrees optimal close optimal 
chosen classifier errors test set degrees digit degree parameters estim delta delta delta delta delta delta delta delta delta delta functions important capacity control chosen type structure 
phenomenon known parzen window method density estimation choice kernel function critical choice appropriate value bandwidth parameter amount data 
predicting optimal decision functions set training data choices nested set decision functions estimate second term right hand side eq 

call term vc confidence 
cases measure empirical risk remp ff compute upper bounds expected risk ff expected error test set 
try bounds predict set decision functions give best performance test data validation test sets 
clearly useful situations amount available data limited 
test idea concentrated polynomial classifiers delta degree decision functions nested structure implemented varying degree polynomial 
estimated vc dimension making assumption bound met estim kwk case right hand side equation dominated vc confidence minimized vc dimension minimized 
kwk determined support vector algorithm order estimate need compute recall radius smallest sphere enclosing training data high dimensional space space corresponds dot product 
formulate problem follows minimize subject kx gamma determined position vector center sphere 
known quadratic programming problem 
objective function gamma gamma gamma vary get wolfe dual problem maximize delta delta gamma delta delta subject lagrange multipliers 
support vector algorithm problem property degree average vc dim 
total 
test errors average vc dimension solid total number test errors class classifiers dotted polynomial degrees degree remp comparably big vc dimension sufficient predicting cf 

baseline error scale corresponds total number test errors best class classifiers degrees 
graph shows vc dimension allows predict degree yields best performance test set 
necessarily case performances class classifiers built class classifier outputs applying sgn functions 
fact class classifier degree far performs better degree see leads believe improved principled way doing class classification 
appear dot products compute dot products high dimensional space replacing delta live low dimensional space high dimensional space provided functions satisfy certain positivity conditions 
way computed radius minimal enclosing sphere training data polynomial classifiers degrees 
trained binary polynomial classifier digit degrees 
obtained values approximation predict digit degree polynomial give best generalization performance 
compare prediction actual polynomial degree gives best performance test set 
results shown table cf 
fig 

method predicting optimal classifier functions gives results 
cases theory predicted correct degree cases predicted degree gave performance close best possible 
comparison support vector sets different classifiers remainder shall optimal parameters table studying support vector sets different types classifiers 
table shows classifiers support vectors class classifier training set 
total number different support vectors class classifiers 
reason times particular vector positive example digit say negative example gamma digit 
table row total number different support vectors different class classifiers number elements union support vector sets obtained choosing different functions second row average number support vectors class classifier 
polynomial rbf sigmoid total svs average svs tables show elements support vector sets different classifiers common 
mentioned support vector expansion unique 
depending way quadratic programming problem solved get different expansions different support vector sets 
quadratic programming algorithm ordering training set cases 
table percentage support vector set column contained support set row class classifiers 
polynomial rbf sigmoid polynomial rbf sigmoid table percentage support vector set column contained support set row classifier digit polynomial rbf sigmoid polynomial rbf sigmoid describing support vector algorithm shown support vector set contains information classifier needs constructing decision function 
due overlap support vector sets different classifiers train classifiers support vector sets classifier 
table shows leads results comparable training database 
table comparison support vector sets time 
class classifiers intersection gives fraction support vector set shared classifiers 
total different support vectors shared classifiers additional common classifiers 
polynomial rbf sigmoid intersection shared union 
supp 
vectors intersection table training classifiers support vector sets classifiers leads performances test set results training full data base shown numbers errors element test set class classifiers separating digit rest 
additionally results training random subset data base size displayed 
trained poly support rbf support sigmoid support full data base random subset classifier size polynomial rbf sigmoid shown ffl support vector algorithm allows construction various learning machines performing similarly ffl able predict degree polynomial classifiers leads high generalization ability ffl considered classification problem postal handwritten digit database constructed learning machines polynomial classifiers neural nets radial basis function classifiers construct decision functions highly overlapping subsets training set 
considering experiments support vector sets constituted training set binary classification point indicates possibility substantial data compression 
holds true various real life problems open interesting question 
boser guyon vapnik 
training algorithm optimal margin classifiers 
fifth annual workshop computational learning theory pittsburgh acm 
bromley sackinger 
neural network nearest neighbor classifiers 
technical report tm cortes vapnik 
support vector networks 
appear machine learning 
le cun boser denker henderson howard hubbard jackel 
handwritten digit recognition backpropagation network 
advances neural information processing systems vol 
morgan kaufman 
vapnik 
nature statistical learning theory 
springer verlag forthcoming 
