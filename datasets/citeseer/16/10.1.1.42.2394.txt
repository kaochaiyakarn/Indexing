statistics summarization step sentence compression kevin knight daniel marcu information sciences institute department computer science university southern california admiralty way suite marina del rey ca isi edu humans produce summaries documents simply extract sentences concatenate 
create new sentences grammatical capture salient pieces information original document 
large collections text pairs available online possible envision algorithms trained mimic process 
focus sentence compression simpler version larger challenge 
aim achieve goals simultaneously compressions grammatical retain important pieces information 
goals conflict 
devise noisy channel decision tree approaches problem evaluate results manual compressions simple baseline 
research automatic summarization focused extraction identifying important clauses sentences paragraphs texts see mani maybury representative collection papers 
determining important textual segments half summarization system needs cases simple catenation textual segments yield coherent outputs 
number researchers started address problem generating coherent summaries mckeown 
barzilay 
jing mckeown context multidocument summarization mani 
context revising single document extracts witbrock mittal context headline generation 
approach proposed witbrock mittal applies probabilistic model trained directly pairs 
model scale generating multiple sentence abstracts formed copyright fl american association artificial intelligence www aaai org 
rights reserved 
grammatical sentences 
approaches employ sets manually written semi automatically derived rules deleting information redundant compressing long sentences shorter ones aggregating sentences repairing links goal generate coherent abstracts 
contrast intend eventually tuples widely available order automatically learn rewrite texts coherent abstracts 
spirit statistical mt community focused sentence sentence translations decided focus simpler problem sentence compression 
chose problem reasons ffl problem complex require development sophisticated compression models determining important sentence determining convey important information grammatically words just scaled version text summarization problem 
problem simple worry discourse related issues coherence anaphors ffl second adequate solution problem immediate impact applications 
example due time space constraints generation tv captions requires important parts sentences shown screen ellis robert 
sentence compression module impact task automatic generation 
sentence compression module provide audio scanning services blind grefenstette 
general systems aimed producing coherent abstracts implement manually written sets sentence compression rules mckeown mani gates bloedorn barzilay mckeown elhadad sentence compression module impact quality systems 
particularly important text genres long sentences 
approaches sentence compression problem 
take input sequence words wn sentence 
algorithm may drop subset words 
words remain order unchanged form compression 
compressions choose reasonable 
approach develops probabilistic noisy channel model sentence compression 
second approach develops deterministic model 
noisy channel model sentence compression section describes probabilistic approach compression problem 
particular adopt noisy channel framework relatively successful number nlp applications including speech recognition jelinek machine translation brown part speech tagging church transliteration knight information retrieval berger lafferty 
framework look long string imagine originally short string added additional optional text 
compression matter identifying original short string 
critical original string real hypothetical 
example statistical machine translation look french string say originally english added noise 
french may may translated english originally removing noise hypothesize english source translate string 
case compression noise consists optional text material pads core signal 
larger case text summarization may useful imagine scenario news editor composes short document hands reporter tells reporter flesh results article read newspaper 
summarizers may access editor original version may may exist guess probabilities come 
noisy channel application solve problems ffl source model 
assign string probability gives chance generated original short string hypothetical process 
example may want low ungrammatical 
ffl channel model 
assign pair strings hs ti probability gives chance short string expanded result long string example extra word may want low 
word optional additional material 
ffl decoder 
observe long string search short string maximizes 
equivalent searching maximizes delta 
advantageous break problem way decouples somewhat independent goals creating short text looks grammatical preserves important information 
easier build channel model focuses exclusively having worry 
specify certain substring may represent unimportant information need worry deleting result ungrammatical structure 
leave source model worries exclusively formedness 
fact extensive prior source language modeling speech recognition machine translation natural language generation 
goes actual compression decoding noisy channel jargon re generic software packages solve problems application domains 
statistical models experiments report build simple source channel models 
departure discussion previous statistical channel models assign probabilities tree expand tree trees strings 
decoding new string parse large tree collins parser hypothesize rank various small trees 
source strings ones normal looking parse tree normal looking word pairs 
tree combination standard probabilistic context free grammar pcfg score computed grammar rules yielded tree standard word bigram score computed leaves tree 
example tree np john vp vb saw np mary assigned score factors tree top top delta np vp delta np john np delta vp vb np vp delta vp saw vb delta np mary np delta john eos delta saw john delta mary saw delta eos mary stochastic channel model performs minimal operations small tree create larger tree internal node probabilistically choose expansion template labels node children 
example processing node tree may wish add prepositional phrase third child 
probability np vp pp np vp 
may choose leave probability np vp np vp 
choose expansion template new child node introduced grow new subtree rooted node example pp np pittsburgh 
particular subtree grown examples parse trees 
probability pcfg factorization bigrams 
example section show tell potential compression statistical models described 
suppose observe tree spans string abcde 
consider compression shown 
compute factors tree expand tree 
breaking source pcfg word bigram factors describe tree top top eos eos channel expansion template factors channel pcfg new tree growth factors describe expand tree different compression scored different set factors 
example consider compression leaves completely untouched 
case source costs tree top top eos eos channel costs expand tree documentation typical quality excellent 
documentation excellent 
design goals achieved delivered performance matches speed underlying device 
design goals achieved 
reach mail product message management system designed initially lans eventually operating system independent 
eventually operating system independent 
modules may physically electrically incompatible cable specific provide industry standard connections 
cable specific provide industry standard connections 
ingres star prices start 
ingres star prices start 
examples parallel corpus 
simply compare expand tree tree delta expand tree tree versus expand tree tree delta expand tree tree select 
note tree pcfg factors canceled appear potential compression 
need compare compressions basis probabilities word bigram probabilities 
quantities differ proposed compressions boxed 
preferred delta delta delta delta delta delta training corpus order train system ziff davis corpus collection newspaper articles announcing computer products 
articles corpus paired human written abstracts 
automatically extracted corpus set sentence pairs 
pair consisted sentence occurred article possibly compressed version occurred human written 
shows sentence pairs extracted corpus 
decided corpus consistent desiderata specific summarization human written sentences grammatical ii sentences represent compressed form salient points original newspaper sentences 
decided keep corpus uncompressed sentences want learn compress sentence 
learning model parameters collect expansion template probabilities parallel corpus 
parse sides parallel corpus identify corresponding syntactic nodes 
example parse tree sentence may np vp pp parse tree compressed version may np vp 
nodes deemed correspond joint event np vp np vp pp normalize 
nodes corresponding partners due incorrect parses due legitimate reformulations scope simple channel model 
standard methods estimate word bigram probabilities 
decoding vast number potential compressions large tree pack efficiently shared forest structure 
node children ffl generate gamma new nodes non empty subset children ffl pack nodes referred 
example consider large tree 
compressions represented forest assign expansion template probability node forest 
example node assign 
observed probability parallel corpus zero assign small floor value gamma reality produce forests consider compressing node ways locally grammatical penn treebank rule type observed appear forest 
point want extract set trees forest account expansion template probabilities word bigram probabilities 
fortunately generic extractor hand langkilde 
extractor designed hybrid symbolic statistical natural language generation system called nitrogen 
application rule component converts semantic representation vast number potential english renderings 
renderings packed forest promising sentences extracted statistical scoring 
purposes extractor selects trees best combination word bigram scores 
returns list trees possible compression length 
example sentence basic level operations products vary obtain best compressions negative log probabilities shown parentheses smaller basic level operations products vary widely level operations products vary widely basic level operations products vary level operations products vary basic level operations products vary operations products vary widely operations products vary widely operations products vary operations products vary operations products vary operations vary operations vary length selection useful multiple answers choose user may seek compression seeks compression 
purposes evaluation want system able select single compression 
rely log probabilities shown choose shortest compression 
note compression scores better word compression models entirely happy removing article 
create fair competition divide log probability length compression rewarding longer strings 
commonly done speech recognition 
plot normalized score compression length usually observe bumpy shaped curve illustrated 
typical difficult case word sentence may optimally compressed word version 
course user requires shorter compression may select region curve look local minimum 
decision model sentence compression section describe decision history model sentence compression 
noisy channel approach assume input advantage broadband distance advantage broadband distance advantage broadband distance advantage distance advantage distance compression particular length adjusted negative log probability best compression length adjusted log probabilities top scoring compressions various lengths lower better 
parse tree goal rewrite smaller tree corresponds compressed version original sentence subsumed suppose observe corpus trees 
model ask may go rewriting 
possible solution decompose rewriting operation sequence shift reduce drop actions specific extended shift reduce parsing paradigm 
model propose rewriting process starts empty stack input list contains sequence words subsumed large tree word input list labeled name syntactic constituents start see 
step rewriting module applies operation aimed reconstructing smaller tree 
context sentence compression module need types operations ffl shift operations transfer word input list stack ffl reduce operations pop syntactic trees located top stack combine new tree push new tree top stack 
reduce operations derive structure syntactic tree short sentence 
ffl drop operations delete input list subsequences words correspond syn shift shift reduce shift reduce drop stack input list stack input list steps steps step step steps step example incremental tree compression 
tactic constituents 
drop operations deletes input list words spanned constituent ffl operations change label trees top stack 
actions assign pos tags words compressed sentence may different pos tags original sentence 
decision model flexible channel model enables derivation trees skeleton differ quite drastically tree input 
example channel model unable obtain tree operations listed enable rewrite tree tree long order traversal leaves produces sequence words occur order words tree example tree obtained tree sequence actions effects shown shift shift reduce drop shift reduce save space show shift operations line reader understand correspond distinct actions 
see operation rewrites pos tag word reduce operations modify skeleton tree input 
increase readability input list shown format resembles closely possible graphical representation trees 
learning parameters decision model associate configuration drop rewriting model learning case 
cases generated automatically program derives sequences actions map large trees corpus smaller trees 
rewriting procedure simulates bottom reconstruction smaller trees 
pairs long short sentences yielded learning cases 
case labeled action name set possible actions distinct actions pos tag 
distinct drop actions type syntactic constituent deleted compression 
distinct reduce actions type reduce operation applied reconstruction compressed sentence 
shift operation 
tree arbitrary configuration stack input list purpose decision classifier learn action choose set possible actions 
learning example associated set features classes operational features reflect number trees stack input list types operations 
encode information denote syntactic category root nodes partial trees built certain time 
examples features original tree specific features denote syntactic constituents start unit input list 
examples features cc pp decision compression module uses program quinlan order learn decision trees specify large syntactic trees compressed shorter trees 
fold crossvalidation evaluation classifier yielded accuracy sigma 
majority baseline classifier chooses action shift accuracy 
employing decision model compress sentences apply shift reduce drop model deterministic fashion 
parse sentence compressed collins initialize input list words sentence syntactic constituents word shown 
incrementally inquire learned classifier action perform simulate execution action 
procedure ends input list empty stack contains tree 
inorder traversal leaves tree produces compressed version sentence input 
model deterministic produces output 
advantage compression fast takes milliseconds sentence 
disadvantage produce range compressions system may subsequently choose 
straightforward extend model probabilistic framework applying example techniques magerman 
evaluation evaluate compression algorithms randomly selected sentence pairs parallel corpus refer test corpus 
sentence pairs training 
shows sentences test corpus compressions produced humans compression algorithms baseline algorithm produces compressions highest word bigram scores 
examples chosen reflect average bad performance cases 
sentence compressed manner humans algorithms baseline algorithm chooses compress sentence 
second example output decision algorithm grammatical semantics negatively affected 
noisy channel algorithm deletes word break affects correctness output 
example noisy channel model conservative decides drop constituents 
constrast decision algorithm compresses input substantially fails produce grammatical output 
original sentence test corpus judges compressions human generated compression outputs noisy channel decision algorithms output baseline algorithm 
judges told outputs generated automatically 
order outputs scrambled randomly test cases 
avoid confounding judges participated experiments 
experiment asked determine scale systems respect selecting important words original sentence 
second experiment asked determine scale grammatical outputs 
investigated sensitive algorithms respect training data carrying experiments sentences different genre scientific 
took sentence articles available archive 
created second parallel corpus refer corpus generating compressed grammatical versions sentences 
sentences corpus extremely long baseline algorithm produce compressed versions reasonable time 
results table show compression rates mean standard deviation results judges algorithm corpus 
results show decision algorithm aggressive average compresses sentences half original size 
compressed sentences produced algorithms grammatical contain important words sentences produced baseline 
test experiments showed differences statistically significant individual judges average scores original basic level operations products vary widely 
baseline basic level operations products vary widely 
noisy channel operations products vary widely 
decision operations products vary widely 
humans operations products vary widely 
original reliable worked accurately testing produces large dxf files 
baseline worked large dxf 
noisy channel reliable worked accurately testing produces large dxf files 
decision reliable worked accurately testing large dxf files 
humans produces large dxf files 
original debugging features including user defined break points variable watching message watching windows added 
baseline debugging user defined variable watching message watching 
noisy channel debugging features including user defined points variable watching message watching windows added 
decision debugging features 
humans debugging features added compression examples corpus avg 
orig 
sent 
length baseline noisy channel decision humans test words compression grammaticality sigma sigma sigma sigma importance sigma sigma sigma sigma words compression grammaticality sigma sigma sigma importance sigma sigma sigma table experimental results judges 
tests showed significant statistical differences algorithms 
table shows performance compression algorithms closer human performance baseline performance humans perform statistically better algorithms 
applied sentences different genre performance noisy channel compression algorithm degrades smoothly performance decision algorithm drops sharply 
due sentences corpus algorithm compressed words 
suspect problem fixed decision compression module extended style magerman computing probabilities sequences decisions correspond compressed sentence 
likewise substantial gains noisy channel modeling see clearly data statistical dependencies processes captured simple initial models 
grammatical output come account subcategory head modifier statistics addition simple word bigrams expanded channel model allow tree manipulation possibilities 
extending algorithms compressing multiple sentences currently underway 
barzilay mckeown elhadad 
information fusion context multi document summarization 
proceedings th annual meeting association computational linguistics acl 
berger lafferty 
information retrieval statistical translation 
proceedings nd conference research development information retrieval sigir 
brown della pietra della pietra mercer 
mathematics statistical machine translation parameter estimation 
computational linguistics 
church 
stochastic parts program noun phrase parser unrestricted text 
proceedings second conference applied natural language processing 
collins 
generative lexicalized models statistical parsing 
proceedings th annual meeting association computational linguistics acl 
grefenstette 
producing intelligent telegraphic text reduction provide audio scanning service blind 
working notes aaai spring symposium intelligent text summarization 
jelinek 
statistical methods speech recognition 
mit press 
jing mckeown 
decomposition human written summary sentences 
proceedings nd conference research development information retrieval sigir 
knight 
machine transliteration 
computational linguistics 
langkilde 
forest statistical sentence generation 
proceedings st annual meeting north american chapter association computational linguistics 
ellis 
closed captioning america looking compliance 
proceedings tao workshop tv closed captions hearing impaired people 
magerman 
statistical decision tree models parsing 
proceedings rd annual meeting association computational linguistics 
mani maybury eds 

advances automatic text summarization 
mit press 
mani gates bloedorn 
improving summaries revising 
proceedings th annual meeting association computational linguistics 
mckeown klavans hatzivassiloglou barzilay eskin 
multidocument summarization reformulation progress prospects 
proceedings sixteenth national conference artificial intelligence aaai 
quinlan 
programs machine learning 
san mateo ca morgan kaufmann publishers 
robert pfeiffer ellison 
semi automatic captioning tv programs australian perspective 
proceedings tao workshop tv closed captions hearing impaired people 
witbrock mittal 
statistical approach generating highly condensed non extractive summaries 
proceedings nd international conference research development information retrieval sigir poster session 
