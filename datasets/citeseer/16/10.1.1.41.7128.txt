learning long term dependencies gradient descent di cult patrice paolo yat bell laboratories 
di sistemi informatica universita recurrent neural networks map input sequences output se quences recognition production prediction problems 
prac tical di culties reported training recurrent neural networks perform tasks temporal contingencies input output sequences span long intervals 
show gradient learning algorithms face increasingly di cult problem duration dependencies captured increases 
results expose trade cient learning gradient descent latch ing information long periods 
understanding problem alternatives standard gradient descent considered 
appear special issue recurrent networks ieee transactions neural networks interested training recurrent neural networks map input sequences output sequences applications sequence recognition production time series prediction 
applications require system store update context infor mation information computed past inputs useful produce desired outputs 
recurrent neural networks suited tasks internal state represent context information 
cycles graph recur rent network allow information past inputs amount time xed priori depends weights input data 
contrast static networks recurrent connection include delays time delay neural networks nite impulse response store bit information inde nite time 
recurrent network inputs xed constitute input sequence transform input sequence output sequence account contextual information exible way 
restrict attention discrete time systems 
learning algorithms recurrent networks usually computing gra cost function respect weights network 
example back propagation time algorithm generalization back propagation static networks stores activations units going forward time 
backward phase backward time recursively uses activations compute required gradients 
algorithms propagation algo rithms computationally expensive fully connected recurrent network local time applied line fashion produc ing partial gradient time step 
algorithm proposed training constrained recurrent networks dynamic neurons single feedback incoming connections input layer 
local time forward propagation algorithms requires computation proportional number weights back propagation time algorithm 
unfortunately networks deal limited storage capabilities dealing general sequences limiting representational power 
task displays long term dependencies computation desired output time depends input earlier time recurrent networks instances outperform static networks appear di cult train optimally 
earlier experiments indicated parameters settle sub optimal solutions take account short term dependencies long term dependencies 
similar results obtained mozer 
back propagation su ciently powerful discover contingencies spanning long temporal intervals 
experimental theoretical results order understanding problem 
comparison evaluation purposes list basic requirements para metric dynamical system learn store relevant state information 
require 
system able store information arbitrary duration 
system resistant noise uctuations inputs random irrelevant predicting correct output 

system parameters trainable reasonable time 
long term storage de nite bits information state variables dynamic system referred information latching 
formalization concept hyperbolic attractors section 
divided sections 
section minimal task solved system satis es conditions 
recurrent network candidate solution negative experimental results indicating gradient appropriate simple problem 
theoretical results section show system stable resistant noise alternatively ciently trainable 
analysis shows trying satisfy conditions magnitude derivative state dynamical system time respect state time decreases exponentially increases 
back propagation algorithm gradient descent general ine cient learning long term dependencies input output sequence failing condition su ciently long sequences 
section analysis previous sections new algorithms approaches proposed compared variants back propagation simulated annealing 
algorithms evaluated simple tasks span input output dependencies controlled 
minimal task illustrating problem minimal task designed test necessarily passed der satisfy conditions enumerated 
parametric system trained classify di erent sets sequences length sequence ut class ut depends rst values external input ut ul suppose xed allow sequences arbitrary length system provide answer sequence 
problem solved dynamic system able store information initial input values arbitrary duration 
simplest form long term computation may ask recurrent network carry 
values ul ut irrelevant determining class sequences 
may ect evolution dynamic system eventually erase internally stored information initial values input 
system latch information robustly away easily deleted events unrelated classi cation criterion 
assume sequence ut zero mean gaussian noise third required condition learnability 
di erent computational aspects involved task 
necessary process ul order extract information class perform classi cation 
second necessary store information subset state variables call latching state variables dynamic system arbitrary duration 
task computation class require accessing latching state variables 
latching state variables need ect evolution state variables 
simple solution task may latching subsystem fed subsystem computes information class 
assessing learning capabilities latching problem independently particular set training sequences way independent speci problem classifying ul 
focus latching tem 
order train module feeding latching subsystem learning algorithm able transmit error information gradient module 
im portant question learning algorithm propagate error information module feeds latching subsystem detects events leading latching 
feeding recurrent network input sequences de ned latching subsystem test system reformulate minimal task follows 
test system input ht output xt discrete time step 
initial inputs ht values algorithm gradient descent ht gaussian noise connection weights test system trainable parameters 
optimization cost function index training sequences target sequence class sequences class 
formulation ht represent result computation extracts class information 
learning ht directly easier task computing ric function ht ut original input sequence learning parameters fact error derivatives backpropagation time ht obtained parametric function ut ht directly trained pa rameters test system vanishing gradient clearly trained parametric function input sequence system uses trainable module feed latching subsystem 
ability learning free input values hl measure ectiveness gradient error information propagated back test system connected output module 
simple recurrent network candidate solution performed experiments minimal task single recurrent neuron shown fig 

types trajectories considered test system classes tanh wf ak hk autonomous dynamic neuron attractors depend value weight easily obtained non zero intersections curve tanh line 
assuming initial state shown exists value input xt maintains sign exists nite number steps ht 
symmetric case occurs increases xed transient length decreases 
recurrent neuron fig 

robustly latch bit information represented sign activation 
storing accomplished keeping large input larger absolute value long time 
small noisy inputs smaller absolute value change sign activation neuron applied arbitrary long time 
robustness essentially depends nonlinearity 
recurrent weight trainable 
solution requires produce stable attractors larger correspond larger critical value consequently robustness noise 
trainable input values bring state neuron order robustly latch bit information input noise 
example accomplished adapting controls transient duration attractors 
fig 
sample sequences feed recurrent neuron 
stated section trainable samples gaussian distribution mean values ht initialized small uniform random values starting training 
set simulations carried evaluate ectiveness back propagation time simple task 
rst experiment investigated ect noise variance di erent initial values self loop weight see 
density plot convergence shown fig 
averaged runs selected pairs 
seen convergence large noise variance small initial values chosen experiments 
fig 
show ect varying keeping xed andw 
case task consists learning input parameters ut explained section learning algorithm unable properly tune inputs ut able learn trigger latching complicated situation 
solving task minimum requirement able transmit error information backwards modules feeding latch unit 
large extremely di cult attain convergence 
experimental results show simple situation want robustly latch bit information input gradient descent output error fails long term input output dependencies initial parameter values 
learning latch dynamical systems section attempt understand better learning simple long term de gradient descent appears di cult 
discuss general case real time recognizer parametric dynamical system 
ditions recurrent network robustly store information way de ned hyperbolic attractors yield problem vanishing gradients learning di cult 
consider non autonomous discrete time system additive inputs ut corresponding autonomous dynamics nonlinear map ut vectors representing respectively system state external input time simplify analysis section consider system additive inputs 
dynamic system non additive inputs ut transformed additive inputs introducing additional state variables corresponding inputs 
suppose ut new system de ned additive inputs dynamics yt isan vector state rst elements ut 
new map de ned terms old map follows yt zeroes elements 
yt ut note system additive inputs map form transformed back equivalent system non additive inputs 
loss generality model eq 

subsection conditions arise hyperbolic attractors latch bits information 
system sensitive noise derivatives cost time respect system activations converge exponentially increases 
situation essential reason di culty gradient descent dynamical system capture long term dependencies input output sequences 
analysis order latch bit state information wants restrict values system activity subset domain 
way possible interpret ways inside outside sure remains region system dynamics chosen region basin attraction attractor attractor sub manifold subspace domain 
erase bit information inputs may push system activity basin attraction possibly 
section show attractor hyperbolic transformed stable periodic attractor derivatives quickly vanish increases 
unfortunately gradients vanish training di cult uence short term dependencies dominates weights gradient 
de nition set points said invariant map 
de nition hyperbolic attractor set points invariant di erentiable map eigenvalues absolute value 
attractor may contain single point xed point attractor nite number points periodic attractor nite number points chaotic attractor 
note stable attracting xed point map stable attracting periodic attractor period map hyperbolic map recurrent net kind attractor depends weight matrix 
particular network de ned tanh ut ifw symmetric minimum eigenvalue greater attractors xed points 
hand jw system linear stable system single xed point attractor origin 
de nition basin attraction attractor set points con map fa km xk de nition call attracting set hyperbolic attractor set points basin attraction eigenvalues 
note de nition hyperbolic attractor 
de nition latched time hyperbolic reduced attracting set map de ning autonomous system dynamics 
case non autonomous dynamics remains robustly latched long inputs ut fort 
see robust store bit information keeping reduced attracting set theorem assume point exist open sphere centered jm 
exist km kx yk 
proof see appendix 
theorem implies hyperbolic attractor ifa size ball uncertainty grow exponentially increases illustrated gure 
small perturbations input push trajectory possibly wrong basin attraction 
means system resistant input noise 
call input noise may simply components inputs relevant predict correct outputs 
contrast results show guaranteed remain certain distance input noise bounded 
de nition map contracting set km kx yk theorem di erentiable mapping convex set jm contracting proof see 
crucial element analysis identify conditions robustly latch information attractor theorem suppose system robustly latched tox starting state inputs ut bt bt autonomous trajectory obtained starting input suppose dt jm dt 
remains inside ball intersects 
proof see appendix 
results justi es term robust de nition robustly latched system long remains reduced attracting set attractor bound inputs guarantees remain certain distance point inx illustrated gure 
smaller jm region looser bound bt inputs meaning system robust input noise 
hand outside contracting expanding size ball uncertainty grows exponentially time 
consequences robust latching vanishing gradient theorem input ut system remains robustly latched attractor time proof see appendix 

results section show storing bit information away resistant noise gradient respect past events rapidly small comparison gradient respect events 
section discuss gradient descent parameter space weights network ine cient 
ect weight gradient consider ect vanishing gradients derivatives cost ct time respect parameters dynamical system say recurrent neural network weights ct ct ct suppose condition network robustly latched 
term ct 
term tends small comparison terms close means exist change allow jump better basin attraction gradient cost respect re ect possibility 
ect small change felt near past close 
see example result hampers training system requires robust latching information 
consider example system sub systems output fed input suppose solution learning problem requires storing information events detected time output distant timet compute error minimal problem de ned section 
trained able store information long time gradients error respect output time small doesn latch outputs time little uence error time hand soon trained reliably store information long time right gradients propagate quickly vanish small values training di cult depending size amount noise 
alternative approaches section helped understand better training recurrent network learn long range input output dependencies hard problem 
gradient methods ap pear inadequate kind problem 
need consider alternative systems optimization methods give acceptable results criterion function smooth long plateaus 
section consider alternative optimization algorithms purpose compare variants back propagation 
way help training recurrent networks set connectivity initial weights constraints weights prior knowledge 
example accomplished prior rules sequentiality constraints 
fact results strongly suggest prior knowledge learning problem di cult 
instances long term input output dependencies unknown learned examples 
simulated annealing global search methods annealing applied problems generally slow 
implemented simulated annealing algorithm optimizing functions continuous variables 
batch learning algorithm updating parameters examples training set seen 
performs cycle random moves coordinate parameter direction 
point accepted rejected metropolis criterion 
new points selected uniform distribution inside hyperrectangle point 
dimensions hyperrectangle updated order maintain average percentage accepted moves half total number moves 
certain number cycles temperature reduced constant multiplicative factor exper iments 
training stops acceptable value cost function attained learning gets stuck maximum number function evaluations performed 
cost value points change small constant values current optimal cost value algorithm 
experiments 
function evaluation corresponds performing single pass network input sequence 
multi grid random search simple algorithm similar simulated annealing algorithm 
simulated annealing tries random points 
main problem learning tasks plateaus local minima algorithm accepts points reduce error cient 
algorithm property 
performs uniform random search current best point 
better point reduces size hyperrectangle factor experiments re centers new point 
stopping criterion simulated annealing 
time weighted pseudo newton optimization pseudo newton algorithm neural networks advantage re scaling learning rate weight dynamically match curvature energy function respect weight 
interest adjusting learning rate potentially circumvent problem vanishing gradient 
pseudo newton algorithm computes diagonal approximation hessian matrix second derivatives cost respect parameters updates parameters line rule wi wi update weight wi pattern isthe wi cost pattern small positive constants 
amounts computing local learning rate parameter inverse second derivative respect parameter normalizing factor 
small curvature small current value direction corresponding wi axis 
larger step taken direction 
algorithm tested experiments described section 
consistently performs better standard back propagation fails increase span input output dependencies 
algorithm theoretical results section inspired time weighted pseudo newton algorithm 
basic idea consider unfolding recurrent network time instantiation weight di erent times separate variable albeit constraint separate variables equal 
simplify problem consider cost depends output network nal time step sequence weight update wi computed follows wi wit instantiation time parameter wi 
way temporal contribution wi inverse curvature respect wit parameter wi time reader may compare equation equation temporal contributions uniformly summed 
consequently updating equation follow gradient equation 
gradient contributions weighted second derivatives order faster moves directions 
pseudo newton algorithm prefer diagonal approximation hessian wit idea second derivatives way inspired discussions bottou 
compute guaranteed positive 
global learning rate experiments 
constant introduced prevent large small 
better performance attained recurrent networks adapted line 
prevents maximum greater certain upper bound experiments smaller certain lower bound experiments 
constant updated momentum term experiments order prevent decreasing rapidly rst second derivatives vary widely sequence sequence small magnitude example norm weight 
discrete error propagation analysis section suggest root problem lies essentially discrete nature process storing information inde nite amount time 
gradient backpropagated time vanishes system stays stable state time steps 
intuitively wewould recover error information time input system reach stable state 
propagating gradient di erentiable units algorithm explicitly designed propagate discrete error information units compute non di erentiable function hard threshold 
hope nd algorithm directly address problem propagating error backwards time process robustly storing information appears discrete nature 
methods explored order train layered networks hard threshold units 
example shown train networks probabilistic approach 
method proposed iterates training steps adjusting network internal representation units activations training parameters produce representation 
algorithm applied recurrent networks 
methods take advantage probabilities order di erentiable error function permitting gradient descent 
approach proposed applies layer networks 
space activities hidden units searched greedy way order reduce output error 
earlier algorithm related propagation targets proposed 
algorithm introduced relies propagating discrete error information obtained nite di erence approach 
neural network represented series local elements forward propagation function error propagation function 
derive functions discrete element show standard di erentiable elements minimize cost function 
building block discrete elements non linear threshold function 
forward propagation yi sign xi yi output unit xi input 
interested nding discrete counterpart gradient propagation unit 
error signal rst establish relation variations output yi variations input xi 
done systematic way 
variation yi xi easily computed equation considering conditions output discrete threshold unit change yi xi xi xi xi xi xi equation compute desired variation xi yi desired variation yi yi xi xi yi xi yi positive constant 
denote yi xi desired changes yi xi respectively 
cost function system certain pattern sequence 
pseudo gradient xi cost experiment set re ect uence change xi yi yi 
pseudo gradient insure yi xi yi de ned values 
achieved stochastic process 
assume exist constants min max error signal gi backpropagated real number satisfying min gi max de ne stochastic function yi gi maps gi follows gi min max min gi max max min provided max easy show expectation gi exactly gi gi take values resulting expected value 
furthermore sum pseudo gradient patterns quickly converges sum continuous valued gi 
non linear threshold unit combination di erentiable elements gradient usual fashion 
important point non linear threshold unit connected loop positive gain stable xed points induced 
pseudo gradient loop doesn vanish time essential reason discrete units 
pseudo gradient doesn vanish loop observed repetitively applying equations noting pseudo gradient large magnitude propagated 
approach optimal discrete error propagation algorithms possible 
promising approach instance trainable discrete ip op unit preserves error information time 
claim discrete propagation error ers interesting solutions vanishing gradient problem recurrent network 
preliminary results toy problems see subsection con rm hypothesis 
experimental results experiments performed evaluate various alternative optimization approaches problems increase temporal span input output dependencies 
course possible rst training shorter sequences helps lot problems short term version problem available 
goal experiments measure algorithms perform possible train sequences equivalent short term dependencies 
experiments formed input noise uniformly distributed varying length input output sequences 
criteria performance gorithms measured average classi cation error training stopping criterion met allowed number function evaluations performed task learned average number function evaluations needed reach stopping criterion 
experiments performed problems latch problem sequence prob lem parity problem 
problems suitable architecture chosen algorithms search resulting parameter space discrete error propagation algorithm hard threshold neurons symmetric sigmoids 
initial parameters networks randomly generated trial uni 
choice inputs noise training sequence randomly generated trial 
initial conditions training set algorithms trial 
trial training set generated sequences length uniformly distributed number maximum sequence length displayed 
tasks involved single input single output time step 
latch problem latch problem described section 
considered adaptive parameters self loop weight initial input value positive sequences positive nal target initial input value negative sequences negative nal target 
network unit 
sequence problem sequence problem classify input sequence sequences rst elements sequence 
varies pattern pattern noise may added input 
network rely particular values saw 
early recognize subsequences belonging classes store information update con icting information arrives output read time done sequence 
initial key subsequences randomly generated uniform distribution 
experiments fully connected recurrent network units bias units received external additive input network free parameters 
parity problem parity problem consists producing parity input sequence produced output number input odd 
target sequence 
length sequence may vary input may noisy 
di cult problem local minima problem appears di cult longer sequences 
local optimization algorithms tend get stuck local minimum initial values parameters 
minimal size network implemented free parameters units inputs connected hidden output units 
requires parameters sequence problem di cult learning problem 
results displayed summarized follows simulated annealing performed problems requires order magnitude training time algorithms 
surprising global search algorithm 
multi grid algorithm faster fails parity problem probably local minima 
interesting note latch problem simulated annealing training time increases sequence length 
best solution sequence lengths error surface longer sequences di cult search simulated annealing 
discrete error propagation algorithm performed reasonably prob lems sequence lengths simulated annealing solve problem 
performs line local search faster simulated annealing 
robust local minima multi grid random search 
pseudo newton back propagation algorithm consistently performs better standard back propagation 
see performance worsen temporal span input output dependencies increasing 
time weighted pseudo newton algorithm appears perform better variants back propagation performance appears worsen increasing sequence length 
powerful ability outperform ing static networks 
theoretical experimental evidence showing gradient descent error criterion may inadequate train tasks involving long term dependencies 
assuming hyperbolic attractors store state information system robust input noise ciently trainable gradient descent long term context required 
note theoretical results hold error criterion mean square error criterion 
simple generalizations obtained follows 
mentioned analysis section periodic attractor transformed subsampling time period attractor 
corresponding xed point stable hyperbolic results hold case 
interesting case situation system doesn remain long near attractor jumps rapidly stable hyperbolic attractor 
arise example continuous dynamics correspond discrete dynamics deterministic nite state automaton 
case results hold norm jacobian map derivatives near attractor 
remains shown similar problems occur chaotic attractors gradients vanish system robust input noise 
interesting note related problems vanishing gradient may occur deep feedforward networks recurrent network unfolded time just deep feedforward network shared weights 
result mean impossible train recurrent network particular task 
says gradient descent increasingly ine temporal span dependencies increases 
furthermore problem help training setting network connectivity initial weights constraints weights prior knowledge 
tasks possible avariety examples input output dependencies including short term dependencies su cient infer similar longer term dependencies 
example latch problem parity problem start training short sequences system rapidly settles correct region parameter space 
better understanding problem driven design alternative algorithms time weighted pseudo newton discrete error propagation algorithms 
rst case consider instantiation weights di erent times dif ferent variables consider curvature cost function variables 
information weight gradient contributions di erent times way larger steps directions cost function 
discrete error propagation algorithm propagates error information mixture discrete continuous elements 
gradient locally quantized stochastic decision rule appears help algorithm locally searching solutions getting local minima 
wehave compared algorithms standard optimization algorithms toy tasks temporal span input output dependencies controlled 
preliminary results obtained encouraging suggest may ways reconcile learning storing 
solutions challenge learning long term dependencies dynamical systems recurrent networks may implications types applications learning systems language related problems long term dependencies essential order correct decisions 
appendix proof theorem hypothesis de nition norm kuk km uk 
taylor expansion small value uk open set ko uk km uk 
ting write km uk ko uk km uk km uk km uk implies triangle ity km kx yk proof theorem denote radius uncertainty ball fa ak tg whichwe sure nd gives trajectory autonomous system 
suppose time certainly true time 
lagrange mean value theorem convexity dt dt km jm yk jm hypothesis 
traction theorem td bt 
bt theorem obtained dt construc tion converges 
proof theorem hypothesis de nitions jm 
ask happens remains near boundary basins lemma suppose ut remains boundary basins attraction attractors exists change yielding state remaining 
limt 
appears hypotheses lemma rarely satis ed reasons 
firstly system evolves discrete time making improbable obtain precisely boundary surface 
second order stay surface say ut satisfy equation ut 
submanifold values ut satisfy equation dimension having null measure 
generalization projection state results obtained far generalized case projection pat state converges attractor map case example subset hidden units recurrent network participate directly dynamics stable attractor 
orthogonal projection matrices zt yt zt pat yt rat pr rp denotes right pseudo inverse aa thatp zt converges attractor dynamics zt mp zt zt yt yt 
specialize previous de nitions lemmas theorems subspace zt 
conclude results infer derivatives respect depend projection subspace ra 
uence changes projection subspace pa ignored computation gradient respect non nitesimal changes yield di erent results jumping di erent basin attraction 
training proceed directions ect parameters uence detecting storing events long term switching stable states taken account 
bartlett downs random weights train multilayer networks hard limiting units ieee transactions neural networks vol 
pp 

becker le cun improving convergence back propagation learning second order methods proceedings connectionist models summer school eds 
touretzky hinton sejnowski morgan kaufmann pp 

bengio frasconi simard problem learning long term dependencies invited ieee international conference neural networks sanfrancisco ieee press 
bengio arti cial neural networks application sequence recogni tion ph thesis mcgill university computer science montreal qc canada 
bengio de mori global optimization neural network hidden markov model hybrid ieee transactions neural networks vol 
pp 

martini minimizing multimodal func tions continuous variables simulated annealing algorithm acm trans actions mathematical software vol 
sept pp 

frasconi gori soda local feedback multilayered networks neural computation pp 

frasconi gori soda uni ed integration explicit rules learning example recurrent networks ieee trans 
knowledge data engineering press 
downs method training multi layer networks characteristics internal representations ieee international conference neural networks sanfrancisco pp 

gori bengio de mori bps learning algorithm capturing dynamic nature speech proc 
ieee int 
joint conf 
neural networks wash dc pp 
ii ii 
giles inserting rules recurrent neural networks neural networks signal processing ii proceedings ieee workshop eds 
kung fallside sorenson kamm ieee press pp 

grossman meir domany learning internal representation neural information processing systems ed 
touretzky pp 

kirkpatrick gelatt vecchi optimization simulated annealing science may pp 
kuhn rst look phonetic discrimination connectionist models recurrent links 
ida working institute defense analysis princeton nj 
lang hinton development time delay neural network architecture speech recognition technical report cmu cs carnegie mellon university 
le cun learning processes asymmetric threshold network disordered systems biological organization eds 
bienenstock fogelman soulie springer verlag les france pp 

marcus waugh nonlinear dynamics stability analog neural networks physica special issue pp 

mozer focused back propagation algorithm temporal pattern recognition complex systems pp 

mozer induction multiscale temporal structure advances neural infor mation processing systems eds 
moody hanson lippman morgan kaufmann pp 

ortega iterative solution non linear equations sev eral variables systems equations academic press new york 
rohwer moving targets training algorithm advances neural informa tion processing systems ed 
touretzky morgan kaufmann pp 

rumelhart hinton williams learning internal representation error propagation parallel distributed processing volume 
rumelhart mcclelland 
eds bradford books mit press pp 

williams zipser learning algorithm continuously running fully recurrent neural networks neural computation pp 

xt tanh ht latching recurrent neuron 
sample input recurrent neuron 
trainable values hl marked bold tuned successfully learning simulations 
wo freq 
experimental results minimal problem 
density training convergence respect initial weight noise variance white high density 
frequency training convergence respect sequence length noise variance initial weight 
domain domain basin attraction reduced attracting set attractor ball uncertainty grows exponentially outside bounded inside 
final classification error latch problem sequence presentations latch problem final classification error sequence problem final classification error parity problem sequence presentations sequence problem sequence presentations parity problem comparative simulation results standard back propagation pseudo newton time weighted pseudo newton discrete error propagation multi grid random search simulated annealing 
horizontal axis represents maximum sequence length 
left vertical axis represents classi cation error training right number sequence presentations reach stopping criterion 

