proceedings supercomputing application level scheduling distributed heterogeneous networks technical fran berman rich wolski silvia jennifer schopf gary shao department computer science engineering university california san diego la jolla calif heterogeneous networks increasingly platforms resource intensive distributed parallel applications 
critical contributor performance applications scheduling constituent application tasks network 
distributed resources brought control single global scheduler application scheduled user 
obtain best performance user take account application specific dynamic system information developing schedule meets performance criteria 
define set principles underlying application level scheduling describe progress building apples application level scheduling agents 
illustrate application level scheduling approach detailed description results distributed jacobi application production heterogeneous platforms 
fast networks possible coordinate distributed cpu memory storage resources provide potential application performance superior achievable single system 
parallel applications targeted systems typically resource intensive require resources available single site 
critical resources may include large aggregated distributed memory fixed data sources authors supported part nsf asc asc scholarship capes brazil 
presenting author 
email addresses authors rich silvia cs ucsd edu 
fran berman phone fax 
local temporary storage computational cycles 
performance defined user may mean different things different applications achieving requires efficient relevant resources 
despite performance potential distributed systems offer resource intensive parallel applications achieving user performance goals difficult 
fundamental problems solved realize performance determination efficient schedule 
effective scheduling application developer user involves integration application specific system specific information dependent dynamic interactions application relevant system 
currently performance seeking user develop schedules distributed heterogeneous applications line intuition predict application perform time execute 
users application developers select configuration resources load availability evaluate potential performance application configurations performance criteria interact relevant resource management systems order implement application 
time users running applications draw set resources seeking achieve performance goals 
multiple users contend resources fraction resource performance delivered 
describe application specific approach scheduling individual parallel applications production heterogeneous systems 
developing software facilitate improve scheduling activities user 
goal develop scheduling agents perform task user machine speeds comprehensive information 
term agents apples applicationlevel schedulers 
application apples determine schedule implement schedule respect appropriate resource management systems 
note apples resource management system interacts systems globus legion pvm perform function 
apples application management system manages scheduling application benefit user 
subsection describe approach apples 
scheduling perspective application application level scheduling underlying principles ffl application system specific information needed schedules 
users determine schedules applications perception system capabilities knowledge structure requirements application 
frequency communication computation amount memory required number type size application data structures matched granularity computational platforms network speed bandwidth system attributes develop performance efficient schedule 
ffl dynamic information necessary determine system state 
users base candidate schedules knowledge machines available heavily lightly loaded 
load varies time usage system resources 
choice networks computational platforms available user combine knowledge application system current predicted load resources 
ffl schedules involve prediction application system performance 
prediction provides basis scheduling 
user predicts application execute system uses prediction choose schedule 
predictions difficult accurately system varies time due contention application performance may dependent data system load 
simplifying model system application excessively prediction task easier fruitful 
optima simplified model may correlate optimal behavior practice 
particular application system models sufficiently complex expose real phenomena 
ffl resources evaluated strictly terms performance deliver application 
notice perspective application user resource judged ultimately benefits application execution 
users define different criteria performance speed cost decision resources perform terms specific criteria executing user application 
apples approach parameterizable application system specific models predict application performance set resources 
models conjunction forecasts expected resource load apples agent select resource set application schedule evaluating various candidate mappings 
mapping generates best expected performance chosen implemented target resource management system 
note fundamental difference apples approach system oriented schedulers apples system experienced point view application 
candidate resources application lightly loaded system appears lightly loaded application regardless load resources 
candidate resources heavily loaded system appears heavily loaded 
section utilize application level scheduling approach develop efficient schedule distributed jacobi data parallel code 
example serves proof concept principles underlying apples approach serves illuminate components required general application oriented scheduling agents 
discussing jacobi example detail describe current efforts build general apples agents scheduling section 
point jacobi computation application level scheduling jacobi section illustrate motivate approach simple application 
discuss development application level schedule distributed jacobi application detail performance data 
consider problem executing distributed data parallel dimensional jacobi iterative solver jacobi heterogeneous network machines 
jacobi method commonly solve finite difference approximation poisson equation arises heat flow electrostatic gravitational problems 
variable coefficients represented elements dimensional grid 
iteration new value grid element defined average nearest neighbors previous iteration see 
typically jacobi computation parallelized partitioning grid rectangular regions assigning region different processor 
decomposition strategy favorable processor need obtain border elements region iteration 
amount computational scales area region amount delay due communication scales perimeter 
small number big regions yield processor efficiencies may sacrifice parallelism 
conversely large number small regions may incur large communication overhead 
example user wishes identify partitioning yields lowest possible execution time 
solving partitioning problem optimally np complete necessary user employ heuristics arrive solution 
deriving partitions optimize resource performance version jacobi example written data parallel spmd style 
system provides high level abstractions form objects support runtime data decomposition 
addition details associated message passing distributed memory computing environments buried strip data partitioning processors processor twice fast processor tions making code portable easy maintain 
ideal partitioning assign regions jacobi grid processors area region matches performance capability processor assigned 
faster processors compute larger regions slower ones 
particular computational time optimized ratio rectangular area grid total grid area closely matches ratio power processor rectangular area allocated total processing power available 
simply processor computational time defines performance capability jacobi 
performance capability processor depends fast processor locally compute element jacobi matrix quickly processor communicate border elements neighboring processors 
factors dramatically affect execution time application 
derive partitions balance resource performance formulate partitioning problem analytical model 
time processor compute region area region time required processor compute single point locally time processor send receive borders regions processors 
time processor spends computing communicating single iteration jacobi represented equation predicts execution time including time spent communicating processor 
partitions scheduled simultaneously execution time single iteration equal maximum value balance time processor spends computing communicating setting equal solving resulting system equations grid rows columns restrict legal partitions consider single dimension strip partitions shown depend type partitioning system equations linear solved quickly conventional methods 
strip partitioning recv gamma recv send gamma send gamma recv send recv gamma send gamma recv time receive elements processor processor send time send elements processor processor number elements dimension partitioned solve linear system equations simple gaussian elimination note guarantee corresponds integral number columns rows 
complete strip decomposition round partitions accordingly 
observe alternative computationally complex solution formulate problem constraint minimization problem 
linear programming techniques derive partitions 
approach viable interest rapid prototyping chose adopt simpler linear systems formulation 
predicting system state network weather service solve linear system equations require parameters time required send receive elements processor neighbors send recv time required compute single element processor 
model send receive times send sizeof element bandwidth recv sizeof element bandwidth bandwidth data rate supported link note sizeof element time invariant parameters problem solved 
similarly model point compute time processor cpu time compute single point unloaded processor cpu percentage time processor spends executing partition quantities vary time due resource contention 
bandwidth defined part volume frequency traffic crossing link cpu depend number additional processes executing processor way cpu managed 
typically system time shared percentage time cpu devoted job fair share total cpu time share change jobs enter leave system 
estimates send recv accurate time application scheduled necessarily time partition derived 
scheduler requires forecast values send recv time frame application execute 
developed separate facility called network weather service dynamically supplies values forecasts cpu bandwidth networked system 
network weather service outlined section 
jacobi network weather service dynamic probes load history help forecast cpu bandwidth time application scheduled 
resource selection scheduling resource selection focuses identification subset resources efficiently support application 
users naturally focus resources perceive close 
jacobi application formally define logical distance resources prioritize resource set metric 
note distance resources meaningful application terms resources 
recall grid region size computation partition scales communication scales 
relationship define distance processors jacobi 
forecast time required processor compute single point locally ce forecast time processor send receive single element processor jp gamma ce ce defines distance measure processors arbitrary problem size processors near jacobi compute capabilities relatively equal interprocess communication fast 
select resources global resource pool start identifying candidate machine serve locus 
example user machine fastest machine cluster may serve locus 
rest machines sorted distance locus 
note different orderings may determined distinct loci 
elements sorted list particular locus defined closest resource set containing machines 
jacobi workstation fastest cpu locus 
algorithm determine candidate resource set 
head locus tail locus find machine tail minimum list add tail list tail prioritizing resources distance 
locus machine having maximum criterion value list sort remaining machines logical distance locus elements list parameterize weather service forecasts solve linear system equations parameterization reject partitioning infeasible exists fit free memory processor reject partitioning infeasible record expected execution time subset implement partitioning corresponding minimum execution time computed resource selection scheduling algorithm jacobi 
algorithm iteratively finds machine closest current tail adds machine tail list 
machines added algorithm terminates machine logically closest adjacent list 
form sorting useful strip decomposition jacobi processors communicate neighbors 
having derived resource list jacobi scheduler proceeds compare different potential partitionings subsets total list 
starts estimating execution time locus machine 
considers processor partition processors list 
parameterizes linear system equations processors consults network weather service performance forecasts pertain machines 
solving linear system records estimated execution time resulting partition 
processor partitioning processors sorted resource list considered 
estimated execution time processor system recorded algorithm continues processors list considered predefined maximum logical distance locus reached 
processor set partitioning schedule yielding minimum estimated execution time chosen best schedule locus 
note resource selector considers single site implementation 
example single site implementation simply sequential version implementation 
optimized implementation particular system available resource selector consider 
time partition generated process checked feasibility 
filters employed remove infeasible partitions ultimately considered scheduling 
filter removes partitions negative values correspond mappings communication time great processor compute negative number elements implying negative execution time order finish processors 
second filter checks sure size partition fits free memory forecast network weather service available machine assigned 
resource selection scheduling method example jacobi scheduler summarized pseudocode 
scheduling jacobi apples principles scheduling approach described jacobi uses principles outlined section fact example apples 
application specific system specific information scheduler generate schedules select resources 
dynamic system information provided network weather service parameterize performance models 
predictive models evaluate rank candidate schedules 
important resources considered strictly terms affect application performance 
application level approach scheduling natural question performance efficient schedule generates 
describe experiments address question section 
performance results jacobi applicationlevel schedule determine effectiveness application level scheduling approach important answer questions ffl execution time jacobi apples schedule compare schedule determined widely accepted conventional method 
ffl effect dynamically forecast resource performance data application level scheduling approach 
ffl effect automatic resource selection application level scheduling approach 
address questions compared partitioning methods implementation jacobi 
method compile time blocked uses conventional hpf style block partitioning processor assigned compile time relatively equal sized square region grid compute 
partitioning methods utilize versions application level scheduling approach described previous section 
partitioning method compile time apples uses static estimates resource performance uses resource selection select resource set total resources 
partitioning method runtime apples select uses dynamic estimates network weather service resource performance assumes user wants available resources 
partitioning method runtime apples uses dynamic estimates resource selection constitutes full application level scheduling approach discussed section 
note partitioning methods utilize network weather service data performed run time partitioning methods static data may performed compile time 
versions partition distribute grid execute jacobi solver 
data computations scheduled processors execution begins remain duration execution 
currently formulating version jacobi application level scheduler effectively redistributes grid response changing load system resources 
flexibility supported apples software described section 
execution performance investigate relative execution performance partitioning methods non dedicated workstations located san diego supercomputer center sdsc san diego parallel computation laboratory ucsd pcl 
workstation set consisted sun sparc sun sparc ibm rs workstations located ucsd dec alpha workstations located sdsc 
numeric format conversions handled uses mpi underlying communication substrate 
network connecting systems heterogeneous non dedicated 
pcl attached ethernet segment shared systems 
rs connected different segment shared ambient machines gateway linked segments 
sdsc alpha workstations connected non dedicated fddi ring 
configuration shown 
sparc sparc rs rs ucsd sdsc workstations networks ucsd sdsc 
systems networks shared production mode ran experiments 
conditions change execution due contention runs problem size reported average execution time single iteration 
experiment ran instance partitioning methods back back hoping executions enjoy similar conditions average 
shows average iteration execution times seconds range problem sizes 
case square grid having problem size dimension shown 
experiments application level scheduling able outperform block partitioning uses performance model predict resource perform executing piece jacobi 
uses prediction determine grid assigned machine 
notice benefit gained dynamic performance forecasts substantial 
obvious improvement gained resource selection 
version resource selection run faster non selecting runtime apples relative improvement compared blocked implementation large 
range feasible partitions non selecting runtime apples limited 
example conditions experiments conducted possible balance execution time element problem communication delay ucsd sdsc great processors need compute negative amount time compensate 
problem size comparison execution times compile time blocked compile time apples runtime apples selection runtime apples selection execution times jacobi 
show execution time data wider range problem sizes compiletime blocked full apples partitioner 
resource selection apples able compute reliably depending contention conditions problem size domain 
show predicted execution time apples computed run 
problem size plot time performance model predicted actual execution time resulted mapping 
accuracy performance model allows apples choose resource mappings 
problem size comparison execution times compile time blocked measured runtime apples selection predicted runtime apples selection execution times jacobi 
note large spike execution time blocked partitioning problem size abscissa 
experimental run size network gateway ucsd sdsc went forcing communications alternative slower route 
apples agent network weather service readings able detect sudden drop available bandwidth avoid partitionings spanned affected link 
partitioning memory availability distributed parallel execution allows application aggregate memory resources problems larger fit single memory may solved 
motivation parallel implementation codes stems need collections memory systems desire concurrent execution 
investigate ability apples approach effectively aggregate memory added resource pool ibm sp processors megabytes real memory 
sp uses virtual memory nodes megabytes memory may 
memory paged disk causing times increase dramatically real memory system exceeded 
experiments dedicated access sp processors link connected rest resources shared ethernet segment 
shows resource pool including sp nodes 
sparc sparc rs rs ucsd sdsc sp sp resource pool including sp processors 
processors completely unloaded connectivity resources suffered contention best partitioning yielding shortest execution time split grid evenly sp nodes long partition exceeded available real memory node 
problem size caused partitions spill available real memory resulting delays due paging caused execution time increase substantially 
show execution time blocked partitioning sp processors versus apples approach jacobi 
problem sizes apples correctly chose mapping sp processors exhibited nearly identical execution times blocked mapping 
problem size increased sp began paging causing execution time increase problem size comparison execution times compile time blocked sp runtime apples selection partitioning memory usage 
point processors longer feasible 
apples agent able locate memory resource pool effectively 
problem size apples able find memory effectively dramatic change performance trajectory 
far shown apples approach effectively determine performance efficient non obvious schedule jacobi 
important walk example detail demonstrate approach 
discuss apples approach basis design general software agents facilitate application level scheduling distributed parallel applications 
developing general apples agents clear previous sections application level scheduling effectively achieve performance distributed applications 
develop general apples agents convince questions answered affirmative ffl application level approach selecting performance efficient schedule generalizable ffl possible efficiently obtain appropriate level application system information user analysis schedules may derived 
address question observe development application level schedule jacobi approach rely particularly choice algorithm implementation language programming style success 
organization apples software mimics diligent user schedule application 
characteristics application relevant pertain modeling performance 
apples modularize application specific system specific dynamic information information parameterize general approach 
address second question developed set data sources provide relevant application system specific information efficiently 
network weather service designed provide dynamic system information short term forecasts 
applicationspecific information provided heterogeneous application template hat information application relevant performance estimation 
additional information reflects user preferences access resources provided user specifications 
note apples practice complete application information available scheduler better schedule 
apples currently progress 
software designed underlying building blocks currently prototyped 
working researchers legion project globus project prototype apples application level scheduler resource management systems 
addition progressing implementation uses mpi underlying substrate 
note apples essentially develops customized scheduler application 
differs approach taken scheduling literature 
application level scheduling related brewer directly mars project 
brewer attempts select correct implementation algorithm machine small set static parameters uses applicationspecific information improve performance 
mars project goal produce general purpose software similar scope intent apples 
important difference apples includes user specific application specific information scheduling decisions 
user specific information provides powerful defined interface allows user influence control scheduling agent behave 
sections describe architecture general apples agents 
apples organization apples organized terms subsystems single active agent called coordinator 
subsystems ffl resource selector chooses filters different resource combinations application execution ffl planner generates description system independent schedule resource combination ffl performance estimator generates performance estimate candidate schedules user performance metric ffl actuator implements best schedule target resource management system 
depicts coordinator subsystems 
application specific dynamic information subsystems constitute information pool subsystems share 
general sources information feeding information pool 
network weather service provides dynamic information system state forecasts system state time frame application scheduled 
heterogeneous application template web oriented interface user provides specific information structure characteristics current implementations application tasks 
user specifications provide information user criteria performance preferences implementation additional application information model pool provides model templates apples subsystems application performance estimation 
apples agents employed follows initially user provides information agent hat user specifications 
agent uses resource selector select set viable resource configurations accessibility user access rights characteristics application input filters exclude resources viable notion distance derived hat information model pool provided default coordinator 
viable resource configuration planner conjunction performance estimator network weather service actuator resource selector planner performance estimator heterogeneous application template user specifications information pool resource management system network weather service coordinator models relationship components apples 
computes potential schedule resources predictive models model pool 
coordinator considers performance candidate schedules selects best schedule implementation 
actuator interacts resource management system implement schedule 
subsections describe components apples agents detail 
coordinator coordinator embodies active thread threads control apples agent 
executes blueprint dictates way uses various subsystems derive schedule initiate application monitor progress 
blueprint specified user system particular application class applications data parallel applications 
show sample blueprint 
typical user scheduling minimum execution time application large set possible resources coordinator select set resource sets number pl planner estimator pl set mins actuate mins resource selector planner performance est 
actuator subsystems coordinator blueprint fact blueprint schedule jacobi 
data sources coordinator directs interactions subsystems blueprint subsystem draws variety data sources perform function 
data sources contribute information data pool available apples functions see 
network weather service nws heterogeneous application template hat user specifications model pool 
section briefly describe form content 
network weather service network weather service provides software monitoring predicting load weather networked resources 
strategy sensors dynamically probe read network weather conditions cpu load available free memory network performance provide forecasts system state network weather service uses number stochastic techniques predicting network load 
experiments different network links predictors show general resource different estimation techniques yield best forecasts different times 
consequently network weather service tracks error predictors sampled data uses predictor lowest cumulative error predictions system state 
prediction measure accuracy resource selector planner performance estimator subsystems apples agent 
prototyped facility results shown jacobi example 
currently integrating network weather service facilities legion globus resource management systems 
heterogeneous application template heterogeneous application template hat provides basic information application tasks implementations terms resource requirements 
information provided web interface explicit structural parameters application information existing implementations application tasks data movement requirements distinct tasks 
figures give sample hat parameters 
hat lets user identify active set set task machine implementations compose entire application 
may multiple implementations active set identifies particular task machine allocations single full implementation application 
jacobi active set composed single task implementation machine 
general may implementations choose multiple active sets 
notice user may information requested hat 
system partial information determine schedule 
case user better comprehensive information available performance efficient schedule 
user specifications hat describes application specific information information specific particular user application developer available apples user specifications html 
important role definition user specified requirements fall broad categories execution constraints performance objectives user preferences 
execution constraints refer access rights resource constraints user 
user performance objective conveyed 
jacobi minimum execution time desired objective 
allows user specify preferences coordinator attempt satisfy 
may resource preferred non performance related reasons 
feature gives user tremendous control actions apples solutions generates 
models model pool provides set model templates application performance estimation planner performance estimator resource selector 
model templates structures composing models characteristics contribute application performance 
example jacobi model template execution time processor computation communication computation instantiated communication instantiated described section 
model templates may provided user 
default model templates classes applications data parallel regular grid applications available model pool 
note model templates leverage successful models literature predict performance application tasks 
resource selector resource selector produces viable active sets considered coordinator 
may iterate multiple times identify set candidate active sets different selection criteria 
potentially viable active set may filtered ensure feasibility 
resources prioritized respect application specific valuation function distance filters applied resource set eliminate resources unusable 
filter may information user access rights memory constraints implementation availability eliminate resources quickly 
viable feasible resource configurations scheduled planner evaluated performance estimator compared coordinator candidate schedules 
jacobi example filters considered characteristics potential schedule area region available memory 
partitions strips negative filtered 
resources meet memory requirements application tasks filtered 
constraints users readily identifiable profitably reduce resource selection space 
planner function planner create schedule feasible active set 
schedule scheduling policy optimizes user performance measure 
practice users employ common performance measures execution time cost speedup planner equipped default scheduling policies measures user chooses recommend policy 
schedule generated planner format actuator described section implement target resource system 
jacobi example planner implemented time balancing scheduling policy 
took list candidate machines communication links feasible resource set produced mapping grid strips machines 
coordinator performance estimator determine execution time mapping generated planner passed best schedule actuator 
performance estimator performance estimator parameterizes model template component models produce estimate application performance schedule provided planner 
parameters component models provided user derived data sources network weather service 
dynamic information included resulting estimates targeted time frame application run actuator 
jacobi formula evaluated obtain estimate time necessary compute strip 
note important estimate behavior application tasks context production systems 
reason developing models forecast slowdown tasks shared resources networks machines 
factoring slowdown model provide realistic estimate application task performance presence contention 
actuator apples function resource manager relies services existing resource managers perform resource allocation task instantiation 
job actuator implement schedule determined planner semantics facilities supported target resource management system 
resource managers pvm limited scope provide little additional functionality 
legion potential communicating considerable information resource application status 
actuator convey feedback information available various subsystems 
acts conduit coordinator underlying resource management facilities 
minimum functionality required actuator ability initiate network connected task remote machine 
accurate scheduling accomplished resource management system returns feedback resources available provide guaranteed service times response requests service 
apples agent working application level actuator minimally access facilities application enjoys 
facilities communicate application manage task execution application uses control tasks 
sense actuator extension apples agent constitute integrated extension program scheduled 
apples application part execution instance 
jacobi example actuator issued directives control grid partitioning 
primitives application manage grid 
summary network speeds increase parallel distributed computing prevalent resource intensive applications increasingly need leverage shared heterogeneous networks resources 
effective coordination application components resources key performance 
described application level scheduling way achieving performance efficient schedules applications execute heterogeneous networks machines 
described principles reflect way applications scheduled users illustrated principles developing proof concept application level scheduler distributed data parallel jacobi application 
described general architecture application level schedulers described subsystems compose apples agent 
results generated prototype clear apples approach achieve substantial performance improvements individual application conventional scheduling methods 
application level scheduling allows user deal heterogeneous system really control multiple system schedulers shared contending applications able deliver dynamically varying fraction resource performance 
characteristics explicitly factored scheduling activity application better leverage system achieve performance 
acknowledgments grateful researchers ucsd parallel computation laboratory particular stephen fink substantive discussions 
grateful andrew grimshaw carl kesselman reagan moore john doug shea darren atkinson thoughtful comments support 
apples home page www cse ucsd edu users berman apples html berman moore heterogeneous working group report proceedings second pasadena workshop system software tools high performance computing environments 
gsfc nasa gov pas index html 
brewer high level optimization automated statistical modeling proceedings principles practice parallel programming ppopp pp 

defanti foster papka stevens overview way wide area visual supercomputing appear international journal supercomputer applications 
berman modeling effects contention performance heterogeneous applications appear proceedings high performance distributed computing conference 
fink www cse ucsd edu groups scg html 
fink baden kohn flexible communication mechanisms dynamic structured applications preparation 
freund proceedings ipps workshop heterogeneous computing 
mars framework minimizing job execution time metacomputing environment proceedings general computer systems 
geist beguelin dongarra jiang manchek sunderam pvm parallel virtual machine users guide tutorial networked parallel computing mit press 
hey performance analysis distributed applications suitability functions proceedings conference 
globus www mcs anl gov globus 
grimshaw wulf french weaver reynolds legion logical step nationwide virtual computer tech 
rep cs university virginia 
hensgen moore kidd freund keith lima campbell adding rescheduling integrating condor smartnet proceedings heterogeneous workshop 
high performance fortran forum high performance fortran language specification rice univeristy houston texas may 
hoffman numerical methods engineers scientists mcgraw hill 
brown virtual environments distributed computing sc gii testbed hpc challenge applications way 
proceedings supercomputing 
legion www cs virginia edu mentat legion legion html 
messina personal communication 
pruyne livny parallel processing dynamic resources proceedings workshop job scheduling strategies parallel processing ipps april 
pvm www ornl gov pvm 
rudolph feitelson proceedings ipps workshop job scheduling strategies parallel processing 
sarkar automatic partitioning program dependence graph parallel tasks ibm journal research development sept nov 
siegel antonio metzger tan li heterogeneous computing 
tech 
rep purdue university ee technical report tr ee 
zhang yan framework performance prediction parallel computing heterogeneous proceedings international conference parallel processing pp 

user account information user specification information hat heterogeneous application template user hat structure template input amount data needed start application current source give full machine name sdsc edu mbytes output amount data returned application current source give full machine name sdsc edu mbytes iteration phase create new iteration phase listing implementations listing active sets structure implementation interface apples manager help structure module hat gives information general functional decomposition application lets user identify active set application 
sequential vector task parallel data parallel hat implementation template task platform paradigm usage dedicated non dedicated data structures number size bytes communication data structure words data structure mflops ratio communication data structure words data structure mflops select approximation fill numerical values communication heavy balanced computation heavy communication patterns pt pt stencil multicast broadcast memory core memory needed core sol structure implementation interface apples manager help single processor multi processor tuning factor bubblesort cs st year grad phd thesis hand tuned implementation module focuses task implemented specific platform 
hat interface template implementation implementation network ethernet atm communication message mbytes total mbytes data conversion performed conversion type format conversion structure conversion pipeline pipelined data strict data structure implementation interface apples manager help communication frequency application mbytes dependent 
iterations size pipeline mbytes interface module hat characterizes communication implementations mapped distinct execution sites 

