packet routing dynamically changing networks reinforcement learning approach justin boyan school computer science carnegie mellon university pittsburgh pa michael littman cognitive science research group bellcore morristown nj describes routing algorithm packet routing reinforcement learning module embedded node switching network 
local communication node keep accurate statistics routing decisions lead minimal delivery times 
simple experiments involving node irregularly connected network routing proves superior nonadaptive algorithm precomputed shortest paths able route efficiently critical aspects simulation network load allowed vary dynamically 
concludes discussion tradeoff discovering shortcuts maintaining stable policies 
field reinforcement learning grown dramatically past years exception backgammon successful applications large scale practical tasks 
demonstrates practical task routing packets communication network natural application reinforcement learning algorithms 
brown university department computer science routing algorithm related certain distributed packet routing algorithms learns routing policy balances minimizing number hops packet take possibility congestion popular routes 
experimenting different routing policies gathering statistics decisions minimize total delivery time 
learning continual online uses local information robust face irregular dynamically changing network connection patterns load 
experiments carried discrete event simulator model transmission packets local area network described detail 
routing reinforcement learning task packet routing policy answers question adjacent node current node send packet get quickly possible eventual destination policy performance measured total time taken deliver packet training signal directly evaluating improving policy packet reaches destination 
reinforcement learning policy updated quickly local information 
time node estimates takes deliver packet bound node way neighbor node including time spend node queue 
sending immediately gets back estimate time remaining trip min neighbors packet spent units time queue units time transmission nodes revise estimate follows deltaq new estimate gamma old estimate learning rate parameter usually experiments 
resulting algorithm characterized version bellman ford shortest paths algorithm performs path relaxation steps asynchronously online measures path length merely number hops total delivery time 
call algorithm routing represent function large table 
tried approximating neural network allowed learner incorporate diverse parameters system including local queue size time day distance estimates 
results experiments inconclusive 
denote function corresponds function reinforcement learning technique learning 
irregular theta grid topology results tested routing algorithm variety network topologies including hypercube node telephone network irregular theta grid 
varying network load measured average delivery time packets system learning settled routing policy compared delivery times static routing scheme shortest paths 
result cases routing able sustain higher level network load shortest paths 
section presents detailed results irregular grid network pictured 
conditions low load network learns fairly quickly route packets shortest paths destinations 
performance vs time curve plotted left part demonstrates routing algorithm initial period inefficiency learns network topology performs shortest path router optimal low load 
network load increases shortest path routing scheme ceases optimal ignores rising levels congestion soon floods network packets 
right part plots performance vs time routing schemes high load conditions shortest path unable tolerate packet load routing learns efficient routing policy 
reason learning algorithm success apparent policy summary diagrams 
diagrams indicate node policy theta point point routes go node 
left part summarizes shortest path routing policy nodes center network labeled shortest paths congested network load high 
contrast diagram right shows routing conditions high load learned policy routes simulator time routing shortest paths simulator time routing shortest paths performance low load high load policy summaries shortest path routing high load traffic longer necessary path top network avoid congestion center network 
basic result captured compares performances shortest path policy routing learned policy various levels network load 
point represents median trials mean packet delivery time learning settled 
load low routing algorithm routes nearly efficiently shortest path policy 
load increases shortest path policy leads exploding levels network congestion learning algorithm continues route efficiently 
significant increase load routing algorithm congestion 
network load level routing shortest paths delivery time various loads routing shortest paths dynamically changing networks advantage learning algorithm static routing policy potential adapting changes crucial system parameters network operation 
tested routing algorithm unmodified networks topology traffic patterns load level changing dynamically topology manually disconnected links network simulation 
qualitatively routing reacted quickly changes able continue routing traffic efficiently 
traffic patterns caused simulation oscillate periodically different request patterns irregular grid traffic directed upper lower halves network traffic directed left right halves 
brief period inefficient routing time request pattern switched routing algorithm adapted successfully 
load level level network traffic raised simulation routing quickly adapted policy route packets new bottlenecks 
network traffic levels lowered adaptation slower converged optimal shortest paths 
effect discussed section 
exploration similarity routing update equation bellman ford recurrence shortest paths surprising difference whatsoever performance routing shortest paths routing low load visible 
close look algorithm reveals routing fine tune policy discover shortcuts best neighbor estimate updated 
instance node learns overestimate delivery time optimal route select suboptimal route long route delivery time erroneous estimate optimal route delivery time 
drawback greedy learning widely recognized reinforcement learning community exploration techniques suggested overcome 
common algorithm select actions amount randomness initial learning period 
approach serious drawbacks context distributed routing network continuously changing initial period exploration ends significantly random traffic extremely negative effect congestion 
packets sent suboptimal direction tend add queue delays slowing packets passing queues adds queue delays nodes policy decisions local information increased congestion changes problem learners trying solve 
sending actual packets random direction node full echo modification routing sends requests information immediate neighbors time needs decision 
neighbor returns single number separate channel contribute network congestion model giving node current estimate total time destination 
estimates adjust values neighbor shortcuts appear inefficiencies policy information propagates quickly network policy adjusts accordingly 
compares performance routing shortest paths routing full echo routing 
low loads performance full echo routing indistinguishable shortest path policy inefficiencies purged 
high load conditions full echo routing outperforms shortest paths basic routing algorithm better 
analysis indicates full echo routing constantly changes policy high load oscillating upper bottleneck central bottleneck majority traffic 
behavior unstable generally leads worse routing times high load 
network load level routing shortest paths full echo delivery time various loads routing shortest paths full echo routing ironically drawback basic routing algorithm exploration fine tuning initially learning viable policy leads improved performance high load conditions 
know single algorithm performs best load conditions 
considers straightforward application learning packet routing 
routing algorithm having know advance network topology traffic patterns need centralized routing control system able discover efficient routing policies dynamically changing network 
simulations described fully realistic standpoint actual telecommunication networks believe shown adaptive routing natural domain reinforcement learning 
algorithms routing specifically tailored packet routing domain perform better 
interesting directions replace table representation routing policy function approximator 
allow algorithm integrate system variables routing decision generalize network destinations 
potentially routing information need stored node extending scale algorithm useful 
plan explore issues context packet routing related applications auto traffic control elevator control 
authors support bellcore cognitive science research group national defense science engineering graduate fellowship program national science foundation iri 
bellman 
routing problem 
quarterly applied mathematics 
boyan 
modular neural networks learning context dependent game strategies 
master thesis computer speech language processing cambridge university 
ford jr flows networks 
princeton university press 

lin 
reinforcement learning robots neural networks 
phd thesis school computer science carnegie mellon university 
littman boyan 
distributed reinforcement learning scheme network routing 
technical report cmu cs school computer science carnegie mellon university 
rudin 
routing delta routing taxonomy performance comparison techniques packet switched networks 
ieee transactions communications com january 
tanenbaum 
computer networks 
prentice hall second edition edition 
tesauro 
issues temporal difference learning 
machine learning may 
sebastian thrun 
role exploration learning control 
david white donald editors handbook intelligent control neural fuzzy adaptive approaches 
van nostrand reinhold new york 
watkins 
learning delayed rewards 
phd thesis king college cambridge 
