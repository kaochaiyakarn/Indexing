duality learning machines bridge supervised unsupervised learning 
nadal laboratoire de physique statistique ecole normale sup erieure rue paris cedex france 
parga departamento de te universidad aut de madrid blanco madrid spain 
exhibit duality perceptrons allows compare theoretical analysis supervised unsupervised learning tasks 
perceptron output asked learn classification patterns 
second dual perceptron outputs asked transmit information possible distribution inputs 
show particular maximum information stored couplings supervised learning task equal maximum information transmitted dual perceptron 
laboratoire associ au 
aux universit es paris vi paris vii 
supervised unsupervised learning main research themes study formal neural networks 
case set input output pairs learned neural network usually architecture 
may interested performance network associative memory may interested ability network generalize rule assumed hidden examples input output pairs learned asks net give correct output new input 
case associative memory emphasis usually put fact memory distributed memory distributed synapses ouput patterns attractors auto associative memory features distributed neurons best studied case random patterns 
generally considered encoding facilitate associative recall high noise tolerance 
second case desired output asking network classify data input patterns 
typically patterns put class nearby input space 
constraint implicit heuristic chosen modifying couplings explicit choice cost function 
famous algorithms kohonen maps algorithm topology introduced output space 
approaches puts emphasis discriminating patterns clustering 
example shown unsupervised hebbian learning single linear output neuron leads principal component analysis 
gaussian input distribution equivalent maximizing amount information output gives input 
fact particular strategy define cost function information theoretic criteria justification general considerations type neural representations codes environment useful brain 
unsupervised learning leads grand mother type cells neuron tends respond specifically type stimuli particular feature 
exemple unsupervised algorithms hebbian learning output unit specific principal component clustering algorithms gets cluster specific cells 
confronted completely opposite approaches differing type issues address type neural codes construct 
propose framework allow better understanding differences supervised unsupervised learning task 
show establish relationship questions relevant task 
done duality neural architectures 
duality interesting context supervised learning bayesian approach tells derive parameters data relating probability parameters model knowing data probability data knowing parameters 
duality introduce explicit implementation exchange model data 
organized follows 
section duality perceptrons show allows relate study supervised learning task unsupervised learning task 
particular show identity various capacities defined context 
section put emphasis differences tasks showing deep relationship problems 
show particular statistical mechanics approach learning related study quantity information relevant context unsupervised learning 
show perceptron thought decoder considers second neural encoder 
perspectives generalization learning machines simple perceptron appendix 
supervised unsupervised learning dual perceptrons consider simple perceptron binary output state oe takes say values inputs neurons couplings fj jn consider continuous inputs specified 
supervised learning task set xi input patterns xi pg set desired outputs learned perceptron 
choice couplings output oe th pattern oe theta theta 
simplicity assume zero threshold consider deterministic rule synaptic noise 
interpret formula ways 
input ouput pairs realized perceptron single output unit couplings 
input gamma gamma gamma gamma gamma gamma gamma phi phi phi phi phi phi phi 
hj oe oe theta gamma 
jn input gamma gamma gamma gamma gamma gamma gamma phi phi phi phi phi phi phi phi phi phi phi phi phi phi gamma gamma gamma gamma gamma gamma gamma 
phi phi phi phi phi phi phi hj hj oe oe oe 
dual perceptrons say perceptron output units input pattern coupling vectors 
call initial perceptron unique output dual perceptron output units just explained show useful duality particular comparison supervised unsupervised learning 
avoid confusions considering dual perceptrons append ambiguous word considering particular write pattern couplings reminder denominations refer respectively 
number domains recall important results concerning supervised learning task particular interest follows geometrical approach computation maximal storage capacity considers space couplings fj ng considered point dimensional space 
pattern defines hyperplane output oe depending side hyperplane point lies 
hyperplanes divide space couplings domains domain associated specific set oe foe oe outputs 
call delta xi number domains delta xi number domains gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma partition space domains oe different output configurations oe delta xi patterns general position delta xi fact independent xi function basic result delta xi delta 

gammak 
particular delta means vapnik chervonenkis dimension perceptron value delta smaller task learn rule examples vc dimension plays crucial role generalization occur number examples large compared 
important parameter asymptotic capacity 
large limit fixed ratio ff fraction output configurations realized remains vanishingly small ff greater critical storage capacity ff ff number domains dual point view reconsider geometrical argument point view dual perceptron defined 
just said choice couplings xi explores possible different output states oe obtained input pattern varies 
represents say light intensities retina oe neural representation visual scene visual pathway 
visual scenes falling domain encoded neural representation delta xi maximal number visual scenes distinguished 
said term coding information specify domain delta xi represents ln delta xi bits information 
maximum amount information information capacity oe convey inputs xi ln delta xi sense xi maximal amount information gained 
consider retina analogy 
visual scene vector dimensional space vector space may correspond possible visual scene 
domains empty stimulus falls inside domains output codes may 
generally statistics visual scenes typically input domains visited equal frequency 
amount information transmitted smaller best equal come back study section 
language information theory channel capacity perceptron memoryless channel communication system 
case input alphabet set possible output alphabet possible output configurations 
xi general position capacity function note capacity smaller equal ln delta 
sees output neuron gives bit information gains information adding new units 
precisely asymptotic behavior lim ff ff ff ff ffs ff ff logarithms expressed base entropy function measured bits gamma ln gamma ln gamma information capacity ff shown 
asymptotic information capacity content perceptron bits input neuron coupling function ff led consider dual perceptron call neural encoder device associates neural representation codeword input signal performance evaluated tools coming information theory 
point view corresponds approach developed particular modeling sensory pathways brain 
context wants system perform efficient coding cost function derived information theory concepts general considerations type coding useful brain 
algorithmic counterpart modification couplings order minimize cost function results unsupervised learning schemes cost function specifies average quality code desired output input come back 
duality perceptrons bridge study supervised unsupervised learning tasks 
information content seen ln delta ff large limit information quantity relevant meaning 
number bits needed specifying domain delta amount information stored couplings learning association xi particular configuration corresponds existing domain 
gives obvious result ff amount information stored bits synapse equal ff 
ff ff probability large limit domain exists configuration chosen random errors result 
shown toulouse ff ff remains maximal amount information stored synapse 
term information capacity dual meaning information content capacity transmitting information 
rest detail comparison study supervised learning task neural encoder defined 
statistical mechanics mutual information information capacity perceptron equal information storage capacity perceptron associative memory important differences analysis tasks 
see specific relevant questions perceptron 
supervised learning start supervised learning task statistical physics bayesian approach supervised learning forces study statistical ensemble machines couplings taken prior distribution ae 
example looks discrete couplings ae may give equal weight possible choice couplings 
example best studied case spherical couplings ae uniform measure sphere 
interested probability set outputs oe chosen random realizable 
deterministic rule probability having oe expression oe ae theta oe words oe fractional volume couplings implement particular set associations xi oe 
delta xi oe important quantity relevant discussion 
typical probability random oe learnable computed patterns drawn statistical ensemble 
principle compute oe choice patterns 
large limit log probability oe ln oe self averaging limit goes infinity exists function distribution ae limit averaged value lim lim 
ln oe 

means average patterns 
ae xi xi statistical mechanics tools replica technique cavity method possible computation various choices patterns distributions uncorrelated bias correlated patterns space couplings continuous discrete 
case gets particular critical asymptotic capacity ff unsupervised learning turn dual perceptron 
having specified distribution ae consider instant dual perceptron receives new input particular pattern occurring ae 
concrete talk image neural encoder give neural representation code oe 
possible different images different neural representations mentioned interested having largest variety available outputs 
variety measured entropy output oe gamma oe oe ln oe considering deterministic system entropy equal mutual information oe input output oe oe configuration oe observed gain information equal gamma ln oe gammal oe oe oe average gain information 
mutual information main quantity interest study study entropy contrasted randomly chosen oe 
note function distribution ae couplings xi ae xi 
relationship capacity ae gives weight domain space oe corresponds domain oe delta xi entropy maximal possible value ln delta xi 
oe max ae oe gamma oe redundancy code oe 
distribution ae optimize performance network proper choice parameters xi 
simple idea network extract information possible environment means maximizing mutual information optimization principle find xi realizes max xi oe strategy linsker modeling stages visual pathway related strategies minimization redundancy 
framework analytical studies done networks linear neurons 
limit study extreme opposite case binary neurons 
detail various proposed strategies general discussion see case perceptron see consider optimization principle 
meaning principle context couplings taken prior distribution optimal patterns xi easiest learn sense ideal case learnable set associations xi equiprobable 
useful free choose patterns random addresses perceptron random access memory storing bit strings 
context learning rule example network fully unbiased particular choice input patterns learnable rule weight 
conversely typical quantity interest vein fact interesting consider typical mutual information results couplings taken statistical ensemble ae xi lim lim 
oe 

denotes average distribution ae xi 
motivation 
considers input distribution ae 
know information transmitted absence optimization obtained considering couplings taken random component independent unbiased random variable 
tells gained optimization 
furthermore trying find optimal couplings may consider statistical ensemble coupling vectors characterized correlations gamma components 
looks correlations maximize 
carried program perceptron details separate 
main result gaussian inputs correlation matrix optimal correlation matrix gamma equal inverse result similar obtained linear units linsker main differences 
explained computing optimal statistical properties couplings exact optimal couplings 
second considering translational invariant couplings gamma locations output neuron input unit case works linsker atick fact study binary units restriction difficult see bialek zee study binary perceptron discrimination task condition 
particular know capacity case say smaller computed restricting couplings 
statistical approach statistical invariance translation correlations couplings depend index decoding learning considering perceptron neural encoder natural ask possibility decoding reconstructing input image generated particular codeword 
aspect may biological relevance system need perform reconstruction second way consider decoding process explained reason biologically plausible 
legitimate question information processing point view 
course obtain exactly input image different inputs give output configuration disposal finite amount information knowing codeword oe information precisely equal gamma ln oe produce input configuration produce codeword prototype images considered identical coding system 
done considering precisely perceptron explain 
knowledge codeword oe couplings xi want generate pattern satisfies equations 
conveniently come back interpretation equations saying looking couplings realizing associations oe problem solved algorithmically perceptron type algorithms 
algorithms known converge solution exists case true input pattern course particular solution 
may look coupling vector input pattern produced codeword 
standard strategy task learning rule example 
sees maximum likelihood decoding ae input distribution equivalent bayesian learning ae prior distribution 
source entropy storage resources point comparison bears aspect efficiency perceptrons 
context supervised learning interested comparing amount information stored number bits storing couplings 
convenient assume finite possibly large number bits synapse nk bits available couplings taken prior distribution ae total number bits effectively available may smaller gamma ae ln ae clearly amount information stored couplings larger language dual perceptron states mutual information larger information content source relates information capacity information capacity computed continuous couplings infinite 
limits number bits nk different coupling vectors 
partition space induced patterns domains may empty 
call delta xi number non empty domains capacity ck xi ln delta xi capacity clearly bounded nk generally upper bound capacity restricted distributions maximum entropy 
analytical results known case discrete couplings apart critical storage capacities ff various choices discrete couplings 
example takes binary couplings gamma corresponds binary inputs asymptotic information capacity equal ff ff binary inputs capacity equal ff ff 
shown existence duality property perceptrons allows comparison theoretical analysis supervised unsupervised learning tasks 
questions relevant case intimately related relevant perceptron 
particular information capacity nice dual meaning 
important differences expressed mainly fact supervised learning task interested performance choice outputs unsupervised task average properties possible outputs matter 
shown statistical physics tools studying typical properties dual perceptron detailed analysis 
considered simplest neural architecture 
stress duality extended general learning machine shown details appendix 
mentioned duality viewed implementation exchange model data appears bayesian approach learning explicit appendix 
considering extension general learning machine useful particular order identify specific role number couplings vc dimension number inputs equal case perceptron 
main result essentially said perceptron remains valid general case provided ff ratio vc dimension ratio number couplings 
result upper bound number domains vapnik book point appendix bound optimal 
fact appears perceptron plays special role learning machines having vc dimension perceptron largest information capacity ln delta xi larger note restricted study deterministic perceptron 
presently working noisy systems said remains valid noise introduces additional interesting differences types learning tasks 
toulouse fruitful discussions 
toulouse critical reading manuscript 
discussions information theory 
checking english correctness 
np hospitality received laboratoire de physique statistique ecole normale sup erieure paris di infn istituto di rome 
partly supported programme 
appendix dual learning machines statement problem results obtained perceptron may misleading number couplings number inputs vc dimension equal 
number domains independent choice patterns general position chosen random 
useful consider case general learning machine 
see valid perceptron remains nearly valid general case provided identifies specific role various parameters 
consider machine defined architecture set adjustable parameters couplings fj ng 
dimension input space machine associates input mg binary output oe 
wants study hetero associative task input patterns xi pg set desired outputs choice couplings outputs oe foe oe oe 
input patterns chosen distribution ae desired chosen random 
ae xi gamma 
oe xi ae oe foe oe learning machine main questions asks storage capacity maximal number input ouput pairs learned 
maximal amount information stored couplings task learn rule example probability making error new pattern function fraction errors done training set xi 
statistical physics bayesian approach learning forces study statistical ensemble machines couplings taken prior distribution ae 
example looks discrete couplings ae may give equal weight possible choice couplings 
having specified machine distributions ae ae introduce dual statistical ensemble learning machines shown 
inputs dimensional patterns occurring probability ae 
want study neural encoder module communication system briefly explained 
main questions distribution ae information output conveys input mutual information oe ae gamma 
xi 
oe foe oe xi ae xi dual machine maximal amount information conveyed irrespective ae channel capacity channel choice parameters xi choice ae xi optimizes performance cost function specified 
number domains relate questions listed study exactly case perceptron 
step considering number domains delta xi 
general machine number depend data xi 
note domain set points space associated configuration oe need connected 
information capacity xi xi ln delta xi large system may expect xi self averaging quantity interested average value 
ln delta xi 
means average respect ae xi 
typical amount information stored couplings typical information capacity neural encoder 
context learning rule example shown vapnik generalization guaranteed probability making error new input pattern tend fraction errors training set lim may define typical vc dimension value smaller critical storage capacity maximal value ff lim 
storage capacity correspond computed statistical mechanics tools 
sufficient condition true vc dimension finite 
vc dimension defined relative worst case best case depends point view considers maximal value number domains delta max xi delta xi delta called growth function 
depends number patterns architecture delta equal vc dimension equal value delta vapnik chervonenkis showed remarkable result finite delta delta delta number domains perceptron inputs patterns general position see delta 

gammak 
upper bound ln delta information capacity note bound optimal bound valid learning machines having value vc dimension bound saturated machines perceptron large network large 
convenient define ff ff curve shown appears universal curve 
large size limit ratio ff lim ff lim ff ff ff ff ff ffs ff ff ln ff ff large interesting note meaning information content context learning rule generalization occurs adding new pattern bring new information 
conclude section sees extrapolate results obtained perceptron general learning machines provided ff ratio number patterns vc dimension interprets ln delta upper bound information capacity replaced hopfield neural networks physical systems emergent computational abilities 
proc 
natl 
acad 
sci 
usa 
modeling neural networks 
cambridge university press 
kohonen self organization associative memory 
springer berlin 
hertz krogh palmer theory neural computation 
addison wesley cambridge ma 
barlow possible principles underlying transformation sensory messages 
editor sensory communication page 
press cambridge ma 
barlow unsupervised learning 
neural comp 
linsker self organization perceptual network 
computer 
atick information theory provide ecological theory sensory processing 
network 
cover geometrical statistical properties systems linear inequalities applications pattern recognition 
ieee trans 
electron 
comput 
vapnik estimation dependences empirical data 
springer series statistics 
springer new york 
vapnik chervonenkis ya 
uniform convergence relative frequencies events probabilities 
theory probability applications 
gardner space interactions neural networks models 
phys 
math 
gen 
blahut principles practice information theory 
addison wesley cambridge ma 
brunel nadal toulouse information capacity perceptron 
phys 
math 
gen 
levin tishby solla statistical approach learning generalization layered networks 
workshop computational learning theory colt 
grassberger nadal editors 
statistical physics statistical inference back 
kluwer acad 
pub dordrecht 
parisi spin glass theory 
world scientific pub singapore 
properties neural networks storing spatially correlated patterns 
phys 
math 
gen 
bialek zee understanding efficiency human perception 
phys 
rev lett 
nadal 
parga information processing perceptron unsupervised learning task 
preprint appear network 
nadal 
parga information processing perceptron 
neural networks biology high energy physics 
int 
journ 

syst 
minsky papert perceptrons 
press cambridge ma 
storage capacity memory networks binary couplings 
physique france 
stein capacity neural networks discrete synaptic couplings 
phys 
math 
gen 

