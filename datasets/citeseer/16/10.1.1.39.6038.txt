intelligent data analysis feature selection classification dash liu www elsevier com locate ida department information systems computer science national university singapore singapore received january revised march accepted march feature selection focus interest quite time done 
creation huge databases consequent requirements machine learning techniques new problems arise novel approaches feature selection demand 
survey comprehensive overview existing methods 
identifies steps typical feature selection method categorizes different existing methods terms generation procedures evaluation functions reveals hitherto combinations generation procedures evaluation functions 
representative methods chosen category detailed explanation discussion example 
benchmark datasets different characteristics comparative study 
strengths weaknesses different methods explained 
guidelines applying feature selection methods data types domain characteristics 
survey identifies research areas feature selection introduces newcomers field paves way practitioners search suitable methods solving domain specific real world applications 
intelligent data analysis vol 
elsevier com locate ida elsevier science rights reserved 
keywords feature selection classification framework 
majority real world classification problems require supervised learning underlying class probabilities class conditional probabilities unknown instance associated class label 
real world situations relevant features unknown apriori 
candidate features introduced better represent domain 
unfortunately partially completely irrelevant redundant target concept 
relevant feature irrelevant redundant target concept irrelevant feature affect target concept way redundant feature add new target concept 
applications size dataset large learning removing unwanted features 
reducing number irrelevant redundant features drastically reduces running time learning mail nus sg 
mail nus sg 
elsevier science rights reserved 
pii dash liu intelligent data analysis algorithm yields general concept 
helps getting better insight underlying concept real world classification problem :10.1.1.155.2293
feature selection methods try pick subset features relevant target concept 
feature selection defined authors looking various angles 
expected similar intuition content 
lists conceptually different cover range definitions 

idealized find minimally sized feature subset necessary sufficient target concept 

classical select subset features set features value criterion function optimized subsets size 

improving prediction accuracy aim feature selection choose subset features improving prediction accuracy decreasing size structure significantly decreasing prediction accuracy classifier built selected features :10.1.1.155.2293

approximating original class distribution goal feature selection select small subset resulting class distribution values selected features close possible original class distribution feature values :10.1.1.155.2293
notice third definition emphasizes prediction accuracy classifier built selected features definition emphasizes class distribution training dataset 
quite different conceptually 
definition considers factors 
feature selection attempts select minimally sized subset features criteria 
criteria 
classification accuracy significantly decrease 
resulting class distribution values selected features close possible original class distribution features 
ideally feature selection methods search subsets features try find best competing candidate subsets evaluation function 
procedure exhaustive tries find best 
may costly practically prohibitive medium sized feature set size 
methods heuristic random search methods attempt reduce computational complexity compromising performance 
methods need stopping criterion prevent exhaustive search subsets 
opinion basic steps typical feature selection method see 
generation procedure generate candidate subset 
evaluation function evaluate subset examination 
stopping criterion decide 
validation procedure check subset valid 
generation procedure search procedure 
basically generates subsets features evaluation 
generation procedure start features ii features iii random subset features 
cases features iteratively added removed case features iteratively added removed produced randomly 
evaluation function measures goodness subset produced generation procedure value compared previous best 
better replaces previous best dash liu intelligent data analysis fig 

feature selection process validation 
subset 
suitable stopping criterion feature selection process may run exhaustively forever space subsets 
generation procedures evaluation functions influence choice stopping criterion 
stopping criteria generation procedure include predefined number features selected ii predefined number iterations reached 
stopping criteria evaluation function addition deletion feature produce better subset ii optimal subset evaluation function obtained 
loop continues stopping criterion satisfied 
feature selection process halts outputting selected subset features validation procedure 
variations step feature selection process discussed section 
validation procedure part feature selection process feature selection method practice validated 
tries test validity selected subset carrying different tests comparing results previously established results results competing feature selection methods artificial datasets real world datasets 
quite attempts study feature selection methods framework structure 
prominent siedlecki sklansky surveys 
siedlecki sklansky discussed evolution feature selection methods grouped methods past categories 
main focus branch bound methods variants 
experimental study conducted 
survey published year new efficient methods introduced focus relief lvf :10.1.1.48.2488
followed similar approach siedlecki sklansky survey grouped different search algorithms evaluation functions feature selection methods independently ran experiments combinations evaluation functions search procedures 
article survey conducted feature selection methods starting early methods 
section major steps feature selection generation procedure evaluation function divided different groups different feature selection methods categorized type generation procedure evaluation function 
dash liu intelligent data analysis framework helps finding unexplored combinations generation procedures evaluation functions 
section briefly discuss methods category select representative method detailed description simple dataset 
section describes empirical comparison representative methods artificial datasets suitably chosen highlight benefits limitations 
section consists discussions various data set characteristics influence choice suitable feature selection method guidelines regarding choose feature selection method application hand number criterial extracted characteristics data 
concludes section discussions research findings section 
objective article assist finding better feature selection methods applications 

study feature selection methods section categorize major steps feature selection generation procedure evaluation function 
different types evaluation functions compared number criteria 
framework total methods grouped types generation procedure evaluation function 

generation procedures original feature set contains number features total number competing candidate subsets generated huge number medium sized different approaches solving problem complete heuristic random 

complete generation procedure complete search optimal subset evaluation function 
exhaustive search complete 
schlimmer argues just search complete mean exhaustive different heuristic functions reduce search chances finding optimal subset 
order search space fewer subsets evaluated 
optimality feature subset evaluation function guaranteed procedure backtrack 
backtracking done various techniques branch bound best search beam search 

heuristic iteration generation procedure remaining features selected rejected considered selection rejection 
variations simple process generation subsets basically incremental increasing decreasing 
order search space exceptions relief dtm discussed detail section 
procedures simple implement fast producing results search space quadratic terms number features 

random generation procedure new feature selection methods compared categories 
search space methods typically search fewer number dash liu intelligent data analysis subsets setting maximum number iterations possible 
optimality selected subset depends resources available 
random generation procedure require values parameters 
assignment suitable values parameters important task achieving results 

evaluation functions optimal subset relative certain evaluation function optimal subset chosen evaluation function may uses evaluation function 
typically evaluation function tries measure discriminating ability feature subset distinguish different class labels 
langley grouped different feature selection methods broad groups filter wrapper dependence inductive algorithm selected subset 
filter methods independent inductive algorithm wrapper methods inductive algorithm evaluation function 
ben grouped evaluation functions existing categories formation uncertainty measures distance measures dependence measures suggested dependence measures divided categories 
considered classification error rate evaluation function wrapper method existed 
divided evaluation functions categories data intrinsic measures classification error rate estimated incremental error rate third category basically variation second category 
data intrinsic category includes distance entropy dependence measures 
considering divisions latest developments divide evaluation functions categories distance information uncertainty dependence consistency error rate 
subsections briefly discuss types evaluation functions 

distance measures known separability divergence discrimination measure 
class problem feature preferred feature induces greater difference class conditional probabilities difference zero indistinguishable 
example euclidean distance measure 

information measures measures typically determine information gain feature 
information gain feature defined difference prior uncertainty expected posterior uncertainty feature preferred feature information gain feature greater feature entropy measure 

dependence measures dependence measures correlation measures qualify ability predict value variable value 
coefficient classical dependence measure find correlation feature class 
correlation feature class higher correlation feature feature preferred slight variation determine dependence feature features value indicates degree redundancy feature 
evaluation functions dependence measures divided distance information dash liu intelligent data analysis table comparison evaluation functions evaluation function generality time complexity accuracy distance measure low information measure low dependence measure low consistency measure moderate classifier error rate high high measures 
kept separate category conceptually represent different viewpoint 
measures ben survey 

consistency measures measures new focus 
characteristically different measures heavy reliance training dataset min features bias selecting subset features 
min features bias prefers consistent hypotheses definable features possible 
bias discussed section 
measures find minimally sized subset satisfies acceptable inconsistency rate usually set user 

classifier error rate measures methods type evaluation function called wrapper methods classifier evaluation function 
features selected classifier uses selected features predicting class labels unseen instances accuracy level high computationally quite costly 
table shows comparison various evaluation functions irrespective type generation procedure 
different parameters comparison 
generality suitable selected subset different classifiers just classifier 
time complexity time taken selecting subset features 
accuracy accurate prediction selected subset 
column means concluded accuracy corresponding evaluation function 
classifier error rate accuracy evaluation functions depends data set classifier classification feature selection 
results table show non surprising trend time spent higher accuracy 
table tells measure different circumstances example time constraints classifiers choose classifier error rate selected evaluation function 

framework subsection suggest framework feature selection methods categorized 
generation procedures evaluation functions considered dimensions method dash liu intelligent data analysis table dimensional categorization feature selection methods evaluation generation measures heuristic complete random distance measure information measure dependency measure consistency measure classifier error rate sec relief relief iv sec dtm koll saha vii sec poe acc preset ii sec branch bound bff iii sec vi viii ix xi sec focus sch xii sec sbs sfs sbs slash bds moor lee rc xiv sec ichi ichi amb bs xii sec lvf xv sec lvw ga sa pf grouped depending type generation procedure evaluation function 
knowledge attempt group methods considering steps 
chosen total methods literature grouped combination generation procedure evaluation function see table 
distinct achievement framework finding number combinations generation procedure evaluation function empty boxes table appear existing method best knowledge 
framework column stands type generation procedure row stands type evaluation function 
assignment evaluation functions categories may evaluation functions may placed different categories considering different perspectives evaluation function may obtained mathematical transformation evaluation function 
tried resolve naturally possible 
sections explain category methods choose method category detailed discussion pseudo code mini dataset section empirical comparison section 

categories differences total types evaluation functions types generation procedures combinations generation procedures evaluation functions table 
blank boxes table signify method exists combinations 
combinations potential 
section discuss category briefly describing methods choosing representative method category 
explain detail help pseudo code hand run prototypical dataset 
methods row represent wrapper methods evaluation function classifier error rate 
typical wrapper method different dash liu intelligent data analysis table sixteen instances corral class class kinds classifiers evaluation representative method chosen categories evaluation function 
discussed briefly section 
prototypical dataset hand run representative methods shown table consists instances originally instances original corral dataset 
mini dataset binary classes boolean features feature irrelevant feature correlated class label time features relevant boolean target concept qa qb 
pseudo codes denotes training set original feature set number features selected subset number selected required features 

category generation heuristic evaluation distance 
brief description various methods seen table prominent method category relief 
discuss relief variant followed brief discussion method 
relief uses statistical method select relevant features 
feature weight algorithm inspired instance learning algorithms 
set training instances chooses sample instances chooses sample instances user provide number instances sample 
relief randomly picks sample instances instance finds near hit near instances euclidean distance measure 
near hit instance having minimum euclidean distance instances class chosen instance near instance having minimum euclidean distance instances different class 
updates weights features initialized zero intuitive idea feature relevant distinguishes instance near relevant distinguishes instance near hit 
exhausting instances sample chooses features having weight greater equal threshold 
threshold automatically evaluated function uses number instances sample determined inspection features positive weights selected 
relief works noisy correlated features requires linear time number features 
works nominal continuous data 
major limitation help redundant dash liu intelligent data analysis fig 

relief 
features generates non optimal feature set size presence redundant features 
overcome subsequent exhaustive search subsets features selected relief 
relief works binary classes 
limitation overcome relief tackles problem incomplete data 
insufficient training instances relief 
limitation user may find difficult choosing proper 
discussed section 
method uses evaluation function minimizes sum statistical discrepancy measure feature complexity measure bits 
finds feature best distinguishes classes iteratively looks additional features combination previously chosen features improve class discrimination 
process stops minimal representation criterion achieved 

hand run corral dataset see relief randomly chooses instance sample 
assume instance chosen 
finds near hit near instance euclidean distance measure 
difference discrete feature values values match zero 
chosen instance instance near hit difference instance near difference 
choose instance near updates feature weight wj iterated times specified user 
features having weights greater equal threshold selected 
usually weights negative irrelevant features positive relevant redundant features 
corral dataset selects section 

category ii generation complete evaluation distance 
brief description various methods combination old methods branch bound 
methods category variations branch bound method considering generation procedure bff evaluation function method 
discuss branch bound method followed brief discussion methods 
dash liu intelligent data analysis narendra fukunaga defined feature selection classical way see definition section requires evaluation functions monotonic subset features better larger set contains subset 
definition severe drawback real world problems appropriate size target feature subset generally unknown 
definition slightly modified applicable general problems saying attempts satisfy criteria selected subset small possible ii bound placed value calculated evaluation function 
modification starts searching original feature set proceeds removing features 
bound placed value evaluation function create rapid search 
evaluation function obeys monotonicity principle subset value bound removed search tree subsets discarded search space 
evaluation functions generally mahalanobis distance discriminant function fisher criterion bhattacharya distance divergence 
xu yan chang proposed similar algorithm bff search procedure modified solve problem searching optimal path weighted tree informed best search strategy artificial intelligence 
algorithm guarantees best global subset exhaustive enumeration criterion satisfying monotonicity principle 
proves homogeneity coefficient ik measuring degree linear dependence measurements shows applicable feature selection problem due monotonicity principle ik ik 
suitably converted feature selection method implementing evaluation function branch bound backtracking better generation procedure 
fact evaluation function monotonic applicable methods prevents common evaluation functions 
problem partially solved relaxing monotonicity criterion introducing approximate monotonicity concept 

hand run corral dataset see authors mahalanobis distance measure evaluation function 
algorithm needs input required number features attempts find best subset features fig 

branch bound 
dash liu intelligent data analysis fig 

decision tree method dtm 
reject 
begins full set features removes feature turn generate subsets current level specifies different subsets lth level 
sl 
sl stops growing branch pruned grows level feature removed 
corral dataset set subset selected best subset features 

category iv generation heuristic evaluation information 
brief description various methods methods category decision tree method dtm koller sahami method :10.1.1.155.2293
dtm shows feature selection improve case learning 
method feature selection application natural language processing 
run training set features appear pruned decision tree selected 
words union subsets features appearing paths leaf node pruned tree selected subset 
second method intuition feature little additional information subsumed remaining features irrelevant redundant eliminated 
realize koller sahami try approximate markov blanket subset markov blanket feature fi fi conditionally independent class label features including fi 
implementation markov blanket suboptimal ways particularly due naive approximations 

hand run corral dataset see uses information heuristic simple form class problem example log log number instances class label number instances class label zero 
assume attribute root tree partition training set feature takes binary values 
entropy feature 
considering corral dataset assume feature evaluated 
takes value instance positive class instances negative class value instances positive class negative class dash liu intelligent data analysis 
fact value minimum features selected root decision tree 
original training set sixteen instances divided nodes representing instances feature nodes features having entropy selected 
process halts partition contains instances single class test offers improvement 
decision tree constructed pruned basically avoid fitting 
dtm chooses features appearing pruned decision tree 

category generation complete evaluation information category minimum description length method 
method authors attempted eliminate useless irrelevant redundant features 
authors features subset expressed fixed non class dependent function features subset values features subset known feature subset useless 
feature selection complete feature set task separating 
solve authors minimum description length criterion introduced rissanen 
formulated expression interpreted number bits required transmit classes instances optimal parameters useful features useless features 
algorithm exhaustively searches possible subsets outputs subset satisfying 
method find useful features observations gaussian 
non gaussian cases may able find useful features section 

hand run corral dataset seen basically evaluation equation represents description length candidate subset features 
actual implementation suggested authors follows 
calculate covariance matrices feature vectors classes class separately 
covariance matrix useful subsets obtained sub matrices 
determine determinants sub matrices dl dl equation shown find subset having minimum description length 
corral dataset chosen feature subset minimum description length 

category vii generation heuristic evaluation dependence 
brief description various methods poe acc probability error average correlation coefficient method preset combination 
techniques feature selection poe acc choose seventh method authors consider important 
method feature selected feature smallest probability error pe 
feature selected feature produces minimum weighted sum pe average correlation coefficient acc 
acc mean correlation coefficients candidate feature features previously selected point 
method rank features dash liu intelligent data analysis fig 

minimum description length method 
fig 

poe acc 
weighted sum required number features stopping criterion 
preset uses concept rough set finds reduct reduct set classifies instances equally removes features appearing reduct 
ranks features significance 
significance feature measure expressing important feature regarding classification 
measure dependence attributes 

hand run corral dataset see feature chosen feature having minimum pe iteration feature having minimum pe acc selected 
experiments consider authors suggest values case study 
calculate pe compute different classes 
mini dataset class class 
feature calculate class conditional probabilities class label 
find class label probability feature having value value class label class conditional probability feature having value value 
dash liu intelligent data analysis feature value feature value find class label product apriori class probability class conditional probability class label maximum 
feature takes value prediction class feature takes value prediction class 
instances count number mismatches actual predicted class values 
feature fraction mismatches pe 
fact features pe selected feature 
second step correlations remaining features feature calculated 
expression pe acc find feature value features chosen second feature 
required number subset selected 

category xi generation complete evaluation consistency 
brief description various methods methods combination developed years 
discuss focus schlimmer method select focus representative method detailed discussion empirical comparison :10.1.1.48.2488
focus implements min features bias prefers consistent hypotheses definable features possible 
simplest implementation breadth search checks inconsistency considering candidate subset features 
strictly speaking focus unable handle noise simple modification allows certain percentage inconsistency enable find minimally sized subset satisfying permissible inconsistency 
methods seen variants 
schlimmer method uses systematic enumeration scheme generation procedure inconsistency criterion evaluation function 
uses heuristic function search optimal subset faster 
heuristic function reliability measure intuition probability inconsistency observed proportional percentage values observed infrequently considering subset features 
supersets unreliable subset reliable 
similar focus selection features 
represents set instances form matrix element stands unique combination positive instance class negative instance class 
feature said cover element matrix assumes opposite values positive instance negative instance associated element 
searches cover features starting features iterates reduction size cover attained 

hand run corral dataset see focus uses breadth generation procedure generates subsets size followed subsets size 
subset generated checks instances dataset having equal values features examination different class labels inconsistency 
case arises rejects subset saying inconsistent moves test subset 
continues finds subset having inconsistency search complete possible subsets inconsistent 
subset instance value different class labels handle binary classes boolean features 
dash liu intelligent data analysis fig 

focus 
respectively 
rejects moves subset 
evaluates total subsets selecting subset 

category xii generation random evaluation consistency 
brief description various methods category new representative lvf 
lvf randomly searches space subsets las vegas algorithm probabilistic choices help guide quickly optimal solution uses consistency measure different focus 
candidate subset calculates inconsistency count intuition frequent class label instances matching subset features probable class label 
inconsistency threshold fixed default subset having inconsistency rate greater rejected 
method find optimal subset datasets noise rough correct noise level specified 
advantage user wait long subset outputs subset better previous best size subset inconsistency rate 
algorithm efficient subsets number features smaller equal current best subset checked inconsistency 
simple implement guaranteed find optimal subset resources permit 
drawback may take time find optimal subset algorithms heuristic generation procedure certain problems take advantage prior knowledge 

hand run corral dataset see lvf chooses feature subset randomly calculates cardinality 
best subset initialized complete feature set 
randomly chosen subset cardinality equal current best subset evaluates inconsistency rate new subset 
rate equal threshold value default value zero new subset current best subset 
say subset randomly chosen 
patterns mixed class labels class distribution class respectively 
inconsistency count sum number matching instances minus number matching instances frequent class label pattern 
subset inconsistency count inconsistency rate 
specified threshold inconsistency rate zero default subset rejected 
randomly chosen subset inconsistency rate zero current best subset 
subset smaller size inconsistency rate zero leading selection 
dash liu intelligent data analysis fig 

lvf 

category xiii xv evaluation classifier error rate section briefly discuss give suitable methods classifier error rate evaluation function commonly known wrapper methods 

heuristic heuristic generation function find popular methods sequential forward selection sfs sequential backward selection sbs sequential backward selection slash sbs slash sequential search bi directional search bds schemata search relevance context rc gelsema method variants sfs sbs 
sfs starts empty set iteration generates new subsets adding feature selected evaluation function 
sbs starts complete feature set iteration generates new subsets discarding feature selected evaluation function 
sbs slash observation large number features classifiers id frequently 
starts full feature set sbs step eliminates slashes feature learned step 
bds allows search ends provides degree backtracking allowing addition deletion subset 
starts empty set adds features discards step starts complete feature set discards features adds features step 
schemata search starts empty set complete set iteration finds best subset removing adding feature subset 
evaluates subset leave cross validation loocv iteration selects subset having loocv error 
continues way single feature change improves 
rc considers fact features relevant parts space 
similar sbs crucial difference local instance specific decisions feature relevance opposed global ones 
gelsema method similar sfs suggests iteration feature various settings considering different interactions set features previously selected evaluated 
simple settings assume independence features consider dash liu intelligent data analysis previously selected features ii assume independence consider previously selected features 
bayesian classifier error rate evaluation function 

complete complete generation procedure find wrapper methods 
sklansky devised methods different classifiers linear classifier box classifier 
methods solve feature selection problem zero integer programming 
approximate monotonic branch bound amb introduced combat disadvantage permitting evaluation functions monotonic 
bound relaxed generate subsets appear subset violating bound selected subset violate bound 
beam search bs type best search uses bounded queue limit scope search 
queue ordered best worst best subsets front queue 
generation procedure proceeds subset front queue producing possible subsets adding feature 
subset placed proper sorted position queue 
limit size queue bs exhaustive search limit size queue equivalent sfs 

random methods random generation procedure lvw genetic algorithm simulated random generation plus sequential selection pf 
lvw generates subsets perfectly random fashion uses las vegas algorithm genetic algorithm ga simulated annealing sa element randomness generation follow specific procedures generating subsets continuously 
injects randomness sfs sbs 
generates random subset runs sfs sbs starting random subset 
random mutation hill climbing prototype feature selection pf selects prototypes instances features simultaneously nearest neighbor classification problem bet vector records prototypes features 
uses error rate nearest neighbour classifier evaluation function 
iteration randomly mutates bit vector produce subset 
methods need proper assignment values different parameters 
addition maximum number iterations parameters may required lvw threshold inconsistency rate ga initial population size crossover rate mutation rate sa annealing schedule number times loop initial temperature mutation probability 

summary shows summary feature selection methods types generation procedures 
complete generation procedures subdivided exhaustive exhaustive category method may evaluate subsets may breadthfirst search searching soon optimal subset non exhaustive category find branch bound best beam search different search techniques 
heuristic generation procedures subdivided forward selection backward selection combined forward backward instance categories 
similarly random generation procedures grouped type type ii type methods probability subset generated remains constant subsets type ii methods probability changes program runs 
dash liu intelligent data analysis fig 

summary feature selection methods 
underlined methods represent categories table implemented empirical comparison categorical methods section 

empirical comparison section discuss different issues involved validation procedure commonly feature selection methods 
briefly discuss artificial datasets chosen experiments 
compare analyze results running categorical methods chosen datasets 

validation commonly validation procedures feature selection methods artificial datasets real world datasets 
artificial datasets constructed keeping mind certain target concept actual relevant features concept known 
validation procedures check output selected subset actual subset 
training dataset known relevant features irrelevant redundant features noise taken 
feature selection method run dataset result compared known relevant features 
second procedure real world datasets chosen may benchmark datasets 
actual subset unknown case selected subset tested accuracy help classifier suitable task 
typical classifiers naive bayesian classifier cart id fringe aq cn 
achieved accuracy may compared wellknown methods efficacy analyzed 
section trying show empirical comparison representative feature selection methods 
opinion second method validation suitable task particular classifiers support specific datasets test combination classifier dataset may wrongly show high accuracy rate variety classifiers chosen statistically different dash liu intelligent data analysis datasets different feature selection methods different bias selecting features similar different classifiers fair certain combinations methods classifiers try generalize results feature selection methods better considering classifier 
fortunately papers introduce new feature selection methods done extensive tests comparison 
reader wants probe referred survey original 
give intuition regarding wrapper methods include wrapper method lvw empirical comparison representative methods categories ii iv vii xi xii categories empty 
choose artificial datasets combinations relevant irrelevant redundant features noise perform comparison 
simple characteristics testing 
dataset training set commonly literature comparing results chosen checking feature selection method find relevant features 

datasets datasets chosen dataset irrelevant correlated features second irrelevant redundant features third irrelevant features noisy misclassification 
attempt find strengths weaknesses methods settings 

corral dataset instances binary classes boolean features feature irrelevant feature correlated class label time features relevant boolean target concept qa qb 

modified par par modified version original parity data redundant feature 
dataset contains binary classes twelve boolean features features relevant redundant irrelevant randomly chosen 
training set contains instances target concept odd parity features 

monk binary classes discrete features second fourth fifth relevant target concept denoting inequality 
training set contains instances noise misclassification 

results analyses table shows results running categorical methods artificial datasets 
paragraphs analyze results compare methods datasets 

relief corral dataset relief selects range 
values relief prefers correlated feature relevant feature 
experiment dash liu intelligent data analysis table table showing features selected datasets method modified par monk ra relief dtm poe acc focus lvf lvw ra relevant attributes inconsistency threshold 
modified par dataset clearly shows relief unable detect redundant features features respectively 
partially successful depends value monk dataset contains noise 
experimentally range selects relevant features range chooses irrelevant features select relevant features 
dataset user may find difficult choose appropriate value 
suggest value large small 

corral dataset rejects feature feature iteration rejects feature case par works asked select features selects asked select selects 
failed monk dataset noise 

dtm explained section dtm prefers features highly correlated class label case corral dataset correlated feature selected root 
par difficult task inductive algorithm fails find relevant features 
monk dataset contains noise dtm partially successful choosing pruning 

works observations gaussian 
datasets perfectly gaussian reason largely failed 
corral selects subset consisting dash liu intelligent data analysis correlated feature indicates prefers correlated features 
datasets results 

poe acc results method show rankings features 
seen successful dataset 
corral chooses feature feature explained section 
manually input features correctly ranks relevant features top positions 
interesting finding points importance feature remaining features poe acc 
par monk 

focus focus works dataset noise case corral par selects extra feature monk noise actual subset satisfy default inconsistency rate zero 
focus fast producing results corral par actual subsets appear supports breadth search 

lvf lvf works datasets experiment 
corral correctly selects actual subset 
particularly efficient par redundant features generate result possible actual subsets quite early randomly generating subsets 
monk dataset needs approximate noise level produce actual subset default case zero inconsistency rate selects extra feature 

discussions guidelines feature selection application areas tool remove irrelevant redundant features 
known single feature selection method applied applications 
choice feature selection method depends various data set characteristics data types ii data size iii noise 
different criteria characteristics give guidelines potential user feature selection method select particular application 

data types various data types practice features class labels 
features feature values continuous discrete nominal including boolean 
nominal data type requires special handling easy assign real values 
partition methods ability handle different data types 
class label feature selection methods handle binary classes deal multiple classes 
methods separated ability handle multiple classes 
dash liu intelligent data analysis 
data size aspect feature selection deals method perform small training set ii handle large data size 
information age datasets commonly large size second criterion practical interest current researchers practitioners area separate methods ability handle large dataset 

noise typical noise encountered feature selection process misclassification ii conflicting date 
partition methods ability handle noise 

guidelines subsection give guidelines criteria extracted characteristics 
criteria follows ability handle different data types denote continuous discrete nominal ability handle multiple classes ability handle large dataset ability handle noise ability produce optimal subset data noisy 
table lists capabilities regarding characteristics sixteen feature selection methods appear framework section 
methods having classifier error rate evaluation function considered capabilities depend particular classifier 
method implemented different ways entries table presentation methods papers 
suggests method discuss particular characteristics 
user may employ prior knowledge regarding characteristics find appropriate method particular application 
example data contains noise methods marked column noise considered 
method satisfying criteria mentioned user user needs closer look methods find suitable design new method suits application 

definition feature selection discussing existing definitions 
steps typical feature selection process recognized generation procedure evaluation function stopping criterion validation procedure 
group generation procedures thee categories complete heuristic random evaluation functions categories distance information dependence consistency classifier error rate measures 
existing feature selection methods categorized combinations generation procedure evaluation function dash liu intelligent data analysis table feature selection methods capabilities category method ability handle produce data types multiple large noise optimal classes dataset subset relief relief ii bff iv dtm koll saha vii poe acc preset xi sch xii lvf handle boolean features certain assumptions valid user required provide noise level provided resources 

methods category described briefly representative method chosen category described detail pseudo code hand run dataset version corral 
representative methods compared artificial datasets different properties results analyzed 
discuss data set characteristics give guidelines regarding choose particular method user prior knowledge problem domain 
article survey feature selection methods 
finding number combinations generation procedure evaluation function attempted best knowledge 
quite interesting considering fact feature selection focus interest various groups researchers decades 
comparison categorical methods reveal interesting facts regarding advantages disadvantages handling different characteristics data 
guidelines section useful users feature selection dash liu intelligent data analysis sheer number methods 
properly guidelines practical value choosing suitable method 
feature selection developed research area researchers try find better methods classifiers efficient 
scenario framework shows combinations generation procedures evaluation functions helpful 
testing particular implementation evaluation function generation procedure combinations previously categories design new efficient methods combinations 
obvious combinations efficient 
combination consistency measure heuristic generation procedure category may give results 
may test different combinations previously exist branch bound generation procedure consistency measure category xi 
acknowledgments authors wish referees professor hiroshi motoda osaka university valuable comments constructive suggestion thorough reviews earlier version article 
lee leslie leon sng wong implementing feature selection methods analyzing results 
aha kibler albert instance learning algorithms 
machine learning 
almuallim dietterich learning irrelevant features 
proceedings ninth national conference artifical intelligence mit press cambridge massachusetts 
almuallim dietterich learning boolean concepts presence irrelevant features 
artificial intelligence november 
ben pattern recognition reduction dimensionality 
handbook statistics krishnaiah kanal eds north holland 
feature selection homogeneity coefficient 
proceedings ninth international conference pattern recognition 
brassard fundamentals algorithms 
prentice hall new jersey 
breiman friedman olshen stone classification regression trees 
wadsworth international group belmont california 
callan fawcett rissland adaptive approach case search 
proceedings twelfth international joint conference artificial intelligence 
cardie decision trees improve case learning 
proceedings tenth international conference machine learning 
caruana freitag greedy attribute selection 
proceedings eleventh international conference machine learning morgan kaufmann new brunswick new jersey 
clark niblett cn induction algorithm 
machine learning 
devijver kittler pattern recognition statistical approach 
prentice hall 
evaluation feature selection methods application computer security 
technical report davis ca university california department computer science 
domingos context sensitive feature selection lazy learners 
artificial intelligence review 
duran odell cluster analysis survey 
springer verlag 
sklansky feature selection automatic classification non gaussian data 
ieee transactions systems man smc 
dash liu intelligent data analysis integer programming implicit enumeration balas method 
siam review 
holte simple classification rules perform commonly datasets 
machine learning 
sklansky feature selection linear classifier 
proceedings seventh international conference pattern recognition volume july aug 
sklansky optimum feature selection zero programming 
ieee trans 
systems man cybernetics smc september october 
john kohavi pfleger irrelevant features subset selection problem 
proceedings eleventh international conference machine learning 
kira rendell feature selection problem traditional methods new algorithm 
proceedings ninth national conference artificial intelligence 
kohavi sommerfield feature subset selection wrapper method overfitting dynamic search space topology 
proceedings international conference knowledge discovery data mining morgan kaufmann 
koller sahami optimal feature selection :10.1.1.155.2293
proceedings international conference machine learning 
kononenko estimating attributes analysis extension relief 
proceedings european conference machine learning 
langley selection relevant features machine learning 
proceedings aaai fall symposium relevance 
liu setiono feature selection classification probabilistic wrapper approach 
proceedings ninth international conference industrial engineering applications ai es 
liu setiono probabilistic approach feature selection filter solution 
proceedings international conference machine learning 
michalski carbonell mitchell eds effect noise concept learning machine learning artificial intelligence approach vol 
ii morgan kaufmann san mateo ca 
michalski hong lavrac aq inductive learning system overview experiments 
technical report uiucdcs university illinois july 
feature selection rough sets theory 
proceedings european conference machine learning brazdil ed 
moore lee efficient algorithms minimizing cross validation error 
proceedings eleventh international conference machine learning morgan kaufmann new brunswick new jersey 
comparison techniques choosing subsets pattern recognition 
ieee transactions computers september 
narendra fukunaga branch bound algorithm feature selection 
ieee transactions computers september 
oliveira vincentelli constructive induction non greedy strategy feature selection 
proceedings ninth international conference machine learning morgan kaufmann aberdeen scotland 
pagallo haussler boolean feature discovery empirical learning 
machine learning 
gelsema feature selection 
proceedings seventh international conference pattern recognition july aug 
quinlan induction decision trees 
machine learning morgan kaufmann 

quinlan programs machine learning 
morgan kaufmann san mateo california 
rissanen modelling shortest data description 
automatica 
schaffer overfitting avoidance bias 
machine learning 
schaffer conservation law generalization performance 
proceedings eleventh international conference machine learning morgan kaufmann new brunswick nj 
schlimmer efficiently inducing determinations complete systematic search algorithm uses optimal pruning 
proceedings tenth international conference machine learning 
feature selection constructive inference 
proceedings seventh international conference pattern recognition 
dash liu intelligent data analysis dom niblack modelling approach feature selection 
proceedings tenth international conference pattern recognition june 
siedlecki sklansky automatic feature selection 
international journal pattern recognition artificial intelligence 
skalak prototype feature selection sampling random mutation hill climbing algorithms 
proceedings eleventh international conference machine learning morgan kaufmann new brunswick 
thrun monk problem performance comparison different algorithms 
technical report cmu cs carnegie mellon university december 
vafaie feature selection methods genetic algorithms vs greedy search 
proceedings international conference fuzzy intelligent control systems 
xu yan chang best strategy feature selection 
proceedings ninth international conference pattern recognition 
