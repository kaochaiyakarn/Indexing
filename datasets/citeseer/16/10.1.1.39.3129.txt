automated learning decision rules text categorization apte damerau weiss acm transactions information systems ibm research report rc 
acm transactions information systems automated learning decision rules text categorization apte fred damerau weiss ibm research division ibm research division rutgers university watson research center watson research center dept computer science yorktown heights ny yorktown heights ny new brunswick nj apte watson ibm com damerau watson ibm com weiss cs rutgers edu describe results extensive experiments large document collections optimized rule induction methods 
goal methods automatically discover classi cation patterns general document categorization personalized ltering free text 
previous reports indicate human engineered rule systems requiring developmental orts successfully built read documents assign topics 
generated decision rules appear comparable human performance identical rule representation 
comparison machine learning techniques results key benchmark reuters collection show large gain performance previously reported recall precision breakeven point 
context high dimensional feature space methodological alternatives examined including universal versus local dictionaries binary versus frequency related features 
keywords information retrieval machine learning text categorization rule induction assigning classi cations documents essential cient management retrieval knowledge 
document classi cations typically assigned humans read documents knowledgeable subject matter 
large organizations huge volumes textual information created examined form categorization textual information ow required 
major problem document retrieval determining document relevant query 
determination inherently imprecise experienced people di er judgments respect document query pair document available considerable range background information draw 
document query representations available computer programs rich results may precise 
number documents potential interest searcher far exceeds hope read 
way limit search relevant topics assign subject codes predetermined list document added storage system 
large number classi cation systems common document collections 
assigning subject classi cation codes manually documents time consuming expensive 
human engineered rule models assigning subject codes relatively ective expensive time ort development continued support 
report presents results experiments derive assignment rules automatically samples text classi ed 
carefully organized text storage retrieval systems texts classi ed codes chosen classi cation system 
examples include ntis national technical information service documents government news services reuters publications acm computing reviews 
shown certain environments knowledge systems code assignment quickly accurately hayes weinstein hayes 
machine learning methods provide interesting alternative automating rule construction process 
ective machine generated solutions obviously increase ciency productivity 
computer readily process information faster humans 
explosion electronically stored text ciency increasing importance 
immediate ciency gains great promise machines appear read machines examine free text correct decisions 
techniques general decisions text categorization adapted individual tastes examining great volumes text ltering documents suit personal interests sheth maes 
claim techniques currently feasible capable processing huge numbers documents reasonable times high performance achievable high quality sample data available 
awell known example expert system task construe system hayes reuters news service 
rule expert system manually constructed rules assign subject categories news stories reported recall precision test cases hayes weinstein 
exceptionally results test set relatively sparse compared number possible topics 
example machine learning system task system memory reasoning masand employs nearest neighbor style classi cation reported accuracy range dow jones news stories 
considering problem categorizing documents rule approach considerable appeal 
weighted solutions linear probabilistic methods lewis nearest neighbor methods may prove reasonable models employ explicitly interpretable 
human engineered systems successfully constructed rule solutions useful continue model compatible human expressed knowledge 
parsimonious interpretable nature decision rules readily augment knowledge verify rules examining pre categorized documents 
remainder describe approach automating task generating text categorization models 
inducing rule categorization models machine learning systems solve problems examining samples described terms measurements features 
application machine learning methods samples documents transformed type representation 
text categorization adaptation machine learning method consider main processes preprocessing step determining values features attributes representing individual documents collection 
essentially dictionary creation process 
representation step mapping individual document training sample dictionary associating label identi es category 
induction step nding patterns distinguish categories 
evaluation step choosing best solution minimizing classi cation error cost 
rst step produce list attributes samples text labeled documents dictionary 
attributes single words word phrases 
attribute list sample cases described terms words phrases documents 
case consists values attributes single article values ether boolean indicating attribute appears text numerical frequency occurrence text processed 
addition case labeled indicate classi cation topic article represents 
rule induction objective nd sets decision rules distinguish category text 
best rule set selected best rule set accurate excessively complex 
accuracy rule sets ectively measured large numbers independent test cases 
complexity measured terms numbers rules rule components smaller rule sets reasonably close best accuracy preferred complex rules sets slightly greater accuracy 
atypical architecture machine learning text categorization illustrated 
discuss issues greater detail 
text representation document retrieval systems supposed choose documents concept interest retriever 
documents concepts words 
words clearly correspond directly concepts 
words concept bank nancial institution bank part river 
concepts require word designation football player running back concepts referenced word phrase doctor physician 
humans relatively inferring concepts words document 
bring bear vast knowledge grammar language world large 
little knowledge available computer system large part sketchy incomplete methods organizing inferring information automatically 
programs 
announced formation wholly owned environmental subsidiary response new environmental protection agency regulations underground storage tanks announce formation wholly wholly environmental subsidiary response regulation underground storage tank storage tank table example text fragment corresponding attribute list parsing sentences representing semantic content formal language rst order logic fail 
simpler tasks deciding particular word bear taken noun verb bank referred sophisticated parsing systems far error free 
despite conceptual weaknesses current research text categorization supports cacy simpler schemes text representation lewis 
conventional approach isa relatively simple selection method creating dictionary vocabulary 
text portion documents scanned produce list single words list word pairs word belongs de ned list words number part proper name 
word pair occurs times eliminated 
words pairs recurring times statistically reliable indicators 
choosing cuto arbitrary statistical natural language preprocessing church hanks 
lists similar identi ed lewis lewis words phrases 
experiments con rm reports literature pairs attributes gives poor results general 
lewis sophisticated phrase selection method reached 
single words relatively successful instances including pairs dictionary give better results 
experiments dictionaries derived full collection documents topics universal dictionaries single words pairs entered dictionaries 
single word pair lists merged sorted frequency terms frequent retained 
list reduced eliminating bottom ranking terms terms frequency set frequent eliminating function words 
result attribute list started approximately attributes 
experiments run full attribute set attributes irrelevant topic widely approach prune attribute size statistical feature selection methods 
categorization problem statistical feature selection techniques techniques select words word pairs related topic 
turn allows processing greater numbers sample cases 
choosing right attribute set represent document critical successful induction classi cation models 
attribute selection process just described creates dictionary terms document family 
individual document family characterized set features boolean indicators denoting term dictionary absent document 
example text dated attributes single words pairs generated text illustrated table 
range tting methods classi er forms approaches feature selection categorization problem extensive varied fung fuhr pfeifer flower jennings lewis 
di cult condense approaches formalism utilize represent atypical architecture text categorization machine learning approach 
universal dictionary created topics feature selection select words phrases dictionary solve speci text categorization problem 
text document represented set boolean true false attributes 
demonstrate signi cant departure approach yields somewhat better results 
illustrates alternative strategy 
universal dictionary replaced local dictionaries classi cation topic 
single words documents topic entered local dictionary 
complicated statistical feature selection step completely eliminated slight expense generating new dictionary topic relatively simple task single words simple word matching strategies 
local dictionary frequently occurring words features optimal value chosen empirical observations 
argued fact constitutes feature selection process easily observe computation free process opposed classical feature selection methods 
addition boolean complicated frequency related features simple counts occurrence words story feature values 
dictionaries single words mean best solution ignores phrases combinations words 
clearly combinations important understanding text 
burden shifted preprocessing program composes dictionary learning program nds solution 
research results suggest di cult nd right combinations words independent ultimate decision model 
implication analysis performance increased improved learning methods 
methods nd higher order relationships feature space dictionary words 
main distinguishing characteristics approach rule induction model representation 
example type rules illustrated table 
example experiments universal dictionaries comprising single words phrases 
problem posed class problem decision classifying football stories 
rules satis ed decision reverts default class non football article 
applications text classi cation involve classes exclusive categories occur simultaneously 
handle problems multiple class problems 
rule class running back football article kicker football article reserve football article award player football article table example induced rule set text classifying football stories rule induction swap rule tree induction methods extensively described published works breiman weiss kulikowski quinlan 
document indexing apparatus rule induction technique called swap weiss indurkhya 
rule induction methods attempt nd compact covering rule set completely partitions examples correct classes michalski clark niblett 
covering set heuristically searching single best rule covers cases class 
having best conjunctive rule class rule added rule set cases satisfying removed consideration 
process repeated cases remain covered 
decision tree induction programs rule induction methods swap advantage uses local optimization techniques dynamically revise improve covering set 
covering set separates classes induced set rules re ned pruning statistical techniques 
train test evaluation methods initial covering rule set scaled back statistically accurate subset rules 
step predictive value rule table example swapping rule components swap rule construction process brie discuss swap problem solving approach 
set sample cases case composed observed features correct classi cation problem nd best rule set rs best error rate new cases err true rs best minimum 
swap derives solutions posed disjunctive normal form dnf class classi ed set disjunctive production rules 
term conjunction tests wherep proposition formed evaluating truth binary valued feature comparing threshold ofthe values numerical feature assumes samples 
model decision tree implicit productions mutually exclusive 
general dnf model require mutual exclusivity rules 
productions mutually exclusive rules classes potentially satis ed simultaneously 
con icts resolved inducing rules class class priority ordering class considered default class 
ruleset rules components error apparent error test table example swap rule induction process summary table tree rule induction look ahead attribute try specialize tree rule 
heuristic mathematical function gini function breiman evaluate order relevance attributes making best classi cation decision speci context node decision tree 
heuristics tend problems combinatorics nding optimal solution alternative search procedures impractical 
methods swap constantly looks back see improvement adding new test 
steps taken form single best rule single best swap possible rule component swaps including deleting component swap add single best component rule 
weiss best evaluated predictive value percentage correct decisions rule 
equal predictive values maximum case coverage secondary criterion 
swapping component addition terminate predictive value reached 
process generating single best rule seen table example rule generated steps 
swap tries maximize predictive value rule fraction examples correctly classi ed rule ideally 
initial rule randomly assigned test component favor single best test component 
step single best component add rule 
step swapped re ning previously selected rule components 
nal step see swapped rst step gets swapped 
seen test swapped necessarily stay added back doing improves predictive accuracy current rule 
completed rule selected single best rule method proceeds usual removal covered cases re application single best rule construction procedure remaining cases 
finding optimal combination attributes values single xed size rule complex task 
optimization problems traveling salesman problem lin kernighan local swapping nds excellent approximate solutions 
set samples covering rule set rs progressively weaken rs increasingly complex decreasing accuracy 
objective select rule set rs best frs rs rs ng collection rule sets decreasing order complexity rs best fewest errors new cases practice optimal solution usually incomplete samples limitations search time 
possible search possible rule sets complexity cx rs cx appropriate complexity measure number components rule set 
independent test cases su cient give highly accurate estimates error rate classi er 
set frs rs rs ng ordered complexity measure cx rs best selected min err rs 
solve problem practice method induce order frs ig cx rs estimate rule set error rate err rs 
rule set error rate de ned fraction misclassi ed cases total classi ed cases result applying rule 
pruning methods adapted rule induction prune rule set form frs ig 
rule set rs covering rule set 
subsequent rs pruning rs weakest link 
quinlan rule set pruned deleting single rules single components 
application form pruning known weakest link pruning results ordered series decreasing complexity rule sets frs ig illustrated table 
complexity rs measured terms size rs 
net result process error rate estimate varying complexity rule sets 
typical result illustrated table 
rule set rs table lists number rules number rule components apparent error rate training cases error rate independent test cases 
example best solution rule set rules components having observed true error rate 
training cases football football football football test cases football football football football table example observed error rates football rule set swap uses criteria minimum error selecting best rule computation error measure adjusted force swap select rule sets may cover higher number correct cases true positives expense covering incorrect cases false positives 
done standard breiman approach substituting costs errors vary true positives false positives 
cost false negative correct cases missed error cost false negative counted errors 
cost equivalent usual minimum error criterion 
ect increasing cost false negatives increase true positives expense increased false positives 
document classi cation application swap induces rules represent patterns combinations attributes determine class article 
result applying swap training set cases results set rules associated error rates training test samples 
results applying rule set table illustrated table 
depth discussions swap algorithm appear weiss indurkhya 
rule induction search space major dimensions number documents document database size dictionary number classes classi cation models learned 
applications may possible access hundreds thousands documents training purposes 
random sampling ective extracting representative subset training cycle 
classes formulate training problem series dichotomous classi cation induction problems 
serious dimensionality problem lies dictionary size tens thousands 
clearly large numbers features pose computational problem learning system 
conventional feature selection algorithm information entropy metric analogous decision tree construction breiman weiss kulikowski quinlan prune search space 
typically approach reduce feature set small subset original universal dictionary 
local dictionary approach adopted substantially faster eliminating major step classication process 
importantly reduces dimensionality 
number topics grows universal dictionary words inadequate handle low prevalence topics 
increasing size universal dictionary increase dimensionality problems 
observed local dictionary approach faster accurate compared classical feature selection universal dictionary 
results reuters newswires develop text categorization methods run experiments number large document collections including scienti abstracts originating national technical information service library catalogue records representing holdings ibm libraries sample newswire sample reuters newswire properly identi ed reuters referred reuters provide objective basis comparison results particularly lewis lewis detailed number runs reuters data 
news stories 
stories april th independent test cases remaining data training cases 
data consist training cases test cases 
topics interest topics occurring training data 
chose experiment topics 
error measures take methods handle non mutually exclusive classes simultaneously neural nets continue dichotomous representation 
problems dictionary dimensionality quite severe ectiveness feature selection substantially diminished large numbers classes 
obtained anonymous ftp pub doc reuters ftp cs umass edu 
free distribution research purposes granted reuters carnegie group 
arrangements access david lewis 
account remaining topics fewer occurrences training data cases associated topics test data 
evaluation model test data cause erroneous classi cation cases uencing performance measures 
original newswires stories empty topic assignments 
chose ignore stories learn test 
result raw data training cases test cases 
derived dictionaries attributes raw document training data applied rule induction machine learning methods swap 
experiment topic random subset corresponding training data reserved error estimation 
recursively pruned rule sets evaluated randomly selected cases help select best rule set 
estimates cases generally performance selected rule sets independent test cases april th 
wheat farm 
wheat wheat commodity 
wheat export 
wheat wheat agriculture 
wheat wheat 
wheat wheat winter soft 
wheat test cases wheat wheat wheat wheat table induced rule set performance test data reuters wheat category dictionaries created di erent ways 
simpler approach local dictionary process frequent words topic generated 
experimented cuto point evaluating cuto threshold 
results suggested approximately corresponded local minimum terms accuracy performance induced rule sets 
brief universal list stopwords maintained words removed frequent words 
actual number features learning categorization models varied topics range 
local dictionaries created fast algorithm simple sub match strategy stemmer pick unique single words encountered documents belonging topic 
second approach create universal dictionary examining documents training set 
depending topic variable number features derived feature selection method breiman 
universal dictionary approximately features number features selected category ranged 
universal dictionary created match strategy employed stemmer pick unique stems encountered entire training set topics 
list local dictionary time application lter words universal dictionary prior application 
text representation experimented frequency boolean features 
boolean features merely indicate entry dictionary document frequency feature indicates number occurrences dictionary entry document 
experiments performed complicated frequency related measures 
performance measured recall precision 
recall percentage total documents topic correctly classi ed 
precision percentage predicted documents topic correctly classi ed 
exclusive document classi cation problems usually analyzed series dichotomous classi cation problems topic vs topic 
example table illustrates rule set induced wheat category local dictionary boolean representation text 
included gure performance table rule set reuters post april test data 
rule evaluation table table measure performance wide variety metrics error rates costs 
purpose study wehave chosen measure lewis ringuette 
toevaluate performance entire set topics results microaveraged performance tables topics added recall precision computed 
point recall equals precision breakeven point single summarizing measure comparison results 
learning method dictionary text representation performance breakeven optimized rule induction local frequency headlines frequency boolean universal frequency boolean decision tree probabilistic bayes table recall precision breakeven points various classi cation methods reuters data breakeven point foreach combinations dictionaries features illustrated table 
addition previously reported breakeven points decision trees lewis ringuette probabilistic method lewis listed 
text treated uniformly breakeven point local dictionary frequency features 
newswire stories contain line headline provide additional clues topic 
words occurring headline additional emphasis counting twice uniform count words headline body article performance local dictionary frequency features increased percentage points breakeven point 
breakeven point summary measure text categorization recall precision may 
illustrates performance rule induction variations 
compares results previously reported results 
determine breakeven point learning experiments performed parameter varied elicit example cost false negatives set times false positives 
tradeo recall precision 
appropriate technique may vary learning method 
rule induction traditional goal minimize number errors may breakeven point 
varied cost setting swap experiment recall precision tradeo experiments reuters data breakeven point achieved near cost setting 
discussion compared previous results reuters data new results appear signi cantly better 
single breakeven measure comparison measure summarizes results dozens relatively independent experiments tens thousands test cases 
assured results highly signi cant improvement previously reported results data 
suggests local dictionaries frequency information ective improved results rule induction methods 
far greatest improvement came learning method swap 
previous experience showed optimization techniques rule induction method substantially improve results competitive methods decision trees text classi cation number characteristics optimized rule induction particularly suitable 
optimization techniques employed quite strong nding feature dependencies 
terms text classi cation means single word dictionaries nd key word combinations separate topics 
applications class label consider truth humanly assigned reader author document 
methods emphasize models compatible human reasoning distinct advantage 
know human engineered systems identical representation production rules successful text classi cation 
demonstrated rule systems text classi cation automatically generated samples comparable performance measures 
possible hope results better breakeven obtained current set experiments 
number possibilities remain explored 
obvious frequency measures measures readily proposed 
local dictionary better faster wehave examine situations universal dictionary performed consistently better 
hints case high prevalence topics 
potential improvements feature selection process universal dictionary 
relied simplest dictionaries text matching strategies 
possible sophisticated matching strategy may yield improved margin performance 
limiting factor evaluation results know true upper bound performance 
natural expectation correct performance achieved 
machine learning perspective application know performance achievable labels correct 
possible topics assigned stories quite reader assignment topics inconsistent reuters test data standard errors slightly single experiment 
combined results multiple experiments far smaller standard error 
assignment topics 
observed topic assignment mistakes reuters collection careful reading human language precise reach full agreement readers stories 
machine learning programs operate uncertain environment nd patterns separate populations degree error 
may case blinded prospective comparison independent observers automated approach versus human assigned approach demonstrate machine better 
look recall gures measure overlap consistency human indexers documents 
studies consistency human indexers context 
survey saracevic reported consistency values ranging 
studies comparing indexing inadvertently duplicated documents information science abstracts medline consistency central concepts main headings roughly analogous subject codes 
high levels precision recall gures exceed percentages 
de nitive results suggest machine learning methods may comparable human performance 
examined variety document collections including newswires ntis technical abstracts library congress card catalogs 
strong results obtained reuters newswires consistent result obtained data 
got favorable results library congress data experiments strictly done holdings ibm library system 
collection inherently skewed technical content need experiment general collection card catalog information drawn 
ntis data obtained results hold favorably expected detailed post induction analyses suggest ntis abstracts frequently prone erroneous classi cations humans apte 
experiments appears optimized rule induction competitive machine learning techniques masand lewis ringuette lewis document classi cation close human engineered systems hayes weinstein 
supported rigorous comparisons 
large volumes data proprietary nature documents surprising comparisons reported literature 
reuters stories widely circulated prove important benchmark objective comparisons 
machine induced rule models permit cient analytical investigations rule sets inspected modi ed easily human machine 
process useful attempting understand documents get misclassi ed allows experiments ne tuning induced models 
inspection detects erroneous classi cations existing document database 
example ntis document family discovered widely populated documents incorrect human assignments topics 
explosive growth electronic documents accompanied expansion availability computing 
information managed extensive assistance machine 
processes thought requiring comprehension understanding may prove machine compute intensive methods discovering classi cation collection examined human assigned topics assigned aid knowledge system 
computer evaluate thousands cases mere seconds 
serious large scale study require huge expenditure human time validate hypothesis 
patterns 
machine learning discovery systems may human developed systems document classi cation 
turn conceivably coupled knowledge lters tools newswire alerts information feeds provide superior information retrieval services user 
apte apte damerau weiss 
knowledge discovery document classi cation 
technical report rc ibm watson research center 
working notes aaai workshop knowledge discovery databases kdd pp 
fuhr lustig 
automatic indexing system air phys research application 
acm sigir pages 
breiman breiman friedman olshen stone 
classi cation regression trees 
wadsworth ca 
church hanks church hanks 
word association norms mutual information lexicography 
proceedings th annual meeting association computational linguistics pages 
clark niblett clark niblett 
cn induction algorithm 
machine learning 
flower jennings flower jennings 
domain classi cation language neural networks 
rd australian conference networks 
fuhr pfeifer fuhr pfeifer 
combining model oriented description oriented approaches probabilistic reasoning 
acm sigir pages 
fung fung crawford 
architecture probabilistic concept information retrieval 
acm sigir pages 
hayes weinstein hayes weinstein 
adding value financial news computer 
proceedings international cial intelligence applications wall street pages 
hayes hayes andersen nirenburg schmandt 
tcs shell content text categorization 
proceedings sixth ieee pages 

design analysis pattern recognition experiments 
bell system technical journal 
lewis ringuette lewis ringuette 
comparison learning algorithms text categorization 
symposium document analysis information retrieval las vegas nv april 
univ nevada las vegas 
appear 
lewis lewis 
evaluation phrasal clustered representations text categorization task 
proceedings fifteenth annual international acm sigir conference research development information retrieval pages june 
edited nicholas belkin peter ingwersen mark pejtersen 
lewis lewis 
feature selection feature extraction text categorization 
speech natural language workshop pages february 
sponsored defense advanced research projects agency 
lin kernighan lin kernighan 
cient heuristic traveling salesman problem 
operations research 
masand masand waltz 
classifying news stories memory reasoning 
proceedings fifteenth annual international acm sigir conference research development information retrieval pages june 
edited nicholas belkin peter ingwersen mark pejtersen 
michalski michalski hong lavrac 
multi purpose incremental learning system aq testing application medical domains 
proceedings aaai pages 
quinlan quinlan 
simplifying decision trees 
international journal man machine studies 
quinlan quinlan 
programs machine learning 
morgan kaufmann 
saracevic saracevic 
individual di erences organizing searching retrieving information 
jose marie gri ths editor proceedings th annual meeting society information science pages october 
sheth maes sheth maes 
evolving agents personalized information filtering 
proceedings ieee pages 
weiss indurkhya weiss indurkhya 
optimized rule induction 
ieee ex pert december 
weiss kulikowski weiss kulikowski 
computer systems learn 
morgan kaufmann 
weiss weiss galen tadepalli 
maximizing predictive value production rules 
arti cial intelligence 
documents document classification rules dictionary creation universal model induction typical machine learning organization document classi cation text representation boolean feature extraction entropy documents document classification rules dictionary creation local model induction modi ed machine learning architecture document classi cation text representation frequency boolean precision recall headline local frequency local frequency local boolean universal frequency universal boolean decision rule learning results recall precision tradeo reuters data varying text representations precision recall probabilistic bayes decision tree optimized rule induction comparison reported results reuters data best decision rule learning result 
