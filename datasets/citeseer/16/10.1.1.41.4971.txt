massachusetts institute technology artificial intelligence laboratory center biological computational learning department brain cognitive sciences memo support vector machine classification microarray data mukherjee tamayo slonim verri golub mesirov poggio publication retrieved anonymous ftp publications ai mit edu 
pathname publication ai publications xxxxx effective approach cancer classification gene expression monitoring dna microarrays introduced golub 
main problem faced accurately assigning leukemia samples class labels acute leukemia aml acute leukemia 
support vector machine svm classifier assign labels 
motivation svm dna microarray problems high dimensional training data 
type situation particularly suited svm approach 
achieve slightly better performance simple classification task golub copyright fl massachusetts institute technology report describes research done center biological computational learning department brain cognitive sciences artificial intelligence laboratory massachusetts institute technology 
research sponsored office naval research contract 

additional support provided eastman kodak daimler benz ag siemens corporate research digital equipment central research institute electric power industry 
problem cancer classification clear implications cancer treatment 
additionally advent dna microarrays introduces wealth genetic expression information diseases including cancer 
automated generic approach classification cancer diseases microarray expression important problem 
generic approach classifying types acute introduced golub 
achieved results problem classifying acute leukemia aml versus acute leukemia gene expressions 
selected genes genes dna microarray training set 
approach classification consisted summing weighted votes gene test data looking sign sum 
constructed support vector machine svm classifiers problem 
motivation svms performed wide variety classification problems including microarray data 
achieve better results golub feature selection step 
classify genes dataset contains genes including control genes 
output classical svms class designation sigma 
particular application important able reject points classifier confident 
introduced confidence interval output svm allows reject points low confidence values 
important application infer genes important classification 
appendix describe preliminary feature selection algorithm svm classifiers 
classification results data consisted training samples test samples 
sample vector corresponding genes 
element vector log normalized expression value 
means expression level gene normalized sample mean variance training set logarithm taken 
golub top informative genes selected classification problem 
generated data sets genes top genes top genes top genes criteria 
criteria 
gene look statistic fi fi fi fi gamma gamma oe oe gamma fi fi fi fi gene index mean class gene gamma mean class gamma gene oe standard deviation class gene oe gamma standard deviation class gamma gene genes ranked descending order top values correspond informative genes 
golub classified test data correctly 
remaining rejects errors 
classification rejections linear svms constructed vectors gene expressions 
svm trained points training set tested points test set 
output svm test set real number gives distance optimal hyperplane 
standard svms classification depends sign training set perfectly separable meaning accuracy classifying training data 
leave estimator training data gave accuracy 
test set performances ranged errors data sets see table 
see values test data 
genes errors table number errors classification rejections various number genes linear svm 
nonlinear svms polynomial kernels improve performance 
indicate additive linear model probability gene expressions class 
classification rejects reject points near optimal hyperplane classifier may confident class label introduced confidence levels svm output confidence levels function computed training data 
allows reject samples certain value jdj fall confidence level 
introducing confidence levels resulted accuracy cases rejects depending data set table 
plots values test data classification rejection intervals 
genes rejects errors confidence level jdj table number errors rejects confidence level jdj corresponding confidence level various number genes linear svm 
computation confidence level bayesian formulation assumption svms cjx cjd rewrite cjd cjd djc problem assume gamma dj gamma allows simply estimate jdj jf gamma 
previous assumptions estimate confidence level jdj confidence levels class class gamma 
leave estimator training data get jdj values 
estimate distribution function jdj jdj values 
done automated nonparametric density estimation algorithm free parameters 
confidence level jdj simply jdj gamma jdj plot confidence level function jdj cases 
look classes separately get confidence levels 
removal important genes higher order information examined svm performed important genes criteria removed 
examined higher order interactions helped important genes removed 
higher order statistics fact increase performance problem artificially difficult removing top features 
high order kernels hindered performance 
result consistent concepts generalization error svm algorithm 
data noisy advantage flexibility complicated model outweighs disadvantage possibility overfitting 
data noisy aspects reversed simpler model performs better 
svm performed features removed see table 
treatment success vs failure problem addressed prediction treatment failure subset aml data 
examples problem leave procedure estimate performance 
performed chance level errors points 
vs cells acute leukemia key subclasses case arose cells cells 
linear svm predict subclasses 
problem training set examples golub 
estimates classified examples rejected example 
case significant genes 
leave estimate classify examples correctly depending genes significant genes table 
results performed leave estimate cell vs cell cases 
features examples classified rejected 
top features classified examples 
linear svm classifier rejection level confidence values performs aml vs vs cell classification problems 
prediction failure vs success chemotherapy chance level 
performance achieved gene genes st order nd order rd order removed table number errors function order polynomial number important genes removed 
genes rejects errors confidence level table number errors rejects confidence level genes vs cell problem linear svm 
selection 
shown svm classifier remained accurate significant genes classifier 
fact linear svm polynomial classifier genes top genes supports assumption golub additive linearity genes classification case 
expect advantages nonlinear svm classifiers obvious difficult problems interactions genes play significant role 
classification gene selection feature selection purposes problem improve generalization performance infer genes relevant discriminating types 
preliminary formulated feature selection algorithm context svm classifier 
basic principle rescale input space margin feature space increases subject constraint volume feature space remains constant feature rescaling steps 
svm classifier form ff number support vectors kernel function 
feature selection uses iterative algorithm 
standard svm functional minimized points fx ir functional minimized respect ff gamma ff ff ff subject ff ff functional minimized respect diagonal matrix elements ff ff px subject interpreted number expected features imposes constant volume constraint 
function linear gaussian kernels due properties mapping input space feature space polynomial kernels function complicated analytic 
functional minimized gradient descent projection 
computed features corresponding top elements retained reducing problem ir ir svm classifier constructed training data ir iteratively minimize functionals select features maximize margin 
details extensions algorithm see 
applied algorithm leukemia data 
linear svm classifier achieved performance rejects test set top genes selected 
able classify cases correctly genes 
brown grundy lin ares jr haussler 
support vector machine classification microarray gene expression data 
ucsc crl department computer science university california santa cruz santa cruz ca june 
burges 
geometry invariance kernel methods 
press cambridge ma 
golub slonim tamayo mesirov loh downing bloomfield lander 
molecular classification cancer class discovery class prediction gene expression monitoring 
science 
mukherjee 
feature selection algorithm support vector machines 
technical report 
progress 
mukherjee vapnik 
multivariate density estimation svm approach 
ai memo massachusetts institute technology 
papageorgiou oren poggio 
general framework object detection 
international conference computer vision pages bombay india 
vapnik 
statistical learning theory 
wiley 
distance hyperplane examples distance hyperplane examples examples examples plots distance hyperplane test points feature vector feature vector feature vector feature vector 
class class aml mistakes line indicates decision boundary 
distance hyperplane examples distance hyperplane examples examples examples plots distance hyperplane test points feature vector feature vector feature vector feature vector 
class class aml mistakes line indicates decision boundary 
confidence confidence confidence confidence plots confidence levels function jdj estimated leave procedure training data feature vector feature vector feature vector feature vector 
plot confidence levels function estimated leave procedure training data feature vector 

