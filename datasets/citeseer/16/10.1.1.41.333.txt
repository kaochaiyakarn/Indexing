pe cep ua intelligence box martigny switzerland phone fax mail secretariat idiap ch internet www idiap ch idiap martigny suisse bayesian network theory usage todd stephenson idiap rr february molle institute perceptual arti cial intelligence idiap research report bayesian network theory usage todd stephenson february 
concepts bayesian networks help beginner familiar eld theory 
bayesian networks combination di erent mathematical areas graph theory probability theory 
rst give basic de nition bayesian networks 
followed elaboration underlying graph theory involves arrangements nodes edges graph 
bayesian networks encode beliefs system variables proceed discuss general update beliefs variables values longer unknown observed values 
learning algorithms involve combination learning probability distributions learning network topology 
conclude part showing bayesian networks various domains time series problem automatic speech recognition 
part ii give detail algorithms needed working bayesian networks 
supported swiss national science foundation 
andrew morris bengio comments 
idiap rr part theory graphical models graphical models general graphical model whittaker tool visually illustrate conditional independencies variables problem 
variables conditionally independent direct impact value 
example conditionally independent ajb ajb cowell section 
furthermore graphical model show intermediary variables separate conditionally independent variables 
intermediary variables conditionally independent variables ect 
graph composed set nodes graphical models represent variables set edges 
edge connects nodes edge optional direction assigned 
directed edges edge said parent child edge variables causal relationship variables edge directional leading cause variable ect variable just correlation variables edge undirected cowell section 
say conditionally independent variables variables directly related variable draw edge nodes variables directly related furthermore say relations equally directions edges undirected 
illustrates dependent variable edge conditionally independent variable note saying totally independent merely means variable encodes information impacts vice versa 
variable probability distribution function continuous discrete deals discrete functions de nition depends edges leading variable 
example probability distribution variable depends value variable probability distribution variable depends value variable value variable graphical model de ned follows 
consists variables nodes kg set dependencies edges variables set probability distribution functions variable 
graphical model illustrating conditional independence cowell de nition 
variables conditionally independent variable 
probabilities omitted 
idiap rr bayesian network 
probabilities omitted 
bayesian networks bayesian networks speci type graphical model bayesian network speci type graphical model directed acyclic graph dag see page neapolitan 
edges graph directed point particular direction cycles way start node travel set directed edges correct direction arrive back starting node 
illustrates bayesian network 
set edges constitutes dag undirected edges edges going directions vertices cycles leave vertex direction edges way cycle back original vertex 
furthermore conditionally independent say ajb ajb means factorization represented bayesian network probability conditioned value irrelevant local probability 
likewise say cja cjb 
edges bayesian network encode particular factorization joint distribution 
example joint distribution variables factorized bayesian network ajb 

cjb general nodes xn joint probability function bayesian network joint probability variables product probabilities variable parents values 
shows meaning directed edges indicate variables variable conditioned 
edges bayesian networks looked causal connections parent node causes ect children 
notion causality useful constructing bayesian networks note joint probability represented set edges equally represented set 
example bayes rule joint probability ajb 

cjb bja 


cjb 
bja 
cjb represented graphically 
graph theory terminology having de ned bayesian networks provide theory needed discussions bayesian networks 
bayesian networks combine probability theory graph types graphical models similar synonyms bayesian networks bayesian belief networks causal networks probabilistic independence networks probabilistic networks markov elds 
idiap rr bayesian network represents joint probability 
edge reversed 
probabilities omitted 
graph graph directionality 
graphs connections nodes graph 
graph undirected edges graph directed edges point certain directions 
theory providing basics graph theory 
information please see neapolitan chap 
provides background graph theory bayesian networks perspective 
section de nes graph set nodes called vertices set edges called arcs edge pair nodes 
called undirected graph 
vertices edge ordered edges direction assigned lead speci cally node node direction called digraph directed graph 
presents multigraphs general digraphs allow multiple connections nodes graphs digraphs respectively 
properties graph deal arrangement edges appear nodes graph digraph 
chains paths 
vertex sequence chain length path 
vertex sequence path length 
vertex sequence simple path length 
vertex sequence cycle length 
vertex sequence simple cycle length 
idiap rr dags parents children ancestors descendants family 
graph dag 
parent child parent children 
ancestor descendants 
possible ancestral ordering graph acceptable ancestral ordering sample families graph family family family chain series nodes successive node chain connected previous node edge regardless direction edge 
path chain constraint digraphs connecting edge chain directionality going direction chain 
cycle path starts ends node 
simple path path unique nodes 
simple cycle cycle start node nodes unique see 
directed acyclic graph dag directed graph cycles see 
parent child relationship directed graph occurs edge called parent called child words edge points parent child 
ancestor descendant relationship furthermore extension parent child relationship 
example parent parent ancestor descendant human relationship extends just sets parent child relationships 
ancestral ordering ordering nodes ancestor comes respective descendants 
possible dags 
see 
family set vertices composed parents example vertices fc family vertex terms parent child de ne relationship vertices connected directed edge term adjacent neighbor describes relationship vertices connected undirected edge nodes said adjacent 
forest dag node parent 
tree forest node called root parent words node root exactly parent 
see figures 
noted books graphical modeling de ne trees cowell neapolitan de ne tree connected undirected acyclic graph bo ey 
brings term directed tree versus regular trees 
am concerned directed trees directed de nition cowell 
neapolitan 
moral graph dag 
dag marry parents node means add undirected edge parent 
doing remove directionality original edges resulting undirected graph 
see 
path cycle chord edge appear path idiap rr forest 
graph consisting nodes forest 
node parent 
parent parents 
graph tree meet requirement nodes having parent 
trees 
graph tree 
dag node exactly parent root node parent 
nodes occur path 
term describes simple path simple cycle chords exist 
example consider cycle graph 
chord cycle edge 
cycle edges non adjacent nodes cycle 
term triangulated describes undirected graph simple cycle nodes chord 
note possible chords cycle nodes 
see 
term complete describes undirected graph node connected nodes 
clique subset nodes complete larger complete 
example say subset nodes complete form clique means node added subset complete 
whittaker section speci es clique maximal strictly enforced 
example golumbic section distinction clique maximal clique 
see 
join tree dag moralized triangulated cliques nodes tree 
see page 
needs characteristic known running intersection property means vertex vertex set cliques tree cliques chain idiap rr moral triangulated graphs 
graph moralized form graph page 
node parents graph graph parents connected 
directions removed edges 
happens triangulated graph simple cycle length chord 
cliques 
subgraphs maximal cliques graph 
cliques complete subgraphs fully connected 
inference bayesian networks inference task computing probability value node bayesian network variables values known 
di erent algorithms doing inference bayesian network looking called junction tree algorithm works arbitrary bayesian network 
describing generic algorithm arbitrary bns give simple inference involves simple bayesian network 
followed belief propagation serves foundation junction tree algorithm 
inference heckerman gives example bayesian network models credit card fraud illustrated 
bn see example case credit card fraud chances gas jewelry bought ected chance jewelry bought ected age sex purchaser 
table indicates probabilities associated idiap rr join tree 
join tree cliques 
nodes tree 
variables bayesian network 
example see es probability gas es smaller probability gas es 
person credit card times buy gas legitimately credit card 
di erent probabilities commonly referred beliefs heckerman section :10.1.1.112.8434
discussion heckerman inference bayesian network proceeds follows 
suppose notice certain value variables network 
variable de nite observed value beliefs probabilities variables need revised 
inference determining updated posterior probability distribution variable known values variables 
prior probability fraud fraud 
notice young man credit card buy jewelry gas 
sex male age jewelry es gas table 
want infer card 
words need calculate jj letter rst letter respective variable 
proceed follows bayes rule jj states mutually exclusive exhaustive transform denominator idiap rr age sex fraud jewelry gas bayesian network heckerman illustrating credit card fraud 
variables network 
probability jewelry variable dependent values fraud age sex variables 
likewise value gas variable dependent value fraud variable 
fraud age sex variables conditioned variable 
see table local probability distributions variable 
probability conditions fraud age sex fraud fraud age age age sex male sex female gas gas jewelry jewelry male male female female table probabilities variables variable jewelry gas sex age value es male table example set unknown observations credit card fraud network 
values observed variable 
observed values give updated probability belief es 
idiap rr get jj product rule probability numerator denominator equation factored follows jj jjg gjs jjg gjs due conditional independencies remove certain variables conditional lists see section 
variable child xn graph jx xn jx xn xn simpli cation provides jj jjs gjf jjs gjf point simplify equation cancelling common factors numerator denominator 
doing indicates prior probabilities age sex direct impact probability fraud computed 
jj jjs gjf jjs gjf computed inserting values variables reading probabilities table 
es male male es es es male substituting actual probabilities get es male prior probability fraud inferred probability fraud variables times probable 
belief propagation belief propagation pearl action updating beliefs variable observations variables 
done arbitrary bayesian network illustrate simpler case bayesian network structured tree 
section page give detailed algorithm belief propagation general bayesian network 
idiap rr pearl method pearl section belief propagation proceeds follows 
set values observed variables 
variable split subsets represents observed variables descendants including observed represents observed variables 
impact observed variables beliefs represented values jx je multiple discrete values vectors elements associated discrete values get posterior probability je 

pairwise multiplication items 
equation want compute nd new beliefs 
values passed variables orderly fashion 
computing computed ym ym children observed elements vector assigned follows case observed expanded jx ym jx jx 
jx 
ym jx 

ym fact ym conditionally independent de ning jx computation proceeds jx jx jy 
jx jy 
jx 
jx idiap rr shows compute value need children conditional probabilities children 
means compute variable need rst compute children 
compact form vector computed children 
computing furthermore computed parent je je jy 
je jy 
je jy 
shows compute value need parent conditional probabilities means compute variable need rst compute parent 
peot shachter method peot shachter provides similiar propagation algorithm pearl 
notable di erence pearl de nition 
peot shachter de nes follows jx di ers joint probability conditional probability je 
contrast pearl algorithm provides 
details method see peot shachter 
junction tree cowell gives general method inference bayesian networks junction tree algorithm 
potential problems doing inference dag cycles removed see pearl section algorithm useful 
dag transformed tree nodes cliques cf 
page 
node graph may occur multiple cliques tree 
leads key property tree node subset nodes graph appears di erent cliques appear cliques path connects gives tree name junction tree cowell junction tree algorithm proceeds follows cowell idiap rr structure observability method known full sample statistics known partial em gradient ascent unknown full search model space unknown partial structural em table learning methods depending known problem 
taken murphy mian 

bayesian network 

triangulate moralized graph 

cliques triangulated graph nodes tree desired junction tree 

propagate values junction tree inference 
provide posterior probabilities unobserved variables 
see section details constructing junction tree section details performing propagation junction tree 
learning bayesian networks point assumed bayesian network learned 
explaining inference section showed inference bayesian network topology prior probability distributions known 
section go learning topology probability distributions bayesian network 
depending problem de ned may pre de ned hand may learned 
table breaks di erent possibilities learned methods utilized 
heckerman provides issues involved learning bayesian network 
known structure full observability likelihood method seen table know structure bayesian network full sampling data determine probability distributions computing statistics data samples 
means want compute probability say jx collect data various possible values data estimate actual probability 
jx number samples number samples note junction tree multiple dimensions dimension representing di erent variable clique associated 
multi dimensional array element possible instantiation component variables respective clique 
idiap rr bayesian method data sparse dirichlet priors murphy mian 
heckerman goes complete bayesian solution 
bayesian method local probability distribution variable calculated parameters probability distribution topology bayesian network 
variable viewed dimensional matrix dimension possible instantiation dimension possible instantiation parents 
probability parameter bayesian network identi ed ijk ranges variables ranges possible parent instantiations variable ranges instantiations 
words ijk goal training maximize posterior distribution jd factored follows assuming parameter independence jd jxj ij jd factors represented dirichlet distributions ij jd dir ij ij ij ij ij ijk represents number times case parents appears ijk represents prior beliefs case parents occur having seen heckerman goes show obtain xjd jxj ijk ijk ij ij results update formula probability distribution parameters ijk ijk ijk ij ij known structure partial observability shown table page know structure observed data expectation maximization em gradient ascent learn parameters 
exact bayesian methods intractable scenario approximations bayesian methods 
likelihood method likelihood method learning probabilities bayesian networks partial observability expectation maximization em algorithm lauritzen generalization algorithm baum 
zweig discusses em discrete bayesian networks 
idiap rr expectation extending page cliques probability clique clique having instantiation observations clique ia je 
ia 
ia ranges possible instantiations clique means converting bayesian network junction tree running inference algorithm compute probabilities clique observations 
want list counts ijk variable instantiation parents having instantiation determine value ijk rst identify clique occurs parents 
parents need clique computed clique probabilities 
jk set clique instantiations parents ijk ijk jk clique variables clique part family 
values ijk accumulated training sample bayesian network 
maximization ijk values computed re estimate local conditional probability distributions variable 
ijk ijk ijk ij bayesian methods exact methods computing probability distribution bayesian methods intractable number parameters increases exponentially amount missing data cowell 
alternative maximum likelihood prior counts 
section heckerman section shows prior counts ijk incorporated learning partial observability giving revised expectation maximization algorithm 
revised em algorithm section maximization step revised ijk ijk ij ij unknown structure full observability heckerman continues discussing learning bayesian networks methods learning structure complete training data 
things needed task metric comparing potential structures search algorithm nding potential structures list potential structures determined edges possible set nodes size list grows exponentially number nodes 
search algorithm nd candidates 
idiap rr structure comparison metric method evaluating potential structure compute joint probability data structure factoring log compute log log djs log breaks task computing posterior probability likelihood prior probability structure data 
method computing log djs bayesian information criterion bic schwarz log djs log dj log network parameters dimensions log dj 
js taken size heckerman relates bic negative minimum description length mdl rissanen 
regarding determine prior probability di erent methods heckerman :10.1.1.112.8434
expert person provide set possible structures assign equal prior assigning structures set prior 
possibility set prior network 
prior probability network decreased depending deviation potential network de ned prior network deviation calculated pre determined metric 
structure search methods assuming data observed know nodes graph 
just nd appropriate connections variables 
doing possibilities np hard heckerman :10.1.1.112.8434
di erent options available doing selective search possibilities order nd model 
possibility greedy search 
greedy search step list possible changes graph 
changes type adding edge direction graph reversing direction edge graph removing edge graph 
list possible changes restricted keep graph dag see page de nition dag 
step chose change increases log second element 
continue iterating change increase probability 
iteration provide potential structure previously chosen previously examined structures lower value log current 
see heckerman details search algorithms 
unknown structure partial observability nal learning scenario table structure data fully known 
friedman explains method learning network structure data model selection expectation maximization ms em algorithm 
friedman algorithm called structural em algorithm sem 
murphy mian give summary structural em algorithm follows 
describe learning structure variables combination methods section adding new variables nodes 
iterate follows add new node network representing hidden variable set nodes nd best get network connections continue long network keeps improving 
idiap rr possible prior network dynamic bayesian network static variables 
probabilities omitted 
possible transition network dynamic bayesian network static variables 
probabilities omitted 
dynamic bayesian networks previous discussions bayesian networks relation time 
variables network single point time 
domain interest deals variables bayesian network change time dynamic bayesian network dbn addresses 
page illustrates static bayesian network looks 
variables time referred static bayesian network 
dynamic bayesian network needs de ned prior network transition network friedman 
indicates prior network 
represents prior probabilities variables network initial time slice 
illustrates transition network dynamic bayesian network 
transition network illustrates time slices probabilities variable conditioned variables possibly previous time slice gives edges lead variable 
dynamic bayesian network set variables probability de nitions time slice zweig section exception prior network initial idiap rr dynamic bayesian network time slices 
connections time prior network connections times transition network 
probabilities omitted 
time slice having probability distributions 
prior transition networks construct dynamic bayesian network length follows 
rst supply set unconnected variables time slice 
time connections nodes local probability functions prior network 
time connections connections local probability functions speci ed transition network 
con guration assuming rst order markov network zweig 
page illustrates dynamic bayesian network time slices built prior transition networks figures 
bayesian networks automatic speech recognition area application concerns speech recognition 
application dynamic bayesian networks automatic speech recognition asr investigated zweig 
advantage dbns asr advantages pointed zweig russell dbns exploiting conditional independencies allowed bayesian networks streamline factorization joint probability reduce number parameters generality inference learning algorithms allows researcher easily explore di erent topologies unmodi ed bayesian network program tying variable time slices leads better transitional modeling things coarticulation speech 
topology dynamic bayesian network asr zweig chapter zweig russell basic topology dbn analogous topology standard hidden markov model hmm speech recognizer rabiner juang variables see position variable variable holds part model st phoneme nd phoneme associated time slice 
phone variable variable gives phone associated current position 
idiap rr ih ih jh ih position phone transition acoustics basic variables dynamic bayesian network speech recognition zweig section zweig russell illustrate word model digit utterance covers time frames 
time slice position variable indicates part word time slice modeling determines value phone 
transition variable indicates time slice position word 
acoustics variables observed shaded indicate 
nal position transition variables treated observed true respectively assumption reach nal time frame nal position word transition nal phone example implicitly observed depends value position 
transition variable variable indicates time frame phone 
current phone variable gives hmm equivalent transition probabilities 
acoustic variable variable holds acoustic feature typically observed speech recognition applications 
current phone model gives hmm equivalent emission probabilities 
examples bayesian networks areas applications bayesian networks speech recognition 
include computer troubleshooting medical diagnosis 
computer troubleshooting best known application bayesian networks actual production printer problem microsoft windows heckerman 
page gives bayesian network taken microsoft belief networks program network help amateur computer user determine source problem trouble www research microsoft com default htm idiap rr printer problem microsoft windows taken microsoft belief networks program 
probabilities omitted 
idiap rr printing 
interface bayesian network enables user indicate observes problem 
certain variables observed tool points user probable cause 
microsoft continuing deploy applications bayesian networks troubleshooting medical diagnosis observed symptoms bayesian network medical diagnosis give probability di erent diseases causing symptoms network diseases causes symptoms ects 
certain symptoms observed infer probabilities di erent causes 
example network structure probabilities necessarily built training methods expert medical doctor gives belief regarding di erent probabilities relationships relevant network 
see og gives detailed explanation sample medical system bayesian networks 
see example support microsoft com support default asp idiap rr part ii algorithms constructing junction tree bayesian network de ned page bayesian network concerned parents child 
procedure follows 
child initial graph graph add undirected edges parents initial graph 
parent node directly connected parent node jensen 
original edges entire graph undirected 
undirected graph 
triangulating graph triangulation process making graph triangulated de ned page 
rst need number vertices graph 
arbitrary numbering nodes 
resultant ordering triangulated graph 
algorithm taken pearl originally tarjan yannakakis numbering construct triangulated graph see 
proceeding node decreasing node 
determine lower numbered nodes adjacent current node including may adjacent node earlier algorithm 

connect nodes 
choosing arbitrary numbering nodes order triangulation maximum cardinality search neapolitan see 
give node value 
subsequent number pick node neighbors numbered nodes tie pick nodes tie 
dag idiap rr moralized version dag vertices numbered maximum cardinality search graph moralized triangulated graph advantage maximum cardinality search test determine graph triangulated 
numbering maximum cardinality search triangulation add edge graph triangulated 
constructing junction tree having triangulated graph move constructing junction tree 
preliminary step need algorithm producing perfect vertex elimination scheme triangulated graph 
perfect vertex elimination scheme vertex subset fv jj ig complete de ned page golumbic section 
means lower numbered neighbors vertex form complete subgraph numbering triangulation step meets criterion 
de nition golumbic modi ed conform conventions neapolitan uses numbers nodes opposed 
algorithm producing junction tree golumbic section 
produces list maximal cliques 
idiap rr cliques 
note common variables occur multiple cliques 
graph ordering vertices vn perfect elimination order initialize cliques decreasing vertex neighbors neighbors occur earlier earlier occurring neighbors occurring element greater jx jx cliques cliques greater jx break loop cliques cliques return cliques having list cliques graph construct junction tree pearl section 
sort clique list highest numbered node maximum cardinality ordering clique 

connect clique previous clique sorted list shares number nodes 
inferring junction tree sections mentioned di erent approaches belief propagation 
pages give details perform belief propagation approach idiap rr cliques formed clique tree 
note variable tree cliques occurs connected running intersection property 
clique tree chain nodes tree general allowed child 
idiap rr pearl 
details method nd details peot shachter zweig applies method automatic speech recognition 
clique potential having set structure junction tree determine potential cliques neapolitan 
clique set potentials represents cross product possible values nodes 
speci cally clique composed nodes furthermore nodes takes values 
di erent combinations variable instantiations variables clique represent values clique ij represent th possible variable instantiation combinations clique vertex assign vertex single clique occurs parents number cliques number possible variable instantiation combinations clique initialize ij clique potential combination 
vertex assigned clique ij ij vertex lj vertex variable may occur multiple cliques probability distribution initially included calculation clique potential 
inferring junction tree inferring done copy junction tree passing values cliques tree 
similar hold values possible joint variable instantiation combinations member variables 
clique generates sends parent clique root sends 
furthermore clique generates di erent children cliques children generate 
member variables intersection variables sending clique variables receiving clique 
intersection scalar vector 
note generated root clique scalar clique pass giving intersection variables 
modi ed version neapolitan section 
clari cations meanings di erent steps algorithm partially worked example please look 
de nitions clique parent clique clique main clique observed variable subset clique clique clique idiap rr leaf clique prop vertex clique receives child clique instantiation variables pj pj ck instantiation variables clique root instantiations clique variables exit joint probability prop vertex clique receives parent clique instantiation variables cj cj 
pk instantiation variables clique leaf prop calculated variable clique determine clique instantiations clique variables prop cl cl single value instantiation variables ck cl ck instantiation variables cl cl ck clique root pass clique parent clique prop child clique clique single value clique clique instantiation variables ck sc clique pass clique idiap rr concepts surrounding discrete bayesian networks 
identi ed concepts graph theory serve basis algorithms bayesian networks 
algorithms allow calculate updated posterior probabilities observed data 
learning algorithms bayesian networks introduced involve learning local probability distributions possibly topology network 
furthermore concepts bayesian networks extended handle time series problems including relating speech recognition 
gave detail important algorithms working bayesian networks 
baum 

equality associated maximization technique statistical estimation probabilistic functions markov processes 
inequalities 
bo ey 

graph theory operations research 
macmillan computer science series 
macmillan press 

introductory combinatorics elsevier science publishing new york second edition 
cowell 

inference bayesian networks 
jordan pages 
cowell dawid lauritzen spiegelhalter 

probabilistic networks expert systems statistics engineering information science 
springer verlag new york friedman 

learning belief networks presence missing values hidden variables 
douglas fisher editor machine learning proceedings fourteenth international conference icml pages nashville tn 
morgan kaufmann publishers san francisco 
friedman 

bayesian structural em algorithm 
fourteenth conference uncertainty arti cial intelligence uai 
friedman murphy russell 

learning structure dynamic probabilistic networks 
uai golumbic 

algorithmic graph theory perfect graphs academic press new york 
heckerman 

tutorial learning bayesian networks 
jordan pages 
heckerman breese 

decision theoretic troubleshooting 
communications acm 
jensen 

bayesian networks ucl press london 
jordan editor 
learning graphical models adaptive computation machine learning 
mit press cambridge massachusetts rst mit press edition 
lauritzen 

em algorithm graphical association models missing data 
computational statistics data analysis 
murphy mian 

modelling gene expression data dynamic bayesian networks 
technical report computer science division university california berkeley ca 
idiap rr neapolitan 

probabilistic reasoning expert systems theory algorithms wiley interscience publication 
john wiley sons new york 
og 

assessing penetrating injury abductive geometric reasoning ph thesis university pennsylvania philadelphia 
pearl 

probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann publishers san francisco california revised second printing edition 
peot shachter 

fusion propagation multiple observations belief networks 
arti cial intelligence 
rabiner juang 

fundamentals speech recognition 
ptr prentice hall englewood cli nj 
rissanen 

stochastic complexity discussion 
journal royal statistical society series 
schwarz 

estimating dimension model 
annals statistics 
tarjan yannakakis 

simple linear time algorithms test graphs test acyclicity hypergraphs selectively reduce acyclic hypergraphs 
siam computing 
whittaker 

graphical models applied multivariate statistics john wiley sons chichester uk 
zweig russell 

compositional modeling dpns 
technical report ucb csd computer science division eecs university california berkeley berkeley ca 
zweig 

speech recognition dynamic bayesian networks ph thesis university california berkeley 
