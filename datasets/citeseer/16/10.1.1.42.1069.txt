quality service packet networks basic mechanisms directions gu erin peris review basic mechanisms packet networks support quality service qos guarantees 
outline various approaches proposed discuss trade offs involve 
specifically starts introducing different scheduling buffer management mechanisms provide service differentiation packet networks 
aim provide exhaustive review existing mechanisms give reader perspective range options available associated trade performance functionality complexity 
followed discussion mechanisms provide specific performance guarantees 
emphasis second part need adapting mechanisms different environments deployed 
particular fine grain buffer management scheduling mechanisms may necessary cost effective high speed backbones aggregate solutions appropriate 
discusses issues possible approaches allow coexistence different mechanisms delivering guarantees 
keywords qos mechanisms real time traffic scheduling buffer management aggregation internet continuously rapidly evolving entity increasingly large number applications diverse requirements 
ip telephony new applications driving need substantial changes current internet infrastructure 
specifically emergence applications different throughput loss delay requirements calling network capable supporting different levels services opposed single best effort level service rule today internet 
providing different levels service network introduces number new requirements classified major axes control path data path 
new data path mechanisms means different services enforced 
responsible classifying mapping user packets intended service class controlling amount network resources service class consume 
new control path mechanisms needed allow users network agree service definitions identify users entitled service network appropriately allocate resources service 
data path mechanisms basic building blocks gu erin peris ibm watson research center box yorktown heights ny usa 
network qos built 
implement actions network needs able take user packets order enforce different levels service 
result goal provide broad overview generic data path approaches developed control access resources packet networks 
resources networks need manage order support service differentiation primarily include buffers bandwidth 
corresponding mechanisms consist buffer management schemes scheduling algorithms respectively 
buffer management schemes decide packets stored wait transmission scheduling mechanisms control actual transmissions packets 
obviously related scheduling frequent packet transmissions flow allocating bandwidth help reduce need buffers conversely limiting amount buffer space flow impacts amount bandwidth able consume 
attempt exhaustive review possible scheduling buffer management schemes 
aim identify basic approaches proposed classify design trade represent 
particular important understand different schemes fare terms fairness access excess capacity isolation protection excess traffic users efficiency number flows accommodated level service complexity terms implementation control overhead 
example certain buffer management schemes maintain flow buffer counts information determine new packet accommodated 
alternatively schemes base decisions global information buffer content thresholds service type indicators packet headers 
finer granularity flow information benefits terms fairness efficiency comes cost greater complexity 
scheduling algorithms face similar trade offs 
example schedulers weighted fair queueing algorithm give rate delay guarantees individual flows class mechanisms cbq provide aggregated service guarantees set flows mapped class 
trade fairness efficiency complexity 
sections illustrate selected set scheduling buffer management schemes alternatives available network designers control user packets access network resources 
mentioned emphasis just reviewing basic methods developed highlighting trade offs 
trade offs data path mechanisms similar issues exist control path 
applies type services available requested 
example services vary greatly type guarantees provide 
service guarantees quantitative range simple throughput guarantees hard bounds delay lossless transmissions 
alternatively proposed service guarantees qualitative nature stipulate better treatment packets ll go higher priority queue experience lower delay lower discard threshold case congestion 
similarly requests service dynamic extend way applications rsvp protocol may handled static configuration information defines bandwidth provisioning specific subnets 
general control path trade offs dimensions processing required support service amount information needs maintained 
example complexity call admission decision determine new request accommodated depends service definition maps underlying scheduling buffer management mechanisms 
similarly service guaranteed service involves number parameters compute delay bound buffer size service mandates 
examples illustrating issues associated trade briefly reviewed section describes contrasts services currently standardized provide qos ip networks 
general trade desirable feasible function scalability requirements environment total amount information processing accommodated 
decision involves data path control path 
example flow scheduling buffer management state maintenance processing may appropriate local campus create scalability problems core backbone number individual flows orders magnitude larger 
environment scalability prime concern flow service mechanisms remain desirable approaches rely service aggregation may needed 
review issues section highlight approaches proposed far point challenges faced 
particular discuss need interoperability fine grain coarse grain solutions outline possible solutions 
scheduling mechanisms nature scheduling mechanism employed network links greatly impacts qos guarantees provided network 
basic function scheduler arbitrate packets ready transmission link 
algorithm scheduling packets traffic characteristics flows multiplexed link certain performance measures computed 
network provide qos guarantees 
describe user traffic characterized ingress network 
traffic specification atm ip networks similar characterization user traffic 
describe concentrate ip networks specification user traffic currently done tspec consists parameters token bucket plus peak rate minimum unit maximum datagram size 
token bucket bucket depth bucket rate token bucket peak rate maximum datagram size define conformance test identifies user packets eligible service guarantees 
conformance test defines maximum amount traffic user inject network expect receive service guarantees contracted 
maximum amount traffic expressed terms traffic envelope specifies upper bound amount traffic generated time interval duration min pt rt equation simply states user send bytes data full peak rate lower rate presence factor term packet nature transmissions maximum size packet worth data sent link speed 
model essentially traditional leaky bucket lets users specify long term transmission rate preserving ability burst higher speed limited periods time gamma 
full tspec includes minimum unit counts packet size size allows accounting packet processing transmission overhead time needed schedule packet transmissions 
scheduling policies turn attention different scheduling policies select packets transmission link 
aspects considered choosing scheduling algorithm 
main ones ffl nature performance measures guaranteed scheduling algorithm 
instance enable network provide flow rate guarantee rates units bytes sec packet sizes bucket depth bytes 
provide simple priority structure class traffic receives priority class 
ffl efficiency algorithm enforcing service guarantees 
basically calls characteristics packed link 
measured terms call admission schedulability region algorithm 
ffl complexity algorithm 
high link speeds today higher ones waiting wings amount time available schedule single packet continuously decreasing 
important algorithm require small number operations packet 
ffl basic mechanism parameters algorithm scheduling decisions 
specific interest algorithm supports separate delay rate guarantees combines :10.1.1.1.5992:10.1.1.38.5534
separate rate delay guarantees allow greater flexibility 
ffl flexibility algorithm handling traffic excess amount service guarantees requested 
algorithms easily handle excess traffic require additional mechanisms 
flexibility algorithm handling excess traffic described terms fairness index attempts measure equally algorithm redistribute available resources flows see discussions issue :10.1.1.40.9809
try cover aspects review different scheduling policies 
come served simplest scheduling policies operation name suggests packets served order received 
may fairly poor way providing differentiated service quite simple implement 
particular insertion deletion queue waiting packets constant time operation require flow state maintained 
surprisingly come served fcfs policy commonly implemented scheduling policies 
fcfs policy lend readily providing delay rate guarantees 
way provide delay bound limit size queue waiting packets 
way packet queued transmission guaranteed sent time takes serve full queue packets 
packets arrive queue full dropped 
ensure drop probability certain threshold limited number flows accepted 
simple mechanism performing decision compute effective bandwidth quantifies amount link capacity required flow 
fcfs policy sophisticated operation explicitly provide mechanism fair sharing link resources 
help buffer management mechanisms possible control sharing bandwidth describe mechanisms section 
fixed priority scheduler policy offers ability provide different grades service coarse granularity 
traffic classified belonging fixed number static priorities 
link multiplexer maintains separate queue priority serves packet lower priority queue higher priority queues empty 
queue served fcfs manner scheduling mechanism simple fcfs policy added complexity having maintain queues 
selecting packet transmission fixed cost operation depends number priority levels independent number flows multiplexed 
priority scheduler offer certain amount service differentiation capability readily allow performance guarantees provided flow basis 
provides class traffic receive better service class traffic single link 
fcfs provision toend delay guarantees fixed priority scheduler achieved limiting buffer sizes 
important difference accounted fact lower priority packet served packets higher priority queues transmitted 
bound affect variability delay observed lower priority packets 
general fcfs fixed priority schedulers ideal candidates comes providing guaranteed delay bounds 
problem priority scheduler different switches may different levels priority matching levels perspective easy feat 
section touch aspects priority levels context type service tos bit semantics currently discussed ietf 
fair queueing schedulers weighted fair queueing wfq service discipline variants popular current decade 
part popularity stems fact overcomes limitations fcfs priority schedulers allowing fine grain control service received individual flows 
allows network provide delay guarantees flow basis 
addition wfq serves excess traffic fair manner fairness measured relative amount resources reserved flow 
variants wfq discipline compared generalized processor sharing gps scheduler theoretical construct form processor sharing 
operation gps scheduler elegantly defined fluid model traffic tight delay bounds computed flow traverses multiple links served gps schedulers 
basic idea gps follows weight oe associated flow link capacity shared active flows direct proportion weights 
words fl denote link speed flow guaranteed obtain minimum service rate oe oe fl time quite flows backlog traffic waiting transmitted link 
translate unutilized link capacity shared back logged active flows 
gps scheduler shares excess capacity back logged flows proportion respective weights 
denotes set flows back logged time flow guaranteed receive minimum service rate oe oe fl reality don fluid flows main gps model 
simulating model possible define service disciplines closely follow gps policy 
precisely way packetized generalized processor sharing pgps referred weighted fair queueing wfq defined 
packet arrival real system modeled arrival certain volume fluid system 
packet considered transmitted system fluid volume corresponding packet completely served gps scheduler 
pgps policy simulates gps scheduler fluid arrival process described computes departure times packets system 
pgps scheduler selects transmission packet departed earliest gps system 
reasons popularity gps policy fair handling excess traffic 
flows backlogged interval time receive service direct proportion weights regardless amount excess traffic may generating 
denote amount flow traffic served interval gps policy ensures flows back logged interval relation holds oe oe relation measures quantify fairness approximations gps 
bound oe gamma oe flows back logged interval compare relative fairness different scheduling policies 
gps scheduler fairness measure 
schedulers gps implemented notion virtual time function 
virtual time relevant busy period defined maximal interval time server idle 
gps virtual time start busy period defined 
busy period advances rate fl oe virtual time define packet flow say kth packet flow virtual start time virtual finish time correspond start finish times respectively gps system 
denote arrival time length relations max phi gamma psi oe previously mentioned simulation gps pgps relies notion virtual time 
implemented follows scheduler picks packet smallest virtual finish time transmission link ties arbitrarily broken 
efficiently performed priority queue finish times packets 
complexity insertion deletion operations queue log 
note additional complexity computing advancement virtual time remain entire busy period 
worst case virtual time computation shown 
advantage pgps policy endto delay bound computed weight assigned flow 
fl denote speed links traversed flow simplicity assume traffic envelope flow minimum rate guaranteed flow links traverses 
assuming stability condition holds delay guarantee flow gamma max fl denotes maximum packet length flow max denotes maximum packet length link notice term appears regardless number hops traversed 
reason bottleneck link encountered pgps scheduler effectively smooths burst flow burst delay longer encountered downstream 
notice delay bound independent number connections multiplexed links traversed flow property pgps policy best terms providing tight delay guarantees 
variants fair queueing main drawbacks pgps complexity involved computing virtual time function 
scheduling policies simpler computations virtual time proposed 
policies guarantee delay bounds form similar equation briefly highlight 
self clocked fair queueing proposed simpler alternative pgps 
gps model computation virtual time evolution virtual start time packet currently service 
greatly reduced amount computation needed keep track virtual time 
reduction complexity results larger endto delay bound pgps 
roughly adds delay term form fl links traversed 
fact delay depends number flows multiplexed link main drawback policy 
start time fair queueing proposed fair queueing policy 
similar main difference scheduler picks packet smallest virtual start time opposed smallest virtual finish time transmission link 
delay close slightly smaller 
pgps tracks gps policy uses finish times packets system select packet transmission 
packets started service system may selected service simply finish times earlier packets waiting transmitted 
result significant difference packet departures pgps system packets departing earlier pgps system 
discrepancy measured worst case fairness index defined 
worst case fair weighted fair queueing wf scheduler proposed uses start finish times packets gps system achieve accurate emulation gps 
wf policy selects packet smallest virtual finish time system provided virtual start time current time 
prevents running far ahead system 
delay bound wf scheduler identical pgps 
scheduling policies seek emulate fair queueing 
interested reader referred description deficit round robin extends weighted round robin account variable sized packets 
frame fair queueing scheduling policy provides delay guarantees pgps simpler implement 
earliest deadline earliest deadline edf scheduler form dynamic priority scheduler priorities packet assigned arrives 
specifically deadline assigned packet sum arrival time delay guarantee associated flow packet belongs 
edf scheduler selects packet smallest deadline transmission link name 
dynamic nature priority edf scheduler evident fact priority packet increases amount time spends system 
ensures packets loose delay requirements obtain better service static priority scheduler sacrificing tight delay guarantees may provided flows 
known packet arrival process deadline associated packet edf policy optimal terms minimizing maximum lateness packets 
lateness defined difference deadline packet time transmitted link 
mentioned earlier gps policy guarantees delay bound flow basis weight assigned flow 
weights closely coupled reserved rate flow receive small toend delay guarantee necessary allocated relatively large rate 
lead inefficient resources particularly low bandwidth flow requires tight delay guarantees 
main attractions edf policy allows separation delay throughput guarantees flow 
terms implementation edf policy complex fcfs static priority scheduler 
complexity arises scheduler pick packet smallest deadline transmission link 
involves keeping priority list deadlines insertion deletion list complexity log operations corresponds number packets awaiting transmission 
obvious optimization keep single packet flows priority list deadlines packets belonging flow leave order arrive 
result complexity reduced log number flows multiplexed link 
assuming flows characterized traffic envelope denoted known edf scheduler provide delay guarantee flow provided feasibility condition satisfied gamma lmax fl lmax denotes maximum transmission unit link 
rate controlled service discipline edf policy provide efficient delay guarantees 
equation provides feasibility check delay guarantees single link 
compute feasibility check link downstream need know traffic characterization link 
act scheduling perturbs flow traffic characterization flow remain traverses network 
possible compute bound traffic characterization flow link output compute feasible delay guarantee flow downstream link 
approach result efficient delay guarantees bound traffic characterization output link pessimistic 
alternatively reshape traffic node pre specified envelope eligible scheduling 
coupled traffic shapers edf policy provide efficient toend delay guarantees flow basis 
refer combination rate controlled service rcs discipline briefly discuss implications rcs discipline 
basic idea rcs discipline reshape traffic flow link traverses 
flow need reshaped original specification fact turns particularly bad idea 
appropriate choice shown rcs discipline efficient terms guaranteeing delays scheduling policies known today 
shown appropriate choice shaper envelopes rcs discipline provide delay guarantees pgps addition accept flows 
mentioned section efficiency scheduling algorithms measured terms number flows support delay guarantees 
corresponds notion schedulable region shown rcs discipline largest schedulable region scheduling policies known today 
example illustrating schedulable region different scheduling policies section 
hierarchical link sharing discussed section network may offer different services single link 
link partitioned support different service classes 
alternatively single link network may shared different organizations departments organization 
may want receive guaranteed portion link capacity willing allow departments organizations borrow unutilized link resources 
hierarchical structure organizations suggests hierarchical partitioning link resources 
hierarchical link sharing structure consisting classes correspond aggregation traffic suggested referred class queueing cbq 
class associated link sharing bandwidth goals cbq roughly guarantee bandwidth traffic belonging class 
excess bandwidth shared fair manner classes 
requirement scheduling policy levels link sharing hierarchy conceivable classes carrying interactive traffic may benefit simple priority scheduling policy opposed rate schedulers see details 
said gps policy hierarchical manner provide link sharing individual qos guarantees flows 
top level hierarchy weights reflect link sharing requirements lower level weights provide individual qos guarantees 
interior node receives service distributed child nodes direct proportion weights 
note may levels hierarchy 
leaf nodes nodes receive logical service 
interested reader referred detailed description hierarchical packet fair queueing algorithms 
buffer management scheduling policy play big role qos provided network effective sufficient buffers hold incoming packets 
link speeds currently order second amount memory required buffer traffic transient periods congestion large exceed amount memory routers switches 
packets arrive transient periods dropped 
transport protocol reliable tcp lost packet causes retransmission reduction size congestion window 
net result lower qos flow concerned 
order avoid haphazard behavior link experiences congestion different buffer management schemes proposed 
roughly classified dimensions 
packet discard decisions 
typically packet discard decisions arrival new packet onset congestion currently stored packets may discarded accommodate new higher priority packet 

information packet discard decisions main aspect granularity information flow buffer accounting done discard packets individual flows global class information kept 
different designs correspond different trade offs performance complexity dimensions performance buffer management scheme measured terms ability control traffic fairly efficiently periods congestion 
efficient treat ment means buffer management mechanism avoid violation service agreement losing high priority conformant packets periods congestion 
example happen low priority packets allowed result packets arriving high priority burst lost 
solution allow high priority packets push low priority ones capability adds implementation complexity scheme 
addition may sufficient see discussion issue additional mechanisms may needed try avoid congestion 
section describe buffer management mechanisms early packet discard random early discard red preventive measure packets discarded onset congestion 
detecting onset congestion associated preventive measures vary link technology 
example atm links ip packet may segmented smaller units byte cells 
case congestion measured respect buffer ability store cells 
single cell loss result entire packet rendered useless advantageous discard entire packet worth cells time 
obvious approach drop subsequent cells packet room buffer arriving cell 
termed packet tail discarding partial packet discard ppd 
technique helps avoid unnecessary buffer waste substantial packet throughput degradation occurs periods high congestion offered traffic twice link capacity 
cases packet lose cell flows large packets penalized 
possible approach remedy problem predict onset congestion discard cells packets expected experience cell loss 
example achieved simple threshold buffer occupancy decide accept drop packet time cell received 
referred early packet discard benefit link capacity wasted transmission partial packets 
performs better ppd results effective throughput close link capacity relatively fewer buffers 
turns unfair low bandwidth flows 
particular buffer occupancy hovering threshold value high rate flows opportunities getting packet accepted assuming identical packet sizes 
packet accepted pushes buffer occupancy threshold subsequent packet low bandwidth flow dropped 
issues discussed presents number enhancements basic scheme aimed improving goodput number complete packets transmitted fairness selecting flows start buffering new packet 
general desirable devise buffer management schemes preserve goodput benefits ensuring fairness goodput distributed flows 
fairness buffer management scheme function penalizes packets non conformant flows flows sending higher rate entitled 
ideally scheme ensure single flow grab disproportionate amount buffer space affecting performance level flows 
flow buffer accounting performed relatively easy identify misbehaving flows take appropriate actions ensure fair allocation buffer space 
cost associated flow buffer accounting environments scalability concern buffer management may need done coarser level 
penalty harder ensure fairness flows share common buffer 
instances adaptive applications tcp possible ensure level flow fairness maintaining flow state 
goals random early drop red mechanism relies random dropping decisions buffer content exceeds threshold heavy flows experience larger number dropped packets case congestion 
red aims penalizing flows proportion amount traffic contribute preventing grabbing disproportionate amount resources 
specifically assuming flows tcp transport protocol red provides feedback mechanism sources notified congestion router accordingly reduce amount traffic inject network 
average queue size computed low pass filter exponentially weighted moving average router react quickly transient phenomena 
queue size compared maximum minimum threshold denoted max th min th average queue size min th max th packets accepted dropped respectively average queue size max th min th arriving packet dropped probability depends average queue size 
probability packet particular flow dropped roughly proportional flow share link bandwidth 
flow utilizing larger share link bandwidth forced reduce rate quickly 
main advantages red require flow state maintained router 
result red relatively simple implement conjunction simple fcfs scheduler described section reduce congestion network 
significance internet backbone may hundreds thousands flows link 
red discriminate particular types flows noted ensure packets alternatively marked dropping packet marking supported network stations flows fair share bandwidth 
fact turns red unfair low speed tcp flows 
red randomly drops packets maximum threshold crossed possible packets belongs flow currently fair share bandwidth 
tcp reacts strongly packet loss lost packet force reduction congestion window resulting lower rate 
fair random early drop fred mechanism modification red order reduce unfairness 
addition red thresholds fred maintains thresholds individual flows current buffer occupancies active flows 
decide flows larger amount bandwidth entitled selective discard mechanism effected 
flow state allows fred detect misbehaving flows information prevent consistently buffers 
environments notably atm quite natural maintain state information flow virtual circuit 
fixed cell size atm facilitates hardware implementation complex scheduling buffer management algorithms 
atm single chip solutions perform flow scheduling buffer management ibm charm chip 
virtual circuit guaranteed receive certain rate regardless flows multiplexed link 
mentioned earlier rate guarantee sufficient memory buffer incoming cells 
argue buffers strictly partitioned virtual circuits necessarily strategy 
shown complete partitioning buffers results higher packet loss probability buffers shared flows 
course complete sharing comes problems particularly diverse set flows multiplexed link 
compromise guarantee buffers individual flow retaining certain fraction total buffer space sharing different flows 
matter fact relationship buffer allocation rate guarantee precise 
shown fcfs scheduler rate guarantees provided appropriately allocating buffers flows 
particular establishes intuitive result rate guarantees proportion buffer allocation 
buffer management schemes currently deployed ensure service guarantees applications met packets conform service contract rarely dropped 
substantial differences may exist excess traffic supported 
particular networks differentiate adaptive non adaptive applications support flow buffer accounting provide relatively poor unfair performance excess traffic 
service definitions associated requirements section review services standardized ietf integrated service working group controlled load cl service guaranteed service gs 
discuss briefly ietf effort introduce rigorous service specifications cl gs facilitate support service aggregation 
expand issue section 
controlled load service definition controlled load service qualitative nature 
aims approximating service user experience unloaded network unloaded meant indicate absence traffic avoidance conditions heavy load congestion 
significance fact controlled load service specification qualitative nature require quantitative user traffic specification form tspec section 
importance represents key input call admission process required limit number flows network willing accept ensure desired unloaded network behavior 
guarantees controlled load service provides essentially guaranteed sustained transmission rate equal token bucket rate possibility send occasional bursts size limited token bucket depth peak rate explicit delay loss guarantees associated service allows flexibility call admission process associated scheduling buffer management mechanisms 
example controlled load flows network link supported basic fcfs scheduler load controlled effective bandwidth call admission control simple threshold buffer management scheme 
benefits approach simplicity terms data path control path 
scheduling buffer management require awareness individual flows call admission process involves additions subtractions flows come go 
efficiency call admission control achieved cost added complexity better models computing effective bandwidth flow tspec suggested measurement call admission control techniques described 
noted approach just outlined assumes excess traffic individual flows readily identified explicit marking suggested 
possible explicitly mark easily identify excess traffic individual flows complex data path mechanisms may required prop support controlled load service 
required ensure excess traffic flow impact service guarantees flows 
possible alternative rely flow buffer accounting mechanisms described section 
tighter control amount buffer space flow occupy provides necessary protection excess traffic comes cost maintaining flow buffer state 
refinement flow buffer state replace fcfs scheduler schedulers section capable providing rate guarantees individual flows 
weighted fair queueing wfq scheduler self clocked fair queueing scheduler strict delay guarantees critical controlled load flows 
increases amount flow state adding transmission state buffer state offers number additional benefits 
improves accuracy buffer management mechanism ensuring packets flow get regularly transmitted increase flow buffer occupancy readily associated traffic increase flow 
contrast fcfs scheduler provides regular service individual flows flow packets back queue wait packets ahead sent 
result difficult determine variations buffer occupancy flow caused increase traffic due service fluctuations 
general relatively loose definition guarantees provided controlled load service allows relatively wide range implementation trade offs 
particular mechanisms described sections build system controlled load specifications section 
shall see section guaranteed service stricter service definition 
guaranteed service guaranteed service shares controlled load service tspec specify user traffic service guarantees apply resemblance stops 
controlled load service offers qualitative service guarantees guaranteed service gives quantitative hard deterministic service guarantees 
guarantees include conformant packets lossless transmission upper bound delay 
goal service emulate packet switched network guarantees provided dedicated rate circuit 
clearly guarantees warranted applications cost important applications hard real time requirements remote process control telemedicine haptic rendering distributed caves part ii chapter requesting guaranteed service carried pass advertisement approach sup losses due line errors 
ported rsvp protocol 
process starts sender specifying traffic form tspec sent associated signalling message rsvp path message intended receiver 
message propagates receiver updated node path characteristics path recorded available receiver message reaches 
characteristics particular interest guaranteed service captured record delay contribution scheduler node 
contribution form error terms account different aspects scheduler behavior impact additive path flow 
words cumulative impact schedulers nodes path represented error terms tot tot form tot tot guaranteed service built model rate schedulers investigated discussed section 
rate schedulers attempt approximate perfect fluid server guarantees delay ensuring user minimal service rate time gps model section 
model scheduler represented deviates behavior perfect fluid server 
error terms guaranteed service capture deviations see details 
behavior guaranteed services scheduler accurately described notion service curves 
analogous way traffic envelope bounds amount input traffic service curve provide lower bound amount service received flow individual network element see definitions service curves 
guaranteed services scheduler represented having service curve rate latency form latency values rate service curve flow computed convolution service curves network elements traversed flow 
advantage service curve methodology enables simple graphical representation worst case delay buffer requirements 
example shown worst case delay buffer requirements flow network element computed horizontal vertical distance traffic envelope service curve respectively 
specifically user tspec form allocated service rate set schedulers characterized cumulative error terms tot tot guaranteed upper bound queueing delay form gamma gamma gamma tot tot tot tot 
similar expression available determine necessary buffer space node avoid packet loss 
context guaranteed service equation receiver determine service rate request order obtain desired delay bound tspec specified user tot tot values associated schedulers path 
request service rate sent back sender rsvp resv message processed node determines capacity accept request performs call admission 
call admission varies type scheduler areas different trade offs efficiency complexity possible 
proceeding issue briefly review number interesting properties guaranteed service 
rate model introduces limitations terms efficiency see example affords significant simplicity 
particular single service rate guaranteed scheduler path avoids altogether complex problem trying determine assign individual resource requirements scheduler 
size fits approach precludes lightly loaded scheduler compensating heavily loaded feasible delay scheduler edf large deadline node compensated small node 
exploiting potential challenging problem second rate model preclude types schedulers 
example delay scheduler edf albeit constraints deadlines chosen node see details 
third mentioned earlier cost associated deterministic guarantees guaranteed service provide 
example shown section average load uncommon link carrying guaranteed service flows 
remaining bandwidth certainly left unused assigned lower priority traffic indicates network charge premium guaranteed service 
hard guarantees provides may important certain applications cost may justified 
optimal assignment local deadlines delay bound known intractable 
slope slope delay buffers slope delay traffic envelope service curve bytes sec delay buffer calculations flow 
trade offs implementing guaranteed service briefly discuss trade offs available implementing guaranteed service 
controlled load service trade offs involve data path control path 
data path trade offs primarily terms scheduler efficiency versus implementation complexity efficiency measured scheduler schedulable region larger better complexity function operations performed scheduling transmission packet 
example discussed section tighter scheduling smaller error terms provided wfq scheduler compared say scheduler comes cost complex computations scheduling packet 
affects implementation cost speed scheduler 
noted tighter scheduling lower buffer cost 
illustrated shows cumulative worst case delay buffer requirements node tightening scheduling node amounts reducing error terms turn shifts left service curve node shift helps reduce delay buffering needed node furthermore reduction buffering applies downstream nodes reshaping point see discussion benefits reshaping 
result choice particular scheduler needs take consideration cost scheduler cost system wide resources link bandwidth buffer space 
trade offs need combined control path issues contribute significant differences terms complexity efficiency 
order better illustrate issues focus discussion flow flow flow flow flow flow transmission max size packet link speed link rate bytes sec call admission rule edf scheduler 
differences rate wfq edf schedulers 
delay schedulers shown yield larger schedulable regions rate ones afford efficient link bandwidth comes providing hard delay guarantees 
comes cost complex call admission process 
case rate scheduler wfq call admission decision amounts determining requested service rate available 
easily accomplished keeping track sum service rate requests granted ensuring sum remains smaller link speed new request added 
words call admission amounts simple addition comparison 
case delay scheduler edf involved 
context guaranteed service requested service rate needs translated local deadline edf scheduler 
mentioned earlier task easily performed approach bulk complexity lies determining new request associated deadline tspec accepted 
exact procedure equation see section easily described example 
provides graphical representation call admission procedure edf scheduler 
essentially amounts ensuring sum shifted traffic envelopes specified equation flows remains maximum size packet line corresponding maximum number bits transmitted link rate 
shift applied traffic envelope equal local deadline edf scheduler 
shows case flows flow flow third flow added 
complexity procedure requires storing current sum shifted traffic envelopes keeping associated points checking adding new shifted envelope cause point new curve cross link capacity line done checking points 
proposals simplified procedures call admission procedure remain complex simple addition rate schedulers 
result important consider cost context system design 
differentiated service section briefly discuss efforts started ietf introduce looser service specifications discussed previous sections 
interesting note represents somewhat backwards evolution 
specifically earlier works area qos support focused simple rules call admission coupled aggregate priority structures service differentiation see example description general framework premises 
main reason data path technology time considered capable finer granularity flow control 
initial works quickly followed proposals aimed strengthening service guarantees sophisticated scheduling buffer management algorithms described sections 
evolution fueled part availability new components capable advanced functions desire better control service experienced individual flows see section example potential discrepancies global flow service guarantees 
activities efforts renewed interest focus simple qos guarantees instances guarantees qualitative controlled load service specification 
example ietf discussions issues focused defining services map different levels sensitivities delay loss associated explicit values guarantees 
reasons mentioned motivate return simple solutions 
clearly difficulty upgrading infrastructure size today internet 
sheer size internet means path flow time cross portions network qos capable support crude service differentiation capabilities 
weakens case strong flow guarantees 
similarly support signalling capabilities rsvp required users network negotiate precise service definitions may widely deployed initially 
fundamental arguments put forward 
prevalent example charm chip reshape schedule individual flows speeds oc today instances components similar capabilities 
discussions currently carried diff serv working group mailing list diff serv com 
real need strong explicit qos guarantees 
proper network engineering broad traffic classification small number priority classes coupled adaptive nature applications sufficient offer necessary functionality 
basic premises reasoning argument small fraction applications need strong guarantees 
result adequate provisioning peak traffic load protection coarse classification lower priority traffic provide desired level service 
adequate provisioning ensure rest traffic experiences time adequate service possibly coarse differentiation 
case congestion flows adapt traffic available resources continue operating albeit lower level service 
benefits higher efficiency flows getting greater simplicity minimal signalling support simple data path mechanisms 
analogy illustrate model comparison ip telephony circuit switched telephone system 
cases unusually high congestion circuit switched system block new calls may operate lower level throughput resources held calls blocked 
contrast detecting congestion ip telephones may able adjust coding rate able operate higher packet losses delays 
assuming achieved preventing emergency calls getting assigning higher priority class claim allow calls get 
obviously assumptions model level provisioning needed range adaptation intent provide comprehensive investigation issues 
topic entire 
controversial perspectives coarser qos may value 
particular level qos control may required environments 
example may warranted provide tight flow delay control access switch router fine control may overkill backbone switch running oc gbps speed higher 
similarly maintaining awareness individual kbps flows oc link translates substantial potentially costly amount state information flows significant signalling load 
result may worth considering different qos models different environments 
particular potential aggregation offered differentiated service model benefit backbone internet finer grain integrated service rsvp model may appropriate access campus networks 
main challenge ensure inter operability environments 
section explore issues possible approaches inter operability 
qos aggregation issues models mentioned previous section scalability requirements introduce need aggregation 
especially applicable core backbone potentially large number flows high speeds links stress cost complexity 
need aggregation means ip networks 
example hierarchical structure vpi vci fields header atm cells primarily intended support data path control path aggregation 
specifically forwarding service differentiation decisions looking shorter bits vpi field full header bits similarly setup take individual vcs vp require processing individual signalling messages 
case int serv rsvp ip network granularity qos requests currently determined filters specify destination address port number source address port number instances see details 
corresponds fine granularity qos guarantees provides users accurate control identified potential problem scalability 
result goal aggregation allow individual qos guarantees requiring awareness individual flows segment path 
goal translates number specific requirements satisfied aggregation technique 
requirements best understood example 
consider network topology consisting separate autonomous systems edge say corresponding local middle representing backbone interconnecting 
purpose discussion scalability concern backbone capable maintaining individual flow state information 
aggregation individual flows satisfy requirements maintain awareness individual flows 
able map individual flows internal service classes 
isolation flows maintained flows aggregated common service class excess traffic flow affect performance guarantees flow 
ensure satisfies qos requirements individual flows resources allocated service class consistent aggregate resources required individual flows mapped 
aggregation prevent support individual flow reservations 
requirement core scalability requirement ex pressed 
requirements specify properties satisfied mapping individual rsvp flows coarser classes 
requirement states qos guarantees provided individual flow remain limited conformant packets avoid affecting service guarantees flows 
turn implies need means identify non conformant packet flows merged packet marking mentioned section 
requirement expresses need coupling resources bandwidth buffer level service priority assigned class aggregation individual flows mapped class 
requirement expresses important constraint satisfying scalability come expense functionality 
words aggregation control data path information reversible reservations default back individual flows crossing 
requirement primarily control path issues represents main interoperability challenge qos aggregation faces 
result focus discussion rest section specific issue assume suggested data path aggregation achieved specific bit patterns ip header type ofservice tos octet field specify different service classes drop precedence control path interoperability context aggregation key aspect service guarantees resources allocated set aggregated flows reflect sum reservation requests conveyed individual reservation messages 
generic solutions satisfy requirement broadly categorized static dynamic 
static solutions rely provisioning support different classes service backbone links soft guarantees provided ingress egress points 
dynamic solutions involve ability adjust level aggregate reservation service class individual backbone links function actual offered traffic 
allows harder service guarantees 
static solutions obviously simpler easily satisfy requirement dynamic reservation requests rsvp path resv messages processed backbone 
result individual setup messages directly propagated backbone require minimal additional processing ingress egress points 
example update fields path messages done pre configured information ingress 
static solutions note possible approach rely tunnels layer layer discuss alternative refer reader details 
limitations handle changes backbone 
particular route changes backbone affect accuracy provisioning backbone links ability properly update field characterize path taken backbone 
result static solutions ones deployed desirable provide solutions allow accurate control service guarantees enjoying benefits aggregation 
satisfying goal requirements implications context int serv rsvp services 
disable processing individual rsvp messages backbone allowing identification arrive egress ingress routers 

identify transparently relying continuous interactions routing egress routers corresponding individual flows entering backbone ingress router 

properly update rsvp path messages individual flows egress routers 

reserve appropriate amount resources backbone links satisfy qos requirements individual flows routed 
items protocol specific handled separate signalling protocol setting aggregate reservation see reusing rsvp protocol 
cases individual rsvp reservations processed hidden inside backbone separate aggregate messages reserve resources requirements individual flows aggregated link 
far qos guarantees concerned main challenges lie properly specifying aggregate reservations ensuring service guarantees individual flows preserved 
complexity tasks varies type service controlled load guaranteed service discussed rest section 
case aggregated controlled load reservation necessary queues assigned controlled load traffic backbone links allocated proper service rate 
rate additive quantity aggregate reservations sum tspec individual flows see discussion tspec summed potential benefits statistical multiplexing gain aggregating flows may allow lower reservation level 
situation similar described section fcfs scheduler support controlled load flows main requirements allocate sufficient aggregate service rate control buffer space excess traffic occupy 
situation complex guaranteed service flows 
main issue provision individual delay guarantees tightly coupled ability precisely guarantee specific clearing service rate flow 
aggregation affects ability characterizing individual flows share total rate allocated difficult impossible task 
characterization feasible aggregation limited flows common ingress ingress points see details aggregation done locally backbone link guaranteed service flows assigned queue 
case aggregate service rate backbone link known ingress egress routers associated individual flows route backbone includes link 
order support aggregated guaranteed service flows necessary change node model represent backbone 
similar problem exist crossing atm networks addressed working group 
approach characterize entire atm network single int serv node contributes error term represent atm network delay node 
value selected term atm network estimate delay bound available atm network ingress egress points 
similar approach representing ip backbone router delay node removes previously mentioned dependency unknown aggregate service rates 
main difference case considered backbone ip routers process update field characteristics path backbone nodes traversed relying estimate network characteristics ingress egress points 
summary described sections wide range data path mechanisms exist enable support qos guarantees packet networks 
offer broad choice tradeoffs complexity performance strength guarantees provided 
new services defined availability mechanisms 
deployment services requires data path innovation 
enhanced control path mechanisms needed enable specification service guarantees identification apply 
described section progress developing control path mechanisms deployment facing number challenges 
particular control path data path cost qos vary greatly function environment qos provided 
result desirable allow different control path data path mechanisms different environments 
specifically possible support qos guarantees flow aggregated mechanisms characteristics capabilities environment guarantees provided 
challenge allow interoperability different mechanisms guarantees assumes rsvp signalling protocol setup aggregate reservations 
provided 
issues possible approaches satisfy goal discussed section 
unanswered questions comes determining appropriate qos model environment certainly attempt answer 
tried clarify trade offs requirements hope help focus investigations 
acknowledgment authors acknowledge colleagues ibm watson research center discussions topic qos 
inputs helped shape material 
gu erin gun lin 
traffic management overview 
ibm systems journal december 

le boudec ferrari 
scalable resource reservation internet 
proceedings santiago november 
bennett zhang 
hierarchical packet fair queueing algorithms 
proceedings sigcomm pages stanford university ca august 
bennett zhang 
wf worst case fair weighted fair queueing 
proceedings infocom pages san francisco ca march 
berson vincent 
aggregation internet integrated services state 
internet draft draft approach txt november 
progress 
braden ed zhang berson herzog jamin 
resource reservation protocol rsvp version functional specification 
request comments proposed standard rfc internet engineering task force september 
gu erin 
protective buffer management policies 
ieee acm trans 
networking june 
gu erin 
cell versus message level performances atm networks 

sys 
clark 
adding service discrimination internet 
telecommunications policy april 
clark wroclawski 
approach service allocation internet 
internet draft diff svc alloc txt july 
progress 
cruz 
service burstiness dynamic burstiness measures framework 
high speed networks 
cruz 
quality service guarantees virtual circuit switched networks 
ieee sel 
areas commun 
august 
delp 
atm function specification charm version 
technical report charm intro ibm division lan development february 
demers keshav shenker 
analysis simulation fair queueing algorithm 
journal internetworking research experience january 
blake 
proposal format semantics tos byte traffic class byte ipv ipv headers 
internet draft tos txt november 
progress 
ferguson 
simple differential services ip tos precedence delay indication drop preference 
internet draft draft ferguson delay drop txt november 
progress 
firoiu kurose towsley 
efficient admission control edf schedulers 
proceedings infocom kobe japan march 
floyd jacobson 
random early detection gateways congestion avoidance 
ieee acm trans 
networking august 
floyd jacobson 
link sharing resource management models packet networks 
ieee acm trans 
networking august 
foster kesselman editors 
grid blueprint new computing infrastructure 
morgan kaufman san francisco ca 
appear garrett 
interoperation controlled load service guaranteed service atm 
internet draft draft ietf txt november 
progress 
georgiadis gu erin parekh 
optimal multiplexing single link delay buffer requirements 
ieee trans 
infor 
theory september 
georgiadis gu erin peris rajan 
efficient support delay rate guarantees internet 
proceedings sigcomm pages san francisco ca august 
georgiadis gu erin peris 
efficient network qos provisioning node traffic shaping 
ieee acm trans 
networking august 
hunt 
effective bandwidths multi type uas channel 
queueing systems september 
golestani 
self clocked fair queueing scheme broadband applications 
proceedings infocom pages toronto canada june 
golestani 
network delay analysis class fair queueing algorithms 
ieee select 
areas commun august 
goyal chen vin 
start time fair queueing scheduling algorithm integrated services packet switching networks 
proceedings sigcomm pages stanford university ca august 
goyal vin 
generalized guaranteed rate scheduling algorithms framework 
technical report tr department computer sciences university texas austin 
grossglauser tse 
framework robust measurement admission control 
proceedings sigcomm pages sophia antipolis france september 
gu erin naghshineh 
equivalent capacity application bandwidth allocation high speed networks 
ieee select 
areas commun sac september 
gu erin blake herzog 
aggregating rsvp qos requests 
internet draft draft guerin rsvp txt november 
progress 
gu erin peris rajan 
scalable qos provision buffer management 
proceedings sigcomm vancouver british columbia canada september 
appear 
gun 
approximation method capturing complex traffic behavior high speed networks 
performance evaluation january 
gun gu erin 
bandwidth management congestion control framework broadband network architecture 
computer networks isdn systems september 

ipv tos octet support differential services 
internet draft draft tos octet txt october 
progress 
jamin danzig shenker zhang 
measurement admission control algorithm integrated service packet networks 
ieee acm trans 
networking february 
kleinrock 
analysis shared finite storage computer network node environment general traffic conditions 
ieee trans 
commun com 
sidiropoulos courcoubetis 
weighted round robin cell multiplexing generalpurpose atm switch chip 
ieee select 
areas commun sac oct 
kelly 
effective bandwidth multi class queues 
queueing systems september 

simple integrated media access 
internet draft draft simple media access 
txt june 
progress 
boyer 
priority management atm switching nodes 
ieee trans 
commun com april 

le boudec 
application network calculus guaranteed service networks 
ieee trans 
infor 
theory may 
lin morris 
dynamics random early detection 
proceedings sigcomm pages september 
parekh gallager 
generalized processor sharing approach flow control integrated services networks single node case 
ieee acm trans 
networking june 
parekh gallager 
generalized processor sharing approach flow control integrated services networks multiple node case 
ieee acm trans 
networking april 
parekh 
generalized processor sharing approach flow control integrated services networks 
phd thesis laboratory information decision systems massachusetts institute technology cambridge ma february 

peris 
architecture guaranteed delay service high speed networks 
phd thesis university maryland college park 
postel 
internet protocol 
request comments rfc standard internet engineering task force september 
gu erin 
flow grouping reducing reservation requirements guaranteed delay service 
internet draft draft flow txt july 
progress 
romanow floyd 
dynamics tcp traffic atm networks 
ieee sel 
areas commun may 
ed 
atm user network interface uni signalling specification version 
atm forum signalling working group july 

service curve approach performance guarantees integrated service networks 
phd thesis university california san diego 
shenker partridge gu erin 
specification guaranteed quality service 
request comments proposed standard rfc internet engineering task force september 
shenker wroclawski 
general characterization parameters integrated service network elements 
request comments proposed standard rfc internet engineering task force september 
varghese 
efficient fair queuing deficit round robin 
ieee acm trans 
netw 
varma 
frame fair queueing new traffic scheduling algorithm networks 
proceedings sigmetrics 
varma 
latency rate servers general model analysis traffic scheduling algorithms 
proceedings infocom pages san francisco ca april 
turner 
maintaining high throughput overload atm switches 
proceedings infocom pages san francisco ca april 
wroclawski 
specification controlled load network element service 
request comments proposed standard rfc internet engineering task force september 
wroclawski 
rsvp ietf integrated services 
request comments proposed standard rfc internet engineering task force september 
zhang ferrari 
rate controlled service disciplines 
high speed networks 

