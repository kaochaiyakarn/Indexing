algorithms inverse reinforcement learning andrew ng ang cs berkeley edu stuart russell russell cs berkeley edu cs division berkeley berkeley ca usa addresses problem inverse reinforcement learning irl markov decision processes problem extracting reward function observed optimal behaviour 
irl may useful apprenticeship learning acquire skilled behaviour ascertaining reward function optimized natural system 
rst characterize set reward functions policy optimal 
derive algorithms irl 
rst deal case entire policy known handle tabulated reward functions nite state space linear functional approximation reward function potentially nite state space 
third algorithm deals realistic case policy known nite set observed trajectories 
cases key issue degeneracy existence large set reward functions observed policy optimal 
remove degeneracy suggest natural heuristics attempt pick reward function maximally di observed policy suboptimal policies 
results eciently solvable linear programming formulation irl problem 
demonstrate algorithms simple discrete nite continuous nite state problems 

inverse reinforcement learning irl problem characterized informally follows russell measurements agent behaviour time variety circumstances needed measurements sensory inputs agent available model environment 
determine reward function optimized 
identify sources motivation problem 
rst arises potential reinforcement learning related methods computational models animal human learning watkins schmajuk touretzky 
models supported behavioural studies neurophysiological evidence reinforcement learning occurs bee foraging montague doya sejnowski 
literature assumes reward function xed known example models bee foraging assume reward ower simple saturating function nectar content 
clear examining animal human behaviour consider reward function unknown ascertained empirical investigation 
particularly true multiattribute reward functions 
consider example bee weigh nectar ight distance time risk wind predators 
hard see determine relative weights terms priori 
similar considerations apply human economic behaviour example 
inverse reinforcement learning fundamental problem theoretical biology econometrics elds 
second motivation arises task constructing intelligent agent behave successfully particular domain 
agent designer agent may rough idea reward function optimization generate desirable behaviour straightforward reinforcement learning may usable 
consider example task driving source information learning behaviour expert agents imitation learning apprenticeship learning 
setting commonly assumed purpose observation learn policy direct representation mapping states actions 
propose recover expert reward function generate desirable behaviour 
suggest reward function provides parsimonious description behaviour 
entire eld reinforcement learning founded presupposition reward function policy succinct robust de nition task 
inverse reinforcement learning may domains provide ective form apprenticeship learning 
knowledge computational task studied computer science control theory psychology biology 
closest economics task multiattribute utility assessment studied depth person combine various attributes available choice making decision 
theory developed keeney rai applications numerous 
eld studies shot decisions single action taken outcome immediate 
sequential case rst considered sargent tried ascertain ective hiring cost labor examining rm hiring behaviour time assuming rational 
decade area structural estimation markov decision processes econometrics grown rapidly rust 
basic ideas carry setting 
irl appeared brie control theory early kalman posed problem recovering objective function deterministic linear system quadratic costs 
solved semide nite program boyd 
address irl problem settings familiar machine learning community nite markov decision processes mdps 
section gives formal de nitions mdps irl problem focus initially setting model known complete policy 
section characterizes set reward functions policy optimal 
demonstrate set contains degenerate solutions including example reward function identically zero 
resolve diculty heuristics attempt identify reward function maximally di observed policy sub optimal policies 
done ciently discrete case linear programming 
section deals case large nite state spaces explicit tabular representation reward function infeasible 
show tted reward function represented linear combination arbitrary xed basis functions irl problem remains class linear programs solved eciently 
section deals realistic case policy known nite set observed trajectories simple iterative algorithm 
algorithms develop applied section simple examples including discrete continuous stochastic navigation problems mountain car problem 
cases able recover reward function explains observed behavior fairly 
section summarizes ndings describes directions 

notation problem formulation section introduce notation de nitions basic theorems markov decision processes 
de ne version irl problem address 
markov decision processes nite mdp tuple fp sa nite set states 
fa set actions 
sa 
state transition probabilities action state discount factor 

reinforcement function bounded absolute value rmax simplicity exposition written rewards extension trivial 
policy de ned map 
value function policy evaluated state 
expectation distribution state sequence pass execute policy starting de ne function psa 
notation sa 
means expectation respect distributed sa 
optimal value function sup optimal function sup 
discrete nite spaces functions represented vectors indexed state adopt boldface notation 
precisely enumeration nite state space rewards may written dimensional vector ith element reward ith state mdp 
similarly vector ith element value function evaluated state action matrix element gives probability transitioning state action state symbols denote strict non strict vectorial inequality goal standard reinforcement learning nd policy maximized 
shown see 
sutton barto bertsekas tsitsiklis exist optimal policy simultaneously maximized basic properties mdps solution irl problem need classical results concerning mdps sutton barto bertsekas tsitsiklis 
theorem bellman equations mdp fp sa policy 

satisfy sa theorem bellman optimality mdp fp sa policy 

optimal policy arg max inverse reinforcement learning inverse reinforcement learning problem nd reward function explain observed behaviour 
simple case state action spaces nite model known complete policy observed 
precisely nite state space set actions fa transition probabilities fp sa discount factor policy problem nd set possible reward functions optimal policy fp sa 
may wish identify functions set satisfying additional criteria 
renaming actions necessary assume loss generality trick simplify notation 

irl finite state spaces section give simple characterization set reward functions policy optimal 
show set contains degenerate solutions propose simple heuristic removing degeneracy resulting linear programming solution irl problem 
characterization solution set main result characterizing set solutions theorem nite state space set actions fa transition probability matrices fp discount factor 
policy optimal reward satis es proof 
equation may written substituting equation theorem see optimal arg max sa sa sa implication derivation equation 
completes proof 

similar argument easy show essentially replacing inequalities proof strict inequalities condition necessary sucient unique optimal policy 
nite state mdps result characterizes set reinforcement functions solutions inverse reinforcement learning problem 
immediately see problems constant vector solution invertible 
see rst note transition matrix eigenvalues unit circle complex plane 
implies matrix eigenvalues interior unit circle particular eigenvalue 
means zero eigenvalues singular 
reward matter action take policy including optimal 
demanding unique optimal policy alleviate problem entirely satisfying usually reward vectors arbitrarily close solutions 
second mdps choices meet criteria 
decide reinforcement functions choose 
answer problems original statement irl problem section describe natural criteria suggest solutions problems 
lp formulation penalty terms clearly linear programming nd feasible point constraints equation 
discussed previous section points may meaningful desire nd way choose solutions satisfying equation 
proposals outlined section large extent chosen incorporated linear program fairly natural 
natural way choose rst demand optimal solves irl problem favor solutions deviation costly possible 
functions satisfying jr rmax choose maximize max ana words seek maximize sum di erences quality optimal action quality best action 
criteria ana possible sake concreteness remain 
addition believe things equal solutions mainly small rewards simpler preferable may optionally add objective function weight decay penalty term adjustable penalty coecient balances twin goals having small reinforcements maximizing 
side ect penalty term suciently large nonzero states consistent idea simple reward function 
common practice applications hand tune penalty coef cients shown assuming solution degenerate increased phase transition point optimal bounded away wanted choose automatically value just phase transition binary search appealing choice gives simplest largest penalty coecient zero particular partially explain optimal 
putting optimization problem maximize min fa jr rmax denotes ith row clearly may easily formulated linear program solved eciently 
section reports simple experiments algorithm 

linear function approximation large state spaces consider case nite state spaces 
apart measure theoretic assumptions minor regularity conditions ignore nite state mdps may de ned way nite state mdps section 
sake concreteness restrict case assume availability subroutine approximating value policy particular mdp 
setting reward function function reals general solution inverse reinforcement learning require working space functions 
calculus variations give tools optimizing space dicult algorithmically 
choose linear approximation reward function expressing 
xed known bounded basis functions mapping unknown parameters want 
linear variables optimized surprise linear programming formulation applies 
denote value function policy mdp reward function linearity expectations value function reward function equation 
fact theorem reader may easily verify essentially argument theorem proof policy optimal appropriate generalization condition psa psa states actions equation know linear coecients set linear constraints problems current formulation 
rst nite state spaces nitely constraints form equation making hard impossible check 
algorithmically circumvent problem sampling large nite subset states constraint states second problem subtle restricted linear function approximator equation express may longer able express reward function trivial optimal 
case linear function approximator class compromise may willing relax constraints paying penalty violated 
nal linear programming formulation maximize min fa psa psa remind reader implicit function equation subsample states 
penalizes violations constraints penalty weight heuristically chosen parameter results sensitive moderately larger values usually giving quite similar results 

irl sampled trajectories section addresses irl problem realistic case access policy set actual trajectories state space 
require explicit model mdp assume ability nd optimal policy reward choice 
initial state distribution assume unknown policy goal nd maximizes subscript expectation denotes expectation respect drawn 
simplify notation ll assume xed start state 
fact dummy state state distribution action 
previous algorithm nite state spaces assume expressed linear function approximator class 
assume ability simulate trajectories mdp initial state optimal policy policy choice 
policy consider including optimal need way estimating setting rst execute monte carlo trajectories 
de ne average empirical return trajectories reward example take trajectories trajectory visited sequence states 
general average empirical returns trajectories 
setting natural estimate 
previous algorithm derivation justi ed fact 

describe algorithm 
start algorithm rst nd value estimates described base case policy case randomly chosen policy 
inductive step algorithm follows set policies want nd setting resulting reward function hopefully satis es previous algorithm modify objective practice truncate trajectories large nite number steps 
discounting introduces small error approximation example log rmax horizon time truncation introduces error estimates 
unhappy approximation way execute trajectory expected length obtain unbiased estimate true reward kearns method 
true 
top grid world optimal policy 
bottom true reward function 
slightly optimization maximize violations constraints penalized heuristically chosen parameter results extremely sensitive 
note just implicit linear functions equation problem easily solved linear programming 
optimization gives new setting new reward function 
nd policy maximizes add current set policies continue large number iterations nd satis ed 

experiments rst experiment grid world agent starts lower left grid square way absorbing grid square receives reward 
actions correspond trying move 
inverse rl grid 
top 
bottom 
compass directions noisy chance resulting moving random direction 
optimal policy shown top true reward function 
inverse reinforcement problem recovering reward structure policy problem dynamics 
running algorithm described section penalty term obtain reward function shown top 
clearly recovered reward structure slightly bumpy 
hard avoid comes arbitrary symmetry breaking chosen policy 
penalty coecient set value just phase transition discussed earlier obtain second reward function close true reward 
experiment run known mountain car task cartoon shown 
true undiscounted reward reach goal top hill state car position velocity 
state space continuous version algorithm described section 
chose function approximator class reward functions interestingly intermediate values give smooth looking functions 
retrospect surprising small results values near large results values near intermediate mix 
goal 
cartoon mountain car problem 
scale 
car position class consisting linear combinations evenly spaced basis functions 
giving optimal policy algorithm typical reward function shown 
note scale axis 
clearly solution nearly perfectly captured structure reward 
challenging problem reran experiment true reward changed interval centered bottom hill 
problem optimal policy go quickly possible bottom hill park 
possible example near top hill right moving quickly may shoot right hill enter absorbing state matter hard 
running algorithm new problem typical solution shown 
large successfully recovered main structure reward large positive speci ed interval artifact right side believe ect unavoidably shooting right 
think solution shown fairly problem 
nal experiment applied sample algorithm continuous version grid world 
precisely state effect compass direction actions move agent intended direction uniform noise added coordinate state nally truncated necessary keep unit square 
true reward non absorbing square 
function approx determined ne discretization state space 
functions needed algorithm way 
run algorithm sample states size js counting states give nontrivial constraints 
car position car position 
typical solutions irl mountain car 
top original problem note scale axis 
bottom problem parking bottom hill 
class consisted linear combinations array dimensional gaussian basis functions 
initial state distribution uniform state space algorithm run trajectories steps evaluate policy 
needed nd optimal policy comparison mdp solved discretization state space 
running experiment solution algorithm usually reasonable just iteration iterations algorithm usually settled fairly solutions 
compared tted reward optimal policy true optimal policy calculating fraction state space action choices disagree top 
discrepancies typically distinct near optimal policies variation expected 
measure algorithm performance compare quality tted reward optimal policy quality true optimal policy 
quality course measured true reward function 
usually iterations algorithm evaluations monte carlo trials steps unable detect statistically signi cant di erence value true optimal policy value tted reward optimal policy bottom 
iteration number fraction states actions disagree iteration number value policy 
results continuous grid world runs 
top fraction states tted reward optimal policy disagrees true optimal policy plotted iteration number 
bottom value tted reward optimal policy 
estimates monte carlo trials length negligible error bars 

results show inverse reinforcement learning problem soluble moderate sized discrete continuous domains 
number open questions remain addressed potential shaping rewards ng produce reward functions dramatically easier learn solution mdp ecting optimality 
design irl algorithms recover easy reward functions 
real world empirical applications irl may substantial noise observer measurements agent sensor inputs actions agent action selection process may noisy suboptimal 
may optimal policies observed 
appropriate metrics tting data 
behaviour strongly inconsistent optimality identify locally consistent reward functions speci regions state space 
experiments designed maximize identi ability reward function 
algorithmic approach carry case partially observable environments 
acknowledgments ng supported berkeley fellowship 
supported nsf ecs 
bertsekas tsitsiklis 

neuro dynamic programming 
athena scienti boyd ghaoui balakrishnan 

linear matrix inequalities system control theory 
siam 
doya sejnowski 

novel reinforcement model learning 
advances neural information processing systems pp 

denver mit press 
kearns mansour ng 

approximate planning large pomdps reusable trajectories 
extended version 
keeney rai 

decisions multiple objectives preferences value tradeo new york wiley 
montague dayan person sejnowski 

bee foraging uncertain environments predictive hebbian learning 
nature 
ng harada russell 

policy invariance reward transformations theory application reward shaping 
proceedings sixteenth international conference machine learning 
bled slovenia morgan kaufmann 
russell 

learning agents uncertain environments extended 
proceedings eleventh annual conference computational learning theory 
acm press 
rust 

people behave bellman principal optimality 
submitted journal economic perspectives 
sargent 

estimation dynamic labor demand schedules rational expectations 
journal political economy 
schmajuk 

escape avoidance imitation neural network approach 
adaptive behavior 
sutton barto 

reinforcement learning 
mit press 
touretzky 

operant conditioning 
adaptive behavior 
watkins 

models delayed reinforcement learning 
doctoral dissertation psychology department cambridge university cambridge united kingdom 
