minimax strategy gaussian density estimation ac jp graduate school information sciences university sendai japan manfred warmuth manfred cse ucsc edu computer science department uc santa cruz ca consider line density estimation gaussian unit variance 
trial learner predicts mean receives instance chosen adversary incurs loss performance learner measured regret defined total loss learner minus total loss best mean parameter chosen line 
assume horizon protocol fixed known parties 
give optimal strategies learner adversary 
value game ln ln ln ln ln ln upper bound norm instances 
consider standard algorithm predicts fixed show regret algorithm ln regardless choice done sabbatical leave uc santa cruz 
manfred warmuth supported nsf ccr 
consider simple repeated game gaussian density estimation 
learner plays adversary 
trial learner produces mean adversary provides instance vector loss learner words assume unit variance 
assume horizon game number trials fixed known parties 
consider regret relative loss inf total line loss learner minus total loss best mean parameter chosen line instances 
goal learner minimize regret goal adversary maximize 
analogous problem density estimation discrete domain log loss shtarkov gave minimax strategy implicit form value game called minimax regret 
freund gives explicit formula minimax regret bernoulli density estimation ln ln :10.1.1.51.3156
minimax strategy computed universal portfolio problem 
case strategy efficiently computable minimax regret universal portfolio problem minimax regret bernoulli density estimation 
minimax regret different large body roots minimum description length community 
short require learner choose line parameters model class best line parameter chosen 
discuss differences section 
give minimax strategy gaussian density estimation learner adversary 
strategies simple efficient 
trial learner intuitively choose average past instances mean 
optimal strategy learner choose slightly smaller ln ln ln 
note depends horizon 
give simple recurrence optimal shrinkage factor learner plays optimally regret get minimax regret need restrict adversary choose instances norm bounded constant adversary regret unbounded just trial 
minimax regret horizon ln ln ln ln ln ln ln ln term surprising line games shown ln upper bounds minimax regret 
intriguing properties optimal strategies parties 
learner need know upper bound norm instances 
second strategies give players optimal opponent plays non optimally past 
third adversary restrict choice instances points plus minus times unit vector 
restricted choice instances adversary force regret large game value 
words algorithm take advantage restricted choice adversary 
natural algorithm gaussian density estimation start initial instance predict ax xq multiplicity initial instance 
initial instance chosen zero gaussian density estimation 
prediction algorithm forward algorithm 
algorithm investigated parallel gordon 
forward algorithm inspired similar related algorithm vovk linear regression 
show regret forward algorithm larger ln regardless choice holds constant allowed depend horizon hand fixed choice forward algorithm works knowing regret ln 
forward algorithm significant gap cases horizon known unknown learner 
conjecture horizon known adversary force learner regret ln 
lower bound techniques know form ln 
lower bounds lie value game expressed ln ln ln ln 
means known techniques strong bring difference cases horizon known unknown 
resolving conjecture raises number open problems 
cases minimax strategies simple efficient 
particular don know minimax strategy linear regression gaussian density estimation arbitrary variance 
line density estimation gaussian give formal framework line density estimation problem gaussian densities 
vector denotes transposition shorthand real dimensional gaussian density function exp assume variance covariance matrix fixed known 
case construct linear transformation maps instance ax subject gaussian unit matrix 
loss generality assume parameter space consists mean vectors 
mean represents density function xj exp instance define loss negative log likelihood ln xj constant independent 
constant term matter analysis define loss simply restrict instance space set vectors norm real 
fx kxk xg kxk denotes norm line algorithm called learner function choose parameter past instance sequence 
protocol proceeds trials 
trial learner chooses parameter instance sequence observed far 
learner receives instance suffers loss total loss learner best parameter hindsight line setting 
arg inf shorthand measure performance learner particular instance sequence regret relative loss defined inf goal learner regret small possible 
concerned worst case regret probabilistic assumption instance sequence generated 
words preceding protocol viewed game learner adversary regret payoff function 
learner tries minimize regret adversary tries maximize 
assume horizon number trials game fixed known learner adversary 
case game value called minimax regret defined inf sup alternatively define minimax regret inf sup 
inf sup xt inf minimax regret achieved learner adversary play optimally 
difference rissanen stochastic complexity model large body proving regret bounds roots minimum description length community 
model learner interpreted coding scheme set sequences length coding scheme specified probability mass function 
note necessarily trial learner coding scheme provides conditional 
jx past sequence conditional defines coding instance 
learner observes instance incurs loss ln jx code length total loss ln jx ln code length sequence regret learner relative family probability mass functions fp 
defined ln ln arg inf ln maximum likelihood estimator 
regret code length minus code length ideal coding scheme parameter space 
regret thought redundancy coding scheme relative 
set sequences length minimax regret set relative defined inf sup wt optimal coding scheme attains minimax regret 
rissanen called code length ln stochastic complexity respect 
particular rissanen showed condition fx kg compact subset ln ln ji dimension ln 
denotes fisher information matrix 
case gaussian density unit variance parameter space fisher information matrix unit matrix 
minimax regret defined different points 

coding scheme arbitrary 
particular need model class fp 
model class just set measure performance learner 
hand setting require predictions learner proper sense lie underlying model class define loss best line parameter 
gaussian density estimation predictions learner gaussian 

individual instances need bounded 
gaussian density estimation natural choice fx kxk xg 
case condition means weaker condition 
comparison setting obvious difference gives choices learner difference gives choices adversary 
fact gaussian density estimation incomparable precisely gaussian density estimation minimax regret fx kxk xg ln ln volume ball radius ln ln ln ln second term equality comes fact fisher information matrix unit matrix 
second equality derived fact volume ball radius 
hand show minimax regret setting ln ln ln interesting see depends dimension 
bound instance space appears term independent appears leading term 
minimax regret gaussian density estimation define sequence shrinkage factors define optimal predictions learner 
definition fc sequence recursively defined suppose trial past sequence learner chooses parameter 
represent parameter 
follows 
denote choice learner 
recall shorthand vector 
offset shown optimal choice learner 
lemma learner instance sequence 
proof 
gaussian best parameter definition regret plugging 
formula 



sum 
plugging lemma 
note lemma need bound instance space 
show 
gives optimal choice learner 
theorem follows immediately lemma 
theorem learner chooses 
instance sequence interesting see equality theorem holds instance space unbounded 
see instances norm kx regret give strategy adversary chooses instances trial learner chooses 
adversary chooses 

convention denotes unit vector 
note lemma 
lemma instance sequence kc proof 
easy induction shows triangular inequality kc kx theorem learner 
sequence instance 
proof 
lemma 

gives 
xk 
lemma 
kc kk 
xk 
completes theorem 
surprisingly adversary restrict choice instances points arbitrary unit vector say 
restricted choice instances claim adversary force regret large game value 
adversary chooses 
je 
xe 
xe 

choice hard see summand second term positive 

kc 
je 
claim holds 
theorem theorem conclude optimal strategy learner choose minimax regret surprising optimal choice learner depend implies learner need know bound instance space 
unfortunately coefficients see recurrence definition depend horizon closed form 
show tight bounds lemma ln ln ln ln ln ln ln ln ln ln ln give proof appendix 
summarize results section corollary 
corollary gaussian density estimation minimax regret inf sup ln ln ln ln ln ln infimum attained learner optimal play non optimal player showed learner strategy optimal sense gives minimum regret assuming adversary plays optimally 
true adversary plays non optimally 
surprisingly show optimal adversary plays non optimally 
similarly adversary choice 

turns optimal learner plays non optimally 
precise extend notion regret situation initial choices players 
history play trial 
note choices history necessarily optimal 
define minimax regret inf sup inf sup 
inf sup xt similarly history followed define minimax regret sup inf sup 
inf sup xt clearly empty history gives minimax regret shown corollary 
arguments previous section combined easy induction show theorem 
theorem history play 
inf 
infimum attained sup 
supremum attained 

lower bound forward algorithm lemma optimal shrinkage factor roughly ln ln 
approximation shrinkage factors form universal constant 
learner called forward algorithm 
constant parameterizes prior 
particular warmuth showed forward algorithm worst case regret ln 
precisely forward algorithm bayes optimal algorithm minimizes expected regret probabilistic setup adversary chooses beta prior trial generates probability 

probabilistic setup expected regret forward algorithm shown ln 
large optimal algorithm expected regret ln probabilistic argument gives lower bound ln worst case regret algorithm 
note lower bound lies minimax regret ln ln ln proven 
section show particular adversary force forward algorithm regret ln 
sequence produced adversary decidedly sake simplicity assume shift 
forward algorithm predicts 
words 
appendix show form monotonically decreasing sequence 
exists tg 
sign 
opposite sign adversary strategy theorem chooses instance sgn sgn strategy produces instance sequence 
odd 
note optimal choice adversary playing forward algorithm forward algorithm plays non optimally 
adversary theorem assumes learner plays optimally 
show sequence appropriate choice regret large obtaining lower bound 
theorem forward algorithm predicts fixed exists tg instance sequence defined gives ln 
proof 
definition instance sequence obvious odd 
straightforward show regret sum lower bounded dx ln ln similarly second sum regret lower bounded ln ln plugging formula ln ln respectively 
show ln ln 
assume ln case choose 
ln ln ln formula monotonically increasing minimized 
simple calculation shows ln 
acknowledgments authors grateful jurgen forster kenji yamanishi useful discussions jun ichi takeuchi helping understand stochastic complexity result quoted section 
warmuth 
relative loss bounds line density estimation exponential family distributions 
proceedings fifteenth conference uncertainty artificial intelligence pages san francisco ca 
morgan kaufmann 
long version see www cse ucsc edu manfred 
cover 
universal portfolios 
mathematical finance 
freund :10.1.1.51.3156
predicting binary sequence optimal biased coin 
proc 
th annu 
conf 
comput 
learning theory pages 
acm press new york ny 
gordon 
approximate solutions markov decision processes 
ph 
thesis department computer science carnegie mellon university pittsburgh 
technical report cmu cs june 
ordentlich cover 
cost achieving best portfolio hindsight 
mathematics operations research 
rissanen 
stochastic complexity learning 
computational learning theory eurocolt pages 
springer verlag 
rissanen 
fisher information stochastic complexity 
ieee transactions information theory 
shtarkov 
universal sequential coding single messages 
prob 

inf 
takeuchi barron 
asymptotically minimax regret exponential families 
sita pages 
takeuchi barron 
asymptotically minimax regret bayes mixtures 
ieee isit 
vovk 
competitive line linear regression 
technical report csd tr department computer science royal holloway university london 
xie barron 
asymptotic minimax regret data compression gambling prediction 
ieee trans 
information theory 
yamanishi 
decision theoretic extension stochastic complexity applications learning 
ieee transaction information theory july 
yamanishi 
extended stochastic complexity minimax relative loss analysis 
proc 
th international conference algorithmic learning theory alt volume lecture notes artificial intelligence pages 
springer verlag 
proof lemma want estimate sequence fc defined recurrence logarithm sides ln ln ln inequalities ln hold ln ln replace get ln ln summing inequalities tg ln ln get upper lower bounds ln ln ln ln need estimate function express bound closed form 
define sequence fd holds plugging equality recurrence respect fd note clear harmonic sum ln ln gives lower bound ln ln similarly plugging second term ln ln ln ln ln ln ln ln ln ln gives upper bound ln ln ln follows ln ln ln ln plugging easily get ln ln ln ln ln ln 
