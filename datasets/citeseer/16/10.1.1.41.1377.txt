distributed reinforcement learning scheme network routing michael littman justin boyan july cmu cs school computer science carnegie mellon university pittsburgh pa describe self adjusting algorithm packet routing reinforcement learning module embedded node switching network 
local communication keep accurate statistics node routing policies lead minimal delivery times 
simple experiments involving node irregularly connected network learning approach proves superior nonadaptive algorithm precomputed shortest paths 
authors support bellcore cognitive science research group national defense science engineering graduate fellowship program national science foundation iri 
views contained document authors interpreted representing official policies expressed implied bellcore national science foundation government 
keywords reinforcement learning network routing algorithm routing packets efficiently communication network unpredictable usage patterns 
algorithm related certain distributed packet routing algorithms learn routing policy balances minimizing number hops packet take possibility congestion popular routes 
experimenting different routing policies gathering statistics policies minimize total delivery time 
principle approach expensive terms learning time storage space algorithm describe variant reinforcement learning algorithm called learning adapts changes network traffic requires little space needed represent complete routing policy 
application learning differs traditional network constructed distributed collection learners responsible portion problem 
approach appears effective routing packets efficiently high load 
experiments carried discrete event simulator model motion packets local area network 
packets periodically introduced network random node random destination 
multiple packets node stored unbounded fifo queue set limit total number packets active network time generally 
unit time node takes top packet queue examines destination chooses neighboring node send packet 
packet sent directly destination node removed network immediately 
routing reinforcement learning task packet routing policy answers question adjacent node current node send packet order get quickly possible eventual destination 
policy performance measured total time taken deliver packet training signal directly evaluating improving policy packet reaches destination 
idea field reinforcement learning update policy quickly local information 
learning scheme policy distributed network follows node keeps estimate neighbor destination pair long takes packet destination arrive sent neighbor node node asked route packet sends neighbor estimates lowest total delivery time 
waiting packet reach updating policy queries find long expects packet take get presumably closer estimate considered accurate update delivery time estimate 
precisely time node estimates takes deliver packet bound node way neighbor node including time spend node queue 
sending immediately gets back estimate time remaining trip min neighbors packet spent units time queue revise estimate follows deltaq new estimate gamma old estimate learning rate parameter experiments 
field reinforcement learning function approximated neural network see allow learner incorporate diverse parameters system local queue size time day distance estimation 
implementation described represented table 
results tested routing algorithm variety network topologies including hypercube node telephone network irregular theta grid 
varying level network traffic measured average delivery time packets system learning settled routing policy compared delivery times conventional routing scheme shortest paths 
result cases learning algorithm able sustain higher level network traffic non learning 
denote function corresponds function reinforcement learning technique learning 
network topology simulator time avg delivery time learn short simulator time learn short performance low load high load results irregular grid network pictured 
conditions low load network learns fairly quickly route packets shortest paths destination 
performance vs time curve plotted left part demonstrates routing algorithm prior knowledge network topology learns route shortest path router performs optimally low load 
policy summaries shortest path learned high load network traffic increases shortest path routing scheme far optimal ignores bottlenecks soon floods network packets 
right part plots performance vs time routing schemes high load conditions shortest path unable tolerate packet load algorithm learns efficient routing policy 
reason learning algorithm success apparent policy summary diagrams 
diagrams indicate node policy theta point point routes go node 
left part summarizes fact interesting note learning update rule mathematically known bellman ford shortest paths algorithm path relaxation steps performed asynchronously 
shortest path routing policy nodes center network labelled shortest paths congested network load high 
contrast diagram right shows algorithm conditions high load learned policy routes traffic longer necessary path top network avoid nodes center network 
basic result captured compares performances shortest path policy learned policy network load increases 
point represents median trials mean packet delivery time learning settled 
load low learning algorithm routes nearly efficiently shortest path policy 
load increases shortest path policy leads exploding levels network congestion learning algorithm continues route efficiently 
significant increase load distributed learning algorithm result congestion 
network load learn short delivery time various loads exhibited learning algorithm having know advance network topology traffic patterns need centralized routing control system able discover efficient routing policy 
simulations described fully realistic standpoint actual telecommunication networks believe reinforcement learning algorithms effective self adapting routing practical tasks 
bellman 
routing problem 
quarterly applied mathematics 
ford jr flows networks 
princeton university press 

lin 
reinforcement learning robots neural networks 
phd thesis school computer science carnegie mellon university 
rudin 
routing delta routing taxonomy performance comparison techniques packet switched networks 
ieee transactions communications com january 
tanenbaum 
computer networks 
prentice hall second edition edition 
tesauro 
issues temporal difference learning 
machine learning may 
watkins 
learning delayed rewards 
phd thesis king college cambridge 

