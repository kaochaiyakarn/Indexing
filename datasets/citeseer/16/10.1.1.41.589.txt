overfitting avoidance bias third draft cullen schaffer department computer science cuny hunter college park avenue new york ny delta delta schaffer hunter cuny edu january strategies increasing predictive accuracy selective pruning widely adopted researchers decision tree induction 
easy get impression research reports statistical reasons believing overfitting avoidance strategies increase accuracy research community making progress developing powerful general methods guarding overfitting inducing decision trees 
fact overfitting avoidance strategy amounts form bias may degrade performance improving 
pruning methods proven successful empirical tests due methods choice test problems 
examples article illustrate overfitting avoidance strategies better worse appropriate specific application domains 
making progress methods powerful general 
research reported supported robert wood johnson pharmaceutical research institute 
preliminary draft appeared proceedings ijcai workshop evaluating changing representation machine learning sydney australia 
widely noted researchers studying decision tree induction performance training data may misleading indication true predictive accuracy 
complex tree achieves high accuracy training set fare worse fresh data simple tree performed spectacularly 
case researchers say complex tree overfit training data reflecting true underlying relationships patterns arising purely chance 
overfitting sense decreases predictive accuracy great deal effort expended developing overfitting avoidance methods tree induction generally form pruning strategies 
methods reported widely compared empirically researchers undertake decision tree induction relying form overfitting avoidance 
fact overfitting avoidance methods improved predictive accuracy induced decision trees methods inherently beneficial 
overfitting avoidance strategy amounts kind bias biases helpful appropriate domain application 
particular article meant show success decision tree pruning strategies published studies due entirely problems researchers chosen test 
alternative choices support best ignore overfitting presence noisy data simply choose tree considered best fits training set 
deny course overfitting avoidance methods worked practice 
apparently biases entail frequently appropriate domains decision tree induction problems drawn 
important point researchers credit observed successes appropriate application bias construe evidence favor bias 
researchers suggest success pruning techniques problems number domains proves techniques domain independent value combined weight empirical successes intuitive arguments theoretical apparently persuaded inherent value avoiding overfitting especially presence noise 
presumption inherent value article intended demonstrating overfitting avoidance form bias 
draw analogy suppose time experiment unfair coin asked predict show heads tails flip 
basic strategy consists flipping coin number times predicting flip whichever side coin appeared 
consider variations strategy 
calls prediction heads heads flipped third preliminary trials 
clearly example mean word bias just clearly inherently inherently bad idea 
apply revised strategy coins high probability turning heads outperform original original strategy superior 
bias appropriate 
second strategy identical basic doubles number preliminary flips 
contrast definite statistical improvement 
regardless kind unfair coin straightforward probabilistic arguments prove revised strategy correct prediction original 
sense inherently beneficial 
light analogy may say researchers concluded overfitting avoidance revision second kind purpose article show 
strong indication prevalence conception overfitting avoidance statistical improvement fact research date focused inventing better pruning techniques improving old ones 
form bias overfitting avoidance method conditionally beneficial adjectives better improved meaningless conditions explicit 
imply course ought abandon accepted pruning techniques 
need understand problems demonstrated accounts successes may delineate domain applicability technique second fundamentally impossible need abandon project attacking decision tree induction full generality inventing pruning techniques concentrate approaches designed expressly specified conditions observed hold important practical domains 
overview main body article demonstrates series experiments effect particular widely respected overfitting avoidance technique depends problem generating environment applied 
illustrates experiments fact known overfitting avoidance techniques likewise form bias statistical improvement inducing decision trees 
experiments suggest domain characteristics may overfitting avoidance appropriate particular refute common notion classification noise justifies overfitting avoidance 
discussion experiments meta points addressed 
theoretical intuitive empirical arguments favor overfitting avoidance reviewed see reconciled results 
second practical significance argument embodied article considered 
methodology strategies comparison experiments reported sections compare tree induction strategies differ approach problem overfitting 
initial phase common strategies large tree constructed recursive splitting 
naive strategy selects tree modification prediction 
sophisticated strategy iteratively prunes obtain sequence trees increasing simplicity decreasing accuracy training set 
uses cross validation procedure attempt decide tree sequence yield best predictive accuracy 
may original large tree best case naive sophisticated strategies coincide 
may compare accuracy chosen trees see superior 
sophisticated strategy closely cart procedure described detail 
trees built gini splitting rule measures impurity node gamma number treatment simpler leaves key details describe cross validation procedure described breiman employed 
class instances node total number instances node 
splitting continues long impurity decreased 
resulting large tree pruned iteratively iteration eliminating branch currently contributes gain accuracy training set leaf node added 
yields sequence trees complexity cost accuracy gain leaf node acceptable 
choose appropriate complexity cost tenfold cross validation strategy applied recommended 
procedure just described repeated times time training cases build sequence trees remaining evaluate trees sequence obtain unbiased information performance level complexity cost 
information trials pooled apparent optimal complexity cost select sequence trees derived full training set 
main difference strategy described breiman understanding information performance various levels complexity cost subject chance variation choose tree complexity cost yields cross validated performance standard deviation apparent optimum 
irrelevant intended promote intelligibility increase predictive accuracy 
case standard error rule amounts additional bias simplicity strengthen results 
cart approach chosen representative sophisticated overfitting avoidance strategies research decision tree induction produced 
clear argument article results qualitatively similar known algorithms employed comparisons 
particular demonstrated section key results holds broad class pruning strategies including best known machine learning researchers 
experiments reporting interpretation experiments reported boolean attribute variables take values independently equal probability 
boolean class variable takes value dependent way illustrations article involve class problems qualitative results general 
attributes noise form complicates dependency 
experiment consists trials naive sophisticated strategies compared 
trial training set cases generated strategies induce decision tree 
experiments exact theoretical accuracy trees predicting value class variable fresh cases generated way training data calculated comparisons 
calculation complex fact noted trees tested fresh cases determine accuracies empirically 
trials naive sophisticated strategies choose identical trees trees predictive accuracy tell strategy superior 
reporting focuses remaining discrepant trials 
trials average accuracy trees induced strategy reported number trials produced superior tree 
strategy produced superior trees large majority discrepant trials statistical significance majority reported 
discrepant trials experiment sophisticated strategy produces superior trees strategies fact evenly matched 
see calculate probability sophisticated strategy proving superior trials strategy equal chance producing superior tree 
probability report significance majority 
reporting focuses discrepant cases interest note accuracies trees produced naive sophisticated strategies averaged trials experiment 
distinguish averages discrepant cases referred gross average accuracies 
methodological note concerns readers bound consider large size training set experiments relative size instance space 
possible attribute vectors cases learning 
fact relative statistically sophisticated standard sided binomial sign test described ff 
advantage test alternatives paired test example non parametric avoiding unwarranted assumptions normality 
test sensitive experiments reported important note choice test data collected 
abundance training data convenient purposes article determine qualitative results 
data attributes superiority naive sophisticated strategy remain experiment reported 
point discussed length section demonstrated analytically training set size irrelevant key experiments 
suffice say amount noise difficulty problems posed size training set excessive 
best simple evidence chosen appropriately naive sophisticated strategies normally operate experiments far extremes null perfect performance 
experiments preliminary experiments preliminary experiments demonstrate sophisticated strategy works expected 
class variable takes value value logically complemented probability simulate effect noise 
trials sophisticated strategy produces tree representing true relationship attribute class variables naive strategy recovers relationship trials 
trials naive sophisticated strategies choose trees equal predictive accuracies 
discrepant trials tree selected sophisticated strategy superior 
trials naive strategy chooses trees average accuracy compared maximum achievable trees selected sophisticated strategy 
second experiment differs definition class variable 
trials 
complemented probability simulate noise 
sophisticated strategy outperforms naive 
recovers true underlying relationship trials naive strategy recovers correct relationship just 
trials strategies choose trees equal accuracy 
discrepant trials sophisticated strategy chooses superior tree cases achieving average accuracy compared trees selected naive strategy 
results line previous reports overfit ting avoidance techniques 
cross validation determine tree pruned away eliminates branches reflect spurious patterns substantially increases predictive accuracy 
overfitting avoidance may decrease predictive accuracy catch effect depends critically choice model data generation simple sense represented small tree 
third experiment illustrates defining parity precisely odd number attributes take value features experiment 
case effect overfitting avoidance exactly opposite hope 
naive sophisticated strategies choose trees different accuracies trials discrepant cases tree chosen naive strategy superior 
superiority naive strategy significant level 
breiman demonstrate cross validation pruning strategy known digits recognition problem show outperforms naive strategy ignores overfitting 
result depends part fact optimal tree digits recognition problem quite small relative full tree constructed attributes 
experiment just reported indicates different problem chosen demonstration effect overfitting avoidance opposite 
easy misinterpret experiment section indicating just parity hard problem decision tree methods idea prune 
parity hard problem decision tree methods naive sophisticated strategies perform poorly third experiment 
hard problem achieve best performance possible case parity problem naive strategy leads significantly better performance sophisticated 
second point experiment described section pit pruned trees unpruned ones 
compares strategy prunes may may prune depending indications data 
question pays consider pruning guard overfitting sophisticated strategy fact nearly optimal tree just leaves full tree 
sophisticated training data naive strategy deciding decision tree induce 
answer experiments relative standing naive sophisticated strategies absolute relative distribution test problems 
problems parity example predominate naive strategy appear superior problems simple relationships predominate sophisticated strategy prevail 
returning coin flipping analogy may say overfitting avoidance confer definite benefit choosing double number flips simply favors certain hypotheses lowering threshold predicting heads 
parity experiment illustrates fact overfitting avoidance bias sense beneficial certain problem distributions detrimental 
case parity problem bias inappropriate overfitting avoidance leads degradation performance 
classification noise may decrease value overfitting avoidance overfitting avoidance commonly considered means coping noisy data objected level noise parity experiment low favor sophisticated strategy 
fact error rate corresponds noise level define noise probability random boolean value substituted true quite high traditional standards 
point case sophisticated strategy performs better relative naive parity problem rate errors class variable increases 
table shows results series variations parity experiment error rate takes values yielding noise levels 
second row repeats results previous section strategies choose trees different accuracies trials tree chosen naive strategy superior average accuracies trees chosen strategies cases 
table shows clearly classification noise increases accuracy trees chosen strategies differs frequently strategies disagree tree chosen ignoring possibility overfitting superior regardless level noise 
naive strategy average accuracy difference error superior naive sophisticated gross rate cases significance strategy strategy accuracy percent percent percent percent percent percent table effect classification noise parity problem sense increase classification noise decreases value avoiding overfitting sophisticated strategy produce inferior tree noise high 
course average accuracy columns suggest accuracies strategies converge error rate approaches maximum 
maximum produce trees average accuracy difference nil 
broad range error rates considered difference gross average accuracies consistently shows advantage percentage point naive strategy 
parity special case sophisticated strategy outperforms naive parity reverse true 
fourth experiment designed show simply anomaly 
experiment conditions previous definition class variable 
trial relation chosen random space boolean functions variables 
table shows pattern results line parity problem table 
error rates increase naive sophisticated strategies disagree level noise naive strategy proves superior 
accuracy trees chosen strategies approaches chance error rate approaches wide range naive strategy average accuracy difference error superior naive sophisticated gross rate cases significance strategy strategy accuracy percent percent percent percent percent percent table effect classification noise random boolean functions error rates studied difference gross accuracies consistently shows advantage percentage point naive strategy 
sense space possible true models anomalous 
boolean relations parity underlie data generation described best ignore overfitting inducing decision trees 
mean course true problems researchers generally tested decision tree induction methods 
fact published methods proven effective lends weight problems decision tree methods applied small special subclass 
effect overfitting avoidance representation dependent random experiments sections rely fixed representation scheme possible elements instance space 
suppose different scheme 
example define parity delta delta delta 
may just vectors represent instance space elements 
element heretofore identified vector identified vector 
recall parity contains odd number values 
scheme relationship parity may represented succinctly decision tree induction parity problem conducted representation relationship discovered quite simple precisely simple fact relationship section expect naive sophisticated strategies produce trees higher predictive accuracies section representation 
straightforward example known principle representation critically influences power induction strategies 
purposes key point example new representation increases performance naive sophisticated strategies parity problem affects relative standing 
representation naive strategy performs best representation parity problem exactly analogous problem section reverse true 
rephrase point case best ignore possibility overfitting second pays guard 
effect overfitting avoidance just problem dependent previous sections shown representation dependent 
sense boolean functions relating attribute values favor naive strategy representation schemes may favor naive strategy sophisticated strategy superior fixed representation 
may illustrated variation experiment 
original define purposes induction different representation employed trial 
representations chosen random 
possible bit schemes mapping vectors error rate held constant 
trials kind naive sophisticated strategies produce trees identical predictive accuracies 
discrepant cases tree chosen naive strategy superior 
cases trees chosen naive strategy attain average accuracy compared trees chosen sophisticated see 
schemes consider particular scheme specified column table vectors listed fixed order column corresponding vectors second 

ways permuting vectors second column gives possible 
superiority naive strategy significant level 
strategy 
sophisticated strategy preferable representation random representations decisively favor naive strategy 
evidently true representations employed published demonstrations 
fact overfitting avoidance successful cases suggests representations employed distinguished minority adopted 
random vs random functions glance random specified boolean function equivalent choosing boolean function random result just merely result section 
consider results new experiment identical previous class variable defined second experiment 
case sophisticated strategy remains superior random strategies choose trees differing accuracy trials tree chosen sophisticated strategy better 
discrepant cases trees chosen naive strategy attain average accuracy compared trees chosen sophisticated strategy 
explanation difference cases simple 
represent boolean functions tabular form table functions attributes table consist rows 
represent function exactly half rows value column 
terms new attributes alter property long vectors correspond vectors likewise function value appear times column 
contrast randomly chosen boolean function may number ts column 
general greater purity column closer consisting wholly ts fs simpler smallest corresponding tree average randomly chosen representation greater advantage sophisticated strategy averaged possible representations 
table illustrates comparing results experiments just described third rows respectively number values 
case class variable defined representation superiority sophisticated strategy significant level 
corresponds node tree 
extreme easy see sophisticated strategy superior 
number naive strategy average accuracy difference superior naive sophisticated average values cases strategy strategy accuracy percent gamma percent gamma percent gamma percent gamma percent table effect purity results follow directly fact overfitting defined outset article depends notion complexity representation dependent 
overfitting occurs complex model outperforms simple training data true accuracy complexity tree model depends representation attribute data 
overfitting avoidance scheme serves bias models considered simple representation time bias models considered complex representation 
know bias induced application particular overfitting avoidance scheme particular representation scheme appropriate reason expect sophisticated strategy outperform naive 
incidental section purity negative entropy values taken boolean function serve measure complexity decision tree induction 
intuitively think simpler reverse true proposed measure 
reflects fact corresponds simpler tree representations relatively conducive application simplicity biased tree induction strategy cart 
proposed measure general need take account likelihood attribute vectors experiments reported equally 
means example expect gauge simplicity underlying function applied problem prevalence various classes training set 
effect size training set instance space noted earlier experiments reported instance space possible attribute vectors extensively sampled training sets consisting cases 
natural question ask degradation performance due overfitting avoidance observed experiments due abundance training data relative size instance space 
fact conditions section boolean functions equally superiority naive strategy holds regardless size training set number attributes 
naive strategy superior conditions simply particular overfitting avoidance strategy chosen representative purposes pruning strategies broad class includes best known machine learning researchers 
main purpose section argument demonstrating claims 
article meant provide insight intuition argument fully formal 
clear stated point perfectly rigorous 
general points worth noting argument 
experiment section discussed size training set affect relative standing naive sophisticated strategies experiments reported article 
second affect relative standing size training set affect ease relative standing may confirmed empirically 
training data equivalently attributes naive sophisticated strategies frequently learn perform null level number discrepant trials decrease necessary run total trials prove significant difference strategies 
training data fewer attributes strategies frequently learn perfectly inducing true underlying relationship decrease number discrepant trials increase total number necessary prove significant difference 
noted section size training set relative number attributes conveniently chosen experiments allowing significant differences tested strategies empirically demonstrated large number trials 
overfitting avoidance decreases predictive accuracy functions equally analytic demonstration argument runs follows 
conditions section boolean functions chosen random function 
suppose represent functions table row possible attribute vector final column showing assigned class may implement random selection boolean functions repeatedly flipping fair coin marked side results flips fill column table 
delta delta delta gamma ttt delta delta delta tt ttt delta delta delta tf 
filled fff delta delta delta ft fff delta delta delta ff table tabular representation ary boolean function clear approach implement equiprobable distribution possible functions implementation behave identically flipping fair coin determine column 
conditions section training cases consist attribute vector class designation 
class designation normally true value listed filled column table fixed probability complement suppose column table filled flipping coin described consider trying induce true value arbitrary row example 
training cases observed clearly know value row equal chance suppose training case attribute vector ttt delta delta matching vector second row table class designation help determine value row 
answer clear training case yields information value row 
gives certain amount probabilistic information value second row outcome second coin flip 
second coin flip entirely independent determined value row 
knew basis training cases attribute vector ttt delta delta value second row table certainly know value row 
key point 
cases training set relevance inducing value row attribute vector ttt delta delta 
independence coin flips determining true values implies single induction problem really faced separate problems row 
easy see row 
optimal induction strategy determining value row example guess coincides whichever class associated attribute vector ttt delta delta training set 
simulated noise strategy fallible best 
ttt delta delta designated class training cases place weight induction true value certainly reason guess 
short optimal strategy conditions previous section rote learning regardless size training set number attributes amount noise corrupting attribute vector remember class observed prediction 
note second key point argument just extended forced single class prediction attribute vector appeared training set true value equally perspective difference guess 
implicitly assuming error rates range corresponding noise levels 
higher error rates simply reverse meanings column 
fixed group attribute vectors 
example assign class vectors ttt delta delta ttt delta delta purposes prediction best choose class observed examples vectors training set 
independence coin flips class observed attribute vectors irrelevant 
far said decision trees 
suppose considering tree induction strategy properties 
begins building tree fit training set 

may choose prune yielding smaller tree 
case tree chosen class predictions assigned match majority class training cases leaf 

positive chance tree chosen differs class predictions 
consider leaf tree set attribute vectors routed 
third property class predicted set chosen optimal strategy noted previous paragraph 
pruned produce single value predicted class attribute vectors value assigned expected accuracy new tree respect attribute vectors unchanged assigns complementary class value vectors expected accuracy reduced assignment optimal 
argument holds leaf change predicted class associated attribute vectors effect pruning expected accuracy null change expected effect negative 
expected accuracy better expected accuracy fourth property worse 
strategy chooses strictly superior satisfies properties chooses related points points worth noting argument just 
section data generation model attribute vectors equally applies fixed level noise class value properties referred argument just 
may assume distribution attribute vectors allow noise vary time parts instance space affecting pruning strategies conforming description decrease predictive accuracy 
key assumptions just functions equally noise affects class variable 
second point argument depends heavily independence class value chosen various attribute vectors tempting conclude independence primarily responsible anomalous degradation performance due overfitting avoidance 
fact independence culprit 
experiment section argument show prior reason believe functions common techniques overfitting avoidance expected degrade performance noise affects solely class variable 
sense independence ignorance key 
know advance functions underlie data generation expect overfitting avoidance help 
useful reason believe implicit bias appropriate 
sections suggest clearly bias appropriate 
overfitting avoidance helps underlying function simple sense may represented small tree hurts underlying function complex 
degradation caused pruning functions equally due property independence simplifies argument fact great majority possible functions complex 
create literally number distributions functions share property implying property independence issue independence degree bias particular overfitting avoidance technique appropriate distribution problems 
reiterate assuming class values chosen independently simplifies argument essential 
long complex functions predominate sufficiently analogous argument show overfitting avoidance strategies broad class described decrease expected predictive accuracy 
section leaves open important questions 
happens noise affects attributes class variable 
second overfitting avoidance techniques improve performance inasmuch necessary assume distribution attribute vectors training test data 
indirectly take account prior knowledge relative performance various models abandon overfitting avoidance take prior knowledge account directly 
third arguments led researchers believe overfitting avoidance increases expected predictive accuracy reconciled argument section 
questions addressed respectively sections 
attribute noise value overfitting avoidance experiments section simulated noise applied class variable attribute values treated noise free 
fact practical application domains attribute noise may common 
effect classification noise observed attributes instance completely determine class 
words classification attribute noise may arise measurement reporting errors consequence mingers calls residual variation class variable 
section experiments attribute noise illustrate main points attribute noise different effect classification noise value overfitting avoidance second noise may affect attribute values different ways distinct consequences 
experiments accuracy trees selected naive sophisticated strategies measured empirically basis fresh examples generated conditions training data 
result trees equal accuracies excluded analysis preceding sections may appear different included 
experiment adds attribute noise conditions described section 
trial class variable defined boolean function chosen random space possibilities 
class variable complemented probability simulate effect classification noise 
addition individual attribute values example tree gives correct prediction attribute vectors accuracy gives correct prediction attribute vectors 
test set contains vector trees may appear different accuracies 
mented probability simulate effect attribute noise 
result trees chosen naive strategy superior original conditions attribute noise tips balance opposite direction 
trials strategies choose trees different empirical accuracies tree chosen sophisticated strategy performs better 
superiority sophisticated strategy significant level 
presence attribute noise kind substantially increases value avoiding overfitting 
intuitively level noise attribute labelling tree node increases usefulness distinction effected node decreases 
eventually harm done splitting training data portions outweighs possible benefit treating parts instance space differently 
experiment just reported provides possible justification overfitting avoidance 
worth noting outcome experiment depends critically particular model attribute noise employed 
model assumes things errors independent 
errors correlated results may quite different 
second experiment conditions attribute vectors individual values complemented probability 
note probability individual value complemented expected number attribute value errors unchanged 
difference probability error dependent occurrence errors attribute vector 
conditions trees different empirical accuracies chosen trials sophisticated strategy picks apparently superior tree naive strategy picks apparent winner 
result close possible tie 
section showed naive strategy superior absence attribute noise effect adding clearly increase value avoiding overfitting 
comparison previous experiment shows corre trees chosen naive sophisticated strategies attain average accuracies respectively cases differ 
small difference average accuracy due high error rates equivalent noise levels affecting attribute class variables 
weaken statistical significance reported simply reflects fact strategies appear superior trials fact equally matched 
tradeoff analyzed ff discussed section 
fact trees chosen naive strategy attain higher average accuracy cases chosen sophisticated strategy lated form attribute noise favorable sophisticated strategy independent model drawn attention decision tree research 
third experiment errors introduced replacing attribute vectors random vectors 
setting probability substitution maintain probability individual attribute value complemented 
experiment naive strategy proves clearly superior producing tree higher empirical accuracy cases different empirical accuracies observed 
result significant level 
accuracy averaged discrepant cases trees chosen naive strategy versus trees chosen sophisticated strategy 
intuitively replacing attribute vector random alternative quite new attribute vector assigning random class 
third type attribute noise acts kind classification noise ought surprised tend produce conditions favorable overfitting avoidance 
models attribute noise illustrated section plausible sense may accurately reflect effect noise practical application domains quite different consequences deciding adopt overfitting avoidance strategy 
conclude effects attribute noise distinct classification noise far problem overfitting concerned effects depend critically manner noise affects attribute values 
recap experiments reiterate overfitting avoidance strategies form bias effect performance determined degree bias appropriate inherent advantage guarding overfitting 
data reflecting simple relationships reported examples bias simplicity appropriate overfitting avoidance improve performance simple best fit approach 
hand difference attribute vector replacement affect prevalence values class variable 
classification noise kind applied article adjusts 
reverse true data reflecting sufficiently complex relationships 
classification noise reinforces negative effect bias data kind 
despite fact overfitting avoidance considered means coping difficulties caused residual variation forms classification noise complex problems increases chances overfitting avoidance result choice inferior tree 
senses problems complex 
complex relationships overwhelming majority possible relationships fixed representation scheme 
second simple problems fixed representation scheme complex respect overwhelming majority alternative representations 
reported success overfitting avoidance strategies empirical trials indicates trials conducted special subclass problems special subclass possible representation schemes 
presence attribute noise may serve justify overfitting avoidance noise affects attribute values independently 
various types correlated attribute noise may inconclusive effect act overfitting avoidance inappropriate 
overfitting avoidance machine learning researchers speak overfitting avoidance techniques purpose distinguish structure noise training data equivalently determine complexity appropriate model data generation process 
ought clear impossible distinction determination basis training data 
models differing complexity account equally observed data data help choose 
take simple example suppose training set consists observations collected table 
huge number models account equally observations omega data tell prefer 
choose rely knowledge inherent plausibility various models domain knowledge example suggest true model course fresh data choose models developed basis previous observations invalidate point just 
distinction discussed length page 
quite complicated involve attribute vector associated class value 
observation table observation training set fact data supplemented way models equally effective accounting observations taken follows formally straightforward standard application bayes rule 
want know number data generation models responsible observed data pick model maximizing 
proportional models equally plausible explaining yielding equal values know relative values decide 
matter sophisticated ingenious algorithms extracting information training set help 
practice machine learning research overfitting avoidance proceeded reverse true 
section examines theoretical intuitive empirical support opposing position see reconciled arguments evidence 
theoretical considerations occam razor reason overfitting avoidance construed inherent improvement form bias theoretical results clarity paragraph focuses problem choosing model problem choosing model highest expected accuracy 
analogous argument complex gist 
decide models perform equally training data knowledge distribution true models problems generating environment 
misunderstood 
known occam razor provides case point 
gist sticking simple models learning algorithm may guarantee performance training data carry substantially fresh data 
formal mathematical arguments valiant pac model demonstrate probabilistic performance guarantee sort algorithms choose restricted model space basis amount training data available considering complex models training set large 
purposes critical point say 
claim restricting consideration simple models increases expected predictive accuracy 
suppose algorithm considers simple models finds ma performs training data 
suppose algorithm considers wider class models finds mb performs better data 
results occam razor may tell sure ma performance fresh data mb say relative expected performance models 
second depends property simple models relatively 
roughly speaking authors assign bit string identifier model associate complexity model length identifier 
may models complexity argument depends critically fact actual assignment identifiers models designation certain models simpler arbitrary 
claim intended claim special importance models human expert naturally think simple 
short mistake interpret sound theoretical arguments demonstrating small trees intuitively simple models perform fresh data complex alternatives better training data 
william occam original statement entities multiplied unnecessarily construed mean things equal simple occam razor heavily earlier pearl point quite explicitly philosophical viewpoint essential note cases examined role simplicity incidental analysis 
gotten identical results lc pearl language describing simple models functions complexity bounded sublanguage substitute arbitrary sublanguage equal number functions 
complexity error rate typical error rate curve hypotheses predictive 
occam razor simply address interpretation occam statement provides justification complex models predictive accuracy goal 
bias variance argument important example theoretical argument misinterpreted bias variance analysis breiman ff 
case authors take responsibility part resulting confusion 
analysis correct useful far goes go far introductory remarks suggest 
depending remarks details theoretical derivations misled 
cart program considers series increasingly pruned versions tree fit greedily training data 
breiman note empirical trials cart true error rates follow pattern shown complexity trees measured number leaf nodes 
error rates decrease rapidly increasing complexity slowly increase 
error rate complex trees twice lowest achievable rate 
bias variance analysis introduced heuristic attempt understand points fact concentrates bounding error rate explaining shape observed error rate curves 
decision tree may viewed partition attribute space leaves receive mutually disjoint exhaustive subsets possi ble attribute vectors 
inasmuch accuracy best possible tree opt partition wrong class assigned partition elements leaves 
breiman call factor bias second variance 
precisely bias tree structure associated partition classes assigned optimally leaves 
breiman consider difference error rates opt bias measure bias suboptimality partition difference error rates bias measure variance suboptimality class assignments 
introducing terms breiman suggest bias decreases variance increases sequence trees cart considers trees ordered simplest complex 
paraphrasing basic claims entering theoretical derivations state variance term increases slowly number leaves partition elements increases bounded slow growth factor 
fact mathematical arguments show bias decreases variance bounded show attempt show variance increases certainly account upward sloping portion shaped error rate curve showing variance increase faster bias decreases sufficient levels complexity 
sequence trees considered cart partitions associated complex trees refinements partitions associated complex trees implies bias decrease increase increasing complexity 
total error rate increases increasing complexity empirical trials variance increased trials error rate curve shaped variance level complexity increased faster bias decreased 
proved true general opposite true 
simple case point true class values assigned independently element instance space 
argued section finer partition case lower expected error rate 
words case counter example proving expected error rate curve shaped 
shaped error rate curve expected clearly optimal meaning bias completely different article 
breiman appealing statisticians machine learning researchers terms bias variance analogy meanings regression analysis 
choose complex trees cart considers 
analysis give reason expect shaped curve 
curve shaped analysis show intended show statistical means employed cart locate low point increasing predictive accuracy choosing tree suboptimal training set 
noted section statistical methods sophisticated inadequate purpose 
large sample results minimum description length methods final theoretical argument invoked justify overfitting avoidance barron cover proof adherence minimum description length principle modeling guarantees models derived training data converge true data collected 
conjunction quinlan rivest practical suggestions minimum description length criterion inducing decision trees data proof easily demonstrating better choose small tree minimizes quinlan rivest measure description length larger performs better training set 
fact quinlan rivest state quite clearly application minimum description length principle tree induction amounts simply application bias 
point effect precisely assuming certain prior distribution true models applying straightforward bayesian modeling techniques 
minimum description length approach effective bias appropriate domain application 
barron cover result proves desirable property minimum description length approach long run size training set tends infinity 
demonstrate approach superior training sets finite size bias effected minimizing description length inherently inherently bad 
intuitive arguments favor overfitting avoidance noisy data overfitting general reason overfitting avoidance construed inherent benefit application bias intuitively necessary means dealing noisy data 
roughly speaking intuitive argument runs way 
patterns noisy data reflect true underlying structure arise merely chance consequence noise 
induction procedure simply strives achieve optimum performance training set construct model captures kinds patterns 
go far fitting noise structure attempting extract information data data really contains 
reason adopt mechanism distinguishing real spurious patterns dealing noisy data 
basic flaws argument 
assuming induced model includes component reflecting true model reflecting chance patterns training set follow training data distinguish 
fact reasons set section definitely extra information sort data distinguish real spurious structure induced model fresh data moment independent information indicating models inherently plausible 
words true induction noisy domains necessarily leads overfitting impossible improve performance strictly statistical means training data sophisticated fashion avoid problem 
data tell patterns real 
fundamental flaw assumption naive fit possible strategy lead overfitting underfitting 
contrary true underlying structure complex relative size training set induction procedure short producing model simple complex 
observation training set best known decision tree induction methods produce tree leaves apply methods training sets size domains trees leaves typically represent true underlying structure normally data noisy 
example contrived illustrates important point need procedure deleting parts induced model adding parts depends entirely mix problems induction methods face 
restate expand point data help decide superfluous lacking 
way knowing true structure tends simple complex 
course data distinguish real apparent structure consists fresh cases 
divide available data portions obtain series pruned versions large tree second portion effectively choose 
tree results training set twophase fashion compared effectively tree obtained directly training set fitting far possible 
fits training set better data prove superior 
data tell patterns trust phase approach yields inherent benefit 
cross validation fresh data true resampling approaches cross validation scheme employed cart sophisticated strategy article 
crossvalidation generally understood provide benefit testing fresh data cost collecting experiments illustrate fact choosing models basis performance estimates amounts applying bias may improve degrade performance relative naive strategy depending problem distribution 
contrast choosing models basis performance true fresh data yield provable increase expected accuracy regardless problem distribution 
analytic argument section extends validity claim cross validation provides inherent benefit decision tree induction schemes particular employed sample sophisticated strategy 
conditions section cross validation approach results choice tree performing best training cases inferior simple pick apparent best alternative 
single example suffices show effect cross validation approach depends problem distribution increases accuracy circumstances may sure decrease accuracy 
statistical significance line intuitive reasoning overfitting avoidance depends statistician notion significance 
inasmuch statistical significance meant indicator reality apparent hypothesized empirical patterns possible improve performance eliminating parts induced model yield statistically significant increases fit 
immediate problems idea necessity fresh data correct model noise valid significance calculations 
fundamental problem statistical significance provides wrong kind information interested optimizing performance 
draw analogy occam razor offers probabilistic guarantees performance training set carry new cases simple models considered say expected performance improved restriction observed 
consider wider space models may sure chosen model perform new cases perform 
likewise rely parts model yield statistically significant increases performance guaranteed probabilistically parts yield increases performance model applied fresh data 
include model parts yielded statistically insignificant increases performance may sure affect performance reason think effect negative 
example suppose consider tree pruned version find performs better training set increase accuracy statistically significant 
means improvement plausibly due chance effects noise real advantage consequence ought surprised find performs fresh cases 
tell surprised continue outperform fact tells model achieve higher accuracy 
short reliance statistical significance means reducing uncertainty performance optimizing 
empirical evidence despite said may objected practice overfitting avoidance necessary beneficial empirically speaking overfitting avoidance proven success case artificial problems considered article experience real data justifies application various known pruning techniques 
problem line argument unfounded leap particular practical real data problems overfitting avoidance techniques demonstrated general class problems 
true overfitting avoidance improved performance practical problems tackled decision tree researchers date may selection problems techniques applied 
holte jensen weiss evidence data sets typically employed testing machine learning induction algorithms reflect simple relationships 
certainly mistake take empirical proof practice real data tends reflect simple relationships 
reason wrong conclude empirical evidence techniques favoring simple relationships generally superior practical applications 
value techniques practice depends problems practitioners choose expect practitioners confine application overfitting avoidance techniques problems qualitatively tried 
key point real data determines overfitting avoidance improve degrade performance characteristics complexity underlying relationship manner noise affects attribute values 
important applications share characteristics overfitting avoidance beneficial widespread assumption overfitting avoidance provides inherent benefit know little today determining characteristics 
result say new problems sufficiently old ones justify application overfitting avoidance techniques 
safe predict indiscriminate techniques sooner lead performance degradation real data problems practical importance 
fact conducted article written pruning methods cart shown decrease predictive accuracy important practical significance article served practical purpose just suggested causes researchers study conditions existing overfitting avoidance techniques expected increase predictive accuracy induced models 
important general lesson drawn observation overfitting avoidance bias 
noted classification data sets employed machine learning researchers reflect simple relationships 
inasmuch simple models appropriate procedure effects bias simplicity improve performance removing statistically insignificant parts model minimizing description length limiting consideration short rules optimizing cross validated performance 
improvement due statistical significance information theory resampling overfitting avoidance indirect application appropriate bias 
techniques decrease performance relative greedy naive strategy apply problems sufficient complexity 
intended message article wrong existing induction techniques 
domain application universal may generally understood certainly broad important 
key point attention misplaced regard best useful techniques 
community concentrated statistical significance information theory overfitting avoidance practical successes due appropriate application bias 
important exercise caution applying existing overfitting avoidance techniques urgent understand expected help 
critical message techniques rely indirect application bias expect better applying directly 
successes overfitting avoidance due implicit reliance key domain characteristics able improve performance domains overfitting avoidance proved quite successful relying characteristics explicitly designing techniques take full advantage 
real data cases drawn standard machine learning database repository 
related idea overfitting avoidance form bias implicit buntine bayesian perspective problem inducing decision trees 
early study quinlan effect noise decision tree induction related reported quinlan concerned effect noise single induction strategy effect relative value alternative strategies 
empirical comparison study mingers cited 
argument article discourage interpretation study indicating pruning strategy better worse relative performance determined choice test suite reflect relative performance problems known essentially similar 
article partly examination minimal overfitting avoidance problem considered 
followup conducted article submitted publication reported 
tom peter clark francesco bergadano discussions motivated reported robert holte wray buntine william cohen extensive comments draft versions article 
andrew barron thomas cover 
minimum complexity density estimation 
ieee transactions information theory july 
blumer andrzej ehrenfeucht david haussler manfred warmuth 
occam razor 
information processing letters 
leo breiman jerome friedman richard olshen charles stone 
classification regression trees 
wadsworth brooks pacific grove california 
wray buntine 
theory learning classification rules 
phd thesis university technology sydney 
cestnik ivan bratko 
estimating probabilities tree pruning 
machine learning ewsl 
springer verlag 
conover 
practical nonparametric statistics 
john wiley new york 
robert holte 
simple classification rules perform datasets 
technical report tr department computer science university ottawa april 
david jensen 
induction randomization testing analysis large data sets 
phd thesis washington university sever institute technology december 
john mingers 
expert systems rule induction statistical data 
journal operational research society 
john mingers 
empirical comparison pruning methods decision tree induction 
machine learning 
tim niblett ivan bratko 
learning decision rules noisy domains 
editor research development expert systems iii 
cambridge university press 
judea pearl 
connection complexity credibility inferred models 
international journal general systems 
ross quinlan 
effect noise concept learning 
michalski jaime carbonell tom mitchell editors machine learning artificial intelligence approach volume chapter 
morgan kaufmann 
ross quinlan 
simplifying decision trees 
international journal man machine studies 
ross quinlan ronald rivest 
inferring decision trees minimum description length principle 
information computation 
cullen schaffer 
deconstructing digit recognition problem 
submitted mlc 
cullen schaffer 
sparse data effect overfitting avoidance decision tree induction 
submitted aaai 
cullen schaffer 
overfitting decrease prediction accuracy induced decision trees rule sets 
machine learning ewsl 
springer verlag 
valiant 
theory learnable 
communications acm 
weiss nitin indurkhya 
reduced complexity rule induction 
proceedings th international joint conference artificial intelligence pages 

