extended stochastic complexity minimax relative loss analysis kenji yamanishi theory nec laboratory real world computing partnership media research laboratories nec miyazaki ku kawasaki japan ccm cl nec jp 
concerned problem sequential prediction hypothesis class continuously prediction strategies 
effective performance measure minimax relative cumulative loss rcl minimum worst case difference cumulative loss prediction algorithm best assignment hypothesis class 
purpose evaluate minimax rcl general continuous hypothesis classes general losses 
derive asymptotical upper lower bounds minimax rcl show match ln error ln dimension parameters hypothesis class sample size constant depending loss function 
show cumulative loss attaining minimax rcl asymptotically coincides extended stochastic complexity esc extension rissanen stochastic complexity sc decision theoretic scenario 
derive non asymptotical upper bounds minimax rcl parametric nonparametric hypothesis classes 
apply analysis regression problem derive worst case cumulative loss bounds date 
minimax regret start minimax regret analysis sequential stochastic prediction problem 
alphabet discrete continuous 
consider simplest case finite 
observe sequence takes value stochastic prediction algorithm performs follows round assigns probability mass function past sequence probability mass function written conditional probability jy assignment receives outcome suffers logarithmic loss defined ln jy 
process goes sequentially 
note specified sequence conditional probabilities fp jy observing sequence length suffers cumulative logarithmic loss ln jy jy 
goal stochastic prediction cumulative loss small possible 
introduce set prediction algorithms call hypothesis class evaluate cumulative loss algorithm relative 
sample size define worst case regret relative hypothesis class rm def sup ln jy inf ln jy means worst case difference cumulative logarithmic loss best assignment single hypothesis define minimax regret sample size rm inf rm infimum taken stochastic prediction algorithms 
note minimax regret analysis require statistical assumption mechanism consider worst case respect sequences 
notice stochastic prediction algorithm specifies joint probability mass function jy minimax regret rewritten rm inf sup ln sup shtarkov showed minimax regret attained joint probability mass function normalized maximum likelihood defined follows sup sup minimax regret amounts rm sup specifically consider case joint distribution product probability mass function belonging parametric hypothesis class fp dimensional compact set ir maximum likelihood estimator arg max 
rissanen proved condition central limit theorem holds uniformly rm asymptotically expanded follows rm ln ln ji jd def ln denotes fisher information matrix goes zero uniformly respect goes infinity 
sequence negative log likelihood joint probability mass function attains minimax regret called stochastic complexity sc relative denote sc 
sc ln ln ln ji jd sc thought extension shannon information sense measures information data sequence relative single hypothesis relative class hypotheses 
purpose overview results extend minimax regret analysis ways 
extend general decision theoretic scenario hypothesis class class real valued functions class probability mass functions prediction loss measured terms general loss functions square loss hellinger loss logarithmic loss 
motivated fact real problems line regression pattern recognition prediction deterministically variety loss functions distortion measure prediction 
analyze minimax relative cumulative loss rcl extension minimax regret general losses 
minimax rcl investigated community computational learning theory restricted specific cases case general loss functions hypothesis class finite case hypothesis class continuous specific loss functions square loss logarithmic loss specific hypothesis classes bernoulli model linear regression model 
offers universal method minimax rcl analysis relative general continuous hypothesis classes general loss functions 
derive asymptotical upper lower bounds minimax rcl show match ln error ln dimension parameters hypothesis class sample size constant depending loss function 
introduce extended stochastic complexity esc show esc approximation minimax solution rcl 
relation esc minimax rcl analogue sc minimax regret 
gives unifying view minimax rcl minimax regret analysis 
way extension introduce method non asymptotical analysis asymptotical result effective sample size sufficiently large 
sample size necessarily large real situations theoretical bounds hold sample size practically useful 
non asymptotical bounds minimax regret rcl derived continuous hypothesis classes specific losses 
take new approach derive non asymptotical bounds minimax rcl parametric non parametric hypothesis classes general losses 
rest organized follows section gives formal definition minimax rcl 
section overviews results asymptotical analysis minimax rcl 
section gives non asymptotical analysis minimax rcl 
section shows application analysis regression problem 
minimax rcl positive integer subset ir call domain 
call range 
set probability mass functions call decision space 
set write element ir loss function 
sequential prediction algorithm performs follows round receives outputs predicted result basis 
receives correct outcome suffers loss 
defines sequence maps ff hypothesis class set sequential prediction algorithms 
definition 
sample size hypothesis class subset depending sequential prediction algorithm define worst case relative cumulative loss rcl rm def sup min output tth round 
define minimax rcl rm inf rm infimum taken sequential prediction algorithms 
assume prediction algorithm knows sample size advance 
consider special case set probability mass functions loss function logarithmic loss ln call case probabilistic case 
easily check probabilistic case minimax rcl equivalent minimax regret 
consider case prediction deterministically 
give examples loss functions case 
square loss ln ln entropic loss hellinger loss ln logistic loss ln 
loss function define def def respectively 
assumption assumption loss function satisfies twice continuously differentiable respect 
define def sup 

example entropic loss square loss hellinger loss 
case condition necessarily required 
asymptotical results introduce notion esc order derive upper bounds minimax rcl 
definition 
probability measure hypothesis class loss function sequence define extended stochastic complexity esc relative def ln yy df sc defined cumulative logarithmic loss attains minimax regret esc defined cumulative loss attains minimax rcl 
possible get explicit form minimax solution rcl general losses 
turn section esc tight upper bound cumulative loss attains minimax rcl error ln lemma 
assumption exists sequential prediction algorithm cumulative loss upper bounded 
lemma proven vovk aggregating algorithm analysis 
order connection esc minimax rcl focus specific case parametric class prediction algorithm written sequence functions belongs parametric class ff dimensional compact set ir case esc relative written ln yy prior probability density 
assumption assumption conditions hold define matrix def fi fi fi largest eigenvalue dm sequence lim lim 
minimum loss estimator defined arg min sup sup dm nm def dm mg def ir dm mg 
sufficiently large vol nm vol vol lebesgue volume lemma gives asymptotical upper bound worst case esc 
lemma 
subset assumption min ln ln rc lim uniformly 
note probabilistic case righthand side coincides sc error 
main technique prove laplace method approximates integral gaussian integral neighborhood minimum loss estimator 
proof sketched follows proof 
sample size ffi dm dm assumption 
minimum loss estimator ffi ffi neighborhood 
denote 
letting ffi ln ffi exp ln ffi exp ln ffi exp ln ln ffi exp point ffi attains minimum 
notations follow assumption 
ffi ir ffi ffi exp ffi exp see inequality 
plugging letting dm go infinity yield 
ut combining lemmas leads asymptotical upper bound minimax rcl 
theorem 
assumptions rm ln ln rc definition set 
order investigate tight derive asymptotical lower bound minimax rcl 
theorem 

entropic loss square loss regularity condition rm ln furthermore regularity conditions general addition assumptions holds 
note current forms regularity conditions required general losses complicated remain simplified 
theorems corollary relating esc minimax regret 
formally summarized follows corollary 
im def sup min conditions theorems lim ji rm ln corollary shows esc thought cumulative loss attains minimax rcl error ln 
corresponds fact sc cumulative logarithmic loss attains minimax regret 
gives rationale esc natural extension sc 
non asymptotical results log loss case bounds asymptotical sense effective sample size sufficiently large 
section derives upper bounds minimax rcl tight hold sample size 
overview results cesa bianchi lugosi probabilistic case logarithmic loss 
metric space class joint probability mass functions decomposed metric sup fi fi ln jy ln jy fi fi ae cardinality smallest subset ae 
theorem shows non asymptotical upper bound minimax regret 
theorem 
class rm inf ln ln ffi dffi deriving bound don require regularity condition central limit theorem condition required satisfied 
holds regardless parametric parametric 
theorem leads special case non asymptotical upper bound minimax regret parametric hypothesis classes 
corollary 
consider class positive constants ln ln ln kc rm ln ln ln note condition holds classes parametrized smoothly bounded subset ir general loss cases decision theoretic case general losses logarithmic loss derive non asymptotical upper bounds minimax rcl different manner cesa bianchi lugosi 
investigate case hypothesis class parametric 
theorem 
assumptions rm ln ln ef definition set 
proof 
main technique prove discretize hypothesis class appropriate size apply aggregating algorithm discretized hypothesis class 
important issue choose number discrete points 
subset size discretization scale sup min means discretization scale component roughly uniform constant factor 
quite natural requirement discretization 
arg min arg min 
write relative cumulative loss algorithm letting output tth round see letting ff aggregating algorithm ag lemma see ln ln leads sup ln taylor expansion argument mf plugging yields sup ag ln mf minimum attained mf plugging optimal size choosing smallest yield 
ut investigate case hypothesis class nonparametric approximated sequence parametric hypothesis classes 
theorem 
fh sequence classes ae ae dimensional parametric hypothesis class 
hypothesis class ff sup inf hk sup jl ff assumption depending ff rm am ff ln ff ff ln ff ln ef proof 
fix rm rm sup inf sup jl ln ln ef mc ff setting ln ff inequality yields 
ut condition means worst case approximation error ff 
eq 
regarded special case ff infinite 
minimax rcl regression apply results sections regression problem 
consider case hypothesis class class linear functions feature vector distortion measure square loss 
case investigated linear regression lr scenario statistics 
analysis different conventional ones regards assumed classical lr noise additive generated gaussian distribution don probabilistic assumption target distribution hypothesis class perform worst case analysis terms worst case rcl 
additionally emphasize consider regression problem line prediction scenario batch learning scenario classical lr 
algorithms investigated classical lr take linear forms feature vector don restrict consider nonlinear prediction algorithms hypothesis class linear functions 
fx 
hypothesis class ff known class linear predictors 
square loss function 

ff ff ff 
describe aggregating algorithm denoted ag 
tth round ag takes input outputs ln jd ln jd jd denote th component pth component pth component respectively ffi ffi set ff upper bound worst case rcl ag rm ag ln mk ln ff note parameters theorem 
rm ln ln ke vovk derived similar non asymptotic upper bound worst case rcl aggregating algorithm linear predictors 
bound matches ln term 
kivinen warmuth proposed gradient descent algorithm gd exponentiated gradient algorithm sequential prediction algorithms linear predictors 
notice outputs gd linear ag linear showed sup gd sup ln eq shows upper bound worst case rcl ag ln smaller gd sufficiently large parameter size fixed 

cesa bianchi lugosi minimax regret log loss general classes experts proc 
colt 

freund predicting binary sequence optimal biased coin proc 
colt 

haussler kivinen warmuth tight worst case loss bounds predicting expert advice computational learning theory eurocolt springer 

kivinen warmuth exponentiated gradient versus gradient descent linear predictors ucsc crl 

opper haussler worst case prediction sequence log loss proc 
ima workshop information coding distribution springer 

rissanen stochastic complexity statist 
soc 
vol 

rissanen stochastic complexity statistical inquiry world scientific singapore 

rissanen fisher information stochastic complexity ieee trans 
inf theory vol 

shtarkov universal sequential coding single messages probl inf transmission 

vovk aggregating strategies proc 
colt morgan kaufmann 

vovk competitive line linear regression proc 
advances nips mit press 

yamanishi decision theoretic extension stochastic complexity applications learning ieee tran 
inf 
theory 

yamanishi minimax relative loss analysis sequential prediction algorithms parametric hypotheses proc 
colt acm press pp 
