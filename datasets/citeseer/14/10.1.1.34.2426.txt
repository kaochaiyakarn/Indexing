limited discrepancy search william harvey matthew ginsberg cirl university oregon eugene oregon ginsberg cs uoregon edu problems practical interest solved tree search methods carefully tuned successor ordering heuristics guide search regions space contain solutions 
problems heuristics lead directly solution 
limited discrepancy search addresses problem heuristics fail 
intuition failing heuristic succeeded small number wrong turns way 
binary tree height ways heuristic single wrong turn gamma ways 
small number wrong turns overcome systematically searching paths differ heuristic path small number decision points discrepancies 
limited discrepancy search backtracking algorithm searches nodes tree increasing order discrepancies 
show formally experimentally limited discrepancy search expected outperform existing approaches 
practice search problems spaces large search exhaustively 
find solutions searching small fraction space relying carefully tuned heuristics guide search regions space contain solutions 
problems heuristics lead directly solution time 
consider heuristics fail 
focus attention procedures tree search 
objective simple search problems heuristically ordered successors develop search procedure find solution time limit existing methods chronological backtracking iterative sampling langley outline follows section discuss existing algorithms 
limited discrepancy search lds introduced section compared existing techniques section 
discuss variations lds believe useful solving realistic problems section 
conclude presenting experimental results section 
existing strategies consider tree search problem successor ordering heuristic leads directly solution 
problems common practice areas ai research planning scheduling smith cheng wilkins heuristic satisfied algorithm follows heuristic just gives heuristic fails lead solution algorithm call samp harvey smith cheng performance samp satisfactory confronted question search algorithm 
iterative sampling backtracking candidates 
iterative sampling iterative sampling langley isamp simple idea random paths probes root eventually path leads solution discovered 
node path successors selected random expanded 
successors selected random goal node dead reached 
path ends dead isamp starts new probe root 
algorithm samples replacement uniform chance finding goal node particular probe 
provided goal node space follows probability find goal node increases uniformly number probes grows limit 
iterative sampling shown effective problems solution density high crawford baker performance fallback procedure samp questionable ignores successor ordering heuristic 
heuristic key solving problem despite low solution density expect iterative sampling effective 
backtracking alternative fallback procedure simply backtrack chronologically samp fails 
experiments section scheduling show approach provides little improvement samp analysis mistakes provides explanation harvey reasonable chance early samp path mistake selecting successor goal nodes entire subtree 
early mistake successor subtree committed subsequent decisions difference 
subtree mistake large chronological backtracking spend allowed run time exploring empty subtree returning decision matters 
counting heuristics find goal node small fraction search space chronological backtracking puts tremendous burden heuristics early search relatively light burden heuristics deep search 
unfortunately problems heuristics reliable early search making decisions reduce problem size heuristics reliable 
uneven reliance heuristics chronological backtracking making best heuristic information 
discrepancies return search problems successor ordering heuristic 
intuition samp fails heuristic probably led solution wrong turns got track 
ought possible systematically follow heuristic decision point 
fails follow heuristic decision points 
number wrong turns small find solution fairly quickly approach 
call decision points follow heuristic discrepancies 
limited discrepancy search embodies idea iteratively searching space limit number discrepancies allowed path 
iteration limit zero discrepancies just samp 
iteration searches possibilities discrepancy 
algorithm shown 
assume search tree binary 
successors function returns list zero successors heuristic preference 
discrepancy limit 
iteratively call lds probe increasing time 
lds probe depth search traversal tree limiting number discrepancies eventually experimented biasing random selection successors heuristic results suggest viable approach harvey 
lds probe node goal node return node successors node null return nil return lds probe result lds probe second gamma result nil return result return lds probe lds node maximum depth result lds probe node result nil return result return nil limited discrepancy search 
reaches maximum depth tree lds probe searches entire tree exhaustively 
search guaranteed find goal node exists guaranteed terminate goal nodes 
iteration lds probe limits number discrepancies restricting search nodes exactly discrepancies iteration nodes considered previous iterations see 
iterative techniques final iteration far away expensive redundancy significant factor complexity search 
shows trace lds exhaustively searching full binary tree height 
heuristic orders nodes left right 
pictures show paths depth order 
dotted lines open circles represent nodes backtracked previous picture trace followed examining pictures sequence 
counting black circles gives total number nodes expanded search 
general number nodes expanded lds discrepancy limit bounded iteration fringe nodes path fringe node expanding nodes 
large cost single iteration dominates summed costs preceding ones 
comparison existing methods practice course typically time search space exhaustively 
know likelihood finding solution various methods amount time willing wait 
question precise formalizing mean heuristic wrong turn 
wrong turns simplicity consider case full binary tree 
children choice point ffl delta delta ffi delta delta delta delta delta delta ffi delta delta delta delta delta delta ffi ffl ffl delta delta ffi delta delta delta delta delta delta ffi ffl delta delta ffi delta delta delta delta delta delta ffi delta delta delta delta delta delta ffi ffl ffl ffl delta delta ffi ffl delta delta ffi delta delta delta delta delta delta ffi delta delta delta delta delta delta ffi ffl ffl delta delta ffi delta delta delta delta delta delta ffi ffl delta delta ffi delta delta delta delta delta delta ffi delta delta delta delta delta delta ffi ffl ffl ffl ffi iteration ffl delta delta ffi delta delta delta delta delta delta ffi delta delta delta delta delta delta ffi ffl ffl delta delta ffi delta delta delta delta delta delta ffi ffl delta delta ffi delta delta delta delta delta delta ffi delta delta delta delta delta delta ffi ffl ffl ffl delta delta ffi ffl delta delta ffi delta delta delta delta delta delta ffi delta delta delta delta delta delta ffi ffl ffl delta delta ffi delta delta delta delta delta delta ffi ffl delta delta ffl ffl ffi iteration ffl delta delta ffi delta delta delta delta delta delta ffi delta delta delta delta delta delta ffi ffl ffl delta delta ffi delta delta delta delta delta delta ffi ffl delta delta ffl ffl delta delta ffi ffl delta delta ffl delta delta ffl ffi iteration ffl delta delta ffl delta delta ffl delta delta ffl iteration execution trace lds 
assumed order heuristic preference 
assume choice point goal node subtree probability heuristic probability child goal node subtree 
child goal child goal choice point children 
case heuristic wrong turn putting children wrong order 
notion wrong turn closely related mistake probability 
define bad node node goal nodes subtree 
define mistake bad node parent bad 
mistake probability probability randomly selected child node bad harvey heuristic orders successors randomly heuristic probability complement mistake probability gamma heuristic better random selection gamma shows possibilities node children 
theta indicates bad node solid dot node 
probability node class classes left children class class parent bad 
mistake probability half probability node class classes mistake child class experimentally appears generally fairly constant search trees harvey omega omega omega omega omega omega omega omega omega omega omega omega theta ffl ffl ffl theta theta theta theta ffl ffl ffl ffl possibilities node children 
order simplify analyses assume constant experimental evidence tends increase somewhat search tree heuristics accurate deep nodes shallow ones 
chance finding solution random path depth isamp simply gamma heuristics assuming constant samp probability finding solution path 
observation allows estimate running samp large training set problems domain interest 
success rate samp training set 
probability success samp small training set may impractically large get reliable estimate 
problems small 
heuristics developed job shop scheduling shown yield probability nearly small research problems smith cheng earlier experimental problems harvey standard csp heuristics yield success rate 
larger scheduling problems vaessens success rate samp sophisticated heuristics operations research keep samp competitive search techniques cheng smith theoretical results specific values figures show theoretical probability success function time iterative sampling isamp chronological backtracking dfs limited discrepancy search lds various heuristic probabilities graphs show probability finding solution number probes define probe search dead reached isamp lds simply search additional nodes dfs 
number probes limited height tree combinatorics solving problem discrepancy limit intractable 
analyses biased dfs depth search highest heuristic probabilities shown 
shows results problem height mistake probability 
problem fringe nodes goals 
solution density expect iterative sampling sample fringe nodes finding solution combinatoric manipulations underlying figures quite involved appear harvey 
number goals gamma number probes probability success pi lds pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi lds lds ffi lds ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffl isamp ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl dfs problem height 
exact 
accounts problem solution density fairly easy problem 
takes delta nodes average find solution iterative sampling 
expected number probes slightly number probes required chance finding solution delta 
practice may interested number nodes required find solution higher probability success 
number nodes required iterative sampling success probability problem delta 
compare performance limited discrepancy search 
lds probability success just eleven probes nodes 
savings nearly factor depends heuristic order successors correctly times successors mistake 
gammam heuristic orders successors correctly half time better random selection 
curve completely obscured isamp curve shows performance lds slightly worse iterative sampling conditions 
heuristic orders nodes correctly times 
curves show expected performance lds increases dramatically better dfs curve rises marginally probability fringe node goal 
futility dfs clearer deeper search shown 
problem height approximately nodes 
density solutions theta gamma iterative sampling needs probes nodes chance suc expected number probes gamma example expected number coin flips getting heads takes single coin flip probability getting heads 
number probes required achieve success probability log gammas log gamma gammam probability fringe node examined dfs goal simply number probes probability success pi lds pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi lds lds ffi lds ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffl isamp ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl dfs problem height 
cess 
earlier problem heuristic orders nodes correctly times lds similar chance just probes nodes savings orders magnitude iterative sampling 
savings similar success probability desired 
higher probabilities success orders magnitude savings doubtful doubtful graph suggest 
discrepancy iteration ends probes 
probes discrepancy iteration paths common likelihood probes succeeds failed small 
probes discrepancy iteration begins explore fresh paths 
consequently expect lds curve rise steadily graph leaves 
variations extensions reason focused analyzing early iterations limited discrepancy search believe practice iterations matter 
earlier argued intuitive grounds important iterations 
take position practice iterations don matter 
reason objective maximize probability finding solution number nodes better things nodes iterations limited discrepancy search 
section discusses promising choices 
involve combinations techniques depend search space properties difficult quantify discussion precise earlier sections 
view limited discrepancy search tool combination techniques craft effective search procedure real world problem 
remarked earlier unfortunately unable verify essentially theoretical claim combinatorics overwhelm 
variable reordering constraint satisfaction problems sat problems formulated tree search fixing order variables instantiated determining order dynamically search progresses 
case node search tree choice point possible instantiations particular variable 
effective heuristic solve problem limit discrepancy chosen variable order may solve problem discrepancy different variable order 
wrong turn instantiations heuristic variable order may follow unit propagation second 
suggests simple technique repeating discrepancy iteration lds different variable orders 
variable order determined dynamically may suffice simply search different variable iteration 
similar technique improves efficiency depth search see section 
different heuristics multiple heuristics exist particular problem try repeating discrepancy limit iterations lds different heuristics 
heuristic unlucky wrong turns problem heuristic may 
general hard heuristic may hard 
lds effective way give heuristic reasonable chance switching 
combining lds bounded backtrack search lds combined bounded backtrack search bbs harvey produce algorithm count small discrepancies fail quickly discrepancy limit 
algorithm viewed modifying heuristic avoid choices seen fail fixed lookahead 
algorithm appears appendix 
combined lds bbs algorithm outperforms lds bbs job shop scheduling problems 
fact lds bbs appears algorithm choice systematic backtracking strategies domain 
compelling theoretical argument 
mistakes result quick immediate failures 
heuristic wrong turns fewer wrong turns exceed backtrack bound 
adding bounded backtrack enables limited discrepancy search discover solutions discrepancy limit number wrong turns exceed backtrack bound potentially reducing number required iterations 
cost lds iteration grows factor savings substantial 
added cost backtrack bound relatively insignificant 
adding backtrack bound node cost factor 
backtrack bound costs factor small cheaper cost additional iteration 
upper bound conservative heuristic assumption mistakes 
local optimization lds problems scheduling lds search neighborhood existing solution 
discrepancy iteration lds modified path previous best solution heuristic 
depth discrepancy algorithm diverges previous solution follows heuristic remaining decisions 
path ends solution better previous best adopted immediately stored contender basis iteration 
variation lds requires measure goodness solution 
scheduling problems schedule length appropriate measure 
searching schedule takes time successful produces schedule takes time set standard lds iterations repeated lower time bound optimization variant lds applied consider variations previous schedule differ discrepancy 
non boolean problems comment possible extension lds constraint satisfaction problems involving variables domain sizes larger 
focussed boolean problems part natural encoding jobshop scheduling problems boolean smith cheng technique obviously applied wider setting 
variety choices need discrepancy search include alternate value variable violates heuristic single attractive choice 
number nodes expanded increase factor iteration need take view 
experimental results experimental results comparing limited discrepancy search chronological backtracking iterative sampling set thirteen job shop scheduling problems taken survey operations research techniques vaessens problems involves scheduling tasks involved producing widgets manufacturing setting job needs performed particular machine takes time indicating jobs need completed started 
effective encoding problems focusses directly resource contentions arise jobs require machine introduce variable ik indicating job job uses machine smith cheng alternatively time bound adjusted binary division 
single iteration lds decision procedure failure find schedule time bound proof schedule exists 
problems obtained sending message ic ac uk 
variables boolean search space far smaller variables start times various jobs 
experimental formulated problem csp loose bound schedule length 
iteratively repeated search decreasing bound time slightly length schedule 
recorded length best schedule function total number nodes expanded reaching final cutoff nodes problem see 
node limit problem thousands avg 
percent optimal ffl lds ffl ffl ffl ffl ffl ffl ffl fflffl ffl ffl fflffl ffl fflfflffl fflffl ffl ffl fflffl fflfflffl fflfflffl fflffl ffl ffi isamp ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi dfs comparison dfs iterative sampling 
node cutoff algorithm completed number iterations problem resulting schedules various lengths 
evaluated schedules lengths measuring percent optimal length problem 
took average percent optimal function measure performance search algorithms 
lds clearly superior chronological backtracking iterative sampling 
chose particular benchmark compare results scheduling research artificial intelligence operations research 
benchmark contemporary scheduling programs score range optimal vaessens performance implementation match best programs appears range 
scheduling implementation uses general csp heuristics weak scheduling standards 
rel optimal lengths taken best reported lengths november 
ative larger pool programs implementation appears comparable limited discrepancy search disastrous chronological backtracking iterative sampling 
limited discrepancy search relies heavily heuristics expect combination lds accurate heuristics dedicated scheduling programs best performance 
experiments vein way 
experimented variety nonsystematic algorithms harvey depth search restarts iterative broadening bounded backtrack search scored optimal benchmark outperformed pure limited discrepancy search slightly lds optimal 
nonsystematic methods rely heuristics lds believe lds benefit significantly improvements heuristics 
commented section limited discrepancy search combined bounded backtrack search 
results shown 
node limit problem thousands avg 
percent optimal ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl fflffl ffl ffl ffl ffl ffl ffl ffl fflfflffl ffl ffl ffl ffl ffl ffl ffl fflffl fflffl fflffl fflffl ffl ffl ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffiffi ffi ffi ffi ffiffi ffi ffi ffi ffiffi ffi ffi ffiffi ffi ffi ffiffi ffi ffiffi ffiffiffi ffi ffi ffiffi ffiffiffi ffi adding bounded backtrack improves bbs 
combination limited discrepancy bounded backtrack search best performance systematic nonsystematic methods tested 
optimal performance node backtrack difference best worst algorithms may appear slight substantial domain problems expected exponentially difficult approaches crossover point corresponding optimality crawford auton 
parameter depth backtrack allowed checking heuristics led dead ends 
bound respectable compared dedicated scheduling programs 
shown theoretically experimentally limited discrepancy search effective way exploit heuristic information tree search problems 
effective chronological backtracking iterative sampling attempted explain 
scheduling problems experiments large contemporary research standards large relative types sizes scheduling problems useful solve real world 
complexity scheduling challenge scaling research problems real world problems met quickly advances heuristics evolution brute force methods 
expect techniques depend heuristics recover gracefully searching alternatives heuristics fail methods choice solving real world problems 
supported air force office scientific research contract arpa rome labs contracts 
authors andrew baker ari onsson jimi crawford david etherington valuable feedback course research 
combined lds bbs algorithm lb probe node look goal node return successors node reverse count child count break gamma hresult lb probe child look max height result nil return hresult height look count count return nil lds bbs node look maximum depth hresult lb probe node look result nil return result return nil cheng smith cheng smith 
generating feasible schedules complex metric constraints 
proc 
national conference artificial intelligence 
crawford auton crawford auton 
experimental results crossover point satisfiability problems 
proc 
th aaai 
crawford baker crawford baker 
experimental results application satisfiability algorithms scheduling problems 
proc 
th aaai 
harvey harvey 
search job shop scheduling 
technical report cirl tr cirl university oregon 
harvey harvey 
nonsystematic backtracking search 
phd thesis stanford university 
langley langley 
systematic nonsystematic search strategies 
artificial intelligence planning systems proceedings international conference 
smith cheng smith cheng 
heuristics constraint satisfaction scheduling 
proc 
eleventh national conference artificial intelligence 
vaessens vaessens aarts lenstra 
job shop scheduling local search 
technical report eindhoven university technology 
wilkins wilkins 
practical planning extending classical ai planning paradigm 
morgan kaufman san mateo california 
