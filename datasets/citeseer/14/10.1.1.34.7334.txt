discretization works naive bayesian classifiers appear th international conference machine learning icml chun nan hsu iis sinica edu tw institute information science academia sinica taipei city taiwan hung ju huang gis cis edu tw department computer information science national tung university city taiwan wong iis sinica edu tw institute technology county taiwan explains known discretization methods entropy bin naive bayesian classifiers continuous variables regardless complexities 
methods usually assume discretized variables dirichlet priors 
perfect aggregation holds show generally wide variety discretization methods perform insignificant difference 
identify situations discretization may cause performance degradation show happen known methods 
empirically test explanation synthesized real data sets obtain confirming results 
analysis leads lazy discretization method simplify training naive bayes 
new method perform known methods experiment 

learning naive bayesian classifier naive bayes langley data important technique data classification 
spite simplicity naive bayes constantly outperforms competing algorithms experiments reported literature 
remarkably kdd cup top naive bayes 
domingos pazzani provided interesting analysis reason naive bayes perform independence assumption violated 
reported experiment compared naive bayes classical learning algorithms large ensemble real data sets 
result showed naive bayes significantly outperform algorithms 
interesting respect result achieved version naive bayes uses naive discretization method handle continuous data 
method usually referred bin divides domain continuous variable equal width bins 
simple method considered inferior complex methods proposed surprisingly simple methods perform improvement complex methods 
dougherty 
conducted empirical study comparing performance known discretization methods concluded discretization methods significantly outperform version naive bayes assumes normal continuous variables 
discretization methods performed approximately entropy method fayyad irani performed slightly better data sets 
kohavi sahami compared entropy method error method considered dougherty 
methods outperformed normal version significant difference performance 
previous provided informal discussion explained discretization methods naive bayes regardless complexities 
explanation phenomenon 
key explanation perfect aggregation property dirichlet distributions 
discretized continuous variable usually assumed dirichlet prior 
perfect aggregation implies estimate class conditional probabilities discretized intervals arbitrary accuracy 
explains naive bayes discretization effective approximating broad range probability distributions continuous data 
achieve optimal performance probability corresponding interval simulate role true density classification 
allows identify situations discretization may cause performance degradation 
show situations happen zero loss wide variety discretization methods including known methods 
methods produce similar performance data sets regardless complexities 
empirically test explanation synthesized real data sets obtain confirming results 
analysis leads lazy discretization method 
experimental results show average new method performs equally known methods 
remainder presents explanation lazy discretization method 

dirichlet distributions naive bayesian classifier discrete variables discretized continuous variables usually assumed dirichlet prior 
section reviews important properties dirichlet distributions 
dirichlet distribution formally defined follows definition random vector variate dirichlet distribution parameters ff density gamma ff gamma ff ff gamma gamma gamma delta delta delta gamma ff gamma delta delta delta distribution denoted ff ff ff ff 
beta distribution univariate dirichlet distributions usually denoted beta ff ff 
closure properties dirichlet distributions critical analysis 
properties greatly simplify computation moments dirichlet distribution bayesian analysis 
suppose random vector dirichlet distribution ff ff ff ff wilks subvector nm variate dirichlet distribution dm ff ff ff nm ff gamma ff nj call subvector lemma 
wilks sum subset phi beta distribution parameters phi ff ff gamma phi ff ff ff ff delta delta delta ff called sum subset lemma 
important property dirichlet distribution conjugate multinomial sampling heckerman 
property basically states posterior distribution dirichlet observation dirichlet 
formally fy data set outcomes trials denotes number trials turning outcome prior distribution dirichlet distribution parameters ff likelihood function dj follows multinomial distribution posterior distribution jd dirichlet distribution parameters ff ff 
similarly beta distribution conjugate binomial sampling 
expected value jd ff ff 
perfect aggregation phi subset probability interest sum variables phi phi suppose prior distribution dirichlet distribution ff ff ff ff 
straightforward application bayesian approach training data update prior distribution obtain posterior distribution jd dirichlet distribution ff ff ff ff posterior distribution qjd derived jd 
subvector lemma sum lemma qjd beta distribution beta phi ff ff gamma phi ff properties dirichlet distribution exists simpler alternative compute qjd 
derive prior distribution probability interest prior distribution 
convert training data new set training data terms computing sum observations interest update prior distribution obtain posterior distribution qjd 
show general case qjd qjd 
multinomial likelihood function dj implies trials obtaining data set independent likelihood function jq follow binomial distribution 
sum lemma prior distribution beta distribution beta phi ff ff gamma phi ff dirichlet distribution 
beta distribution conjugate binomial sampling posterior distribution qjd beta distribution parameters phi ff ff gamma phi ff 
exactly equation 
property true dirichlet distribution derived called perfect aggregation wong 
example suppose interested probability showing odd number throwing die 
probability die shows number trial number trials die shows trials 
probability interest represented straightforward approach derive distribution qjd data fy alternative approach fn obtain result 
implication perfect aggregation dirichlet distributions estimate posterior probability union disjoint events need estimate probabilities individual events 

discretization naive bayes naive bayes classifies feature vector selecting class maximizes posterior probability cjx xjc variable xjc density class denote vector elements parameters density xjc 
bayesian learning framework assume uncertain variable heckerman learned training data set 
estimation heart training naive bayes 
suppose discrete variable possible values 
principle class label data vector dictates probability value appropriate multinomial distribution parameters set probabilities possible value jc 

choose dirichlet distribution parameters ff ff prior 
train data set update jc expected value jc ff cj ff number training examples belonging class cj number class examples dirichlet distribution conjugate multinomial sampling training posterior distribution dirichlet updated parameters ff cj property allows incrementally train naive bayes 
practice usually choose jaynes prior ff ff xjc cj nc training data set small yields xjc impedes classification 
avoid problem popular choice ff known smoothing laplace estimate cestnik bratko 
continuous variable conventional approach assume xjc oe oe normal distribution 
case training involves learning parameters oe training data 
approach shown effective discretization xjc normal john langley discretization 
generally discretization involves partitioning domain intervals pre processing step 
treat discrete variable possible values conduct training classification 
precisely th discretized interval 
training classification naive bayes discretization jc estimate xjc equation continuous variable 
equivalent assuming discretization class conditional density dirichlet prior 
call assumption dirichlet discretization assumption 
apparently assumption holds known discretization methods including bin entropy see dougherty comprehensive survey 
dirichlet discretization assumption reasonable implicit assumption described 
xjc true probability density function xjc 
assuming xjc integrable 
discretized interval true probability jc xjc dx 
choosing equivalent sample size ff dirichlet parameter corresponding random variable jc ff xjc dx 
call assumption partition independence assumption 
partition independence assumption discretization dirichlet prior 
partition independence assumption implies interval dirichlet parameter corresponding interval depends area curve xjc independent shape curve interval 

performance discretization methods section discuss factors affect performance different discretization methods 
density continuous variable known point density xjc equation classification 
discretize variable probability jc discretized interval 
probability assumed dirichlet prior due dirichlet discretization assumption 
perfect aggregation holds dirichlet distributions jc estimated arbitrarily close real jc 
proved follows 
xjc true density xjc jc xjc dx delta delta disjoint sub interval arbitrary accuracy achieved increasing number sub intervals perfect aggregation need estimate delta 
estimate jc directly obtain result 
partition independence assumption result independent shape curve xjc 
discretization methods accurately approximate distribution naive bayes assumes normal continuous variables 
shows discretization effective 
suppose true densities xjc shown equation bayes rate optimal classification accuracy achievable classifier duda hart 
discretization achieve optimal classification accuracy jc simulate role xjc distinguishing class gives high density class gives low density 
precisely jc largest classes require jc largest classes 
performance degradation may occur 
consider simple case continuous variable normally distributed classes 
plot curves conditional densities shown 
domain divided regions decision boundaries correspond intersection points curves ties occur largest conditional densities 
optimal classification pick class xjc largest pick class different different sides decision boundary 
suppose discretization method creates interval contain decision boundary 
values interval different sides decision boundary classifier pick different class value 
jc values 
values sides decision boundary misclassified 
minimize distortion boundaries intervals contain decision boundaries close decision boundaries intervals narrow possible 
hand widths intervals contain decision boundary large possible 
widths affect performance intervals narrow number examples training data set intervals small allow accurate estimation jc 
case discretization cause performance degradation 
loose bound derived follows 
size training data set 
number examples fall interval binomial distribution parameters jc expected value jc 
want estimate accurate digits need jc 
bound loose sufficient illustrate idea 
entropy method rd see dougherty designed avoid narrow intervals performance slightly better 
summary avoid performance degradation discretization method partition domain continuous variable intervals cut points close decision boundaries minimize distortion due discretization width large cover sufficient examples 
analysis extended multivariate cases right choice cut points widely spread 
naive bayes takes variables account simultaneously impact wrong discretization variable absorbed variables zero loss performance measure 
due way naive bayes learns correct classification example depends interval containing values example completely independent regions discretized 
result wide variety discretization methods similar performance general 

empirical evidence section report empirical verification analysis synthesized real data sets 

conditional distributions synthesized data controlled experiments synthesized artificial data set composed examples different classes 
class number examples 
example continuous attributes class label 
values generated normal distributions fn class fn class fn class 
attribute divided domain intervals shown contain decision boundaries variable 
intersection points peak values classes 
experiment investigated distance cut points intersection points affect performance 
initialized small width boundaries extended constant percentage initial width intersection points kept middle regions sides intersection points intervals width 
resulting discretization ran fold cross validation obtain average accuracy 
plots results 
predicted wide accuracies drop rapidly 
second experiment see factors may result significant changes accuracies addition width kept unchanged randomly partitioned intervals 
repeated random partition times ran fold cross validation repetition 
variance resulting accuracies 
average accuracy maximum minimum accuracies respectively 
repeated experiment time entire domains randomly partitioned 
contrast variance resulting accuracies jumps 
average accuracy maximum minimum accuracies respectively 
width accuracy 
accuracies drop increase widths intervals contain intersection points 
results show widths fixed partitions intervals tiny impact classification accuracy 
third experiment show equal width bins bin effective cases 
fact number bins extreme performance significantly different 
analysis predicts number intervals intervals contain intersection points performance generally poor 
number bins increases accuracies improve reach plateau 
number bins extremely large widths bins small intervals accuracies drop gradually 
curve accuracy function number bins appear side 
fast curve drops depend size training data set 
small data set cause accuracy drop faster 
conducted experiment data sets synthesized data set examples subset examples 
resulting curves confirms prediction 
experiments real data sets select real data sets uci ml repository blake merz experiments 
attributes data sets continuous 
experiment similar third experiment described section 
experiment partitioned continuous variable equal width bins increased number equal width bins 
shows results 
number bins accuracy 
experimental results match prediction fewer data accuracies drop faster number bins increases 
just expectation curves data sets match pattern especially large data sets 
accuracy figures obtained fold cross validation 
variances accuracies data sets surprisingly small 
long numbers equal width bins large small accuracies approximately 
curves data sets glass iris liver wine variances accuracy appear depend size data sets 
general small data sets examples relatively large variances 
discussed earlier examples sufficient accurate probability estimation 
data set glass particularly zigzag curve number examples data set small 
addition data set classes produce intersection points accuracies sensitive discretization 
experiment focus domains entirely continuous 
performed experiment data sets mixed domains 
curves mix domain data sets generally match pattern 
examined data sets follow pattern continuous variables exceptional data sets impact classifying example 
remove continuous variable exceptional data sets accuracies classification 
continuous variable data set nearly dormant classification distributions different classes variable similar 
naive bayes bins achieve high accuracy 
number bins german glass vehicle waveform wine number bins breast iris liver pima yeast 
accuracy results increasing number equal width bins 
results exclude possibility carefully determining cut points performance improved drastically 
conducted similar experiment entropy method 
entropy method selects cut point training data boundary classes switch fayyad irani method select cut point near intersection point 
data set recursively called entropy heuristic determine cut points reported fold cross validation accuracy 
recursion cut points selected resulting intervals previous calls applying mdl stopping criterion 
shows resulting curves 
horizontal axis indicates levels recursion invoked 
th level recursion continuous variable cut points 
data containing interval class cut point selected 
level german glass vehicle waveform wine level accuracy breast iris liver pima yeast 
accuracy results entropy determine cut points 
number levels recursions correspond number intervals created variable may partitioned different number intervals 
resulting curves show accuracies different levels recursions surprisingly similar data sets 
shapes match pattern curves 
confirms discretization methods perform similarly select radically different sets cut points 

lazy discretization question possible improvement analysis 
section describes preliminary investigation question 
propose lazy discretization method perfect aggregation 
method waits test data determine cut points continuous variable 
method produces pair cut points surrounding value test datum variable 
creates interval denoted leaves region untouched 
training data estimate jc equation estimate classify test datum 
method invoke different instantiations determine cut points 
example select pair cut points value middle width interval width intervals created bin 
call instantiation lazy bin 
similarly lazy entropy lazy bin log discretization method derived perfect aggregation properties dirichlet distribution 
suppose partition independence assumption holds 
sum subset lemma dirichlet distribution xjc beta prior parameters ff xjc dx ff gamma xjc dx class ff equivalent sample size 
perfect aggregation estimate jc counting examples 
words need check exact value examples value may simplify training effort 
empirically compared lazy bin bin entropy normal version naive bayes real data sets 
ran fold cross validation data set reported average standard deviation accuracies 
table gives results reveal lazy method perform known discretization methods 

explanation discretization works naive bayes perfect aggregation property dirichlet distributions known discretization methods perform regardless complexities 
explanation perfect aggregation ensures naive bayes discretization effectively approximate distribution continuous variable 
new lazy discretization method showed works equally 
note result may applicable learning algorithms decision trees 
plan investigate analysis shed light handling continuous variables general bayesian networks 
table 
accuracies naive bayes different discretization methods 
data set lazy bin bins entropy continuous breast sigma sigma sigma sigma german sigma sigma sigma sigma glass sigma sigma sigma sigma iris sigma sigma sigma sigma liver sigma sigma sigma sigma pima sigma sigma sigma sigma vehicle sigma sigma sigma sigma waveform sigma sigma sigma sigma wine sigma sigma sigma sigma yeast sigma sigma sigma sigma average win loss research reported supported part national science council taiwan 
nsc 


graphical belief modelling 
new york chapman hall 


perfect aggregation reliability models bayesian updating 
doctoral dissertation department industrial engineering university wisconsin madison madison wi 
blake merz 

uci repository machine learning databases 
cestnik bratko 

estimating probabilities tree pruning 
machine learning ewsl european working session learning 
berlin germany springer verlag 
domingos pazzani 

optimality simple bayesian classifier zero loss 
machine learning 
dougherty kohavi sahami 

supervised unsupervised discretization continuous features 
machine learning proceedings twelfth international conference 
san francisco ca morgan kaufmann 
duda hart 

pattern classification scene analysis 
new york wiley sons 
fayyad irani 

multi interval discretization continuous valued attributes classification learning 
proceedings thirteenth international joint conference artificial intelligence pp 

heckerman 

tutorial learning bayesian networks 
jordan ed learning graphical models 
boston kluwer academic publishers 
levin 

aggregation model ecosystem perfect aggregation 
ecological modeling 
john langley 

estimating continuous distributions bayesian classifiers 
proceedings eleventh annual conference uncertainty artificial intelligence pp 

kohavi sahami 

error entropy discretization continuous features 
proceedings second international conference knowledge discovery data mining pp 

portland 
langley iba thompson 

analysis bayesian classifiers 
proceedings tenth national conference artificial intelligence pp 
pp 

san jose ca aaai press 


kdd cup presentation 
wilks 

mathematical statistics 
new york wiley sons 
wong 

perfect aggregation dependent bernoulli systems bayesian updating 
doctoral dissertation department industrial engineering university wisconsin madison madison wi 
