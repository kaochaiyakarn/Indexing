efficient clustering high dimensional data sets application matching andrew mccallum zy whizbang 
labs research henry street pittsburgh pa usa mccallum cs cmu edu kamal nigam school computer science carnegie mellon university pittsburgh pa usa cs cmu edu lyle ungar computer info 
science university pennsylvania philadelphia pa usa ungar cis upenn edu important problems involve clustering large datasets 
naive implementations clustering computationally expensive established ecient techniques clustering dataset limited number clusters low feature dimensionality small number data points 
methods eciently clustering datasets large ways example having millions data points exist thousands dimensions representing thousands clusters 
new technique clustering large highdimensional datasets 
key idea involves cheap approximate distance measure eciently divide data overlapping subsets call canopies 
clustering performed measuring exact distances points occur common canopy 
canopies large clustering problems impossible practical 
reasonable assumptions cheap distance metric reduction computational cost comes loss clustering accuracy 
canopies applied domains variety clustering approaches including greedy agglomerative clustering means expectation maximization 
experimental results grouping bibliographic citations sections research papers 
canopy approach reduces computation time traditional clustering approach order magnitude decreases error comparison previously algorithm 
categories subject descriptors pattern recognition clustering information storage retrieval information search retrieval clustering 
unsupervised clustering techniques applied important problems 
clustering patient records health care trends discovered 
clustering address lists duplicate entries eliminated 
clustering astronomical data new classes stars identi ed 
clustering documents hierarchical organizations information derived 
address applications variety clustering algorithms developed 
traditional clustering algorithms computationally expensive data set clustered large 
di erent ways data set large large number elements data set element features clusters discover 
advances clustering algorithms addressed eciency issues partially 
example kd trees provide ecient em style clustering elements require dimensionality element small 
algorithm eciently performs means clustering nding initial starting points ecient number clusters large :10.1.1.157.392
algorithms eciently data set large senses millions elements thousands features thousands clusters 
introduces technique clustering cient problem large ways 
key idea perform clustering stages rst rough quick stage divides data overlapping subsets call canopies rigorous nal stage expensive distance measurements points occur common canopy 
di ers previous clustering methods uses di erent distance metrics stages forms overlapping regions 
rst stage extremely inexpensive methods nding data elements near center region 
proximity measurement methods inverted index commonly information retrieval systems ecient high dimensions nd elements near query examining small fraction data set 
variants inverted index real valued data 
canopies built approximate distance measure second stage completes clustering running standard clustering algorithm rigorous expensive distance metric 
signi cant computation saved eliminating distance comparisons points fall common canopy 
hard partitioning schemes blocking kd trees algorithm tolerant inaccuracies approximate distance measure create canopies canopies may overlap 
theoretical standpoint guarantee certain properties inexpensive distance metric show original accurate clustering solution recovered canopies approach 
words inexpensive clustering exclude solution expensive clustering lose clustering accuracy 
practice small accuracy increases due combined usage distance metrics 
clustering canopies applied di erent underlying clustering algorithms including greedy agglomerative clustering means expectation maximization 
presents experimental results apply canopies method greedy agglomerative clustering problem clustering bibliographic citations sections computer science research papers 
cora research search engine gathered referring unique papers text segment existing vocabulary dimensions 
multiple citations di er ways particularly way author editor journal names abbreviated formatted ordered 
goal cluster citations sets refer article 
measuring accuracy hand clustered subset consisting nd canopies approach speeds clustering order magnitude providing modest improvement clustering accuracy 
full data set expect speedup orders magnitude 

efficient clustering canopies key idea canopy algorithm greatly reduce number distance computations required clustering rst cheaply partitioning data overlapping subsets measuring distances pairs data points belong common subset 
canopies technique uses di erent sources information cluster items cheap approximate similarity measure household address proportion words common address expensive accurate similarity measure detailed eld string edit distance measured tuned transformation costs dynamic programming 
divide clustering process stages 
rst stage cheap distance measure order create number overlapping subsets called canopies 
canopy simply subset elements data points items approximate similarity measure distance threshold central point 
signi cantly element may appear canopy element appear canopy 
canopies created intention points appearing common canopy far apart possibly cluster 
distance measure create canopies approximate may guarantee property allowing canopies overlap choosing large distance threshold understanding properties approximate distance measure guarantee cases 
circles solid outlines show example overlapping canopies cover data set 
method canopies may created described subsection 
second stage execute traditional clustering algorithm greedy agglomerative clustering means accurate distance measure restriction calculate distance points appear canopy assume distance nite 
example items trivially placed single canopy second round clustering degenerates traditional unconstrained clustering expensive distance metric 
canopies large overlap large number expensive distance measurements avoided amount computation required clustering greatly reduced 
furthermore constraints clustering imposed canopies include traditional clustering solution possibilities canopies procedure may lose clustering accuracy increasing computational eciency signi cantly 
state formally conditions canopies procedure perfectly preserve results traditional clustering 
underlying traditional clusterer means expectation maximization greedy agglomerative clustering distance cluster measured centroid cluster clustering accuracy preserved exactly traditional cluster exists canopy elements cluster canopy 
perform greedy agglomerative clustering clustering distance cluster measure closest point cluster clustering accuracy preserved exactly cluster exists set canopies elements cluster connect canopies 
example data clusters canopies cover 
points belonging cluster colored shade gray 
canopies created method outlined section 
point selected random forms canopy consisting points outer solid threshold 
points inside inner dashed threshold excluded center forming new canopies 
canopies formed similarly note optimality condition holds cluster exists canopy completely contains cluster 
note overlap points excluded canopy 
expensive distance measurements pairs points canopies far fewer possible pairs data set 
practice dicult create inexpensive distance measures nearly satisfy canopy properties 
creating canopies cases user canopies technique able leverage domain speci features order design cheap distance metric eciently create canopies metric 
example data consist large number hospital patient records including diagnoses treatments payment histories cheap measure similarity patients diagnosis common 
case canopy creation trivial people common diagnosis fall canopy 
sophisticated versions take account hierarchical structure diagnoses icd codes include secondary diagnoses 
note people multiple diagnoses fall multiple canopies canopies overlap 
small number features suce build canopies items clustered patients thousands features 
example bibliographic citations clustered cheap similarity metric looks names authors year publication text article available 
times especially individual features noisy may want features item 
section describes distance metric method creating canopies provides performance cases 
concreteness consider case documents items words document features method broadly applicable problems similar structure 
cheap distance metric fast distance metrics text search engines inverted index 
inverted index sparse matrix representation word directly access list documents containing word 
want nd documents close query need explicitly measure distance documents collection need examine list documents associated word query 
great majority documents words common query need considered 
inverted index eciently calculate distance metric number words documents common 
distance metric create canopies follows 
start list data points order distance thresholds 
thresholds set user experiments selected cross validation 
pick point list approximately measure distance points 
extremely cheap inverted index 
put points distance threshold canopy 
remove list points distance threshold repeat list empty 
shows canopies created procedure 
idea inverted index applied highdimensional real valued data 
dimension discretized number bins containing balanced number data points 
data point effectively turned document containing words consisting unique bin identi ers dimension point 
worried edge ects boundaries bins include data point document identi ers bin point bins side 
cheap distance measure number bin identi ers points common 
similar procedure previously success 
canopies greedy agglomerative clustering greedy agglomerative clustering gac common clustering technique group items similarity 
standard greedy agglomerative clustering input set items means computing distance similarity pairs items 
items combined clusters successively combining closest clusters reduced number clusters target number 
standard implementation greedy agglomerative clustering gac initialize element cluster size compute distances pairs clusters sort distances smallest largest repeatedly merge clusters closest left desired number clusters 
standard gac implementation need apply distance function times calculate pair wise distances items 
canopies implementation gac drastically reduce required number comparisons 
cheap approximate distance measure overlapping canopies created 
canopies property holds guaranteed points share canopy fall cluster 
need calculate distances pairs points 
equivalently initialize distances nity replace pairwise distances items fall canopy 
discussed section vastly reduces required number distance calculations greedy agglomerative clustering 
canopies expectation maximization clustering canopies idea speed clustering methods means expectationmaximization em 
general means em specify clusters 
canopies technique help choice 
canopies reduce number expensive distance comparisons need performed 
create canopies 
describe di erent methods canopies prototype clustering techniques 
method approach prototypes estimates cluster centroids associated canopies contain prototypes uenced data inside associated canopies 
creating canopies decide prototypes created canopy 
done example number data points canopy aic bic points occur canopy counted fractionally 
place prototypes canopy 
initial placement random long canopy question determined inexpensive distance metric 
calculating distance prototype point traditional nk operation step calculates distance prototype smaller number points 
prototype nd canopies contain cheap distance metric calculate distances expensive distance metric prototype points prototypes moved number prototypes points constant initialized canopy prototype canopy points constant initialized canopy prototype data set plus summarized canopy centers points initialized canopy canopy prototype created destroyed dynamically table summary di erent methods combining canopies em 
canopies 
note procedure prototypes may move canopy boundaries canopies overlap 
prototypes may move cover data overlapping region move entirely canopy order cover data 
canopy modi ed em algorithm behaves similarly traditional em slight di erence points outside canopy uence points canopy minute uence 
canopy property holds points cluster fall canopy canopy modi ed em converge maximum likelihood traditional em 
fact di erence iterative step apart enormous computational savings computing fewer terms negligible points outside canopy exponentially small uence 
means gives just similar results canopies traditional setting exactly identical clusters 
kmeans data point assigned single prototype 
long cheap expensive distance metrics suciently similar nearest prototype calculated expensive distance metric boundaries canopies contain data point calculated cheap distance metric prototype win 
method version canopies method forcing pick number prototypes canopy separately allows select number prototypes cover data set 
method possible 
prototypes associated canopies uenced individual data points inside canopies 
method prototypes uenced data data points outside prototype associated canopy represented simply mean canopies 
respect method identical omohundro 
method di ers canopies created highly eciently cheap distance metric 
computationally ecient compute distance points cheap distance metric inverted indices avoids completely computing distance points 
method existing traditional methods dynamically determining number prototypes :10.1.1.33.3047:10.1.1.33.3047
techniques creating destroying prototypes particularly attractive thinking method 
simplicity completely ignoring data points outside prototype associated cluster problem may prototypes canopy 
method method prototypes associated canopies see points canopy 
techniques create possibly destroy prototypes dynamically clustering 
avoid creating multiple prototypes cover points fall canopy invoking conservation mass principle points 
practice means contribution point divided canopies falling naturally normalization step traditional membership prototype determined dividing inverse distance prototype sum inverse distances prototypes distance measured prototypes fall di erent canopies 
computational complexity formally quantify computational savings canopies technique 
technique components relatively fast step canopies created followed relatively slow clustering process 
create canopies inverted index need perform complete pair wise distance comparisons 
assume document words words evenly distributed vocabulary compare document documents nw jv 
canopy creation cost typically insigni cant comparison detailed clustering phase 
assume data points originated clusters 
concreteness rst consider greedy agglomerative clustering algorithm 
clustering canopies requires calculating distance pairs points operation 
dominating complexity term gac clustering 
consider reduced canopies 
assume canopies data point average falls canopies 
factor estimates amount canopies overlap 
optimistic scenario canopy equal size roughly fn data points canopy 
clustering canopies need calculate distances pairs points canopy 
requires fn distance measurements 
probably estimate points tend fall multiple canopies need calculate distances 
represents reduction complexity general larger typical case small constant canopies technique reduces computation factor 
case means expectation maximization clustering canopies requires nk distance comparisons iteration clustering nding distance data point cluster prototype 
consider em method cluster belongs canopy 
assume clusters roughly fahlman scott lebiere christian 
learning architecture 
touretzky editor advances neural information processing systems volume pp 
san mateo ca 
morgan kaufmann 
fahlman lebiere cascade correlation learning architecture nips vol 
pp 
morgan kaufmann 
lebiere 

learning architecture 
advances neural information processing systems nips denver colorado 
sample citations 
note di erent layout formats mistakes spelling dicult identify citations 
overlap factor data points 
cluster needs compare fn points di erent canopies 
clusters iteration yielding complexity reduction seen gac 
experimental results described section order savings signi cant 
industrial sized merge purge problem far canopies full pair wise distance calculations feasible 

clustering textual bibliographic section provide empirical evidence canopies clustering increase computational eciency order magnitude losing clustering accuracy 
demonstrate results domain bibliographic 
cora web site www cora whizbang com provides search interface computer science research papers 
part site functionality provide interface traversing citation graph 
provide links papers links papers respective bibliography section 
provide interface data necessary recognize citations di erent papers referencing third text citations may di er 
example may abbreviate rst author names second may include 
typical citations shown 
note di erent styles formatting abbreviation outright citation mistakes dicult task automate 
pose clustering problem task take citations papers collection cluster cluster contains citations single 
cora collection contains bibliographic entries necessary perform clustering eciently 
straightforward gac take cpu year assuming unlimited memory 
estimate total number canopies average canopy membership labeled dataset canopies approach provide speedup orders magnitude reducing clustering time couple hours 
distance metrics citations basic clustering approach greedy agglomerative clustering 
order perform clustering domain provide distance metric space bibliographic citations 
powerful choice measuring distance strings string edit distance calculated dynamic programming di erent costs associated various transformation rules 
know strings choose transformation cost parameters speci domain 
di erent transformation costs deleting character deleting character operation deletion deleting period deleting character string string currently period substituting character character substituting non alphabetic character non alphabetic character deleting non alphabetic character 
diculty applying string edit distances domain citations easily represent eld transpositions placing year author citation atomic cost operation dynamic programming 
expect strong similarity individual elds citations expect strong correlations ordering elds 
de ne distance metric weighted average distances elds occurring citations 
elds citation automatically hidden markov model eld extraction process described 
string edit distance calculations relatively expensive dynamic program solved account possible insertions deletions transpositions 
doing full clustering cora dataset involve solving dynamic programs clearly untenable 
feasible canopies approach pass process rst inexpensive distance metric limit number string edit distances compute 
described section inverted index method thresholds inexpensively create canopies 
speci cally distance citations measured considering citation short document measuring similarity text search engine archer 
similarity citations standard tfidf cosine similarity information retrieval 
coarse metric allows approximate overlapping canopies created quickly 
dataset protocol evaluation metrics order evaluate speed accuracy clustering algorithms take subset cora citation data hand label correct clustering 
labeled data consist citations collection papers authored michael kearns robert schapire yoav freund 
identify papers generous substring matching author names prune papers authored subjects 
comprises citations distinct papers 
quarter papers papers 
popular cited times 
cluster citations thresholds canopy creation nearest neighbor greedy agglomerative clustering string edit distances extracted elds expensive distance metric 
elds citation string edit distance calculation author title date venue journal name conference name 
elds lower cased 
dates reduced digit year 
titles venues truncated characters 
author elds simpli ed automatically abbreviating rst name rst author 
hmm number errors build natural variation citations 
worst xed pre processing stage 
tunable parameters canopies clustering tight loose thresholds cheap distance metric stopping point greedy agglomerative clustering 
tuned parameters separate similarly sized validation dataset authors scott fahlman dennis kibler paul string edit costs di erent operations set hand coded values 
learning tuning string edit weights automatically area ongoing research 
analyzing results metrics evaluating quality clustering 
metrics de ned terms pairs citations 
error rate fraction pairs citations correctly cluster di erent clusters di erent papers 
vast majority pairs citations fall di erent clusters error informative metric 
consider precision recall metric credit algorithm correctly placing citations different clusters 
metrics standard measures information retrieval 
precision fraction correct predictions pairs citations predicted fall cluster 
recall fraction correct predictions pairs citations truly fall cluster 
harmonic average precision recall 
single summary statistic credit algorithm correctly placing overwhelming number pairs di erent clusters 
series increasingly sophisticated methods comparing computational eciency clustering accuracy canopy framework 
naive baseline simply place citation cluster 
represents straw man performance computation clustering 
second baseline create hand regular expression identi es name rst author year citation 
citation placed cluster published year author name 
baseline represents bibliographic map done large amount manual tuning automatic clustering algorithm 
third comparison current grouping algorithm cora 
word matching algorithm similar thresh method error precision recall minutes canopies complete expensive existing cora author year baseline naive baseline table error time costs di erent methods clustering 
naive baseline places citation cluster 
author year baseline clusters citation year rst author citation identi ed hand built regular expressions 
existing cora method word matching technique 
note canopy clustering quicker slightly accurate complete clustering 
time measured minutes perl implementations existing cora implemented 
loose threshold tight threshold table results created varying parameters tight loose thresholds canopy creation 
number parenthesis computation time minutes clustering 
decrease loose threshold increase tight threshold computation time increases required calculate pairwise expensive distances 
parameter setting chosen cross validation indicated bold 
olds matching uses titles authors years citation creates clustering non overlapping 
nal method perform complete hierarchical agglomerative clustering string edit distance metric 
represents expensive baseline perform accurately 
methods implemented perl existing cora algorithm implemented experiments run mhz pentium ii memory paging activity 
experimental results table presents summary experimental results canopy clustering threshold parameters number clusters tuned separate validation set 
canopy clustering algorithm achieves minutes 
comparison complete clustering takes hours slightly worse error 
note represents order magnitude reduction computation time 
original motivation algorithm computation time size canopies approach provides slightly accurate clustering algorithm 
hypothesis somewhat surprising result levels clustering loose canopies strict string edit sense 
cheap distance metric produces errors independent expensive distance metric 
allows cheap canopy partitioning remove errors expensive clustering 
comparison techniques give worse clustering solutions canopies 
error baseline naive approach twice clustering approach 
author year existing cora techniques improves baseline accuracy signi cantly worse clustering technique 
performing canopy clustering sets parameters need chosen values canopy thresholds number nal clusters 
table shows experimental results vary values tight loose thresholds 
number comparisons done expensive clustering algorithms varies thresholds change number size canopies changes 
general tight similarity threshold increases loose threshold decreases distance measurements required 
example loose expensive distance calculations required 
loose best performance seen distances required nearly times 
means canopy creation nding pairs belong distances close 
comparison doing complete clustering canopies requires expensive distance calculations parameters picked validation set loose required 
larger datasets di erence pronounced number distance calculations required full clustering grows square number items 
table shows error canopy clustering varies nal number clusters changed 
note having correct number clusters slightly provides best accuracy 
detailed error analysis shows error comes citations clusters distance precision recall table accuracy clustering vary nal number clusters 
note best score occurs near correct number clusters 
allow clusters precision increases recall decreases 
di erent clusters predicted cluster 
canopy clustering times mistakes falsely putting cluster falsely putting di erent clusters 

related researchers practitioners recognized desirability diculty grouping similar items large data sets clustering 
methods tend fall categories database methods nding near duplicate records clustering algorithms 
extensive literature clustering algorithms goes back years 
see 
methods single method computing similarity pairs items 
items greedily iteratively clustered similarity 
standard clustering methods include greedy agglomerative methods greedy divisive methods iterative methods means clustering 
variants classical clustering methods iterative em methods estimate parameters formal statistical models forms overlapping clusters improve precision 
may represent subsets data single points incremental clustering improve clustering speed large data sets 
em statistical methods tend slow pass incremental methods tend accurate 
canopies method described di ers methods di erent similarity measures 
similarity measure quickly form canopies re ned similarity measure form smaller clusters high speed high precision obtained 
closely related methods large number extensions variants kd trees multiresolution kd trees recursively partition data subgroups 
methods su er doing hard partitions item single side partition 
cheap approximate similarity measures item put wrong side partition way correct error 
kd tree methods typically scale poorly items large numbers attributes splits generally single attribute 
method overlapping regions assumes tree structure 
kd tree methods expensive cheap similarity measures canopies method 
separate line clustering comes users databases 
record linkage merge purge problem occurs purchases multiple databases multiple mailing lists wishes determine records near duplicates refer person lists merged duplicates purged 
special case clustering problem addressed clusters small items cluster typically quite distinct items clusters 
approach merge purge problem multiple different keys determine duplicate records results di erent clusters combined 
example database created combining multiple databases may contain duplicate near duplicate records 
composite database sorted separately multiple keys address social security number 
sorting items small window checked see near duplicates combined 
closely related approach database sorted application speci key name expensive similarity computed items close sorted list :10.1.1.28.8405:10.1.1.23.9685
merge purge researchers monge elkan assume exact match single eld establishes items duplicates canopies algorithm allows complex comparisons clustering methods means tolerant noise 
explicitly assigning items multiple canopies allows rigorous methods em canopy clear computational cost estimates 
comes closer canopies algorithm 
proposes method clustering published articles uses step algorithm 
rst step item pool potentially matching items selected doing full text searches query search consists authors names words title selected random 
second step items pool compared item generate pool string matching algorithm 
forces items grouped single group 
example items grouped queries items grouped queries group 
method follows spirit canopies initial rough grouping unique partition followed ne grained comparison items group 
di ers canopies forms group pool terms single item 
terms requires number canopies equal number items 
expensive 
pool centered single item support arbitrary clustering methods ne grained clustering portion algorithm 
giles study domain clustering citations :10.1.1.30.6847
experiments di erent techniques string edit technique 
nd best performing method word matching algorithm similar cora technique section augmented eld extraction information bigrams 
lesser performing string edit distance metric similar clustering algorithm metric greedy agglomerative clustering 
iterative match remove algorithm perform clustering 

clustering large data sets ubiquitous task 
astronomers classify stars similar sets images 
search engines web seek group similar documents words contain citations 
marketers seek clusters similar shoppers purchase history demographics 
seek identify similar products product descriptions 
biologists seek group dna sequences proteins code group proteins function cells 
cases objects characterized large feature vectors images text dna sequences 
furthermore case inexpensive measures similarity compute number words common expensive accurate measures parse trees part speech tagging text string edit distances dna 
large data sets hundreds thousands millions entries computing pairwise similarities objects intractable ecient methods called 
increasingly people trying complex models mixture distributions hmms large data sets 
computing global models observations ect parameters intractable methods grouping observations similarly grouping objects needed 
canopies provide principled approach problems 
focused matching particular class problems arise different descriptions di erent objects wishes know descriptions refer object best description object experimental results domain bibliographic citation matching 
important instance class merge purge problem 
companies purchase merge multiple mailing lists 
resulting list multiple entries household 
single person name address version list may di er slightly middle initials absent words abbreviated expanded zip codes absent 
problem merging large mailing lists eliminating duplicates complex wishes collapse records multiple people live household 
information extracted web matching problem severe 
search web extract descriptions products attributes di erent models palm pilots weight memory size descriptions companies industries countries business 
information extraction error prone redundant di erent sources sell product 
goal cluster descriptions sets describe product determine canonical description 
large number items feature dimensions clusters carefully comparing item item intractable cases 
fortunately cheap approximate means group items overlapping subsets call canopies accurate expensive comparisons items canopy 
canopies ideally property items true cluster fall canopy guarantees accuracy lost restricting comparisons items canopy 
canopy approach widely applicable 
cheap measures binning comparison attributes complex record nding similarity inverted index 
expensive measure detailed similarity measurements string edit distance computed dynamic programming 
clustering greedy agglomerative nearest neighbor means wide variety em methods 
commonality methods creation canopies cheap measure expensive measure drastically reduced 
demonstrated success canopies approach matching problem domain bibliographic citations 
reduced computation time order magnitude slightly increasing accuracy 
ongoing running canopies algorithm full set citations expect reduced computation orders magnitude cpu year couple hours 
quantify analytically correspondence cheap expensive distance metrics perform experiments em wider variety domains including data sets real valued attributes 
acknowledgments done rst authors just research www com 
cora supported whizbang 
labs originally created just research 

akaike 
entropy maximization principle 
applications statistics pages 
anderberg 
cluster analysis application 
academic press 
bradley fayyad reina :10.1.1.157.392
scaling clustering algorithms large databases 
proc 
th international conf 
knowledge discovery data mining kdd 
aaai press august 
sunter 
theory record linkage 
journal american statistical society 
friedman bentley finkel 
algorithm nding best matches logarithmic expected time 
acm tras 
math 
software 
giles bollacker lawrence :10.1.1.30.6847
citeseer automatic citation indexing system 
digital libraries third acm conference digital libraries 
hernandez stolfo 
merge purge problem large databases 
proceedings acm sigmod may 
hirsh 
integrating sources information text classi cation 
snowbird learning conference april 

identifying merging related bibliographic records 
mit lcs masters thesis 
alvey editors 
record linkage techniques 
statistics income division internal revenue service publication 
available www gov 
mccallum nigam rennie seymore 
automating construction internet portals machine learning 
information retrieval 
appear 
mccallum 
bow toolkit statistical language modeling text retrieval classi cation clustering 
www cs cmu edu mccallum bow 
monge elkan 
eld matching problem algorithm applications 
proceedings second international conference knowledge discovery data mining august 
monge elkan 
ecient domain independent algorithm detecting approximately duplicate database records 
proceedings sigmod workshop data mining knowledge discovery may 
moore 
fast em mixture model clustering multiresolution kd trees 
advances neural information processing systems 
newcombe kennedy james 
automatic linkage vital records 
science 
omohundro 
construction algorithms 
technical report international computer science institute berkeley california 
rose :10.1.1.33.3047
deterministic annealing clustering compression classi cation regression related optimization problems 
proceedings ieee 
salton buckley 
term weighting approaches automatic text retrieval 
information processing management 
suresh wong 
method incremental aggregation dynamically increasing database data sets 
patent 
kruskal 
macromolecules theory practice sequence comparison 
addison wesley 
tukey pedersen 
method apparatus information access employing overlapping clusters 
patent 
zhang ramakrishnan livny 
birch ecient data clustering method large databases 
proceedings acm sigmod international conference management data pages 
