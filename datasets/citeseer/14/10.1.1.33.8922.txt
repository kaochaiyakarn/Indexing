active learning parameter estimation bayesian networks simon tong computer science department stanford university simon tong cs stanford edu daphne koller computer science department stanford university koller cs stanford edu bayesian networks graphical representations probability distributions 
virtually learning networks assumption data set consisting randomly generated instances underlying distribution 
situations option active learning possibility guiding sampling process querying certain types samples 
addresses problem estimating parameters bayesian networks active learning setting 
provide theoretical framework problem algorithm chooses active learning queries generate model learned far 
experimental results showing active learning algorithm significantly reduce need training data situations 
machine learning applications time consuming costly task collection sufficiently large data set 
important find ways minimize number instances required 
possible method reducing number instances choose better instances learn 
universally machine learning literature assumes set instances chosen randomly underlying distribution 
assume learner ability guide instances gets selecting instances lead accurate models 
approach called active learning 
possibility active learning arise naturally variety domains variants 
selective active learning ability explicitly asking example certain type ask full instance attributes take requested values 
example domain involves webpages learner able ask human teacher examples homepages graduate students computer science department 
variant selective active learning pool active learning learner access large pool instances knows value certain attributes 
ask instances pool known attributes take certain values 
example redesign census fill short form active learner select respondents fill detailed long form 
example cancer study list people ages smoke ask subset people undergo thorough examination 
active learning settings need mechanism tells instances select 
problem explored context supervised learning 
consider application unsupervised learning task density estimation 
formal framework active learning bayesian networks bns 
assume graphical structure bn fixed focus task parameter estimation 
define notion model accuracy provide algorithm selects queries greedy way designed improve model accuracy possible 
sight applicability active learning density estimation unclear 
simply sampling initially clear active learning algorithm learns correct density 
fact show algorithm consistent converges right density limit 
furthermore clear active learning necessarily beneficial setting 
trying estimate distribution random samples distribution best source 
surprisingly provide empirical evidence showing range interesting circumstances approach learns significantly fewer instances random sampling 
learning bayesian networks fx xn set random variables variable values finite domain dom 
bayesian network pair represents distribution joint space directed acyclic graph nodes correspond random variables structure encodes conditional independence properties joint distribution 
denote set parents set parameters quantify network specifying conditional probability distributions cpds 
assume cpd node consists separate multinomial distribution dom instantiation parents parameter ij ju ij dom ju represent vector parameters associated multinomial 
focus parameter estimation task network structure goal data estimate network parameters 
bayesian parameter estimation keeping density possible parameter values 
usual assumption parameter independence allows represent joint distribution set independent distributions multinomial ju multinomials conjugate prior dirichlet distribution parameterized hyperparameters ff ff ff intuitively ff represents number imaginary samples observed prior observing data 
particular distributed multinomial parameters dirichlet probability observation ff ff obtain new instance sampled distribution posterior distribution distributed dirichlet hyperparameters ff ff ff 
bn parameter independence assumption dirichlet distribution multinomial distribution ju distribution ff ij ju denote hyperparameter corresponding parameter ij ju active learning assume start network structure prior distribution parameters standard machine learning framework data instances independently randomly sampled underlying distribution 
active learning setting ability request certain types instances 
formalize idea assuming subset variables controllable 
learner select subset variables ae particular instantiation request called query 
result query randomly sampled instance conditioned myopic active learner querying function takes selects query takes resulting instance uses update distribution obtain posterior 
repeats process note summarizes relevant aspects data seen far need maintain history previous instances 
fully specify algorithm need address issues need describe parameter distribution updated random sample need construct mechanism selecting query answer issue assume simplicity query single node clear resulting instance update parameters node 
subtle problem 
consider parent give information distribution information conveniently 
intuitively sampled distribution specified complex formula involving multiple parameters 
avoid problem simply ignoring information provided nodes upstream generally define variable updateable context selective query ancestor node update rule simple 
prior distribution instance query standard bayesian updating case randomly sampled instances update dirichlet distributions updateable nodes 
denote distribution obtained algorithm read density asking query obtaining response 
second task construct algorithm deciding query current distribution key step approach definition measure quality learned model 
allows evaluate extent various instances improve quality model providing approach selecting query perform 
formulation framework bayesian point estimation 
bayesian learning framework maintain distribution model parameters 
asked reason model typically collapse distribution parameters generate single representative model answer questions relative 
choose true model incur loss loss 
goal minimize loss 
course access posterior distribution represents optimal beliefs different possible values prior knowledge evidence 
define risk particular respect thetap loss theta loss define bayesian point estimate value minimizes risk 
shall considering bayesian point estimate define risk density risk risk optimal respect risk density measure quality current state knowledge represented 
greedy scheme goal obtain instance risk obtained updating lowest 
course know exactly going get 
know sampled distribution induced query 
expected posterior risk thetap xp theta risk definition leads immediately simple algorithm candidate query evaluate expected posterior risk select query lowest 
active learning algorithm obtain concrete algorithm active learning framework shown previous section pick loss function 
possible choices best justified relative entropy kullback leibler divergence kl divergence kl ln kl divergence independent justifications variety properties particularly suitable measure distance distributions 
proceed kl divergence loss function 
analogous analysis carried natural loss function negative loglikelihood data case multinomial cpds dirichlet densities parameters results identical final algorithm 
want find efficient approach computing risk 
properties kl divergence turn crucial 
value minimizes risk relative mean value parameters thetap 
bayesian network independent dirichlet distributions parameters expression reduces ij ju ff ij ju ff ju standard bayesian approach collapsing distribution bn models single model 
second observation bns kl divergence decomposes graphical structure network kl kl kl conditional kl divergence kl 
facts prove theorem gamma ff gamma function psi ff digamma function gamma ff gamma ff entropy function 
define ffi ff ff ff ff psi ff gamma psi ff ff ff ff ff ji risk decomposes risk dom ffi ff ju ff ir ju eq 
gives concrete expression evaluating risk 
evaluate potential query need expected posterior risk 
recall expectation possible answers query risk posterior distribution words average exponentially large set possibilities 
understand evaluate expression efficiently consider simpler case 
consider bn child node parents edges nodes restrict attention queries control parents case query instantiation possible outcomes query possible values variable expected posterior risk contains term variable instantiation parents 
particular contains term parent variables variables updateable hyperparameters remain query contribution risk prior 
ignore terms corresponding parents focus terms associated conditional distribution 
risk ffi ff ju ff xr ju ffi ff ju ff xr ju ff ju hyperparameter 
evaluating expected posterior risk directly evaluate reduction risk obtained asking query delta risk gamma risk gamma key observation relies fact variables updateable query hyperparameters change 

second observation hyperparameters corresponding instantiation terms cancel expression simplifies ffi ff jq ff xr jq gamma ffi ff jq ff xr jq advantage certain functional properties psi obtain delta ff jq ff jq ff xr jq ff jq gamma ff jq ff jq ff xr jq ff jq select query maximize difference current risk expected posterior risk get natural behavior select query leads greatest reduction entropy parents 
gain insight active learning edge random sampling 
consider situation times lead update parameter current density dirichlet lead update parameter current density dirichlet 
delta updating worth 
words confident commonly occurring situations worth ask rare cases 
generalize derivation case arbitrary bn arbitrary query 
average possible query answers encompasses exponentially terms 
fortunately utilize structure bn avoid exhaustive enumeration 
theorem arbitrary bn arbitrary query expected kl posterior risk decomposes dom words expected posterior risk weighted sum expected posterior risks conditional distributions individual nodes node consider queries complete instantiations parents similar decompositions risk expected posterior risk 
obvious step consider difference simplify case single variable 
unfortunately case general bns longer exploit main simplifying assumptions 
recall expression risk eq 
term involving weighted 
expected posterior risk weight 
case single node full parent query hyperparameters parents change weights necessarily 
general setting instantiation change hyperparameters network leading different weights 
believe single data instance usually lead dramatic change distributions 
weights quite close 
simplify formula associated computation choose approximate posterior probability prior probability 
assumption simplification single node case 
assuming approximation delta risk gamma dom delta delta defined eq 

notice need sum updateable delta zero non updateable analysis provides efficient implementation general active learning scheme 
simply choose set variables bayesian network wish control instantiation controllable variables compute expected change risk eq 

ask query greatest expected change update parameters updateable nodes 
consider computational complexity algorithm 
turns potential query desired quantities obtained inference passes standard join tree algorithm 
run time complexity algorithm jqj delta cost bn join tree inference set candidate queries 
algorithm approximately finds query reduces expected risk 
show specific querying scheme including approximation consistent 
mentioned statement non trivial depends heavily specific querying algorithm 
number queries kl divergence random active number queries kl divergence random active number queries kl divergence random active alarm network controllable nodes 
asia network controllable nodes 
cancer network controllable node 
axes zoomed resolution 
theorem set nodes updateable candidate query querying step 
assuming underlying true distribution deterministic querying algorithm produces consistent estimates cpd parameters member experimental results performed experiments commonly networks alarm asia cancer 
alarm nodes independent parameters asia nodes independent parameters cancer nodes independent parameters 
needed set priors network 
standard approach eliciting network equivalent sample size 
experiments assumed fairly background knowledge domain 
simulate obtained prior sampling instances true network counts smoothing uniform prior prior 
akin asking prior network domain expert existing set complete data find initial settings parameters 
compared refining parameters active learning random sampling 
permitted active learner abstain choosing value controlled node wish node sampled usual 
presents results networks 
graphs compare learned networks true network generating data 
see active learning provides substantial improvement networks 
improvement alarm network particularly striking control just nodes 
extent improvement depends extent queries allow reach rare events 
example smoking controllable variables asia network 
original network smoking 
significant gain active learning network greater increase performance altered generating network smoking graph shown 
experimented specifying uniform priors small equivalent sample size 
obtained significant benefit asia network marginal improvement 
possible reason improvement washed randomness active learner standard learner learning different instances 
explanation approximation eq 
may hold prior uninformed easily perturbed single instance 
indicates algorithm may perform best refining existing domain model 
situations active learning performed better random sampling 
situations active learning produced benefit unsurprisingly prior confident correct commonly occurring cases uncertain incorrect rare ones 
clearly precisely scenario encounter practice prior elicited expert 
experimenting forcing different priors active learning worse type situation prior confident incorrect commonly occurring cases uncertain correct rare ones 
type scenario occur practice 
factor affecting performance degree controllable nodes influence updateable nodes 
discussion formal framework resulting querying algorithm parameter estimation bayesian networks 
knowledge applications active learning unsupervised context 
algorithm uses parameter distributions guide learner ask queries improve quality estimate 
bn active learning performed causal setting 
query acts experiment intervenes model forces variables take particular values 
pearl intervention theory easily extend analysis deal case 
difference notion updateable node simpler node part query updateable 
regrettably space prohibits complete exposition 
demonstrated active learning significant advantages task parameter estimation bns particularly case parameter prior type human expert provide 
intuitively benefit comes estimating parameters associated rare events 
important estimate probabilities rare events accurately number instances obtained randomly sample distribution 
note advantage arises loss function considers accuracy distribution 
practical settings medical fault diagnosis rare cases important ones critical system deal correctly 
direction pursuing active learning causal structure domain 
words domain causal structure wish understand want know best sequence experiments perform 
experiments performed system developed primarily lise getoor uri lerner ben taskar 
carlos guestrin andrew ng helpful discussions 
supported darpa information assurance program subcontract sri international aro daah muri program integrated approach intelligent systems 
atkinson 
optimal experimental designs 
oxford university press 
cohn ghahramani jordan 
active learning statistical models 
journal artificial intelligence research 
cover thomas 
information theory 
wiley 
degroot 
optimal statistical decisions 
mcgraw hill new york 
heckerman geiger chickering 
learning bayesian networks combination knowledge statistical data 
machine learning 
lauritzen spiegelhalter 
local computations probabilities graphical structures application expert systems 
royal statistical society 
mackay 
information objective functions active data selection 
neural computation 
pearl 
causality models reasoning inference 
cambridge university press 
seung opper sompolinsky 
query committee 
proc 
colt pages 
