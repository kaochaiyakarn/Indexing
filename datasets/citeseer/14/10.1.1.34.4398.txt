exploring bounds web latency reduction caching prefetching thomas kroeger department computer engineering university california santa cruz darrell long department computer science university california santa cruz jeffrey mogul digital equipment western research laboratory prefetching caching techniques commonly systems reduce latency 
researchers advocated caching prefetching reduce latency web 
derive bounds performance improvements seen techniques traces web proxy activity taken digital equipment quantify bounds 
traces local proxy caching reduce latency best prefetching reduce latency best combined caching prefetching proxy provide best latency reduction 
furthermore far advance prefetching algorithm able prefetch object significant factor ability reduce latency 
note latency reduction caching significantly limited rapid changes objects web 
conclude workload studied caching offers moderate assistance reducing latency 
prefetching offer twice improvement caching limited ability reduce latency 
growth web past years inspired researchers investigate prefetching caching techniques reduce latency 
techniques seen significant success reducing latency storage systems processor tmk cse ucsc edu 
supported part digital equipment office naval research 
darrell cse ucsc edu 
supported part office naval research 
mogul pa dec com 
memory hierarchies remains seen effective techniques world wide web 
classify caching prefetching different methods derive bounds methods 
traces taken week period digital equipment quantify bounds 
assume proxy server intermediary client browser web server 
proxy server accepts requests clients satisfies data prefetched cached retrieved directly appropriate web server 
configuration quite common web today 
proxy model serves partition latency web retrievals components external latency caused network web server latencies external organization internal latency caused networks computers bounds organization 
illustrates common configuration 
proxies normally located organization network communication client proxy normally small portion latency 
side proxy server communication normally accounts significant majority total event latency 
primary goal proxy caching prefetching reduce amount time client waits data reducing removing external latency 
traces external latency accounts latency seen entire trace set latency seen subset clients geographically close proxy 
potential significant performance gain best improvement saw caching prefetching reduced total latency 
additionally saw prefetching lead time amount time prefetching begins object needed significantly affects amount latency latency proxy internal latency clients servers external typical proxy configuration reduction 
limited simulator providing minute warning latency reduction dropped 
additionally observe latency reduction caching half data set data objects change 
observation agrees studies show high rate change objects web :10.1.1.31.5701
comparing results external latency observe workload examined web latency consists internal latency external latency cached prefetched external latency removed prefetching caching 
key point take results caching prefetching helpful current conditions limit ability reduce latency 
rest article organized follows section categorizes caching prefetching presents methods techniques 
bounds method 
discuss methodology quantify bounds 
results simulations 
related addressed 
bounding caching prefetching goal determine upper bound effectiveness proxy caching prefetching reducing latency web 
classify caching prefetching algorithms different categories 
construct models bounding performance category algorithm 
order ensure bounds widely applicable specific algorithms attempt keep analysis general possible 
categories caching prefetching distinguish caching algorithms cache remain passive action result requests active prefetching data anticipation requests 
distinguish prefetching algorithms information determine prefetch originates 
traditionally caching thought system reacts requests 
define passive cache loads data object result client request access object 
contrast term active cache refer caches employ mechanism prefetch data 
note passive caching systems serve reduce network traffic 
focus passive caching latency reduction 
categorize prefetching categories local server hint information determining objects prefetch generated 
local prefetching agent doing prefetching browser client proxy uses local information patterns determine objects prefetch 
prefetching algorithms information server employed client proxy considered local prefetching 
server hint prefetching server able content specific knowledge objects requested patterns far greater number clients determine objects prefetched 
actual prefetching done agent closer client 
server provides hints assist agent proxy client prefetching 
implementation model complicated requirement modifications client proxy side server side 
options caching prefetching examine different methods passive proxy caching unlimited storage active cache local prefetching unlimited storage server hint prefetching active cache server hint prefetching unlimited storage 
bounding analysis prefetching caching set upper bounds model basing simulations best case assumptions 
assume method works full knowledge events 
place certain restrictions method 
passive caching assume previously accessed object changed available cache 
local prefetching object seen predicted prefetching assume access object prefetched subsequent accesses successfully prefetched 
assume additional information outside traced stream allow prediction url seen trace 
server hint prefetching assume prefetching client contact server 
assume system full knowledge events restriction server schedule transfer object complete just object requested 
provided bandwidth model eliminate communication latency 
system servers suddenly transfer objects clients contact 
provide realistic useful bound performance prefetching assume order server provide prefetch hints client accessed client 
contact model contact client proxy simultaneously prefetch client requests server 
example contact www cnn com tuesday model assume requests www cnn com wednesday thursday friday prefetched tuesday 
contact model may somewhat unrealistic 
investigated effects placing limits prefetching lead time amount data prefetched simultaneously 
limit amount data prefetched simultaneously place threshold bandwidth prefetching 
contact subsequent requests scheduled immediate prefetch bandwidth threshold reached exceeded 
request scheduled current prefetches completed bandwidth gone threshold 
varied bandwidth lead time independently combination 
bound performance active caching server hint prefetching unlimited storage test object passive cache 
case test object successfully prefetched contact model 
quantifying bounds trace simulation quantify bounds 
obtain traces modified firewall web proxy digital equipment record requests clients digital servers internet 
traces ran august september 
event trace stream represents web request response interaction total events clients connecting servers 
proxy provided caching prefetching simply served method crossing corporate firewall 
provide separate samples comparison extracted mid week segments traces covering monday friday 
labeled samples week week week entire trace stream 
examined subset trace data consisting clients palo alto ca proxy located 
samples labeled pa pa pa 
workload subset representative smaller localized organization 
main concern latency seen retrieving web page simulations focus requests protocol get method 
ignored events failed reason connection server failed data transfer 
analyses assumed query events events cgi bin url prefetched cached 
convention proxy caches 
simulations observed total external latencies estimate total external internal gammae event latencies seen request occur 
model says request successfully prefetched cached internal latency estimate modeled event new duration 
previously observed total latency 
values quantify fraction latency reduced gamma approximation ignores possibility proxy transferring information client communication server 
looking lower bound latency event upper bound latency reduction ignore possibility 
approximations bounding models described simulation steps event trace stream determines cached prefetched 
compare distribution event latencies seen bounding models original trace measured caching prefetching order compute average latency reduction workload represented traces 
sources inaccuracy note possible sources inaccuracy results 
assumption urls table passive caching unlimited storage 
measurement week week week pa pa pa pa total latency external latency new latency percentage external latency total latency reduction hit ratio cache size gb latencies averages seconds 
pa pa represent week week entire trace stream palo alto subset 
table bounds latency reductions local prefetching 
measurement week week week pa pa pa pa percentage external latency total latency reduction queries cgi bin prefetched cached cause upper bound latency reduction best possible latency reduction 
algorithm able cache prefetch items see greater latency reductions 
traces lack modified timestamps half entries servers provide information 
left changes response size indicator stale data requests 
result simulation cases simulate cache hit occur causing overestimate potential latency reduction 
models assume latency retrieving successfully prefetched cached item proxy time original event minus time communications gamma 
assumes little overlap server proxy data transfer proxy client data transfer 
assumption wrong larger responses distant clients simulation overestimate possible latency reduction 
results goal simulations workload traced quantify bounding models 
latency reductions passive caching active caching local prefetching 
examine contact model unlimited storage caching 
address variation latency reduction examining distribution latency reduction event different types objects requested 
passive proxy caching performance simulated passive caching proxy unlimited storage 
table shows results 
rows show averages original total latency external latency simulated latency passive caching proxy 
row shows fraction original total latency contributed external latency 
ratio serves limit remove external latency average event latency reduced percentage 
rows show percent latency reduced simulated caching proxy cache hit ratio size cache required hold cached data 
table see passive caching able reduce latency smaller organization far cry external latency seen traces 
cache hit ratio ranges latency reduction half 
implies majority requests saw cache hit objects smaller average event confirms similar observation williams 
study showed weak inverse relationship number accesses unchanged response response size 
table latency reduction hit ratio cache size larger entire trace columns single week columns 
occurs longer traces higher chance object referenced 
local prefetching simulated prefetching information 
assume object table results server hint prefetching unlimited contact model 
measurement week week week pa pa pa pa external latency total latency reduction caching total latency reduction caching percent original latency avoided time seconds latency improvement versus time week week week clients digital percent original latency avoided time seconds latency improvement versus time week week week digital palo alto clients percentage latency reduced versus far advance requests may prefetched 
may prefetched seen stream 
model provides bound active caching local prefetching unlimited storage 
model differs passive caching model object changed indicated change size modified timestamp subsequent access object prefetched 
table shows latency reduction bound local prefetching double passive caching see table 
results differ passive cache pays object changes high observed rate change causes poor performance passive caching :10.1.1.31.5701
bounds prefetching server hints simulate server hint prefetching assume prefetching client contacted server time 
simulate combination caching prefetching simulator checks object cache 
simulator uses server hint model determine object successfully prefetched 
table shows contact model requests simultaneously prefetched contact reduce latency little half 
smaller centralized organization represented palo alto subsets external latency increases total latency improvement server hint prefetching slightly better 
combination unlimited storage caching server hint prefetching provides best latency reduction 
limiting contact model modified contact model provide realistic bound placing limit far advance request data prefetched known prefetch lead time 
modified model limit amount data prefetched simultaneously 
order examine effects limiting prefetch lead time modified simulation forget contacts client server pairs inactive longer specified interval 
means subsequent request client server treated contact 
shows results varying prefetch lead times second represented figures seconds 
note lead times seconds available reduction latency significantly impressive 
limit amount overlap prefetch requests set threshold bandwidth percent original latency avoided prefetch bandwidth threshold kbits sec latency improvement versus prefetch bandwidth threshold week week week clients digital percent original latency avoided prefetch bandwidth threshold kbits sec latency improvement versus prefetch bandwidth threshold week week week digital palo alto clients percentage latency reduced versus bandwidth threshold 
prefetching 
shows bandwidth limit affects amount latency reduction 
left graph small increase amount bandwidth limiting prefetching significantly improves reduction latency 
total difference unlimited simultaneous prefetching sequential prefetching greater 
table show effects varying limits 
slope axis prefetch lead time shows parameter dominant factor 
relatively consistent slopes surface graphs imply parameters relatively independent effects latency 
curves figures representative behavior model variations parameters 
table shows latency reduction seen prefetching model unlimited cache storage representative values prefetch time bandwidth available prefetching 
comparing table table note weak moderately capable prefetching algorithm caching especially important 
example prefetching algorithm predict requests minutes advance bandwidth threshold kbits sec caching offer increase approximately 
hand unbounded prefetching model caching offers improvement approximately 
reductions different object types examined type object requested affects latency reduced object types determined extension url path 
table shows results caching prefetching listed object type 
category represents urls extension category represents objects extensions listed 
rows show average original total latency simulated latency 
rows show percentage total event stream type accounts event count event duration respectively 
rows show percentage latency reduced type passive caching local prefetching unlimited cache storage server hint prefetching caching server hint prefetching unlimited storage caching 
table shows part common types gif html jpeg class data offer slightly average latency reductions common type ambiguous requests offer significant benefit 
result suggests cache kept objects common types effective storage resources 
variation latency reduction examine variation latency reduction separate events shows histograms percent latency reduced event passive caching local prefetching server hint prefetching server hint unlimited storage caching 
graphs show bi modal distribution event sees significant latency reductions peaks little reduction peak 
distributions show models simulated expect web request see minimal latency reduction significant latency improvement bandwidth limit kbits sec lead time seconds latency reduction clients digital latency improvement bandwidth limit kbits sec lead time seconds latency reduction digital palo alto clients reduction latency prefetching bandwidth kbits sec time seconds 
note scale axis logarithmic 
table selected latency reduction percentages prefetching bandwidth kbits sec time seconds 
time week week week pa pa pa pa reduction total latency 
related researchers looked ways improve current caching techniques 
padmanabhan mogul described server predictive model 
model server observes incoming stream create markov model predicting probability object followed requests object parameter algorithm 
server uses model generate prediction subsequent sends prediction hint client including response current 
client may hint prefetch object object client cache 
simulations estimated reductions note techniques double network traffic 
show reasonable balance latency reduction network traffic increase 
bestavros model speculative dissemination world wide web data 
shows patterns web server effective source information drive prefetching 
observed latency reduction cost significant increase bandwidth 
williams taxonomy replacement policies caching web 
experiment examined hit rates various workloads 
observed hit rates range high majority ranging 
table latency reduction percentages prefetching caching bandwidth kbits sec time seconds 
time week week week pa pa pa pa table latency reduction prefetching caching type data requested times seconds 
type html gif data class jpeg mpeg new latency original latency percentage count percentage time passive caching local prefetching server hints caching server hints caching workload hit rate comes cache placed close server close clients purpose reducing internal bandwidth consumed external requests 
workload addresses different problem examined 
workloads cache hit rates reasonably comparable 
results studies close higher bounds resulting simulations 
note results highly dependent workload environment modeled 
take care applying results simulation studies included specific situation 
summary trace driven simulations explored potential latency reductions caching prefetching 
workload studied passive caching unlimited cache storage reduce latency approximately 
disappointing result passive caching part data web continually changes 
hand prefetching local information offers saw bound approximately reduction latency 
adding server hints increased bound approximately 
observed prefetch lead time important factor performance prefetching 
realistic constraints prefetch bandwidth threshold kbits sec lead time minutes best hope reduction latency 
saw uncommon types objects provided significantly average latency reduction 
observations comparing results models total external latency 
workload studied total latency external latency removed caching prefetching external latency removed events contacts queries internal latency 
results serve illustrate 
caching prefetching effective reducing latency web 
second effectiveness techniques limits 
bounds produced simulations line algorithms full knowledge 
expect line algorithm effective 
caution results highly dependent workload environment modeled 
applied care 
results emphasize need improved prefetching techniques additional techniques caching prefetching 
techniques include wide area replication delta encoding persistent tcp connections included protocol 
number requests millions percent original latency avoided passive caching number requests millions percent original latency avoided local prefetching number requests millions percent original latency avoided server hint caching number requests millions percent original latency avoided server hint caching histograms percentage latency reduced event 
acknowledgments grateful digital equipment support facilities office naval research support prof mary baker kind guidance dr richard golding insightful comments enthusiasm prof pei cao comments input carlos dr kathy richardson squid proxy dr lawrence brakmo input comments early portions randal burns tracey members concurrent systems group provided comments fry 
availability additional details traces traces available ftp ftp digital com pub dec traces proxy html bestavros cunha prefetching protocol client speculation www tech 
rep tr boston university department computer science boston ma apr 
burns long efficient distributed backup delta proceedings parallel distributed systems san jose ca usa nov 
douglis feldmann krishnamurthy mogul rate change metrics live study world wide web proceedings usenix symposium internet technologies systems dec 
fielding gettys mogul nielsen berners lee hypertext transfer protocol rfc working group jan 
gwertzman seltzer case geographical push caching fifth annual workshop hot operating systems orcas island wa pp 
ieee computer society may 
gwertzman seltzer world wide web cache consistency proceedings usenix annual technical conference san diego ca pp 
usenix jan 
tomkins patterson bershad cao gibson karlin li trace driven comparison algorithm parallel prefetching caching proceedings second usenix symposium operating systems design implementation pp 
usenix october 
kroeger long predicting filesystem actions prior events proceedings usenix annual technical conference usenix january 
lei duchamp analytical approach file prefetching proceedings usenix annual technical conference usenix january 
mogul case persistent connection proceedings sigcomm pp 
acm sept 
mogul douglis feldmann krishnamurthy potential benefits delta encoding data compression proceedings sigcomm cannes france pp 
acm sept 
padmanabhan mogul predictive prefetching improve world wide web latency computer communications review vol 
pp 
july 
smith cache memories acm computing surveys vol 
pp 
sept 
vitter krishnan optimal prefetching data compression journal acm vol 
pp 
september 
williams abrams abdulla fox removal policies network caches world wide web documents proceedings sigcomm pp 
acm july 
