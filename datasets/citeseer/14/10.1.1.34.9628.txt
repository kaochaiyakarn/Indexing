athapascan line building data flow graph parallel language francois jean louis mathias lmc imag apache project grenoble france www apache imag fr order achieve practical efficient execution parallel architecture knowledge data dependencies related application appears key point building efficient schedule 
restricting accesses shared memory show data dependency graph computed line distributed architecture 
overhead introduced bounded respect parallelism expressed user basic computation corresponds user defined task data dependency userdefined data structure 
introduce language named athapascan allows built graph dependencies strong typing shared memory accesses 
detail compilation implementation language 
performance code parallel time communication arithmetic works memory space defined cost model need machine model 
exhibit efficient scheduling respect costs theoretical machine models 
keywords multithreading macro data flow languages line scheduling parallel complexity 

field parallel programming resulted definition extensions sequential languages theoretically proven efficient scheduled machine models 
languages allow explicit parallelism independently specific architecture 
performance program evaluated directly language performance model specifies costs primitive instructions rules composing costs program expressions :10.1.1.141.5884
adequacy theoretical costs effective performances execu capes brazilian scholarship cnrs inria tion architecture related scheduling algorithm 
cope programs including branching instructions unpredictable compile time schedule computed line 
costs include scheduling overheads 
line scheduling algorithms rely greedy list schedule 
consists line mapping ready tasks idle processors 
schedule leads nearly times optimal executions including scheduling overheads theoretical machine models pram 
achieve efficiency regardless various criteria knowledge execution required 
instance bound amount memory usage knowledge sequential schedule total ordering tasks results correct sequential execution may 
way list scheduling leads parallel computations achieving linear speed requiring space related sequential execution certain classes programs strict computations nested computations planar graphs 
furthermore practice due magnitude ratio local remote memory access costs significant improvement brought schedule knowledge data flow corresponding execution 
programming environments graph input 
languages designed enable line building data flow describing execution 
built top standard sequential language commonly high performance computing 
bound related overhead parallelism expressed user defines grain data control 
jade bsp instructions grouped block languages parallelism expressed asynchronous function calls cilk 
synchronizations occur sequence instructions expressed task creation specific statements sync instruction allows task synchronize 
explicit synchronization instruction bounds line computation data flow dependencies occur synchronization 
forbids online static strategies theoretical practical interest tasks known cost 
enable efficient scheduling distributed architecture migration mechanism required eventually move task blocked ready processor 
typing memory accesses task perform exhibit parallel language named athapascan ath stands asynchronous tasks handling explicit synchronization instruction allows line analysis data dependencies 
athapascan inspired jade concerning typing memory accesses cilk concerning parallelism expression 
section detail syntax semantic language 
section presents macro data flow computed line bounded overhead prove space time efficient executions achieved theoretical machine models need migration 
particularities implementation uses local multithreading detailed section section presents experimental measures distributed shared memory architecture 

athapascan language 
overview order deal data control flow grain defined user macro data flow parallelism expressed asynchronous remote procedure calls denoted tasks communicate synchronized access shared memory 
athapascan semantics rely shared data access ensure value returned read statements written value copy lexicographic order defined program statements lexicographically ordered 
choice sequential semantic motivated direct readability program source obvious example fig 

order defines total ordering tasks execution 
control accesses semantic execution entirely data driven precedences tasks needed communications data copies ensured automatically runtime system 
consistency scheme objects entries done tasks corresponding release tasks 
prototype task specifies accesses performed shared objects stands read write 
tasks priori independent conflicts tasks access object solved total lexicographic ordering 
instance fig 
task update precedes print lexicographic order print delayed update resumes 
program print output 
athapascan implemented library fully compatible languages 
simple ansi extension handled basic preprocessor 
sake simplicity syntax recognized preprocessor library easier replacing typical constructions keywords 
task update shared write task print shared printf read task test shared fork update fork print 
lexicographic semantic 

syntax representation tasks closures task definition similar procedure definition having simply void returned type replaced task keyword task user task task implements sequential computation granularity fixed user created program statements prefixing standard procedure call fork keyword fork user task 
statement creates object called closure gives scheduling current scheduler returns continuation asynchronous task creation 
closure data structure contains instantiation user task defines method run list effective parameters 
closure said ready arguments read task ready waiting argument read ready 
parameter said ready task iff task predecessor completed lexicographic order write parameter 
state closure directly linked state effective parameters read task 
types parameters distinguished classical parameters value ready closure possesses copy second parameters shared data versions 
shared data versions shared memory allows tasks synchronize composed shared data 
memory object type declared follows shared type defines granularity data handled algorithm 
succession versions associated shared data shared data version represents value certain instant execution 
declared shared data creates object called evolution containing pointers transitions transitions objects managing allocation state accesses data versions 
transition manages version data read current version shared data second version generated say written execution task version shared data 
shared data version possess methods access void write const const read void const example 
fig 
shows different data structures compose athapascan system 
objects dynamically allocated heap return heap completed task execution case closures access data case transitions 
shared data ready closure waiting closure waiting transition waiting transition evolution task value executable code ready transition parameters 
internal data structures 
fig 
shows codes computing nth fibonacci number 
version familiar recursive procedure computes nth fibonacci number 
threshold reached fibo task just writes result shared data res creates tasks compute concurrently previous fibonacci numbers task sum add numbers write result shared data res 
summing task delayed fibonacci tasks completed access shared data incompatible write read fibonacci tasks created 
second version cumulative version sum tree developed version 

lexicographic ordering parallelism order respect sequential consistency lexicographic order semantic athapascan identify related task fibo int shared res shared fork fibo fork fibo fork sum res task sum shared shared shared write read read recursive version cumulative add int const int task fibo int shared cw res fork fibo fork fibo cumulative version 
versions code compute nth fibonacci number 
data version read performed shared data 
parallelism detection easily possible context tasks define shared data objects accessed execution independent tasks detection type access performed tasks precedence detection shared data versions evolution 
reason athapascan tasks perform side effects manipulated shared data located parameter list shared parameters typed manipulation 
access right evolution shared data versions 
declaration formal parameters tasks shared data version typed access right kind manipulation task sub tasks due lexicographic order semantic allowed perform shared data 
rights read write modifications writing cw accumulation read 
concerning accumulation 
accumulation law user defined law supposed associative commutative 
note law part type mix different laws allowed obey lexicographic order semantic fig 

initial value previous value shared data lexicographic access order 
postponed access right 
improve parallelism refinement access right denotes access performed shared data 
access said postponed access right task perform access shared data create tasks may benefit right 

data flow building cost model adding restrictions accesses performed task establish representation data dependencies computed line distributed architecture bounded overhead time memory usage 
enables definition cost model related language 
exhibit scheduling algorithms provable performances respect cost model theoretical parallel machines 

shared data version access graph order able determine state transitions closures athapascan dynamically maintains graph shared data versions access 
graph composed closures transitions nodes pointers evolutions edges 
instant gives partial description data flow dependencies occur program 
evolution graph happens task shared data creation addition edges nodes task termination removing edges nodes node state evolution shown fig 

schedulers take benefit graph maintained semantic reasons system 
cost relevant information application schedule 

bounding cost athapascan statements due access semantic lexicographic order accessed values easily determined source code 
possible evaluate time memory cost athapascan program 
order bound cost line building evolution graph restrictions added ffl graph modifications task creations local need communication 
discussed section 
ffl shared data versions read read right direct mode task ready task execution 
follows tasks directly read shared data allowed create tasks write shared data creator task wait resumption newly created task read new shared data version access semantic respected 
consequence type shared wp sense example task having shared create task requiring shared formal parameter 
ffl task creation fork generates data copy 
follows tasks directly write shared data allowed create tasks read shared data copy written value stored created task creator stopped created resumes reads shared data version 
consequence type shared rp sense instance task having shared create task requiring shared formal parameter 
definition correct athapascan program verifies syntax conditions note strong typing accesses shared memory enables verification correctness program compile time 
allowed conversions shared data version types task creation summarized fig 

waiting ready waiting res res ready ready fibo fibo waiting ready ready waiting waiting fibo fibo fibo res ready ready waiting sum res waiting waiting fibo fibo fibo waiting ready sum ready waiting waiting fibo fibo waiting waiting res ready waiting res sum waiting ready waiting fibo 
dynamic evolution shared data versions access graph recursive version fibo initial state creation shared creation fibo tasks creation sum task just fibo just execution completion fibo 
formal parameter required type effective parameter shared shared shared rp wp shared shared shared rp wp shared cw shared cw shared rp wp shared shared rp wp 
allowed conversion passing shared data version parameter task 
lemma correct program athapascan statement fork read write access shared bounded cost time memory space 
graph modification constant time space 
results shared types conditions lemma precedence relation task task creates 
results shared types condition property enables exhibit sequential schedule tasks denoted order respects sequential semantics different classical depth sequential 
denotes sequence statements containing fork statement denotes fork statement 
trace corresponding sequential depth execution fork statements represented word gamma fn gamma may empty 
entire independence created task creator task lemma implies trace semantically equivalent trace gamma fn gamma order execution corresponds inner outer order evaluation called sequential order statements evaluation denoted sections 
fig 
illustrates execution order 
proposition athapascan order execution respects semantic requires implicit copy 
results conditions note value passing mode generates copy considered explicit 
assumptions concerning copies synchronizations ensure athapascan system responsible memory requirement decisions taken scheduling policies user 
enables evaluate cost program directly code 
task user task stmts fork task stmts fork task stmts depth sequential order task user task stmts stmts stmts fork task fork task sequential order 
equivalent programs 

cost measures proposition enables non preemptive schedules execution closure delayed parent task created resumes 
order costs time space depth communications defined directly code related recurrence equations max 
notations costs defined trace execution 
trace represented bipartite dag see fig 
node sets corresponding respectively tasks oval nodes fig 
shared data versions box nodes fig 

task node weighted computation cost related execution body excluding forked tasks data node weighted size data direct access white box unit constant postponed access black box 
note costs related trace may unknown execution completes usually bounded respect size input 
denote sequential time space parallel time communication volume delay 
sequential time space defined serial execution program ther order fork statements pushed memory completion task noted space larger required depth execution 
number fork statements bounded constant increased constant factor 
instance space related program fig 

parallel time arithmetic depth parallel time depth account weights task nodes 
lower bound minimal time required non preemptive schedule unbounded number processors ignoring communications times pram model 
communication volume delay 
communication volume delay evaluated similarly account weights shared data version nodes 
sum weights data nodes assuming shared memory emulated auxiliary file upper bound total number accesses performed file serial execution 
length critical communication path scheduling overhead oe 
overhead involved scheduling graph involved denote oe size scheduling program require oe scheduling operations may performed parallel 
sections study scheduling athapascan program various machine models time space required execution machine including cost scheduling algorithm related costs defined program independent machine 

scheduling pram consider pram bounded number processors allows eliminate communication overheads 
order consistent athapascan consider crcw pram cumulative concurrent write ones 
sake simplicity assume execution task performs bounded number say fork statements 
proposition including scheduling overheads athapascan program executed pram time oe gamma log log oe oe schedules deterministic non preemptive 
schedules greedy list strategy processor idle gets ready closure performs execution 
bounds differs implementation strategy 
obtain global lock implemented pram modification evolution graph performed mutual exclusion 
number modifications bounded oe idle time corresponding busy wait lock management evolutions graph bounded oe 
distributed management list leads bound modifications evolution graph performed specific subset log processors time log oe 
gamma log processors dedicated execution closures 
bounds differs scheduling algorithm knowledge weights possible decide achieves best bound 

scheduling distributed architecture consider scheduling distributed architecture dcm identical processors 
shared memory emulated help universal hashing functions 
delay occurring access assumed bounded 
order obtain efficient constant small slackness strategy 
consists emulating pram distributed architecture larger compared distributed machine assumed emulate pram delay 
proposition including cost schedule computation athapascan program executed dcm time oe schedule non deterministic preemptive requires migration running closures 
proof deduced proposition applied pram processors 
number remote access delay bounded leads schedule virtual processors length bounded obtain emulation processors virtual processors emulated processor synchronous preemptive threads 
due emulation shared memory algorithm nondeterministic 
processor threads dedicated execution closures emulated preemptively 
closure starts execution processor migrated 
corollary athapascan programs verifying oe scheduled asymptotically optimally distributed architecture time ffl result similar cilk programs 
negative result computations involves large number communications may efficiently scheduled 
instance program linear serial cost size input requires omega gamma accesses 
schedule time efficiency depends heavily 
scheduling space constraint previous schedules guarantee bound concerning space required 
list ready closures sorting list order allows bound memory space required respect 
due lexicographic semantic list maintained sorted overhead order insertions deletions performed constant time 
tasks assumed allocate memory space execution body space required resulting schedule distributed machine bounded 

distributed implementation section describe distributed implementation athapascan 
focus transition distributed management scheduling implementation 
particularly exhibits performance results obtained list scheduling theoretical efficiency studied previous section 
order bound delay occurring remote accesses compute node parallel machine emulates certain number virtual processors threads share single address space 
threads implemented runtime kernel called athapascan defines parallel machine composed set nodes executing runtime environment providing communication synchronization facilities local scheduling threads enabling hide communications latencies remote memory access 
athapascan implemented runtime environment bounded number threads architecture 
provides global scheduling closures threads architecture 

shared data versions access graph management compile time verification correctness athapascan program code generated fork statement order create corresponding closure 
management data dependencies closures shared data versions access graph distributed closures edges unique system transitions may replicated fig 

closure locally accesses connected transitions pointers possessed evolutions 
accesses shared data versions tasks creations local events create communication 
time required task creation proportional number parameters 
order detect termination accesses transition distributed asynchronous environment termination algorithm implemented 
transition associated master node computes balance increasing counters related replicate 
counters managed locally site possesses replicate 
local access site values local counters sent master node 

scheduling implementation global scheduling algorithm determine site date trigger execution closure 
algorithm implemented level scheduling distributes generated application attempting optimize global index performance memory execution time 
creation fork completed state closure depends graph evolutions 
added states waiting ready paragraph closure may get state running instructions executing sequentially executed closure completed 
change state potentially trigger scheduling action 
operation graph signal sent related scheduler allowing explore new graph configuration 
get information requires current global state execution instance closure attributes parameters state precedence constraints 

scheduling layers experimentation fig 
performance results time seconds obtained execution fibonacci program fig 
input values threshold halting task generation roughly half generated closures require micro second ones sequentially compute fibo milliseconds 
tasks generated producing edges 
experiments performed architectures ibm rs aix pentium solaris distributed architectures nodes ibm sp architecture ibm rs aix nodes network workstation sum res fibo node node ready waiting waiting waiting waiting ready 
replicated transitions distributed graph 
fibo completes message sent node warn transition writers exist 
node architecture nodes parallel architecture node smp architecture ibm sp ibm sp proc node proc node proc node proc node proc node sequential ath sequential thread threads threads threads 
influence local scheduling threads global greedy line scheduling 
solaris lam mpi myrinet smp pentium pro solaris 
table lines correspond respectively execution pure sequential algorithm athapascan program compiled generate sequential code parallel execution execution threads node 
comparing performances pure sequential algorithm sequential version athapascan code verify overhead introduced athapascan programming style small 
true parallel version observe overhead produced scheduling graph management 
overhead partially overlapped number threads node increased 
parallel architectures speed obtained processors 
speed ups close obtained thresholds larger 

athapascan language enables line building data flow dependencies bounded overhead 
semantic lexicographic order instructions enables implicit sequential nonpreemptive schedule 
grain data computation explicit independent target architecture 
parallelism expressed asynchronous creation tasks 
task scheduled started continue execution preemption 
property enables provable scheduling 
language related cost model defines parallel depth sequential space communication volume delay 
efficient schedules developed achieve optimal time bounded space distributed architecture large class programs 
lots practical applications scheduling algorithm distributed architecture leads poor performances far ones obtained simple static strategies 
scheduling changed code annotation issue take benefit line partial knowledge annotated data flow graph order implement line context strategies 
blelloch :10.1.1.141.5884
programming parallel algorithms 
communications acm 
blelloch gibbons matias 
provably efficient scheduling languages fine grained parallelism 
proc 
th symp 
parallel algorithms architectures pp santa barbara 
acm press 
blelloch gibbons matias 
space efficient scheduling parallelism synchronization variables 
proc 
th symp 
parallel algorithms architectures 
acm press 
blumofe leiserson 
space efficient scheduling multithreaded computations 
siam journal computing 
ginzburg plateau 
athapascan runtime efficiency irregular problems 
proc 
europar 
passau 
aug 
fortune wyllie 
parallelism random access machines 
proc 
th acm symposium theory computing san diego ca 
acm press 
graham 
bounds multiprocessing timing anomalies 
siam appl 
math 
gautier 

regular versus irregular problems algorithms 
proc 
irregular lyon france 
springer verlag lncs sept 
hill lang mccoll 
proposal bsp worldwide standard library preliminary version tr www bsp worldwide org oxford university 

parallel algorithms 
addisonwesley reading massachussets 
joerg 
cilk system parallel multithreaded computing 
phd thesis massachussets inst 
tech jan 
karp luby auf der heide 
efficient pram simulation distributed memory machine 
algorithmica 

emulate shared memory 
proc 
th annual symp 

computer science ieee 
rinard 
design implementation evaluation jade portable implicitly parallel programming language 
phd thesis stanford university sept 
valiant 
bridging model parallel computation 
comm 
acm 
yang 
static task scheduling code generation message passing multi processors 
proc 
vi acm int 
conf 
supercomputing july 
