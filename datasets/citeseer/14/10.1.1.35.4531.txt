self organized formation various invariant feature filters adaptive subspace som teuvo kohonen samuel kaski helsinki university technology neural networks research centre fin espoo finland neural computation august pp 

fl massachusetts institute technology 
self organized formation various invariant feature filters adaptive subspace som teuvo kohonen samuel kaski helsinki university technology neural networks research centre sf espoo finland adaptive subspace som assom modular neural network architecture modules learn identify input patterns subject simple transformations 
learning process unsupervised competitive related traditional som self organizing map 
neural module adaptively specic restricted class transformations modules close network tuned similar features orderly fashion 
dioeerent transformations exist input signals dioeerent subsets assom units tuned transformation classes 
long standing problem articial perception recognize patterns subject simple transformations translation rotation scaling 
frequently held view invariances achieved ltering pattern features mapped respective invariance classes 
equally important intriguing problem ensues determine set eoeective feature lter functions arbitrary statistics dynamic signals 
problems solved biological systems eoeective way 
article devise corresponding technical solution 
traditional image analysis pattern recognition techniques primary observations usually preprocessed separate stage extracts number 
basis features statistical decision making machine able identify classify target invariantly 
applications prior standardization samples possible instance targets moving illumination varying wavelet transforms gabor transforms gabor daubechies popular extractors approximately invariant local features 
transforms optimizable standard techniques limitation mathematical forms xed priori 
learning scheme discussed previous approaches forms lter functions learned directly short segments episodes dynamical signals pattern samples 
samples episode derived linear transformation translation span special manifold input signal space linear subspace 
assom adaptive subspace self organizing map architecture discussed article introduced authors kohonen kohonen kohonen kohonen continuing study 
scheme various feature lters emerge learning tuned manifolds input patterns dened transiently short segments signals occur suoeciently 
lters learn pattern prototypes dioeerent low dimensional manifolds spanned consecutive signal pattern vectors 
discussion believed constitute viable alternative standard pca principal component analysis methods feature extraction hotelling neural approaches oja rubner cichocki unbehauen 
extract average features global stationary statistical properties pattern sequence 
assom contrary creates set statistical representations referring different temporal events competitive winner take selection learning representations selectively concentrates dioeerent class temporal episodes 
due spatial interactions processing units assom learning characteristic som self organizing map methods kohonen kohonen general various feature lters emerging process organized spatially processing units ordered characteristic feature coordinates 
mention couple works ldi wallis neural units learn sequences simple patterns 
works xed input layer layer architecture projects competitive second layer winning unit learns trace successive inputs 
results obtained model explain emergence input layer lters essential learning arbitrary manifolds 
eoeective organized non trivial invariant feature lters ensue cooperation kinds neural components constituting assom network architecture discussed 
auxiliary concepts set discuss subproblems section invariant recognition pattern invariant recognition feature components 
ii denition invariance classes feature components linear subspaces 
iii adaptive learning invariance classes short sequences input patterns called episodes 
decomposition pattern features mapped invariance classes vectors represent dynamic sensory environments usually distributed manifolds continuous subsets zones vector space 
consider tentatively input pattern represents fundamental feature component complex object simple gabor functions 
variations caused particular class transformations basic pattern translation thought span low dimensional manifold 
simplest transformations manifold represented linear subspace vector linearly dependent 
class linear transformations fundamental component mapped linear subspace regarded invariance class 
complex pattern decomposed sum simple feature components approximately terms sum mapped respective linear subspaces invariantly pattern represented invariantly respect linear transformation 
sample vectors input neural network model may described real vector consider sample vectors consist scalar samples collected intensity function dioeerent time lags referred instant time gamma gamma gamma alternatively input vector may composed samples dimensional input image collected points intensity time dependent point sampling points displaced relative amounts gamma gamma gamma respectively form regular irregular sampling grid 
time series images patterns eventually representable higher dimensional domains discussed framework general vector space formalism 
episodes signal subspaces orthogonal projections basic problems linear subspaces invariance classes dened test unknown pattern belongs subspace 
consider instance time domain signals 
sample vectors collected successive points set nearby sample vectors fx spans linear signal subspace total input space 
set time instants ft called episode 
maximum dimensionality subspace equal number sampling instants dimensionality smaller sample vectors dependent case 
connection subspaces invariances lies realizing signal subspace invariant feature input vector linear subspace dimensionality general completely dened general linear combination linearly independent basis vectors bh choice unique exist equivalent combinations orthonormal 
set basis vectors known set equivalent orthonormal basis vectors computed familiar gram schmidt process textbook account cf kohonen 
basic operation determine unknown vector belongs orthogonal projection denoted belongs arbitrary dene distance xk kx gamma xk norm euclidean 
orthogonal projection computed simple expressions provided orthonormal expression computationally lighter 
belong subspace exactly closest 
factor rst parenthesis projection operator matrix abbreviated holds pattern space divided pattern zones contains subspaces 
optimal separating surface ith jth zone set points projections respective subspaces equal norms 
separating surface dened quadratic equation gamma gamma projection operators respectively 
due subspaces squared lengths projections neural network consists linear subspace modules 
modules separated dashed lines 
quadratic neuron 
network model may helpful imagine set subspaces invariance classes corresponds layered neural architecture organized modules function processing units fig 

modules represents subspace denoted neurons input layer linear weight vector neuron input layer module denoted input respectively output neuron weight vectors form basis subspace referred basis vectors 
neurons output layer quadratic transfer function practical simulations vectors module kept mutually orthonormal simply sum squares inputs second layer neuron 
sum squares formed output neuron module equals squared norm projection input subspace kx output module interpreted degree matching input subspace 
output module invariant neglecting eventual changes norm input restricted linear transformations occur subspace represented module 
competitive learning process discussed modules learn represent dioeerent invariance classes 
competitive learning network involves neighborhood interactions modules similar neighborhood function standard som algorithm kohonen kohonen 
due interactions neighboring modules learn represent similar features similarly transformed features array modules tuned features orderly fashion 
scale network due interactions dioeerent areas network may specialized dioeerent input types transforms 
theory assom emphasized want create sets feature lters want lters self organized spatial order similarity feature values detect 
easier inspect produced better compared existing neural realms 
learning assom units learn invariant approximately transformations exist environment modifying subspaces improve matching input signal subspaces 
spanned successive samples episodes discussed sec 

order approximate input episodes subspaces low dimensionality experiments described xed assom modules lters minimum number required extract invariant features cf 
gabor lters 
quite possible lters 
modules compete input signal subspaces module represents signal subspace best wins competition neighbors array adapted represent input subspace better 
way dioeerent modules gradually specialized represent dioeerent types input subspaces 
modules neighborhood winner adapted represent input better neighboring modules gradually learn represent similar inputs eoeect regarded kind smoothing neighboring representations 
basis self organizing map algorithm weight vectors processing units spatially ordered map array 
assom learning process summarized general terms follows 
locate module subspace closest signal subspace input episode 
adjust subspace winning module modules neighborhood assom array closer signal subspace 
direct comparison signal subspace subspaces modules dioecult instance dimensionality may dened vaguely due noise samples 
simpler robust denition match input subspace subspaces comparison input sample episode separately subspaces computation sum squares projections input samples episode subspaces assom modules 
module largest energy sum squared projections samples collected episode wins competition 
denote projections samples collected 
episode indexed dened expression arg max kx equivalently arg min nal goal competitive learning subspaces minimize average expected error stochastically selected input subspace represented subspace winning module 
somewhat related error principle classical vector quantization gersho gray makhoul 
distance subspace spanned input samples subspace measured terms relative projection errors input samples 
consider tentatively neighborhood interactions modules taken account input vectors normalized 
average expected distance input subspace subspace winning module computed space possible cartesian products input samples collected episodes measured terms projection error expressed objective function dx probability density dx shorthand notation volume dioeerential integration space cartesian product samples episode 
index winning module depends episode subspaces modules exact minimization may complicated task kohonen kohonen alternative treatments cf buhmann hnel luttrell 
shall resort classical robbins monro stochastic approximation robbins monro minimizing way derivation traditional som algorithms kohonen kohonen 
emphasized objective function need lyapunov energy function convergence stochastic approximation established generally cf 
albert gardner kushner clark 
objective function describes goal pure unordered competitive learning 
kohonen kohonen enforce ordering old vector quantization problems integrand error function smoothed locally respect processing modules leads som algorithms 
multiplying integrand neighborhood kernel decreasing function distance modules assom array 
new objective function reads dx attempts optimize cost functions type succeeded closed form possible robbins monro stochastic approximation derive som algorithm kohonen 
comparison exact approximate optima 
approximation gradient evaluated basis input values sampling sequence 
set regarded constitute stochastic approximation 
optimizing original objective function approximation step direction negative gradient sample function taken recall gamma function 
basis vectors kept orthonormal gradient sample function respect basis vector gamma comment 
index changed abruptly signal subspace passes border closest subspaces necessary stipulate signal subspace belong neighborhood border 
continuous stochastic signals possibility neglected 
cf 
similar discussion kohonen kohonen 
comment 
basis vectors span subspace unique gradient descent method optimum unique 
notwithstanding may clear optima minimized equivalent solution optimal basis 
step length direction negative gradient obtain rule gamma parameter called learning rate factor 
stochastic approximation satisfy conditions robbins monro kushner clark 
learning law operator applied basis vector derivation learning law input vectors tentatively assumed normalized 
normalization incorporated objective function considering relative projection errors kx rotation operator kx earlier kohonen kohonen shown slightly different law product unnormalized rotation operators competing units learn dioeerent input subspaces kx small immediately equivalent 
fact seen batch version episode operation 
authors earlier proved called adaptive subspace theorem kohonen states subspace converge surely signal subspace samples picked random 
shall henceforth operator applies input samples succession extra operations explained sec 
shall operation 
operator fact corresponds hebb type learning synapse component basis vector proportional product input output neuron 
expressions preserve norms basis vectors performance basis vectors number dozen learning steps 
enhancement organizing power emphasized order achieve suoecient stability selforganizing process obtain distributed lter sets advice section necessary 
ensure eoeective self organization correction neural unit monotonically increasing function error 
case corrections neighboring units strongly depend basis vectors correction lead unstable self organization 
magnitude correction subspace monotonically increasing function true error kx gamma monotonically decreasing function kx projective methods correction cf 
eqn proportional decreases increasing angle simple method guaranteeing correction increasing function error cf 
kohonen divide learning rate factor scalar value kx kxk changes eoeective learning rate 
operator kx course simulations reported sec 
turned organization assom perfect modied rotation operator 
instance input consisted speech signals modules prone tuned separate frequency bands 
explanation phenomenon high dimensional lter general easily learn complicated function mainly virtue competitive learning process single lters formed process tries distribute orderly fashion frequency domain 
band lters formed self organization frequency range bands remained unrepresented assom array gap distribution formed cf 
sec 

speculation may mentioned instability eliminated simple method empirically demonstrated produce lters better distributed frequency domain 
learning step magnitude small components basis vectors set zero basis vectors forced approximate stronger fundamental frequency components 
denote basis vectors hn simple kind described equation hj sgn hj max jb hj gamma small term 
amount dissipation fraction magnitude correction gamma gamma ff small scalar parameter 
dissipation applied rotation normalization 
regularizing principles increased stability self organization experiments 
instance exist excessive degrees freedom representation basis vectors various symmetries 
degrees freedom result asymmetric lters instance lters may centered point sampling grid 
order avoid assom organized irrelevant feature dimensions exists simple method favoring middle sampling grid learning process respects viz weighting lattice points gaussian function centered middle 
function wide form lters quite near edges 
input vectors weighted similarly weighting alter transformation group inherent input vectors enhances symmetry 
speed organization process gaussian weighting function time dependent 
organizing process function selected narrow order middle points rst 
width gaussian increased modules learn represent inputs accurately peripheral points sampling grid 
method enhancing organizing power standard method organization ordinary self organizing maps neighborhood kernel dependent time 
learning kernel wide comprising half map 
width gradually decreases nal value map approximates inputs closely time preserving global ordering achieved 
simulations simple box shaped kernel 
modules certain radius winner updated equally modules updated learning step 
summary assom learning algorithm learning episode consisting successive time instants ffl find winner indexed arg max jjx jj ffl sample 
rotate basis vectors modules kx 
dissipate components hj basis vectors hj sgn hj max jb hj gamma gamma gamma 
basis vectors module 
seldom say steps 
ff suitable small scalar parameters 
experiments shall show kinds invariant feature lters emerge assom learning process dioeerent sensory environments input assom subjected various transformations 
emerging lters statistical characteristics short sequences input patterns waveforms 
lters learn pattern components short input sequences satisfy certain linear constraints lie linear subspace 
patterns satisfy constraints averaged long run 
resulting lters fact represent kernels transforms produce invariant outputs 
reporting nal results shall rst dene details simulations 
demonstrate enhancement organizing power due nonlinear eoeect introduced sec 

details simulations section summarize kinds input patters transformed 
translated speech signal 
rst experiments input signal consisted digitized time domain speech waveforms timit speech data set contains phrases spoken large number speakers sampled khz 
picking input vectors high frequencies signal enhanced dioeerences successive samples high pass ltering 
sample speech waveform 
locations input vectors starting times belonging episode line segments speech waveform 
input vectors consisted successive samples speech waveform 
learning episode began randomly chosen time instant consisted vectors displaced random amount samples average previous 
transformation group inherent data consisted translations time 
segment original signal input vectors forming episode fig 

dimensional colored noise 
earlier kohonen demonstrated dimensional lters formed response photographic images 
wanted specify images statistically 
rest experiments reported dimensional patterns consisting colored noise 
patterns formed low pass ltering white noise second order butterworth lter cut ooe frequency cent nyquist frequency 
image sampled circular uniformly spaced sampling grid consisting pixels 
experiments inputs translated horizontally rotated scaled 
center rotation scaling operations xed center sampling grid 
signal subspaces spanned samples episode regarded low dimensional transformations samples modest 
amount translation varied pixels equal steps cent random displacement 
amount rotation ranged radians divided equal steps having cent random rotation step 
amount scaling episode increased equal steps cent random variation applied step 
translated images weighted vertical gaussian kernel force lters vertically centered location cf 
sec 

kinds inputs scaled rotated weighted 
samples transformed colored noise patterns 
translated rotated scaled inputs 
partial gure forms episode 
inputs high pass ltered time dioeerences successive input vectors enhance changing parts input 
high pass ltering necessary scaled images center scaling remains approximately constant 
functioning invariance lter model predominantly invariant area changes stimuli produced scaling 
ltering number samples episode number decreased dioeerences 
exemplary episodes translated rotated scaled colored noise shown fig 
methods 
assom lattice consisted stated units organized dimensional array 
assom unit contained basis vectors initialized random values learning 
inputs normalized unit length 
learning process summarized sec 

neighborhood kernel box shaped neighborhood winner units 
basis vectors adaptation step 
case computing time critical seldom 
learning process parameters changed fig 

demonstration enhancement organizing power preliminary experiment dissipation 
rst experiment assom algorithm input consisted translated speech signals 
learning episodes assom units represented dioeerent frequency components orderly fashion fig 

basis vectors unit tuned approximately similar frequency band degree phase shift respect 
output squared projection input subspace unit units selective certain frequency band invariant translations time long frequency content input change 
learning episode fwhm radius radius fwhm learning parameters vs number learning episodes 
learning rate factor form delta total number episodes learning 
steps radius box shaped neighborhood decreased linearly lattice spacings spacing 
gaussian function weighting sampled input pattern changed curve marked fwhm full width half maximum expressed terms sampling points 
fwhm increased linearly sampling points 
tentative results translated speech waveforms inputs 
notice diagrams parts 
rst basis vectors assom units 
second basis vectors 
learning period extended turned organizing result stable 
continued learning gap appeared representation 
units tendency representing usually distinct frequency components compact frequency band range input frequencies left unrepresented 
occurred especially minima frequency spectrum 
fig 
shows lters learning process consisting episodes 
clear defect organization wide gap frequencies hz distribution dominant frequencies units fig 

notable frequency response dened output unit function input frequency unit number closest gap consists separate peaks corresponding borders frequency gap fig 

ability assom lters represent frequencies time discussed sec 

reducing excessive degrees freedom dissipation 
dissipation term discussed sec 
introduced frequency responses units obtained single relatively narrow fig 

value dissipation constant ff 
gap distribution dominant frequencies assom units closed fig 

steepest slope distribution lters corresponds local minimum frequency spectrum 
fig 
ab inspected carefully eoeect nonlinearity due dissipation seen small shoulders basis vectors small values 
may interesting note eoeective gaussian shaped determined wavelet shapes fig 
width optimized lter separately 
formation various invariant feature filters environment transforms modify inputs determines kinds lters emerge learning process 
previous section input consisted translated speech signals lters developed wavelet modulated waves 
experiments show dioeerent types lters learned input translated horizontally rotated scaled 
system model time input samples experiments consisted type colored noise dioeerences resulting lters totally determined characteristics transformation type 
total episodes consisting transformed sample patterns 
value dissipation constant ff 
inputs translated randomly image plane horizontal vertical direction shown earlier basis vectors resembling twodimensional gabor functions emerge kohonen 
show comparison translation mainly restricted direction say unit number results longer learning process tentative results turned unstable long run 
notice diagrams ad parts 
rst basis vectors assom units 
second basis vectors 
response units dioeerent frequencies fourier transforms respectively 
dominant frequencies units 
bars indicate fwhm frequency responses unit number nal results speech waveforms nonlinear dissipation eoeect included 
notice diagrams ad parts 
rst basis vectors assom units 
second basis vectors 
response units dioeerent frequencies fourier transforms respectively 
dominant frequencies units 
bars indicate fwhm frequency responses basis vectors basis vectors assom invariant horizontal translations 
notice diagrams parts 
learning fwhm vertical gaussian weighting inputs changed 
horizontal lters invariant movements direction 
filters emerged input translated horizontally weighted vertical gaussian eliminate asymmetries cf 
sec 
shown fig 

lters modulated horizontal direction basis vectors unit degree phase shift 
environment rotated input patterns modulated basis vectors emerge 
time modulation occurs azimuthal direction fig 

phases basis vectors degree phase shift respect responses units invariant rotation inputs 
assom subspaces readily represent invariance classes linear transformations rotation translation 
nonlinear transformations approximately represented linearly better smaller amount transformation 
linear approximations nonlinear operation emerge assom inputs scaled respect center sampling grid 
basis vectors acquire sinusoidal modulation radial direction outwards scaling center fig 

phases basis vectors unit degree phase shift responses units invariant changing scale inputs 
competition dioeerent transformations input patterns subjected transformation types active dioeerent episodes assom units compete dioeerent transformations 
due neighborhood interactions learning neighboring basis vectors basis vectors rotation invariant assom 
notice diagrams parts 
basis vectors basis vectors scale invariant assom 
notice diagrams parts 
units invariant similar transformations areas devoted dioeerent transformations emerge 
demonstrate formation areas specializing dioeerent transformation types transformed input patterns dioeerent transformation types horizontal translation rotation scaling respectively dioeerent episodes 
transformations dened precisely previous section gaussian weighting applied inputs 
vertical centering lters horizontal translation favored varying tilt pattern respect sampling grid translation random amount uniformly distributed radians 
assom lattice consisted units radius neighborhood kernel decreased units learning 
learning proceeded exactly 
learning process distinct areas discerned assom fig 

units depicted rst row mainly invariant scaling units row mainly rotation 
units middle assom especially units best describable invariant horizontal translations 
dioeerent areas assom invariant dioeerent transformations input consisted horizontally translated rotated scaled patterns 
rst basis vectors assom units 
second basis vectors 
notice diagrams parts 
discussion article demonstrated invariant feature lters areas specializing dioeerent transformations emerge assom result hebbian type competitive learning 
learning rule derived minimizing error function robbins monro stochastic approximation 
objective function describes average expected error random sequence input samples compared corresponding best representation subspace best matching processing module 
simple nonlinear learning rule described sec 
invariant feature lters ordered robustly reliably similarity transformations 
capability network learn invariant features demonstrated natural speech signals broadband random images 
processing module network invariant transformation type decode certain range features invariantly transformation 
dioeerent competing modules specialize represent dioeerent kinds invariant features 
network function learning preprocessing stage extraction 
number basis vectors neural modules experiments 
resulting dimensional subspaces corresponded basic wavelet lters 
exists restriction number basis vectors algorithm subspaces larger dimensions readily represent complex features 
intriguing possibility neurons assom architecture biological counterparts ensues forms operation lters input output layers 
receptive elds simple cells mammalian primary visual cortex hypothesized describable gabor type functions daugman jones palmer assom process produces gabor type lters rst layer neurons 
output layer neurons respond invariantly moving transforming targets sense resemble complex cells 

authors professor erkki oja useful discussions 
albert gardner jr 
stochastic approximation nonlinear regression 
press cambridge ma 
buhmann hnel 
vector quantization complexity costs 
ieee trans 
information theory 
cichocki unbehauen 
neural networks optimization signal processing 
john wiley new york 
daubechies 
wavelet transform time frequency localization signal analysis 
ieee trans 
information theory 
daugman 
dimensional spectral analysis cortical receptive eld 
vision res 

daugman 
uncertainty relation resolution space spatial frequency orientation optimized dimensional visual cortical lters 
opt 
soc 
am 

ldi 
learning invariance transformation sequences 
neural comp 

gabor 
theory communication 
iee 
gersho 
asymptotically optimal block quantization 
ieee trans 
information theory 
gray 
vector quantization 
ieee assp magazine april pp 

hotelling 
analysis complex statistical variables principal components 

psych 

jones palmer 
evaluation dimensional gabor lter model simple receptive elds cat striate cortex 
neurophysiol 

kohonen 
self organized formation topologically correct feature maps 
biol 
cybern 

kohonen 
self organization associative memory 
rd ed 

springerverlag berlin 
kohonen 
self organizing maps optimization approaches 
articial neural networks kohonen simula kangas eds vol 
ii pp 

north holland amsterdam netherlands 
kohonen 
adaptive subspace som assom implementation invariant feature detection 
proc 
icann int 
conf 
articial neural networks fogelman gallinari eds vol 
pp 

ec cie paris 
kohonen 
emergence invariant feature detectors self organization 
computational intelligence 
dynamic system perspective marks ii fogel fukuda eds pp 

ieee press new york 
kohonen 
self organizing maps 
springer berlin 
kohonen 
emergence invariant feature detectors adaptive subspace som 
appear biol 
cybern 
kushner clark 
stochastic approximation methods constrained unconstrained systems 
springer verlag new york 
luttrell 
bayesian analysis self organizing maps 
neural comp 

makhoul gish 
vector quantization speech coding 
proc 
ieee 

mathematical description responses simple cortical cells 
opt 
soc 
am 

oja 
subspace methods pattern recognition 
research studies press england 
oja 
principal components minor components linear neural networks 
neural networks 
robbins monro 
stochastic approximation method 
ann 
math 
statist 

rubner 
self organizing network principal component analysis 
europhys 
lett 

timit 
cd rom prototype version darpa timit acoustic phonetic speech database 
wallis rolls ldi 
learning invariant responses natural transformations objects 
proc 
ijcnn int 
joint conf 
neural networks nagoya pp 

ieee service center piscataway nj 

stochastic approximation 
cambridge university press cambridge great britain 

