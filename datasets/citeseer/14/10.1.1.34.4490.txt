collective data mining new perspective distributed data analysis kargupta park hershberger erik johnson school electrical engineering computer science washington state university wa usa eecs edu introduces collective data mining cdm framework new approach distributed data mining ddm heterogeneous sites 
points naive approaches distributed data analysis heterogeneous environment may result ambiguous incorrect global data models 
notes function expressed distributed fashion set appropriate basis functions orthogonal basis functions ectively developing general ddm framework guarantees correct local analysis correct aggregation local data models minimal data communication 
develops foundation cdm discusses decision tree learning polynomial regression cdm discrete continuous variables describes bodhi cdm experimental system distributed knowledge discovery 
distributed data mining ddm fast growing area deals problem nding data patterns environment distributed data computation 
today data analysis systems require centralized storage data increasing merger computation communication demanding data mining environments exploit full bene distributed computation 
example consider data analysis applications 

example interested nding dependency emergence hepatitis weather pattern 
access large hepatitis database center disease control environmental database environmental protection agency 
di erent places analyzing data conventional data mining software requires combining databases single location may quite impractical 

example ii major nancial organizations want cooperate preventing fraudulent intrusion computing system 
need share data patterns relevant fraudulent intrusion 
want share data sensitive 
combining databases may feasible 
existing data mining systems handle situation 

example iii defense organization monitoring situation 
sensor systems monitoring situation collecting data 
fast analysis incoming data quick response imperative 
collecting data central location analyzing consumes time approach may scalable modern situation monitoring systems large number di erent sensors 

example iv major multi national wants analyze customer transaction records quickly developing successful business strategies 
thousands establishments world collecting data centralized data warehouse followed analysis existing commercial data mining software takes long 
ddm ers viable solution practical problems 
ddm algorithm analyzes data distributed fashion modest data communication overhead 
typically ddm algorithms involve local data analysis followed generation global data model aggregation local results 
unfortunately naive approaches local analysis may produce ambiguous incorrect global data model 
particularly general case di erent sites observe di erent sets features problem critical 
developing grounded methodology address general case important 
ers viable approach analysis distributed heterogeneous data sets collective data mining cdm framework 
section describes ddm problem considered problems naive data analysis algorithms ddm environment 
section foundation cdm followed discussion construction orthonormal representation incomplete domains relation representation mean square error minimization 
sections development cdm versions popular data analysis techniques decision tree learning regression 
section presents overview cdm experimental system called bodhi currently development 
section concludes discusses research directions 
background section presents background material 
rst explains general model distributed heterogeneous data sites considered 
review related research ddm 
section concludes showing naive approaches ddm heterogeneous environment ambiguous incorrect simple data modeling problems 
problem description site site site site site site distributed left homogeneous right heterogeneous data sites 
ddm environment data may distributed di erent sites various reasons 
example environment may di erent databases 
hand data may arti cially partitioned di erent sites achieving better scalability 
data sites may homogeneous site stores data exactly set features 
general case data sites may heterogeneous site storing data di erent sets features possibly common features sites 
document consider relational model data 
data sets viewed tables rows columns 
general case hierarchy relational tables may exist site considers table site order build solid foundation starting simpler case 
table site homogeneous heterogeneous cases called horizontally vertically partitioned 
illustrates cases 
case homogeneous sites row de nes unique observation 
heterogeneous case di erent observed features event distributed di erent locations 
result heterogeneous case come prede ned way de ne correspondence di erent components event 
row index key common database schema features possibilities may identify correspondence di erent rows stored di erent sites 
chapter consider problem supervised learning analysis heterogeneous case 
set observed feature values task learn function computes unknown value desired feature function observed features 
set observed feature values called training data set 
right column denotes feature value predicted denote features predict data sets available di erent sites training data 
column observed broadcasted site 
exists little general case ddm 
section reviews related distributed data mining 
related distributed data mining fairly new eld enjoying growing amount attention inception 
large fraction ddm literature considers homogeneous data sites 
distributed data analysis homogeneous data sites involves combining di erent models data di erent sites 
exist techniques combine multiple data models 
class statistical techniques aggregating multiple models generated homogeneous data sets proposed :10.1.1.32.9399
bagging approach increases accuracy model generating multiple models di erent data sets chosen uniformly replacement averaging outputs models 
bagging technique initially developed increasing accuracy unstable data analysis algorithms extended multiple model aggregation ddm homogeneous data sets 
perspective 
bagging stacking ers alternate technique increase accuracy data model aggregating multiple data models 
stacking rst multiple models learned di erent homogeneous data sets joint generalization behavior observed di erent testing data set 
new model learned data sets knowledge joint generalization behavior classi cation target function prediction 
bagging stacking extended combining models distributed environment 
experimental investigation techniques combining multiple models reported 
meta learning ers class techniques mining homogeneous distributed data 
approach shares lot similarities bagging stacking 
approach supervised learning techniques rst detect concepts local data sites meta level concepts learned data set generated locally learned concepts resulting meta classi er 
di erent inductive learning algorithms may employed learn local concepts meta level learning may applied recursively producing hierarchy meta classi ers 
jam system meta learning distributed data mining framework 
fraud detection banking domain 
knowledge probing approach reported 
technique similar meta learning 
approach particularly designed inducing descriptive data model predictions black box classi ers learned distributed environment 
distributed cooperative bayesian learning approach developed 
technique considers homogeneous data sets 
approach di erent bayesian agents estimate parameters target distribution population learner combines outputs bayesian models 
agents population learner probabilistic version gibbs algorithm 
theoretical analysis algorithm shows performs centralized bayesian learning algorithm 
proposes monte carlo technique addressing problems hypotheses space hierarchically parameterized feedback mechanism population learner agent learners 
fragmented approach mining classi ers distributed data sources suggested 
method single best rule generated distributed data source 
rules ranked criterion number top ranked rules selected form rule set 
authors report technique automatically produce bayesian belief network discovered knowledge distributed approach 
system deals problem distributed data mining homogeneous data sites 
system implemented distributed clustering algorithm aided relevance supervised learning techniques 
system agent distributed architecture partial data cluster models rst computed distributed fashion di erent sites individual models collected central site second level clustering di erent models performed generate cluster model 
second level clustering performed di erent clusters exploiting statistical representations 
means clustering algorithm distributed environment reported 
algorithm notes inherent data parallelism means algorithm asymptotically approaches near optimal performance 
fast distributed mining fdm algorithm mining association rules distributed homogeneous data sets 
fdm notes distributed environment globally large itemset locally large sites 
explores relationship locally globally large itemsets order minimize communication overhead 
fdm requires message communication number data sites determining candidate set large 
architecture distributed data mining system plays important role performance 
architectural requirements ecient data communication wide area network explored 
reports tools persistent object manager called modeling language called predictive model markup language pmml model manager called object transportation layer named bast facilitate local data mining wide area combining processes 
doall primitive proposed reducing complexity parallel programming distributed environment 
reports scheduling algorithms assigning tasks processors improve load balancing 
presents empirically evaluation doall primitive scheduling algorithms dimensional discretization clustering problems 
technique problem decomposition local model selection proposed 
approach rst learns regression models local data sites 
identi es subset data local model works ne 
information partitions data di erent disjoint subsets 
learns distribution functions order identify appropriate local model data point 
approach applied analyze agricultural data authors report substantial improvement performance compared single global model 
exists little literature analyzing data heterogeneous sites 
learning heterogeneous data sites discussed perspective inductive bias 
notes partitioning feature space addressed decomposing problem smaller sub problems problem site wise decomposable 
world system addressed problem concept learning heterogeneous sites developing activation spreading approach 
approach rst computes cardinal distribution feature values individual data sets 
distribution information propagated di erent sites 
features strong correlations concept space identi ed rst order statistics cardinal distribution 
selected features learning appropriate concept 
technique rst order statistical approximation underlying distribution may appropriate general non convex concept space 
propagation marker activation records site accomplished basic database operations 
approach easily implementable database systems 
general methodology learning functions distributed heterogeneous data sites guaranteed control accuracy minimal communication overhead open issue 
ers possible solution problem 
describes collective data mining methodology learn di erent popular data models regression decision trees distributed environment 
interested readers may refer additional experimental theoretical analysis framework 
main motivation framework direct application existing machine learning statistical algorithms local data sites may produce partials models completely incorrect possibly ambiguous 
section illustrates observation 
naive approach may ambiguous incorrect data modeling mature eld understood techniques arsenal 
traditional techniques directly distributed environment vertically partitioned feature space 
section shall see simple decomposable data modeling problem ambiguous misleading distributed environment 
consider data set generated 
variables corresponding value observed site tting linear model form data quite straight forward 
consider distributed environment sites observes observes consider dataset da site left global error function 
right local error function site tries local linear model form fa data get di erent solutions coecients similar situation arises site resolving ambiguities requires communication sites 
collective data mining approach ers solution decomposable problem communication 
discussing cdm investigate possibility generating local models minimizes error correct value models 
unfortunately shown may lead misleading results 
consider function real valued variables 
consider sample data set entry form 
try model data minimizing mean square error 
mean square error computed data set left shows error surface global minima 
simple quadratic function nding minima quite straight forward 
consider data set vertically partitioned meaning observed site observed di erent site choose linear model mean square error function site 
right shows local error function 
clearly shows minima error function globally optimal value 
example demonstrates simple linear data model naive approaches minimize mean square error may misleading distributed environment 
cdm ers correct viable solution problem 
section presents foundation cdm 
foundations collective data mining inducing function structure data common problem statistics machine learning 
considers distributed function induction data sets known function values 
machine learning literature called supervised learning 
supervised inductive learning goal learn function data set 
generated underlying function approximates member domain 
xn tuple correspond individual features domain 
heterogeneous case mean learning function terms features observed di erent sites 
practical situations distributed inductive learning requires availability values 
sites 
applications observed data particular feature usually feature predicted class label events may naturally available 
information may broadcasted sites 
cdm approaches supervised data analysis modeling perspective function learning 
foundations blending theory communications machine learning statistics typical ddm algorithm works 
performing local data analysis generating partial data models 
combining local data models di erent data sites order develop global model 
saw previous section conventional approaches local analysis ambiguous misleading 
need sure local analysis produces correct partial models building blocks global model 
partial models generated local data sites step combine generating global model 
need keep mind nonlinear dependency features di erent data sites may exist 
locally generated partial models may sucient generate global model 
cdm addresses issues described 
foundation cdm fact function represented distributed fashion appropriate basis 
basis set 
index basis functions denote th basis function set indices basis functions 
function represented denotes th basis function denotes corresponding coecient 
objective learning algorithm viewed task generate function approximates data set denotes subset denotes approximate estimation coecients basis representation underlying learning task essentially compute non zero signi cant negligible coecients 
inductive data modeling algorithm regression decision trees neural networks viewed perspective 
cdm notes direct application modeling algorithms may produce correct results orthonormal spectrum models accurately learned distributed fashion nally model constructed spectrum 
orthonormality property guarantees correct independent local analysis building block global model 
main steps cdm approach 
choose appropriate orthonormal representation type data model built 
example preliminary investigation noted suitability fourier wavelet representations decision tree multi variate regression respectively 

construct orthonormal representation data distributed fashion 

construct model canonical representation tree case decision tree orthonormal representation continuous discrete functions learned fashion 
discussion shall discrete functions explaining basic concepts 
sections explain basic mechanism cdm simple example 
generating correct partial models local features consider quadratic data modeling problem data set generated function boolean variables 
note function site wise decomposable 
data set sites observe respectively 
naive approach faces ambiguous situation problem exists orthonormal basis functions modeling data 
example consider discrete fourier basis representation 
case set bit strings 
function boolean variables written denotes th fourier basis function denotes corresponding fourier coecient case fourier coecients de ned total number members domain 
fourier basis functions function variables clearly computation require computation requires information concerning respectively 
de nitions get site site computed site site computation require feature values 
easily demonstrate locally generated fourier coecients represent partial models nally put generating correct global data model 
order show locally generated model site generate data set fx similarly site generate dataset fx model solving set linear equations generate partial models canonical representation site results local model site similarly generated model site combining partial models need generate non linear term involving features site section describes process 
generating cross terms involving features di erent sites model completely decomposable data sites cross terms involving features di erent sites exist model 
determination cross terms essential accurate modeling data 
particular example hand exists coecient corresponds basis function requiring coecient computed feature values single row datasets 
general cross terms involving features di erent sites posed problem solving terms requires exactly rows data sites 
large databases typically smaller number compared total number rows databases 
example bring information rst data row site order complete row data fx 
combining locally generated partial models get expression know coecients easily solved get 
see fourier representation nonlinear model generated moving small fraction data 
cost moving data row site common site computing cross terms cost communication 
real life interesting problems exhibit bounded non linearity variables non linearly interact expected small number compared typical number rows large database 
partial model involving cross term generate remaining part global model canonical representation 
term involves general model part takes form generate dataset fx function resulting model canonical representation turns combine locally generated models canonical representation resulting illustrates locally generated fourier coecients cross terms computed communication cost ectively distributed data modeling 
nal issue need address 
far discussion considered data set exactly complete domain 
typically learning data analysis performed sample domain complete domain 
require ecient estimation coecients incomplete knowledge domain 
section addresses issue 
orthonormal representation construction incomplete domain construction orthonormal representation requires computation basis coecients 
exact procedure computing coecients depends speci set chosen basis functions 
di erent specialized fast techniques exist computing di erent orthonormal representations 
regardless speci choice function large number signi cant basis coecients require signi cant time computing orthonormal representation 
polynomial time computation coecients requires things sparse representation coecients zero negligible approximate evaluation signi cant coecients 
fortunately requirements deep connection foundation search machine learning optimization 
practical data mining applications conditions satis ed sacri cing quality process 
typically main objectives data mining application capture salient data pattern simple easy understand representation 
example classi cation problem want complex rules fairly accurate covers reasonable portion event space 
trying build predictive model polynomial regression techniques practical purposes model degree polynomial bounded constant 
black box learning techniques neural networks specialized restricted representations required ecient polynomial time learning 
simple decision tree learning algorithm id may engage exponential time worst case impose restriction depth tree 
practical applications decision tree want tree short depth give rise simple rules 
performance short tree may conclude decision tree may representation learning problem 
restriction representation certainly trade imposes algorithmic bias restricts scope algorithm 
necessary perspective eciency ecacy 
fortunately restriction data model re ected orthonormal representation 
example bounded depth decision tree sparse fourier representation polynomial number non zero coecients 
see chapter coecients exponentially larger rest coecients 
essentially means fourier representation decision tree approximated small number non zero coecients 
similar properties show decision rules involving bounded number features 
dependency di erent features bounded constant number features may depend particular feature orthonormal representation going sparse 
estimation orthonormal coecients done regular basis daily appliances require signal processing 
sample size reasonable typically case data mining adequately addressed 
illustrate rationale observation fourier basis example 
consider happens multiply sides equation get 
denote sample data set summing side members get wk note get sample size 
write sample mean population mean complete domain zero sample mean approach zero sample size increases 
large sample size typically case data mining problems term approach zero 
result fourier coecients computed large samples approximate exact coecients 
approximate computation basis coecients ecient di erent techniques 
possibility group coecients di erent equivalence classes space indices estimating individual groups 
consider classes fw fw character represents wild card 
de ne denote similarity equivalence classes de ned rst values index respectively examples flow computation di erent values 
respectively 
denotes intersection classes example note individual magnitude greater threshold value value greater fourier coecients index string starting signi cant magnitude 
schematically illustrates ow algorithm 
node tree approximately compute th node children value greater subtree discarded 
number non zero fourier coecients bounded polynomial recall assumption bounded nonlinearity able discard sub trees just checking root sub tree 
idea polynomial time algorithm developed learning boolean functions sparse fourier representation 
note intersection locally detected signi cant de nes superset indices corresponding signi cant cross terms 
eciently approximating orthonormal representation 
past proposed randomized deterministic algorithm eciently constructing fourier representation 
underlying data sampling process biased available data set large alternate techniques may estimate coecients 
may appropriate orthonormal representation standard error minimization algorithm square minimization technique tting orthonormal coecients available data 
approach illustrated section detecting decision rules 
earlier section saw minimizing mean square error local model lead incorrect results 
section shows orthonormal basis function representation su er problem 
orthonormal representation mean square error search appropriate data model posed model error minimization problem 
section show minimization error local model leads correct partial model building block correct global data model 
consider summing data points training set abbreviated representation 
note basis functions assumed orthonormal 
sum space consideration hand 
de ne random variable 
law large numbers approaches increases 
large write clearly sum square error minimized derivation assumes feature variables observed available model building time 
investigate situation changes feature space vertically partitioned 
assume feature space divided sets feature spaces respectively 
set basis functions de ned feature variables respectively ab set basis functions feature variables ab write denote basis function write denote basis function ab explore happens sites try learn local features 
de ne equations write equation write law large number write equation tells takes minimum value minimum value error non zero optimal solution value remains correct global context features considered 
di erence global learning local learning process error term introduced basis functions de ned feature variables observed site discussion far considered boolean features cdm certainly restricted cases 
section brie discusses issue 
non binary features cdm orthonormal representations non binary discrete continuous valued features computed extending cdm domains 
exists choices orthonormal basis functions handle cases 
example discrete fourier functions easily extended ary features feature take di erent values 
exp ary strings length 
words 

set possible ary strings de nes basis 
wavelet representation possible choice orthonormal basis functions dealing continuous valued features 
shall discuss possibilities detail sections 
section identi es cdm algorithm 
boolean decision tree cdm framework cdm framework ers methodology develop distributed data analysis algorithms heterogeneous sites 
main steps approach may summarized follows 
choose orthonormal representation appropriate type data model constructed 
generate approximate orthonormal basis coecients local site 
necessary move appropriately chosen sample datasets site single site generate approximate basis coecients corresponding non linear cross terms 
combine local models transform model user described canonical representation output model 
development speci instances cdm di erent data mining techniques currently going 
coming sections avor popular data mining techniques 
section presents cdm versions rule learning decision tree decision tree learning cdm decision trees popular techniques learning classi ers 
section describe ongoing research cdm approach construct numeric symbolic decision trees heterogeneous data sites 
overview id algorithm id algorithm builds decision tree labeled data set 
sake simplicity consider boolean decision tree depicted 
boolean class labels correspond positive negative instances concept class 
express boolean decision tree function 
function maps positive negative instances zero respectively 
node tree labeled feature downward link node labeled attribute value th feature 
path root node successor node represents subset data satis es di erent feature values labeled path 
data subsets essentially similarity equivalence classes shall call schemata schema singular form 
schema denotes wild card matches value corresponding feature 
example path represents schema members data subset nal node path take feature values respectively 
id algorithm builds tree rst constructing root node scheme 
computes information gain variable assigns variable root maximizes expected information gain 
schema path node root information gained replacing wild card value feature de ned gain entropy jh jhj entropy set possible values attribute jh set members value attribute proportions positive negative instances entropy log log variable assigned root id algorithm computes information gain children continues add new nodes information gain signi cant 
see computation entropy schema need decision tree construction 
computation entropy node requires knowledge distribution class labels data set 
heterogeneous environment comparing information gain requires exchange distribution information di erent sites 
section points naive approach synchronous message exchange may scalable 
naive approach may scale decision trees constructed independent asynchronous fashion di erent heterogeneous data sites may combined generate correct tree centralized construction produce 
alternate approach guarantee correctness synchronous construction sites contribute selecting feature node globally maintained tree 
approach may scalable number sites grows 
node tree id computes information gain choice feature sites need informed regarding particular subset data subsumed particular node 
complete data set distributed nodes particular level level communication cost number data rows number di erent data sites 
tree depth bounded constant communication cost 
table columns rows cost moving complete data sets single site 
distributed decision tree construction naive approach computationally worse centralized approach 
distributed environments large number data sites may major bottle neck 
fourier spectrum decision tree cdm approach distributed decision tree learning new perspective decision tree construction 
notes decision tree de nes function domain consideration 
features symbolic tree de nes function 
de ne table replaces symbolic values feature numeric value transform symbolic function numeric function 
compute fourier spectrum decision tree 
section shall rst consider boolean decision trees shall gradually introduce cdm approach non boolean decision tree 
practical purposes decision trees bounded depth 
turns fourier representation decision tree bounded depth interesting properties quite useful distributed construction 
observations discussed 
decision trees bounded depth normally useful data mining purposes 

fourier representation bounded depth say boolean decision tree polynomial number non zero coecients coecients corresponding partitions involving feature variables zero 

order partition number de ning features magnitude fourier coecients decay exponentially order corresponding partition words low order coecients exponentially signi cant higher order coecients 
dataset fourier coefficients dataset decision tree fourier coefficients tree non uniform fourier transform id algorithm id algorithm coefficient fitting data decision tree fourier analysis 
theorems rigorously proves spectrum decision tree approximated computing small number low order proposed cdm approach exploits observation 
stead moving data single data approach simply computes low order fourier coecients distributed sites collects single site generate decision rules 
main strength approach fundamentally suitability fourier basis representing decision tree bounded depth 
fourier spectrum decision tree sparse approximated low order coecients computation coecients data set slightly tricky 
simply training data set small subset complete domain 
naive computation fourier coecients sample data set may give fourier spectrum tree 
accurate computation spectrum sample data set require appropriate estimation 
describing technique discuss big picture captures course data tree fourier analysis 
section 
data decision tree goal di erent roads cdm approach constructs decision tree data set fourier analysis 
presents schematic diagram di erent possible ways 
note computation fourier coecients require members domain refer section 
learning data set typically 
turns assume class label members domain learning data set fourier spectrum exactly represents data 
called non uniform fourier transformation data 
explained details coming sections 
faithfully represents data words member learning data set correctly predict corresponding class label 
fortunately construction decision tree id requires information 
shall prove exactly construct decision tree data 
link connecting blocks lower left corner upper right corner represent route 
direct link connecting blocks corresponding dataset decision tree represents approach traditional id algorithm takes 
fourier spectrum tree order coecient number features de ning corresponding partition boolean features order essentially number partition 
low order coecients ones orders partitions relatively small table data vectors original complemented function values related 
share outcome member learning data set 
may able convert fact may er viable way construct tree distributed fashion 
explored section 
may able estimate fourier spectrum tree directly learning data 
fourier spectrum tree functionally equivalent tree construction tree representation estimated fourier coecients explored 
link upper right lower right blocks represents possible route 
note fourier spectrum decision tree algorithm id may produce data 
simply tree inductive leap generalizes complete domain assigning class labels information learning data set 
interesting implications discussed 
explores possible approaches distributed construction tree fourier analysis 
route construction data explored 
discussion primarily theoretical interest help understanding subsequent practical approaches 
alternate possibly practical approach direct estimation fourier spectrum tree discussed 
non uniform fourier coecients entropy table left sets fourier coecients obtained normal complemented function values 
right schemata order proportion weighted tness averages computed normal complemented function values 
noted earlier section construction decision tree id algorithm requires computation entropy di erent equivalence classes choose call schema 
section shows schema entropies computed directly 
section considers rst case features boolean 
de ne rigorous fashion 
proportion function number instances divided size data set 
de ne proportion weighted class label fourier transformation follows th basis function fourier transform 
nding fourier coecients values calculate proportion positive instances class label schema follows details binary features 
illustrate equations example 
consider table left shows fourier coecients 
example 
note average values members schema jhj jhj equation get jhj equation represents number members function value positive instance 
know number members order get number negative instances need set non uniform fourier coecients obtained data set complemented values 
rightmost column table right shows number positive negative instance vectors schema order 
check equation calculate number positive negative instances schemata 
schema number positive negative instances obtained denote complemented version functions respectively 
schema write get exactly numbers table right 
easily calculate number positive negative instances schema measuring information gain achieved choosing attribute straightforward 
need give weights dividing number instances schema total number instances schemata consideration 
example weights assigned respectively 
resulting decision tree fourier coecients identical constructed regular id approach shown gure 
section considers general case building decision trees non binary features 
outlook humidity wind sunny rain overcast normal high weak strong non binary feature decision tree 
decision trees non binary features data mining generally deal data sets non binary features 
outlook feature may cardinality number attribute values 
fourier basis functions decision tree building algorithm developed previous section fails non binary feature set 
section extend analysis applying generalized nonuniform fourier transform order build decision tree non binary features 
analysis handle ary features di erent cardinality values 
recall ary fourier basis functions set features cardinality de ned exp 
generalized ary fourier function data vector length de ned exp xm jm denotes cardinality th feature 
similarly rede ne proportion weighted class label ary fourier transform complex conjugate 
rede ned compute average proportion weighted class label follows 
exp xed bits positions cardinality table left shows proportion weighted class label averages order schemata table right shows fourier coecients appeared average computations 
outlook sunny rain overcast positive negative instances split choosing outlook outlook temperature humidity wind sunny hot high weak sunny hot high strong overcast hot high weak rain mild high weak rain cool normal weak rain cool normal strong overcast cool normal strong sunny mild high weak sunny cool normal weak rain mild normal weak sunny mild normal strong overcast mild high strong overcast hot normal weak rain mild high strong table data examples vector representations example schema average obtained exp 
exp 
exp 
illustrate calculate information gain generalized discrete fourier transform consider data set table 
notice attribute values class labels mapped integers ary fourier transform 
outlook cardinality chosen root 
number positive negative instances split choosing outlook shown 
schema representations paths sunny overcast rain links respectively 
proportion weighted class label averages schemata table 
previous section apply equation check number positive negative instances schema 
notice table proportion weighted class label averages rst bit xed schemata fourier coecients generated data table 
schema schema schema get exactly numbers 
see decision tree non binary features constructed directly spectrum data 
data guarantee nice properties polynomial description exponentially decaying magnitude listed section 
communication data guaranteed low overhead 
may need technique estimate spectrum tree 
possibility estimate fourier spectrum tree data directly data 
approach strong advantages 
recall fourier spectrum bounded depth tree favorable properties refer section 
polynomial number non zero coecients approximated small number low order coecients 
distributed computation tree ecient approach 
section presents 
building decision trees direct estimation fourier spectrum fourier representation bounded depth decision tree sparse approximated small number coecients coecients exponentially decay orders corresponding partitions increase 
nd direct way estimate coecients data construction global tree require minimal communication 
section describes approach 
note construction globally accurate tree requires low order coecients spectrum 
need estimate coecients order bounded small constant 
set coecient indices set indices order value equal divide indices groups 
indices computed data site local feature values 
rest indices need features values di erent sites 
write error approximating coecients error term iteratively modeled di erent sets coecients error converges 
training data set jsj cardinality set 
proposed algorithm works manner 
local site initialize set 
set coe cients iterate continues decrease acceptable level approximation error compute jsj set 
update estimate coecients global site collect locally computed coecients small representative sample data sites 
set 
iterate coecients corresponding cross terms iterative algorithm exactly computing local terms 
di erence replaced stage iteration error computed sampled data set 
note calculation implicitly asserts domain members training data set computing fourier representation error 
computation coecients possible considering training data set 
approach estimate fourier spectrum assumes tree bounded depth low order coecients required approximate tree 
algorithm tries approximate function learned low order coecients de ned local features 
assumes function value zero domain member training data set 
decision tree constructed data 
tree takes inductive leap assigning di erent function values member training set training data 
iterative process 
continues approximate contribution higher order coecients partitions inducing di erent possibly non zero function values domain members training data set 
error stops decreasing local partitions help 
approximation error signi cant need compute cross terms 
bounded depth decision tree contains bounded number cross terms estimation cross terms relatively small sample set feasible 
noted earlier estimation cross terms require data row communication sites 
number training examples moved central site upto order coefficients variation classi cation accuracy respect number data points transferred central site computing cross terms 
order show estimation accuracy varies size data sent central site consider experimental result 
set considers dataset boolean features divided vertically placed sites 
site stores training instances complete domain 
classi cation problem designed decision tree bounded depth generated manually 
shows variation classi cation accuracy testing data set similar size training data set respect number data points transferred central site learning cross terms 
experimental result shows small fraction data instances approximate spectrum decision tree accuracy 
portion preliminary 
experiment considered coecients order 
decision tree greedy search fair comparison require incorporation greedy search fourier spectrum space 
hill climbing technique expected reduce number higher order cross terms considered central site 
pruning accurate cross terms learned smaller number examples 
result reduction communication overhead 
extended study technique 
section discusses extension cdm polynomial regression applied large problems 
cdm regression regression decision tree learning popular data modeling technique 
noted earlier section naive application standard regression technique may produce misleading ambiguous results heterogeneous distributed environment 
earlier saw simple cdm regression example toy problem discrete binary valued feature variables 
section address issue details method distributed polynomial regression continuous valued feature domain 
cdm exploits strength representation describe data model sparse concise fashion 
saw fourier representation representing decision trees 
fourier representation type data model 
known technique construct appropriate technique domain independent fashion cdm research explores suitability representations domain speci fashion 
section choose wavelet representation wavelet widely acknowledged sparser representation continuous domain periodic patterns 
wavelets applications signal processing image table sample data sparse representation example 
fourier transform wavelet packet transform table discrete fourier transform vs wavelet packet transform spare representation example compression data analysis 
detailed description applications 
order motivate choice wavelet fourier consider function associated data samples shown table 
apply discrete fourier transform wavelet packet described coming section transform data obtain results table 
wavelet transform seen provide sparser representation feature variables re ecting orthogonal basis feature space 
particular example wavelet transform produces orthogonal representation superior fourier transform 
wavelet basis wavelet packet analysis wavelet basis consists sets functions scaling basis functions wavelet basis functions wavelet functions dilated translated versions scaling functions 
understand relation scaling wavelet functions consider vector space dimensions de ned interval 
contains functions de ned piece wise constant equal sub intervals 
de ned function interval may considered correspond contiguous intervals subspace orthogonal complement assert basis functions scaling functions basis functions wavelet functions note complementary orthogonal spaces orthogonal addition form orthogonal basis form orthogonal basis combined form orthogonal basis simple set scaling functions scaled translated box functions de ned interval xxxxxxx hs gs hhs ghs hgs hs gs 
level partitions 
application quadrature lters wavelet packet decomposition 
wavelet functions corresponding box basis functions haar wavelets function may represented terms basis functions coecients generated convolution set orthogonal quadrature lters haar wavelets wavelet packet transform function calculated recursively applying quadrature lters coecients lower dimension scale space wavelet space represented separate scale space 
way subspace coecients calculated scale space wavelet space representing higher dimension scale wavelet spaces 
shows quadrature lters recursively applied scale wavelet subspaces generating wavelet packet transform 
original function recursive applications result orthogonal subspaces 
top level scale space exists function values coecients box function basis space 
selecting haar wavelets basis functions results coecients orthogonal subspaces representing walsh transform original function 
polynomial regression technique implementing polynomial regression wavelet cdm rst estimate locally regression coecients terms feature subsets single partition 
local coecients determined small subset sample data communicated central site facilitate estimation coecients terms feature subsets represent multiple partitions 
partitioned set real valued features term polynomial function features set indices terms functions features partition partition form terms polynomial sample apply wavelet packet transform samples representing term samples 
estimates local model coecients may generated standard regression techniques directly transforms tk tk sparse making nearly orthogonal basis coecients local terms estimated coecients terms containing cross partition feature variables may determined communicating samples described previously standard system linear equations multiple regression techniques depending sample data characteristics 
demonstrate cdm regression algorithm technique consider estimation local model coecients coecients majority information contained partitioned data set communicated aggregate model 
example consider quadratic polynomial real variables non linear terms observe ect sample set size cdm regression algorithm applied series data sample sets increasing size 
data sample set series contains data samples allow simpli ed implementation wavelet packet transform algorithm 
data sample set series contains sets 
wickerhauser provides guidance implementing wavelet packet transform algorithm handle general case sample size single data sample consisted randomly generated values associated value determined applying polynomial 
random values generated subtractive method 
evaluation results summarized 
plot shows ratio estimated actual local term coecient vertical axis log sample set size horizontal axis 
plots show local term sample set size increases ratio estimated actual coecient value converges 
results demonstrate wavelet packet transform cdm regression algorithm produces accurate estimates model coecients 
technique implementing polynomial regression wavelet cdm 
second technique di ers method estimating local regression coecients communicating sample data subset central site signi cant wavelet coecients feature communicated central site 
coecients terms may estimated performing regression directly set signi cant wavelet coecients representing terms 
technique may provide superior performance relative terms higher accuracy lower communication cost depending characteristics feature set applied 
technique evaluated polynomial 
technique second technique accurate number samples data set increases 
accuracy second technique show inversely depend number terms polynomial feature subsets represent multiple partitions depend wavelet basis selected data characteristics 
obtainable wavelet representation data set relatively sparse wavelet coecient values uniform technique exhibits relatively better performance factor equal 
particular time series data appears exhibit characteristics result better performance realized second technique 
likewise cases wavelet representation relatively spares uniform wavelet coecient values technique performs relatively better 
presents results application cdm regression problem classi cation widely bench marked iris data 
assumption iris features resided separate partition fold cross validation test produced accurate classi cation single wavelet coecient partition build aggregate classi er 
axes ratio estimated actual coefficient value axes log sample size 
ratio estimated actual coecient value terms converges sample size increases 
section describes experimental system called bodhi cdm distributed knowledge discovery 
bodhi collective data mining experimental system application cdm practical problems requires development system employs cdm distributed knowledge discovery 
successful ddm system characteristics communication facilities system able communicate ectively various sites context system 
furthermore system built existing protocols concern underlying transport protocol tcp ip 
communication facilities system able handle transfer raw data extracted knowledge commands command parameters learning algorithms learning algorithms 
far di erent approaches algorithms machine learning incorporate single system constantly developed 
cdm system able incorporate new algorithms methods needed 
flexibility system exible adapted di erent problems maintaining capabilities noted previously signi cant changes core system 
axes ratio estimated actual coefficient value axes log sample size 
ratio estimated actual coecient value terms converges sample size increases 
axes ratio estimated actual coefficient value axes log sample size 
ratio estimated actual coecient value terms converges sample size increases 
axes ratio estimated actual coefficient value axes log sample size 
ratio estimated actual coecient value terms converges sample size increases 
axes ratio estimated actual coefficient value axes log sample size 
ratio estimated actual coecient value terms converges sample size increases 
mobility important portions mining process performed sequential manner opposed parallel manner cdm system capable allowing algorithm start location continue location 
application independent representation data models ective cdm system able convey information regarding partial data models various sites environment 
representation system speci words various sites able communicate knowledge agreed format regardless learning algorithm applied problem 
platform independence system function context heterogeneous mixture platforms operating systems system platform independent possible 
security issue security addressed 
primary areas need addressed authentication encryption access permissions system 
centralized distributed control system able handle centralized distributed control 
implementing centralized control relatively straight forward 
mode useful users system centrally located 
hand distributed control mechanism needed support collaborative distributed user interaction 
bodhi knowledge distributed heterogeneous induction system currently development addresses issues 
additional facets ddm systems emerge believe characteristics listed er starting point developing ddm system 
example bodhi system rst reported point time undergone signi cant design changes 
changes came practical issues arose implementation process study re ection problems addressed 
design principles bodhi system agent distributed knowledge discovery system ers transparent running environment message exchange system capable handling mobile agents 
primary design goal bodhi system create communication system run time environment collective data mining bound speci platform learning algorithm representation knowledge 
considered critical put limitations system prevent variety contexts variety distributed learning applications 
bodhi system built extensible system providing framework communication control distributed agents system bound speci platform knowledge representation machine learning algorithms 
prevent limitation platforms system core system developed java 
order prevent limitations placed system concerning learning algorithms utilized knowledge data format system provides generic extensible framework easily adapted number learning algorithms knowledge data formats 
noted bodhi system operates independently learning algorithms implemented machine learning agents operate control 
bodhi system modularized system designed object oriented principles 
precise division responsibility various tasks various components system 
primary component system facilitator module responsible directing data control ow various distributed sites interfaces 
local site communication module known agent station responsible providing communication local site sites addition providing runtime environment agents 
furthermore agent stations responsible communication security 
agent object bodhi framework extensible java object interface user implemented learning algorithm 
learning algorithm may implemented extended agent java native code local machine 
individual agents learning algorithms intended autonomous provided portion bodhi system 
discussion extensible agent class referred simply agent class simply agent agent class extended perform speci learning task referred learning agent 
user interface forms nal component system allows user system control individual agents schedule events occurring context system necessary 
bodhi system designed extensible hierarchy 
forming base hierarchy individual agents 
agents components system extended speci learning algorithms perform actual machine learning task 
may number agents operating site system agents site system 
agents system autonomous minimal restraints placed implementation 
restrictions criteria discussed general restrictions agents instantiation system able communicate user de ned data knowledge format certain control functions implemented extended learning agent 
agent station forms second layer hierarchy 
distributed site system required single agent station 
agent station site responsible starting stopping agents operating site addition keeping track capabilities agents site 
furthermore agent station responsible providing run time environment agents operating control addition routing incoming outgoing communication agents operating site 
third layer hierarchy consists facilitator 
facilitator purposes ease communication extended agent noted design perspective facilitator higher hierarchy agent extended perform speci machine learning task 
facilitator responsible conjunction agent stations coordination communication security 
furthermore facilitator responsible tracking access permissions individual agents agent types 
communication individual agents accomplished message subsystem built bodhi system 
generic messaging system allows communication individual agents agent stations facilitator user interface 
messaging system generic message class loosely kqml format intended allow new types formats messages added system necessary 
message may carry data command combination thereof 
message structure bodhi system designed generic possible constrain expansions system limit capabilities system unnecessary constraints placed representation knowledge data 
responsibility individual agent aware message types agent may encounter 
furthermore new agent added system knowledge data formats understood new agent registered facilitator 
types security issues arose design process bodhi system security transmissions access control various individual agents 
security transmissions di erent sites considered critical point design system 
particular concerns arose outsiders able insert commands data ow messages system outsiders able intercept data transmitted site 
issues addressed current ongoing implementation rsa encryption scheme 
issue access control individual agents currently addressed development system permissions permissions individual agent permissions types agents domain operating 
bodhi system designed perform certain speci tasks including control related tasks initialization shutdown movement agents data ow related tasks transfer raw data extracted knowledge individual agents user 
minimal set basic functionality provided bodhi system listed initialization shutdown primary tasks system initialize shut system trusted remote sites 
agent control bodhi system provides basic framework passing control sequences agents agent stations system 
required set basic commands incorporated agent types incorporated system speci implementation commands left user system 
course additional commands may added individual agent implementations needed 
agent mobility cases necessary agent act mobile agent 
agents extensions basic agent object bodhi system easily capable transferring agent site agent environment con guration current state learned knowledge 
transmission information bodhi system provides basic functionality agents communicate information knowledge data agents system 
control ow capabilities provided bodhi system speci cs type format information data left user 
wit framework passing data provided message subsystem type content elds messages may de ned user 
user interface bodhi system designed allow distributed mining process controlled user interface resides primary initializing node system 
bodhi system provides framework user interface monitor control agents system 
speci interfaces speci agents provided agent extended perform speci machine learning task user interface likewise extended 
system components bodhi system consists primary component types individual agents autonomous entities perform speci learning tasks agent stations responsible providing run time environment communication agents sites system facilitator responsible coordinating communication various agent stations user interface user system able con gure control system individual messages passed system 
described 
agents agent extensible base class speci learning algorithm incorporated bodhi system 
bodhi system provide speci learning algorithms bound agent class 
basis agent class interface actual learning agent rest bodhi system provided 
noted actual machine learning algorithm implemented java extension agent class extending agent class manner cause call native code host machine agent 
agents bodhi system mobile entities capable transferred site 
agent moves site necessary information agent including current state store acquired knowledge environment con guration moved agent 
methods re ect various states life cycle agent 
methods extended particular type learning agent system 
intended manual bodhi system informative examine basis methods re ect life cycle agents init init method initializes agent 
necessary initializations con guration agent performed method 
start start method initiates actual main learning algorithm agent perform 
agent station agent agent agent agent agent user interface agent station facilitator agent agent agent station agent station primary site remote site remote site remote site systems diagram bodhi system 
life cycle state diagram agent bodhi system 
moving moving method prepare agent moved site 
agent moved con guration information acquired knowledge environment information current state bundled agent 
accomplished agent agent station transmits agent message 
necessary agent moved site call arriving method 
agent required maintain list sites visited 
arriving arriving method method restore agent moved site 
method restore state agent new site cause agent continue learning process left point moving call 
returning returning method similar moving method case agent prepared return site originally came 
method stops execution learning algorithm results learning process 
necessary cleanup performed kill method called 
kill kill method halts execution agent removes agent system nalization processes called 
agent associated message queue 
responsibility individual agent check status queue periodically respond messages queue appropriate 
certain control messages arriving agent site agent station concerning agent may trapped acted agent station see 
responsibility agent station check arriving message type kill message 
arriving message fall category simply placed agent message queue agents responsibility act needed 
individual agent agent type associated certain permissions control sites domains agent may instantiated sort movement allowed 
furthermore agents agent types associated certain le system privileges 
information see section concerning security 
agent stations node system contains single instance agent station 
agent station daemon process responsible providing runtime environment agents passing messages individual agents agent stations facilitator 
addition encryption decryption messages performed agent station 
individual agents permitted direct access encryption decryption routines 
agent station node responsible number speci tasks 
responsibility agent station receive messages passed nodes system take action immediately pass appropriate agent control agent station agent message queue 
responsibility agent station initialize individual agents called existence clean agents 
message received agent station special message listener message passed message handler 
received message instruction create new instance agent message handler passes message agent loader responsible creating new instance agent network local le system 
message passed message queue individual agent responsibility agent act 
prior loading new agent agent loader verify request generation new instance agent permissible possible 
special con guration class associated agent veri cation lists necessary components agent generated necessary classes external les 
furthermore representation knowledge type veri ed agents operating system ensure compatibility instantiated agents 
communication ow bodhi agent station 
facilitator facilitator system special purpose agent 
intended perform machine learning task job facilitator coordinate communication control ow individual agents 
facilitator responsible routing messages agents tracking location individual agents 
facilitator derived agent operates agent station 
agent type facilitator allowed direct access encryption decryption routines 
agent type facilitator permitted move site remain site originated 
primary task facilitator coordinate communication agents 
facilitator able respond appropriately calls similar kqml scheme ask ask tell tell 
functions allow message routed facilitator sent appropriate destination destinations 
facilitator responsible resolution type content knowledge represented messages passed agents 
resolution necessary request passed agent agent station facilitator pass response back agent 
facilitator responsible pre scheduled coordination tasks system various agents agent stations 
scheduling necessary facilitator class extended include functionality 
adaptive functionality system framework functionality provided left user implement speci algorithms 
user interface user interface bodhi system designed components system expandable adaptable types agents user may incorporate system 
base user interface intended nal user interface general basis controlling system 
user interface bodhi system runs machine facilitator communicates facilitator components system 
user interface process move machine started 
messages messages nal component bodhi system 
communication various components including agents user interface facilitator bodhi system accomplished messages 
security subsystem implemented messages encrypted agent station sending machine decrypted receiving machine agent station 
furthermore message signed allow veri cation origin message 
order preserve ability system handle data knowledge control strings needed system message format de ned manner allow carry virtually kind content 
message consists primary sections envelope message type content description message content 
envelope portion message addressing portion message 
allows sender receiver addition node agent station sender receiver operating identi ed 
envelope portion message contains primary entries sender receiver sending agent station receiving agent station 
noted responsibility facilitator route message sender sends message need know actual real location ip address recipient need know system assigned name recipient 
message type determines purpose message message type divided types divided sub parts 
primary types command ask tell 
command type message passes command agent station telling station perform action creating agent individual agent 
ask type request information speci agent may unicast broadcast request information 
tell type agents way sending data unicast broadcast type 
content description message code signifying content message contains allows recipient parse content message 
responsibility facilitator maintain table types messages responsibility speci agent able respond message appropriate manner 
content portion message main body message 
encoding determined content description eld message 
responsibility individual agent able understand section content description message indicates format content section 
security issues types security issues relating bodhi system 
rst message security involves outsiders able impersonate intercept transmissions 
second issue involves user agent privileges terms locations agent may operate data able access moves site 
message security primary concerns relating message security design 
rst concern outsiders able insert command data message system 
second concern outsider able intercept data transmitted node 
noted site bodhi portion system operates single threaded process danger interception messages agent agent station 
description security portion bodhi system design performed 
actual implementation security system underway 
approach message security taken bodhi system involves phases 
initial transmission creates new agent station remote machine encrypted rsa public key encryption scheme 
requires remote machine private key associated initial node public key remote machine 
included argument encrypted initialization command agent station process remote machine private key 
associated public key broadcast agent stations system 
transmissions new agent station signed sender encrypted public key 
ensure false messages inserted system furthermore messages intercepted outside party 
access control second security issue relating sort system involves access control control permissions 
necessary data available agent controlled especially agent may move site site 
furthermore necessary agents allowed send control commands method controlling permissions agents issuing commands 
access control performed associating list permissions individual agent agent type 
facilitator responsible keeping track permissions 
permissions include listing site agent allowed operate sort agent movement permissible les available local le systems speci agent type agent 
global con guration type agent listing parameters 
instance agent instantiated permissions may modi ed 
noted modi cation may include limitation permissions increase available permissions 
section concludes 
heterogeneous data sites common business government defense scienti information processing environments 
eld ddm develop grounded approach deal general situation 
ddm viable alternative current centralized data mining systems 
cdm technology ers possible approach 
material result preliminary investigation framework currently explored extended 
preliminary results demonstrate cdm may er grounded methodology generating accurate global data models distributed fashion 
cdm notes practical data models polynomial description 
result possible nd polynomial description data model appropriate representation 
able build model distributed fashion polynomially bounded amount communication 
see success cdm depends nding appropriate representation designing ecient algorithms compute representation 
seen may possible polynomial regression decision trees 
need investigate approach types data models bayesian hidden markov models neural networks 
extending cdm domain unsupervised learning algorithms hierarchical clustering 
additional ecient construction orthonormal representations bodhi system development multi media ddm various applications ongoing 
authors acknowledge support national science foundation iis american cancer society 
aronis kolluri provost buchanan 
world knowledge discovery multiple distributed databases 
proceedings florida arti cial intelligence research symposium flairs page available 
breiman 
bagging predictors 
machine learning 
breiman 
combining predictors 
combining arti cial neural nets pages 
springer verlag 
bridges goldberg 
nonuniform walsh schema transform 
rawlins editor foundations genetic algorithms pages 
morgan kaufmann san mateo ca 
chan stolfo 
experiments multistrategy learning meta learning 
proceeding second international conference information knowledge management pages 
chan stolfo 
parallel distributed learning meta learning 
working notes aaai 
knowledge discovery databases pages 
aaai 
chan stolfo 
scalable learning non uniform class cost distribution case study credit card fraud detection 
proceeding fourth international conference knowledge discovery data mining page aaai press september 
cheung ng fu fu 
ecient mining association rules distributed databases 
ieee transaction knowledge data engineering 
cho 
real time discovery distributed information sources 
wu ramamohanarao kevin korb editors research development knowledge discovery data mining number lecture notes computer science lecture notes arti cial intelligence pages new york 
springer verlag 
second paci asia conference melbourne australia april 
dhillon modha 
data clustering algorithm distributed memory multiprocessors 
proceedings kdd workshop high performance knowledge discovery 
finin labrou may eld 
kqml agent communication language 
bradshaw editor software agents pages 
mit press 
fisher 
multiple measurements taxonomic problems 

grossman 
preliminary design papyrus system high performance distributed data mining clusters meta clusters super clusters 
advances distributed parallel knowledge discovery page available 
aaai mit press 
guo 
knowledge probing distributed data mining 
advances distributed parallel knowledge discovery page available 
aaai mit press 
hershberger kargupta 
distributed multivariate regression wavelet collective data mining 
technical report eecs school eecs washington state university 
barbara burke hubbard 
world wavelets 
peters wellesley ma 
johnson kargupta 
collective hierarchical clustering distributed heterogeneous data 
published lecture notes computer science volume springer verlag 
kargupta sta ord 
scalable distributed data mining agent architecture 
david heckerman heikki mannila pregibon uthurusamy editors proceedings knowledge discovery data mining pages menlo park ca 
aaai press 
kargupta sta ord 
parallel data mining agent scalable text classi cation 
proceedings conference high performance computing pages 
society computer simulation international 
kargupta johnson riva park hershberger 
scalable data mining distributed heterogeneous data collective learning gene expression genetic algorithms 
technical report eecs school electrical engineering computer science washington state university 
kargupta park 
large scale distributed learning decision trees heterogeneous data 
november 
kargupta park 
fast construction distributed decomposed evolutionary representation 
late breaking papers genetic evolutionary computation conference pages 
aaai press 
kargupta sarkar 
function induction gene expression evolutionary representation construction 
proceedings genetic evolutionary computation conference pages 
aaai press 
donald knuth 
art computer programming 
addison wesley reading ma 
kushilevitz mansour 
learning decision trees fourier spectrum 
proc 
rd annual acm symp 
theory computing pages 
lam 
distributed data mining probabilistic knowledge 
proceedings th international conference distributed computing systems pages washington 
ieee computer society press 
lee stolfo mok 
data mining framework adaptive intrusion detection 
proceedings ieee symposium security privacy page available 
ieee press 
mitchell 
machine learning 
mcgraw hill usa 
frederick mosteller john tukey 
data analysis regression 
addison wesley menlo park ca 

plotting wavelets 
mathematics magazine december 

image compression haar wavelet transform 
science mathematics journal 

distribution comparison site speci regression modeling agriculture 
published international joint conference neural networks www cas american edu ijcnn ijcnn html july 
provost buchanan 
inductive policy pragmatics bias selection 
machine learning 
quinlan 
induction decision trees 
machine learning 
rivest shamir adleman 
method obtaining digital signatures public key cryptosystems 
communications acm 
stolfo jam java agents meta learning distributed databases 
david heckerman heikki mannila pregibon uthurusamy editors proceedings third international conference knowledge discovery data mining pages menlo park ca 
aaai press 
eric tony derose david salesin 
wavelets computer graphics primer part 
ieee computer graphics applications may 
eric tony derose david salesin 
wavelets computer graphics primer part 
ieee computer graphics applications june 

architecture distributed data mining 
advances distributed parallel knowledge discovery page available 
aaai mit press 
ting low 
model combination multiple data base scenario 
maarten van someren widmer editors machine learning ecml number lecture notes computer science lecture notes arti cial intelligence pages new york 
springer verlag 
th european conference machine learning 
wickerhauser 
adapted wavelet analysis theory software 
peters 
yamanishi 
distributed cooperative bayesian learning strategies 
proceedings colt pages new york 
acm 

