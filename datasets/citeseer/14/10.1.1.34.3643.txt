temporal sequence processing recurrent som timo markus kimmo kaski helsinki university technology laboratory computational engineering box fin hut finland keywords recurrent self organizing map temporal sequence processing recurrent self organizing map rsom studied temporal sequence processing 
rsom includes recurrent difference vector unit map allows storing temporal context consecutive input vectors fed map 
rsom modification temporal kohonen map tkm 
shown rsom learns correct mapping temporal sequences simple synthetic data tkm fails learn mapping 
addition case studies rsom applied eeg epileptic activity detection time series prediction local models 
results suggest rsom efficiently temporal sequence processing 
temporal sequence processing tsp research area having applications diverse fields varying weather forecasting time series prediction speech recognition remote sensing 
order construct model process data gathered measuring values certain variables sequentially time 
usually data incomplete includes noise 
goal model building reveal underlying process data 
model estimated statistical methods find regularities nonlinear dependencies exist data 
usually model predicts process accurately considered best model 
computational techniques proposed gain insight processes phenomena include temporal information 
statistical methods linear ar arma non linear mars effectively applications 
neural networks gained lot interest tsp due ability learn effectively nonlinear dependencies large volume possibly noisy data learning algorithm 
architectures multilayer perceptron mlp radial basis function network rbf universal function approximators necessarily imply usability tsp 
traditional way neural networks tsp convert temporal sequence concatenated vector tapped delay line feed resulting vector input network :10.1.1.31.3701
time delay neural network approach know drawbacks serious ones difficulty determine proper length delay line 
number dynamic neural networks models designed tsp capture inherently essential context temporal sequence need external time delay mechanics 
models learning equations described differential difference equations interconnections network units may include set feedback connections networks recurrent nature see 
recurrent neural networks trained supervised learning rules 
quite rare unsupervised neural networks models proposed tsp argued temporal sequence analysis unsupervised neural networks reveal useful information temporal sequences hand analogy unsupervised neural networks reported power cluster analysis dimensionality reduction visualization static input spaces 
tsp applications unsupervised learning utilize effectively available temporal data supervised learning methods input data needed 
needs unsupervised learning methods tsp immense 
temporal kohonen map tkm interesting unsupervised approach tsp derived kohonen self organizing map algorithm 
tkm involvement earlier input vectors unit represented recursive difference equation defines current unit activity function previous activations current input vector 
recurrent self organizing map rsom proposed originally enhancement tkm algorithm 
brief rsom defines difference vector unit map selecting best matching unit adaptation weights map 
difference vector captures magnitude direction error weight vectors allows learning temporal context 
weight update similar som algorithm weight vectors moved recursive linear sum past difference vectors current input vector 
rest organized follows 
rsom algorithm described detail section 
classification temporal sequences clustering eeg patterns time series prediction considered section 

recurrent self organizing map extension selforganizing map recurrent self organizing map rsom allows storing certain information past input vectors 
information stored form difference vectors map units 
mapping formed training topology preservation characteristic som 
self organizing map som vector quantization method topology preservation dimension map matches true dimension input space 
brief topology preservation means input patterns close input space mapped units close som lattice units organized regular dimensional grid 
furthermore units close som close input space 
topology preservation achieved topological neighborhood connects units som neighborhood function training algorithm som unsupervised learning sample input vector input space selected randomly compared weight vector unit map space vm best matching unit input pattern selected metric criterion kx gamma min vm gamma kg kk denote euclidean vector norm 
initially weight vectors set randomly initial positions input space 
learning phase weights map updated input pattern fl ib gamma vm fl fl scalar valued adaptation gain 
neighborhood function ib gives excitation unit best matching unit typical choice ib gaussian function ib exp gamma oe oe controls width function dimensional som index vectors unit best matching unit learning function ib normally approaches delta function oe slowly approaches zero training progresses 
quantization desired map trained ib map organized 
quantization stage gain sufficiently small avoid losing map order small exactly varies case case 
order guarantee convergence algorithm gain fl decrease function time training steps conditions lim fl dt lim fl dt map trained properly gain neighborhood functions properly decreased training mapping formed weight vectors specify centers clusters satisfying vector quantization criterion minf jjx gamma jjg seek minimize sum squared distance input patterns respective best matching units weight vectors furthermore relates point density function weight vectors point density function sampled underlying distribution density function better approximation underlying approximation dimension achieved random quantizer 
chappell taylor proposed modification original som 
modification named temporal kohonen map capable separating different input patterns capable giving context patterns appearing sequences 
describe temporal kohonen map detail 
temporal kohonen map temporal kohonen map tkm differs som outputs 
outputs normal som reset zero presenting input pattern selecting best matching unit typical winner take strategy making unit output values map sensitive input pattern 
tkm sharp outputs replaced leaky integrator outputs activated gradually lose activity 
modeling outputs tkm close behavior natural neurons retain electrical potential membranes decay 
tkm decay modeled difference equation dv gamma gamma kx gamma viewed time constant activation unit step weight vector unit input pattern 
best matching unit unit maximum activity 
equation eq 
general solution gamma gamma kx gamma gamma gamma involvement earlier inputs explicit 
analysis eq 
shows optimal weight vectors vector quantization sense solved explicitly assumed sufficiently large render residual term corresponding initial activity insignificant 
analysis assumed constant goes follows gamma gamma gamma gamma optimal vector quantization sense eq 
derivative eq 
zero minimizes sum eq 

substituting left hand side eq 
yields gamma gamma gamma gamma gamma gamma result shows optimal weight vectors vector quantization sense linear combinations input patterns 
tkm trained normal som training rule attempts minimize normal vector quantization criterion eq 
criterion suggested eq 

consequence appears may possible properly train tkm relatively simple input spaces 
modified tkm rsom problems original tkm convenient solution simply moving leaky integrators unit outputs inputs 
gives rise modified tkm called recurrent self organizing map rsom 
moving leaky integrators outputs inputs yields gamma ff gamma ff gamma temporally leaked difference vector map unit 
ff leaking coefficient analogous tkm leaked difference vector previous meanings 
schematic picture rsom unit shown fig 

large ff corresponds short memory small values ff correspond long memory slow decay activation 
extremes ff rsom behaves normal som ff schematic picture rsom unit acts recurrent filter 
extreme units tend mean input data 
eq 
written familiar form replacing gamma yielding gamma ff gamma ffx describes exponentially weighted linear iir filter impulse response ff gamma ff 
analysis eq 
see 
feedback quantity rsom vector scalar captures direction error exploited weight update training map 
best matching unit step searched min kg vm map trained slightly modified hebbian training rule eq 
difference vector gamma replaced unit moved linear combination sequence input patterns captured repeating mathematical analysis rsom earlier done tkm eqs 
yields ff gamma ff gammak gamma square norm jjy jj ff gammaff gammak gammaw gammaw optimizing vector quantization criterion general form eq 
respect yields condition jjy jj gamma ff gamma ff gammak gamma optimal 
optimal analytically solved gamma ff gammak gamma ff eq 
immediately observes optimal linear combinations 
note result essentially identical result eq 
algorithms essentially 
rsom trained seeks minimize quantization criterion suggested eq 
tkm seeks minimize normal vector quantization criterion eq 

resolution rsom limited linear combinations input patterns different responses operator unit inputs 
sophisticated memory required resort multilayer structures 
case studies different cases evaluate usability rsom temporal sequence processing 
case synthetic data consist random sequences symbols limited alphabet additive noise 
second case rsom cluster feature vectors extracted eeg classify signal normal containing epileptic activity 
third case local models rsom time series prediction 
laser data set publicly available results compared multilayer perceptron mlp linear ar models 
temporal sequence classification synthetic case aims underline differences tkm rsom 
rsom tkm trained onedimensional input patterns additive approximately gaussian noise 
optimal weights ff directly computed eq distribution weights choose optimal ff goal known 
case seek distinguish sequences input patterns 
heuristic criterion weights weights evenly distributed entire input space possible 
heuristic criterion arrived optimal ff accordingly optimal 
different maps tkm rsom units ff trained demonstrate behavior training algorithms analytically solved optimal weights ff 
maps trained iterations unit give maps ample time converge 
map unit weights ff 
optimal weights ffi rsom weights tkm weights theta 
unit weights ff 
optimal weights ffi rsom weights tkm weights theta 
weights computed optimal ff heuristic criterion 
properly trained able distinguish ordered pairs inputs shown map sequence length 
resulting weights optimal weights shown figures 
optimal weights marked small circles ffi weights rsom marked plus signs corresponding weights tkm marked small theta 
case rsom units updated explicitly linear combinations input patterns map seeks learn optimal weights 
tkm situation complicated 
basically tkm mechanisms retain contextual information 
consider example section 
input unit weight weights ff 
optimal weights ffi rsom weights tkm weights theta 
activity moves short period time unit close remains best matching unit replaced unit near activity decayed raising activity consequence neighbors trained brief moment 
motivation tkm learn temporal context past learning distinguish activity came learn activity went 
mechanism tkm preserves contextual information topological neighborhood inherent som algorithms 
consider situation separate regions activity straight line map units 
map trained units converge centroids regions activity remaining left dangling zero probability regions separating regions activity 
dangling units may capable separating sequences 
explicit method capturing past 
distinctive feature trained tkm concentration units near edges input manifold 
brief study properties training rule showed phenomena 
clustering eeg patterns second case related eeg spectral feature clustering epileptic activity detection 
eeg important clinical tool diagnosing monitoring managing neurological disorders related epilepsy 
activity noticed eeg clearly distinguishable transient waveforms wavelets efficiently extract suitable features epilepsy detection 
sampling rate eeg data test hz 
spectral feature extraction time feature extraction window samples collected original eeg sequence follows gamma 
wavelet transform continuous signal wavelet psi gamma gamma psi gamma dt scaling dilation factor wavelet psi 
case daubechies mother wavelets employed discrete wavelet transform window done mallat holes algorithm 
total sixteen energy features determined computing squared sum wavelet coefficients scale 
extracted eeg features clustered soms theta theta theta theta units dimensional lattice 
luttrell method training 
training data contained total dimensional feature vectors patterns correspond epileptic activity 
training unit map labeled plurality rule belong normal epileptic activity soms classifiers evaluate discrimination potentials feature clusters 
note labeling number epileptic activity samples mapped unit multiplied factor equal weighting classes 
table 
shows confusion matrices som classifiers diagonal entries give correctly classified samples 
results compared sizes 
ff parameter set value learning cycle past feature values gamma gamma gamma gamma shown map randomly selected time rsom maps taught luttrell approach 
labeled employed classify training samples results better normal soms number units see table 
clustering results suggest context eeg epileptic activity detection improve analysing classification results rsom may valuable tool purpose 
table som rsom confusion matrices obtained clustering eeg spectral features see text details units som rsom theta theta theta theta time series prediction third case time series prediction rsom local linear models 
laser time series fig 
consists measurements intensity infrared laser chaotic state 
data available anonymous ftp server series samples training rest samples testing 
series scaled 
prediction task step prediction 
laser time series 
learning algorithm rsom time series prediction implemented follows 
map episode consecutive input vectors starting random point data 
number vectors belonging episode dependent leaking coefficient ff units 
episode impulse response recurrent filter see fig 
initial value 
best matching unit selected episode norm difference vector 
updating ftp ftp cs colorado edu pub timeseries santafe containing files dat samples cont continuation dat samples vector neighbors carried eq 

updating difference vectors set zero new random starting point series selected 
scenario repeated mapping formed 
training set divided local data sets best matching unit map associated local models estimated data sets 
prediction best matching unit rsom searched input vector 
local model associated best matching unit selected prediction task time 
time series data windowing training data test data local data set building rsom vectors local models local model prediction select local model rsom training local model estimation construction local models 

shows procedure building rsom models evaluating prediction abilities testing data 
time series divided training testing data 
input vectors rsom formed windowing time series 
free parameters training rsom include input vector length time step consecutive input vectors number units leaking coefficient ff units map giving rise model rsom ff 
parameters varied ff corresponding episode lengths 

local linear regression models estimated squares algorithm matlab statistics toolbox local data sets generated rsom 
model selection purposes fold crossvalidation 
best model cross validation trained training data 
model predict test data set model 
crossvalidation scheme mlp ar models 
mlp network trained levenberg marquardt learning algorithm implemented matlab neural networks toolbox 
mlp network hidden layer inputs hidden units 
variation parameters chosen rsom models varied 
ar models inputs estimated matlab squares algorithm 
order ar model varied 
results ar model serve example accuracy global linear model current tasks 
sum squared errors gained step prediction task shown table 
laser series highly nonlinear errors gained ar model considerably higher models 
series stationary noiseless explains accuracy mlp model predictions 
case rsom gives results better ar model worse mlp model 
table step prediction errors laser time series 
cv error test error rsom mlp ar recurrent self organizing map architecture learning algorithm 
studied cases show potentials rsom temporal sequence processing 
results synthetic data show rsom learns correct mapping agrees vector quantization criterion 
eeg case feature vectors clustered som rsom 
temporal context captured rsom provided better results simple rule classification 
results prediction case best possible rsom search space free parameters model quite small 
important property rsom visualization ability 
rsom visualization tool built property som construct topological maps data 
instance case time series prediction dimensional map visualize state changes process location best matching unit map function time 
unsupervised learning temporal context attractive property rsom 
allows building models large amount data little priori knowledge classification need cases 
property allows rsom explorative tool find statistical temporal dependencies process consideration 
rsom feedback structure units 
possible allow units rsom different recurrent structures 
kind architecture yield topological maps different temporal contexts data 
intention study extensions rsom near 
acknowledgments wish dr tech 
tampere university technology finland generously providing scored eeg recordings project 
study funded academy finland technology development centre 
chappell taylor 
temporal kohonen map 
neural networks 
cottrell 
theoretical aspects som algorithm 
proc 
workshop selforganizing maps pages 
helsinki university technology 
gershenfeld weigend 
time series learning understanding 
weigend gershenfeld editors time series prediction forecasting understanding past pages 
addison wesley 
holmstrom laaksonen oja 
neural statistical classifiers taxonomy case studies 
ieee trans 
neural networks 
kangas 
analysis pattern sequences self organizing maps 
phd thesis helsinki university technology may 
kohonen 
self organization associative memory 
springer verlag berlin heidelberg 
kohonen 
self organizing maps 
springer verlag 
kaski 
time series prediction recurrent som local linear models 
int 
knowledge intelligent engineering systems press 
available research reports helsinki university technology lab 
computational engineering 
luttrell 
image compression multilayer neural network 
pattern recognition letters 
mallat 
theory multiresolution signal decomposition wavelet representation 
ieee trans 
pattern anal 
mach 
intell 
pami 
mozer :10.1.1.31.3701
neural net architectures temporal sequence processing 
weigend gershenfeld editors time series prediction forecasting understanding past pages 
addison wesley 
proakis 
digital signal processing principles algorithms applications 
macmillan publishing 
ritter schulten 
convergence properties kohonen topology preserving maps fluctuations stability dimension selection 
biological cybernetics 
tsoi back 
locally recurrent globally feedforward networks critical review architectures 
ieee transactions neural networks 
del mill 
epileptic activity detection eeg neural networks 
proc 
int 
conf 
engineering applications neural networks eann pages 
royal institute technology stockholm 
del ruiz mill 
context learning selforganizing map 
proc 
workshop self organizing maps pages 
helsinki university technology 
del mill 
recurrent self organizing map temporal sequence processing 
proc 
th int 
conf 
artificial neural networks icann pages 
springerverlag 
weigend gershenfeld editors 
time series prediction forecasting understanding past 
addisonwesley 
