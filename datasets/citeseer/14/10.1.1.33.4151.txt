locally weighted projection regression algorithm incremental real time learning high dimensional space vijayakumar usc edu stefan schaal usc edu dept computer science neuroscience kawato dynamic brain project neuroscience bldg university southern california los angeles ca usa locally weighted projection regression new algorithm achieves nonlinear function approximation high dimensional spaces redundant irrelevant input dimensions 
core uses locally linear models spanned small number univariate regressions selected directions input space 
evaluates different methods projection regression derives nonlinear function approximator 
nonparametric local learning system learns rapidly second order learning methods incremental training ii uses statistically sound stochastic cross validation learn iii adjusts weighting kernels local information iv computational complexity linear number inputs deal large number possibly redundant inputs shown evaluations dimensional data sets 
knowledge truly incremental spatially localized learning method combine properties 

nonlinear function approximation high dimensional input data remains nontrivial problem 
ideal algorithm tasks needs eliminate redundancy input data detect irrelevant input dimensions keep computational complexity low course achieve accurate function approximation generalization 
route accomplish goals sought techniques projection regression 
projection regression pr copes high dimensional inputs decomposing multivariate regressions superposition single variate regressions particular projections input space 
major difficulty pr lies select efficient projections achieve best fitting result possible dimensional regressions 
previous focused finding global projections fitting nonlinear dimensional functions 
best known algorithms projection pursuit regression friedman stutzle generalization form generalized additive models hastie tibshirani 
sigmoidal neural networks equally conceived method projection regression particular new projections added sequentially cascade correlation fahlman lebiere 
suggest alternative method projection regression focusing finding efficient local projections 
local projections accomplish local function approximation neighborhood query point 
methods allow fit locally simple functions low order polynomials projection greatly simplifies function approximation problem 
local projection regression borrow statistical properties established methods locally weighted learning nonparametric regression hastie loader atkeson moore schaal 
counterintuitive curse dimensionality scott local regression methods successfully high dimensional spaces shown vijayakumar schaal 
techniques principal component regression schaal vijayakumar atkeson observation globally high dimensional movement data usually lie locally low dimensional distributions exploited 
principal component regression address efficient selection local projections suited detect irrelevant input dimensions 
explore methods remedy shortcoming 
introduce novel algorithm covariance projection regression generalizes principal component regression family algorithms capable discovering efficient projections locally weighted linear regression compare partial squares regression successful global linear projection regression methods 
empirical evaluations highlight table 
pseudocode implementation pls pcr cpr projection regression pls pcr cpr pseudocode 
initialize res res 
res diagonal weight matrix 
pls res pcr cpr eigenvector max 
fi res res 
res res gamma fi 
res res gamma res 
pros cons different methods 
embed projection regression algorithms incremental nonlinear function approximation vijayakumar schaal 
evaluations resulting incremental learning system demonstrates high accuracy function fitting high dimensional spaces robustness irrelevant inputs low computational complexity 

linear projection regression dimensionality reduction section outline pr algorithms fit linear functions individual projections 
spatially localizing algorithms serve core nonlinear function approximation techniques 
assume data generated standard linear regression model fi ffl vector input variables scalar mean zero noise contaminated output 
loss generality inputs output assumed mean zero 
notational convenience input vectors summarized rows matrix corresponding outputs elements vector number training data dimensionality input data 
pr techniques considered project input data orthogonal directions carry univariate linear regressions name projection regression 
linear model data known straightforward determine optimal projection direction vector regression coefficients fi gradient direction single univariate regression suffice obtain optimal regression result 
partial squares partial squares pls wold frank friedman technique extensively chemometrics recursively computes orthogonal projections input data performs single variable regressions projections residuals previous iteration step 
table illustrates pls pseudocode implementation 
noted pls matrix step algorithm needs identity matrix 
key ingredient pls direction maximal correlation residual error input data projection direction regression step 
additionally pls regresses inputs previous step projected inputs order ensure orthogonality projections step 
additional regression avoided replacing step similar techniques principal component analysis sanger 
regression step leads better performance algorithm 
effect due fact pls chooses effective projections input data spherical distribution projection pls find direction gradient achieve optimal regression results 
regression step modifies input data res resulting data vectors coefficients minimal magnitude push distribution res spherical 
principal component regression computationally efficient technique dimensionality reduction linear regression principal component regression pcr vijayakumar schaal 
pcr projects input data principal components performs univariate regressions directions 
principal components correspond largest eigenvalues input covariance matrix 
algorithm pcr identical pls identity matrix 
step table different difference essential 
pcr chooses projection solely input distribution 
interpreted method maximizes confidence univariate regressions prone choose quite inefficient projections 
covariant projection regression section introduce new algorithm flavour pcr pls 
covariant projection regression cpr transforms input data step table diagonal weight matrix goal distribution direction gradient input output relation data 
subsequently major principal component deformed distribution chosen direction univariate regression step 
contrast pcr projection reflects influence input distribution regression outputs 
pls input distribution spherical cpr obtain optimal regression result single univariate fit irrespective actual input dimensionality 
cpr family algorithms depending weights chosen 
consider options weighting scheme cpr cpr ii oe ik see fig 
weighting scheme cpr cpr ii ik ik see fig 
cpr spheres input data weights slope data point taken power increasing impact input output relationships 
cpr variant bit idiosyncratic best average performance practice cpr maps input data unit hyper sphere stretches distribution direction maximal slope regression direction fig method fairly insensitive noise input data 
fig shows effect transforming gaussian input distribution cpr weighting schemes 
additionally compares regression gradient projection direction extracted cpr 
seen gaussian distributions cpr finds optimal projection direction single projection 
monte carlo evaluations performance comparison order evaluate candidate methods linear data sets consisting training points test points input dimension output generated random 
outputs calculated random linear coefficients gaussian noise added 
input data augmented additional constant dimensions rotated stretched random variances dimensions 
test cases input dimensions cpr projection vs regression direction input distribution regression direction transformed data cpr projection direction cpr projection vs regression direction input distribution regression direction transformed data cpr projection direction 
cpr projection different weighting schemes random noise added explore effect irrelevant inputs regression performance 
empirically determined choice cpr 
simulations considered permutations 
low noise high noise output data 

irrelevant non constant input dimensions 
algorithm run times random data sets combinations test conditions 
results noise parametrised coefficient determination 
add noise scaled output variance oe noise oe gamma best normalized mean squared error nmse achievable learning system noise level gamma nmse regression results projections proj nmse regression results projection proj pcr pls cpr cpr nmse regression results projections proj low noise high noise low noise irrelevant dim 
high noise irrelevant dim 

simulation results pcr pls cpr cpr 
show results projection dimensions 
additional conditions permutations low high noise ii irrelevant non constant inputs 
compiled number projection dimensions employed methods varied 
fig show summary results 
seen average pls cpr methods outperform pcr methods large margin case irrelevant inputs included 
attributed fact pcr projections solely rely input distributions 
cases irrelevant inputs high variance pcr choose inappropriate projection directions 
low noise cases cpr performs marginally better pls especially projections 
high noise cases pls slightly better 
cpr candidates cpr slight advantage cpr low noise cases advantage flipped larger noise 
summarizing said cpr pls perform 
contrast pcr accomplish excellent regression results relatively projections choice projections just try span input distribution gradient data 

locally weighted projection regression going linear regression nonlinear regression accomplished localizing linear regressions vijayakumar schaal atkeson moore schaal 
key concept approximate nonlinear functions means piecewise linear models 
course addition learning local linear regression determine locality particular linear model data 
lwpr network section briefly outline schematic layout lwpr learning mechanism 
fig 
shows associated local units inputs feed 
weighting kernel determining locality defined computes weight data point distance center kernel local unit 
gaussian kernel exp gamma gamma gamma corresponds distance metric determines size shape region validity linear model 
assume local linear models combining prediction 
linear model calculates prediction total output network weighted mean linear models shown fig 

parameters need learned includes dimensionality reducing transformation projection directions local regression parameter fi distance metric local module 
learning projection directions local regression previous schaal atkeson computed outputs linear model traditional recursive squares regression input variables 
learning system required number input dimensions computations infeasible dimensional input spaces 
pls cpr framework able reduce computational burden local linear model applying sequence dimensional regressions selected projections input space note output pls train learning module input reg weighted average receptive field centered inputs linear unit correlation computation module input 
information processing unit lwpr drop index necessary distinguish explicitly different linear models shown table 
important ingredient pls choose projections correlation input data output data 
locally weighted projection regression lwpr algorithm shown table uses incremental locally weighted version pls determine linear model parameters 
table forgetting factor determines older data regression parameters forgotten recursive system identification techniques ljung 
variables ss sr sz memory terms enable univariate regression step recursive squares fashion fast newton method 
step regresses projection current projected data current input data step guarantees projection input data univariate regression result orthogonal entire input space spanned projections regression results identical traditional linear regression 
emphasize important properties local projection scheme 
input variables statistically independent pls find optimal projection direction single iteration optimal projection direction corresponds gradient assumed locally linear function approximated 
second choosing projection direction correlating input output data step automatically excludes irrelevant input dimensions inputs contribute output 
third dan incremental pls pseudocode training point 
update means input output wx fi fi wy fi update local model 
initialize res gamma fi 
wz res ss ss sr sr res sz sz fi sr ss sz ss gamma sp res res gamma sfi mse mse res predicting novel data initialize fi gamma 

fi 
gamma sp table 
pls pseudocode ger numerical problems pls due redundant input dimensions univariate regressions singular 
learning locality far described process finding projection directions local linear regression local area 
validity local model size shape receptive field determined distance metric possible optimize distance metric individually receptive field incremental gradient descent stochastic leave oneout cross validation criterion 
update rule writ lwpr outline ffl initialize lwpr receptive field rf ffl new training sample rf calculate activation eq update incremental pls distance metric update linear model activated gen create new rf def ffl table 
lwpr outline upper triangular gamma ff ffi ffi cost function minimized res gamma fl ij update rules embedded incremental learning system automatically allocates new locally linear models needed 
outline algorithm shown table 
pseudo code algorithm gen threshold determines create new receptive field def initial usually diagonal distance metric eq 
initial number projections set 
algorithm simple mechanism determining increased recursively keeping track mean squared error mse function number projections included local model step incremental pls 
mse projection decrease certain percentage previous mse mse mse oe oe algorithm adding new projections local model 
diagonal distance metric assumption remains small computational complexity update parameters lwpr linear number input dimensions 
empirical evaluations implemented lwpr similar development vijayakumar schaal 
local model projection regressions performed locally weighted pls distance metric learned stochastic incremental cross validation schaal atkeson learning methods employed second order learning techniques 
test ran lwpr noisy training data drawn dimensional function gamma exp gamma exp gamma shown fig 
kind function spatial mixture strong non linearities significant linear regions excellent test learning generalization capability 
models low complexity find hard capture non linearities easy overfit complex models especially linear regions 
second test added constant dimensions inputs rotated new input space random dimensional rotation matrix 
third test added input dimensions inputs second test having gaussian noise obtaining dimensional input space 
learning results data sets illustrated fig 
cases lwpr reduced normalized mean squared error thick lines noiseless test set rapidly epochs training nmse converged excellent function approximation result nmse data presentations 
fig illustrates reconstruction original function dimensional test perfect approximation 
rising thin lines fig show number local models lwpr allocated learning 
thin lines bottom graph indicate average number projections local models allocated average remained initialization value projections appropriate originally dimensional data set 
previous schaal atkeson quantitatively compared performance rfwr predecessor lwpr baseline techniques sigmoidal neural networks advanced techniques mixture experts systems jordan jacobs jacobs jordan jacobs cascade correlation algorithms fahlman lebiere 
results show rfwr competitive outperforms techniques especially robust non static input distributions interference learning 
note stripping lwpr algorithm dimensionality reduction preprocessing essentially gives rfwr algorithm 
nmse test set receptive fields average projections training data points cross cross cross 
target learned nonlinear cross function learning curves data second evaluation approximated inverse dynamics model degree freedom anthropomorphic robot arm see fig data set consisting data points collected hz actual robot performing various rhythmic discrete movement tasks corresponds minutes data collection 
inverse dynamics model robot strongly nonlinear due vast amount superpositions sine cosine functions robot dynamics 
data consisted input dimensions joint positions velocities accelerations 
goal learning approximate appropriate torque command robot motor response input vector 
increase difficulty learning added irrelevant dimensions inputs gaussian noise 
data points excluded training data test set 
fig shows learning results comparison state art techniques field parameter estimation rigid body dynamic models levenberg marquardt sigmoidal neural networks 
parameter estimation technique uses apriori knowledge analytical form robot dynamics equations equations linear unknown inertial kinematic parameters robot 
linear regression techniques complex analytical data preprocessing obtain parameters resulting complete analytical model robot inverse dynamics 
lwpr outperformed global parameter estimation technique 
training presentations lwpr converged excellent result nmse 
employed average projection dimensions local model input dimensionality 
learning number local models increased factor initial models models 
increase due adjustment distance metric eq initialized form large kernel 
large kernel smoothes data lwpr reduced kernel size response kernels needed allocated 
comparison lm backprop method computationally intensive achieved nmse statistically similar 
evident fig took longer converge optimal value compared lwpr 
key issue compared algorithms incremental online 
able find incremental online algorithm literature scales input dimensionality redundancy handled tasks 

discussion discussed methods linear projection regression spatially localized nonlinear function approximation high dimensional input data nmse test set receptive fields training data points lwpr lm back prop parameter estimation average lwpr projection 
sketch arm learning curves dimensional robot dynamics learning redundant irrelevant components 
derived family linear projection regression methods bridged gap principal component regression commonly algorithm inferior performance partial squares regression known algorithm superior performance 
algorithms core nonparametric function approximation spatially localized weighting kernels 
example demonstrated nonlinear function approximator derived family leads excellent function approximation results dimensional data sets 
showing fast robust learning performance due second order learning methods stochastic cross validation new algorithm excels low computational complexity updating projection direction linear computational cost number inputs algorithm accomplishes approximation results projections irrespective number input dimensions computational complexity remains linear inputs 
knowledge spatially localized incremental learning system efficiently high dimensional spaces 
atkeson moore schaal locally weighted learning 
artificial intelligence review 
fahlman lebiere cascade correlation learning architecture 
advances neural information processing systems 
frank friedman statistical view regression tools 
technometrics 
friedman stutzle projection pursuit regression 
journal american stat 
assoc 
hastie loader local regression automatic kernel carpentry 
statistical science 
hastie tibshirani generalized additive models chapman hall 
jacobs jordan nowlan hinton adaptive mixture local experts neural computation 
jordan jacobs mixture experts em algorithm 
neural computation 
principal component regression exploratory statistical research 
journal amer 
stat 
assoc 
sanger optimal unsupervised learning single layer liner feedforward neural network neural networks 
scott multivariate density estimation 
schaal atkeson receptive field weighted regression technical report tr atr human information processing labs kyoto japan 
schaal atkeson constructive incremental learning local information 
neural computation 
schaal vijayakumar atkeson local 
advances neural information processing systems 
vijayakumar schaal local adaptive subspace regression 
neural processing letters 
wold soft modeling latent variables nonlinear iterative partial squares approach 
perspectives probability statistics 
ljung theory practice recursive identification cambridge mit press 
