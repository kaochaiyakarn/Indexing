applying mdl learning best model granularity gao chinese academy sciences ming li university california santa barbara paul vit anyi cwi university amsterdam minimum description length mdl principle provably ideal method inference kolmogorov complexity 
test theory behaves practice general problem model selection learning best model granularity 
performance model depends critically granularity example choice precision parameters 
high precision generally involves modeling accidental noise low precision may lead confusion models distinguished 
precision determined ad hoc 
mdl best model compresses part code data set embodies occam razor 
quite different experimental settings theoretical value determined mdl coincides best value experimentally 
experiment task recognize isolated handwritten characters subject handwriting irrespective size orientation 
new modification elastic matching multiple prototypes character optimal prediction rate predicted learned parameter length sampling interval considered mdl shown coincide best value experimentally 
second experiment task model robot arm degrees freedom layer feed forward neural network need determine number nodes hidden parts ijcai esann 
chinese academy sciences institute botany beijing china 
supported part onr aro daal harvard university nserc operating ogp york university nserc operating ogp university waterloo 
address computer science department university california santa barbara ca usa leave university waterloo waterloo canada 
email mli cs ucsb edu partially supported european union th framework neurocolt ii working group ep th framework project ist th framework network excellence ist working group european science foundation nserc international scientific exchange award ise 
address centrum voor wiskunde en informatica kruislaan sj amsterdam netherlands 
email cwi nl layer giving best modeling performance 
optimal model best unseen examples predicted number nodes hidden layer considered mdl coincide best value experimentally 
keywords phrases minimum description length principle mdl kolmogorov complexity universal prior bayes rule occam razor learning best model granularity line handwritten character recognition improved elastic matching learning optimal feature extraction interval modeling robot arm feedforward neural network learning optimal hidden layer commonly accepted learning involves compression experimental data compact theory hypothesis model phenomenon investigation 
authors analysed theory approaches related shortest effective description length kolmogorov complexity 
question arises theoretical insights directly applied real world problems 
selecting models basis compression properties ignores meaning models 
aim optimizing model parameter direct semantics precision represent parameters high precision causes accidental noise modeled low precision may cause models distinct confused 
quite different experimental settings theoretically predicted values shown coincide best values experimentally 
general performance model data sample depends critically may call degree discretization granularity model choice precision parameters number nodes hidden layer neural network 
granularity determined ad hoc 
give theoretical method determine optimal granularity application minimum description length mdl principle supervised learning 
cost describing set data respect particular model sum lengths model description description data model error 
mdl best model minimum cost model explains data concise way 
authors carried experiment line learning recognize isolated characters written subject handwriting irrespective size orientation 
novel features multiple prototypes character mdl principle choose optimal feature extraction interval 
satisfactory case general learning theory ado applied obtain best sampling rate 
believe method applicable wide range problems 
obtain evidence assertion third author te brake kok applied method modeling robot arm 
genesis rooted traditional approaches artificial intelligence ai new exciting general learning theories developed computational complexity theory statistics descriptional kolmogorov complexity 
new theories received great attention theoretical computer science statistics 
hand design real learning systems dominated ad hoc trial error methods 
applications theoretical results real world learning system design scarce far 
exception elegant quinlan rivest 
companion develop theory mathematical validation mdl principle ultimate data compression kolmogorov complexity 
purpose trying bring theory practice testing theory simple real applications 
give brief accessible exposition theoretical background mdl mathematical proof works 
principle applied distinct practical issues line character recognition robot modeling 
cases issue supervised learning best model granularity classification extrapolation unseen examples 
systems experiments quite simple intended just demonstrations theoretical approach works practice 
applications topical areas results give evidence theoretical approach extended demanding settings 
contents theoretical point view explain general modeling principle mdl relation bayesianism 
show mdl theory provably ideal method inference kolmogorov complexity demonstrate valid assumption set data typical precise rigorous sense targeted model 
apply theory experimental tasks concern supervised learning best model granularity quite dissimilar task want obtain best model sampling rate design line hand written character learning system second task goal determine best number nodes hidden layer layer feedforward neural network modeling robot arm degrees freedom 
turns theoretically predicted optimal precision coincides best experimentally determined precision 
conclude discussion concerning equivalence code length methods probability methods mdl bayesianism 
learning line handwritten characters important aspects ai research machine cognition various aspects natural human languages 
enormous effort invested problem recognizing isolated handwritten characters character strings 
recognizing isolated hand written characters applications example signature recognition chinese character input 
character learning experiment reported pilot project ultimately aims providing practicable method learn chinese characters 
problem knows practical difficulties quantitatively qualitatively 
independent chinese characters 
current keyboard input method natural casual users 
methods require user memorize separate code characters 
methods require user know ping ying phonological representation mandarin chinese latin characters 
translation computer representation characters easy homonyms 
similarly sound recognition techniques help commonly chinese character commonly 
non professional casual users hand written input mechanically scanned processed quite reasonable way entering data computer 
variety approaches algorithms proposed achieve high recognition rate 
recognition process usually divided steps 
feature extraction sample characters 
classification unknown characters 
uses deterministic statistical inference sample data various known mathematical statistical approaches 
contribution level 
leaves technical problem feature extraction purpose capture essence raw data 
state art art science 
existing methods explain 
feature extraction common significant features extracted center gravity moments distribution points character loci planar curve transformation coordinates slopes curvatures certain points character curve 
obvious difficulty recognition task variability involved handwritten characters 
shape characters depend writing style varies person person person trying write consistently writing style changes time 
way deal problem idea elastic matching 
roughly speaking elastic matching method takes coordinates slopes certain points approximately equally spaced curve character drawing feature establish character prototypes 
classify unknown character drawing machine compares drawing prototypes knowledge base distance function entered character classified character represented closest prototype 
unknown character compared prototype comparison features strictly corresponding points prototype points adjacent corresponding point prototype 
method feature extraction new modification standard elastic matching method 
classification implemented algorithm character recognition embodies model family models character recognition 
problem extant research lack common basis evaluation comparison various techniques 
especially true line character recognition due lack common standard limited raw data source 
model family models induced particular method feature extraction usually specified set parameters 
varying parameters gives class models similar characteristics 
consider mentioned elastic matching 
uses certain points character curve features 
interval size extract points curve parameter 
determine value parameter gives optimal recognition 
practically speaking set interval size different values experiment sample set data see value gives best performance 
experiment particular set data know interval size value gives similar optimal performance possible observations data source 
theory needed guide parameter selection order obtain best model class models 
model selection suppose models 
hypothesis gives best recognition rate 
problem selecting best model consists finding hypothesis 
minimum description length principle referred mdl purpose 
mdl finds root known bayesian inference known kolmogorov complexity 
give classic bayes rule 
bayes rule specific hypothesis preferred probability hypothesis takes maximum value set data prior probability distribution set hypotheses 
happens hypothesis product conditional probability data hypothesis prior probability hypothesis maximal 
take negative logarithm bayes formula maximal probability achieved hypothesis sum terms minimized description length error data hypothesis description length model hypothesis 
finding maximum value conditional probability set hypotheses data minimizing combined complexity description length error model set candidate models 
quantify idea description lengths expressed terms coding length model set prototypes coding length error combined length data failed described model 
trade simplicity complexity quantities follows 

model simple sense having short encoding may fail capture essence mechanism generating data resulting increased error coding lengths 

model complicated sense having long code length consists table data may contain lot data sensitive minor irregularities give accurate predictions data 
mdl principle states set models minimum combined description lengths model error set data best approximation mechanism data predict data best accuracy 
objective implement small system learns recognize handwritten elastic matching statistical inference 
mdl guide model selection specifically selection interval feature extraction 
result tested experimentally validate application theory 
modeling robot arm consider problem modeling robot arm consisting joints stiff limbs connected joint limb joint limb 
entire arm moves fixed dimensional plane 
joint fixed origin 
position arm determined lengths limbs angle rotation joint limb angle rotation second joint second limb respect limb 
mobile arm degrees freedom angles 
problem modeled bayesian framework obtain backpropagation network model 
mdl obtain best number nodes hidden layer layer feedforward network model 
method essentially character recognition experiment 
just validated test set unseen data 
theoretic preliminaries explain idea bayesian reasoning give ideal mdl noncomputable provably approach learning 
dilute approach obtain feasible modification form real mdl 
viewpoint relation bayesianism mdl see 
sections apply mdl learning best model granularity 
bayesianism bayesianism induction principle derivation allows estimate relative likelihood different possible hypotheses hard impossible commonly pearson neyman testing 
tests accept reject zero hypothesis confidence 
reject zero hypothesis mean accept alternative hypothesis 
data test alternative 
alternative hypothesis note hypotheses different zero hypothesis taken form alternative hypothesis 
fact type testing establish relative likelihood competing hypotheses 
definition consider discrete sample space omega gamma countable set events subsets omega gamma fh called hypotheses space hypotheses exhaustive true 
definition conditional probability ajb easy derive bayes formula rewrite different ways jd djh hypotheses mutually exclusive djh despite fact bayes rule just rewriting definition conditional probability interpretation applications profound caused bitter controversy past centuries 
equation represent possible alternative hypotheses concerning phenomenon wish discover 
term represents empirically known data concerning phenomenon 
term probability data may considered normalizing factor jd 
term called priori probability initial probability hypothesis probability true see data 
term jd called posteriori inferred probability model selection want select hypothesis model maximum posteriori probability map 
interesting term prior probability 
context machine learning considered learner initial degree belief hypothesis essence bayes rule mapping priori probability posteriori probability jd determined data general problem limit inferred hypothesis concentrate true hypothesis inferred probability gives information possible possible hypotheses limited number data 
fact continuous debate bayesian non bayesian opinions centered prior probability 
controversy caused fact bayesian theory say initially derive prior probabilities hypotheses 
bayes rule tells updated 
real world problems prior may unknown uncomputable conceivably non existent 
prior probability word written english 
different sources different social backgrounds living different ages 
problem solved find single probability distribution prior distribution different case approximately result real distribution 
surprisingly turns possible mild restrictions 
kolmogorov complexity divert main thrust recapitulate basic formal definitions notations give informal overview 
universal description length descriptions finite binary strings 
want able determine description ends require set descriptions prefix code description proper initial segment proper prefix description 
intuitively prefix kolmogorov complexity finite object conditional length xjy bits shortest effective description input 
fixed set shortest effective descriptions required prefix code 
define ffl means zero input 
shortest effective descriptions effective sense compute described objects 
unfortunately general method compute length shortest description prefix kolmogorov complexity object described 
obviously impedes actual 
needs consider computable approximations shortest descriptions example restricting allowable approximation time 
course followed sense practical incarnations mdl 
want predict determine expected posteriori probability integrating hypotheses choosing hypothesis maximises posterior 
uses simply shannon fano code assigns prefix code length gamma log irrespective regularities gammal code word length zero equals code word length truly irregular shannon fano code gives expected code word length close entropy distinguish regular elements probability ensemble random ones compressing individual regular objects irregular ones 
prefix code consisting shortest programs prefix kolmogorov complexities code word length set computable distribution expected code word length prefix kolmogorov complexity close entropy individual element compressed possible effective code 
universal probability distribution just kolmogorov complexity measures shortest effective description length object algorithmic universal probability xjy conditional measures greatest effective probability conditional turns set xjy gammak xjy appendix precise definitions notion greatest effective probability reader referred appendix wel 
expresses property probability individual entropy measures average expectation entire ensemble elements tell happens individual elements 
algorithmic universal probability universal prior bayes rule analyze ideal mdl 
individual randomness common meaning random object outcome random source 
outcomes expected properties particular outcomes may may possess expected properties 
contrast notion randomness individual objects 
elusive notion long history goes back initial attempts von mises formulate principles application calculus probabilities real world phenomena 
classical probability theory express notion randomness individual objects 
half century unsuccessful attempts theory kolmogorov complexity martin lof tests randomness succeeded formally expressing novel notion individual randomness correct manner see 
individually random object possesses individually effectively testable properties expected outcomes random source concerned 
typical general position satisfy effective tests randomness known unknown alike 
major result states object individually random respect conditional probability distribution iff log xjy xjy close zero 
particular means typical general position respect conditional distribution iff real probability xjy close algorithmic universal probability xjy gammak xjy prefix kolmogorov complexity xjy close shannon fano code aside fixed conditional entropy gamma xjy log xjy 
length element set probability distribution 
example hypothesis deal fair coin data sample outcomes heads row isn typical truly random individual sequence respect notion precise formal quantitative meaning typical probability atypical sequences small goes zero data sample grows unboundedly 
prediction model selection shown solomonoff continuous variant astonishing performance predicting sequences probability element computable initial segment 
come punch line bayes rule algorithmic universal prior distribution suggested solomonoff yields occam razor principle rigorously shown correctly companion 
shown implies data compression best strategy hypothesis identification prediction 
minimum description length principle scientists formulate theories steps 
firstly scientist scientific observations formulates alternative hypotheses infinity alternatives secondly definite hypothesis selected 
second step subject inference statistics 
historically done different principles fisher maximum likelihood principle various ways bayesian formula different prior distributions 
dominant ones common sense idea applying occam razor principle choosing simplest consistent theory 
simple 
equate simplicity shortness binary description reducing razor objective data compression 
single principle theoretically sound practically satisfiable situations 
fisher principle ignores prior probability distribution hypotheses 
apply bayes rule difficult usually know actual prior probability distribution 
prior distribution words written english sources ages social classes 
single principle turned satisfiable situations 
philosophically speaking relative shortness achievable ultimate data compression presents ideal way solving induction problems 
due non computability kolmogorov complexity associated algorithmic universal prior function theory directly 
approximation needed real world applications 
rissanen follows solomonoff idea substitutes computable approximation obtain called minimum description length principle 
rissanen gives principle importantly gives detailed formulas principle 
possible mdl principle 
basic form mdl principle intuitively stated follows minimum description length principle 
best theory explain set data minimizes sum ffl length bits description theory ffl length bits data encoded help theory 
survey development mdl principle statistical inference applications 
relationship bayesian approach minimum description length approach established 
general modeling principle mdl sharpened clarified abstracted ideal mdl principle defined bayes rule means kolmogorov complexity 
argument runs follows data sample family models hypotheses wants select model produced data 
priori possible data atypical model produced 
meaningful induction possible ignoring possibility 
strictly speaking selection true model improper usage modeling data irrespective truth falsehood resulting model appropriate 
fact data sample model class truth models impossible ascertain modeling possible hope 
wants select model data typical 
best models part description data model concise possible 
simplest best accordance occam razor principle summarizes relevant properties data concisely possible 
probabilistic data data subject noise involves separating regularities structure data random effects 
bayes formula choose hypothesis maximizes posterior hjd 
negative logarithm sides equation gamma log hjd gamma log djh gamma log log log constant ignored just want optimize left hand side equation maximizing hjd possible equivalent minimizing gamma log hjd minimizing gamma log djh gamma log obtain ideal mdl principle suffices replace terms sum djh respectively 
view appendix justified provided gamma log djh gamma log djh gamma log gamma log 
show basic condition substitution justified encapsulated fundamental inequality broad terms states substitution valid data random relative contemplated hypothesis hypotheses random relative universal prior 
basically ideal mdl principle states prior probability associated model algorithmic universal probability sum log universal probability model plus log probability data model minimized 
technical reasons probability djh computable 
important note algorithmic universal prior compress model prefix kolmogorov complexity gamma log 
applying ideal mdl principle compresses de description data encoded model djh prefix kolmogorov complexity djh gamma log djh model minimizing sum complexities 
roughly speaking mdl selection assumes data set typical selected model mdl aims selecting model data typical happened different true model inappropriately generated atypical data 
manner application mdl resilient overfitting model 
ideal mdl versus real mdl algorithmic universal prior ideal mdl principle valid set data samples lebesgue measure random typical outcomes contemplated hypothesis 
typical outcomes djh gamma log djh means classic shannon fano code length reaches prefix kolmogorov complexity data samples 
shannon fano code assigns code words length gamma log delta elements randomly drawn probability density delta fact applied statistical version mdl 
assumption data sample typical contemplated hypotheses ideal mdl principle applied statistical coincide valid set data samples lebesgue measure 
result obtained statistical theory probabilistic arguments 
term gamma log djh known self information information theory negative log likelihood statistics regarded number bits takes encode ideal code relative different applications hypothesis mean different things decision trees finite automata boolean formulas polynomial 
example general statistical applications assumes model set parameters precision number may vary influence descriptional complexity 
example want determine distribution length beans normal distribution oe parameters median variation oe 
essentially determine correct hypothesis described identifying type distribution normal correct parameter vector oe 
cases minimize gamma log dj gamma log example consider fitting best polynomial sample points dimensional plane 
question notoriously st degree polynomial best fit gamma th degree polynomial perfect fit arguably right solutions 
mdl principle find objective best polynomial polynomials degrees 
fixed gamma best polynomial degree fitted points minimizes error error gamma assume coefficient takes bits 
encoded bits 
interpret measurements argument true polynomial determined 
assume measurement process involves errors 
errors accounted commonly gaussian normal distribution error 
true polynomial pr jf exp gammao gamma negative logarithm delta error computable mdl principle tells choose gamma minimizes delta error 
original solomonoff approach hypothesis turing machine 
general avoid general approach order keep things computable 
different applications mean different things 
example infer decision trees decision tree 
case learning boolean formulas may boolean formula 
fitting polynomial curve set data may polynomial degree 
experiment model particular character 
encoded binary string prefix free set set codes prefix free code set prefix 
experiment line handwritten characters model development basic assumptions character drawn planar surface viewed composite planar curve shape completely determined coordinates sequence points curve 
order sequence determined line processing data scanning machinery time writing character 
shape tends vary person person time time coordinates point sequence 
key assumption treatment particular person writing consistently shape curve tends converge average shape sense means corresponding coordinates sampled point sequences converge 
assume ffl shape example curve particular character contains set distinguished feature points 
ffl point average instances different examples converges mean 
ffl fixed probability distribution possibly unknown point symmetric mean value variance assumed character drawings 
essentially assume person hand writing fixed associated probability distribution change 
feature space feature extraction prototypes digitizer tablet inch resolution horizontal vertical directions scanner obtain send coordinates sequential points character curve tablet microprocessor ibm ps model computer 
system implemented programming language coordinates normalized grid horizontal vertical directions 
character drawn tablet processed line 
sequence coordinates order time entry stored form linked list 
list preprocessed order remove repeating points due hesitation time writing fill gaps sampled points resulted sampling rate limit tablet 
needs explanation digitizer maximum sampling rate points second 
person writes character seconds points character curve sampled leaving gaps points 
preprocessing procedure ensures resulting linked list pair consecutive points curve component coordinates stored integers differing coordinate component differing 
preprocessed list jx gamma jy gamma jx gamma jy gamma preprocessed curve coordinate list sent feature extraction process 
far coordinates integers range 
coordinates certain points character curves taken relevant features 
feature extraction done follows 
character may consist stroke stroke trace pen drop pen lift starting points stroke taken features 
feature points taken fixed interval say point points preprocessed curve called feature extraction interval 
ensure feature points roughly equally spaced 
euclidean length points stroke curve excluding point stroke varies diagonal 
sequence feature point coordinates extracted character drawing constitute feature vector 
character drawing contains stroke feature vector consists concatenation feature vectors individual strokes time order 
dimension feature vector number entries twice number entry coordinate components 
obviously dimension feature vector random variable shape total number points character curve varies time time 
dimension feature vector largely determined feature extraction interval 
extracted feature vector character viewed prototype character stored knowledge base system 
comparison feature vectors system employed recognize characters learn 
trained examples character drawings data source supposed recognize 
data source means person writing consistently 
basic technique training recognition comparison matching prototypes feature vectors 
compare prototypes feature vectors equal dimension simply take euclidean distance vectors 
mathematically means subtracting component vector corresponding component feature vector summing square differences square root sum 
prototypes distance gamma gamma knowledge base system collection feature vectors stored form linked list 
feature vector example particular character called prototype character 
newly entered character drawing form feature vector compared prototypes knowledge base 
assume feature vectors extracted examples character dimension 
comparison technique system follows spirit mathematical definition elastic 
consequence corresponding feature points may located different places feature vectors 
problem solved called elastic matching compares newly sampled feature vector set stored feature vectors prototypes 
elasticity reflected aspects dimension tolerance constant integer new feature vector compared stored feature vector dimension different 
new feature vector feature points compared matched prototypes number feature points range gamma 
local extensibility integer constant ith feature point new feature vector compared feature points index ranging gamma prototype satisfying dimension tolerance 
euclidean distance way considered true difference vectors ith feature point 
definition dimension new feature vector elastic distance ffi prototype defined ffi dimension tolerance ffi 
particular problem experimental evidence indicates suffices set 
experiment elastic distance new feature vector prototype computed 
knowledge base learning knowledge base constructed learning phase system sampling feature vectors handwritten characters telling system character example 
system uses learning algorithm establish knowledge base 
step 
initialize knowledge base empty set 
elements triples preprocessed feature vector prototype character value prototype counter 
assign values weights ff fi combine prototypes ff fi 
step 
sample new example character feature vector say preprocessing character value say 
user draws new example handwritten character tablet indicates character value character drawing represents system 
step 
check prototypes exist character prototype store prototype setting fy zg nonempty list prototypes determine elastic distances ffi ffi 
min set prototypes min ffi ffi min def min may may prototypes character value step 
min minimum distance ffi min prototypes ffx fix new prototype combined existing prototype weighted average coordinate produce modified prototype character 
add counter associated prototype step 
step applicable min delta minimum distance prototype character value 
distinguish cases 
case 
ffi ffi min 
number character drawings consecutively combined form current prototype 
set ffx fix expected modified prototype minimal distance ffi min time similar drawing character value arrives 
case 
condition case satisfied 
new prototype saved knowledge base new prototype character setting notice prototype single character allowed knowledge base 
recognition unknown character drawing recognition algorithm simple 
assume learning algorithm executed knowledge base prototypes possible characters constructed 
new character drawing system compared prototypes knowledge base dimension variation range specified dimension tolerance 
character prototype minimum ffi distance feature vector considered character value feature vector 
rationale prototypes considered mean values feature vectors characters variances distribution assumed prototypes 
formally recognition algorithm follows 
step 
sample new example character feature vector say preprocessing 
user draws new example handwritten character tablet preprocessed form feature vector 
step 
delta delta knowledge base determine elastic distances ffi ffi 
ffi minimal distance set case prototype induces minimum distance set character value recognize character concludes main procedure training classification 
remarks order explain differences original elastic matching method 

process differs original elastic matching method way prototype construction 
prototype allowed single character 
procedure learning algorithm prototype statistical mean number positive examples character 

prototype feature vector turn point feature space dimension 
classification statistical inference rate correct classification depends prototypes knowledge base constructed variability handwriting subject 
prototype allowed character knowledge base prototypes may result overly dense feature space 
ffi distance points prototypes knowledge base feature space comparable variability subjects handwriting rate correct classification may drop considerably 

prototypes knowledge base constitute model system 
prototypes constructed essentially determine rate correct classification performance model 
scheme described prototypes constructed extracting points constant interval 
generally speaking points prototypes gives detailed image character drawing may insert random noise model 
application mdl guide selection best feature extraction interval main thrust proceed 
implemented description lengths minimization expression mdl consists terms model error coding lengths 
coding efficiency terms comparable minimizing resulted expression total description length give complicated simple models 
particular problem coding lengths determined practical programming considerations 
set character drawings exactly characters processed feature vectors learning algorithm form raw database 
character drawings stored standardized integer coordinate system standardized axis 
preprocessing input learning algorithm establish knowledge base collection prototypes normalized real coordinates selected feature extraction interval 
experimentally determined error model code lengths subsequent construction knowledge base system tested having classify set character drawings recognition algorithm 
procedure served establish error code length model code length defined follows 
definition error code length exception complexity sum total number points incorrectly classified character drawings 
represents description data hypothesis 
model code length model complexity total number points prototypes machine knowledge base multiplied 
represents hypothesis 
total code length sum error code length model code length 
factor model code length due fact prototype coordinates stored real numbers takes twice memory programming language character drawing coordinates represented integer form 
wonder prototype coordinates real integer numbers 
reason facilitate elastic matching give small resolution comparisons classification 
model error code lengths directly related feature extraction interval 
smaller interval complex model smaller error code length 
effect reversed feature extraction interval goes larger values 
total code length sum code lengths value feature extraction interval minimizes total code length 
feature extraction interval considered best spirit mdl 
corresponding model knowledge base considered optimal sense contains essence raw data eliminates redundancy due noise raw data 
optimal feature extraction interval experimentally determined carrying described build building knowledge base test set characters built number different feature extraction intervals 
fraction correctly classified training data actual optimization process implemented actual system constructed available user 
particular set characters trial results classifying recognition algorithm set character drawings learning algorithm establish knowledge base 
quantities depicted model code length error code length total code length versus different feature extraction intervals feature extraction interval 
larger feature extraction intervals model complexity small character drawings misclassified giving large error code length large total code length 
hand feature extraction interval low extremal value training characters get correctly classified gives zero error coding length 
model complexity reaches largest value resulting large total code length 
minimum total code length occurred experiment extraction interval gives percent correct classification 
illustrates fraction correctly classified character drawings training data 
fraction correctly classified new test data validation model optimal model determined choosing interval yielding minimal total code length training data really performs better models class different feature extraction intervals tested classification new data new character drawings 
executed test having set characters drawn anew person provided raw data base build knowledge base 
preprocessing feature vectors resulting data entered recognition algorithm 
new data considered source previous data set 
new data set classified system knowledge bases built learning algorithm training data set character drawings different feature extraction intervals 
test results plotted terms fraction correct classification correct ratio versus feature extraction interval feature extraction interval interesting see correct classification occurred feature extraction intervals 
values feature extraction intervals close optimal value resulting mdl considerations 
furthermore lower feature extraction intervals correct classification rate drops indicating disturbance caused model 
recommended working feature extraction interval particular type character drawings 
experiment modeling robot arm second problem model jointed robot arm described 
mathematical description follows 
lengths limbs constituting arm 
limb length located joint origin dimensional plane arm moves 
angle limb horizontal axis angle limb length second limb limb second joint relationship coordinates free second limb hand speak variables cos cos sin sin goal construct feedforward neural network correctly associates coordinates coordinates 
set 
setup similar character recognition experiment data real world computer generated 
generated random examples relation formula gaussian noise magnitude added outputs 
want learned model extrapolate training examples interpolate training sets consist random examples taken limited separate areas domain 
training data angle degrees degrees second angle degrees 
test extrapolation learned model unseen test set ranges degrees 
model features model class consists layer feedforward networks 
layer input layer consisting input nodes input real values angles nodes input layer connected node second layer hidden layer number nodes determined 
node second layer connected nodes third output layer yielding real valued output values connections pairs nodes 
second layer nodes transfer functions third layer output nodes linear transfer functions 
unknowns network number nodes hidden layer weights connections biases nodes hidden layer 
number nodes hidden layer learned weights biases network standard methods presenting training set 
learned models evaluated prediction errors unseen test set 
experiments noticed test set domain training set testing interpolation extrapolation increase error increasing number nodes hidden layer optimal number small 
unseen test set described earlier testing extrapolation generalization increase error optimal network size steep 
generalization test set 
determining size hidden layer mdl verify contention experimental setting hypothesis selected mdl principle training data set expected predictor classification unseen data test set 
neural networks coded way 
topology biases nodes weights links coded 
assume network contains nodes 
code starts number list bias values encoded bits bias value 
need theta gamma bits describe pairs nodes connected directed arcs possibly ways 
weight link precision bits 
concatenating descriptions binary string retrieve network parse constituent parts 
keeping order constituents know start encoding prefix free code log log log bits 
total description takes log log log theta gamma theta bits number directed edges links 
layer feedforward networks constitute models input nodes output nodes nodes hidden layer topology fixed 
stated choose weights links biases hidden nodes 
gives descriptions length log log log theta bits 
range consider logarithmic terms ignored 
model cost set kl bits precision model cost linear bits 
encoding output data neural network determine error cost mdl setting depends integers reals 
integers takes bits maxint format requires real numbers standard prefix free coding see appendix usually twice bits 
reals encoding introduces new problem output correct 
consider correct real distance output vector target vector small fixed real value 
mdl code example consists reals encoded bits 
examples exceeding small fixed error cut level set encoded bits 
ignore amount misclassified output real value differs target real value may large small 
total error encoded explicit list misclassified examples 
mdl selects model minimizes sum model length total error length important large training set choose 
coding length models fixed training set size total error length depends 
small training set number erroneous misclassified examples may small compared model code length difference simple models small complex models large 
large training sets opposite happens 
exactly right small number examples simpler models encouraged 
complex model justified size training set 
intuitively increasing training set size eventually smallest model low error set expected stabilize low prediction error 
coding length bits number hidden nodes mdl codelength model code error code prediction mdl 
experiment random training set examples network nodes hidden layer trained training cycles 
shows results terms mdl model code length error code length total description length function number nodes hidden layer 
total code length reached hidden nodes mdl predicts hidden nodes average squared error number hidden nodes generalization error average squared error number hidden nodes training error error training set test set granularity hidden layer speak give best model 
node away optimal network size determined experimentally 
validation model determine best number hidden nodes different random training sets examples 
network trained training cycles 
sophisticated criteria checking showed general performance network training cycles close optimal 
error example real distance output vector target vector 
average squared error training set average squared prediction error unseen test set displayed function number nodes hidden layer 
optimal network sense having best extrapolation generalization properties modeling unseen examples test set correctly network hidden nodes 
expected error training set keeps decreasing increasing number nodes hidden layer model increasingly complex capable modeling detail 
look prediction error new examples training data see average squared prediction error decreases model complexity increases optimum minimum error error starts increase 
experimentally best number hidden nodes problem training set size examples predicted simplified application mdl 
discussion applied theoretical minimum description length principle different experimental tasks aimed learning best model discretization 
application learning recognize isolated handwritten characters line elastic matching statistical technique 
model collection prototypes built raw training character drawings line points curves executed character drawing constant feature extraction interval combining closely related character drawings 
novel features multiple prototypes character mdl principle choose optimal feature extraction interval 
model optimized spirit mdl minimizing total code length sum model error model code lengths different feature extraction intervals 
resulting model optimal theory 
validated testing different set character drawings source 
believe result small test gives evidence mdl may tool area handwritten character recognition 
second application modeling robot arm layer feedforward neural network precision parameter learned number nodes hidden layer 
mdl predicted number nodes validated extensive testing model respect extrapolation generalization capabilities unseen examples test set 
optimal granularity models predicted sensible values marginally different experimentally determined optimal ones 
shows rigorous ad hoc form occam razor quite 
comparison performance admittedly limited experiments robot arm problem principles nic aic indicated mdl performance better competitive 
similar theory practice validation case bayesian framework model comparison mackay 
inspired robot arm problem mdl setting 
note bayesian framework genuinely different rigorously demonstrated companion 
known prefix code length equivalent negative log probability code code corresponds equivalent probability 
mdl coding approach principle translated back bayesian approach model code gives prior 
analysis shows data model error may correspond conditional data model probability data atypical contemplated prior 
authors coding large data natural reasoning possibly probabilities 
directions general mdl method appears suited supervised learning best model discretization parameters classification problems error coding straightforward 
applying mdl method simple computationally expensive 
central point mdl optimal granularity model parameters computed automatically tuned manually 
approach constitutes rational feasibly computable approach feature selection opposed customary ad hoc approaches 
purpose presenting theory outline example applications stimulate re different areas pattern recognition classification image understanding region segmentation color clustering segmentation 
grateful les valiant discussions machine learning suggestion research guido te brake joost kok executing robot arm experiment referees insightful comments 
appendix kolmogorov complexity kolmogorov complexity finite object length shortest effective binary description give brief outline definitions properties 
details see 
denotes natural numbers identify correspondence ffl ffl denotes empty word letters 
length number bits binary string example ffl 
emphasis binary sequences convenience observations alphabet encoded way theory neutral 
binary string proper prefix binary string write yz ffl 
set fx prefix free pair distinct elements set proper prefix 
prefix free set called prefix code 
binary string special type prefix code called self delimiting code 
code self delimiting determine code word ends reading left right backing 
code define standard self delimiting code easy check log develop theory turing machines set lisp programs set fortran programs 
standard enumeration turing machines oe oe enumeration corresponding functions computed respective turing machines 
computes oe functions partial recursive functions computable functions 
kolmogorov complexity length shortest binary program computed 
formally define follows 
definition kolmogorov complexity free special input tape xjy min fl oe ng define 
defined terms particular machine model kolmogorov complexity machine independent additive constant acquires asymptotically universal absolute character church thesis ability universal machines simulate execute effective process 
kolmogorov complexity object viewed absolute objective quantification amount information 
leads theory absolute information contents individual objects contrast classic information theory deals average information communicate objects produced random source 
technical reasons need variant complexity called prefix kolmogorov complexity associated turing machines set programs resulting halting computation prefix free 
realize turing machine way input tape separate tape way output tape 
turing machines called prefix machines halting programs form prefix free set 
universal prefix machine define prefix complexity analogously plain kolmogorov complexity 
shortest program set fx prefix code 
code word code words prefix deltai standard invertible effective encoding theta recursive subset example set hx yi insist prefix freeness recursiveness want universal turing machine able read image deltai left right determine ends 
definition prefix kolmogorov complexity free xjy min fl hp ii oe hp yi ng define 
nice thing interpret gammak probability distribution length shortest prefix free program fundamental kraft inequality see example know code word lengths prefix code gammal 
leads notion algorithmic universal distribution rigorous form occam razor 
appendix universal distribution turing machine computes function natural numbers 
consider computation real valued functions 
purpose consider argument oe value oe pair natural numbers standard pairing function deltai 
define function reals turing machine computing function oe follows 
interprete computation oe hx ti hp qi mean quotient rational valued tth 
definition function enumerable turing machine computing total function oe oe oe lim oe 
means computably approximated 
computably approximated call recursive 
function probability distribution 
inequality technical convenience 
consider surplus probability concentrated undefined element 
consider family ep enumerable probability distributions sample space equivalently 
known ep contains element multiplicatively dominates elements ep ep constant call algorithmic universal distribution shortly universal distribution 
family ep contains distributions computable parameters name conceivably interested considered 
dominating property means assigns probability object distribution family ep 
sense universal priori accounting maximal ignorance 
turns true priori distribution bayes rule recursive single distribution continuous analogue measure sample space prediction provably true priori distribution 
know choose gamma log means assigns high probability simple objects low probability complex random objects 
example log log log program print times prints 
additional log log term penalty term self delimiting encoding 
log 
flip coin obtain string bits overwhelming probability contain effective regularities allow compression 
algorithmic universal distribution astonishing properties 
interest ai community gives rigorous meaning occam razor assigning high probability simple regular objects low probability complex irregular ones 
popular account see 
celebrated result states object individually random respect conditional probability distribution iff log xjy xjy 
implied constant notation fact related length shortest program computes probability xjy input particular means typical general position respect conditional distribution iff real probability xjy close algorithmic universal probability xjy gammak xjy barron rissanen yu minimum description length principle coding modelling ieee trans 
inform 
theory 
bishop neural networks pattern recognition oxford university press 
te brake kok vit anyi model selection neural networks comparing mdl nic proc 
european symposium artificial neural networks brussels april 
cover thomas elements information theory wiley new york 
gao li 
application minimum description length principle online recognition 
th international joint conference artificial intelligence pages 
morgan kaufmann publishers 
jeffreys 
theory probability 
oxford clarendon press oxford 
third edition 
kearns vazirani computational learning theory mit press 
li vit anyi universal distribution mathematical 
kolmogorov 
approaches quantitative definition information 
problems inform 
transmission 

feature analysis symbol recognition elastic matching 
ibm res 
develop 
li vit anyi 
inductive reasoning kolmogorov complexity 
comput 
syst 
sci 
li vit anyi 
kolmogorov complexity applications springer verlag new york nd edition 
mackay 
practical bayesian framework backpropagation networks 
neural computation 
martin lof definition random sequences inform 
contr 
von mises grundlagen der 

quinlan rivest 
inferring decision trees minimum description length principle 
inform 
computation 
rissanen 
modeling shortest data description 
automatica ifac 
rissanen 
universal coding information prediction estimation 
ieee transactions information theory 
rissanen 
minimum description length principle 
kotz johnson editors encyclopaedia statistical sciences vol 
pages 
wiley new york 
rissanen 
stochastic complexity 
royal stat 
soc series 
discussion pages 
solomonoff formal theory inductive inference part part inform 
contr 
solomonoff 
complexity induction systems comparisons convergence theorems 
ieee trans 
inform 
theory 
suen 
automatic recognition characters state art 
proc 
ieee 

cursive script recognition elastic matching 
ibm res 
develop 
valiant 
deductive learning 
phil 
trans 
royal soc 
lond 
valiant 
theory learnable 
comm 
assoc 
comput 
mach 
vit anyi li minimum description length induction bayesianism kolmogorov complexity ieee trans 
inform 
theory 

