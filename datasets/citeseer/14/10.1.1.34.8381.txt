adaptation relief attribute estimation regression marko university ljubljana faculty computer information science ljubljana slovenia marko fri uni lj si igor kononenko university ljubljana faculty computer information science ljubljana slovenia igor kononenko fri uni lj si heuristic measures estimating quality attributes assume independence attributes domains strong dependencies attributes performance poor 
relief extension relieff capable correctly estimating quality attributes classification problems strong dependencies attributes 
exploiting local information provided different contexts provide global view 
analysis relieff lead adaptation regression continuous class problems 
experiments artificial real world data sets show relieff correctly estimates quality attributes various conditions non myopic learning regression trees 
relieff relieff provide unified view estimating attribute quality regression classification 
majority current propositional inductive learning systems predict discrete class 
solve regression called continuous class problems discretizing prediction class advance 
approach inappropriate 
regression learning systems called function learning systems cart breiman quinlan directly predict continuous value 
problem estimating quality attributes important issue classification regression machine learning general feature selection constructive induction 
heuristic measures estimating attribute quality assume independence attributes information gain hunt gini index breiman distance measure mantaras measure smyth goodman discrete class mean squared mean absolute error breiman regression 
appropriate domains strong dependencies attributes 
relief kira rendell extension relieff kononenko aware contextual information correctly estimate quality attributes classification problems strong dependencies attributes 
similar approaches contextual merit hong geometrical approach elomaa ukkonen 
analysis relieff lead adapt regression problems 
researchers investigated local information profited aware domingos atkeson friedman 
approach described directly comparable theirs provides different perspective 
relief commonly viewed feature selection method applied step model learned learning process select splits building phase decision tree kononenko 
experimented similar regression trees 
section analyze novel rrelieff relieff algorithm estimating quality attributes regression problems 
section describes experiments estimation attributes various conditions section describes rrelieff learning regression trees 
section summarizes gives guidelines 
algorithm relief input training instance vector attribute values class value output vector estimations qualities attributes 
set weights 

randomly select instance 
find nearest hit nearest 
attributes 
diff diff 
basic relief algorithm rrelieff relief relieff classification key idea original relief algorithm kira rendell estimate quality attributes values distinguish instances near 
purpose randomly selected instance line relief searches nearest neighbors class called nearest hit different class called nearest line 
updates quality estimation attributes depending values lines 
process repeated times user defined parameter 
function diff attribute instance instance calculates difference values attribute instances 
discrete attributes defined diff ae value value continuous attributes diff gamma value max gamma min function diff calculating distance instances find nearest neighbors 
total distance simply sum distances attributes 
relief estimate quality attribute approximation difference probabilities kononenko diff value inst diff class gammap diff value inst class complexity relief training instances attributes theta theta 
original relief deal discrete continuous attributes 
deal incomplete data limited class problems 
kononenko shown relief estimates strongly related impurity functions developed extension called relieff robust tolerate incomplete noisy data manage multiclass problems 
difference original relief interesting regression nearest hit nearest relieff uses nearest hits misses averages contribution 
power relief ability exploit information locally context account provide global view 
rrelieff regression regression problems predicted value class continuous nearest hits misses 
requiring exact knowledge instances belong class introduce kind probability predicted values instances different 
probability modeled relative distance predicted class values instances 
estimate information sign contributed term missing 
derivation reformulate directly evaluated probability predicted values instances different 
rewrite diff value instances different instances different value nearest instances obtain bayes rule gamma gamma gamma estimate approximating terms defined equations 
done algorithm 
weights different prediction class different attribute different prediction different attribute collected ndc ndc da respectively 
final estimation attribute equation computed lines 
term lines take account distance instances closer instances greater influence exponentially decreased influence instance distance instance gamma rank oe rank rank instance sequence instances ordered distance oe user defined parameter 
experimented constant influence nearest instances instance results differ significantly 
consider general chosen presentation 
note time complexity rrelieff original relief theta theta 
complex operation main loop selection nearest instances compute distances done theta steps instances 
complex operation 
needed build heap nearest instances extracted log steps theta 
estimating attributes section examine ability rrelieff recognize rank important attributes section abilities learning regression trees 
compare estimates rrelieff mean squared error mse measure attribute quality breiman 
measure standard regression tree systems 
criterion best attribute minimizes equation mse pl delta pr delta subsets cases go left right respectively split pl pr proportions cases go left right 
standard deviation predicted values cases subset gamma minimum possible splits attribute considered quality estimate results 
families artificial data sets check behavior rrelieff different circumstances 
fraction domain contains continuous attributes values 
predicted value fractional part sum important attributes gamma domains floating point generalizations parity concept order domains highly dependent pure continuous attributes 
modulo domains described set attributes value attribute integer value range 
half attributes treated discrete half continuous continuous attribute exact match discrete attributes 
predicted value sum important attributes modulo mod 
domains integer generalizations parity concept sum modulo order shall show rrelieff recognizes highly dependent attributes ranks discrete continuous attributes equal importance 
parity domain consists discrete boolean attributes 
informative attributes define parity concept parity bit predicted value set random number randomly chosen 
rand mod rand mod algorithm rrelieff input training instance vector attribute values predicted value output vector estimations qualities attributes 
set ndc ndc da 

randomly select instance 
select instances nearest 

ndc ndc jf gamma delta 
attributes 
diff delta 
ndc da ndc da jf gamma delta 
diff delta 



attributes 
ndc da dc gamma ndc da gamma ndc pseudo code rrelieff relieff concepts blurred versions parity concept order 
shall test behavior rrelieff discrete attributes 
linear domain described continuous attributes values chosen randomly predicted value computed linear formula gamma gamma included domain compare performance rrelieff mse known recognize linear dependencies 
domain continuous attributes values prediction computed follows gamma cos 
shall show ability heuristics handle non linear dependencies 
experiments important attributes 
domains irrelevant random attributes values range important attributes 
domain generated examples computed estimates average fold cross validation 
collected data eliminate probabilistic effect caused random selection instances rrelieff 
evaluated significance differences estimates paired test level 
experiments rrelieff run default set parameters constant main loop nearest oe see 
varying number examples investigated number available examples influences estimates 
generated domains important attributes added gamma random attributes values range total number attributes linear fixed respectively cross validated estimates attributes altogether data sets varying size data set steps examples 
shows dependence fraction domain important attributes 
note rrelieff gives higher scores better attributes mse opposite 
see small number examples random attribute highest estimate best random estimated better important attributes 
increasing number examples estimates important attributes significantly better estimate best random attribute 
bottom shows mse incapable distinguishing important random attributes 
random attributes lowest estimate esti rrelieff number examples fraction number examples best random mse number examples fraction number examples best random varying number examples fraction domain important random attributes 
note rrelieff gives higher scores better attributes mse opposite 
mated better 
behaviors rrelieff mse similar fraction modulo parity data sets 
due lack space omit graphs comment summary results table 
numbers limiting number examples needed estimates important attributes estimated worst best important attributes significantly better estimates attribute estimated best random attributes 
sign means estimator succeed significantly distinguish groups attributes 
observe number examples needed increasing increasing complexity number important attributes problem 
parity fraction modulo solvable rrelieff mse completely lost 
modulo important attributes difficult rrelieff 
examples problem complexity complexity grows exponentially number peaks instance space modulo domain confirmed additional experiment examples rrelieff succeeded separate groups examples 
table results varying number examples 
numbers number examples required ensure relevant attributes ranked significantly higher irrelevant attributes 
rrelieff mse domain fraction modulo parity linear interesting modulo problem important discrete attributes considered better continuous counterparts 
understand consider behavior diff function see 
take cases values attribute respectively 
discrete attribute value diff categorical values different 
continuous attribute diff gamma 
form diff function continuous attributes underestimated 
overcome problem ramp function proposed hong 
defined generalization diff function continuous attributes diff eq diff gammat eq diff gammat eq eq diff gamma value presents distance attribute values instances eq diff user definable threshold values eq maximum distance attribute values consider equal diff minimum distance attribute values consider different 
omitted ramp function presentation complicates basic idea tests sensible default thresholds shown continuous attributes longer underestimated 
results linear domains show separating worst important attribute respectively best random attribute easy rrelieff mse mse better 
rre ri rrelieff estimates noise added predicted value 
fraction domain important attributes 
distinguish attributes significantly examples occasionally peak estimation random attributes caused value fall significance threshold 
mse distinguish groups lower variation gives slight advantage 
separating best important attribute random attributes rrelieff mse successful rrelieff needed examples 
probably compensates negative result rrelieff worst important attribute regression tree learning single best attribute selected node 
difference estimators ranking attributes importance domain 
correct decreasing order replicated rrelieff mse orders unable detect non linear dependencies 
linear domains show relatively simple relations performance rrelieff mse comparable 
tested types non linear dependencies attributes logarithmic exponential polynomial trigonometric 
rrelieff superior mse 
adding noise changing predicted value checked robustness rrelieff noise setting data sets important attributes altogether attributes number examples fixed 
added noise data sets changing certain percent predicted values random value range correct values 
varying noise 
shows dependence fraction domain important attributes 
see rrelieff robust significantly distinguish worst important attribute best random corrupted prediction values 
table summarizes results domains 
columns give maximal percentage corrupted prediction values estimates worst important best important attribute respectively significantly better estimates best random attribute 
sign means estimator succeed significantly distinguish groups noise 
table results adding noise predicted values 
numbers tell percent predicted values corrupt get significant differences estimations 
rrelieff mse domain fraction modulo parity linear adding random attributes rrelieff mse sensitive random attributes 
tested sensitivity similar settings data sets important attributes number examples fixed added random attributes 
shows dependence fraction domain important attributes 
see rrelieff quite tolerant kind noise random attributes prevent assign significantly different estimates worst informative best random attribute 
table summarizes results 
mse estimates attribute separately sensitive kind noise include experiment 
columns give maximal number random attributes added estimates worst best important attribute respectively significantly better estimates ri rrelieff estimates random attributes added fraction domain important attributes 
table results adding random attributes 
numbers tell random attributes add significantly differentiate worst best important attribute best random attribute domain worst best fraction modulo parity linear best random attribute 
sign means estimator succeed significantly distinguish groups single random attribute 
building regression trees developed learning system builds binary regression model trees named quinlan recursively splitting training examples values attributes 
attribute node selected estimates attributes quality rrelieff mse 
estimates computed subset examples reach current node 
relieff classification problems shown sensible significantly outperform impurity measures kononenko 
run system sets parameters procedures 
named type models leaves 
point mode similar cart breiman uses average prediction class value examples leaf node predictor 
cost complexity pruning cart demands cross validation separate set examples setting complexity parameter pruning estimate probability produces comparable better results 
linear mode similar quinlan uses pruned linear models leaf node predictor 
procedures pruning smoothing trees 
trees linear models offer greater expressive power perform better real world problems contain form near linear dependency quinlan 
problem overfitting data relieved pruning procedure employed reduce linear formula constant term evidential support 
modes uses default set parameters growing pruning tree 
stopping criteria minimal number cases leaf minimal purity leaf proportion root standard deviation leaf 
ran system artificial data sets domains continuous prediction value uci murphy aha 
artificial data sets described data sets consisting attributes important rest random containing examples 
domain collected results average fold cross validation 
results table 
compare relative mean squared error predictors oe re oe oe gamma th example written ordered pair vector attribute values 
value predicted predictor returns mean value prediction values 
sensible predictors re oe 
error included measure complexity tree 
number occurrences attributes tree plus table relative error complexity regression model trees rrelieff mse estimators quality attributes 
linear point rrelieff mse rrelieff mse domain re re re re fraction fraction fraction modulo modulo modulo parity parity parity linear auto mpg auto price cpu housing servo constant term leaves 
column labeled presents significance differences rrelieff mse computed paired test level significance 
indicates differences significant level means predictor rrelieff significantly better implies significantly better score mse 
linear mode left side table predictor generated rrelieff artificial data sets significantly better predictor generated mse 
uci databases predictors comparable rrelieff better data sets mse insignificant differences 
complexity models induced rrelieff considerably smaller cases artificial uci data sets indicates rrelieff successful detecting dependencies 
rrelieff strong dependencies detected expressed selection appropriate attributes upper part tree remaining dependencies incorporated models leaves tree 
mse blind strong dependencies splitting examples solely minimize impurity mean squared error prevented successfully model weaker dependencies linear formulas leaves tree 
point mode rrelieff produces smaller accurate trees problems strong dependencies fraction modulo parity uci databases mse better data sets rrelieff data set differences significant 
mse produced complex trees linear uci datasets 
reason similar opposite effect observed linear mode 
probably strong dependencies domains ability detect bring advantage rrelieff 
mse minimizes squared error favored stopping criterion percentage standard deviation pruning procedure rely estimation error 
similar effect observed classification brodley empirically shown classification error appropriate selecting splits near leaves decision tree 
effect hidden linear mode trees smaller complex seen table linear models leaves play role minimizing accuracy 
experiments show rrelieff discover strong dependencies attributes domains dependencies performs mean squared error 
robust noise tolerant 
intrinsic contextual nature allows recognize contextual attributes 
experimental results conclude learning regression trees rrelieff promising especially combination linear models leaves tree 
rrelieff regression relieff classification kononenko estimators gives unified view estimation quality attributes classification regression 
rrelieff performance robustness indicate appropriateness feature selection 
planning rrelieff detection context switch incremental learning guide constructive induction process 
atkeson moore 

locally weighted learning 
technical report institute technology 
breiman friedman olshen stone 

classification regression trees 
wadsworth belmont california 
brodley 

automatic selection split criterion tree growing node location 
proceedings xii international conference machine learning 
morgan kaufmann 
domingos 

context sensitive feature selection lazy learners 
artificial intelligence review 
appear 
elomaa ukkonen 

geometric approach feature selection 
de raedt bergadano editors proceedings european conference machine learning pages 
springer verlag 
friedman 

flexible metric nearest neighbor classification 
technical report stanford university 
available ftp stanford edu pub friedman 
hong 

contextual information feature ranking discretization 
technical report rc ibm 
hunt martin stone 

experiments induction 
academic press new york 


employing linear regression regression tree leaves 
neumann editor proceedings ecai pages 
john wiley sons 
kira rendell 

practical approach feature selection 
sleeman edwards editors proceedings international conference machine learning pages 
morgan kaufmann 
kononenko 

estimating attributes analysis extensions relief 
de raedt bergadano editors machine learning ecml pages 
springer verlag 
kononenko 

overcoming myopia inductive learning algorithms relieff 
applied intelligence 


increasing performance consistency classification trees accuracy criterion leaves 
proceedings xii international conference machine learning 
morgan kaufmann 
mantaras 

id revisited distance criterion attribute selection 
proceedings int 
symp 
methodologies intelligent systems charlotte north carolina usa 
murphy aha 

uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
quinlan 

combining instance model learning 
proceedings international conference machine learning pages 
morgan kaufmann 
smyth goodman 

rule induction information theory 
piatetsky shapiro frawley editors knowledge discovery databases 
mit press 
